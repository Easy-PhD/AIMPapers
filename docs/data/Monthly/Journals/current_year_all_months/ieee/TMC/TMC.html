<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmc">TMC - 821</h2>
<ul>
<li><details>
<summary>
(2025). Joint DNN model deployment, selection, and configuration for heterogeneous inference services toward edge intelligence. <em>TMC</em>, <em>24</em>(11), 12726-12741. (<a href='https://doi.org/10.1109/TMC.2025.3586793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge intelligence is an emerging paradigm in edge computing that deploys Deep Neural Network (DNN) models on edge servers with limited storage and computation capacities to provide inference services for high mobility and real-time applications, such as autonomous driving or smart surveillance, with varying accuracy and delay requirements. Adapting application configurations (e.g., image resolution or video frame rate) while selecting different DNN models and deployment locations can provide high-accuracy, low-delay inference services that meet user requirements. However, the configurations and DNN models of various inference services are highly heterogeneous. As balancing inference accuracy, resource cost, and delay is a multi-objective programming problem, it is a great challenge to obtain the optimal solution. To address this challenge, we propose a novel online framework to jointly optimize the configuration adaption, DNN model selection, and deployment for heterogeneous inference services. Specifically, we first formulate this joint optimization problem as an integer linear programming problem and prove it is NP-hard. Then, we further model the problem as a Partial Observable Markov Decision Process (POMDP) and solve it by developing a Heterogeneous-Agent Reinforcement Learning (HARL) based algorithm, named Heterogeneous Inference Service ProvidER (HISPER). It allows agents to have different action spaces corresponding to different types of configurations and DNN models. Finally, extensive experiments demonstrate that the proposed algorithm outperforms other state-of-the-art counterparts.},
  archive      = {J_TMC},
  author       = {Hebin Huang and Junbin Liang and Geyong Min},
  doi          = {10.1109/TMC.2025.3586793},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12726-12741},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint DNN model deployment, selection, and configuration for heterogeneous inference services toward edge intelligence},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing transmission cost of distributed principal components analysis in wireless networks with accuracy guaranteed. <em>TMC</em>, <em>24</em>(11), 12711-12725. (<a href='https://doi.org/10.1109/TMC.2025.3586615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a classic data processing tool, Principal Component Analysis (PCA) has been widely applied in various data analysis applications. To mitigate the high computational complexity of PCA on Big Data, distributed PCA methods have been extensively studied, which disperse the computational tasks across multiple computation units while guaranteeing the accuracy. For the scenarios of distributed PCA in wireless networks, as the data is originally dispersed across different locations, it is further required to reduce the communication cost of distributed PCA in networks, which however has been seldom studied. Reducing the communication cost of distributed PCA in wireless networks requires not only appropriately partitioning the computation of PCA, ensuring accuracy, but also effectively assigning the partitioned computations and routing strategies to the nodes. In this paper, we propose CD-PCA, a communication-efficient distributed PCA (CD-PCA) scheme. This scheme implements a transmission-benefit equipartition strategy for the network to facilitate high-accuracy distributed computation and designs novel routing strategies for nodes to execute the distributed PCA within each partitioned region. Extensive simulation results demonstrate that the proposed CD-PCA scheme can reduce transmission costs by over 30% on average compared to related methods and baseline approaches.},
  archive      = {J_TMC},
  author       = {Yiyi Zhang and Peng Guo and Xuefeng Liu and Chao Cai and Kui Zhang and Jiang Liu},
  doi          = {10.1109/TMC.2025.3586615},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12711-12725},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reducing transmission cost of distributed principal components analysis in wireless networks with accuracy guaranteed},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive computation offloading scheme based on a collaborative architecture with heterogeneous MEC nodes: A DRL approach. <em>TMC</em>, <em>24</em>(11), 12692-12710. (<a href='https://doi.org/10.1109/TMC.2025.3586623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) has become an effective paradigm to support computation-intensive applications by providing services in close proximity to user devices (UDs). In MEC networks, computation offloading technology is devoted to balancing system load and prolonging UDs’ battery life. However, most existing studies on computation offloading take the impractical assumption of the MEC scenario with homogeneous users, ignoring security requirement from certain users. Moreover, with users mobility and task arrivals correlation, most existing computing offloading approaches suffer from inefficient or suboptimal decision making in practical MEC environments. To tackle these issues, by integrating task arrivals correlation within a time slot and environment dynamics between time slots, we propose an adaptive computation offloading scheme based on a collaborative architecture with heterogeneous MEC nodes. First, considering additional security requirement from very important people (VIP) users, we present a novel collaborative architecture by separating edge/cloud servers into public and private nodes. Then, with the architecture, we develop a dynamic computation offloading (DCO) algorithm to realize adaptive computation offloading scheme in MEC environment with mobile users. Particularly, the algorithm involves three stages. 1) By extending Poisson process into Markovian arrival process (MAP), we construct an MAP-based system model to capture the behavior of time-dependent task arrivals and then analyze the system model to derive the system delay in steady state. 2) For the purpose of minimizing the system delay in each time slot, we formulate a computation offloading problem in MEC environment with mobile users. 3) Under a deep reinforcement learning (DRL) framework, by taking the system delay as environmental feedback, we solve the formulated problem and provide offloading decisions in each time slot. We evaluate the performance of DCO algorithm by comparing it with other benchmark algorithms in various application scenarios. Results demonstrate that the proposed DCO algorithm outperforms the compared algorithms in response performance.},
  archive      = {J_TMC},
  author       = {Haixing Wu and Jiameng Zheng and Shunfu Jin},
  doi          = {10.1109/TMC.2025.3586623},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12692-12710},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive computation offloading scheme based on a collaborative architecture with heterogeneous MEC nodes: A DRL approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSMAAC: Multi-agent reinforcement learning based flight control in partially observable multi-UAV assisted crowd sensing systems. <em>TMC</em>, <em>24</em>(11), 12672-12691. (<a href='https://doi.org/10.1109/TMC.2025.3586429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile crowd sensing systems, existing flight control methods enable uncrewed aerial vehicles (UAVs) to provide high-quality data collection services for various applications. However, due to limited communication range, UAVs typically collect data under partial observability, hindering optimal performance without global environmental information. Additionally, many methods fail to enforce critical safety constraints. This paper proposes a communication-assisted safe multi-agent actor-critic-based UAV flight control method (CSMAAC). First, we propose an independent prediction communication partner model to address the partial observability problem. Based on the UAV’s local observation, causal inference is used to obtain prior communication information between UAVs through a feed-forward neural network to help UAVs determine potential communication partners. Second, we utilize a critic-network to predict and quantify inter-UAV influence and determine the necessity of communication. By exchanging necessary information inter-UAV, UAVs can perceive global information, thereby solving the UAV’s partial observability problem and reducing communication overhead. Moreover, we propose a similarity enhancement mechanism to improve the learning efficiency of the model by enhancing the connection between UAV observations and the policies of other UAVs. Finally, we introduce a safety layer to Actor-Network to ensure safe UAV flight. The simulation results show that the proposed method outperforms the baselines.},
  archive      = {J_TMC},
  author       = {Zhen Gao and Gang Wang and Lei Yang and Chenhao Ying},
  doi          = {10.1109/TMC.2025.3586429},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12672-12691},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CSMAAC: Multi-agent reinforcement learning based flight control in partially observable multi-UAV assisted crowd sensing systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARSys: An efficient and cross-platform development, deployment, and runtime system for mobile augmented reality. <em>TMC</em>, <em>24</em>(11), 12655-12671. (<a href='https://doi.org/10.1109/TMC.2025.3586797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) offers users immersive experiences to interact with digital contents in their physical space. However, practical AR applications are challenged by the tight coupling of algorithm and engineering during the development and deployment phases as well as the execution requirements of hybrid AR subtasks on heterogeneous and resource-constraint mobile devices. In this work, we build an end-to-end, cross-platform, and efficient AR system, called ARSys. The infrastructure in ARSys adopts the new principle of integrated design, unifies and refines AR fundamental capabilities, supports streaming media processing, model inference, and real-time rendering by exposing high-performance tensor compute engine to top, and constructs a Python multi-instance virtual machine as the cross-platform AR task execution container. The runtime mechanism of ARSys schedules AR tasks in a pipeline parallelism way and allocates subtasks to hardware backends by optimizing the slowest node. The development workbench and the deployment platform in ARSys allow the decoupling of algorithms written in Python from engineering components in C/C++ and further support remote debugging and quick validation of AR algorithms. We extensively evaluate ARSys in practical AR applications across high-end, mid-end, and low-end Android and iOS devices, demonstrating higher development, deployment, and runtime efficiency than existing MediaPipe-oriented framework. ARSys has been integrated into Mobile Taobao for production use.},
  archive      = {J_TMC},
  author       = {Chengfei Lv and Chaoyue Niu and Yu Cai and Xiaotang Jiang and Fan Wu and Guihai Chen},
  doi          = {10.1109/TMC.2025.3586797},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12655-12671},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ARSys: An efficient and cross-platform development, deployment, and runtime system for mobile augmented reality},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSNN model: A lightweight spiking neural network-based depression classification model for wearable EEG sensors. <em>TMC</em>, <em>24</em>(11), 12640-12654. (<a href='https://doi.org/10.1109/TMC.2025.3586591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression detection via wearable Electroencephalogram (EEG) sensor-assisted diagnosis system demands computationally efficient models compatible with resource-constrained edge devices. Spiking Neural Networks (SNNs) offer inherent advantages for processing the spatio-temporal patterns of EEG through event-driven neuromorphic computing. In this study, we innovatively present LSNNet, a lightweight SNN model specifically designed for wearable EEG sensors. The model exhibits low computational complexity with 7.18 K parameters and 67.68 M Floating-Point Operations (FLOPs). It requires only 246.88 KB of Random Access Memory (RAM) and 57.33 KB of Read-Only Memory (ROM) for on-board execution, and has been validated on both the single-core STM32U535CET6 and the multi-core GAP8 microcontrollers. Despite its minimal computational and memory requirements, LSNNet achieves impressive performance metrics, with a classification accuracy of 89.2%, specificity of 92.4%, and sensitivity of 86.4% in independent tests conducted on EEG data collected from 73 depressed patients and 108 healthy controls using our three-lead EEG sensor. Especially, when running on the GAP8 microcontroller, the LSNNet model has a low power consumption of 21.43 mW and a satisfactory inference time of 0.63 s while maintaining a classification accuracy of 87.5% (only with a reduction of 1.98% ). These results underscore the potential of integrating wearable EEG sensors with the LSNNet model for depression detection in the Internet of Things (IoT) era.},
  archive      = {J_TMC},
  author       = {Qinglin Zhao and Lixin Zhang and Haojie Zhang and Hua Jiang and Kunbo Cui and Zhongqing Wu and Jingyu Liu and Mingqi Zhao and Fuze Tian and Bin Hu},
  doi          = {10.1109/TMC.2025.3586591},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12640-12654},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LSNN model: A lightweight spiking neural network-based depression classification model for wearable EEG sensors},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Elevator, escalator, or neither? classifying conveyor state using smartphone under arbitrary pedestrian behavior. <em>TMC</em>, <em>24</em>(11), 12626-12639. (<a href='https://doi.org/10.1109/TMC.2025.3586618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowing a pedestrian’s conveyor state of “elevator,” “escalator,” or “neither” is fundamental to many applications such as indoor navigation and people flow management. Previous studies on classifying the conveyor state often rely on specially designed body-worn sensors or make strong assumptions on pedestrian behaviors, which greatly strangles their deployability. To overcome this, we study the classification problem under arbitrary pedestrian behaviors using the inertial navigation system (INS) of the commonly available smartphones (including accelerometer, gyroscope, and magnetometer). This problem is challenging, because the INS signals of the conveyor states are entangled by the arbitrary and diverse pedestrian behaviors. We propose ELESON, a novel and lightweight deep-learning approach that uses phone INS to classify a pedestrian to elevator, escalator, or neither. Using causal decomposition and adversarial learning, ELESON extracts the motion and magnetic features of conveyor state independent of pedestrian behavior, based on which it estimates the state confidence by means of an evidential classifier. We curate a large and diverse dataset with 36,420 instances of pedestrians randomly taking elevators and escalators under arbitrary unknown behaviors. Our extensive experiments show that ELESON is robust against pedestrian behavior, achieving a high accuracy of over 0.9 in F1 score, strong confidence discriminability of 0.81 in AUROC (Area Under the Receiver Operating Characteristics), and low computational and memory requirements fit for common smartphone deployment.},
  archive      = {J_TMC},
  author       = {Tianlang He and Zhiqiu Xia and S.-H. Gary Chan},
  doi          = {10.1109/TMC.2025.3586618},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12626-12639},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Elevator, escalator, or neither? classifying conveyor state using smartphone under arbitrary pedestrian behavior},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel information-theoretical framework for quantifying coding performance in scalable mobile video streaming. <em>TMC</em>, <em>24</em>(11), 12611-12625. (<a href='https://doi.org/10.1109/TMC.2025.3586587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, scalable video coding (SVC) has gained significant recognition in mobile video streaming because it can adapt bitstreams to time-varying transmission conditions. However, the coding performance of SVC, which is determined by its coding structure, has not been thoroughly studied. To address this issue, we propose analyzing the redundancy, reduction, distortion, and mutuality of video information within the video coding processes. This analysis facilitates the development of a novel information-theoretical framework for quantifying coding performance, which includes an information theory (IT)-based quantification method and a graphical representation system. The representation system accurately delineates the coding reference structure for encoding each video frame, while the proposed method utilizes mutual information to quantify the achievable coding performance of SVC under the delineated structure. To demonstrate the significance of our research, we apply the proposed framework to encode a basic coding unit, showcasing its effectiveness in improving SVC schemes. Consequently, our framework not only provides an efficient approach for quantifying the coding performance of SVC but also serves as an invaluable tool for optimizing SVC in various applications.},
  archive      = {J_TMC},
  author       = {Weijia Han and Chuan Huang and Yanjie Dong and Yangyingzi Zhang and Yuxiang Yue and Wei Teng},
  doi          = {10.1109/TMC.2025.3586587},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12611-12625},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel information-theoretical framework for quantifying coding performance in scalable mobile video streaming},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the robust topology recovery of UAV swarm for detection and localization of electronic signals. <em>TMC</em>, <em>24</em>(11), 12595-12610. (<a href='https://doi.org/10.1109/TMC.2025.3586447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At present, Unmanned Aerial Vehicle (UAV) swarm has been extensively applied in various fields. In the application of detection and localization of electronic signals, some UAVs could become disabled due to some abnormal events (e.g. electromagnetic interference and battery electricity exhaustion), and the topology connectivity of UAV swarm could be impaired, i.e., the topology of UAV swarm could be partitioned. For the topology recovery issue, we first propose Robust Topology Recovery Algorithm of UAV swarm (RTRA) to recover the topology connectivity of UAV swarm and enhance the topology robustness (reduce the number of potential topology recoveries in future) by relocating some UAVs to new positions with shortest flight distance. Furthermore, we note that the relocated UAVs are easy to exhaust the battery electricity and fail due to the extra flight movements for the topology recoveries, which affects the topology robustness. To this end, we present Cascading Robust Recovery Topology Algorithm of UAV swarm (CRTRA), which adopts a cascading movement strategy to share the flight movements among multiply relocated UAVs, thus avoiding the battery electricity exhaustion of the relocated UAVs. Extensive simulations and comparisons demonstrate that our proposed CRTRA can effectively recover the topology connectivity of UAV swarm while enhancing the topology robustness and shortening the flight distance of relocated UAVs, and CRTRA is especially suitable for some missions such as the detection and localization of electronic signals where UAVs are prone to fail.},
  archive      = {J_TMC},
  author       = {Linfeng Liu and Wenzhe Zhang and Xingyu Li and Jia Xu},
  doi          = {10.1109/TMC.2025.3586447},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12595-12610},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On the robust topology recovery of UAV swarm for detection and localization of electronic signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achievable rate maximization for multi-IRS assisted AAV-NOMA networks. <em>TMC</em>, <em>24</em>(11), 12580-12594. (<a href='https://doi.org/10.1109/TMC.2025.3586768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution towards Internet of Things (IoT) in the forthcoming sixth generation (6G) is facing massive amounts of transmitted data and harsh wireless transmission environment, which severely degrade the quality of communication. To overcome these difficulties, a novel multiple intelligent reflecting surfaces (IRSs) assisted autonomous aerial vehicle (AAV) network framework with non-orthogonal multiple access (NOMA) is proposed in this article, where the AAV applies the NOMA scheme to deliver the information to the ground users assisted by multiple IRSs. We aim to maximize the achievable rate of the considered network while guaranteeing the minimum communication rate of each user, by jointly optimizing the multi-IRS phase shifts, AAV transmit power, AAV trajectory, and NOMA decoding order. To handle the coupled variables and integer constraints, we decompose the original problem into three subproblems based on the block coordinate descent (BCD) framework. Specifically, we first obtain the multi-IRS phase shifts by applying the semidefinite relaxation (SDR) technique. Next, the AAV transmit power allocation is derived by exploiting the concave convex procedure (CCCP) method. The AAV trajectory and NOMA decoding order are finally obtained by invoking the penalty-based method and the successive convex approximation (SCA) technique. Based on these, an alternating optimization algorithm is proposed. The numerical results show that: 1) the NOMA scheme enhances the utilization of the spectrum and enhances the access capacity of the communication system; 2) the multi-IRS cooperative structure increases the reflective channels and effectively improves the air-ground transmission environment, thus enhancing the system achievable rate; 3) the proposed multi-IRS assisted AAV NOMA algorithm achieves a significant network rate improvement compared to other benchmark schemes.},
  archive      = {J_TMC},
  author       = {Dingcheng Yang and Kangqing Wu and Yu Xu and Fahui Wu and Tiankui Zhang},
  doi          = {10.1109/TMC.2025.3586768},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12580-12594},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Achievable rate maximization for multi-IRS assisted AAV-NOMA networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally robust contract theory for edge AIGC services in teleoperation. <em>TMC</em>, <em>24</em>(11), 12567-12579. (<a href='https://doi.org/10.1109/TMC.2025.3586606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced AI-Generated Content (AIGC) technologies have injected new impetus into teleoperation, enhancing its security and efficiency. Edge AIGC networks have been introduced to meet the stringent low-latency requirements of teleoperation. However, the inherent uncertainty of AIGC service quality and the need to incentivize AIGC service providers (ASPs) make the design of a robust incentive mechanism essential. This design is particularly challenging due to uncertainty and information asymmetry, as teleoperators have limited knowledge of the remaining resource capacities of ASPs. To this end, we propose a distributionally robust optimization (DRO)-based contract theory to design robust reward schemes for AIGC task offloading. Notably, our work extends the contract theory by integrating DRO, addressing the fundamental challenge of contract design under uncertainty. In this paper, we employ contract theory to model information asymmetry while utilizing DRO to capture the uncertainty in AIGC service quality. Given the inherent complexity of the original DRO-based contract theory problem, we reformulate it into an equivalent, tractable bi-level optimization problem. To efficiently solve this problem, we develop a Block Coordinate Descent (BCD)-based algorithm to derive robust reward schemes. Simulation results on our unity-based teleoperation platform demonstrate that the proposed method improves teleoperator utility by 2.7% to 10.74% under varying degrees of AIGC service quality shifts and increases ASP utility by 60.02% compared to the SOTA method, i.e., Deep Reinforcement Learning (DRL)-based contract theory.},
  archive      = {J_TMC},
  author       = {Zijun Zhan and Yaxian Dong and Daniel Mawunyo Doe and Yuqing Hu and Shuai Li and Shaohua Cao and Lei Fan and Zhu Han},
  doi          = {10.1109/TMC.2025.3586606},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12567-12579},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributionally robust contract theory for edge AIGC services in teleoperation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical optimizing UAV trajectory in wireless charging networks: An approximated approach. <em>TMC</em>, <em>24</em>(11), 12550-12566. (<a href='https://doi.org/10.1109/TMC.2025.3586457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicles (UAVs) can be easily deployed as auxiliary base stations due to their convenience and flexibility. However, limited battery capacity becomes a bottleneck. Promising wireless power transfer (WPT) technologies can provide a continuous power supply for UAVs. Many of the recent works treat the UAV battery capacity as a constraint, which hinders the assurance of continuous UAV operation. Furthermore, most studies employ intelligent path-planning algorithms that lack explicit performance guarantees. In this paper, we study the problem of Practical Optimizing UAV Trajectory in Wireless Charging Networks (POTWCN), which involves planning the trajectory of the wireless-powered UAV in the practical environment with obstacles by selecting candidate passing positions and determining the access order in the charging network. The goal is to maximize the benefit, i.e., balancing the total task completion time and the number of charging stations visited, so as to minimize path length and flight time, and ensure energy constraints with performance bound. To solve this problem, we first formalize the problem and prove its submodularity. Then, we propose the obstacle-aware weighted graph generation algorithm (OWGGA) to deal with the obstacles in the environment, which forms an obstacle-avoidance path using tangents and arcs between two hovering positions and the blocking obstacles. Next, we propose a dynamic charging station selection algorithm (ACSA), which maximizes the UAV’s energy utilization by limiting the number of charging stations that can be included. In the algorithm, we introduce the Christofides algorithm and use the path length calculated by OWGGA as the edge weights of the graph. Subsequently, considering the UAV’s energy constraints, we iteratively solve the UAV trajectory planning problem by adding the charging station with a maximized marginal benefit to the path. We prove that the proposed algorithm achieves an approximation ratio $1 - 1/e$ as well as the path length is at most $3\pi /4$ times the optimal solution. Simulation results show that our algorithm reduces the flight distance by 38.01% and the task completion time by 34.00% on average.},
  archive      = {J_TMC},
  author       = {Yundi Wang and Xiaoyu Wang and He Huang and Haipeng Dai},
  doi          = {10.1109/TMC.2025.3586457},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12550-12566},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Practical optimizing UAV trajectory in wireless charging networks: An approximated approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive parameter-efficient federated fine-tuning on heterogeneous devices. <em>TMC</em>, <em>24</em>(11), 12533-12549. (<a href='https://doi.org/10.1109/TMC.2025.3586644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated fine-tuning (FedFT) has been proposed to fine-tune the pre-trained language models in a distributed manner. However, there are two critical challenges for efficient FedFT in practical applications, i.e., resource constraints and system heterogeneity. Existing works rely on parameter-efficient fine-tuning methods, e.g., low-rank adaptation (LoRA)1, but with major limitations. Herein, based on the inherent characteristics of FedFT, we observe that LoRA layers with higher ranks added close to the output help to save resource consumption while achieving comparable fine-tuning performance. Then we propose a novel LoRA-based FedFT framework, termed LEGEND, which faces the difficulty of determining the number of LoRA layers (called, LoRA depth) and the rank of each LoRA layer (called, rank distribution). We analyze the coupled relationship between LoRA depth and rank distribution, and design an efficient LoRA configuration algorithm for heterogeneous devices, thereby promoting fine-tuning efficiency. Extensive experiments are conducted on a physical platform with 80 commercial devices. The results show that LEGEND can achieve a speedup of 1.5-2.8× and save communication costs by about 42.3% when achieving the target accuracy, compared to the advanced solutions.},
  archive      = {J_TMC},
  author       = {Jun Liu and Yunming Liao and Hongli Xu and Yang Xu and Jianchun Liu and Chen Qian},
  doi          = {10.1109/TMC.2025.3586644},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12533-12549},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive parameter-efficient federated fine-tuning on heterogeneous devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RF-DEGO: A range free localization algorithm for non uniform node distributions and obstacle environments. <em>TMC</em>, <em>24</em>(11), 12517-12532. (<a href='https://doi.org/10.1109/TMC.2025.3586636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Range-free localization algorithms have attracted considerable attention for outdoor wireless sensor network (WSN) positioning because they are less susceptible to environmental factors when estimating inter node distances and require only a few beacon nodes with known locations to rapidly determine all node positions. Among these, the connectivity based DV Hop algorithm has become widely used due to its simplicity and ease of implementation. However, its localization accuracy is limited and it is easily degraded by non uniform node distributions and obstacle environments. To address these shortcomings, this paper proposes a novel range free localization algorithm (RF-DEGO). First, a new distance estimation formula is derived from node connectivity and the probability distribution of distances. Next, the estimated distances are corrected using the local node density along communication paths, and paths identified as detouring around obstacles receive a further correction. Finally, an enhanced hierarchical Grey Wolf Optimization algorithm computes the node positions. Extensive simulation experiments under various network scenarios and parameter settings show that the proposed algorithm outperforms several existing localization methods in both accuracy and computation time, demonstrating superior overall performance and strong competitiveness.},
  archive      = {J_TMC},
  author       = {Haibin Sun and Yongzheng Zhang},
  doi          = {10.1109/TMC.2025.3586636},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12517-12532},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RF-DEGO: A range free localization algorithm for non uniform node distributions and obstacle environments},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On information collection in multi-tagged COTS RFID systems. <em>TMC</em>, <em>24</em>(11), 12505-12516. (<a href='https://doi.org/10.1109/TMC.2025.3586660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of target object information collection in multi-tagged COTS RFID systems. Unlike its single-tagged peers, the multi-tagged COTS RFID scenario poses new challenges in devising information collection algorithms: 1) Tags attached to the same object carry identical information. Hence, reusing single-tagged information collection algorithms leads to unnecessary redundancy; 2) Multi-tagged RFID systems are often deployed in applications where tags are vulnerable to damage. Such faulty tags may severely degrade the performance of information collection; 3) Most state-of-the-art information collection algorithms rely heavily on the hashing operation that is not seamlessly supported by the C1G2 standard, rendering these solutions inefficient and impractical. To tackle these technical challenges, this paper makes three contributions. First, we develop an efficient and compact tag pseudo-ID design, enabling the reader to select a single tag from each target object to collect information with only one Select command. Second, we construct a robust fault-handling mechanism capable of recognizing faulty tags without executing the entire slot. Third, armed with the above two techniques, we develop a novel information collection algorithm by leveraging the functionality offered by C1G2 to optimize the information collection sequence, thus minimizing the overall execution time. Empirical experiments on a COTS RFID system prototype demonstrate that our algorithm outperforms the best existing solution by 35–50% on average.},
  archive      = {J_TMC},
  author       = {Kanghuai Liu and Jihong Yu and Lin Chen},
  doi          = {10.1109/TMC.2025.3586660},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12505-12516},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On information collection in multi-tagged COTS RFID systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source routing for LEO mega-constellations based on bloom filter. <em>TMC</em>, <em>24</em>(11), 12487-12504. (<a href='https://doi.org/10.1109/TMC.2025.3586626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-earth-orbit (LEO) mega-constellations with inter-satellite links (ISLs) are becoming the Internet backbone in space. Satellites within LEO often need the capability to enforce data forwarding paths. For example, they may need to bypass the satellites over the untrusted areas for the data of mission-critical applications or minimize latency for the data of time-sensitive applications. However, typical source/segment routing techniques (e.g., SRv6) suffer from scalability issue, since they record source-route-style forwarding information via the list-based structure. This results in great payload and forwarding overhead. To overcome this drawback, we propose a source/segment routing architecture for LEO mega-constellations, which is named as Link-identified Routing (LiR). LiR leverages in-packet bloom filter (BF) to record source-route-style forwarding information. BF could efficiently record multiple elements via a probabilistic data structure, but overlooks the order of the encoded elements. To address this, LiR identifies each unidirectional ISL, and represents the path by encoding ISL identifiers into BF. We investigate how to optimize BF configuration and ISL encoding policy to address false positives caused by BF. We implement LiR in Linux kernel and develop a container-based emulator for performance evaluation. Results show that LiR significantly outperforms SRv6 in terms of packet forwarding and data delivery efficiency.},
  archive      = {J_TMC},
  author       = {Hefan Zhang and Zhiyuan Wang and Wenhao Lu and Shan Zhang and Hongbin Luo},
  doi          = {10.1109/TMC.2025.3586626},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12487-12504},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Source routing for LEO mega-constellations based on bloom filter},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling effective OOD detection via plug-and-play network for mobile visual applications. <em>TMC</em>, <em>24</em>(11), 12471-12486. (<a href='https://doi.org/10.1109/TMC.2025.3586625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices have increasingly integrated with numerous deep learning-based visual applications, such as object classification and recognition models. While these models perform well in controlled environments, their effectiveness declines in real-world environment due to out-of-distribution (OOD) data not seen during training. Existing methods for detecting OOD data often compromise normal data recognition and require extensive training on unattainable OOD data. To address these issues, we propose $\mathtt {POD}$, a framework designed to enhance mobile visual applications by providing high-precision OOD detection without affecting original model performance. In the offline phase, $\mathtt {POD}$ generates OOD detectors from any classification model by analyzing model’s neuron responses to various data types. In the online phase, it continuously adjusts decision boundaries by integrating results from both the original model and the detector. Evaluated on two public datasets and one self-collected dataset across various popular classification models, $\mathtt {POD}$ significantly improves OOD detection performance while maintaining the accuracy of original models.},
  archive      = {J_TMC},
  author       = {Zixiao Wang and Qi Dong and Tianzhang Xing and Zhidan Liu and Zhenjiang Li and Xiaojiang Chen},
  doi          = {10.1109/TMC.2025.3586625},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12471-12486},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling effective OOD detection via plug-and-play network for mobile visual applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HybridRDN: Delay-optimal computation offloading for autonomous vehicle fleets based on RSMA. <em>TMC</em>, <em>24</em>(11), 12456-12470. (<a href='https://doi.org/10.1109/TMC.2025.3586638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rate-splitting multiple access (RSMA), space division multiple access (SDMA), and non-orthogonal multiple access (NOMA) have gained significant popularity and are extensively utilized across various domains. However, it is still unclear whether hybrid RSMA-SDMA-NOMA (HybridRDN) would seamlessly combine the advantages of RSMA, SDMA, and NOMA to contribute to the computation offloading of autonomous vehicle systems. To address the above issue, this paper introduces a novel HybridRDN-assisted computation offloading fleet (COF) scheme tailored for autonomous vehicle systems. First, we propose a stochastic-geometry-aided method to model the offloading framework. Afterwards, the task vehicles (TVs) ingeniously employ the proposed HybridRDN scheme to offload tasks to the resource vehicles (RVs) in each COF to relieve their computational burden. Diverging from the sole optimization of the task segmentation ratio or the transmission rate, a joint optimization problem involving the transmission weighting factor, the HybridRDN precoding matrix, the common rate, and the task segmentation ratio, is formulated, which aims to minimize the average delay of the COF system while approaching the rate performance of the ideal HybridRDN. Furthermore, a delay-optimal alternating optimization algorithm (DOAOA) is developed to obtain the solution for the optimization problem. Experimental results validate the plausibility and superiority of the proposed framework compared to the state-of-the-art schemes.},
  archive      = {J_TMC},
  author       = {Zhijian Lin and Yang Xiao and Yi Fang and Hongbing Chen and Xiaoqiang Lu},
  doi          = {10.1109/TMC.2025.3586638},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12456-12470},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HybridRDN: Delay-optimal computation offloading for autonomous vehicle fleets based on RSMA},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance analysis of direct acyclic graph-based ledgers in low-to-high load regime. <em>TMC</em>, <em>24</em>(11), 12441-12455. (<a href='https://doi.org/10.1109/TMC.2025.3586668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct acyclic graph (DAG)-based ledgers and distributed consensus algorithms have been proposed for use in the Internet of Things (IoT). The DAG-based ledgers have many advantages over single-chain blockchains, such as low resource consumption, low transaction fee, high transaction throughput, and short confirmation delay. However, the scalability of the DAG consensus has not been comprehensively verified on a large scale. This paper explores the scalability of DAG consensus within the low-to-high load regime (L2HR) using the tangle model, where L2HR characterizes the transition from a phase of low network load to another phase of high network load. In particular, we determine the average number of tips in the tangle in L2HR when adopting the uniform random tip selection (URTS) and rigorously prove that using the tangle model, the average number of tips at the end of L2HR converges to a constant. We also analyze the probability that a transaction in L2HR becomes an abandoned tip, the approximate average time required for the network load to transition from low load regime (LR) to high load regime (HR), and the average time required for a tip being approved for the first time in L2HR. All analytics are verified by numerical simulations.},
  archive      = {J_TMC},
  author       = {Qingwen Wei and Shuping Dang and Zhihui Ge and Xiangcheng Li and Zhenrong Zhang},
  doi          = {10.1109/TMC.2025.3586668},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12441-12455},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Performance analysis of direct acyclic graph-based ledgers in low-to-high load regime},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multipath differential routing and traffic scheduling in ultra-dense LEO satellite networks: A DRL with stackelberg game approach. <em>TMC</em>, <em>24</em>(11), 12424-12440. (<a href='https://doi.org/10.1109/TMC.2025.3586262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth orbit satellite networks (LSNs) are envisioned as key enablers of 6G by offering ubiquitous, low-latency connectivity. Their mesh topology enables multipath differential routing, which improves bandwidth utilization and reduces transmission delay. However, the growing demand for data and the dynamic, self-organizing nature of LSNs pose significant challenges for joint multipath routing and traffic scheduling under strict latency and energy constraints. To address these challenges, this paper proposes a multipath routing optimization (MRO) and traffic scheduling method tailored for multipath differential routing. Specifically, a dynamic multi-attribute graph model is developed to precisely capture the dynamic properties of LSNs. Building on this model, a MRO algorithm, integrated with a Stackelberg game framework, is introduced. The MRO algorithm employs a decomposition-based approach to identify multiple optimal paths that minimize delay and energy consumption, while the Stackelberg game framework ensures efficient traffic distribution across these paths. Numerical results demonstrate that the proposed approach significantly outperforms existing baseline methods, achieving cumulative reward improvements of 26.77% to 43.8% across four real-world network topologies and exhibiting better Pareto front coverage. Furthermore, by leveraging the rapid convergence properties of the Stackelberg game model, the proposed method enhances network throughput by 12% to 43% and reduces transmission time by 14% to 49%.},
  archive      = {J_TMC},
  author       = {Shuyang Li and Qiang Wu and Ran Wang and Long Chen and Hongke Zhang},
  doi          = {10.1109/TMC.2025.3586262},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12424-12440},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient multipath differential routing and traffic scheduling in ultra-dense LEO satellite networks: A DRL with stackelberg game approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fed$n$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math>P: Federated unlearning with multiple client set partitions. <em>TMC</em>, <em>24</em>(11), 12406-12423. (<a href='https://doi.org/10.1109/TMC.2025.3586441'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has garnered increased attention in the field of distributed machine learning and privacy computing. In the FL setup, effective and efficient unlearning algorithms are required to remove the impact of specific training data from the trained model, called federated unlearning. However, traditional machine unlearning algorithms face limitations in FL systems because the client data is private and even non-IID. In this paper, we propose a new federated unlearning algorithm called Fed$n$P. Our approach involves dividing the client set into subsets using multiple different partitions. We then train constituent models for each client subset within these partitions using existing FL algorithms and aggregate the results of constituent models for predictions. With multiple partitions, Fed$n$P limits the influence of the data to be erased within its belonging subsets, while it also improves the accuracy of the aggregated prediction. Based on the multiple-partition framework, we design partition creation methods to effectively enhance the prediction accuracy. Furthermore, we propose a cost reduction method to reduce the cost of training/retraining. Our extensive experiments on various datasets and model architectures demonstrate that Fed$n$P improves prediction accuracy while well-controls the additional cost.},
  archive      = {J_TMC},
  author       = {Juncheng Jia and Weipeng Zhu and Bing Luo and Xiaodong Lin and Liuchen Ma},
  doi          = {10.1109/TMC.2025.3586441},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12406-12423},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fed$n$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math>P: Federated unlearning with multiple client set partitions},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantum-driven efficient learning model for enhancing robustness of IoT topology. <em>TMC</em>, <em>24</em>(11), 12391-12405. (<a href='https://doi.org/10.1109/TMC.2025.3586749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness of Internet of Things (IoT) topologies measures a network structure’s tolerance to random failures, or attacks, which is crucial for stable network communication. Research on optimizing network topology robustness has shifted from empirical rules and heuristics to machine learning, which can extract the features of robust network from topology data, thereby reducing the complexity of traditional topology optimization. However, machine learning approaches typically require a large number of parameters, resulting in high costs associated with parameter tuning and inference. To address these issues, this paper combines parameterized quantum circuits, and proposes a Quantum-Driven efficient Learning Model (QDLM) for enhancing robustness of IoT topology. This model leverages quantum exponential states to significantly reduce the number of training parameters while preserving learning performance. For inputs, QDLM integrates arithmetic encoding and quantum state encoding based on topological adjacency matrix, reducing the number of neurons. In training phase, parameterized quantum rotation gates and controlled quantum gates are used to achieve efficient training. A quantum measurement method is designed to ensure the output topology is a connected graph with the required number of edges. Compared to existing topology learning models, QDLM achieves an order-of-magnitude reduction in training parameters while maintaining topology learning effectiveness.},
  archive      = {J_TMC},
  author       = {Songwei Zhang and Tie Qiu and Xiaobo Zhou and Yusheng Ji},
  doi          = {10.1109/TMC.2025.3586749},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12391-12405},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A quantum-driven efficient learning model for enhancing robustness of IoT topology},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UltraWrite: A lightweight continuous gesture input system with ultrasonic signals on COTS devices. <em>TMC</em>, <em>24</em>(11), 12375-12390. (<a href='https://doi.org/10.1109/TMC.2025.3586259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the advantages of device ubiquity, natural interaction and privacy preservation, acoustic-based gesture input has received widespread attention. Researchers have proposed various techniques for different applications. However, the existing work has shortcomings of heavy data-collection overhead, non-continuous input, and performance degradation in cross-user scenarios. To overcome these shortcomings, we propose $\mathsf{UltraWrite}$, an acoustic-based gesture input system that only needs extremely low data-collection overhead, supports continuous input, and achieves high cross-user recognition accuracy. The key idea of our solution is to synthesize training data of continuous gestures from isolated ones, build a lightweight continuous gesture recognition model based on connectionist temporal classification (CTC) mechanism, and design a novel decoupled model training strategy to improve its cross-user recognition capability. We have implemented prototype systems on commercial devices and conducted comprehensive experiments to evaluate their performance. The results show that $\mathsf{UltraWrite}$ achieves an average top-1 word accuracy of 99.3% and top-1 word error rate of 0.34%. In addition, we have also evaluated $\mathsf{UltraWrite’s}$ robustness to the sensing distance, angle, background noise, and device. The results reveal that $\mathsf{UltraWrite}$ possesses strong robustness to these factors.},
  archive      = {J_TMC},
  author       = {Yongpan Zou and Weiyu Chen and Yunshu Wang and Canlin Zheng and Wenfeng He and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3586259},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12375-12390},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UltraWrite: A lightweight continuous gesture input system with ultrasonic signals on COTS devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RISensing: Leveraging reconfigurable intelligent surfaces to empower wi-fi sensing. <em>TMC</em>, <em>24</em>(11), 12359-12374. (<a href='https://doi.org/10.1109/TMC.2025.3583916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi technology has emerged as a promising solution for contact-free sensing owing to the pervasiveness of Wi-Fi signals in indoor environments. However, Wi-Fi sensing faces several fundamental issues, including limited sensing range and unstable orientation-dependent sensing performance, hindering the widespread adoption of Wi-Fi sensing in real-life scenarios. In this paper, we propose RISensing, a novel system that leverages Reconfigurable Intelligent Surfaces (RIS) to address these two fundamental issues of Wi-Fi sensing and bring Wi-Fi sensing one step closer to real-world adoption. Unlike prior Wi-Fi sensing works which typically rely on a single target reflection signal to capture the target movement, RISensing utilizes two target reflection signals, i.e., the direct target reflection signal and RIS-based target reflection signal, to boost the sensing capability. RISensing characterizes the RIS-based target reflection signal, and constructively combines it with the direct target reflection. We evaluate the sensing performance of RISensing in various environments, including corridor, office and lab. Extensive experiments demonstrate RISensing can improve the sensing range of Wi-Fi from 4 m to 23 m, and effectively mitigate the orientation-dependent issue.},
  archive      = {J_TMC},
  author       = {Xiaojing Wang and Binbin Xie and Guanghui Lv and Boyang Liu and Chenhao Ma and Renjie Zhao and Chao Feng and Xiaojiang Chen},
  doi          = {10.1109/TMC.2025.3583916},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12359-12374},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RISensing: Leveraging reconfigurable intelligent surfaces to empower wi-fi sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmOrbit: Micrometer-level vibration and rotor orbit measurement via synchronized dual mmWave radars. <em>TMC</em>, <em>24</em>(11), 12345-12358. (<a href='https://doi.org/10.1109/TMC.2025.3582545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce mmOrbit, a mmWave-based rotor orbit measurement system that can estimate rotor orbit by analyzing machinery surface vibration. To measure the 2D rotor orbit, two synchronized mmWave radars are deployed to build the orbit from different viewpoints. The existing literature shows that the micro-displacement measurement accuracy of mmWave radar is not enough to meet the application requirements of micron-level resolution without appropriate fine-gained processing methods. Therefore, we propose a three-step vibration displacement extraction algorithm with sliding table to extract mechanical vibration micro-displacement and increase measurement precision. Accurate mechanical vibration displacements in two vertical directions are used to estimate the 2D rotor orbit. Our extensive experiments indicate that mmOrbit can accurately measure mechanical vibration micro-displacement with an error of 4.4 $\mu \text{m}$ for the $80\text{th}$ percentile. Furthermore, mmOrbit can estimate 2D rotor orbit with high precision, showing an orbit eccentricity error of 7%, an orbit direction error of $7^\circ$, and a disjoint area proportion of 17% for the $80\text{th}$ percentile. The imbalance detection experiment verifies the accuracy and dependability of our measured rotor orbit.},
  archive      = {J_TMC},
  author       = {Changlin Mao and Haocheng Ni and Jianpin Han and Yingxiao Wu},
  doi          = {10.1109/TMC.2025.3582545},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12345-12358},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmOrbit: Micrometer-level vibration and rotor orbit measurement via synchronized dual mmWave radars},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Suite-IN++: A FlexiWear BodyNet integrating global and local motion features from apple suite for robust inertial navigation. <em>TMC</em>, <em>24</em>(11), 12329-12344. (<a href='https://doi.org/10.1109/TMC.2025.3583055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of wearable technology has established multi-device ecosystems comprising smartphones, smartwatches, and headphones as critical enablers for ubiquitous pedestrian localization. However, traditional pedestrian dead reckoning (PDR) struggles with diverse motion modes, while data-driven methods, despite improving accuracy, often lack robustness due to their reliance on a single-device setup. Therefore, a promising solution is to fully leverage existing wearable devices to form a flexiwear bodynet for robust and accurate pedestrian localization. This paper presents Suite-IN++, a deep learning framework for flexiwear bodynet-based pedestrian localization. Suite-IN++ integrates motion data from wearable devices on different body parts, using contrastive learning to separate global and local motion features. It fuses global features based on the data reliability of each device to capture overall motion trends and employs an attention mechanism to uncover cross-device correlations in local features, extracting motion details helpful for accurate localization. To evaluate our method, we construct a real-life flexiwear bodynet dataset, incorporating Apple Suite (iPhone, Apple Watch, and AirPods) across diverse walking modes and device configurations. Experimental results demonstrate that Suite-IN++ achieves superior localization accuracy and robustness, significantly outperforming state-of-the-art models in real-life pedestrian tracking scenarios.},
  archive      = {J_TMC},
  author       = {Lan Sun and Songpengcheng Xia and Jiarui Yang and Ling Pei},
  doi          = {10.1109/TMC.2025.3583055},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12329-12344},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Suite-IN++: A FlexiWear BodyNet integrating global and local motion features from apple suite for robust inertial navigation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Layer-aware cost-effective container updates with edge-cloud collaboration in edge computing. <em>TMC</em>, <em>24</em>(11), 12314-12328. (<a href='https://doi.org/10.1109/TMC.2025.3583153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containers have become popular for deploying applications in Edge Computing (EC) for their seamless integration and easy deployment. Frequent container updates are essential to enhance performance and introduce new challenges for cutting-edge applications such as large language models and digital twins. However, traditional container update methods result in substantial download costs and task interruptions, which are unacceptable for latency-sensitive tasks in resource-constrained EC. Existing work has largely overlooked the layered structure of container images. By leveraging this layered structure, duplicate downloads can be reduced, and various layers can be transferred from other edges, reducing burden on the remote cloud. In this paper, we model the layer-aware container update problem with edge-cloud collaboration to minimize update and scheduling costs. We present the Layer-aware Edge-cloud collaborative Container Update (LECU) algorithm based on reinforcement learning to make container update decisions. Moreover, a task scheduling algorithm is devised to schedule tasks affected by container updates to other edges, minimizing the impact of task interruptions. We implement our LECU algorithm on an edge system with real-world data traces to demonstrate its effectiveness and conduct larger-scale simulations to evaluate its scalability. Results demonstrate that our algorithms reduce container update and task scheduling costs by 14% and 19%, respectively, compared to baselines.},
  archive      = {J_TMC},
  author       = {Hanshuai Cui and Zhiqing Tang and Yuan Wu and Weijia Jia},
  doi          = {10.1109/TMC.2025.3583153},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12314-12328},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Layer-aware cost-effective container updates with edge-cloud collaboration in edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OpenRFI: Open-set radio frequency fingerprint identification via test-time fine-tuning. <em>TMC</em>, <em>24</em>(11), 12297-12313. (<a href='https://doi.org/10.1109/TMC.2025.3582980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of low-cost mobile edge devices, security and reliability have become crucial for mobile edge computing networks, especially for high-stakes applications. To facilitate it, Radio Frequency Fingerprint Identification (RFFI) has emerged as a promising physical layer security paradigm, offering a non-cryptographic and lightweight solution. However, most existing Deep Learning (DL) based RFFI methods operate under a closed-set assumption, limiting their ability to recognize devices not seen during training and posing a risk of misclassification. Addressing the open-set RFFI problem is critical for real-world deployments, where the system must handle both known and unknown devices, ensuring robust security in dynamic environments. In this paper, we propose OpenRFI, a novel test-time fine-tuning-based RFFI framework, consisting of two sequential stages: pre-training and test-time fine-tuning. During the pre-training stage, we design a data augmentation module, a feature extraction module, and an efficient hybrid loss function to minimize intra-class feature distances and tighten decision boundaries, enhancing the model’s ability to distinguish between different classes. In the test-time fine-tuning stage, we introduce a fine-tuning dataset construction module and a full-parameter fine-tuning module to dynamically adapt to the test environment and capture information from unknown samples, further improving open-set recognition. We theoretically establish the performance boundary of the fine-tuning dataset construction method, providing insights into its robustness and scalability. Extensive numerical results based on an open source dataset demonstrate the effectiveness of the proposed OpenRFI framework in comparison with existing baselines.},
  archive      = {J_TMC},
  author       = {Jian Yang and Shuai Feng and Yatong Wang and Xinghang Wu and Mu Yan},
  doi          = {10.1109/TMC.2025.3582980},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12297-12313},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {OpenRFI: Open-set radio frequency fingerprint identification via test-time fine-tuning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video conferencing with predictive generation and collaborative computation across mobile headsets. <em>TMC</em>, <em>24</em>(11), 12282-12296. (<a href='https://doi.org/10.1109/TMC.2025.3582284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual Reality (VR) has emerged as a transformative platform for remote collaboration, but its adoption for video conferencing is hindered by challenges related to facial expression reconstruction and computational resource constraints, especially on economical mobile VR headsets. This paper introduces a novel system for VR video conferencing that addresses these challenges through two key modules: Predictive Generation and Collaborative Computation. Predictive Generation leverages multimodal inputs, including voice, head motion, and eye blinks, to synthesize realistic facial animations with low latency, eliminating the need for high-precision hardware. Collaborative Computation enhances computational efficiency by employing a game-theoretic framework for resource sharing among users. Experimental evaluations demonstrate that our system delivers immersive and realistic VR video conferencing experiences with superior facial expression reconstruction and efficient resource utilization. Our approach makes VR video conferencing more accessible and practical for a broader audience across mobile headsets.},
  archive      = {J_TMC},
  author       = {Yili Jin and Xize Duan and Kaiyuan Hu and Fangxin Wang and Xue Liu and Jiangchuan Liu},
  doi          = {10.1109/TMC.2025.3582284},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12282-12296},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Video conferencing with predictive generation and collaborative computation across mobile headsets},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pricing game for federated learning supporting lightweight local model training. <em>TMC</em>, <em>24</em>(11), 12264-12281. (<a href='https://doi.org/10.1109/TMC.2025.3583013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pervasive distribution of data across clients with privacy concerns and heterogeneous performance in edge networks presents a significant opportunity to enhance AI model performance. Federated learning (FL) enables a model owner (MO) to recruit these clients, offering compensation for their contributions, and to improve model quality by aggregating knowledge from their locally trained models. However, several challenges arise in this process. Clients may decline participation if they do not achieve positive utility. Moreover, due to constraints in memory, computing, and communication resources, some clients can only train lightweight models that represent partial versions of the global model. Importantly, the MO’s pricing for client contributions and the proportions of local model training are interdependent, collectively influencing client utilities and participation decisions. To address these challenges, we first model the utility functions of both the MO and the clients, accommodating the support for lightweight local models. We then formulate their interactions as a Stackelberg game and theoretically prove the existence of a Nash equilibrium. Based on this equilibrium, we derive optimal collaboration strategies for both the MO and the clients. Additionally, we design an efficient approximation algorithm to enable the MO to maximize its utility by selecting suitable clients to participate in FL. Finally, extensive experiments validate our theoretical findings, demonstrating the superior performance and effectiveness of the proposed algorithms.},
  archive      = {J_TMC},
  author       = {Fengsen Tian and Mingzi Wang and Yu Zhang and Guoqiang Deng and Lingyu Liang and Xinglin Zhang},
  doi          = {10.1109/TMC.2025.3583013},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12264-12281},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A pricing game for federated learning supporting lightweight local model training},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDR-empowered environment sensing design and experimental validation using OTFS-ISAC signals. <em>TMC</em>, <em>24</em>(11), 12252-12263. (<a href='https://doi.org/10.1109/TMC.2025.3581716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the system design and experimental validation of integrated sensing and communication (ISAC) for environmental sensing, which is expected to be a critical enabler for next-generation wireless networks. We advocate exploiting orthogonal time frequency space (OTFS) modulation for its inherent sparsity and stability in delay-Doppler (DD) domain channels, facilitating a low-overhead environment sensing design. Moreover, a comprehensive environmental sensing framework is developed, encompassing DD domain channel estimation, target localization, and experimental validation. In particular, we first explore the OTFS channel estimation in the presence of fractional delay and Doppler shifts. Given the estimated parameters, we propose a three-ellipse positioning algorithm to localize the target’s position, followed by determining the mobile transmitter’s velocity. Additionally, to evaluate the performance of our proposed design, we conduct extensive simulations and experiments using a software-defined radio (SDR)-based platform with universal software radio peripheral (USRP). The experimental validations demonstrate that our proposed approach outperforms the benchmarks in terms of localization accuracy and velocity estimation, confirming its effectiveness in practical environmental sensing applications.},
  archive      = {J_TMC},
  author       = {Jun Wu and Yuye Shi and Weijie Yuan and Qingqing Cheng and Buyi Li and Xinyuan Wei},
  doi          = {10.1109/TMC.2025.3581716},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12252-12263},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SDR-empowered environment sensing design and experimental validation using OTFS-ISAC signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Covert communications for intelligent reflecting surface-enabled D2D networks. <em>TMC</em>, <em>24</em>(11), 12239-12251. (<a href='https://doi.org/10.1109/TMC.2025.3583779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we explore covert communications in a device-to-device (D2D) network consisting of an intelligent reflecting surface (IRS), a base station, a cellular user, a D2D pair, and an adversary warden. With the help of the IRS, the D2D pair attempts to perform covert communication, while the warden also tries to detect the very existence of such a transmission. To investigate the covert performance under the scenario, we derive the detection error probability at Warden, the optimal detection threshold for minimizing the probability, and the transmission outage probabilities for D2D and cellular communications, respectively. We further jointly optimize the transmission powers of the cellular user and the D2D transmitter, the reflection phase shifts, and the amplitudes of the IRS reflecting elements to improve covert communication performance. Finally, we provide numerical results to reveal the impact of system parameters on the covert performance and also to exhibit the merits of IRS-enabled D2D networks for achieving covert communications.},
  archive      = {J_TMC},
  author       = {Yihuai Yang and Bin Yang and Shikai Shen and Yumei She and Xiaohong Jiang and Tarik Taleb},
  doi          = {10.1109/TMC.2025.3583779},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12239-12251},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Covert communications for intelligent reflecting surface-enabled D2D networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated digital twin construction via distributed sensing: A game-theoretic online optimization with overlapping coalitions. <em>TMC</em>, <em>24</em>(11), 12221-12238. (<a href='https://doi.org/10.1109/TMC.2025.3582755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel federated framework for constructing the digital twin (DT) model, referring to a living and self-evolving visualization model empowered by artificial intelligence, enabled by distributed sensing under edge-cloud collaboration. In this framework, the DT model to be built at the cloud is regarded as a global one being split into and integrating from multiple functional components, i.e., partial-DTs, created at various edge servers (ESs) using feature data collected by associated sensors. Considering time-varying DT evolutions and heterogeneities among partial-DTs, we formulate an online problem that jointly and dynamically optimizes partial-DT assignments from the cloud to ESs, ES-sensor associations for partial-DT creation, and as well as computation and communication resource allocations for global-DT integration. The problem aims to maximize the constructed DT’s model quality while minimizing all induced costs, including energy consumption and configuration costs, in long runs. To this end, we first transform the original problem into an equivalent hierarchical game with an upper-layer two-sided matching game and a lower-layer overlapping coalition formation game. After analyzing these games in detail, we apply the Gale-Shapley algorithm and particularly develop a switch rules-based overlapping coalition formation algorithm to obtain short-term equilibria of upper-layer and lower-layer subgames, respectively. Then, we design a deep reinforcement learning-based solution, called DMO, to extend the result into a long-term equilibrium of the hierarchical game, thereby producing the solution to the original problem. Simulations show the effectiveness of the introduced framework, and demonstrate the superiority of the proposed solution over counterparts.},
  archive      = {J_TMC},
  author       = {Ruoyang Chen and Changyan Yi and Fuhui Zhou and Jiawen Kang and Yuan Wu and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3582755},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12221-12238},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated digital twin construction via distributed sensing: A game-theoretic online optimization with overlapping coalitions},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaFlowLite: Scalable and non-blocking inference on asynchronous mobile data. <em>TMC</em>, <em>24</em>(11), 12206-12220. (<a href='https://doi.org/10.1109/TMC.2025.3582060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of mobile devices equipped with numerous sensors, such as LiDAR and cameras, has driven the adoption of multi-modal deep intelligence for distributed sensing tasks, such as smart cabins and driving assistance. However, the arrival time of mobile sensory data vary due to modality size and network dynamics, which can lead to delays (if waiting for slow data) or accuracy decline (if inference proceeds without waiting). Moreover, the diversity and dynamic nature of mobile systems exacerbate this challenge. In response, we present a shift to opportunistic inference for asynchronous distributed multi-modal data, enabling inference as soon as partial data arrives. While existing methods focus on optimizing modality consistency and complementarity, known as modal affinity, they lack a computational approach to control this affinity in open-world mobile environments. ${\sf AdaFlowLite}$ pioneers the formulation of structured cross-modality affinity in mobile contexts using a hierarchical analysis-based normalized matrix. This approach accommodates the diversity and dynamics of modalities, generalizing across different types and numbers of inputs. Employing an multi-modal lightweight Swin Transformer (MMLST), ${\sf AdaFlowLite}$ facilitates real-time and flexible data imputation, adapting to various modalities and downstream tasks without retraining. Experiments show that ${\sf AdaFlowLite}$ significantly reduces inference latency by up to 80.4% and enhances accuracy by up to 62.1%, while achieving nearly a 50% reduction in energy consumption, outperforming status quo approaches. Also, this method can enhance LLM performance to preprocess asynchronous data.},
  archive      = {J_TMC},
  author       = {Sicong Liu and Fengmin Wu and Yuan Gao and Bin Guo and Zimu Zhou and Hongkai Wen and Zhiwen Yu},
  doi          = {10.1109/TMC.2025.3582060},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12206-12220},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AdaFlowLite: Scalable and non-blocking inference on asynchronous mobile data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward efficient verifiable data streaming without cryptographic accumulator. <em>TMC</em>, <em>24</em>(11), 12193-12205. (<a href='https://doi.org/10.1109/TMC.2025.3582087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifiable data streaming (VDS) enables the client to incrementally store a sequence of ordered data on an untrusted cloud server, and verify the validity of the retrieved data. Moreover, the client can replace a data with another value. The common security problem caused by updating operation is the cloud server may use old authentication information to make expired data pass the verification. To solve this problem, the known approaches use the cryptographic accumulator that actually influences the performance of VDS scheme. The main concerns can be generalized as how to design a VDS scheme without cryptographic accumulator, in such a way that further optimizes the performance of VDS scheme. We put forward the idea to convert the standard digital signature relevant to the updated data into chameleon digital signature whose non-transferability is the key to solve the problem. This is the first attempt to securely authenticate the dynamic data without cryptographic accumulator. In the proposed VDS scheme, the client’s local storage overhead, computation overheads of the cloud server in responding to a query and updating the data are constant. As the experimental results shown, the proposed VDS scheme outperforms the scheme in terms of the efficiency.},
  archive      = {J_TMC},
  author       = {Haining Yang and Jinlu Liu and Pingyuan Zhang and Jing Qin and Huaxiong Wang},
  doi          = {10.1109/TMC.2025.3582087},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12193-12205},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward efficient verifiable data streaming without cryptographic accumulator},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing collaborative machine learning in resource-limited networks through knowledge distillation and over-the-air computation. <em>TMC</em>, <em>24</em>(11), 12177-12192. (<a href='https://doi.org/10.1109/TMC.2025.3582683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional collaborative machine learning (CML) faces significant challenges in resource-constrained environments, such as emergency scenarios with limited power, bandwidth, and computing resources, leading to increased communication delays and energy consumption. To address these issues, this paper introduces Air-CoKD, a novel CML framework designed to reduce resource consumption and training latency while preserving model performance. Air-CoKD leverages knowledge distillation (KD) to minimize data transmission by avoiding the direct sharing of model parameters. It also integrates over-the-air computation (AirComp) to aggregate local logits, optimizing bandwidth utilization. To address the dimensional differences in local logits caused by the unbalanced device data class, Air-CoKD employs orthogonal frequency division multiplexing (OFDM) to transmitting local logits for different target classes. To handle aggregation errors introduced by AirComp, we conduct a detailed analysis of error bounds. Specifically, we convert the Kullback–Leibler (KL) divergence, used in KD loss function, into a quadratic upper bound for precise error quantification and effective optimization. Based on these insights, we propose a strategy to manage bandwidth constraints, transmission power limits, and device energy budgets within Air-CoKD. Extensive simulations demonstrate that Air-CoKD surpasses state-of-the-art methods, effectively balancing training efficiency and model performance. The framework proves to be a robust solution for CML in resource-constrained networks.},
  archive      = {J_TMC},
  author       = {Yue Zhang and Guopeng Zhang and Kun Yang and Yao Wen and Kezhi Wang},
  doi          = {10.1109/TMC.2025.3582683},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12177-12192},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing collaborative machine learning in resource-limited networks through knowledge distillation and over-the-air computation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-edge collaborative optimization of microservice caching in D2D-assisted network. <em>TMC</em>, <em>24</em>(11), 12162-12176. (<a href='https://doi.org/10.1109/TMC.2025.3582579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Employing the caching resources of end users via Device-to-Device (D2D) communication to assist the edge server in microservice caching is promising to further alleviate the network congestion of the Internet of Things (IoT). However, significant extra energy consumption prevents the caching system from maximizing cache utility if all end users cache simultaneously. In this paper, we propose two novel end-edge collaborative microservice caching algorithms in D2D-assisted networks. First, we construct a D2D caching sharing link graph from the aspects of physical and social attributes of end users and introduce the Entropy-based Partitioning Around Medoid (EPAM) algorithm to identify critical users. Second, to address the challenges posed by unknown time-varying user preferences, we model the end-edge collaborative caching problem as a Multi-Agent Multi-Armed Bandit (MAMAB) problem, thus developing two caching decision schemes, i.e, Edge-Centric Scheme (ECS) and User-Centric Scheme (UCS), to accommodate different decision sequences. The simulation results show that the EPAM-ECS and EPAM-UCS have at least 29.2% and 39.3% improvement compared with other baseline algorithms.},
  archive      = {J_TMC},
  author       = {Qingyong Deng and Mengyao Li and Zhetao Li and Haolin Liu and Yong Xie},
  doi          = {10.1109/TMC.2025.3582579},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12162-12176},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {End-edge collaborative optimization of microservice caching in D2D-assisted network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cell-free massive MIMO detection: A distributed expectation propagation approach. <em>TMC</em>, <em>24</em>(11), 12149-12161. (<a href='https://doi.org/10.1109/TMC.2025.3583019'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell-free massive MIMO is one of the core technologies for next-generation wireless networks. It is expected to bring enormous benefits, including ultra-high reliability, data throughput, energy efficiency, and uniform coverage. However, the radically distributed architecture of cell-free massive MIMO necessitates new paradigms for transceiver design, especially by exploiting efficient distributed processing algorithms. In this paper, we propose a distributed expectation propagation (EP) detector for cell-free massive MIMO and THz ultra-massive MIMO systems, which consists of two modules: a nonlinear module at the central processing unit (CPU) and a linear module at each access point (AP). The turbo principle in iterative channel decoding is utilized to compute and pass the extrinsic information between the two modules. An analytical framework is provided to characterize the asymptotic performance of the proposed EP detector with a large number of antennas. Furthermore, a distributed iterative channel estimation and data detection (ICD) algorithm is developed to handle the practical scenario with imperfect channel state information (CSI). Simulation results will show that the proposed method outperforms existing detectors for cell-free massive MIMO systems in terms of the bit-error rate and the developed theoretical analysis can be utilized as an asymptotic lower bound. Finally, it is shown that with imperfect CSI, the proposed ICD algorithm can significantly improve the system performance and reduce the pilot overhead.},
  archive      = {J_TMC},
  author       = {Hengtao He and Xianghao Yu and Jun Zhang and Shenghui Song and Ross D. Murch and Khaled B. Letaief},
  doi          = {10.1109/TMC.2025.3583019},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12149-12161},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cell-free massive MIMO detection: A distributed expectation propagation approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An online computation offloading approach with dual stability guarantee for heterogeneous tasks in MEC-enabled IIoT. <em>TMC</em>, <em>24</em>(11), 12137-12148. (<a href='https://doi.org/10.1109/TMC.2025.3581600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of information and the continuous expansion of applications, Industrial Internet of Things (IIoT) is facing huge data processing and storage pressure. With mobile edge computing (MEC) technology, the computing power network connects the geographically distributed computing nodes and then coordinates the allocation and scheduling of resources, transmits data, and eventually relieves the pressure of the industrial site. However, the rigorous demands of IIoT for real-time and stability pose some daunting challenges. To this end, we propose an online computation offloading approach with dual stability guarantee, named OCODSG. Specifically, the Lyapunov function is used to optimize the stability of the virtual queue, and the system stability is optimized based on the network jitter measurement. Moreover, the Dueling Double Deep Q Network (D3QN) algorithm based on deep reinforcement learning (DRL) is used for model autonomous training, while Gaussian noise is added to the network parameter space to encourage exploration and enhance algorithm robustness. Finally, experimental results on both simulated and real datasets demonstrate that OCODSG improves service efficiency and system stability.},
  archive      = {J_TMC},
  author       = {Kai Peng and Chengfang Ling and Shangguang Wang and Victor C.M. Leung},
  doi          = {10.1109/TMC.2025.3581600},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12137-12148},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An online computation offloading approach with dual stability guarantee for heterogeneous tasks in MEC-enabled IIoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional and secure spatial keyword query with arbitrary ranges in mobile cloud. <em>TMC</em>, <em>24</em>(11), 12121-12136. (<a href='https://doi.org/10.1109/TMC.2025.3581562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial keyword query has emerged as a critical service in mobile cloud, enabling cloud servers to retrieve spatio-textual objects within a mobile user’s query range that contain specified query keywords. Numerous secure spatial keyword query schemes have been developed to enable geometric range queries and keyword searches on encrypted spatial data. However, spatial keyword queries are typically designed for searching high-dimensional spatial data across arbitrary geographic ranges. Most of them fail to handle arbitrary geometric range queries and efficient spatial keyword query over high-dimensional encrypted data. To address these issues, we propose a high-dimEnsional and Privacy-preserving Spatial Keyword Query (EPSKQ) scheme with arbitrary geometric ranges over encrypted spatial data, leveraging Hilbert curve encoding and Enhanced Matrix-based Inner Product Encryption (EMIPE). In EPSKQ, spatial locations and multi-keywords are encoded into compact vectors, and arbitrary geometric range queries are transformed into range intersection tests. To reduce computational overhead, we employ vector bucketing technique to partition large-size vectors into several small-size sub-vectors. Furthermore, we design a novel index structure called Hilbert Binary tree (HB-tree) to optimize range intersection tests. Based on HB-tree, we propose an enhanced spatial keyword query scheme, named EPSKQ+, which further improves query performance. Security analysis demonstrates that both EPSKQ and EPSKQ+ achieve semantic security against indistinguishability under chosen-plaintext attack (IND-CPA). Extensive experimental evaluations show that the proposed EPSKQ and EPSKQ+ schemes significantly outperform state-of-the-art schemes in terms of computational and communication costs, with EPSKQ+ being 9× and 3× faster than the state-of-the-art schemes in the index build and query phase, respectively.},
  archive      = {J_TMC},
  author       = {Fuyuan Song and Yunlong Gao and Mingyang Zhao and Chuan Zhang and Zheng Qin and Bin Xiao},
  doi          = {10.1109/TMC.2025.3581562},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12121-12136},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {High-dimensional and secure spatial keyword query with arbitrary ranges in mobile cloud},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical and heterogeneous federated learning via a learning-on-model paradigm. <em>TMC</em>, <em>24</em>(11), 12103-12120. (<a href='https://doi.org/10.1109/TMC.2025.3581534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) collaboratively trains a shared global model without exposing clients’ private data. In practical FL systems, clients (e.g., smartphones and wearables) typically have disparate system resources. Traditional FL, however, adopts a one-size-fits-all solution, where a homogeneous large model is sent to and trained on each client. This method results in an overwhelming workload for less capable clients and starvation for others. To tackle this, we propose FedConv, a client-friendly FL framework, minimizing the system overhead on resource-constrained clients by providing heterogeneous customized sub-models. FedConv features a novel learning-on-model paradigm that learns the parameters of heterogeneous sub-models via convolutional compression. To aggregate heterogeneous sub-models, we propose transposed convolutional dilation to convert them back to large models with a unified size while retaining personalized information. The compression and dilation processes, transparent to clients, are tuned on the server using a small public dataset. We further propose a hierarchical and clustering-based local training strategy for enhanced performance. Extensive experiments on six datasets show that FedConv outperforms state-of-the-art FL systems in terms of model accuracy (by more than 35% on average), computation and communication overhead (with 33% and 25% reduction, respectively).},
  archive      = {J_TMC},
  author       = {Leming Shen and Qiang Yang and Kaiyan Cui and Yuanqing Zheng and Xiao-Yong Wei and Jianwei Liu and Jinsong Han},
  doi          = {10.1109/TMC.2025.3581534},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12103-12120},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hierarchical and heterogeneous federated learning via a learning-on-model paradigm},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NestQuant: Post-training integer-nesting quantization for on-device DNN. <em>TMC</em>, <em>24</em>(11), 12088-12102. (<a href='https://doi.org/10.1109/TMC.2025.3582583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying quantized deep neural network (DNN) models with resource adaptation capabilities on ubiquitous Internet of Things (IoT) devices to provide high-quality AI services can leverage the benefits of compression and meet multi-scenario resource requirements. However, existing dynamic/mixed precision quantization requires retraining or special hardware, whereas post-training quantization (PTQ) has two limitations for resource adaptation: (i) The state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying multiple PTQ models with diverse bitwidths consumes large storage resources and switching overheads. To this end, this paper introduces a resource-friendly post-training integer-nesting quantization, i.e., NestQuant, for on-device quantized model switching on IoT devices. The proposed NestQuant incorporates the integer weight decomposition, which bit-wise splits quantized weights into higher-bit and lower-bit weights of integer data types. It also contains a decomposed weights nesting mechanism to optimize the higher-bit weights by adaptive rounding and nest them into the original quantized weights. In deployment, we can send and store only one NestQuant model and switch between the full-bit/part-bit model by paging in/out lower-bit weights to adapt to resource changes and reduce consumption. Experimental results on the ImageNet-1 K pretrained DNNs demonstrated that the NestQuant model can achieve high performance in top-1 accuracy, and reduce in terms of data transmission, storage consumption, and switching overheads. In particular, the ResNet-101 with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and part-bit models, respectively, and reduce switching overheads by approximately 78.1% compared with diverse bitwidths PTQ models.},
  archive      = {J_TMC},
  author       = {Jianhang Xie and Chuntao Ding and Xiaqing Li and Shenyuan Ren and Yidong Li and Zhichao Lu},
  doi          = {10.1109/TMC.2025.3582583},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12088-12102},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {NestQuant: Post-training integer-nesting quantization for on-device DNN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSID: Enhancing wi-fi based gait recognition via adversarial learning. <em>TMC</em>, <em>24</em>(11), 12076-12087. (<a href='https://doi.org/10.1109/TMC.2025.3583946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Wi-Fi sensing, wireless-based gait recognition has become increasingly important as it supports a wide range of applications (person identification, disease diagnosis, etc.). However, two serious challenges limit the universal deployment of such Wi-Fi vision schemes: i) the limited bandwidth of Wi-Fi severely restricts the granularity of gait recognition, and ii) users’ non-gait behaviors (e.g., stopping and turning) interfere with the extraction of gait-related features. In this paper, we propose CSID, which can achieve robust gait recognition under the limited bandwidth conditions of commercial Wi-Fi devices. Specifically, we use a neural network to generate super-resolution spectrograms of channel state information (CSI), overcoming the limitation of insufficient Wi-Fi bandwidth. To overcome the challenge of non-gait behavior interference, considering the human-incomprehensible nature of Wi-Fi spectrograms, we adopt cross-domain adversarial training and further extract gait features that are independent of the interference behaviors by learning domain-independent representations. We conducted a large number of experiments in different indoor environments, and the average person identification rate of the CSID system reached 91.6%. These results demonstrate that the CSID system is promising and could be used as a complement to visual person identification systems in the future.},
  archive      = {J_TMC},
  author       = {Yu Liu and Jingyang Hu and Hongbo Jiang and Kehua Yang and Wei Zhang and Zheng Qin},
  doi          = {10.1109/TMC.2025.3583946},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12076-12087},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CSID: Enhancing wi-fi based gait recognition via adversarial learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced stereo UWB bearing scheme via network ambiguity resolution and online phase calibration. <em>TMC</em>, <em>24</em>(11), 12061-12075. (<a href='https://doi.org/10.1109/TMC.2025.3583257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-wideband (UWB) is a prominent technology for wireless localization, mainly attributed to its superior ranging performance enabled by the large signal bandwidth. However, its bearing capability remains underdeveloped due to practical issues such as phase deviations, antenna coupling, and phase ambiguity. This paper presents a high-accuracy stereo ultra-wideband (UWB) bearing scheme through network ambiguity resolution and online phase calibration. Specifically, we propose a sparse variational Gaussian process regression-based calibration technique to eliminate phase deviations and a range-assisted network solution to resolve phase ambiguities. Building on these techniques, we present an online angle estimation scheme that performs real-time phase calibration, ambiguity resolution, and calibration model updates, significantly reducing calibration complexity in large-scale networks. Real-world experiments on 4-element stereo UWB platforms achieve root mean square errors of 2.3$^\circ$ and 1.1$^\circ$ for azimuth and elevation angles, respectively. The success rate for ambiguity resolution exceeds 96%, a 20% improvement over existing methods.},
  archive      = {J_TMC},
  author       = {Yi Li and Hanying Zhao and Yiman Liu and Yuan Shen},
  doi          = {10.1109/TMC.2025.3583257},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12061-12075},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An enhanced stereo UWB bearing scheme via network ambiguity resolution and online phase calibration},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural networks for the optimization of collaborative federated learning energy efficiency. <em>TMC</em>, <em>24</em>(11), 12049-12060. (<a href='https://doi.org/10.1109/TMC.2025.3582911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper delves into the design of an energy efficient collaborative federated learning (CFL) methodology using which mobile devices exchange their FL model with a subset of their neighbors without reliance on a parameter server based on the distributed graph neural network (GNN) method. Each device is unable to send its FL model to every neighboring device due to device mobility and wireless resource limitations. To reduce the energy consumption of FL model transmission, each device must choose a subset of devices with which to share its FL model. This problem is formulated as an optimization problem to meet the constraints of delay and training loss while minimizing the energy consumption for model transmission. However, the formulated problem is difficult to solve since the device mobility patterns, and the relationship between the device connection scheme and CFL performance are unknown. To address this challenge, we analytically characterize the relationship between dynamic device connections and the performance of CFL methodology. Based on the analysis, a GNN based algorithm is proposed to enable each device to select a subset of its neighbors and the transmit power in a decentralized method. Compared to standard optimization methods that must determine device connections in a centralized manner, the GNN based method enables each device to use its neighboring devices’ location and connection information to individually determine a subset of devices to transmit the local model. Given the device connections, the optimal transmit power of each device can be determined by convex optimization. Simulation results show that the proposed method can reduce the energy consumption for model transmission and training loss by up to 46$\%$ and 2$\%$, respectively.},
  archive      = {J_TMC},
  author       = {Nuocheng Yang and Sihua Wang and Yuchen Liu and Christopher G. Brinton and Changchuan Yin and Mingzhe Chen},
  doi          = {10.1109/TMC.2025.3582911},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12049-12060},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Graph neural networks for the optimization of collaborative federated learning energy efficiency},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-design of sensing, communications, and control for low-altitude wireless networks. <em>TMC</em>, <em>24</em>(11), 12035-12048. (<a href='https://doi.org/10.1109/TMC.2025.3581616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of Internet of Things (IoT) services and the evolution toward the sixth generation (6G) have positioned aerial drones as critical enablers of low-altitude wireless networks (LAWNs). This work investigates the co-design of integrated sensing, communication, and control ($\mathbf {SC^{2}}$) for multi-aerial drone cooperative systems with finite blocklength (FBL) transmission. In particular, the aerial drones continuously monitor the state of the field robots and transmit their observations to the robot controller to ensure stable control while cooperating to localize an unknown sensing target (ST). To this end, a weighted optimization problem is first formulated by jointly considering the control and localization performance in terms of the linear quadratic regulator (LQR) cost and the determinant of the Fisher information matrix (FIM), respectively. The resultant problem, optimizing resource allocations, the aerial drones’ deployment positions, and multi-user scheduling, is non-convex. To circumvent this challenge, we first derive a closed-form expression of the LQR cost with respect to other variables. Subsequently, the non-convex optimization problem is decomposed into a series of sub-problems by leveraging the alternating optimization (AO) approach, in which the difference of convex functions (DC) programming and projected gradient descent (PGD) method are employed to obtain an efficient near-optimal solution. Furthermore, the convergence and computational complexity of the proposed algorithm are thoroughly analyzed. Extensive simulation results are presented to validate the effectiveness of our proposed approach compared to the benchmark schemes and reveal the trade-off between control and sensing performance.},
  archive      = {J_TMC},
  author       = {Haijia Jin and Jun Wu and Weijie Yuan and Fan Liu and Yuanhao Cui},
  doi          = {10.1109/TMC.2025.3581616},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12035-12048},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Co-design of sensing, communications, and control for low-altitude wireless networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crowdsensing for emergency response in unknown environments: A rapid strategic sensing approach. <em>TMC</em>, <em>24</em>(11), 12019-12034. (<a href='https://doi.org/10.1109/TMC.2025.3583816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating Unmanned Aerial Vehicles (UAVs) and autonomous vehicles within the crowdsensing paradigm offers a promising approach to collecting environment-relevant data over large spatial areas, particularly in disaster-stricken or high-risk regions. However, deploying crowdsensing systems in emergency response scenarios presents substantial challenges. The lack of prior environmental knowledge complicates the selection of optimal sensing locations and strategy optimization, often relying on costly trial-and-error methods. Additionally, real-time decision-making is critical in such scenarios, requiring the rapid identification of optimal deployment strategies. Yet, the absence of prior knowledge further complicates the assessment of the optimality of these strategies. This gap remains inadequately addressed in existing research. To address this, we present the first framework that frames these challenges as a rapid online strategy optimization problem for mobile agent-based crowdsensing systems operating in unknown environments during emergency response scenarios. We propose ${\sf DGap\!-\! UCB}$, a novel approach within the multi-armed bandit (MAB) framework, which efficiently identifies the optimal sensing strategy with high-confidence guarantees. Leveraging the Upper-Confidence Bound (UCB) technique, ${\sf DGap \!-\! UCB}$ iteratively refines strategy selection based on reward feedback. To accelerate learning, we introduce a gap-confidence pair $(\Delta _{t}, \delta _{t})$-based Quick Stopping Criterion, enabling rapid and high-confidence identification of the optimal strategy. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of ${\sf DGap\!-\!UCB}$ over state-of-the-art techniques.},
  archive      = {J_TMC},
  author       = {Shan Su and Liang Wang and Zhiwen Yu and Xiaofang Xia and Lianbo Ma and Fei Xiong and Yao Zhang and Bin Guo},
  doi          = {10.1109/TMC.2025.3583816},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12019-12034},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Crowdsensing for emergency response in unknown environments: A rapid strategic sensing approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient multi-access edge computing for heterogeneous satellite-maritime networks: A hybrid harvesting-and-offloading design. <em>TMC</em>, <em>24</em>(11), 12001-12018. (<a href='https://doi.org/10.1109/TMC.2025.3581607'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low earth orbit (LEO) constellation integrated maritime networks have recently attracted much interest due to the rapid development of maritime applications and services. LEO satellites have the advantages of wide coverage to provide seamless connection for maritime wireless devices. However, due to the limited battery and computing capacity of uncrewed aerial vehicles (UAVs) for ocean information perception and processing, the computing-intensive and delay-sensitive oceanic data suffer from long latency and high energy consumption, which degrades the efficiency of maritime services. In this paper, to enhance the perception and offloading endurance of UAVs in maritime networks, we propose an energy efficient multi-access edge computing scheme for heterogeneous satellite-maritime networks, with the objective of minimizing the cumulative transmitted energy for UAVs. Specifically, we first present a heterogeneous satellite-maritime network framework in which LEO satellites and uncrewed surface vehicles (USVs) equipped with edge servers can process workloads simultaneously. Next, considering the limited battery supply of UAVs, we propose a hybrid harvesting-and-offloading scheme for resource allocation, where UAVs first harvest energy from solar power and radio frequency power from USV, and then UAVs determine the offloading strategy for task processing. Moreover, a joint optimization problem is formulated to optimize the offloading decision, the time scheduling, and the transmitting power. We also exploit a vertical architecture to solve the formulated problem. Regarding each decomposed sub-problem, we propose efficient algorithms to derive the corresponding solutions. Finally, we provide numerical results to validate the performance of our proposed algorithms in comparison with several benchmark algorithms.},
  archive      = {J_TMC},
  author       = {Minghui Dai and Shan Chang and Yixuan Wang and Zhou Su},
  doi          = {10.1109/TMC.2025.3581607},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {12001-12018},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy-efficient multi-access edge computing for heterogeneous satellite-maritime networks: A hybrid harvesting-and-offloading design},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age of computing: A metric of computation freshness in communication and computation cooperative networks. <em>TMC</em>, <em>24</em>(11), 11987-12000. (<a href='https://doi.org/10.1109/TMC.2025.3582741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In communication and computation cooperative networks (3CNs), timely computation is crucial but not always guaranteed. There is a strong demand for a computational task to be completed within a given deadline. The time taken involves processing time, transmission time, and the impact of the deadline. However, a measure of such timeliness in 3CNs is lacking. To address this gap, we propose the novel concept of Age of Computing (AoC) to quantify computation freshness in 3CNs. Built on task timestamps, AoC serves as a practical metric for dynamic and complex real-world 3CNs. We evaluate AoC under two types of deadlines: (i) soft deadline, tasks can be fed back to the source if delayed beyond the deadline, but with additional latency; (ii) hard deadline, tasks delayed beyond the deadline are discarded. We investigate AoC in two distinct networks. In point-to-point, time-continuous networks, tasks are processed sequentially using a first-come, first-served discipline. We derive a general expression for the time-average AoC under both deadlines. Utilizing this expression, we obtain a closed-form solution for M/M/1-M/M/1 systems under soft deadlines and propose an accurate approximation for hard deadlines. These results are further extended to G/G/1-G/G/1 systems. Additionally, we introduce the concept of computation throughput, derive its general expression and an approximation, and explore the trade-off between freshness and throughput. In the multi-source, time-discrete networks, tasks are scheduled for offloading to a computational node. For this scenario, we develop AoC-based Max-Weight policies for real-time scheduling under both deadlines, leveraging a Lyapunov function to minimize its drift.},
  archive      = {J_TMC},
  author       = {Xingran Chen and Yi Zhuang and Kun Yang},
  doi          = {10.1109/TMC.2025.3582741},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11987-12000},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age of computing: A metric of computation freshness in communication and computation cooperative networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MobiMixer: A multi-scale spatiotemporal mixing model for mobile traffic prediction. <em>TMC</em>, <em>24</em>(11), 11972-11986. (<a href='https://doi.org/10.1109/TMC.2025.3585007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding mobile traffic data and predicting future trends are essential for wireless operators and service providers to allocate resources efficiently and manage energy effectively. Despite the strong performance of existing models, accurately forecasting mobile traffic remains a challenge due to limited spatial and temporal modeling capabilities and high computational complexity. This paper introduces MobiMixer, a lightweight and efficient multi-scale spatiotemporal mixing model. Its core concept is to integrate multi-scale information from both spatial and temporal dimensions to improve performance on mobile traffic data. We develop a hierarchical interaction module that incorporates super nodes to enable global high-level feature interactions among nodes with common patterns. Additionally, we employ a dynamic time warping strategy to decouple mobile traffic sequences into stable and seasonal components, which are then modeled at different scales using a multi-scale temporal mixing module. We conduct extensive experiments on mobile traffic datasets collected from four international cities. Compared with 21 state-of-the-art benchmark models, MobiMixer demonstrates highly competitive performance, achieving a maximum improvement of 48.49% on the Milan mobile dataset. The model achieves an improvement in training efficiency of up to 10.69 times and reduces memory usage by 33.01%.},
  archive      = {J_TMC},
  author       = {Jiaming Ma and Binwu Wang and Pengkun Wang and Zhengyang Zhou and Yudong Zhang and Xu Wang and Yang Wang},
  doi          = {10.1109/TMC.2025.3585007},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11972-11986},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MobiMixer: A multi-scale spatiotemporal mixing model for mobile traffic prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wi-GR: Wi-fi-based gait recognition using multi-part velocity profile. <em>TMC</em>, <em>24</em>(11), 11957-11971. (<a href='https://doi.org/10.1109/TMC.2025.3581549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with increasing user demands for convenience, privacy, and personalized experiences, gait recognition has been widely studied across various domains, such as indoor intrusion detection and smart homes. Although computer vision solutions are extensively researched for their visual intuitiveness, Wi-Fi sensing is emerging as a new research focus due to its ability to preserve privacy. However, previous studies have primarily relied on abstract features with limited interpretability or required multiple Wi-Fi links. To address these issues, we propose Wi-GR, which utilizes a Wi-Fi link to extract robust and highly interpretable gait features for user recognition. First, we construct a multi-path gait signal model to establish a clear relationship between Channel State Information (CSI) and gait motion. Then, we design a gait signal separation and enhancement method to mitigate the effects of external non-target reflections and internal multi-part reflections, which significantly impact the extraction and interpretability of gait features. Finally, fine-grained gait features that visualize gait patterns are generated using MUSIC-based and GAN-based multi-part velocity profile generation algorithms, tailored for single-person and multi-person scenarios, respectively. Numerous experiments have demonstrated that Wi-GR achieves single-person recognition accuracies of 95.3%, 94.0%, and 93.2% for 30 persons in the meeting room, corridor, and lobby, respectively, and an average accuracy of 88.3% for two-person recognition.},
  archive      = {J_TMC},
  author       = {Hao Chen and Penghao Wang and Jingyang Hu and Feng Li and Hongbo Jiang and Minglu Li and Chao Liu},
  doi          = {10.1109/TMC.2025.3581549},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11957-11971},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Wi-GR: Wi-fi-based gait recognition using multi-part velocity profile},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust intrusion detection system for vehicular networks: A federated learning approach based on representative client selection. <em>TMC</em>, <em>24</em>(11), 11942-11956. (<a href='https://doi.org/10.1109/TMC.2025.3582237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of network technology has allowed numerous vehicular applications to be deployed in vehicles, thereby enriching the driving experience of users. However, the openness of vehicular networks enables attackers to launch network attacks on vehicles through network ports, leading to the destruction of vehicular networks. To develop an intrusion detection system suitable for distributed vehicular networks, researchers have utilized federated learning to train detection models. Nevertheless, most federated learning-based vehicular intrusion detection systems seldom consider rapidly updating the detection model and fail to detect unknown attacks effectively. In this study, we propose a federated learning-based vehicular intrusion detection system that fully considers the traffic characteristics of multiple network regions and selects representative clients to participate in model aggregation, thereby accelerating the convergence of the global model. Furthermore, to enhance the robustness of the detection system, we utilize extreme value theory and multilayer activation vectors to construct an unknown attack discriminator that can determine whether a network flow is an unknown attack. Comprehensive experiments on three open datasets demonstrate that the proposed intrusion detection system can quickly update and effectively identify known/unknown attacks in open vehicular networks.},
  archive      = {J_TMC},
  author       = {Chunyang Fan and Jie Cui and Hulin Jin and Hong Zhong and Irina Bolodurina and Debiao He},
  doi          = {10.1109/TMC.2025.3582237},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11942-11956},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust intrusion detection system for vehicular networks: A federated learning approach based on representative client selection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic topology and resource allocation for distributed training in mobile edge computing. <em>TMC</em>, <em>24</em>(11), 11927-11941. (<a href='https://doi.org/10.1109/TMC.2025.3581510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing (MEC), edge servers and mobile terminals use federated learning distributed architecture to build a deep model, so that terminals can cooperate in training without sharing data. Distributed training requires network virtualization to provide high bandwidth and low latency characteristics to support large-scale parallel computing. Traditional virtual network embedding (VNE) relies on a static network topology, which lacks flexibility and incurs high resource costs during model training. To improve the efficiency of embedding distributed training tasks, we propose a novel Node Selection and Dynamic Topology resource allocation scheme for VNE of distributed training, NSDT-VNE, based on reconfigurable network topology. This algorithm divides the underlying network into static and dynamic topologies, enhancing low latency for small flows while providing high bandwidth for large flows as needed. Additionally, we introduce a two-phase coordinated alternating optimization algorithm that optimizes embedding decisions at both computational and topological levels, ensuring optimal node selection. Overall, NSDT-VNE follows demand-aware network design principles, allowing continuous optimization of the underlying topology. Compared to state-of-the-art heuristic and reinforcement learning-based virtual network algorithms, NSDT-VNE achieves superior performance, with request acceptance rates improving by 6.67% to 25.68% and embedding revenue increasing by approximately 7% to 32%.},
  archive      = {J_TMC},
  author       = {Weibei Fan and Donglai Wang and Fu Xiao and Yiping Zuo and Mengjie Lv and Lei Han and Sun-Yuan Hsieh},
  doi          = {10.1109/TMC.2025.3581510},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11927-11941},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic topology and resource allocation for distributed training in mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FreshSpec: Sashimi freshness monitoring with low-cost multispectral devices. <em>TMC</em>, <em>24</em>(11), 11910-11926. (<a href='https://doi.org/10.1109/TMC.2025.3581714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring sashimi freshness, i.e., histamine levels, in showcases poses a critical challenge for sushi restaurants and fresh food stores. Current histamine monitoring methods involve labor-intensive chemical experiments or expensive devices, making affordable on-site monitoring difficult. This paper proposes FreshSpec, a low-cost and automatic spectral imaging system capable of precisely monitoring histamine levels in sashimi with minimal human intervention. The low concentration of histamine, combined with the potential for other ingredients to mask its spectral characteristics, complicates precise histamine level predictions using coarse or redundant spectral data from low-cost devices. To address this issue, FreshSpec employs an innovative feature-wise spectral reconstruction (SR) framework that effectively eliminates irrelevant and redundant data while preserving critical histamine-related spectral features. Specifically, we redefine the SR reconstruction target by utilizing features derived from the encoder of the spectral foundation model that is enhanced to focus on histamine-related spectral features. Furthermore, inspired by the monotonic accumulation properties of histamine over time, we propose a histamine regression model with unsupervised continual adaptation to new sashimi samples during practical deployment. Experimental results from 240 samples of salmon, tuna, and snapper demonstrate that FreshSpec achieves an R2 of 0.9319 and an RMSE of 3.101 mg/100 g, comparable to laboratory spectral imaging systems, while outperforming baseline schemes with a 46.95% RMSE reduction and a 0.1631 R2 improvement.},
  archive      = {J_TMC},
  author       = {Yinan Zhu and Haiyan Hu and Baichen Yang and Qianyi Huang and Qian Zhang and Wei Li},
  doi          = {10.1109/TMC.2025.3581714},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11910-11926},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FreshSpec: Sashimi freshness monitoring with low-cost multispectral devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AIGC-assisted federated learning for vehicular edge intelligence: Vehicle selection, resource allocation and model augmentation. <em>TMC</em>, <em>24</em>(11), 11896-11909. (<a href='https://doi.org/10.1109/TMC.2025.3581983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To leverage the vast amounts of onboard data while ensuring privacy and security, federated learning (FL) is emerging as a promising technology for supporting a wide range of vehicular applications. Although FL has great potential to improve the vehicular edge intelligence(VEI), challenges arise due to vehicle mobility, wireless channel instability, and data heterogeneity. To mitigate the issue of heterogeneous data across vehicles in FL, artificial intelligence-generated content (AIGC) can be employed as an innovative data synthesis technique to enhance FL model performance. In this paper, we propose AIGC-assisted Federated Learning for Vehicular Edge Intelligence (GenFV). We further propose a weighted policy using the Earth Mover’s Distance (EMD) to measure data distribution heterogeneity and introduce a convergence analysis for GenFV. Subsequently, we analyze system delay and formulate a mixed-integer nonlinear programming (MINLP) problem to minimize system delay. To solve this MINLP NP-hard problem, we propose a two-scale algorithm. At large communication scale, we implement label sharing and vehicle selection based on mobility and data heterogeneity. At the small computation scale, we optimally allocate bandwidth, transmission power and amount of generated data. Extensive experiments show that GenFV significantly improves the performance and robustness of FL in dynamic, resource-constrained environments, outperforming other schemes and confirming the effectiveness of our approach.},
  archive      = {J_TMC},
  author       = {Xianke Qiang and Zheng Chang and Geyong Min},
  doi          = {10.1109/TMC.2025.3581983},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11896-11909},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AIGC-assisted federated learning for vehicular edge intelligence: Vehicle selection, resource allocation and model augmentation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge computing underwater optical wireless sensor networks. <em>TMC</em>, <em>24</em>(11), 11879-11895. (<a href='https://doi.org/10.1109/TMC.2025.3581690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Optical Wireless Sensor Networks (UOWSNs) play important roles in resource exploration and maritime rescue. However, they face significant challenges in real-time data transmission due to the limited propagation range of optical signals (typically 10–100 m), frequent link disconnections caused by node mobility, and the extended distances to onshore servers. Traditional cloud computing solutions, designed for stable terrestrial networks with stationary edge servers and continuous connectivity, experience high latency (3-15 s) in UOWSNs, rendering them unsuitable for real-time applications in underwater environments. To address this issue, we propose a cloud-edge-end architecture tailored for UOWSNs, which can not only combat unique underwater environmental interference on link connection and topological changes but also guarantee robust and real-time communication. We develop a dynamic link-stability-based task offloading path selection (DLS-TOPS) algorithm for maximizing network resource profits. Afterward, we propose an online primal-dual task offloading (OPD-TO) algorithm for minimizing task completion time. Simulation results indicate that the proposed method significantly improves the real-time performance and resource profits of the network, reducing the total task completion time by more than 50% compared to baseline algorithms. We implemented a UOWSN with a cloud-edge-end architecture using commercial off-the-shelf and verified the applicability and effectiveness of the proposed scheme in emergency detection through testbed experiments.},
  archive      = {J_TMC},
  author       = {Yang Chi and Chi Lin and Jing Deng and Kaiwen Ning and Xin Fan and Guowei Wu},
  doi          = {10.1109/TMC.2025.3581690},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11879-11895},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge computing underwater optical wireless sensor networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OTFS-assisted ISAC system: Delay doppler channel estimation and SDR-based implementation. <em>TMC</em>, <em>24</em>(11), 11865-11878. (<a href='https://doi.org/10.1109/TMC.2025.3582421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal Time-Frequency Space (OTFS) modulation is an emerging technique that characterizes wireless channels and transmits information in the delay-Doppler domain. This work focuses on estimating fundamental sensing parameters, i.e., the delay and Doppler shifts of individual propagation paths, which serve as critical enablers for downstream positioning techniques, such as time-difference-of-arrival (TDOA)-based localization. Specifically, we propose a parameter-inherited (PI) channel estimation method that integrates sparse Bayesian learning (SBL) with unitary approximate message passing (UAMP), achieving low computational complexity and high estimation robustness. To accelerate the convergence of the UAMP-based iterative estimation, we explore the strategy of initializing parameters by inheriting prior estimates from adjacent OTFS transmission blocks. Furthermore, the overall computational burden is significantly reduced by employing large-scale matrix operations via two-dimensional fast Fourier transform (2D FFT). The proposed algorithms are implemented and evaluated on a software-defined radio (SDR)-based ISAC platform. Experimental results demonstrate that the proposed dual-functional system outperforms existing benchmarks in both communication quality and sensing parameter accuracy.},
  archive      = {J_TMC},
  author       = {Xinyuan Wei and Weijie Yuan and Kecheng Zhang and Fan Liu},
  doi          = {10.1109/TMC.2025.3582421},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11865-11878},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {OTFS-assisted ISAC system: Delay doppler channel estimation and SDR-based implementation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The price of forgetting: Incentive mechanism design for machine unlearning. <em>TMC</em>, <em>24</em>(11), 11852-11864. (<a href='https://doi.org/10.1109/TMC.2025.3582904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data protection policies (e.g., GDPR) enforce the right to be forgotten and require companies to perform machine unlearning once users request data removal. This process incurs costs for server and degrades model performance, impacting users’ satisfaction. In this paper, we propose the first incentive mechanism for machine unlearning, where server compensates users to retain their data. We characterize server’s major unlearning costs, accuracy degradation and consumed time, in data redemption amount through experiments on three datasets and two unlearning algorithms. We model server-users interaction as a two-stage Stackelberg game. In Stage I, server optimizes compensation unit prices to minimize costs. In Stage II, users jointly decide data redemption amounts as a non-cooperative game. By restricting the feasible set of Stage I to Nash Equilibrium of Stage II, we formulate a challenging non-convex bilevel optimization problem. We propose an iterative algorithm to compute optimal unit prices in Stage I and equilibrium data redemption amounts in Stage II by characterizing bilevel problem’s convexity. We prove the distributed convergence of best response updates to the unique Nash equilibrium by showing Stage II is a submodular game. Experimental results show that our mechanism minimizes server cost and maximizes social welfare over two practical baselines.},
  archive      = {J_TMC},
  author       = {Yue Cui and Man Hon Cheung},
  doi          = {10.1109/TMC.2025.3582904},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11852-11864},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {The price of forgetting: Incentive mechanism design for machine unlearning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV swarm-enabled collaborative post-disaster communications in low altitude economy via a two-stage optimization approach. <em>TMC</em>, <em>24</em>(11), 11833-11851. (<a href='https://doi.org/10.1109/TMC.2025.3583510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-altitude economy (LAE), as a new economic paradigm, plays an indispensable role in cargo transportation, healthcare, infrastructure inspection, and especially post-disaster communications. Specifically, uncrewed aerial vehicles (UAVs), as one of the core technologies of the LAE, can be deployed to provide communication coverage, facilitate data collection, and relay data for trapped users, thereby significantly enhancing the efficiency of post-disaster response efforts. However, conventional UAV self-organizing networks exhibit low reliability in long-range cases due to their limited onboard energy and transmit ability. Therefore, in this paper, we design an efficient and robust UAV-swarm enabled collaborative self-organizing network to facilitate post-disaster communications. Specifically, a ground device transmits data to UAV swarms, which then use collaborative beamforming (CB) technique to form virtual antenna arrays and relay the data to a remote access point (AP) efficiently. Then, we formulate a rescue-oriented post-disaster transmission rate maximization optimization problem (RPTRMOP), aimed at maximizing the transmission rate of the whole network. Given the challenges of solving the formulated RPTRMOP by using traditional algorithms, we propose a two-stage optimization approach to address it. In the first stage, the optimal multi-path traffic routing and the theoretical upper bound on the transmission rate of the network are derived. In the second stage, we transform the formulated RPTRMOP into a variant named V-RPTRMOP based on the obtained optimal multi-path traffic routing, aimed at rendering the actual transmission rate closely approaches its theoretical upper bound by optimizing the excitation current weight and the placement of each participating UAV via a diffusion model-enabled particle swarm optimization (DM-PSO) algorithm. Simulation results show the effectiveness of the proposed two-stage optimization approach in improving the transmission rate of the constructed network, which demonstrates the great potential for post-disaster communications. Moreover, the robustness of the constructed network is also validated via evaluating the impact of three unexpected situations on the system transmission rate.},
  archive      = {J_TMC},
  author       = {Xiaoya Zheng and Geng Sun and Jiahui Li and Jiacheng Wang and Qingqing Wu and Dusit Niyato and Abbas Jamalipour},
  doi          = {10.1109/TMC.2025.3583510},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11833-11851},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UAV swarm-enabled collaborative post-disaster communications in low altitude economy via a two-stage optimization approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A user-centric energy-saving method for dynamic 5G heterogeneous networks using deep reinforcement learning. <em>TMC</em>, <em>24</em>(11), 11820-11832. (<a href='https://doi.org/10.1109/TMC.2025.3582623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy consumption (EC) represents a significant challenge for 5G and 6G mobile networks, necessitating a primary focus on optimizing energy savings (ES). This paper illustrates the practical benefits of a user-centric deep reinforcement learning (DRL) models in achieving a green cellular network. The primary objective is to optimize energy usage in a heterogeneous network (HetNet). The optimization of power consumption (PC) in such networks is a non-convex and NP-hard problem. To address this challenge, we propose using reinforcement learning (RL). Due to the extensive state and action space, classical RL approaches are unsuitable. Therefore, the adoption of DRL methods, notably the deep Q-network (DQN) and deep deterministic policy gradient (DDPG) methods, is necessary. The proposed approach entails a user-centric connection establishment, whereby small base stations (SBSs) are switched to an on mode. The mode switching determined by the DRL methods is controlled by an anti-abrupt transition mechanism, which prevents unnecessary oscillations in the network. The results are benchmarked against existing approaches, specifically genetic algorithm (GA) and particle swarm optimization (PSO) for ES. The proposed methods outperform both GA and PSO optimization techniques in terms of ES and significantly reduce time consumption, enhancing its practical implementation feasibility.},
  archive      = {J_TMC},
  author       = {Mohammad Ali Arami and Erfan Rasti and Abbas Mohammadi},
  doi          = {10.1109/TMC.2025.3582623},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11820-11832},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A user-centric energy-saving method for dynamic 5G heterogeneous networks using deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint power allocation and phase shifts design for distributed RIS-assisted multiuser systems. <em>TMC</em>, <em>24</em>(11), 11808-11819. (<a href='https://doi.org/10.1109/TMC.2025.3582750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed reconfigurable intelligent surfaces (RISs) provide rich macro-diversity coverage due to different locations of the RISs, which is beneficial to combat coverage holes. However, the system performance relies on the effective coordination of multiple RISs. In particular, distributed RIS-assisted power allocation and the phase shifts of RISs should be jointly designed under nonlinear scheduling constraints. Thus, the resource allocation scheme for distributed RIS-assisted multiuser system is a crucial challenge. To tackle these issues, joint power allocation, phase shifts and communication scheduling design for distributed RIS-assisted systems is investigated in this paper, where all RISs simultaneously and cooperatively serve multiple users. To overcome the formulated nonconvex optimization problem, the original problem is decoupled into three subproblems and solved in an iterative manner. Specifically, we first consider the subproblem of power allocation, which can be solved via maximizing the ergodic achievable rate. By applying the ergodic rate, an approximate closed-form solution is formed for the power allocation. Subsequently, the phase shifts are optimized using the minimization-maximization optimization methods. Finally, a communication scheduling scheme is presented to address the scheduling variables. Numerical simulations are conducted to demonstrate that the considered solution outperforms the existing benchmark and achieves a near-optimal spectral efficiency.},
  archive      = {J_TMC},
  author       = {Zhen Chen and Gaojie Chen and Xiu Yin Zhang and Jie Tang and Shi Jin and Kai-Kit Wong and Jonathon Chambers},
  doi          = {10.1109/TMC.2025.3582750},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11808-11819},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint power allocation and phase shifts design for distributed RIS-assisted multiuser systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LOCA: Long-term optimization based on chunk-level analysis in edge-assisted massive mobile live streaming. <em>TMC</em>, <em>24</em>(11), 11793-11807. (<a href='https://doi.org/10.1109/TMC.2025.3581922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an edge-assisted massive mobile live streaming (MMLS) framework named LOCA, integrating chunk-level analysis and long-term optimization to design resource allocation, bitrate adaptation, and source selection strategies. The proposed method ensures sustained real-time video delivery while minimizing latency and communication costs. First, a chunk-level analysis of the entire process of video streaming is introduced, aiming at modeling fetch queue waiting time and rebuffering duration in each time slot. By embedding this mathamatical model into consideration, a long-term optimization is formulated to minimize rebuffering and communication overhead while maintaining high video qualities for massive users. Leveraging Lyapunov optimization, we transform this problem into a computationally tractable form. Further simplification via linearization achieves near-optimal solutions by adopting the mixed-integer linear programming method with enhanced computational efficiency. Simulation results demonstrate superior stability and long-term performance compared to the state-of-the-art and baseline methods, validating the framework’s efficacy in MMLS scenarios.},
  archive      = {J_TMC},
  author       = {Fangzheng Feng and Yu Zhang and Xinkun Zheng and Ting Bi and Tao Jiang},
  doi          = {10.1109/TMC.2025.3581922},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11793-11807},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LOCA: Long-term optimization based on chunk-level analysis in edge-assisted massive mobile live streaming},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiSec-RTF: Reinforcing RPL resilience against routing table falsification attack in 6LoWPAN. <em>TMC</em>, <em>24</em>(11), 11779-11792. (<a href='https://doi.org/10.1109/TMC.2025.3581561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routing Protocol for Low-Power and Lossy Networks (RPL) is an energy-efficient routing solution for IPv6 over Low-Power Wireless Personal Area Networks (6LoWPAN), recommended for resource-constrained devices. While RPL offers significant benefits, its security vulnerabilities pose challenges, particularly due to unauthenticated control messages used to establish and maintain routing information. These messages are susceptible to manipulation, enabling malicious nodes to inject false routing data. A notable security concern is the Routing Table Falsification (RTF) attack, where attackers forge Destination Advertisement Object (DAO) messages to promote fake routes via a parent node’s routing table. Experimental results indicate that RTF attacks significantly reduce packet delivery ratio, increase end-to-end delay, and leverage power consumption. Currently, no effective countermeasures exist in the literature, reinforcing the need for a security solution to prevent network disruption and protect user applications. This paper introduces a Lightweight Security Solution against Routing Table Falsification Attack (LiSec-RTF), leveraging Physical Unclonable Functions (PUFs) to generate unique authentication codes, termed “Licenses.” LiSec-RTF mitigates RTF attack impact while considering the resource limitations of 6LoWPAN devices in both static and mobile scenarios. Our testbed experiments indicate that LiSec-RTF significantly improves network performance compared to standard RPL under RTF attacks, thereby ensuring reliable and efficient operation.},
  archive      = {J_TMC},
  author       = {Shefali Goel and Vinod Kumar Jain and Abhishek Verma},
  doi          = {10.1109/TMC.2025.3581561},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11779-11792},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LiSec-RTF: Reinforcing RPL resilience against routing table falsification attack in 6LoWPAN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint dynamic VNF placement and delay and jitter aware multicast routing in NFV-enabled SDNs. <em>TMC</em>, <em>24</em>(11), 11764-11778. (<a href='https://doi.org/10.1109/TMC.2025.3581905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For reliability, security and scalability, Multicast Request flows (MRs) need to traverse a Service Function Chain (SFC) that consists of series of Virtual Network Functions (VNFs) such as firewalls, encoder-decoder in Network Function Virtualization-enabled Software-Defined Networks (NFV-enabled SDNs). There are typically multiple identical VNF-instances in the network, it brings significant challenges when dynamically choosing or placing the requisite VNF-instances to construct a Service Function Tree (SFT) consisting of SFCs for fulfilling the MRs’s routing. This paper investigates the Delay and Jitter Aware Dynamic SFT Embedding and Routing Problem (DJA-DSERP) considering VNF placement, network resources, delay and jitter constraints as well as network load balance in NFV-Enabled SDNs. First, we formulate DJA-DSERP as an integer linear programming model and prove it to be NP-hard. Then, an auxiliary edge-weighted graph and an Optimal Link Selection Function (OLSF) are devised, and SFT Embedding Algorithm (SFT-EA) is proposed to address the problem aiming at minimizing the resource consumption costs while satisfying multiple QoS constraints and network load balance. Furthermore, we theoretically prove the effectiveness of the OLSF and the SFT-EA. Simulation results demonstrate that the SFT-EA exhibits superior performance compared to existing algorithms in terms of throughput, traffic acceptance rate, and network load balance.},
  archive      = {J_TMC},
  author       = {Liang Liu and Siyuan Tan and Songtao Guo and Guiyan Liu and Hao Feng},
  doi          = {10.1109/TMC.2025.3581905},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11764-11778},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint dynamic VNF placement and delay and jitter aware multicast routing in NFV-enabled SDNs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile phone-based digital biomarkers empowered by knowledge distillation for diagnosis of parkinson’s disease. <em>TMC</em>, <em>24</em>(11), 11748-11763. (<a href='https://doi.org/10.1109/TMC.2025.3581525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile phones have evolved from basic communication tools to feature-rich mobile devices. These ubiquitous and portable devices, equipped with inertial sensors and high-speed network access, create opportunities for remote health monitoring, especially for movement disorders such as Parkinson’s disease (PD). Inertial sensors (gyroscopes and accelerometers) endow smartphones with a natural ability to monitor movement disorders. Based on this, we develop a novel vision-based time-series feature augmentation framework for remote diagnosis and severity grading of PD using mobile phone walking records. Specifically, preprocessed time-series data is encoded into RGB images for the teacher model, while the time-series data is input into the student model, with the teacher guiding the student’s learning. The teacher model is based on MobileNetV2 and incorporates spatial and channel relation-aware attention mechanisms to capture important features and filter out irrelevant information. The inter-modal feature fusion module combines attention and CNN to emphasize both global and local features. The student model utilizes a simple CNN to directly extract features from time-series data and perform classification. For the three-level classification task, the teacher model achieves accuracies of 0.887, 0.886, and 0.896 across the three datasets, while the distillation student model reaches 0.779, 0.828, and 0.827, generally surpassing state-of-the-art algorithms.},
  archive      = {J_TMC},
  author       = {Tongyue He and Junxin Chen and Chi Lin and Wei Wang},
  doi          = {10.1109/TMC.2025.3581525},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11748-11763},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobile phone-based digital biomarkers empowered by knowledge distillation for diagnosis of parkinson’s disease},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-enhanced healthcare monitoring service refreshment in human digital twin-assisted fabric metaverse. <em>TMC</em>, <em>24</em>(11), 11731-11747. (<a href='https://doi.org/10.1109/TMC.2025.3582084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human digital twin bridges humans with digital avatars in the fabric metaverse, assisting users and healthcare professionals with real-time visualization, analysis, and prediction of personal data sensed by fabric sensors. The human digital twin-assisted healthcare monitoring (HHM) service refreshment refers to sending personal health data to corresponding services hosted on nearby edge servers and receiving the results to update local digital avatars continuously. However, the malicious nature and resource limitations of edge servers may lead to user privacy leaks and refreshment timeout, thereby impacting diagnostics. In this paper, we investigate a novel privacy-enhanced HHM service refreshment maximization problem in the fabric metaverse by considering privacy data encryption, model compression, and personalized user requirements. To this end, we first formulate the above issue as an Integer Linear Programming (ILP) problem, and prove its NP-hardness. Then, a resource scheduler named Wiper is designed, consisting of a shallow-deep distiller and an agile refresher library. To enable efficient inference while preserving user privacy, the former replaces violation modules in existing models with approximations and conducts shallow distillation on model layers to meet operation type and depth limits of homomorphic encryption, and then deep distillation on model parameters to decrease end-to-end refreshment delay. Finally, to satisfy user requirements on accuracy and delay during encrypted refreshments while maximizing the throughput of HHM services in offline and online situations with different problem scales, a series of HHM service refreshment algorithms are merged into the latter, including exact, performance-guaranteed approximation, and residual diffusion reinforcement learning algorithms. Theoretical analyses and experiments demonstrate that our algorithms are promising compared with baseline algorithms.},
  archive      = {J_TMC},
  author       = {Yu Qiu and Min Chen and Weifa Liang and Lejun Ai and Dusit Niyato and Gang Wei},
  doi          = {10.1109/TMC.2025.3582084},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11731-11747},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-enhanced healthcare monitoring service refreshment in human digital twin-assisted fabric metaverse},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From non-IID to IID: Mobility-aware hierarchical federated learning with client-edge association control. <em>TMC</em>, <em>24</em>(11), 11717-11730. (<a href='https://doi.org/10.1109/TMC.2025.3585538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying federated learning (FL) in wireless network with hierarchical client-edge-cloud architecture enables large-scale distribution collaboration without long-distance communication latency. However, the ongoing edge dynamics with uncertain client mobility and imbalanced data distributions, poses great challenge for collaboration efficiency of FL. In this work, we first model the client mobility with a Markov chain, and formulate the minimization of performance degradation as a client-edge association control problem. With the analysis of client mobility patterns, we propose ALPHA, a new client-edge association control framework for mobility-aware FL, to reshape the edge-level data distributions close to i.i.d in both offline and online mobility scenarios. In the offline scenario with deterministic client mobility trajectories, we leverage alternating optimization theory to transform the client-edge association control problem into a weighted bipartite b-matching problem, and derive an efficient solution with linear relaxation and dependent rounding techniques. As for the online scenario, where each client arrives at different edge access points (APs) in an online manner, we design a fast and simple online subgradient projection algorithm with a bounded regret to make an online decision on client-edge association. Extensive experiment results on three public datasets and a real-world mobility trajectory dataset show that ALPHA has a superior learning performance with 1.40× – 2.89× convergence speedup compared to state-of-the-art solutions.},
  archive      = {J_TMC},
  author       = {Haibo Liu and Zhenzhe Zheng and Fan Wu and Guihai Chen},
  doi          = {10.1109/TMC.2025.3585538},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11717-11730},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {From non-IID to IID: Mobility-aware hierarchical federated learning with client-edge association control},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IFresher: Information freshening for mobile augmented reality with multi-agent reinforcement learning in edge computing. <em>TMC</em>, <em>24</em>(11), 11703-11716. (<a href='https://doi.org/10.1109/TMC.2025.3581523'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the IFresher framework to improve the timeliness of multi-agent mobile augmented reality (MAR) systems. Existing works have made strides in accuracy-latency trade-offs, but fail to directly address real-time task responsiveness and multi-agent contention challenges. To bridge this gap, we introduce the concept of the age of analytics information (AoAI), which quantifies the combined impact of video analytics (VA) accuracy, transmission delay, and computational efficiency. By deriving a closed-form expression for AoAI, IFresher establishes a central control mechanism that jointly optimizes bandwidth allocation and video configuration to minimize AoAI while ensuring accuracy. Due to the mixed-integer nonlinear characteristics of the problem and the fact that each agent only has local observations, the problem is reformulated into a decentralized partially observable Markov decision process (Dec-POMDP). We propose a multi-agent reinforcement learning (MARL) algorithm, named convex-embedded transformer QMIX (CTQMIX), using the centralized training and decentralized execution (CTDE) framework for agent collaboration. Specifically, the convex optimization ensures optimal bandwidth distribution, and the transformer captures temporal dependencies between observations and actions across time steps to improve decision-making in dynamic environments. Evaluations with real-world experiments show that the CTQMIX outperforms state-of-the-art (SOTA) algorithms.},
  archive      = {J_TMC},
  author       = {Shuang Cheng and Zhaoyang Wang and Fangzheng Feng and Yu Zhang and Ting Bi and Tao Jiang},
  doi          = {10.1109/TMC.2025.3581523},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11703-11716},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {IFresher: Information freshening for mobile augmented reality with multi-agent reinforcement learning in edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CADTR: Context-aware trust routing algorithm based on priority sampling DDPG for UASNs. <em>TMC</em>, <em>24</em>(11), 11688-11702. (<a href='https://doi.org/10.1109/TMC.2025.3581512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The underwater acoustic sensor network (UASN) is a pivotal paradigm within the underwater Internet of Things, where multi-hop forwarding-based underwater data routing is essential for information acquisition. However, the dynamic nature of underwater network topology and the instability of underwater acoustic communication pose significant challenges to achieving efficient and reliable data transmission. In light of unreliable underwater environments and potential malicious attacks, studying trusted routing strategies for UASNs is crucial. This study introduces a context-aware trust routing scheme (CADTR) based on deep reinforcement learning (DRL), which integrates real-time environmental state perception with AI-driven routing decisions, thereby enhancing the reliability and robustness of data routing in dynamic and potentially hostile underwater scenarios. First, a unified trust evidence framework is developed to strengthen the support of evidence experience for subsequent trust decisions by mapping multi-dimensional trust evidence to a unified scale. This framework is tightly coupled with the DRL agent, allowing the agent to evaluate and update trust levels based on real-time evidence. Second, a dynamic topology perception model and an underwater acoustic communication perception model are constructed to enable real-time perception of the interactive experience context. These models provide continuous input to the DRL agent, enabling it to adapt to topological changes and communication conditions dynamically. This facilitates priority experience sampling during the training process of the routing decision model, indirectly boosting model training efficiency and decision accuracy. Finally, the DRL agent learns optimal routing policies by interacting with the environment, leveraging the trust evidence and perception models to make informed decisions. Experimental results demonstrate that the proposed CADTR algorithm significantly improves the overall performance of the routing strategy in terms of packet delivery rate, energy utilization efficiency, and data transmission delay compared to the benchmark algorithms.},
  archive      = {J_TMC},
  author       = {Yu He and Guangjie Han and Jinfang Jiang and Xin Cheng and Pengfei Xu},
  doi          = {10.1109/TMC.2025.3581512},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11688-11702},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CADTR: Context-aware trust routing algorithm based on priority sampling DDPG for UASNs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AoI-aware air-ground mobile crowdsensing by multi-agent curriculum learning with collaborative observation augmentation. <em>TMC</em>, <em>24</em>(11), 11675-11687. (<a href='https://doi.org/10.1109/TMC.2025.3583499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By harnessing the capabilities of unmanned aerial and ground vehicles (UAVs and UGVs), equipped with high-precision sensors, air-ground mobile crowdsensing (AG-MCS) has proven to be effective for data collection in urban environments. In this paper, by optimizing the metric of age-of-information (AoI) that measures the freshness of collected data, we consider the problem of AoI-Aware AG-MCS (A3G-MCS), where UGVs dispatch UAVs from multiple UGV stops to collect data from point-of-interests (PoIs). We propose a novel multi-agent curriculum learning framework called “MACL(MCS)”, that explicitly balances the individual and team goals of both UAV/UGV controllers to facilitate the exploration of policy towards globally-optimal performance. It is further enhanced by a UAV/UGV collaborative observation augmentation (COA) module for improved inter-controller communication. Extensive results reveal that MACL(MCS) consistently outperforms five baselines, and achieves comparable performance to exact method with better scalability and efficiency. It also showcases strong generalization capability towards real-world scenarios on both TSPLIB and Purdue, KAIST and NCSU datasets.},
  archive      = {J_TMC},
  author       = {Yuxiao Ye and Yuxuan Tian and Chi Harold Liu and Linkang Dong and Guangpeng Qi and Dapeng Wu},
  doi          = {10.1109/TMC.2025.3583499},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11675-11687},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AoI-aware air-ground mobile crowdsensing by multi-agent curriculum learning with collaborative observation augmentation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly detection and localization in NFV systems by utilizing masked-autoencoder and XAI. <em>TMC</em>, <em>24</em>(11), 11657-11674. (<a href='https://doi.org/10.1109/TMC.2025.3582195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Network Functions Virtualization (NFV) systems into mobile edge and core networks has heightened the need for effective anomaly detection and localization methods. The complexity of NFV demands robust mechanisms for network resilience, security, and performance. Machine Learning approaches have demonstrated promising solutions in crafting adaptive and efficient mechanisms for detecting and localizing potential anomalies within NFV systems. Particularly, Unsupervised Learning (UL) methods have garnered significant attention for their potential to detect anomalies without the need for labeled data. However, UL methods are susceptible to even minor levels of anomalous samples in the training data, termed contamination, which can severely compromise their performance. This paper proposes a novel approach using the Noisy-Student technique for anomaly detection. It addresses data contamination by combining a density-estimation teacher model for pseudo-labeling with a weakly-supervised student model based on a Masked Autoencoder trained on the pseudo-labeled data. For anomaly localization, we introduce a heuristic tailored for our anomaly detection model and two Explainable Artificial Intelligence (XAI)-based approaches applicable to any detection model. Extensive experiments on three NFV datasets demonstrate superior performance, with up to a 20% improvement in anomaly detection and up to a 22% improvement in localization, in terms of F1-score.},
  archive      = {J_TMC},
  author       = {Seyed Soheil Johari and Nashid Shahriar and Massimo Tornatore and Raouf Boutaba and Aladdin Saleh},
  doi          = {10.1109/TMC.2025.3582195},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11657-11674},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Anomaly detection and localization in NFV systems by utilizing masked-autoencoder and XAI},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative edge and cloud computing: Optimal configuration and computation management. <em>TMC</em>, <em>24</em>(11), 11641-11656. (<a href='https://doi.org/10.1109/TMC.2025.3584524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) plays an increasingly important role in the rapidly increasing mobile applications by providing high-quality computing services. The majority of current research has focused on designing efficient computing task offloading schemes to ensure the effectiveness of the MEC system. However, the configuration and resource management of the MEC system, which are crucial for its scattered features, have not received due attention. This paper investigates the configuration and computation resource management problem for the MEC system by formulating a profit maximization problem. To address this problem, we first analyze the relationship among mobile users’ offloading decisions, the configuration and computation management of the MEC system, and the service quality. Then, we design an optimal configuration and computation management scheme for the MEC system, which can not only maintain the efficiency of computing processes but also make a good trade-off between profitability and service quality. In such a way, the total expected profit of the MEC system can be maximized. Numerical evaluations show that the proposed optimal configuration and computation management scheme can efficiently improve the total profit of the MEC system.},
  archive      = {J_TMC},
  author       = {Yongmin Zhang and Wei Wang and Rui Huang and Junfan Zhou and Yang Xu and Ju Ren and Yaoxue Zhang},
  doi          = {10.1109/TMC.2025.3584524},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11641-11656},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative edge and cloud computing: Optimal configuration and computation management},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized AR content and POI recommendation in mobile cultural heritage systems using semantic relatedness. <em>TMC</em>, <em>24</em>(11), 11628-11640. (<a href='https://doi.org/10.1109/TMC.2025.3587655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a mobile augmented reality (AR) system designed to enhance user experiences at cultural heritage (CH) sites by providing personalized content and points of interest (POI) recommendations. Applying semantic relatedness, we address the heterogeneity of CH POIs to improve personalization accuracy. Our system constructs a POI-based co-occurrence graph to model semantic relationships, enriching users’ visited AR content items for better recommendations. The proposed method integrates content-filtering-based recommendations with this graph and recommends personalized POIs and AR content items. A user study demonstrated that our system outperforms conventional methods by 9.4% in recommendation precision and recall, significantly enhancing user engagement and attention focus during CH tours.},
  archive      = {J_TMC},
  author       = {Maryam Shakeri and Abolghasem Sadeghi-Niaraki and Jens Grubert and Soo-Mi Choi},
  doi          = {10.1109/TMC.2025.3587655},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11628-11640},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Personalized AR content and POI recommendation in mobile cultural heritage systems using semantic relatedness},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling state shifting via local-global distillation for event-frame gaze tracking. <em>TMC</em>, <em>24</em>(11), 11614-11627. (<a href='https://doi.org/10.1109/TMC.2025.3581317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper tackles the problem of passive gaze estimation using both event and frame (or 2D image) data. Considering the inherently different physiological structures, it is intractable to accurately estimate gaze purely based on a given state. Thus, we reformulate gaze estimation as the quantification of the state shifting from the current state to several prior registered anchor states. Specifically, we propose a two-stage learning-based gaze estimation framework that divides the whole gaze estimation process into a coarse-to-fine approach involving anchor state selection and final gaze location. Moreover, to improve the generalization ability, instead of learning a large gaze estimation network directly, we align a group of local experts with a student network, where a novel denoising distillation algorithm is introduced to utilize denoising diffusion techniques to iteratively remove inherent noise in event data. Extensive experiments demonstrate the effectiveness of the proposed method, which surpasses state-of-the-art methods by a large margin of 15$\%$.},
  archive      = {J_TMC},
  author       = {Zhiyu Zhu and Jinhui Hou and Jiading Li and Jinjian Wu and Junhui Hou},
  doi          = {10.1109/TMC.2025.3581317},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11614-11627},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Modeling state shifting via local-global distillation for event-frame gaze tracking},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-knowledge neighbor discovery for underwater optical wireless sensor networks. <em>TMC</em>, <em>24</em>(11), 11596-11613. (<a href='https://doi.org/10.1109/TMC.2025.3581371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighbor discovery poses significant challenges in Underwater Optical Wireless Sensor Networks (UOWSNs) due to the unique characteristics of directional transceivers, line-of-sight communication, and mobility induced by water currents. Traditional methods typically rely on prerequisites and prior knowledge, such as centralized coordination, time synchronization, and neighbor-related information, which are often unavailable or impractical in underwater environments. In this paper, we make the first attempt to address the issue of Robust and Efficient Neighbor Discovery (termed the REND problem) in UOWSNs with zero-knowledge. Here, zero-knowledge refers to the capability that enables sensors to identify neighbors in dynamic underwater optical channel conditions without prerequisites or prior knowledge. We design a zero-knowledge distributed directional neighbor discovery scheme inspired by gear meshing. We then propose a deterministic algorithm for the REND problem based on theoretical analysis. Additionally, to further reduce the discovery delay for the periodic REND problem, we develop a greedy-based approximation algorithm with a performance guarantee. Finally, extensive simulations demonstrate that the proposed scheme reduces the discovery delay by 34.9% on average and achieves an additional 54.4% reduction for periodic neighbor discovery. Furthermore, test-bed experiments are carried out to verify the applicability of our zero-knowledge scheme in real-world scenarios.},
  archive      = {J_TMC},
  author       = {Yu Tian and Lei Wang and Chi Lin and Bin Han and Lupeng Zhang and Zhiyi Zhou and Yu Sun and Bingxian Lu},
  doi          = {10.1109/TMC.2025.3581371},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11596-11613},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Zero-knowledge neighbor discovery for underwater optical wireless sensor networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and asynchronous multi-node cooperative vehicular fog computing enhanced IoV. <em>TMC</em>, <em>24</em>(11), 11582-11595. (<a href='https://doi.org/10.1109/TMC.2025.3581397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vehicular Fog Computing (VFC) provides low-latency computing service to support emerging intelligent transportation applications in Internet of Vehicles (IoV). Multi-node cooperative VFC can utilize Connected and Autonomous Vehicles (CAVs) to implement cooperative intelligence. Due to the mobility of vehicles, service migration is necessary when task offloading service providers change. This paper proposes an Asynchronous Task Offloading Scheme (ATO-S) that allows each CAV to choose an independent optimization period and provides robust task offloading services under unknown vehicle mobility probability distribution. To the best of our knowledge, this is the first work to investigate asynchronous and robust multi-node cooperative task offloading in dynamic VFC-enhanced IoV scenarios. Furthermore, we formulate the long-term energy consumption minimization problem of VFC and transfer it into each time slot problem by Lyapunov optimization. Then we design Asynchronous Task Offloading Algorithm (ATO-A) to jointly optimizing CAVs matching, communication and computation resource allocation, and transmission power based on multiple mathematical techniques and hybrid heuristic algorithm. Extensive simulations based on real-world traffic scenario are conducted by varying multiple crucial parameters. Simulation results demonstrate the energy efficiency and task queue stability achieved by ATO-A, and service robustness achieved by ATO-S, in comparison with benchmark solutions.},
  archive      = {J_TMC},
  author       = {Yifeng Zhao and Chenyi Liang and Zhibin Gao and Lianfen Huang},
  doi          = {10.1109/TMC.2025.3581397},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11582-11595},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust and asynchronous multi-node cooperative vehicular fog computing enhanced IoV},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RAPOO: An efficient privacy-preserving facial expression recognition via mobile crowdsensing. <em>TMC</em>, <em>24</em>(11), 11568-11581. (<a href='https://doi.org/10.1109/TMC.2025.3581687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition is a technology that involves analyzing and interpreting human facial expressions to determine individual expressions or states. Mobile crowdsensing (MCS), a promising sensing paradigm, makes it easy to capture facial images and benefits facial expression recognition. Existing inference models for facial expression recognition usually rely on facial feature vectors or facial images, increasing privacy concerns about expression. For this reason, this paper proposes a privacy-preserving facial expression recognition scheme through MCS, named RAPOO, which falls in a client-server architecture. Roughly speaking, a user captures facial images using mobile devices and requests a recognition service provided by a cloud computing center. To protect the privacy of expressions, our approach focuses on designing secure computation protocols required by facial expression recognition necessarily, such as secure vector distance calculation and secure top-$k$ query. These protocols enable facial expression recognition over encrypted data directly. To speed up the recognition and store encrypted feature vectors, a $k$-D tree data structure is introduced. The security analysis confirms that RAPOO effectively preserves the confidentiality of personal expressions. Extensive experimental evaluations show that our solution obtains a three-order-of-magnitude speedup in terms of computational overhead compared with the state-of-the-art.},
  archive      = {J_TMC},
  author       = {Bo Tian and Bowen Zhao and Yang Xiao and Yang Liu and Qingqi Pei and Yulong Shen},
  doi          = {10.1109/TMC.2025.3581687},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11568-11581},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RAPOO: An efficient privacy-preserving facial expression recognition via mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLM-CoSen: Revisiting collaborative sensing with large language models (LLMs). <em>TMC</em>, <em>24</em>(11), 11555-11567. (<a href='https://doi.org/10.1109/TMC.2025.3583345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative sensing has emerged as a novel sensing paradigm, entailing multi-sensor data sharing and multimodal modeling to collaboratively understand sensing behaviors. However, current solutions, i.e., data-level and decision-level fusion methods, fall short of generality, expert knowledge, and holistic/chronic perspective. In this paper, we propose LLM-CoSen to revisit collaborative sensing with Large Language Models (LLMs). Specifically, LLM-CoSen designs a semantic-level fusion approach for inference results for collaborative sensing. Such an approach is characterized by its generality, making it applicable to any heterogeneous devices, and its expert knowledge incorporation, which provides chronic, holistic, and insightful perspectives on the inference results. Regarding inference absence challenges, we propose a personalized model design method to constrain inference time, and a voting-based two-pass prompt engineering strategy for token completion. Regarding inference error challenges, we propose an accuracy restoration strategy for personalized models, and a two-level error estimator coupled with self-correction. Experimental results of human digital system use case on four corresponding benchmark datasets show LLM-CoSen can decrease inference absence by 72.83% and inference errors by 7.65% on average.},
  archive      = {J_TMC},
  author       = {Xingyu Feng and Zehua Sun and Zhuangzhuang Chen and Chengwen Luo and Zhangbing Zhou and Victor C.M. Leung and Weitao Xu},
  doi          = {10.1109/TMC.2025.3583345},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11555-11567},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LLM-CoSen: Revisiting collaborative sensing with large language models (LLMs)},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PHandover: Parallel handover in mobile satellite network. <em>TMC</em>, <em>24</em>(11), 11541-11554. (<a href='https://doi.org/10.1109/TMC.2025.3582245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of Low Earth Orbit satellite constellations has recently spurred tremendous attention from both academia and industry. 5G and 6G standards have specified the LEO satellite network as a key component of the mobile network. However, due to the satellites’ fast traveling speed, ground terminals usually experience frequent and high-latency handover, which significantly deteriorates the performance of latency-sensitive applications. To address this challenge, we propose a parallel handover mechanism for the mobile satellite network which can considerably reduce the handover latency. The main idea is to use plan-based handovers instead of measurement-based handovers to avoid interactions between the access and core networks, hence eliminating the significant time overhead in the traditional handover procedure. Specifically, we introduce a novel network function named Satellite Synchronized Function (SSF), which is designed for being compliant with the standard 5G core network. Moreover, we propose a machine learning model for signal strength prediction, coupled with an efficient handover scheduling algorithm. We have conducted extensive experiments and results demonstrate that our proposed handover scheme can considerably reduce the handover latency by 21× compared to the standard NTN handover scheme and two other existing handover schemes, along with significant improvements in network stability and user-level performance.},
  archive      = {J_TMC},
  author       = {Jiasheng Wu and Shaojie Su and Wenjun Zhu and Xiong Wang and Jingjing Zhang and Xingqiu He and Yue Gao},
  doi          = {10.1109/TMC.2025.3582245},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11541-11554},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PHandover: Parallel handover in mobile satellite network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parasitic communication: Opportunistic utilization of interference using asymmetric demodulation. <em>TMC</em>, <em>24</em>(11), 11527-11540. (<a href='https://doi.org/10.1109/TMC.2025.3581798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of wireless communication technologies, interference has become a key impediment to the improvement of wireless data transmission performance. Traditional interference management (IM) suppresses or adjusts interference at the cost of additional communication resources without exploiting interference effectively. Moreover, wirelessly transmitted data is susceptible to eavesdropping. To address these issues cooperatively, we propose Opportunistic Parasitic Communication with Asymmetric Demodulation(OPC-AD). In particular, we consider the interference experienced by the intended/target communication (i.e., parasitic) receiver (Rx) as the host signal. The target communication constructs a selection signal carrying parasitic indication information based on the data it intends to send and the data decoded by its Rx using asymmetric demodulation from the host signal, and then sends it to its Rx. This signal is used to instruct the parasitic Rx to extract the desired information from the host signal. OPC-AD allows for the exploitation of the interference (i.e., host signal) for data transmission to an interfered Rx. Using AD can also ensure the privacy of the host communication. Since the parasitic communication is concealed within the host signal, eavesdroppers cannot compromise the confidentiality of the parasitic transmission without precisely decoding the selection signal. Furthermore, considering more practical situations, we extend the OPC-AD design to cover a broader range of realistic scenarios. Our experimental results validate the applicability of OPC-AD, while our in-depth simulations demonstrate that parasitic communication can effectively thwart eavesdropping and achieve higher spectral efficiency (SE) than other existing IM methods, particularly in strong interference environments.},
  archive      = {J_TMC},
  author       = {Zhao Li and Lijuan Zhang and Kang G. Shin and Jia Liu and Yicheng Liu and Pintian Lyu and Zheng Yan},
  doi          = {10.1109/TMC.2025.3581798},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11527-11540},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Parasitic communication: Opportunistic utilization of interference using asymmetric demodulation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconsidering sparse sensing techniques for channel sounding using splicing. <em>TMC</em>, <em>24</em>(11), 11511-11526. (<a href='https://doi.org/10.1109/TMC.2025.3581446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-band splicing offers a promising solution to extend existing band-limited communication systems to support high-precision sensing applications. This technique involves performing narrow-band measurements at multiple center frequencies, which are then combined to effectively increase the bandwidth without changing the sampling rate. In this paper, we introduce a mmWave channel sounder based on multi-band splicing, leveraging the sparse nature of wireless channels through compressed sensing and sparse recovery techniques for channel reconstruction. We focus on three sparse recovery methods: the widely used grid-based orthogonal matching pursuit (OMP) algorithm as a baseline, our newly developed two-stage mmSplicer algorithm, which extends the OMP method by introducing an additional stage for improving its performance for our application, and our adaptation of sparse reconstruction by separable approximation (SpaRSA), named Net-SpaRSA, optimized for wireless applications. All three algorithms are integrated into an experimental OFDM-based IEEE 802.11ac system. Our analysis centers on evaluating the performance of these algorithms under limited number of narrow-band measurements, demonstrating that accurate CIR estimation is achievable even using only 50% of the full wideband spectrum. Additionally, we analyze and compare the computational complexity of these algorithms to assess their practical feasibility.},
  archive      = {J_TMC},
  author       = {Sigrid Dimce and Anatolij Zubow and Alireza Bayesteh and Giuseppe Caire and Falko Dressler},
  doi          = {10.1109/TMC.2025.3581446},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11511-11526},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reconsidering sparse sensing techniques for channel sounding using splicing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embodied AI-enhanced vehicular networks: An integrated vision language models and reinforcement learning method. <em>TMC</em>, <em>24</em>(11), 11494-11510. (<a href='https://doi.org/10.1109/TMC.2025.3582864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates adaptive transmission strategies in embodied AI-enhanced vehicular networks by integrating vision language models (VLMs) for semantic information extraction and deep reinforcement learning (DRL) for decision-making. The proposed framework aims to optimize both data transmission efficiency and decision accuracy by formulating an optimization problem that incorporates the Weber-Fechner law, serving as a metric for balancing bandwidth utilization and quality of experience (QoE). Specifically, we employ the large language and vision assistant (LLAVA) model to extract critical semantic information from raw image data captured by embodied AI agents (i.e., vehicles), reducing transmission data size by approximately more than 90% while retaining essential content for vehicular communication and decision-making. In the dynamic vehicular environment, we employ a generalized advantage estimation-based proximal policy optimization (GAE-PPO) method to stabilize decision-making under uncertainty. Simulation results show that attention maps from LLAVA highlight the model’s focus on relevant image regions, enhancing semantic representation accuracy. Additionally, our proposed transmission strategy improves QoE by up to 36% compared to DDPG and accelerates convergence by reducing required steps by up to 47% compared to pure PPO. Further analysis indicates that adapting semantic symbol length provides an effective trade-off between transmission quality and bandwidth, achieving up to a 61.4% improvement in QoE when scaling from 4 to 8 vehicles.},
  archive      = {J_TMC},
  author       = {Ruichen Zhang and Changyuan Zhao and Hongyang Du and Dusit Niyato and Jiacheng Wang and Suttinee Sawadsitang and Xuemin Shen and Dong In Kim},
  doi          = {10.1109/TMC.2025.3582864},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11494-11510},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Embodied AI-enhanced vehicular networks: An integrated vision language models and reinforcement learning method},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient on-device federated learning system through the interplay of client selection and batch size with watermarked data. <em>TMC</em>, <em>24</em>(11), 11480-11493. (<a href='https://doi.org/10.1109/TMC.2025.3585033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables edge devices to collaboratively train a global model using local data. However, the increasing prevalence of watermarks in datasets presents a new challenge to efficient FL. While watermarks assert data ownership and copyright, they introduce complexities that can lead to shortcut learning problems and mislead utility measurements for client selection. These issues are further exacerbated by batch size variations in efficient FL frameworks, ultimately undermining their time-to-accuracy performance. We introduce LotusFL, an FL system designed to address the challenges posed by watermarked datasets in efficient FL. Specifically, it tackles the increased time-to-accuracy due to erroneous client selection and the accuracy degradation observed with larger batch sizes. LotusFL first estimates the characteristics of watermarks through statistical estimation and then adjusts the batch size using this estimated watermark information to balance the negative impact of the watermark against device idle waiting time. Additionally, its client selection mechanism, based on historical information, avoids the misleading utility signals from watermarks. This mechanism, working in conjunction with batch size adjustment, aims to accurately predict device runtime and identify potentially valuable devices. We evaluated LotusFL through a real-world deployment on 40 edge devices. Compared to state-of-the-art efficient FL frameworks, LotusFL achieves superior performance, enhancing accuracy by up to 8.2% and reducing training time by 1.97×.},
  archive      = {J_TMC},
  author       = {Tao Ling and Siping Shi and Hao Wang and Chuang Hu and Dan Wang},
  doi          = {10.1109/TMC.2025.3585033},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11480-11493},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An efficient on-device federated learning system through the interplay of client selection and batch size with watermarked data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive-XLight: Proactive traffic signal control with pluggable and reliable traffic prediction. <em>TMC</em>, <em>24</em>(11), 11465-11479. (<a href='https://doi.org/10.1109/TMC.2025.3581938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic signal control (TSC) plays a crucial role in the intelligent transportation system. Among existing TSC approaches, Proactive TSC (PTSC) predicts future traffic states at intersections and proactively adjusts control policies. It is evident that PTSC methods are highly effective in alleviating both current and future traffic congestion at intersections. However, existing PTSC methods focus on point estimation prediction while neglecting prediction reliability. Additionally, they fail to adaptively coordinate between current and future traffic states for optimal control. To address these limitations, we propose an innovative Proactive-Plugin that can be combined with existing TSC methods to enhance the accuracy and robustness of traffic signal control policies. This plugin enhances two critical aspects: 1) Prediction reliability is achieved through Fine-grained Traffic Uncertainty Quantification. This module generates probabilistic forecasts along with confidence intervals to explicitly indicate the credibility of the predictions. 2) Coordination adaptiveness is enabled by a Current-Future Tradeoff Integration mechanism. This mechanism dynamically adjusts the relative influence of current traffic states and probabilistic forecasts on control policies. To further ensure robustness, we design a multi-task joint optimization to reduce the negative impact of inaccurate predictions during training. Experimental results on six real-world datasets demonstrate consistent improvements in traffic efficiency, validating the effectiveness of our approach.},
  archive      = {J_TMC},
  author       = {Yang Jiang and Shengnan Guo and Hanyang Chen and Xiaowei Mao and Youfang Lin and Huaiyu Wan},
  doi          = {10.1109/TMC.2025.3581938},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11465-11479},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Proactive-XLight: Proactive traffic signal control with pluggable and reliable traffic prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-assisted lightweight cross-domain authentication for multi-UAV wireless networks. <em>TMC</em>, <em>24</em>(11), 11449-11464. (<a href='https://doi.org/10.1109/TMC.2025.3582833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of future network and control technologies has enabled unmanned aerial vehicles (UAVs) to collaborate across diverse geographical areas and task domains, enhancing task execution efficiency through data and resource sharing. In response to the increasing demand for cross-domain task allocation and operations for UAVs, establishing robust authentication mechanisms within trusted domains has become a critical foundation for ensuring secure cross-domain access. Despite significant progress in UAV identity authentication and cross-domain access, challenges persist, such as cumbersome and inefficient processes, UAV resource limitations, and establishing trust relationships across different domains. To address these challenges, this paper introduces a dual blockchain-assisted trusted authentication scheme for UAVs’ cross-domain access. Our approach utilizes a certificateless signcryption algorithm for lightweight UAV authentication, thereby eliminating the need for certificate management. Then, an efficient credit-based trust model is designed to measure the trustworthiness of data-in-transit and cross-domain entities. Furthermore, blockchain technology is introduced to store the relevant information of UAVs and credibility to assist cross-domain authentication. Theoretical security analysis and extensive simulations have been conducted, demonstrating the effectiveness and efficiency of our proposed scheme.},
  archive      = {J_TMC},
  author       = {Mingyue Xie and Zheng Chang and Li Wang and Geyong Min},
  doi          = {10.1109/TMC.2025.3582833},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11449-11464},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-assisted lightweight cross-domain authentication for multi-UAV wireless networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online collaborative resource allocation and task offloading for multi-access edge computing. <em>TMC</em>, <em>24</em>(11), 11430-11448. (<a href='https://doi.org/10.1109/TMC.2025.3580365'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) is emerging as a promising paradigm to provide flexible computing services close to user devices (UDs). However, meeting the computation-hungry and delay-sensitive demands of UDs faces several challenges, including the resource constraints of MEC servers, inherent dynamic and complex features in the MEC system, and difficulty in dealing with the time-coupled and decision-coupled optimization. In this work, we first present an edge-cloud collaborative MEC architecture, where the MEC servers and cloud collaboratively provide offloading services for UDs. Moreover, we formulate an energy-efficient and delay-aware optimization problem (EEDAOP) to minimize the energy consumption of UDs under the constraints of task deadlines and long-term queuing delays. Since the problem is proved to be non-convex mixed integer nonlinear programming (MINLP), we propose an online joint communication resource allocation and task offloading approach (OJCTA). Specifically, we transform EEDAOP into a real-time optimization problem by employing the Lyapunov optimization framework. Then, to solve the real-time optimization problem, we propose a communication resource allocation and task offloading optimization method by employing the Tammer decomposition mechanism, convex optimization method, bilateral matching mechanism, and dependent rounding method. Simulation results demonstrate that the proposed OJCTA can achieve superior system performance compared to the benchmark approaches.},
  archive      = {J_TMC},
  author       = {Geng Sun and Minghua Yuan and Zemin Sun and Jiacheng Wang and Hongyang Du and Dusit Niyato and Zhu Han and Dong In Kim},
  doi          = {10.1109/TMC.2025.3580365},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11430-11448},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Online collaborative resource allocation and task offloading for multi-access edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge graph fusion based semantic communication framework. <em>TMC</em>, <em>24</em>(11), 11416-11429. (<a href='https://doi.org/10.1109/TMC.2025.3583605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic communication (SemCom), a paradigm that emphasizes conveying the meaning of information, faces challenges in precise reasoning in semantic coding models. Knowledge graphs (KGs) offer a potential solution by providing structured triples (entities and relations), enabling inference via entity attributes and relational logic. Several key challenges exist in leveraging KGs within SemCom. The first challenge lies in developing methods to create semantic representations aligning and integrating source data and KG information. Second, reconstructing the original data using KGs becomes challenging particularly under poor communication conditions. Moreover, integrating KGs with source data inevitably increases the transmission overhead. In this paper, we propose a novel SemCom framework named KG-SemCom with sophisticated KG-based semantic encoding and decoding designs to solve these challenges. This framework aligns KG entities with message tokens, and then encodes messages into a semantic fusion of contextual and knowledge-based information. Furthermore, KG-SemCom can utilize the KG and contextual relationships to assist in predicting incomplete or distorted messages during the decoding process. Finally, simulation results demonstrate that KG-SemCom achieves higher accuracy and greater robustness compared to existing benchmarks without incorporating KGs, especially in challenging communication environments.},
  archive      = {J_TMC},
  author       = {Chengsi Liang and Yao Sun and Dusit Niyato and Muhammad Ali Imran},
  doi          = {10.1109/TMC.2025.3583605},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11416-11429},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Knowledge graph fusion based semantic communication framework},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency-aware neural radio-frequency radiance fields. <em>TMC</em>, <em>24</em>(11), 11401-11415. (<a href='https://doi.org/10.1109/TMC.2025.3583580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Maxwell discovered the physical laws of electromagnetic waves about 160 years ago, accurately modeling the propagation of RF signals in large and complex electrical environments remains a persistent challenge. This complexity arises from the interactions between the RF signal and various obstacles, including reflection and diffraction. Inspired by the success of neural networks in mapping the optical field in computer vision, we introduce the neural radio-frequency radiance field, or NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>. This represents a continuous volumetric scene function that effectively models RF signal propagation. Remarkably, after only a sparse amount of training with signal measurements, NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math> can accurately predict the nature and origin of signals received at any location, assuming the transmitter’s position is known. Additionally, we propose the frequency-aware NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math> to enhance channel prediction performance for wideband signals using an RF prism module. Compared to the vanilla NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>, the frequency-aware NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math> achieves a 4 dB improvement in SNR for FDD OFDM channel estimation and is nearly 3.5 × faster. Functioning as a physical-layer neural network, NeRF$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math> also supports application-layer artificial neural networks (ANNs) by generating synthetic training datasets. Our empirical results demonstrate that augmented sensing enhances the accuracy of AoA estimation, achieving an approximate 50% improvement.},
  archive      = {J_TMC},
  author       = {Xiaopeng Zhao and Zhenlin An and Qingrui Pan and Lei Yang},
  doi          = {10.1109/TMC.2025.3583580},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11401-11415},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Frequency-aware neural radio-frequency radiance fields},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cellular infrastructure sharing for network robustness: A citywide empirical study. <em>TMC</em>, <em>24</em>(11), 11386-11400. (<a href='https://doi.org/10.1109/TMC.2025.3580605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual cellular networks have been very robust to random cell tower failures due to redundant cell tower deployments. However, a large-scale clustered failure with multiple cell towers can lead to the loss of services of a cellular network. Recently, off-the-shelf smartphones can support multiple network standards, so cellular network infrastructure sharing is a promising direction to improve the service robustness under potential large-scale clustered tower failures. The existing work on cellular network robustness is usually limited to large-scale studies of individual networks or small-scale studies of multiple networks. In this work, we conduct the first investigation, to our knowledge, on cross-network infrastructure sharing benefits for enhancing robustness with a full cellular penetration rate. Our work is based on all cellular networks in Shenzhen, China, covering over 10 million cellular users. Specifically, we design a new metric to quantify cellular network robustness with or without cross-network sharing under both random and clustered cell tower failures. We further study the impact of different factors on robustness, including the number of networks, spatiotemporal dynamics, contextual factors, and a case study at two key transportation hubs. We provide a set of lessons learned based on our study, along with discussions of the results.},
  archive      = {J_TMC},
  author       = {Zhihan Fang and Guang Yang and Wenjun Lyu and Zhiqing Hong and Shuxin Zhong and Weijian Zuo and Yuelei Xie and Yu Yang and Guang Wang and Yunhuai Liu and Desheng Zhang},
  doi          = {10.1109/TMC.2025.3580605},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11386-11400},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cellular infrastructure sharing for network robustness: A citywide empirical study},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seamless physical-layer cross-technology communication from ZigBee to LoRa via neural networks. <em>TMC</em>, <em>24</em>(11), 11369-11385. (<a href='https://doi.org/10.1109/TMC.2025.3580396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRa, designed for Low-Power, Wide-Area Networks (LPWANs), is widely used in the Internet of Things (IoT). In contrast, Wireless Personal Area Network (WPAN) technologies like ZigBee struggle to connect directly to LPWANs due to their limited communication range and differing modulation schemes. ZigBee uses Offset Quadrature Phase-Shift Keying (OQPSK) modulation, while LoRa employs Chirp Spread Spectrum (CSS) modulation, complicating cross-technology communication. To address this challenge, we propose a novel approach for seamless physical-layer cross-technology communication between ZigBee and LoRa networks, bridging the gap between short-range and long-range communication technologies. We introduce ZigRa, a communication method that leverages neural networks for efficient modulation translation between ZigBee’s IEEE 802.15.4 standard and LoRa’s CSS modulation. The core of ZigRa is a deep learning model that adapts and optimizes the transformation of ZigBee signals into ultra-narrowband single-tone sinusoidal signals, which can be reliably detected by LoRaWAN base stations. Our solution enables ZigBee devices to seamlessly connect to LoRa-based LPWANs, overcoming modulation mismatches and providing long-range connectivity. Extensive evaluations with both USRP hardware and commercial devices demonstrate that ZigRa achieves a frame reception rate exceeding 85% at distances up to 500 meters, significantly enhancing the interoperability and coverage of heterogeneous IoT networks.},
  archive      = {J_TMC},
  author       = {Demin Gao and Yongrui Chen and Ye Liu and Honggang Wang},
  doi          = {10.1109/TMC.2025.3580396},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11369-11385},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Seamless physical-layer cross-technology communication from ZigBee to LoRa via neural networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASSUME: An optimal algorithm to minimize UAV energy by altitude and speed scheduling. <em>TMC</em>, <em>24</em>(11), 11351-11368. (<a href='https://doi.org/10.1109/TMC.2025.3581929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed aerial vehicles (UAVs) are being widely employed in wireless communication applications, e.g., collecting data from ground nodes (GNs). Minimizing UAV energy in these applications is crucial due to the limited energy supply onboard. Unlike previous studies that assume UAVs fly at a fixed altitude and simplify the energy consumption model of UAVs, we consider the impact of varying UAV altitudes on the ground-to-air communication and utilize a general communication model for GN. Furthermore, we conduct real-world flight tests and introduce a practical speed-related flight energy consumption model of UAVs. This paper focuses on the UAV altitude-speed scheduling and GN transmission switching (UASS-GTS) problem, specifically in scenarios where the UAV flies straight for monitoring applications such as power transmission lines, roads, and water/oil/gas pipes. However, minimizing energy consumption presents challenges due to the tight coupling of altitude scheduling and speed scheduling. To tackle this, first, we develop the looking before crossing algorithm for speed scheduling. We then extend this algorithm by integrating altitude scheduling to propose the Altitude-Speed Scheduling of UAV for Minimizing Energy (ASSUME) algorithm, using a dynamic programming method. The ASSUME algorithm is theoretically proven to be optimal. Additionally, based on ASSUME, we propose an offline-inspired online heuristic algorithm to handle agnostic situations where GN information is not available unless flies close. Simulations indicate that the ASSUME algorithm saves an average of 26.1%–62.7% energy compared to the baseline methods, and the performance gap between the online algorithm and the offline optimal algorithm ASSUME is 22.8%.},
  archive      = {J_TMC},
  author       = {Jianping Huang and Feng Shan and Junzhou Luo and Runqun Xiong and Wenjia Wu},
  doi          = {10.1109/TMC.2025.3581929},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11351-11368},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ASSUME: An optimal algorithm to minimize UAV energy by altitude and speed scheduling},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast multimodal edge inference via selective feature distillation. <em>TMC</em>, <em>24</em>(11), 11337-11350. (<a href='https://doi.org/10.1109/TMC.2025.3580102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring user status at the edge is essential for delivering personalized services, such as detecting emotional states. However, deploying large-scale models directly on user devices is impractical due to substantial computational overhead and the scarcity of labeled data. Conversely, uploading raw data to the cloud for processing raises significant privacy concerns and incurs prohibitive communication costs. To address this challenge, we propose a privacy-preserving multimodal inference framework that leverages large-scale public data while safeguarding sensitive information and optimizing computational efficiency. Specifically, we first train a teacher model in the cloud using publicly available data. Through a feature distillation process, the knowledge from this teacher model is transferred to a lightweight encoder deployed at the user end. This transfer is tailored to the user’s data, ensuring that only relevant knowledge is distilled. To accommodate varying communication constraints, we introduce a feature compression mechanism that significantly reduces communication overhead without compromising inference accuracy. Extensive experiments on emotion recognition tasks demonstrate that the proposed framework effectively balances privacy preservation, resource efficiency, and inference accuracy, facilitating seamless collaboration between cloud and edge devices.},
  archive      = {J_TMC},
  author       = {Jinyu Chen and Wenchao Xu and Yunfeng Fan and Haozhao Wang and Quan Chen and Jing Li},
  doi          = {10.1109/TMC.2025.3580102},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11337-11350},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fast multimodal edge inference via selective feature distillation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling cross-band backscatter communication with twaltz. <em>TMC</em>, <em>24</em>(11), 11323-11336. (<a href='https://doi.org/10.1109/TMC.2025.3581900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequency switching is a fundamental capability for wireless communication systems. However, this capability is significantly constrained in backscatter systems. The difficulty is to generate tunable high-frequency modulation signals on a backscatter tag at an acceptable power budget. In this paper, we present Twaltz, a new design paradigm for backscatter communication that enables frequency switching across large frequency bands. By exploiting a low-power semiconductor device, i.e., tunnel diode, and carefully addressing its physical features, Twaltz generates oscillation signals up to 1.2 GHz while maintaining micro-watt level power consumption. Twaltz further facilitates on-tag oscillation signal stabilization and programmable oscillation frequency tuning. We prototype Twaltz on a PCB board, demonstrating its efficiency in cross-band communication for LoRa backscatter, and verifying its performance in concurrent transmission, channel hopping, and data transmission.},
  archive      = {J_TMC},
  author       = {Xiuzhen Guo and Boya Liu and Nan Jing and Chaojie Gu and Yuanchao Shu and Jiming Chen},
  doi          = {10.1109/TMC.2025.3581900},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11323-11336},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling cross-band backscatter communication with twaltz},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). X5G: An open, programmable, multi-vendor, end-to-end, private 5G O-RAN testbed with NVIDIA ARC and OpenAirInterface. <em>TMC</em>, <em>24</em>(11), 11305-11322. (<a href='https://doi.org/10.1109/TMC.2025.3580764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Fifth generation (5G) cellular systems transition to softwarized, programmable, and intelligent networks, it becomes fundamental to enable public and private 5G deployments that are (i) primarily based on software components while (ii) maintaining or exceeding the performance of traditional monolithic systems and (iii) enabling programmability through bespoke configurations and optimized deployments. This requires hardware acceleration to scale the Physical (PHY) layer performance, programmable elements in the Radio Access Network (RAN) and intelligent controllers at the edge, careful planning of the Radio Frequency (RF) environment, as well as end-to-end integration and testing. In this paper, we describe how we developed the programmable X5G testbed, addressing these challenges through the deployment of the first 8-node network based on the integration of NVIDIA Aerial RAN CoLab Over-the-Air (ARC-OTA), OpenAirInterface (OAI), and a near-real-time RAN Intelligent Controller (RIC). The Aerial Software Development Kit (SDK) provides the PHY layer, accelerated on Graphics Processing Unit (GPU), with the higher layers from the OAI open-source project interfaced with the PHY through the Small Cell Forum (SCF) Functional Application Platform Interface (FAPI). An E2 agent provides connectivity to the O-RAN Software Community (OSC) near-real-time RIC. We discuss software integration, network infrastructure, and a digital twin framework for RF planning. We then profile the performance with up to 4 Commercial Off-the-Shelf (COTS) smartphones for each base station with iPerf and video streaming applications, as well as up to 25 emulated User Equipments (UEs), measuring a cell rate higher than 1.65 Gbps in downlink and 143 Mbps in uplink.},
  archive      = {J_TMC},
  author       = {Davide Villa and Imran Khan and Florian Kaltenberger and Nicholas Hedberg and Rúben Soares da Silva and Stefano Maxenti and Leonardo Bonati and Anupa Kelkar and Chris Dick and Eduardo Baena and Josep M. Jornet and Tommaso Melodia and Michele Polese and Dimitrios Koutsonikolas},
  doi          = {10.1109/TMC.2025.3580764},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11305-11322},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {X5G: An open, programmable, multi-vendor, end-to-end, private 5G O-RAN testbed with NVIDIA ARC and OpenAirInterface},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age of information-aware multi-objective optimization for heterogeneous UAV-USV-UUV networks in underwater target hunting. <em>TMC</em>, <em>24</em>(11), 11292-11304. (<a href='https://doi.org/10.1109/TMC.2025.3581836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater target hunting (UTH) is a critical and complex mission involving the search, tracking, and hunting of targets in an underwater environment. However, the unpredictable trajectories and flexibility of these targets, along with complex underwater environments, significantly impede the efficiency and success of traditional methods that depend solely on unmanned underwater vehicles (UUVs). Consequently, this paper presents the “3U network”, a novel heterogeneous framework integrating unmanned aerial vehicles (UAVs), unmanned surface vehicles (USVs), and UUVs for UTH. Within this framework, a UAV identifies the target, a USV acts as a communication relay, and a swarm of UUVs hunts the target. Moreover, to improve the timeliness of target detection, we incorporate the age of information (AoI) concept into the UAV’s search strategy. Additionally, we develop a constrained multi-objective optimization problem to minimize energy consumption and mission duration by optimizing vehicles’ trajectories, considering mobility limitations, safety, and connectivity constraints. Furthermore, to tackle this problem, we design an AoI- and energy-aware multi-vehicle twin-delayed deep deterministic policy gradient algorithm (AE-MVTD3) to optimize control policies for heterogeneous vehicles. The experimental results show that the proposed method performs effectively across diverse complex scenarios.},
  archive      = {J_TMC},
  author       = {Xiangwang Hou and Tianyu Xing and Jingjing Wang and Jun Du and Chunxiao Jiang and Yong Ren and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3581836},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {11},
  number       = {11},
  pages        = {11292-11304},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age of information-aware multi-objective optimization for heterogeneous UAV-USV-UUV networks in underwater target hunting},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combating BLE weak links by combining PHY layer symbol extension and link layer coding. <em>TMC</em>, <em>24</em>(10), 11277-11291. (<a href='https://doi.org/10.1109/TMC.2025.3579934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bluetooth Low Energy (BLE) technology supports various Internet-of-Things (IoT) applications. However, because of their limited transmission power and channel interference, their performance is deficient over weak links. Extending physical layer symbols or using error correction code to the link layer is effective somehow. Introducing excessive BLE bits to both respectively can also decrease the network throughput. To optimize the BLE technology performance, we propose CPL, a combining PHY and link layer optimization technology that adaptively allocates BLE bits to both the physical layer and link layer. Then we propose the Cross-Layer BLE Bits Dynamic Allocation Model that unifies the gain of BLE bits in different layers. Finally, we propose an Interference-Aware Controlled CFO Fine-Tuning Method that calibrates the model according to different interference patterns. We implement CPL on Commercial-Off-The-Shelf (COTS) BLE chips and SDR. The experiment results show that under various interference conditions, CPL achieves 50× and 32.16% throughput improvement over RSBLE and Symphony. CPL reduces energy consumption by 60.42% to 97.95% compared to RSBLE, and 11.04% to 25.15% compared to Symphony.},
  archive      = {J_TMC},
  author       = {Renjie Li and Yeming Li and Jiamei Lv and Hailong Lin and Yi Gao and Wei Dong},
  doi          = {10.1109/TMC.2025.3579934},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11277-11291},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Combating BLE weak links by combining PHY layer symbol extension and link layer coding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning for joint trajectory control and task offloading in large-scale partially observable UAV-assisted MEC. <em>TMC</em>, <em>24</em>(10), 11259-11276. (<a href='https://doi.org/10.1109/TMC.2025.3579748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing joint trajectory control and task offloading (JTCTO) algorithms offer ultra-low latency services for smart devices (SDs) in uncrewed aerial vehicle (UAV)-assisted mobile edge computing (MEC). However, these JTCTO algorithms typically require large training datasets to learn the optimal policies, leading to low learning efficiency. Additionally, most existing JTCTO algorithms are difficult to scale to environments with more than a few UAVs, as their complexity increases exponentially with the number of UAVs. In this paper, we propose a decentralized JTCTO algorithm based on the Policy Transfer and Mean Field-based Multi-Agent Actor-Critic (PTMF-MAAC). First, a novel policy transfer algorithm is proposed to determine which UAV’s JTCTO strategy is helpful for each UAV and when to terminate the strategy to accelerate the learning efficiency of the UAV. Second, we propose a partially observable mean field algorithm that significantly reduces the model space by replacing the influence of all other UAVs on a particular UAV with an average value, thereby adapting to large-scale UAV scenarios. Experiments have shown that compared to the baseline, PTMF-MAAC reduces the system cost by 18.44%$\sim$28.57% and improves the model learning efficiency and adaptability to partially observable large-scale UAV-assisted MEC.},
  archive      = {J_TMC},
  author       = {Zhen Gao and Gang Wang and Lei Yang and Yu Dai},
  doi          = {10.1109/TMC.2025.3579748},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11259-11276},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Transfer learning for joint trajectory control and task offloading in large-scale partially observable UAV-assisted MEC},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cooperative UAV-mounted RISs-assisted energy-efficient communications. <em>TMC</em>, <em>24</em>(10), 11241-11258. (<a href='https://doi.org/10.1109/TMC.2025.3579597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative reconfigurable intelligent surfaces (RISs) are promising technologies for 6G networks to support a great number of users. Compared with the fixed RISs, the properly deployed RISs may improve the communication performance with less communication energy consumption, thereby improving the energy efficiency. In this paper, we consider a cooperative uncrewed aerial vehicle-mounted RISs (UAV-RISs)-assisted cellular network, where multiple RISs are carried and enhanced by UAVs to serve multiple ground users (GUs) simultaneously such that achieving the three-dimensional (3D) mobility and opportunistic deployment. Specifically, we formulate an energy-efficient communication problem based on multi-objective optimization framework (EEComm-MOF) to jointly consider the beamforming vector of base station (BS), the location deployment and the discrete phase shifts of UAV-RIS system so as to simultaneously maximize the minimum available rate over all GUs, maximize the total available rate of all GUs, and minimize the total energy consumption of the system, while the transmit power constraint of BS is considered. To comprehensively solve EEComm-MOF which is an NP-hard and non-convex problem with constraints, a non-dominated sorting genetic algorithm-II with a continuous solution processing mechanism, a discrete solution processing mechanism, and a complex solution processing mechanism (INSGA-II-CDC) is proposed. Simulations results demonstrate that the proposed INSGA-II-CDC can solve EEComm-MOF effectively and outperforms other benchmarks under different parameter settings. Moreover, the stability of INSGA-II-CDC and the effectiveness of the improved mechanisms are verified. Finally, the implementability analysis of the algorithm is given.},
  archive      = {J_TMC},
  author       = {Hongyang Pan and Yanheng Liu and Geng Sun and Qingqing Wu and Tierui Gong and Pengfei Wang and Dusit Niyato and Chau Yuen},
  doi          = {10.1109/TMC.2025.3579597},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11241-11258},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cooperative UAV-mounted RISs-assisted energy-efficient communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive charging with beam steering. <em>TMC</em>, <em>24</em>(10), 11224-11240. (<a href='https://doi.org/10.1109/TMC.2025.3579692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the maturation of wireless power transfer technology, Wireless Rechargeable Sensor Networks (WRSNs) have been able to provide a continuous energy supply by scheduling a Mobile Charger (MC). However, traditional charging modes suffer from fixed charging areas that lack the ability to adapt to variable sensor distributions. This inflexibility yields a gap between energy supply and utilization, resulting in relatively low charging efficiency. To address this issue, we propose an adaptive charging mode that utilizes beam steering to dynamically adjust the charging area, thereby catering to different sensor distributions encountered during the charging process. First, we build a dual-symmetric steering charging model to describe the characteristics of dynamic beam steering, enabling precise manipulation of the charging area. Then, we develop a charging power discretization based on steering angle and charging distance to obtain a finite feasible charging strategy set for MC. We reformalize charging utility maximization under energy constraints as a submodular function maximization problem, and propose an approximate algorithm to solve it. Lastly, simulations and field experiments demonstrate that our scheme outperforms other algorithms by 43.9% on average.},
  archive      = {J_TMC},
  author       = {Meixuan Ren and Haipeng Dai and Linglin Zhang and Tang Liu},
  doi          = {10.1109/TMC.2025.3579692},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11224-11240},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive charging with beam steering},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-empowered game theoretical incentive for secure bandwidth allocation in UAV-assisted wireless networks. <em>TMC</em>, <em>24</em>(10), 11209-11223. (<a href='https://doi.org/10.1109/TMC.2025.3579505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the promising unmanned aerial vehicle (UAV)-assisted wireless networks (UAWNs) have emerged by advocating the UAVs to provide wireless transmission services. However, owing to the ever-growing volume of data traffic and the untrusted network operation environment, efficiently and securely assigning limited bandwidth for high-quality wireless communication between UAVs and mobile users poses a significant challenge. To address this challenge, we propose a novel secure UAV-bandwidth allocation scheme to provision reliable wireless transmission services for mobile users in UAWNs. Specifically, we first introduce a novel blockchain-empowered framework for secure bandwidth allocation, designed to automate payment processes and deter malicious activities through the immutable logging of transactional and behavioral data. Wherein, a smart contract is designed to regulate the honest behaviors of both mobile users and UAVs during bandwidth allocation with a distributed manner. Besides, a delegated proof-of-stake (DPoS) with reputation consensus protocol is presented to ensure the authenticity and efficiency of the decision-making process. Further, we apply the Stackelberg game theory to model the dynamic of the bandwidth allocation between mobile users and UAVs. In this game, the UAVs act as game leaders to determine the bandwidth price, while each mobile user acts as a game follower, making decision on the bandwidth request. We utilize the backward induction method to derive the optimal strategies of both parties, culminating in the identification of the Stackelberg equilibrium of the formulated game. Finally, extensive simulations are carried out to show the superiority of the proposed scheme over conventional schemes in terms of security, efficiency, and fairness in bandwidth allocation.},
  archive      = {J_TMC},
  author       = {Qichao Xu and Zhou Su and Haixia Peng and Yuan Wu and Ruidong Li},
  doi          = {10.1109/TMC.2025.3579505},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11209-11223},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-empowered game theoretical incentive for secure bandwidth allocation in UAV-assisted wireless networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed modulation exploiting IRS for secure communications. <em>TMC</em>, <em>24</em>(10), 11193-11208. (<a href='https://doi.org/10.1109/TMC.2025.3579960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the broadcast nature of wireless communications, users’ data transmitted wirelessly is susceptible to security/privacy threats. The conventional modulation scheme “loads” all of the user’s transmitted information onto a physical signal. Then, as long as an adversary overhears and processes the signal, s/he may access the user’s information, hence breaching communication privacy. To counter this threat, we propose IRS-DMSC, a Distributed Modulation based Secure Communication (DMSC) scheme by exploiting Intelligent Reflecting Surface (IRS). Under IRS-DMSC, two sub-signals are employed to realize legitimate data transmission. Of these two signals, one is directly generated by the legitimate transmitter (Tx), while the other is obtained by modulating the phase of the direct signal and then reflecting it at the IRS in an indirect way. Both the direct and indirect signal components superimpose on each other at the legitimate receiver (Rx) to produce a waveform identical to that obtained under traditional centralized modulation (CM), so that the legitimate Rx can employ the conventional demodulation method to recover the desired data from the received signal. IRS-DMSC incorporates the characteristics of wireless channels into the modulation process, and hence can fully exploit the randomness of wireless channels to enhance transmission secrecy. However, due to the distribution and randomization of legitimate transmission, it becomes difficult or even impossible for an eavesdropper to wiretap the legitimate user’s information. Furthermore, in order to address the problem of decoding error incurred by the difference of two physical channels’ fading, we develop Relative Phase Calibration (RPC) and Constellation Point Calibration (CPC), to improve decoding correctness at the legitimate Rx. Our method design, experiment, and simulation have shown the proposed IRS-DMSC to prevent eavesdroppers from intercepting legitimate information while maintaining good performance of the legitimate transmission.},
  archive      = {J_TMC},
  author       = {Zhao Li and Lijuan Zhang and Siwei Le and Kang G. Shin and Jia Liu and Zheng Yan},
  doi          = {10.1109/TMC.2025.3579960},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11193-11208},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed modulation exploiting IRS for secure communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint trajectory and beamforming optimization for AAV-relayed integrated sensing and communication with mobile edge computing. <em>TMC</em>, <em>24</em>(10), 11180-11192. (<a href='https://doi.org/10.1109/TMC.2025.3573702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate joint trajectory and beamforming design for autonomous aerial vehicle (AAV)-relayed integrated sensing and communication (ISAC) systems with mobile edge eomputing (MEC) under the clutter environment. Due to the limited on-board computing capability, the AAV has to offload sensing echoes to the base station (BS) for efficient processing. A novel relay-based ISAC-then-offload frame structure is considered. We aim to maximize the throughput of the BS-AAV-user relaying link while ensuring sensing accuracy and efficient sensing data offloading. The non-convex problem is solved using an alternating optimization algorithm based on successive convex approximation (SCA). Simulation results illustrate that our proposed algorithm achieves near-optimal communication performance while guaranteeing sensing accuracy, addressing the balance between the communication and sensing performance. Furthermore, we evaluate the impact of critical system parameters including sensing constraints, power control factor, and AAV flight duration on communication performance, and explore the trade-offs between energy efficiency and spectral efficiency under varying sensing data intensity and offloading duration.},
  archive      = {J_TMC},
  author       = {Shanfeng Xu and Zhipeng Liu and Le Zhao and Ziyi Liu and Xinyi Wang and Zesong Fei and Arumugam Nallanathan},
  doi          = {10.1109/TMC.2025.3573702},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11180-11192},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint trajectory and beamforming optimization for AAV-relayed integrated sensing and communication with mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmZeAR: Zero-effort cross-category action recognition with mmWave radar. <em>TMC</em>, <em>24</em>(10), 11164-11179. (<a href='https://doi.org/10.1109/TMC.2025.3573168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the widespread application of radio frequency (RF) signal-based human action recognition, traditional solutions can only recognize seen categories and the perception scope is restrained by the limited activity classes. When a novel category emerges, the model needs to be optimized again on additionally collected samples at the cost of computation and labor burden. To address this challenge, we develop the mmZeAR system, which learns semantic knowledge from available vision data as class attributes and then transforms the classification into a matching problem. Specifically, we build the attribute space by fusing the coarse-grained video classification features and fine-grained angle change features of 3D joint skeletons. Then we design an efficient feature extraction backbone named TriSqN, which integrates triple radar heatmaps into the final representations by sufficiently exploring the heterogeneous and complementary characteristics. Finally, a projection network is developed between semantic attributes and radar features to construct indirect relationships between samples and labels. By implementing mmZeAR on millimeter wave (mmWave) radar signal datasets, our extensive experiments have demonstrated its remarkable recognition accuracy in novel category recognition with zero effort and achieved state-of-the-art performance.},
  archive      = {J_TMC},
  author       = {Biyun Sheng and Jiabin Li and Hui Cai and Yiping Zuo and Li Lu and Fu Xiao},
  doi          = {10.1109/TMC.2025.3573168},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11164-11179},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmZeAR: Zero-effort cross-category action recognition with mmWave radar},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Overcoming catastrophic forgetting in federated continual graph learning for resource-limited mobile devices. <em>TMC</em>, <em>24</em>(10), 11151-11163. (<a href='https://doi.org/10.1109/TMC.2025.3573964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Graph Learning (FGL) enables multiple clients to collaboratively learn node representations from private subgraph data, such as user transactions or social networks. Local models are trained on clients and then aggregated by a central server, supporting large-scale graph learning without sharing raw data. However, most existing FGL methods assume that the number of nodes in the graph remains constant, while real-world scenarios often evolve, with new nodes and edges continually added and older ones removed due to limited device memory. We define this setting as Federated Continual Graph Learning (FCGL). In FCGL, global model aggregation may cause interference occur inter-task and inter-client, therefore, FCGL suffers from the global catastrophic forgetting, as the global model adapts to newly added nodes, it loses knowledge acquired from earlier graph data of clients. To address this, we propose GRE-FL, a generative replay framework, which can mitigate global catastrophic forgetting by generating a global summary graph at the server to preserve critical information from historical nodes. It also improves performance by equipping local models with a gating graph attention network for better feature extraction. Experiments show that GRE-FL achieves strong performance across multiple datasets.},
  archive      = {J_TMC},
  author       = {Jiyuan Feng and Xu Yang and Dongyi Zheng and Weihong Han and Binxing Fang and Qing Liao},
  doi          = {10.1109/TMC.2025.3573964},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11151-11163},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Overcoming catastrophic forgetting in federated continual graph learning for resource-limited mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Characterizing and scheduling of diffusion process for text-to-image generation in edge networks. <em>TMC</em>, <em>24</em>(10), 11137-11150. (<a href='https://doi.org/10.1109/TMC.2025.3574065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence-Generated Content (AIGC) technology is transforming content creation by enabling diverse customized and quality services. However, the limited computing resources on mobile devices hinder the provisioning of AIGC services at scale, pose challenges in guaranteeing user-satisfied content quality requirement. To address these challenges, we first investigate the characteristics of prompt category and inference models in Text-to-Image (T2I) diffusion process. It is observed that, model size, denoising steps, and computing resource, are three deciding factors to image generation utility. Based on this insight, we first design an edge-assisted AIGC service system to efficiently process multi-user T2I generative requests, employing a multi-flow queuing model to capture multi-user dynamics and characterize the impact of diffusion scheduling on service latency. The system schedules the diffusion process of T2I generation across edge-deployed models, balancing service quality and computing resource. To maximize generation utility under resource constraints, we propose a Monte Carlo Tree Search-based diffusion scheduling algorithm embedded with adaptive computing resource allocation subroutine. This algorithm ensures that, resource allocation dynamically adapts to scheduling decisions in real time, enabling an effective trade-off between service quality and latency. Extensive experimental comparison against baseline approaches demonstrates that, the proposed system can enhance the generation utility by up to 7.3$\%$, achieving a 2.9$\%$ improvement in quality score and a 33.3$\%$ reduction in service latency.},
  archive      = {J_TMC},
  author       = {Shuangwei Gao and Peng Yang and Yuxin Kong and Feng Lyu and Ning Zhang},
  doi          = {10.1109/TMC.2025.3574065},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11137-11150},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Characterizing and scheduling of diffusion process for text-to-image generation in edge networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMTO: Multi-vehicle multi-hop task offloading in MEC-enabled vehicular networks. <em>TMC</em>, <em>24</em>(10), 11125-11136. (<a href='https://doi.org/10.1109/TMC.2025.3576154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC)-enabled vehicular networks have emerged as a promising approach to enhancing the performance and efficiency of the Internet-of-Vehicles (IoV) applications. By leveraging some vehicles to act as transmission relays, multi-hop task offloading addresses the problem of intermittent connectivity between vehicles and edge servers to cope with the issues of network congestion or obstacles. However, two critical issues, i.e., uncooperative behaviors of selfish vehicles and network resource dynamics, resulting from multi-vehicle concurrent offloading are not fully considered in the existing work. To fill this gap, this paper proposes a novel and efficient task offloading scheme, namely MMTO, that exploits the multi-hop computational resources to maximize the system-wide profit, and supports incentive compatibility of vehicular users and concurrent offloading. Specifically, an iterative hierarchical estimation algorithm is designed to estimate the offloading delay and energy cost in order to iteratively optimize the offloading decisions. An energy-efficient routing approach is then proposed to schedule the transmission paths for the offloading vehicles. Furthermore, an effective reward-driven auction-based incentive mechanism is designed for incentivizing relayers and calculators to engage in collaboration. Both simulation and field experiments are conducted; extensive results demonstrate that MMTO outperforms the state-of-the-art approaches in terms of the system-wide profit improvement and overall task delay reduction.},
  archive      = {J_TMC},
  author       = {Wenjie Huang and Zhiwei Zhao and Geyong Min and Yang Wang and Zheng Chang},
  doi          = {10.1109/TMC.2025.3576154},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11125-11136},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MMTO: Multi-vehicle multi-hop task offloading in MEC-enabled vehicular networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential federated learning in hierarchical architecture on non-IID datasets. <em>TMC</em>, <em>24</em>(10), 11110-11124. (<a href='https://doi.org/10.1109/TMC.2025.3573928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a real federated learning (FL) system, communication overhead for passing model parameters between the clients and the parameter server (PS) is often a bottleneck. Hierarchical federated learning (HFL) that poses multiple edge servers (ESs) between clients and the PS can partially alleviate communication pressure but still needs the aggregation of model parameters from multiple ESs at the PS. To further reduce communication overhead, we remove the central PS, so that each iteration only completes model training by transmitting the global model between two adjacent ES. We call this serial learning method Sequential FL (SFL). For the first time, we introduced SFL into HFL and proposed a novel algorithm adapted to this combined framework, called Fed-CHS. Convergence results are derived for strongly convex and non-convex loss functions under various data heterogeneity setups, which show comparable convergence performance with the algorithms for HFL or SFL solely. Experimental results provide evidence of the superiority of our proposed Fed-CHS on both communication overhead saving and test accuracy over baseline methods.},
  archive      = {J_TMC},
  author       = {Xingrun Yan and Shiyuan Zuo and Rongfei Fan and Han Hu and Li Shen and Puning Zhao and Yong Luo},
  doi          = {10.1109/TMC.2025.3573928},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11110-11124},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sequential federated learning in hierarchical architecture on non-IID datasets},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward deterministic satellite-terrestrial integrated networks via resource adaptation and differentiated scheduling. <em>TMC</em>, <em>24</em>(10), 11092-11109. (<a href='https://doi.org/10.1109/TMC.2025.3574740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite-terrestrial integrated network (STIN) is a full-scale communication paradigm, which can support joint information processing and seamless service provision by leveraging satellites’ wide coverage and terrestrial networks’ high capacity. The existing STIN operates with insufficient synergy in transmission scheduling, impacting resource allocation efficiency and transmission delay optimization, particularly in complex transmission scenarios. In this paper, we design Deterministic STIN (DetSTIN), a novel architecture for STIN, along with two algorithms tailored for transmission scheduling to collaboratively optimize resource adaptation and service flow scheduling. Specifically, the DetSTIN enables the smooth interconnection and integration of heterogeneous networks by providing layered deterministic services. Besides, a genetic-based resource adaptation algorithm is designed for fixed-mobile-satellite heterogeneous networks to reduce resource allocation overhead while maintaining the network performance. Furthermore, we propose a deep reinforcement learning-based differentiated scheduling algorithm to solve the routing-queue two-dimensional decision problem to differentially optimize transmission delay of service flows, thus obtaining higher transmission scheduling benefit. By addressing resource adaptation and differentiated scheduling synergistically, the proposed solution achieves reduced resource allocation overhead and increased transmission scheduling benefit, ultimately leading to increased network operation revenue of the DetSTIN. Simulation results demonstrate that the proposed solution delivers effective performance across various flow proportions, and as the number of flows increases, the network operation revenue exhibits a noticeable improvement, compared with benchmark algorithms.},
  archive      = {J_TMC},
  author       = {Weiting Zhang and Peixi Liao and Dong Yang and Qiang Ye and Shiwen Mao and Hongke Zhang},
  doi          = {10.1109/TMC.2025.3574740},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11092-11109},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward deterministic satellite-terrestrial integrated networks via resource adaptation and differentiated scheduling},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMART: Sim2Real meta-learning-based training for mmWave beam selection in V2X networks. <em>TMC</em>, <em>24</em>(10), 11076-11091. (<a href='https://doi.org/10.1109/TMC.2025.3576203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twins (DT) offer a low-overhead evaluation platform and the ability to generate rich datasets for training machine learning (ML) models before actual deployment. Specifically, for the scenario of ML-aided millimeter wave (mmWave) links between moving vehicles to roadside units, we show how DT can create an accurate replica of the real world for model training and testing. The contributions of this paper are twofold: First, we propose a framework to create a multimodal Digital Twin (DT), where synthetic images and LiDAR data for the deployment location are generated along with RF propagation measurements obtained via ray-tracing. Second, to ensure effective domain adaptation, we leverage meta-learning, specifically Model-Agnostic Meta-Learning (MAML), with transfer learning (TL) serving as a baseline validation approach. The proposed framework is validated using a comprehensive dataset containing both real and synthetic LiDAR and image data for mmWave V2X beam selection. It also enables the investigation of how each sensor modality impacts domain adaptation, taking into account the unique requirements of mmWave beam selection. Experimental results show that models trained on synthetic data using transfer learning and meta-learning, followed by minimal fine-tuning with real-world data, achieve up to 4.09× and 14.04× improvements in accuracy, respectively. These findings highlight the potential of synthetic data and meta-learning to bridge the domain gap and adapt rapidly to real-world beamforming challenges.},
  archive      = {J_TMC},
  author       = {Divyadharshini Muruganandham and Suyash Pradhan and Jerry Gu and Torsten Braun and Debashri Roy and Kaushik Chowdhury},
  doi          = {10.1109/TMC.2025.3576203},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11076-11091},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SMART: Sim2Real meta-learning-based training for mmWave beam selection in V2X networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scene-aware model adaptation scheme for cross-scene online inference on mobile devices. <em>TMC</em>, <em>24</em>(10), 11061-11075. (<a href='https://doi.org/10.1109/TMC.2025.3574766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging Artificial Intelligence of Things (AIoT) applications desire online prediction using deep neural network (DNN) models on mobile devices. However, due to the movement of devices, unfamiliar test samples constantly appear, significantly affecting the prediction accuracy of a pre-trained DNN. In addition, unstable network connection calls for local model inference. In this paper, we propose a light-weight scheme, called Anole, to cope with the local DNN model inference on mobile devices. The core idea of Anole is to first establish an army of compact DNN models, and then adaptively select the model fitting the current test sample best for online inference. The key is to automatically identify model-friendly scenes for training scene-specific DNN models. To this end, we design a weakly-supervised scene representation learning algorithm by combining both human heuristics and feature similarity in separating scenes. Moreover, we further train a model classifier to predict the best-fit scene-specific DNN model for each test sample. We implement Anole on different types of mobile devices and conduct extensive trace-driven and real-world experiments based on unmannedaerial vehicles (UAVs). The results demonstrate that Anole outwits the method of using a versatile large DNN in terms of prediction accuracy (4.5% higher), response time (33.1% faster) and power consumption (45.1% lower).},
  archive      = {J_TMC},
  author       = {Yunzhe Li and Hongzi Zhu and Zhuohong Deng and Yunlong Cheng and Zimu Zheng and Liang Zhang and Shan Chang and Minyi Guo},
  doi          = {10.1109/TMC.2025.3574766},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11061-11075},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A scene-aware model adaptation scheme for cross-scene online inference on mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security-aware designs of multi-UAV deployment, task offloading and service placement in edge computing networks. <em>TMC</em>, <em>24</em>(10), 11046-11060. (<a href='https://doi.org/10.1109/TMC.2025.3574061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) has emerged as a promising solution to support wireless devices’ computation-intensive services in the absence of terrestrial infrastructures. Nevertheless, the heterogeneous nature of MEC services and the security vulnerability of wireless channels present significant challenges to achieving efficient and secure computation offloading. In this paper, we investigate a multi-UAV-assisted MEC network in which wireless devices need to process diverse computation tasks. The devices can perform local computing or offload their computation tasks to UAV servers that have pre-cached relevant service programs in the presence of eavesdroppers. To facilitate secure service provisioning, we propose a cooperative jamming-based scheme in which a UAV jammer transmits jamming signals to interfere with eavesdroppers during devices’ computation offloading processes. Taking into account UAV servers’ constrained caching spaces and secure offloading requirements, we minimize the total task completion delay of devices by jointly optimizing multi-UAV deployment, task offloading decisions, service placement, UAV jammer’s transmit power, and devices’ transmit power. To tackle the formulated mixed-integer nonlinear programming problem, we design an optimization-embedding multi-agent twin delayed deep deterministic policy gradient (OE-MATD3) algorithm. Specifically, the MATD3 approach is leveraged to deal with optimization variables concerning UAVs, while a closed-form solution for devices’ transmit power is derived and guides MATD3-based decision-making. Simulation results demonstrate that the proposed scheme outperforms baselines in terms of devices’ task completion delay.},
  archive      = {J_TMC},
  author       = {Mengru Wu and Haonan Wu and Weidang Lu and Lei Guo and Inkyu Lee and Abbas Jamalipour},
  doi          = {10.1109/TMC.2025.3574061},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11046-11060},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Security-aware designs of multi-UAV deployment, task offloading and service placement in edge computing networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-source domain generalization for CSI-based human activity recognition. <em>TMC</em>, <em>24</em>(10), 11034-11045. (<a href='https://doi.org/10.1109/TMC.2025.3573457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain generalization remains a key challenge in human activity recognition based on channel state information (CSI). Different domains correspond to distinct data distributions, deviating from the typical assumption of independent and identically distributed (i.i.d.) data, which leads to significant performance degradation when models are applied to unseen domains. To address this issue, we propose a novel domain generalization model that integrates meta-learning initialization and an adaptive channel grouping attention mechanism. First, a meta-learning strategy is employed to acquire well-initialized parameters from multiple source domain tasks, enabling the model to implicitly enhance its cross-domain generalization ability. Second, an adaptive grouping attention mechanism is designed in the feature extraction stage to effectively capture the sensitivity differences of different subcarriers to human activities. Meanwhile, a random masking training mechanism is introduced to simulate real-world domain variations and improve model robustness. In addition, a domain adversarial training framework based on the gradient reversal layer (GRL) is adopted to mitigate domain-specific feature dependency, further enhancing the model’s generalization capability. We evaluate our proposed method on both a self-collected dataset, which includes human activity data from nine volunteers across six different environments, and a public CSI dataset. The experimental results demonstrate that our method significantly outperforms existing approaches in domain generalization performance, verifying its effectiveness and practical applicability.},
  archive      = {J_TMC},
  author       = {Tianqi Fan and Sen Qiu and Wei Gong and Yuguang Fang},
  doi          = {10.1109/TMC.2025.3573457},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11034-11045},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-source domain generalization for CSI-based human activity recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedDSV: Shapley value-based contribution estimation in federated learning with dynamic participation. <em>TMC</em>, <em>24</em>(10), 11019-11033. (<a href='https://doi.org/10.1109/TMC.2025.3574784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) succeeds in collaborative and privacy-preserving ML model training among multiple distributed data owners. To maintain a healthy FL ecosystem, it is crucial to estimate the contributions of all participants fairly. Due to provable fairness, Shapley value (SV) is widely used for contribution estimation in FL. However, current studies focus on static scenarios with fixed participants and neglect the dynamic settings with the random joining or leaving of participants in practice. This paper fills the gap by proposing FedDSV, a novel contribution estimation framework for FL with dynamic participation. FedDSV supports flexible weighting mechanisms and is compatible with the SV fairness properties in dynamic scenarios. To reduce the computational complexity, we propose a Monte Carlo variant sampling method (SMC), which can adapt well to dynamic scenarios and approximate the true SVs. To evaluate the effectiveness and efficiency of our proposed approaches, extensive experiments under different settings (e.g., frequency switching, low-quality detection, etc.) are conducted on both i.i.d and non-i.i.d. distributions. Experimental results demonstrate that FedDSV can reflect the real utility contribution of data sources for dynamic FL, and SMC can approximate the exact dynamic SVs with larger similarities in a much shorter time than the state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Kaijia Lei and Xuebin Ren and Shusen Yang and Xiaocheng Wang and Fangyuan Zhao},
  doi          = {10.1109/TMC.2025.3574784},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11019-11033},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedDSV: Shapley value-based contribution estimation in federated learning with dynamic participation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPREAR:An efficient attribute-based proxy re-encryption scheme with fast revocation for data sharing in AIoT. <em>TMC</em>, <em>24</em>(10), 11005-11018. (<a href='https://doi.org/10.1109/TMC.2025.3573288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Artificial Intelligence of Things (AIoT) is driving human society from “information” to “intelligence”, and the information technology industry is undergoing tremendous changes. However, AIoT data faces security threats such as leakage and illegal access when assisted by third parties. Therefore, some scholars use attribute-based proxy re-encryption (ABPRE) for secure sharing of data. However, the existing ABPRE schemes suffer from high computational overhead and inefficient attribution revocation, which seriously hinders practical application. To solve these problems, in this paper, we propose an efficient attribute-based proxy re-encryption scheme with fast attribute revocation (EPREAR). We design a non-interactive zero-knowledge proof protocol based on blockchain to ensure the verifiability of the key during attribute revocation. Furthermore, we devise a boundless encryption and decryption mechanism to enable the system’s encryption and decryption with a fixed computation overhead, regardless of the size of the attribute set. And EPREAR possesses the ability to add infinite attributes without re-initializing the system. Finally, we perform theoretical and experimental analyses that show EPREAR has excellent computational performance. As a consequence, it has better application value in AIoT.},
  archive      = {J_TMC},
  author       = {Xiaoxiao Li and Yong Xie and Cong Peng and Entao Luo and Xiong Li and Zhili Zhou},
  doi          = {10.1109/TMC.2025.3573288},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {11005-11018},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EPREAR:An efficient attribute-based proxy re-encryption scheme with fast revocation for data sharing in AIoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoE-driven proactive caching with DRL in sustainable cloud-to-edge continuum. <em>TMC</em>, <em>24</em>(10), 10992-11004. (<a href='https://doi.org/10.1109/TMC.2025.3577197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-assisted edge computing scenarios can intelligently cache and update the content periodically, thereby enhancing users’ overall perception of service, which is called quality of experience (QoE). To maximize QoE in cloud-to-edge continuum, we formulate a multi-objective optimization problem, which optimizes the cache hit ratio while simultaneously minimizing traffic load and time latency. Particularly, we present an innovative algorithm named Hyperdimensional Transformer with Priority Experience Playback-based Agent Deep network (HT-PAD), which provides a complete solution for prediction and decision-making for proactive caching. First, to improve the prediction accuracy of cached content, we use the encoding layer in hyperdimensional (HD) computing to extract the information features. Second, HD-Transformer, as the prediction part of HT-PAD, is proposed to make predictions based on user preferences, historical information, and popular information. HD-Transformer uses deep neural networks to predict user preferences and process time series data by combining hyperdimensional computation with the Transformer. Third, to avoid errors in the prediction content, we employ PER-MADDPG as the decision-making part of HT-PAD, which consists of Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Prioritized Experience Replay (PER). We use MADDPG to improve content decision-making and utilize PER to select appropriate training samples for PER-MADDPG. Finally, our experiments show that our proposed approach achieves strong performance in terms of edge hit ratio, latency, and traffic load, thus improving QoE.},
  archive      = {J_TMC},
  author       = {Xiaoming He and Yunzhe Jiang and Huajun Cui and Yinqiu Liu and Mingkai Chen and Maher Guizani and Shahid Mumtaz},
  doi          = {10.1109/TMC.2025.3577197},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10992-11004},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoE-driven proactive caching with DRL in sustainable cloud-to-edge continuum},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated meta-learning based computation offloading approach with energy-delay tradeoffs in UAV-assisted VEC. <em>TMC</em>, <em>24</em>(10), 10978-10991. (<a href='https://doi.org/10.1109/TMC.2025.3573278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) provides an applicable solution for computation offloading in Unmanned Aerial Vehicle(UAV)-assisted Vehicular Edge Computing (VEC) by preserving privacy. However, the heterogeneity of clients brings challenges to the generalization of models. Therefore, we propose a federated meta-learning (FML) framework to solve computation offloading for UAV-assisted VEC. In this paper, we are concerned with computation offloading of temporary hotspot regions due to traffic congestion. First, we construct a computation offloading problem with energy-delay tradeoffs and convert the problem to a Markov Decision Process (MDP). Then, we use FML to train personalized models for different vehicles while enhancing the generalization, we propose a Graph neural network-based FL Probabilistic Embedding for Actor-critic RL (GFL-PEARL) algorithm. We model the context as a Directed Acyclic Graph (DAG) and use GNN to reconstruct the inference network of the PEARL algorithm to extract the correlation between contexts fully. We dynamically adjust the task priority during the FML training process to improve the sampling efficiency. Finally, we verify the performance of the algorithm through simulation and physical experiments. Experimental results show that our algorithm can reduce average cost and task overtime rate by 31% and 56% respectively compared with the benchmarks.},
  archive      = {J_TMC},
  author       = {Chunlin Li and Chaoyue Deng and Yong Zhang and Shaohua Wan},
  doi          = {10.1109/TMC.2025.3573278},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10978-10991},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated meta-learning based computation offloading approach with energy-delay tradeoffs in UAV-assisted VEC},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual dependency-aware collaborative service caching and task offloading in vehicular edge computing. <em>TMC</em>, <em>24</em>(10), 10963-10977. (<a href='https://doi.org/10.1109/TMC.2025.3573379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although some studies in recent years have focused on the coexistence of service and task dependencies in the collaborative optimization of service caching and task offloading in Vehicle Edge Computing (VEC), the challenges brought by dual dependencies have not been fully addressed. Therefore, this paper proposes a more comprehensive joint optimization method for service caching and task offloading under dual dependencies. First, this paper proposes a service criticality prediction method based on the Gated Graph Recurrent Network (GGRN) to perceive complex task dependencies and accurately capture the service requirements of critical task types. Based on this, a hierarchical active-passive hybrid caching strategy is designed, which aims to satisfy diverse service demands while reducing the additional overhead caused by remote service requests. Second, a global task priority computation method based on application heterogeneity has been developed to prevent cascading delays in task chains. Finally, this paper formulates a joint optimization problem for service caching and task offloading in a three-layer VEC system, models it as a markov decision process, and applies a proximal policy optimization-driven collaborative optimization algorithm named COHCTO. Simulation results show that COHCTO achieves multi-objective optimization across metrics such as delay, energy consumption, caching hit rate, and application success rate under conditions different from those of other algorithms.},
  archive      = {J_TMC},
  author       = {Liang Zhao and Lu Sun and Ammar Hawbani and Zhi Liu and Xiongyan Tang and Lexi Xu},
  doi          = {10.1109/TMC.2025.3573379},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10963-10977},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dual dependency-aware collaborative service caching and task offloading in vehicular edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource-efficient collaborative edge transformer inference with hybrid model parallelism. <em>TMC</em>, <em>24</em>(10), 10945-10962. (<a href='https://doi.org/10.1109/TMC.2025.3574695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based models have unlocked a plethora of powerful intelligent applications at the edge, such as voice assistant in smart home. Traditional deployment approaches offload the inference workloads to the remote cloud server, which would induce substantial pressure on the backbone network as well as raise users’ privacy concerns. To address that, in-situ inference has been recently recognized for edge intelligence, but it still confronts significant challenges stemming from the conflict between intensive workloads and limited on-device computing resources. In this paper, we leverage our observation that many edge environments usually comprise a rich set of accompanying trusted edge devices with idle resources and propose Galaxy+, a collaborative edge AI system that breaks the resource walls across heterogeneous edge devices for efficient Transformer inference acceleration. Galaxy+ introduces a novel hybrid model parallelism to orchestrate collaborative inference, along with a heterogeneity and memory-aware parallelism planning for fully exploiting the resource potential. To mitigate the impact of tensor synchronizations on inference latency under bandwidth-constrained edge environments, Galaxy+ devises a tile-based fine-grained overlapping of communication and computation. Furthermore, a fault-tolerant re-scheduling mechanism is developed to address device-level resource dynamics, ensuring stable and low-latency inference. Extensive evaluation based on prototype implementation demonstrates that Galaxy+ remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving a $1.2\times$ to $4.24\times$ end-to-end latency reduction. Besides, Galaxy+ can adapt to device-level resource dynamics, swiftly rescheduling and restoring inference in the presence of unexpected straggler devices.},
  archive      = {J_TMC},
  author       = {Shengyuan Ye and Bei Ouyang and Jiangsu Du and Liekang Zeng and Tianyi Qian and Wenzhong Ou and Xiaowen Chu and Deke Guo and Yutong Lu and Xu Chen},
  doi          = {10.1109/TMC.2025.3574695},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10945-10962},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resource-efficient collaborative edge transformer inference with hybrid model parallelism},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicular edge intelligence: DRL-based resource orchestration for task inference in vehicle-RSU-edge collaborative networks. <em>TMC</em>, <em>24</em>(10), 10927-10944. (<a href='https://doi.org/10.1109/TMC.2025.3572296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular edge intelligence, distinct from traditional edge intelligence, exhibits unique characteristics, including the mobility of vehicles, uneven spatial and temporal distribution of vehicles, and variability in the AI models deployed on vehicles, Roadside Units (RSUs), and edge servers (ESs). In this paper, we propose a Deep Reinforcement Learning (DRL)-based resource orchestration scheme for task inference in vehicle-RSU-edge collaborative networks. In our approach, vehicles’ inference tasks can be processed on the vehicles, RSUs, or ESs, encompassing a total of 9 possible scenarios based on the cross-RSU mobility of vehicles. The scheme jointly optimizes task processing decision-making, transmission power allocation, computational resource allocation, and transmission rate allocation. The objective is to minimize the total cost, which involves a trade-off between task processing latency, energy consumption and inference error rate across all vehicle tasks. We design a DRL algorithm that decomposes the original optimization problem into sub-problems and efficiently solves them by combining the Softmax Deep Double Deterministic Policy Gradients (SD3) algorithm with multiple numerical methods. We analyzed the complexity and convergence of the algorithm. Specifically, we demonstrated its low complexity and fast, stable convergence, which prove its effectiveness in solving the problem. And we demonstrate the superiority of our scheme by comparing it with 5 benchmark schemes across 6 different scenarios.},
  archive      = {J_TMC},
  author       = {Wenhao Fan and Yang Yu and Chenhui Bao and Yuan’an Liu},
  doi          = {10.1109/TMC.2025.3572296},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10927-10944},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Vehicular edge intelligence: DRL-based resource orchestration for task inference in vehicle-RSU-edge collaborative networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSIPose: Unveiling human poses using commodity WiFi devices through the wall. <em>TMC</em>, <em>24</em>(10), 10914-10926. (<a href='https://doi.org/10.1109/TMC.2025.3571469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of WiFi devices and the development of WiFi sensing have alerted people to the threat of WiFi sensing-based privacy leakage, especially the privacy of human poses. Existing work on human pose estimation is deployed in indoor scenarios or simple occlusion (e.g., a wooden screen) scenarios, which are less privacy-threatening in attack scenarios. To reveal the risk of leakage of the pose privacy to users from commodity WiFi devices, we propose CSIPose, a privacy-acquisition attack that passively estimates dynamic and static human poses in through-the-wall scenarios. We design a three-branch network based on transfer learning, auto-encoder, and self-attention mechanisms to realize the supervision of video frames over CSI frames to generate human pose skeleton frames. Notably, we design AveCSI, a unified framework for preprocessing and feature extraction of CSI data corresponding to dynamic and static poses. This framework uses the average of CSI measurements to generate CSI frames to mitigate the instability of passively collected CSI data, and utilizes a self-attention mechanism to enhance key features. We evaluate the performance of CSIPose across different room layouts, subjects, devices, subject locations, and device locations. Evaluation results emphasize the generalizability of CSIPose. Finally, we discuss measures to mitigate this attack.},
  archive      = {J_TMC},
  author       = {Yangyang Gu and Jing Chen and Congrui Chen and Kun He and Ju Jia and Yebo Feng and Ruiying Du and Cong Wu},
  doi          = {10.1109/TMC.2025.3571469},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10914-10926},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CSIPose: Unveiling human poses using commodity WiFi devices through the wall},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). You can wash hands better: Accurate daily handwashing assessment with a smartwatch. <em>TMC</em>, <em>24</em>(10), 10900-10913. (<a href='https://doi.org/10.1109/TMC.2025.3571805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand hygiene is among the most effective daily practices for preventing infectious diseases such as influenza, malaria, and skin infections. While professional guidelines emphasize proper handwashing to reduce the risk of viral infections, surveys reveal that adherence to these recommendations remains low. To address this gap, we propose UWash, a wearable solution leveraging smartwatches to evaluate handwashing procedures, aiming to raise awareness and cultivate high-quality handwashing habits. We frame the task of handwashing assessment as an action segmentation problem, similar to those in computer vision, and introduce a simple yet efficient two-stream UNet-like network to achieve this goal. Experiments involving 51 subjects demonstrate that UWash achieves 92.27% accuracy in handwashing gesture recognition, an error of $&lt; $0.5 seconds in onset/offset detection, and an error of $&lt; $5 points in gesture scoring under user-dependent settings. The system also performs robustly in user-independent and user-independent-location-independent evaluations. Remarkably, UWash maintains high performance in real-world tests, including evaluations with 10 random passersby at a hospital 9 months later and 10 passersby in an in-the-wild test conducted 2 years later. UWash is the first system to score handwashing quality based on gesture sequences, offering actionable guidance for improving daily hand hygiene.},
  archive      = {J_TMC},
  author       = {Fei Wang and Tingting Zhang and Xilei Wu and Pengcheng Wang and Xin Wang and Han Ding and Jingang Shi and Jinsong Han and Dong Huang},
  doi          = {10.1109/TMC.2025.3571805},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10900-10913},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {You can wash hands better: Accurate daily handwashing assessment with a smartwatch},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast robustness enhancement for dynamic IIoT topology with adaptive bayesian learning. <em>TMC</em>, <em>24</em>(10), 10886-10899. (<a href='https://doi.org/10.1109/TMC.2025.3571431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In resource-constrained and dynamic Industrial Internet of Things (IIoT) environments, ensuring robust and adaptable network topologies remains a significant challenge. Existing reinforcement learning-based approaches tackle topology optimization but face scalability issues due to high computational complexity and latency under strict time constraints. To address these challenges, we propose FRED-ABL (Fast Robustness Enhancement for Dynamic IIoT topology optimization with Adaptive Bayesian Learning), a novel paradigm that delivers lightweight topology solutions within a constrained time frame. FRED-ABL introduces an innovative topology structure compression method leveraging auxiliary continuous coding, enabling lossless representation of network structures as model inputs. It further defines a new robustness performance metric that integrates considerations of node failures and connection capabilities, serving as a comprehensive evaluation function. By developing an adaptive Bayesian learning model, FRED-ABL efficiently maps the relationship between topology structures and robustness metrics, enabling rapid optimization while significantly reducing computational overhead. Extensive experiments demonstrate that FRED-ABL consistently outperforms state-of-the-art methods, delivering superior robustness and optimization efficiency even in large-scale IIoT deployments.},
  archive      = {J_TMC},
  author       = {Ning Chen and Songwei Zhang and Xiaobo Zhou and Song Cao and Tie Qiu},
  doi          = {10.1109/TMC.2025.3571431},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10886-10899},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fast robustness enhancement for dynamic IIoT topology with adaptive bayesian learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STAGR: Simultaneous tracking and gait recognition with commodity wi-fi. <em>TMC</em>, <em>24</em>(10), 10868-10885. (<a href='https://doi.org/10.1109/TMC.2025.3570993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location-based services and identification hold promise for future smart home applications. Through them, we can provide customized services for specific users in current locations. Recent studies have demonstrated that Wi-Fi signals can be leveraged to achieve device-free tracking and gait recognition. Despite their good performance, these two technologies are not effectively integrated for the following reasons: First, the device-free tracking method might yield tracking results that conflict with human gait. Second, extracting gait features relies on knowing or accurately estimating the user’s trajectory. Consequently, gait recognition and tracking are inherently linked, but there has been no effective approach to integrate these two techniques. In this paper, we present STAGR, a system capable of Simultaneous Tracking And Gait Recognition. The main contribution of our technique is that we establish a theoretical model that reveals how to transform path-dependent spectra into path-independent spectra directly. Specifically, we conduct a preliminary study to demonstrate the need for simultaneous tracking and gait recognition. Second, we propose a novel method to extract path-independent gait features, which can significantly save execution time compared with the learning-based method. Third, we design a polar-coordinate filtering method to retain the gait features while correcting the trajectory. We implement a prototype STAGR system and conduct extensive experiments to verify the proposed mechanism. The experimental results show that we can realize simultaneous tracking and gait recognition. The median tracking error is $ 0.45\;{\rm{m}}$, while the recognition accuracy is 95.3% for 6 users.},
  archive      = {J_TMC},
  author       = {Xinyu Tong and Xiaoqiang Xu and Aiwen Yu and Xin Xie and Xiulong Liu and Wenyu Qu},
  doi          = {10.1109/TMC.2025.3570993},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10868-10885},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {STAGR: Simultaneous tracking and gait recognition with commodity wi-fi},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributionally robust optimization for aerial multi-access edge computing via cooperation of UAVs and HAPs. <em>TMC</em>, <em>24</em>(10), 10853-10867. (<a href='https://doi.org/10.1109/TMC.2025.3571023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With an extensive increment of computation demands, the aerial multi-access edge computing (MEC), mainly based on uncrewed aerial vehicles (UAVs) and high altitude platforms (HAPs), plays significant roles in future network scenarios. In detail, UAVs can be flexibly deployed, while HAPs are characterized with large capacity and stability. Hence, in this paper, we provide a hierarchical model composed of an HAP and multi-UAVs, to provide aerial MEC services. Moreover, considering the errors of channel state information from unpredictable environmental conditions, we formulate the problem to minimize the total energy cost with the chance constraint, which is a mixed-integer nonlinear problem with uncertain parameters and intractable to solve. To tackle this issue, we optimize the UAV deployment via the weighted K-means algorithm. Then, the chance constraint is reformulated via the distributionally robust optimization (DRO). Furthermore, based on the conditional value-at-risk mechanism, we transform the DRO problem into a mixed-integer second order cone programming, which is further decomposed into two subproblems via the primal decomposition. Moreover, to alleviate the complexity of the binary subproblem, we design a binary whale optimization algorithm. Finally, we conduct extensive simulations to verify the effectiveness and robustness of the proposed schemes by comparing with baseline mechanisms.},
  archive      = {J_TMC},
  author       = {Ziye Jia and Can Cui and Chao Dong and Qihui Wu and Zhuang Ling and Dusit Niyato and Zhu Han},
  doi          = {10.1109/TMC.2025.3571023},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10853-10867},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributionally robust optimization for aerial multi-access edge computing via cooperation of UAVs and HAPs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward generalized urban computing: Pretraining a spatial-temporal model for diverse urban tasks. <em>TMC</em>, <em>24</em>(10), 10840-10852. (<a href='https://doi.org/10.1109/TMC.2025.3573373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban computing leverages data analysis to improve urban areas’ efficiency and sustainability, tackling tasks like traffic management, crime forecasting, and air quality predictions. Current models, while efficient, often struggle with tasks beyond their initial training due to limited flexibility. Typically, new tasks require developing specialized models, which may not perform optimally with limited data. To overcome these challenges, we propose the development of a universal pretrained model that understands a city’s various aspects comprehensively. This model serves as a robust foundation, ready to be quickly adjusted for different urban tasks as they arise, even if they occur in different cities. Unlike language models, urban computing models must handle unique spatial-temporal dynamics, making standard pretraining techniques inadequate. Our approach includes a spatial-temporal module with multi-graph convolution and temporal attention mechanisms, capturing the necessary spatial-temporal patterns during pretraining. We also integrate a prompt-tuning module within this framework, which can be adapted for new predictive tasks. The results of extensive experiments on four urban predictive tasks across two cities demonstrate the effectiveness of our model.},
  archive      = {J_TMC},
  author       = {Yingqian Zhang and Chao Li and Shibo He and Xiangliang Zhang and Jiming Chen},
  doi          = {10.1109/TMC.2025.3573373},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10840-10852},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward generalized urban computing: Pretraining a spatial-temporal model for diverse urban tasks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-attribute consistency segment resilient routing for LEO satellite mega constellations. <em>TMC</em>, <em>24</em>(10), 10823-10839. (<a href='https://doi.org/10.1109/TMC.2025.3570670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low earth orbit (LEO) satellite mega constellations are regarded to provide pervasive intelligent services in the upcoming sixth generation network via the inter-satellite links (ISL). However, the inherent challenges of LEO satellites including limited onboard resources and failure-prone topology, create substantial hurdles for multi-attribute services routing in mega constellations. In this paper, we propose a multi-attribute consistency segment resilient (MCSR) routing algorithm, and a segmentation approach is designed to partition the mega constellation into non-intersecting segment routing domains (SRDs) through joint optimization of intra- and inter-SDRs update time, which leads to the potential of balancing network load and minimizing routing convergence time. Then, we utilize the multi-attribute consistency to determine the dominant paths of ISLs within and between SRDs for multi-attribute services. Furthermore, we develop a resilient rerouting strategy that utilizes the ephemeris to manage periodic ISL handovers, and selects a reserved/recalculated candidate path from the dominant paths for ISL random failures. Thus, our MCSR routing can converge to an optimal path for multi-attribute services from the dominant paths under ISL failures in mega constellations. Finally, we develop a testbed and simulation results validate the advantages of MCSR routing in handling multi-attribute services and rerouting capability in response to failures.},
  archive      = {J_TMC},
  author       = {Zhuang Du and Jian Jiao and Hao Liu and Ye Wang and Qinyu Zhang},
  doi          = {10.1109/TMC.2025.3570670},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10823-10839},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-attribute consistency segment resilient routing for LEO satellite mega constellations},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward integrated sensing and communication: Interference-resistance design for WiFi sensing. <em>TMC</em>, <em>24</em>(10), 10807-10822. (<a href='https://doi.org/10.1109/TMC.2025.3570752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi has been widely used for local area networking of devices and Internet access in the past two decades. Many researchers exploit WiFi signals for target sensing through analyzing the Channel State Information (CSI) of signals affected by the target movement. With the development of 6G Integrated Sensing and Communication (ISAC), some researchers further consider using communication data packets for WiFi sensing. However, all the current works do not analyze the impact of ubiquitous interference on WiFi sensing performance. In this paper, we propose IRSensing, an interference-resistance design to improve the CSI quality under interference in the ISAC scenario, aiming to improve the WiFi sensing performance. IRSensing exploits the overall WiFi packet for CSI optimization. It first measures the interference level of each subcarrier based on variance analysis, then proposes a CSI optimization method based on maximal ratio combining to improve the CSI quality. It finally proposes a practical CSI enhancement process to adapt to complex interference situations in actual networks. We implement IRSensing on a hardware testbed and evaluate its performance under different settings. Experiment results show that it can significantly decrease the activity detection error rate by up to 80% and improve the classification accuracy by up to 15$\%$.},
  archive      = {J_TMC},
  author       = {Junmei Yao and Chaoyang Liu and Sheng Luo and Lu Wang and Tingting Zhang and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3570752},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10807-10822},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward integrated sensing and communication: Interference-resistance design for WiFi sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AOA sensor placement for anchor-assisted target localization in GNSS-denied environment: Formulation, bounds and optimization. <em>TMC</em>, <em>24</em>(10), 10792-10806. (<a href='https://doi.org/10.1109/TMC.2025.3570768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Target localization technology is widely applied in various applications, such as rescue missions, robot navigation, and the Internet of Things. However, in some scenarios, the positions of sensors are unknown due to the load limitation of the sensor carriers and environmental interferences, resulting in the instability of the global navigation satellite system (GNSS). This paper focuses on optimal angle-of-arrival (AOA) sensor placement using multiple position-unknown sensors for target localization accuracy improvement. To guarantee the uniqueness of the target coordinate, at least two anchors are needed. The anchors are some static benchmark objects in the environment with priori known positions. Firstly, a new optimization problem for AOA target localization accuracy improvement incorporating position-unknown sensors and anchors is formulated. Secondly, the optimal theoretical localization accuracies of the unknown sensors and target are derived by minimizing the trace of the Cramér-Rao lower bounds (CRLBs). Thirdly, a mixture optimization method, including a geometrical initialization and the new proposed simultaneous perturbation stochastic approximation and adaptive momentum estimation (SPSA-Adam) algebraic algorithm, is developed. Then, the correctness of the new theoretical findings and the effectiveness of the proposed sensor placement optimization method are verified by simulation examples.},
  archive      = {J_TMC},
  author       = {Sheng Xu and Linlong Wu and Xianliang Li and Xinyu Wu and Tiantian Xu},
  doi          = {10.1109/TMC.2025.3570768},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10792-10806},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AOA sensor placement for anchor-assisted target localization in GNSS-denied environment: Formulation, bounds and optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ICGNN: Graph neural network enabled scalable beamforming for MISO interference channels. <em>TMC</em>, <em>24</em>(10), 10778-10791. (<a href='https://doi.org/10.1109/TMC.2025.3570648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the graph neural network (GNN)-enabled beamforming design for interference channels. We propose a model termed interference channel GNN (ICGNN) to solve a quality-of-service constrained energy efficiency maximization problem. The ICGNN is two-stage, where the direction and power parts of beamforming vectors are learned separately but trained jointly via unsupervised learning. By formulating the dimensionality of features independent of the transceiver pairs, the ICGNN is scalable with the number of transceiver pairs. Besides, to improve the performance of the ICGNN, the hybrid maximum ratio transmission and zero-forcing scheme reduces the output ports, the feature enhancement module unifies the two types of links into one type, the subgraph representation enhances the message passing efficiency, and the multi-head attention and residual connection facilitate the feature extracting. Furthermore, we present the over-the-air distributed implementation of the ICGNN. Ablation studies validate the effectiveness of key components in the ICGNN. Numerical results also demonstrate the capability of ICGNN in achieving near-optimal performance with an average inference time less than 0.1 ms. The scalability of ICGNN for unseen problem sizes is evaluated and enhanced by transfer learning with limited fine-tuning cost. The results of the centralized and distributed implementations of ICGNN are illustrated.},
  archive      = {J_TMC},
  author       = {Changpeng He and Yang Lu and Bo Ai and Octavia A. Dobre and Zhiguo Ding and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3570648},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10778-10791},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ICGNN: Graph neural network enabled scalable beamforming for MISO interference channels},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DI2SDiff++: Activity style decomposition and diffusion-based fusion for cross-person generalization in activity recognition. <em>TMC</em>, <em>24</em>(10), 10760-10777. (<a href='https://doi.org/10.1109/TMC.2025.3572220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing domain generalization (DG) methods for cross-person sensor-based activity recognition tasks often struggle to capture both intra- and inter-domain style diversity, leading to significant domain gaps with the target domain. In this study, we explore a novel perspective to tackle this problem, a process conceptualized as domain padding. This proposal aims to enrich the domain diversity by synthesizing intra- and inter-domain style data while maintaining robustness to class labels. We instantiate this concept using a conditional diffusion model and introduce a style-fused sampling strategy to enhance data generation diversity, termed Diversified Intra- and Inter-domain distributions via activity Style-fused Diffusion modeling (DI2SDiff). In contrast to traditional condition-guided sampling, our style-fused sampling strategy allows for the flexible use of one or more random style representations from the same class to guide data synthesis. This feature presents a notable advancement: it allows for the maximum utilization of possible combinations among existing styles to generate a broad spectrum of new style instances. We further extend DI2SDiff into DI2SDiff++ by enhancing the diversity of style guidance. Specifically, DI2SDiff++ integrates a multi-head style conditioner to provide multiple distinct, decomposed substyles and introduces a substyle-fused sampling strategy that allows cross-class substyle fusion for broader guidance. Empirical evaluations on a wide range of datasets demonstrate that our generated data achieves remarkable diversity within the domain space. Both intra- and inter-domain generated data have been proven significant and valuable, enabling DI2SDiff and DI2SDiff++ to surpass state-of-the-art DG methods in various cross-person activity recognition tasks.},
  archive      = {J_TMC},
  author       = {Junru Zhang and Cheng Peng and Zhidan Liu and Lang Feng and Yuhan Wu and Yabo Dong and Duanqing Xu},
  doi          = {10.1109/TMC.2025.3572220},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10760-10777},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DI2SDiff++: Activity style decomposition and diffusion-based fusion for cross-person generalization in activity recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I sense you fast: Simultaneous action and identity inference by slimming multi-branch RadarNet. <em>TMC</em>, <em>24</em>(10), 10743-10759. (<a href='https://doi.org/10.1109/TMC.2025.3570757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing connection between internet and human society, millimeter-wave radar based action recognition and user authentication exhibit remarkable prospects in security scenarios. Existing solutions usually focus on one of the tasks and mainly emphasize accuracy without reducing the inference time. In this paper, we propose a dual-task based Polymorphic Lightweight (PolyLite) RadarNet framework, in which the shared features are fed into two split streams for different tasks under joint supervision. The polymorphic concept here means that the trained network with parallel designs can be slimmed as a single-branch structure for inference. By this design strategy, we can not only efficiently extract spatial-temporal features during the training stage but also largely improve the response speed for simultaneously testing human activities and identities. Specifically, we design triple-view (TRIview) video-like data as the input by successively concatenating the range-velocity and range-angle matrices. Then a PolyLite module with linear and lightweight designs in each branch is integrated into our RadarNet framework to learn discriminative representations. Experimental results demonstrate that our approach is able to reach the accuracy over 98${\%}$ within 0.21 ms inference time. Especially, untrained intruders can also be successfully identified by a simple matching computation.},
  archive      = {J_TMC},
  author       = {Biyun Sheng and Yan Bao and Hui Cai and Linqing Gui and Fu Xiao},
  doi          = {10.1109/TMC.2025.3570757},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10743-10759},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {I sense you fast: Simultaneous action and identity inference by slimming multi-branch RadarNet},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning resilient to byzantine attacks and data heterogeneity. <em>TMC</em>, <em>24</em>(10), 10729-10742. (<a href='https://doi.org/10.1109/TMC.2025.3571058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses federated learning (FL) in the context of malicious Byzantine attacks and data heterogeneity. We introduce a novel Robust Average Gradient Algorithm (RAGA), which uses the geometric median for aggregation and allows flexible round number for local updates. Unlike most existing resilient approaches, which base their convergence analysis on strongly-convex loss functions or homogeneously distributed datasets, this work conducts convergence analysis for both strongly-convex and non-convex loss functions over heterogeneous datasets. The theoretical analysis indicates that as long as the fraction of the data from malicious users is less than half, RAGA can achieve convergence at a rate of $\mathcal {O}({1}/{T^{2/3- \delta }})$ for non-convex loss functions, where $T$ is the iteration number and $\delta \in (0, 2/3)$. For strongly-convex loss functions, the convergence rate is linear. Furthermore, the stationary point or global optimal solution is shown to be attainable as data heterogeneity diminishes. Experimental results validate the robustness of RAGA against Byzantine attacks and demonstrate its superior convergence performance compared to baselines under varying intensities of Byzantine attacks on heterogeneous datasets.},
  archive      = {J_TMC},
  author       = {Shiyuan Zuo and Xingrun Yan and Rongfei Fan and Han Hu and Hangguan Shan and Tony Q. S. Quek and Puning Zhao},
  doi          = {10.1109/TMC.2025.3571058},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10729-10742},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated learning resilient to byzantine attacks and data heterogeneity},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Satellite edge intelligence: DRL-based resource management for task inference in LEO-based satellite-ground collaborative networks. <em>TMC</em>, <em>24</em>(10), 10710-10728. (<a href='https://doi.org/10.1109/TMC.2025.3570799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distinguished from terrestrial edge intelligence, satellite edge intelligence has unique characteristics, including the rapid mobility of satellites, limitations in computing and energy resources, and differences in the artificial intelligence models deployed on user devices, satellites, and ground cloud servers. In this paper, we propose a Deep Reinforcement Learning (DRL)-based resource management scheme for task inference in Low Earth Orbit (LEO)-based satellite-ground collaborative networks. In our approach, the task of a user can be inferred by the user device itself, the edge server of the current satellite via user-to-satellite transmission, the edge server of a neighboring satellite via satellite-to-satellite transmission, or a ground cloud server via satellite-to-cloud transmission. Our scheme jointly optimizes task offloading, computing resource allocation, and communication resource allocation to minimize the total system cost, which encompasses trade-offs among the task inference delays for all tasks, the energy consumption of system, and the task inference accuracies for all tasks, while ensuring that the transmit power budgets of all satellites and the satellite coverage time constraints for each user are met. A DRL-based algorithm combining the Softmax Deep Double Deterministic Policy Gradients (SD3) algorithm and two numerical methods is designed to solve the optimization problem efficiently. We prove the convergence of our algorithm and demonstrate the superiority of our scheme by performing extensive simulations in 4 scenarios with 4 reference schemes.},
  archive      = {J_TMC},
  author       = {Wenhao Fan and Qingcheng Meng and Guan Wang and Hengwei Bian and Yabin Liu and Yuan’an Liu},
  doi          = {10.1109/TMC.2025.3570799},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10710-10728},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Satellite edge intelligence: DRL-based resource management for task inference in LEO-based satellite-ground collaborative networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age-energy analysis in multi-source systems with wake-up control and packet management. <em>TMC</em>, <em>24</em>(10), 10696-10709. (<a href='https://doi.org/10.1109/TMC.2025.3571419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been an increasing focus on real-time mobile applications, such as news updates and weather forecast. In these applications, data freshness is of significant importance, which can be measured by age-of-synchronization (AoS). At the same time, the reduction of carbon emission is increasingly required by the communication operators. Thus, how to reduce energy consumption while keeping the data fresh becomes a matter of concern. In this paper, we study the age-energy trade-off in a multi-source single-server system, where the server can turn to sleep mode to save energy. We adopt the stochastic hybrid system (SHS) method to analyze the average AoS and power consumption with three wake-up policies including N-policy, single-sleep policy and multi-sleep policy, and three packet preemption strategies, including Last-Come-First-Serve with preemption-in-Service (LCFS-S), LCFS with preemption-only-in-Waiting (LCFS-W), and LCFS with preemption-and-Queueing (LCFS-Q). The trade-off performance is analyzed via both closed-form expressions and numerical simulations. It is found that N-policy attains the best trade-off performance among all three sleep policies. Among packet management strategies, LCFS-S is suitable for scenarios with high requirements on energy saving and small arrival rate difference between sources. LCFS-Q is suitable for scenarios with high requirements on information freshness and large arrival rate difference between sources.},
  archive      = {J_TMC},
  author       = {Jie Gong and Jiajie Huang},
  doi          = {10.1109/TMC.2025.3571419},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10696-10709},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Age-energy analysis in multi-source systems with wake-up control and packet management},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication-efficient multi-server federated learning via over-the-air computation. <em>TMC</em>, <em>24</em>(10), 10683-10695. (<a href='https://doi.org/10.1109/TMC.2025.3573600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the Internet of Things (IoT), there has been explosive growth in edge devices, which generate a tremendous amount of data that holds invaluable potential. However, conventional data mining and machine learning (ML) paradigms require transmitting raw data to data centers for further use, which puts a heavy burden on communication networks and is exposed to high privacy risks. Federated learning allows for the training of ML models using distributed datasets, which can be applied to protect data privacy and alleviate transmission burdens. Meanwhile, the technique of over-the-air (OTA) computation can be utilized to exploit the superposition property of wireless communication channels. Motivated by this, in this paper, we propose a co-phase OTA approach for communication-efficient uploading in multi-server federated learning, which does not require expansion of the uplink channel bandwidth when the numbers of users and models increase. Besides, the digital OTA with randomized transmission is proposed to overcome the disadvantages of analog OTA, where the performance analyses of analog OTA and digital OTA are deduced, respectively. Simulation results show that a lower cost function can be obtained by digital OTA while requiring fewer iterations for convergence than that in analog OTA as more users can upload.},
  archive      = {J_TMC},
  author       = {Rui Han and Jiahao Ma and Lin Bai and Jinho Choi and Wei Zhang},
  doi          = {10.1109/TMC.2025.3573600},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10683-10695},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Communication-efficient multi-server federated learning via over-the-air computation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FDLoRa: Scaling downlink concurrent transmissions with full-duplex LoRa gateways. <em>TMC</em>, <em>24</em>(10), 10668-10682. (<a href='https://doi.org/10.1109/TMC.2025.3572130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike traditional data collection applications which primarily rely on uplink transmissions, emerging applications (e.g., device actuation, firmware update, packet reception acknowledgment) increasingly demand robust downlink transmission capabilities. Current LoRaWAN systems struggle to support these applications due to the inherent asymmetry between downlink and uplink capabilities. While uplink transmissions can handle multiple packets simultaneously, downlink transmissions are restricted to a single logical channel at a time, significantly limiting the deployment of applications that require substantial downlink capacity. To address this challenge, FDLoRa introduces an innovative in-band full-duplex LoRa gateway design, featuring novel solutions to mitigate self-interference (i.e., the strong downlink interference to ultra-weak uplink reception). This approach enables full-spectrum in-band downlink transmissions without compromising the reception of weak uplink packets. Building on the capabilities of full-duplex gateways, FDLoRa presents a new downlink framework that supports concurrent downlink transmissions across multiple logical channels of available gateways. Evaluation results show that FDLoRa enhances downlink capacity by 5.7× compared to LoRaWAN in a three-gateway testbed and achieves 2.58× higher downlink concurrency per gateway than the current leading solutions.},
  archive      = {J_TMC},
  author       = {Shiming Yu and Xianjin Xia and Ziyue Zhang and Ningning Hou and Yuanqing Zheng},
  doi          = {10.1109/TMC.2025.3572130},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10668-10682},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FDLoRa: Scaling downlink concurrent transmissions with full-duplex LoRa gateways},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative perception against data fabrication attacks in vehicular networks. <em>TMC</em>, <em>24</em>(10), 10654-10667. (<a href='https://doi.org/10.1109/TMC.2025.3571013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative perception in vehicular networks enables the connected autonomous vehicle (CAV) to gather sensing data, such as feature maps of light detection and ranging (LiDAR) point clouds, from neighboring CAVs to achieve higher perception accuracy, which has performance degradation against data fabrication attacks that share falsified sensing data with random probability. In this paper, we exploit the spatial consistency check to detect the potentially manipulated regions in LiDAR point clouds and measure the inconsistency degree of the received sensing data based on the number of conflict regions, which is the basis for determining the falsified sensing data if the inconsistency degree exceeds the threshold of the hypothesis test. The reinforcement learning (RL)-based collaborative vehicular perception scheme against data fabrication attacks is further proposed to choose CAVs based on the inconsistency degrees, the data quality measured by the confidence scores, the channel gains and the CAV reputations, which enhances the utility as the weighted sum of perception accuracy, speed and minimum latency requirement for data transmission. In addition, the multi-layer perceptron-based neural networks extract the perception features of sensing data from historical experiences, such as the data quality of received feature maps, as well as compress the RL state that linearly increases with the network scales and the spatial granularity of LiDAR point clouds for faster learning. Experimental results based on 10 CAVs equipped with LiDAR sensors and NVIDIA computational units to detect 20 vehicles against data fabrication attacks show that our proposed scheme outperforms the benchmarks in terms of perception accuracy and speed.},
  archive      = {J_TMC},
  author       = {Zhiping Lin and Liang Xiao and Hongyi Chen and Zefang Lv},
  doi          = {10.1109/TMC.2025.3571013},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10654-10667},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative perception against data fabrication attacks in vehicular networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving and autonomous-path access delegation for mobile cloud healthcare systems. <em>TMC</em>, <em>24</em>(10), 10640-10653. (<a href='https://doi.org/10.1109/TMC.2025.3570769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile cloud healthcare systems are gaining popularity due to their remote data collection through mobile devices and flexible data access through cloud services. The collected electronic health records (EHRs) are generally encrypted on mobile devices before being uploaded to the cloud, and accessed only by specific users. This incurs inconvenience when re-sharing EHRs with new receivers, especially in heterogeneous scenarios. Although cross-domain proxy re-encryption (CD-PRE) schemes have been studied to transform ciphertexts between different cryptosystems, they can neither support EHRs’ controlled multi-hop delegation between trusted delegatees chosen by the delegator nor prevent EHRs’ privacy inference through delegatees’ information. To this end, we present CAP-PRE for mobile cloud healthcare systems, which is a multi-hop CD-PRE scheme that supports lightweight encryption and re-encryption key generation on mobile devices, as well as privacy-preserving and autonomous-path access delegation. In CAP-PRE, the delegator creates a delegation path that includes preferred delegatees, and generates corresponding re-encryption keys which enables the cloud server to convert the collected ciphertext to an inner product ciphertext for privacy-preserving EHR re-sharing and pass the access rights along the delegation path for controlled multi-hop delegation. We finally prove the security of CAP-PRE and show the better performance of CAP-PRE with extensive experiments.},
  archive      = {J_TMC},
  author       = {Genghui Chi and Qinlong Huang and Caiqun Shi},
  doi          = {10.1109/TMC.2025.3570769},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10640-10653},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-preserving and autonomous-path access delegation for mobile cloud healthcare systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MobiRescue: Optimal dispatching of rescue teams under flooding disasters. <em>TMC</em>, <em>24</em>(10), 10622-10639. (<a href='https://doi.org/10.1109/TMC.2025.3569757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective dispatching of rescue teams under flooding disasters is crucial. However, previous methods are either incapable of handling flooding disaster situations, or cannot accurately estimate the distribution of rescue requests and accordingly adjust the search of the rescue teams. We propose MobiRescue, a human Mobility based Rescue team dispatching system, which aims to maximize the total number of rescued people, minimize the rescue delay and the number of serving rescue teams. We studied a city-scale human mobility dataset collected under the Hurricane Florence, and observed that several natural and demographic factors are closely related to impact severity, and road segment passability must be considered. Accordingly, we first propose a Support Vector Machine based method to predict the distribution of rescue requests considering the disaster-related factors. Then, we design an Euler path based method to determine the search paths for rescue team dispatching. Subsequently, we develop a Reinforcement Learning based method to guide the search of the rescue teams. Finally, we design a multi-objective optimization problem based method to adapt to the changed road segment passability. Our experiments demonstrate that compared with the other methods, MobiRescue increases the total number of timely served rescue requests by 43.4% in average.},
  archive      = {J_TMC},
  author       = {Li Yan and Bin Yang and Haiying Shen and Shohaib Mahmud and Natasha Zhang Foutz and Joshua Anton},
  doi          = {10.1109/TMC.2025.3569757},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10622-10639},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MobiRescue: Optimal dispatching of rescue teams under flooding disasters},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compression meets security: Low-complexity linear collaborative federated learning with enhanced accuracy. <em>TMC</em>, <em>24</em>(10), 10606-10621. (<a href='https://doi.org/10.1109/TMC.2025.3569669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has been regarded as a promising paradigm for enabling distributed model training over resource-limited edge devices. Although FL maintains data locality and enhances model generalization, it faces challenges such as model leakage and pressure from frequent model updates. Some existing schemes, such as differential privacy and model encryption, can partially alleviate these issues while sacrificing the accuracy of the modeling training or increasing the computational overheads in training. To address this issue, we design a low-complexity linear collaborative FL (LCFL) framework to enhance the privacy and accuracy of FL. Specifically, we propose the collaborative secrecy transmission (CST) algorithm by integrating a variant of Shamir’s secret-sharing with the model segmentation, which can compresses and encrypts the local models for FL. The decoding complexity of the CST algorithm is only $O(N^{3})$ under the compression ratio of $N$, which reduces the communication overhead and computational complexity. We conduct a quantitative analysis of the model error induced by the CST algorithm and derive its closed-form upper bound. Within LCFL, we formulate an optimization problem to maximize the global model accuracy in wireless FL by optimizing compression ratios, bandwidth allocation, and transmit-powers. Subsequently, we propose a low-complexity algorithm to solve this problem effectively. Numerical simulations demonstrate the efficacy of LCFL in improving FL’s accuracy and security, and the results validate the efficiency of the proposed optimization scheme for wireless FL.},
  archive      = {J_TMC},
  author       = {Tianshun Wang and Peichun Li and Panpan Feng and Xin Wei and Liping Qian and Yuan Wu},
  doi          = {10.1109/TMC.2025.3569669},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10606-10621},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Compression meets security: Low-complexity linear collaborative federated learning with enhanced accuracy},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PACIFISTA: Conflict evaluation and management in open RAN. <em>TMC</em>, <em>24</em>(10), 10590-10605. (<a href='https://doi.org/10.1109/TMC.2025.3570632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The O-RAN ALLIANCE is defining architectures, interfaces, operations, and security requirements for cellular networks based on Open Radio Access Network (RAN) principles. In this context, O-RAN introduced the RAN Intelligent Controllers (RICs) to enable dynamic control of cellular networks via data-driven applications referred to as rApps and xApps. RICs enable for the first time truly intelligent and self-organizing cellular networks. However, enabling the execution of many Artificial Intelligence (AI) algorithms making autonomous control decisions to fulfill diverse (and possibly conflicting) goals poses unprecedented challenges. For instance, the execution of one xApp aiming at maximizing throughput and one aiming at minimizing energy consumption would inevitably result in diametrically opposed resource allocation strategies. Therefore, conflict management becomes a crucial component of any functional intelligent O-RAN system. This article studies the problem of conflict mitigation in O-RAN and proposes PACIFISTA, a framework to detect, characterize, and mitigate conflicts generated by O-RAN applications that control RAN parameters. PACIFISTA leverages a profiling pipeline to tests O-RAN applications in a sandbox environment, and combines hierarchical graphs with statistical models to detect the existence of conflicts and evaluate their severity. Experiments on Colosseum and OpenRAN Gym demonstrate PACIFISTA’s ability to predict conflicts and provide valuable information before potentially conflicting xApps are deployed in production systems. We use PACIFISTA to demonstrate that users can experience a 16% throughput loss even in the case of xApps with similar goals, and that applications with conflicting goals might cause severe instability and result in up to 30% performance degradation. We also show that PACIFISTA can help operators to identify conflicting applications and maintain performance degradation below a tolerable threshold.},
  archive      = {J_TMC},
  author       = {Pietro Brach del Prever and Salvatore D’Oro and Leonardo Bonati and Michele Polese and Maria Tsampazi and Heiko Lehmann and Tommaso Melodia},
  doi          = {10.1109/TMC.2025.3570632},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10590-10605},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PACIFISTA: Conflict evaluation and management in open RAN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaShift: Anti-collapse and real-time deep model evolution for mobile vision applications. <em>TMC</em>, <em>24</em>(10), 10573-10589. (<a href='https://doi.org/10.1109/TMC.2025.3572215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As computational hardware advance, integrating deep learning (DL) models into mobile devices has become ubiquitous for visual tasks. However, “data distribution shift” in live sensory data can lead to a degradation in the accuracy of mobile DL models. Conventional domain adaptation methods, constrained by their dependence on pre-compiled static datasets for offline adaptation, exhibit fundamental limitations in real-time practicality. While modern online adaptation methodologies enable incremental model evolution, they remain plagued by two critical shortcomings: computational latency from excessive resource demands on mobile devices that compromise temporal responsiveness, and accuracy collapse stemming from error accumulation through unreliable pseudo-labeling processes. To address these challenges, we introduce AdaShift, an innovative cloud-assisted framework enabling real-time online model adaptation for vision-based mobile systems operating under non-stationary data distributions. Specifically, to ensure real-time performance, the adaptation trigger and plug-and-play adaptation mechanisms are proposed to minimize redundant adaptation requests and reduce per-request costs. To prevent accuracy collapse, AdaShift introduces a novel anti-collapse parameter restoration mechanism that explicitly recovers knowledge, ensuring stable accuracy improvements during model evolution. Through extensive experiments across various vision tasks and model architectures, AdaShift demonstrates superior accuracy and 100ms-level adaptation latency, achieving an optimal balance between accuracy and real-time performance compared to baselines.},
  archive      = {J_TMC},
  author       = {Ke Ma and Bin Guo and Sicong Liu and Cheng Fang and Siqi Luo and Zimu Zheng and Zhiwen Yu},
  doi          = {10.1109/TMC.2025.3572215},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10573-10589},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AdaShift: Anti-collapse and real-time deep model evolution for mobile vision applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-variate time series prediction of traffic and users for dynamic RRH-BBU mapping in C-RAN. <em>TMC</em>, <em>24</em>(10), 10557-10572. (<a href='https://doi.org/10.1109/TMC.2025.3570851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cellular operators face significant challenges in cutting operating expenses while maintaining the quality of service (QoS) for users due to growing network traffic and dynamic user connections. These challenges are addressed by the cloud radio access network (C-RAN) architecture, which includes a centralized pool of baseband units (BBUs) and distributes them from remote radio heads (RRHs). The key to improving C-RAN performance is to dynamically allocate large-scale RRHs to different BBUs in real time. In this paper, we propose a user behavior-aware RRH-BBU mapping framework to improve the performance of large-scale C-RANs by predicting RRH traffic and users in advance. First, we propose a Multivariate RRH time series Prediction Model (MRPM) that captures the spatio-temporal patterns in the data to predict the traffic volume and the number of users of RRHs, which represent key indicators of RRH connection states. Second, we formulate the RRH-BBU mapping as a Markov decision process problem to optimize cost and QoS by considering BBU utilization, BBU energy consumption, RRH migration frequency, and BBU load balancing. Third, we propose a prediction-based RRH-BBU mapping scheme (PB-RBM) to find the optimal RRH-BBU mapping strategy by leveraging the prediction information of MRPM. In the PB-RBM algorithm, we employ an A3C algorithm to learn the mapping policy and group the RRHs based on a defined popularity metric to reduce the state and action space of the reinforcement learning algorithm. Finally, extensive experiments are conducted on a real-world dataset, and our algorithm is compared with several matching algorithms, such as ACKTR, heuristic, etc., to demonstrate its superiority, especially reducing 17.5% in RMSE compared to the best-performing baseline.},
  archive      = {J_TMC},
  author       = {Fan Wu and Shanshan Wang and Jieyu Zhou and Haoye Pan and Conghao Zhou and Wang Yang and Feng Lyu and Yaoxue Zhang},
  doi          = {10.1109/TMC.2025.3570851},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10557-10572},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-variate time series prediction of traffic and users for dynamic RRH-BBU mapping in C-RAN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mimir: Data-free federated unlearning through client-specific prompt generation for personalized models. <em>TMC</em>, <em>24</em>(10), 10537-10556. (<a href='https://doi.org/10.1109/TMC.2025.3570018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated unlearning (FU) has become an important area of research due to an increasing need for federated learning (FL) applications to comply with emerging data privacy regulations such as GDPR. It facilitates the removal of certain clients’ data from an already trained FL model while preserving the performance on the remaining client without the need to retrain from scratch. Existing FU methods typically require clients to have access to their training data or historical model updates, which may be impractical in real-world scenarios due to privacy constraints and changes in data availability. Moreover, FU methods may cause catastrophic unlearning, where removing a client’s data from heterogeneous, non-IID settings can negatively impact the model’s performance on data from retained clients. To address the aforementioned issues and leverage the capabilities of personalized federated learning (pFL) in handling non-IID data distributions, this paper introduce Mimir, a novel data-free federated unlearning framework designed for pFL settings. Mimir integrates both learning and unlearning phases by utilizing personalized prompts for each client. We design a distillation structure based on Generative Adversarial Networks (GANs) for client-level unlearning that does not require access to original data or historical updates. By leveraging client-specific prompts generated during the pFL phase, Mimir adapts to heterogeneous data distributions and mitigates catastrophic unlearning on the retained data. We demonstrate the effectiveness of Mimir through extensive experiments on benchmark datasets, showing its ability to forget target client data while preserving model accuracy on the remaining clients.},
  archive      = {J_TMC},
  author       = {Wenhan Wu and Huanghuang Liang and Tianyu Tu and Jiawei Jiang and Chuang Hu and Dazhao Cheng},
  doi          = {10.1109/TMC.2025.3570018},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10537-10556},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mimir: Data-free federated unlearning through client-specific prompt generation for personalized models},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedEx: Expediting federated learning over heterogeneous mobile devices by overlapping and participant selection. <em>TMC</em>, <em>24</em>(10), 10523-10536. (<a href='https://doi.org/10.1109/TMC.2025.3572516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training latency is critical for the success of numerous intrigued applications ignited by federated learning (FL) over heterogeneous mobile devices. By revolutionarily overlapping local gradient transmission with continuous local computing, FL can remarkably reduce its training latency over homogeneous clients, yet encounter severe model staleness, model drifts, memory cost and straggler issues in heterogeneous environments. To unleash the full potential of overlapping, we propose, FedEx, a novel federated learning approach to expedite FL training over mobile devices under data, computing and wireless heterogeneity. FedEx redefines the overlapping procedure with staleness ceilings to constrain memory consumption and make overlapping compatible with participation selection (PS) designs. Then, FedEx characterizes the PS utility function by considering the latency reduced by overlapping, and provides a holistic PS solution to address the straggler issue. FedEx also introduces a simple but effective metric to trigger overlapping, in order to avoid model drifts. Experimental results show that compared with its peer designs, FedEx demonstrates substantial reductions in FL training latency over heterogeneous mobile devices with limited memory cost.},
  archive      = {J_TMC},
  author       = {Jiaxiang Geng and Boyu Li and Xiaoqi Qin and Yixuan Li and Liang Li and Yanzhao Hou and Miao Pan},
  doi          = {10.1109/TMC.2025.3572516},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10523-10536},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedEx: Expediting federated learning over heterogeneous mobile devices by overlapping and participant selection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated communication and computation resource allocation for the compressive sensing based image transmission. <em>TMC</em>, <em>24</em>(10), 10510-10522. (<a href='https://doi.org/10.1109/TMC.2025.3570447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The data compression based transmission has been envisioned as a promising solution to improve the data transmission efficiency with the limited radio resources in the future sixth-generation (6G) wireless networks. In this paper, we propose an integrated communication and computation resource allocation system for image transmission based on compressive sensing (CS), which consists of several camera devices and a base station (BS). The device side first compresses the images, after which the compressed images are transmitted using non-orthogonal multiple access (NOMA) transmission, and finally the BS restores the received compressed images. Due to the limited energy supply, the total system energy consumption is minimized by jointly optimizing the image sampling rate, the image data transmission power, the number of floating point operations per second (FLOPS), the time of image compression and the time of data transmission under the constraints of latency and the peak signal-to-noise ratio (PSNR). Due to the non-convexity of the proposed problem, after a series of equal substitutions we convexify the problem. Then, the Karush–Kuhn–Tucker (KKT) condition and the gradient descent method are used to obtain the optimal solution of the target problem. After simulation experiments, it is concluded that the proposed CS-based image transmission scheme effectively reduces the total energy consumption by a factor of 2.7 compared with frequency division multiple access (FDMA), and the total latency by 180% compared with the original image transmission.},
  archive      = {J_TMC},
  author       = {Qianru Wang and Li Ping Qian and Wei Jiang and Yuan Wu and Xiaoniu Yang},
  doi          = {10.1109/TMC.2025.3570447},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10510-10522},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Integrated communication and computation resource allocation for the compressive sensing based image transmission},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A practical DoS attack on commercial UWB ranging systems. <em>TMC</em>, <em>24</em>(10), 10492-10509. (<a href='https://doi.org/10.1109/TMC.2025.3569972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-wideband (UWB) ranging systems are increasingly deployed in critical, security-sensitive applications due to their precise positioning and secure ranging capabilities. In this work, we introduce a practical DoS attack via reactive jamming, referred to as UWBAD+, which targets commercial UWB ranging systems by exploiting the vulnerabilities of the normalized cross-correlation process. This allows UWBAD+ to selectively and effectively disrupt ranging sessions without requiring prior knowledge of the victim devices’ configurations, leading to potentially severe consequences, such as property loss, unauthorized access, or vehicle theft. The enhanced effectiveness and low detectability of UWBAD+ stem from the following: (i) it can rapidly sniff the physical layer structures of unknown UWB systems, even in the presence of multiple UWB devices operating simultaneously; (ii) it blocks each ranging session efficiently by employing field-level jamming, thus exerting a significant impact on commercial UWB ranging systems; and (iii) its compact, reactive, and selective design based on COTS UWB chips, which makes it both affordable and less noticeable. We successfully executed real-world attacks on commercial UWB ranging systems produced by the three largest UWB chip vendors in the market, including Apple, NXP, and Qorvo. We disclosed our findings to Apple, relevant Original Equipment Manufacturers (OEMs), and the Automotive Security Research Group. As of the time of writing, the involved OEM has acknowledged this vulnerability in their automotive systems and has issued a ${\$} 5,000$ bounty as a reward.},
  archive      = {J_TMC},
  author       = {Yongzhao Zhang and Yuqiao Yang and Zhiwei Chen and Zhongjie Wu and Ting Chen and Jun Li and Jie Yang and Guowen Xu and Wenhao Liu and Xiaosong Zhang and Jingwei Li and Yu Jiang and Zhuo Su},
  doi          = {10.1109/TMC.2025.3569972},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10492-10509},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A practical DoS attack on commercial UWB ranging systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-based edge computing service with dynamic entry and exit mechanism. <em>TMC</em>, <em>24</em>(10), 10474-10491. (<a href='https://doi.org/10.1109/TMC.2025.3570646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of 5G and artificial intelligence (AI) technology, the Internet of Things (IoT) has been expanding and integrated into various aspects of our daily lives. However, this also poses challenges such as the ubiquitous demand for communication and computing resources, and data privacy issues. Considering its flexible deployment, high security, and ease of scalability, blockchain-enabled edge computing IoT network (BECIN) has become a promising solution to provide secure and fast communication and computing services. However, existing research on computation offloading in edge computing largely overlooks the stochastic arrival of computational tasks and the potential variability in the number, locations, and resource provisions of edge computing service providers. Therefore, we propose a dynamic, self-adjusting BECIN framework aimed at providing long-term stable, efficient, and secure edge computing data offloading services for ground users in a specific region. This framework supports the dynamic entry and exit of edge computing service providers. Additionally, we introduce a novel dynamic Dueling DDQN approach to update the offloading and resource management policies based on changes in resource provisioning. Experimental results demonstrate the feasibility and superior performance of our framework on system cost and system latency.},
  archive      = {J_TMC},
  author       = {Qiang He and Zheng Feng and Hui Fang and Xingwei Wang and Liang Zhao and Keping Yu and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TMC.2025.3570646},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10474-10491},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-based edge computing service with dynamic entry and exit mechanism},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and optimization of heterogeneous coded distributed computing with nonuniform file popularity. <em>TMC</em>, <em>24</em>(10), 10456-10473. (<a href='https://doi.org/10.1109/TMC.2025.3570907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies MapReduce-based heterogeneous coded distributed computing (CDC) where, besides different computing capabilities at workers, input files to be accessed by computing jobs have nonuniform popularity. We propose a file placement strategy that can handle an arbitrary number of input files. Furthermore, we design a nested coded shuffling strategy that can efficiently manage the nonuniformity of file popularity to maximize the coded multicasting opportunity. We then formulate the joint optimization of the proposed file placement and nested shuffling design variables to optimize the proposed CDC scheme. To reduce the high computational complexity in solving the resulting mixed-integer linear programming (MILP) problem, we propose a simple two-file-group-based file placement approach to obtain an approximate solution. Both numerical studies and experimental tests show that the optimized CDC scheme outperforms other alternatives. Also, the proposed two-file-group-based approach achieves nearly the same performance as the conventional branch-and-cut method in solving the MILP problem but with substantially lower computational complexity that is scalable over the number of files and workers. For computing jobs with aggregate target functions that commonly appear in machine learning applications, we propose a heterogeneous compressed CDC (C-CDC) scheme to further improve the shuffling efficiency. The C-CDC scheme uses a local data aggregation technique to compress the data to be shuffled for the shuffling load reduction. We again optimize the proposed C-CDC scheme and explore the two-file-group-based low-complexity approach to find an approximate solution. Numerical studies show that the proposed C-CDC scheme provides a considerable shuffling load reduction over the CDC scheme, and the two-file-group-based file placement approach maintains good performance.},
  archive      = {J_TMC},
  author       = {Yong Deng and Min Dong},
  doi          = {10.1109/TMC.2025.3570907},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10456-10473},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Design and optimization of heterogeneous coded distributed computing with nonuniform file popularity},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing mobile-friendly viewport prediction for live 360-degree video streaming. <em>TMC</em>, <em>24</em>(10), 10441-10455. (<a href='https://doi.org/10.1109/TMC.2025.3571186'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Viewport prediction is the crucial task for adaptive 360-degree video streaming, as the bitrate control algorithms usually require the knowledge of the user's viewing portions of the frames. Various methods are studied and adopted for viewport prediction from less accurate statistic tools to highly calibrated deep neural networks. Conventionally, it is difficult to implement sophisticated deep learning methods on mobile devices, which have limited computation capability. In this work, we propose an advanced learning-based viewport prediction approach and carefully design it to minimize transmission and computation overhead for mobile terminals. To improve viewport prediction accuracy, we utilize both spatial information through a saliency prediction model and temporal information through a modified LSTM model. Different computations introduced by the neural network models are distributed across the network to keep the computation light on mobile devices. To better adapt to the content dynamics in live streaming, we employ the model-agnostic meta-learning (MAML) method for video saliency prediction. The learned saliency prediction model with optimized initialization via offline meta-training can be fast fine-tuned online using a few samples. We further discuss how to integrate this mobile-friendly viewport prediction (MFVP) approach into a typical 360-degree video live streaming system by formulating and solving the bitrate adaptation problem. Extensive experiment results demonstrate that our approach achieves real-time prediction for live video streaming and surpasses existing methods in prediction accuracy on mobile terminals, which, together with our bitrate adaptation algorithm, significantly improves the streaming QoE from various aspects. Compared to baseline methods, MFVP achieves a 4.7–28.7% improvement in accuracy and demonstrates faster adaptability to dynamic content changes, enabling rapid fine-tuning and adjustment. When integrated into a streaming system and paired with our adaptive bitrate allocation algorithm, MFVP enhances overall video quality by 5.6–12.9% and reduces quality fluctuations by 33.3–50.9%.},
  archive      = {J_TMC},
  author       = {Lei Zhang and Peng Chen and Cong Zhang and Cheng Pan and Tao Long and Weizhen Xu and Laizhong Cui and Jiangchuan Liu},
  doi          = {10.1109/TMC.2025.3571186},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10441-10455},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing mobile-friendly viewport prediction for live 360-degree video streaming},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical learning and computing over space-ground integrated networks. <em>TMC</em>, <em>24</em>(10), 10423-10440. (<a href='https://doi.org/10.1109/TMC.2025.3569887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-ground integrated networks hold great promise for providing global connectivity, particularly in remote areas where large amounts of valuable data are generated by Internet of Things (IoT) devices, but lacking terrestrial communication infrastructure. The massive data is conventionally transferred to the cloud server for centralized artificial intelligence (AI) models training, raising huge communication overhead and privacy concerns. To address this, we propose a hierarchical learning and computing framework, which leverages the low-latency characteristic of low-earth-orbit (LEO) satellites and the global coverage of geostationary-earth-orbit (GEO) satellites, to provide global aggregation services for locally trained models on ground IoT devices. Due to the time-varying nature of satellite network topology and the energy constraints of LEO satellites, efficiently aggregating the received local models from ground devices on LEO satellites is highly challenging. By leveraging the predictability of inter-satellite connectivity, modeling the space network as a directed graph, we formulate a network energy minimization problem for model aggregation, which turns out to be a Directed Steiner Tree (DST) problem. We propose a topology-aware energy-efficient routing (TAEER) algorithm to solve the DST problem by finding a minimum spanning arborescence on a substitute directed graph. Extensive simulations under real-world space-ground integrated network settings demonstrate that the proposed TAEER algorithm significantly reduces energy consumption and outperforms benchmarks.},
  archive      = {J_TMC},
  author       = {Jingyang Zhu and Yuanming Shi and Yong Zhou and Chunxiao Jiang and Linling Kuang},
  doi          = {10.1109/TMC.2025.3569887},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10423-10440},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hierarchical learning and computing over space-ground integrated networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XHGA: Expanding the capabilities of cross-modal wrist-worn devices for multi-task hand gesture applications. <em>TMC</em>, <em>24</em>(10), 10405-10422. (<a href='https://doi.org/10.1109/TMC.2025.3569841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture applications (HGA) are essential for human-machine interaction. Although the existing solutions achieve good performance in specific tasks, they still face challenges when users navigate through different application contexts, i.e., requiring multi-task ability to support newly arrived HGA tasks. In this paper, we propose a novel wrist-worn multi-task HGA system named XHGA, which can implement modal-domain combination, data-domain adaptation and label-domain extension to ensure the performance in multi-task scenarios. The system introduces a novel two-stage training strategy, i.e., task-agnostic stage to align cross-modal features from unlabeled arbitrary gestures through contrastive learning, and task-related stage to learn modality contributions with limited labeled data in specific tasks through self-attention mechanism, while achieves multi-objective recognition simultaneously by employing an adaptive loss function weighting method. Extensive experiments demonstrate that XHGA can achieve an average accuracy of 92.7% with only using 15 labeled data per gesture under three HGA tasks. Compared with the state-of-the-art multi-modal approach, XHGA reduces 82.7% training time, and 47.7% storage, with about 5% improvements in accuracy.},
  archive      = {J_TMC},
  author       = {Kaiwen Guo and Hui Tang and Tianyi Xu and Hao Zhou and Mengxia Lyu and Zhi Liu and Xiaoyan Wang and Xiang-Yang Li},
  doi          = {10.1109/TMC.2025.3569841},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10405-10422},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {XHGA: Expanding the capabilities of cross-modal wrist-worn devices for multi-task hand gesture applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRL-based pricing-driven for task offloading and dynamic resource in vehicle edge computing. <em>TMC</em>, <em>24</em>(10), 10389-10404. (<a href='https://doi.org/10.1109/TMC.2025.3569817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle Edge Computing (VEC) assists vehicles in performing latency-sensitive tasks by deploying resources near the vehicle. Designing an incentive mechanism for vehicles and VEC is crucial for realizing an intelligent transmission system. Considering the rationality of resource allocation, we model the utility functions of the VEC and the vehicle, which are used as optimization objectives. Specifically, the VEC allocates resources through pricing to maximize revenue under resource-constrained conditions, and the vehicle weighs payments against energy consumption to determine offloading and resource allocation. Given the vehicle movement and the variable channel state, we use the Deep Reinforcement Learning (DRL) algorithm to solve these optimization problems. To reduce the learning difficulty of the DRL algorithm in complex VEC scenarios with multiple optimization variables, we propose a Pricing-Driven Resource Allocation (PDRA) algorithm that performs mobility-aware task offloading and calculates the optimal values of the optimization variables in the utility function of the vehicle to reduce the decision dimension. Furthermore, we also propose a DRL-based Pricing-Driven Dynamic Resource Allocation (DPDDRA) algorithm to achieve efficient resource allocation. Extensive experimental results show that the proposed algorithms can reduce the learning difficulty while maximizing VEC and vehicle revenue in complex VEC scenarios.},
  archive      = {J_TMC},
  author       = {Sijun Wu and Liang Yang and Junjie Li and Hongzhi Guo and Ishtiaq Ahmad and Daniel Benevides Da Costa and Hongbo Jiang and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3569817},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10389-10404},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DRL-based pricing-driven for task offloading and dynamic resource in vehicle edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Maximizing service provider’s profit in multi-UAV 5G network via deep reinforcement learning and graph coloring. <em>TMC</em>, <em>24</em>(10), 10377-10388. (<a href='https://doi.org/10.1109/TMC.2025.3571804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current 5G network is expected to have a densely populated architecture comprising radio-enabled Service Provider (SP) and heterogeneous User Equipment (UE). Addressing the real-time service demands of UEs with strict deadlines is a critical challenge. Uncrewed Aerial Vehicle (UAV) assisted service provisioning is emerging as an efficient solution for timely service transfers. Therefore, SPs are interested in offering UAV-assisted service transmission to get profited by deploying UAVs. However, this introduces challenges like optimizing the locations of UAVs and Power Level (PL) along with interference management within limited available radio resources. Hence, we proposed a novel framework for multi-UAV-assisted service provisioning, consisting of Base Station (BS), UAVs, and heterogeneous UEs in 5G network. We formulate the SP’s profit maximization problem, optimizing UAVs’ location, PL, and resource allocation while considering service latency, interference management, and UAVs’ energy constraints collectively as an optimization problem. Furthermore, we propose a semi-centralized sub-optimal solution utilizing Multi-agent Deep Reinforcement Learning (MaDRL) and a Graph Coloring-based approach. Extensive simulation analysis demonstrates the proposed algorithm’s effectiveness, achieving an average of 99.05% profit compared to the optimal value.},
  archive      = {J_TMC},
  author       = {Shilpi Kumari and Ajay Pratap},
  doi          = {10.1109/TMC.2025.3571804},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10377-10388},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Maximizing service provider’s profit in multi-UAV 5G network via deep reinforcement learning and graph coloring},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Game-theoretic incentive mechanism for blockchain-based federated learning. <em>TMC</em>, <em>24</em>(10), 10363-10376. (<a href='https://doi.org/10.1109/TMC.2025.3567355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-based federated learning (BFL) has gained attention for its potential to establish decentralized trust. While existing research primarily focuses on personalized frameworks for various applications, essential aspects including incentive mechanisms—critical for ensuring stable system operation—remain under-explored. To bridge this gap, we propose a game-theoretic incentive mechanism designed to foster active participation in BFL tasks. Specifically, we model a BFL system comprising a model owner (MO), i.e., task publisher, multiple miners, and training terminals, framing their interactions through two-tier Stackelberg games. In the first-tier game, the MO designs reward strategies to incentivize training terminals to contribute more data, enhancing model accuracy. The second-tier game introduces a multi-leader multi-follower Stackelberg game, enabling miners to set model packaging prices based on competitors’ strategies and anticipated user behavior. By deriving the Stackelberg equilibrium, we identify optimal strategies for all participants, leading to an incentive mechanism balancing individual interests with overall performance. Compared to its benchmarks, our incentive mechanism offers 5.8% and 53.4% higher utilities in the two games compared to its alternatives, accelerating convergence and improving accuracy.},
  archive      = {J_TMC},
  author       = {Wenzheng Tang and Erwu Liu and Wei Ni and Xinyu Qu and Butian Huang and Kezhi Li and Dusit Niyato and Abbas Jamalipour},
  doi          = {10.1109/TMC.2025.3567355},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10363-10376},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Game-theoretic incentive mechanism for blockchain-based federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint partitioning, allocation, and transmission optimization for federated learning in satellite constellations via multi-task MARL. <em>TMC</em>, <em>24</em>(10), 10345-10362. (<a href='https://doi.org/10.1109/TMC.2025.3568470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orbital edge computing (OEC) is crucial for supporting space intelligence applications within satellite networks. However, individual satellites face resource constraints, and implementing distributed processing techniques, such as federated learning (FL), across multiple satellites introduces significant scheduling complexity. To address these challenges, we first model the key factors influencing complex satellite networks, including satellite constellations, regional resource demands, inter-satellite communication and routing, energy consumption, and battery aging—a novel aspect invoked by OEC operations. We propose an adaptive aggregation method to fundamentally improve communication efficiency in OEC-based FL. To enhance scheduling performance, we formulate a unified optimization problem that jointly considers data partitioning, resource allocation, and aggregation transmission tasks within a decentralized partially observable Markov decision process (Dec-POMDP) framework. Furthermore, we introduce an episodic-phase-recalling reward shaping (EPRS) method to correlate the influences across these phases. Inspired by multi-task learning, we propose an efficient multi-agent reinforcement learning (MARL) algorithm featuring a multi-head actor-critic (MH-AC) network structure and task-equalized adaptation (TEA) technology, designed to optimize latency, energy consumption, network traffic, and battery aging. Extensive experiments validate the effectiveness of the proposed method, showing a 29.9% reduction in total training time, an 11.5% reduction in network traffic, and superior overall performance compared to rule-based methods.},
  archive      = {J_TMC},
  author       = {Chengjia Lei and Shaohua Wu and Yi Yang and Jiayin Xue and Dawei Chen and Pengfei Duan and Qinyu Zhang},
  doi          = {10.1109/TMC.2025.3568470},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10345-10362},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint partitioning, allocation, and transmission optimization for federated learning in satellite constellations via multi-task MARL},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized local differential privacy for multi-dimensional range queries over mobile user data. <em>TMC</em>, <em>24</em>(10), 10330-10344. (<a href='https://doi.org/10.1109/TMC.2025.3568511'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-dimensional range queries performed on the mobile user data records become increasingly important and popular in the fields of e-commerce, social media, transportation logistics, etc. Meanwhile, mobile users usually have different privacy requirements for different attributes of the records. A straightforward and effective approach is to first get low-dimensional range query outcomes by using existing LDP mechanisms at different privacy levels, and then derive high-dimensional range query results at each level, and finally aggregate the results from all levels. However, it incurs low utility of the query results, since the non-fixed privacy budgets and the correlation between dimensions (attributes) detrimentally impact the utility of LDP methods, ultimately rendering them ineffective in practice. In this paper, we propose a new Personalized LDP approach for Multi-dimensional Range queries (PLDP-MR) over mobile user data, consisting of the user grouping, data perturbing, data re-perturbing, and range query results aggregating steps. First, PLDP-MR offers flexible dual grouping based on user-selected privacy levels and relevant attributes to obtain the corresponding one-dimensional and two-dimensional grids. PLDP-MR optimizes the grid granularity to minimize errors from perturbing users’ attribute data with different LDP noises at non-fixed privacy levels. Furthermore, PLDP-MR carefully re-perturbs the LDP-noisy data from mobile users at lower privacy levels (i.e., having the higher utility) to achieve LDP with higher privacy levels and supplement the data volume of the corresponding groups. Thus, the data utility is effectively improved without additional privacy losses. Finally, PLDP-MR aggregates the frequencies in all the one-dimensional and two-dimensional grids related to the multi-dimensional range query at all query intervals and all privacy levels to derive the final query result with considering the correlation between attributes. The aggregations use maximum entropy optimization and maximum likelihood methods to further enhance its utility. The privacy and utility of PLDP-MR are analyzed, and extensive experiments demonstrate its effectiveness.},
  archive      = {J_TMC},
  author       = {Yuanyuan He and Meiqi Wang and Xianjun Deng and Peng Yang and Qiao Xue and Laurence T. Yang},
  doi          = {10.1109/TMC.2025.3568511},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10330-10344},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Personalized local differential privacy for multi-dimensional range queries over mobile user data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly optimizing the energy and time for multi-UAV 3-D coverage of terrestrial regions. <em>TMC</em>, <em>24</em>(10), 10312-10329. (<a href='https://doi.org/10.1109/TMC.2025.3568788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-rotor uncrewed aerial vehicles(UAVs) have been widely employed in various sensing tasks, e.g., environmental monitoring and disaster rescuing, many of which often require full coverage of terrestrial regions by UAVs. Efforts have been devoted to minimizing one of two objectives, i.e., energy consumptions and time costs of UAVs fulfilling such tasks, whereas it is still challenging to jointly optimize both objectives due to their complicated interdependent relationship. Therefore, this paper deals with the tasks of sensing terrestrial regions with multiple UAVs, and focuses on the three-dimensional (3-D) coverage problem by formulating a multi-objective optimization problem of jointly minimizing both objectives. Specifically, in order to optimize energy consumption effectively, an advanced closed-form energy consumption model for multi-rotor UAVs is developed based on a rigorous theoretical analysis by introducing the influences of torque and acceleration, which are often ignored by existing heuristic models. Moreover, considering the NP-hardness of the problem, an innovative swarm intelligence optimization framework is established by leveraging a multitasking learning pattern to exploit cross-task knowledge transfer and adopting an improved multi-objective salp swarm algorithm. Therein, two novel operators, i.e., a variable characteristic-guided hybrid solution initialization operator and a large-scale search-space-oriented multi-mechanism solution update operator, are designed to handle continuous, discrete and even high-dimensional variables involved. Real-world experiments validate the proposed energy model due to the reduction of power consumption estimation error by up to 59% compared to baselines, and besides, extensive simulations demonstrate that the proposed algorithm significantly outperforms the benchmarks in terms of both energy consumptions and time costs.},
  archive      = {J_TMC},
  author       = {Hao Gong and Baoqi Huang and Bing Jia and Lifei Hao and Zhenwei Shi},
  doi          = {10.1109/TMC.2025.3568788},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10312-10329},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Jointly optimizing the energy and time for multi-UAV 3-D coverage of terrestrial regions},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized federated averaging via random walk. <em>TMC</em>, <em>24</em>(10), 10295-10311. (<a href='https://doi.org/10.1109/TMC.2025.3569423'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a communication-efficient distributed machine learning method that allows multiple devices to collaboratively train models without sharing raw data. FL can be categorized into centralized and decentralized paradigms. The centralized paradigm relies on a central server to aggregate local models, potentially resulting in single points of failure, communication bottlenecks, and exposure of model parameters. In contrast, the decentralized paradigm, which does not require a central server, provides improved robustness and privacy. The essence of federated learning lies in leveraging multiple local updates for efficient communication. However, this approach may result in slower convergence or even convergence to suboptimal models in the presence of heterogeneous and imbalanced data. To address this challenge, we study decentralized federated averaging via random walk (DFedRW), which replaces multiple local update steps on a single device with random walk updates. Traditional Federated Averaging (FedAvg) and its decentralized versions commonly ignore stragglers, which reduces the amount of training data and introduces sampling bias. Therefore, we allow DFedRW to aggregate partial random walk updates, ensuring that each computation contributes to the model update. To further improve communication efficiency, we also propose a quantized version of DFedRW. We demonstrate that (quantized) DFedRW achieves convergence upper bound of order $\mathcal {O}(\frac{1}{k^{1-q}})$ under convex conditions. Furthermore, we propose a sufficient condition that reveals when quantization balances communication and convergence. Numerical analysis indicates that our proposed algorithms outperform (decentralized) FedAvg in both convergence rate and accuracy, achieving a 38.3% and 37.5% increase in test accuracy under high levels of heterogeneities, without increasing communication costs for the busiest device.},
  archive      = {J_TMC},
  author       = {Changheng Wang and Zhiqing Wei and Lizhe Liu and Qiao Deng and Yingda Wu and Yangyang Niu and Yashan Pang and Zhiyong Feng},
  doi          = {10.1109/TMC.2025.3569423},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10295-10311},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Decentralized federated averaging via random walk},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trading fresh IoT data with strategic users. <em>TMC</em>, <em>24</em>(10), 10278-10294. (<a href='https://doi.org/10.1109/TMC.2025.3571452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immense value of IoT data in real-time applications has led to the rise of fresh IoT data trading. Existing research often neglects strategic users who optimally time their data purchases, significantly affecting market demand and revenue. This paper studies a fresh data market with strategic users arriving stochastically and having heterogeneous data valuations. Strategic users decide purchase timing based on data freshness and price, while the platform optimizes its data pricing policy to maximize profit. We first examine a dynamic pricing policy, offering a price menu to each arriving user. This analysis is technically challenging due to the varied integer programming problems faced by heterogeneous users, making direct price optimization infeasible. To address this, we adopt a mechanism design approach, analytically deriving the optimal dynamic pricing policy. To reduce implementation complexity, we also study two simpler pricing policies: single and two-price pricing. In a two-period refreshing model, we derive the optimal single and two-price pricing policies analytically. Our findings reveal that the optimal two-price policy significantly outperforms the single pricing policy, guaranteeing at least 96% of the revenue achieved by the optimal dynamic pricing policy in a two-period refreshing model. Surprisingly, despite having more purchasing options, strategic users may be worse off than if they were myopic due to higher prices. The platform actually benefits from strategic users, generating up to five times more profit with strategic users than with myopic users, even while reducing data refresh frequency.},
  archive      = {J_TMC},
  author       = {Junyi He and Meng Zhang and Qian Ma and Jianwei Huang},
  doi          = {10.1109/TMC.2025.3571452},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10278-10294},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Trading fresh IoT data with strategic users},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GDSG: Graph diffusion-based solution generator for optimization problems in MEC networks. <em>TMC</em>, <em>24</em>(10), 10264-10277. (<a href='https://doi.org/10.1109/TMC.2025.3568248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimization is crucial for the efficiency and reliability of multi-access edge computing (MEC) networks. Many optimization problems in this field are NP-hard and do not have effective approximation algorithms. Consequently, there is often a lack of optimal (ground-truth) data, which limits the effectiveness of traditional deep learning approaches. Most existing learning-based methods require a large amount of optimal data and do not leverage the potential advantages of using suboptimal data, which can be obtained more efficiently. To illustrate this point, we focus on the multi-server multi-user computation offloading (MSCO) problem, a common issue in MEC networks that lacks efficient optimal solution methods. In this paper, we introduce the graph diffusion-based solution generator (GDSG), designed to work with suboptimal datasets while still achieving convergence to the optimal solution with high probability. We reformulate the network optimization challenge as a distribution-learning problem and provide a clear explanation of how to learn from suboptimal training datasets. We develop GDSG, a multi-task diffusion generative model that employs a graph neural network (GNN) to capture the distribution of high-quality solutions. Our approach includes a straightforward and efficient heuristic method to generate a sufficient amount of training data composed entirely of suboptimal solutions. In our implementation, we enhance the GNN architecture to achieve better generalization. Moreover, the proposed GDSG can achieve nearly 100% task orthogonality, which helps prevent negative interference between the discrete and continuous solution generation training objectives. We demonstrate that this orthogonality arises from the diffusion-related training loss in GDSG, rather than from the GNN architecture itself. Finally, our experiments show that the proposed GDSG outperforms other benchmark methods on both optimal and suboptimal training datasets. Regarding the minimization of computation offloading costs, GDSG achieves savings of up to 56.62% on the ground-truth training set and 41.06% on the suboptimal training set compared to existing discriminative methods.},
  archive      = {J_TMC},
  author       = {Ruihuai Liang and Bo Yang and Pengyu Chen and Xuelin Cao and Zhiwen Yu and Mérouane Debbah and Dusit Niyato and H. Vincent Poor and Chau Yuen},
  doi          = {10.1109/TMC.2025.3568248},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10264-10277},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {GDSG: Graph diffusion-based solution generator for optimization problems in MEC networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoE-oriented cooperative VR rendering and dynamic resource leasing in metaverse. <em>TMC</em>, <em>24</em>(10), 10247-10263. (<a href='https://doi.org/10.1109/TMC.2025.3569695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of the Metaverse has ushered in a new era of social networking, offering users deeply engaging spaces to connect and participate in social activities. However, rendering these virtual environments is resource-intensive. With many users accessing simultaneously and requiring diverse Metaverse services, optimizing Metaverse resources to deliver the best quality-of-experience (QoE) for users is a significant challenge. In this paper, we propose a cooperative virtual reality (VR) rendering and dynamic resource leasing mechanism to address this issue. Specifically, we first introduce a cooperative VR scene pre-rendering framework between users and Planets (i.e., edge servers hosting users), and establish a new user QoE metric named EdgeVRQoE which considers both rendering delay and visual quality. We formulate the multidimensional rendering resources (e.g., GPU, CPU, and outbound bandwidth) leasing problem between Planets and users as a double-layer decision problem, and devise a hybrid action multi-agent reinforcement learning-based dynamic resource auction mechanism to efficiently allocate limited resources of Planets in a distributed and adaptive manner. Extensive simulations demonstrate that our proposed scheme outperforms the representatives in user QoE and resource utilization efficiency. Particularly, the proposed scheme shows at least an 18-fold improvement in QoE over other schemes, demonstrating its capability in providing immersive Metaverse experiences.},
  archive      = {J_TMC},
  author       = {Nan Liu and Tom H. Luan and Yuntao Wang and Yiliang Liu and Zhou Su},
  doi          = {10.1109/TMC.2025.3569695},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10247-10263},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoE-oriented cooperative VR rendering and dynamic resource leasing in metaverse},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TraCemop: Toward federated learning with traceable contribution evaluation and model ownership protection. <em>TMC</em>, <em>24</em>(10), 10230-10246. (<a href='https://doi.org/10.1109/TMC.2025.3569547'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) allows multiple clients to collaboratively train machine learning models without the need to share their local private data. As a result, it can effectively address the issue of data fragmentation. Nevertheless, insufficient evaluation of individual contributions and the lack of protections for both the intellectual property rights (IPR) of models and client privacy can greatly reduce clients’ motivations in federated training. To address these challenges, this paper introduces the Traceable Contribution Evaluation and Model Ownership Protection (TraCemop) framework for federated learning, which allows each client to swiftly assess the contributions of others in each round, with integrated support for the traceability of evaluation results. To safeguard the intellectual property of models, a collective watermark is embedded in the global model. Additionally, a secure mechanism for verifying model ownership is also available in case of disputes. Security analysis indicates that TraCemop is capable of resisting data reconstruction attacks as well as various types of model copyright infringements. Finally, we evaluate the proposed framework using two commonly-used datasets, and the experimental results show a significant improvement in the efficiency of contribution evaluation compared to existing methods. Meanwhile, IPR infringement tests on TraCemop reveal that the proposed framework is resilient against malicious efforts to monopolize model ownership.},
  archive      = {J_TMC},
  author       = {Jingwei Liu and Zihan Zhou and Rong Sun and Lei Liu and Rongxing Lu and Schahram Dustdar and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3569547},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10230-10246},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TraCemop: Toward federated learning with traceable contribution evaluation and model ownership protection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient model training in edge networks with hierarchical split learning. <em>TMC</em>, <em>24</em>(10), 10214-10229. (<a href='https://doi.org/10.1109/TMC.2025.3569407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an efficient model training scheme, named Group-based Hierarchical Split Learning (GHSL), which can accelerate the artificial intelligence (AI) training process in edge networks in a “first-sequential-then-parallel” manner. Specifically, the proposed scheme hierarchically splits an AI model into a user-side and server-side model, while dividing a number of users into multiple groups. Users in each group train user-side models with the interaction of the shared server-side model sequentially; different groups perform the above training process parallelly; the AI models of each group are aggregated into a global model. We also carry out the convergence analysis for the proposed scheme over non-independent and identically distributed data, which reveals that the convergence rate depends on user grouping. Furthermore, we propose a data-driven two-stage user grouping algorithm to minimize the overall training delay, taking user resource heterogeneity and the black-box training process into account. The proposed algorithm first utilizes the Gaussian process regression approach to determine the number of groups, and then employs the coalition game theory to determine the optimal user grouping decision. Comprehensive simulation results demonstrate that the proposed scheme can reduce training delay, user-side computational workload, and communication overhead by up to 19%, 53%, and 54%, respectively, comparing to state-of-the-art benchmarks.},
  archive      = {J_TMC},
  author       = {Songge Zhang and Wen Wu and Lingyang Song and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3569407},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10214-10229},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient model training in edge networks with hierarchical split learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-based intelligent trusted computational resource allocation for low-altitude networks. <em>TMC</em>, <em>24</em>(10), 10200-10213. (<a href='https://doi.org/10.1109/TMC.2025.3568614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In low-altitude networks, unmanned aerial vehicles (UAVs) can offer services such as logistics, intelligence surveillance, and environmental monitoring, aided by base stations (BSs) with substantial computational resources. However, BSs must defend against malicious UAVs that may overload resources or launch denial-of-service attacks. In this paper, we formulate a blockchain-enabled access control model, which uses the UAV identities (IDs) and trajectories, positive and negative interactions with the BS to evaluate the reputations of UAVs. In the blockchain, the elected miner generates blocks containing UAV IDs, coordinates, interactions, and reputation values. To defend against malicious UAVs, this paper formulates a trusted computational resource allocation optimization problem, solved by safe reinforcement learning (RL) with a three-level hierarchical structure. Specifically, this method uses the designed structure to optimize the BS access control, resource allocations, and block size. In particular, we design an E-network to evaluate the long-term risk resulting from the chosen policy, which is used to refine the policy distribution for safe exploration. A modified reward function accounts for immediate risks, preventing short-term dangerous explorations that could lead to illegal access or computational failures. We prove the Lyapunov asymptotic stability of the proposed system and derive the reward upper bound. Simulation results show that our scheme can converge to the upper bound, outperform the benchmark, and validate the effectiveness via ablation experiments.},
  archive      = {J_TMC},
  author       = {Xiaozhen Lu and Lixin Liu and Zhibo Liu and Qihui Wu and Liang Xiao},
  doi          = {10.1109/TMC.2025.3568614},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10200-10213},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-based intelligent trusted computational resource allocation for low-altitude networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Embedding chips over the air: Rethink IoT architecture for ubiquitous sensing. <em>TMC</em>, <em>24</em>(10), 10186-10199. (<a href='https://doi.org/10.1109/TMC.2025.3567635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale IoT sensor deployment calls for inexpensive, low-power sensor nodes that still perform long-range, large-scale networking at the system level. However, current sensor nodes are constructed according to the ’one-size-fits-all’ embedded design, where the processor and RF transceiver are indispensable but underutilized in low-duty cycles, resulting in overwhelmingly significant unit price and run-time power. In this paper, we propose a novel processor-sharing IoT architecture that converts the vast majority of sensor nodes from embedded computers to low-end RF peripherals. The conventional full-fledged sensor nodes are smashed into the air, and the scattered chips are scaled well with negligible overheads through a virtual I$^{2}$C bus called RFBus. Specifically, RFBus interface is designed to be backward compatible with the I$^{2}$C bus interface, and thus, RFBus network inherits versatile link layer services transparently from the well-established I$^{2}$C link layer protocol. We design RFBus with joint consideration of system-level performance and deployment costs and evaluate the prototypes both indoors and outdoors. The result indicates that the proposed architecture achieves 6.09 × (indoor) and 6.69 × (outdoor) energy saving and reduces the unit price of sensor nodes by 23.5% (indoor) and 33.5% (outdoor).},
  archive      = {J_TMC},
  author       = {Qianhe Meng and Han Wang and Chong Zhang and Yihang Song and Songfan Li and Li Lu and Hongzi Zhu},
  doi          = {10.1109/TMC.2025.3567635},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10186-10199},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Embedding chips over the air: Rethink IoT architecture for ubiquitous sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal non-uniformity-aware online task scheduling in collaborative edge computing for industrial internet of things. <em>TMC</em>, <em>24</em>(10), 10169-10185. (<a href='https://doi.org/10.1109/TMC.2025.3567615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing mitigates the shortcomings of cloud computing caused by unpredictable wide-area network latency and serves as a critical enabling technology for the Industrial Internet of Things (IIoT). Unlike cloud computing, mobile edge networks offer limited and distributed computing resources. As a result, collaborative edge computing emerges as a promising technology that enhances edge networks’ service capabilities by integrating computational resources across edge nodes. This paper investigates the task scheduling problem in collaborative edge computing for IIoT, aiming to optimize task processing performance under long-term cost constraints. We propose an online task scheduling algorithm to cope with the spatiotemporal non-uniformity of user request distribution in distributed edge networks. For the spatial non-uniformity of user requests across different factories, we introduce a graph model to guide optimal task scheduling decisions. For the time-varying nature of user request distribution and long-term cost constraints, we apply Lyapunov optimization to decompose the long-term optimization problem into a series of real-time subproblems that do not require prior knowledge of future system states. Given the NP-hard nature of the subproblems, we design a heuristic-based hierarchical optimization approach incorporating enhanced discrete particle swarm and harmonic search algorithms. Finally, an imitation learning-based approach is devised to further accelerate the algorithm's operation, building upon the initial two algorithms. Comprehensive theoretical analysis and experimental evaluation demonstrate the effectiveness of the proposed schemes.},
  archive      = {J_TMC},
  author       = {Yang Li and Xing Zhang and Yukun Sun and Wenbo Wang and Bo Lei},
  doi          = {10.1109/TMC.2025.3567615},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10169-10185},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Spatiotemporal non-uniformity-aware online task scheduling in collaborative edge computing for industrial internet of things},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TouchHBC: Touch-based human body communication via leakage current. <em>TMC</em>, <em>24</em>(10), 10153-10168. (<a href='https://doi.org/10.1109/TMC.2025.3569282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable devices, including smartwatches, are increasingly popular among consumers due to their user-friendly services. However, transmitting sensitive data like social media messages and payment QR codes via commonly used low-power Bluetooth exposes users to privacy breaches and financial losses. This study introduces TouchHBC, a secure and reliable communication scheme leveraging a smartwatch’s built-in electrodes. This system establishes a touch-based human communication system utilizing a laptop’s leakage current. As the transmitting device, the laptop modulates this current via the CPU. Simultaneously, the smartwatch, equipped with built-in electrodes, captures the current traversing the human body and decodes it. The modulation and decoding processes involve techniques such as amplitude modulation, variational mode decomposition, channel estimation, and retransmission mechanisms. TouchHBC facilitates communication between laptops and smartwatches. Real-world tests demonstrate that our prototype achieves a throughput of $ 19.83\ \text{bps}$. Moreover, TouchHBC offers the potential for enhanced interaction, including improved gaming experiences through vibration feedback and secure touch login for smartwatch applications by synchronizing with a laptop. Furthermore, the system can be integrated with high-throughput communication protocols such as Bluetooth, enhancing its scalability while maintaining a strong foundation of security.},
  archive      = {J_TMC},
  author       = {Dian Ding and Hao Pan and Yongzhao Zhang and Yijie Li and Yu Lu and Yi-Chao Chen and Guangtao Xue},
  doi          = {10.1109/TMC.2025.3569282},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10153-10168},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TouchHBC: Touch-based human body communication via leakage current},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROTR: Role-transformable multi-agent resource allocation for nonstationary vehicular communications. <em>TMC</em>, <em>24</em>(10), 10135-10152. (<a href='https://doi.org/10.1109/TMC.2025.3567652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient wireless resource allocation is essential for supporting multi-vehicle cooperation. The service data exchanged among intelligent vehicles is typically diverse, with varying transmission requirements that shift according to applications and traffic conditions, leading to major fluctuation in communication situations. Existing multi-agent reinforcement learning based resource allocation methods are often inefficient in handling such nonstationary communication situations due to their rigid cooperation patterns. To this end, we propose a ROle-TRansformable multi-agent resource allocation method, named ROTR. This method adopts a hierarchical decision-making process, where a high-level agent at a base station (BS) dynamically plans and distributes cooperation roles (CRs) and cooperation behaviors (CBs) in response to fluctuating communication situations. The Low-level agents within the transmitting vehicles (TVs) perform role transformations based on the assigned CRs and subsequently receive behavioral guidance according to CBs, enabling dynamic adjustments in cooperation patterns to adapt to variable communication situations and make resource allocation decisions. Additionally, we introduce a non-BS-assisted mode based on policy distillation, which enables a seamless transition to independent operation without the BS, relying solely on local states to generate CRs and CBs, thereby facilitating global resource cooperation. Extensive simulation experiments demonstrate that the proposed framework optimizes resource efficiency in nonstationary vehicular communications.},
  archive      = {J_TMC},
  author       = {Yang Li and Quan Yuan and Xiaoyuan Fu and Guiyang Luo and Jiawen Kang and Jinglin Li and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3567652},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10135-10152},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ROTR: Role-transformable multi-agent resource allocation for nonstationary vehicular communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoS-oriented joint resource and trajectory optimization in NOMA-enhanced AAV-MEC systems. <em>TMC</em>, <em>24</em>(10), 10118-10134. (<a href='https://doi.org/10.1109/TMC.2025.3575451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous Aerial Vehicle (AAV)-assisted Mobile Edge Computing (MEC) has received extensive attention because it provides resilient computation services for multiple Mobile Users (MUs). However, due to the increasing scale of offloaded tasks, the uncertain mobility of MUs, and the limited energy budget of AAV and MUs, it is extremely challenging to achieve satisfactory Quality-of-Service (QoS). Non-Orthogonal Multiple Access (NOMA), a promising technology to serve multiple MUs with limited communication resources, has great potential to be integrated with MEC. To this end, this paper proposes a QoS-oriented NOMA-enhanced AAV-MEC system, which aims to capture the potential gains of uplink NOMA and enable more MUs to benefit from edge computing servers in resource-constrained AAV-assisted MEC environments. This synergy reduces MUs’ uplink energy consumption but poses new challenges in resource allocation and AAV trajectory design. To address these challenges, we define a new metric called System Overhead Ratio (SOR) to reflect the system’s QoS, and then consider a joint optimization problem of resource allocation, transmission power control, and AAV trajectory design, with the goal of minimizing the SOR. Given the NP-hard nature of the optimization problem, we propose a Lyapunov and convex optimization-based Low-complexity Online Resource allocation and Trajectory optimization method (LORT) to solve it, and further analyze the convergence and complexity of LORT. Finally, extensive simulations show that the proposed method surpasses other benchmarks, reducing the SOR by approximately $10\%$-$ 25\%$ under various scenarios.},
  archive      = {J_TMC},
  author       = {Huan Zhou and Yadong Lu and Geyong Min and Zhiwen Yu and Liang Wang and Yao Zhang and Bin Guo},
  doi          = {10.1109/TMC.2025.3575451},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10118-10134},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoS-oriented joint resource and trajectory optimization in NOMA-enhanced AAV-MEC systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VerDT: A versatile digital twins framework for UAVs-based industrial cyber-physical systems. <em>TMC</em>, <em>24</em>(10), 10099-10117. (<a href='https://doi.org/10.1109/TMC.2025.3567284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of cyber-physical systems, Digital Twins (DT)-powered network autonomy is emerging to embrace the fifth-generation industrial revolution. In this context, Unscrewed Aerial Vehicles (UAVs)-based low-altitude networks are expected to be the engines that drive industrial development. As an attractive industry application, UAVs-based intelligent logistics has been widely investigated to achieve a fully automated distribution manner without the aid of a workforce. However, it is difficult to perform real-time DT implementations due to limited computing resources and the high mobility of UAVs. To address the mentioned problems, we propose a Versatile DT (VerDT) framework operating at the edge. It can enable a double DT cooperation manner with a resource scheduling model and a path planning model for real-time and accurate logistics distributions. The resource scheduling model can implement the integration of computing and communication resources among UAVs for feasible cooperative distribution decisions. With the decisions, the path planning model can imitate to derive positions and velocities of UAVs for low-latency distribution performance with energy saving. Experiment results demonstrate the efficiency of our VerDT framework. Compared to state-of-the-art logistics distribution solutions, our solution reduces the distribution latency by 63.9% while improving the successful distribution ratio by 10.9%.},
  archive      = {J_TMC},
  author       = {Longyu Zhou and Supeng Leng and Tony Q. S. Quek},
  doi          = {10.1109/TMC.2025.3567284},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10099-10117},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {VerDT: A versatile digital twins framework for UAVs-based industrial cyber-physical systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure enhanced IoT-WLAN authentication protocol with efficient fast reconnection. <em>TMC</em>, <em>24</em>(10), 10085-10098. (<a href='https://doi.org/10.1109/TMC.2025.3569593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing integration of Internet of Things (IoT) devices in Wireless Local Area Networks (WLANs) necessitates robust and efficient authentication mechanisms. While existing IoT authentication protocols address certain security concerns, they often fail to provide comprehensive protection against threats such as perfect forward secrecy violations, insider attacks, and key compromise impersonation, or impose significant computational and communication overhead on resource- constrained IoT systems. This paper presents a novel Extensible Authentication Protocol (EAP) based scheme for IoT-WLAN environments that addresses these security challenges while maintaining cost-effectiveness. Our approach utilizes elliptic curve cryptography and incorporates advanced features including perfect forward secrecy, strong identity protection, and explicit key confirmation. We provide a thorough security analysis using informal heuristics, formal methods (Random Oracle Model and BAN Logic), and automated verification with ProVerif. Performance evaluations demonstrate that our protocol achieves lower communication, storage, and computational costs compared to state-of-the-art solutions, with an average 79.6% reduction in computation time. A detailed comparison with existing schemes highlights the efficiency and enhanced security features of our proposed authentication mechanism for IoT-WLAN deployments.},
  archive      = {J_TMC},
  author       = {Weizheng Wang and Qipeng Xie and Zhaoyang Han and Chunhua Su and Joel J. P. C. Rodrigues and Kaishun Wu},
  doi          = {10.1109/TMC.2025.3569593},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10085-10098},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Secure enhanced IoT-WLAN authentication protocol with efficient fast reconnection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FBDT: Sum-throughput achieving transport layer solution for multi-RAT networks. <em>TMC</em>, <em>24</em>(10), 10069-10084. (<a href='https://doi.org/10.1109/TMC.2025.3569453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging mobile applications give rise to new bandwidth-hungry and latency-sensitive traffic classes that challenge existing wireless systems. Addressing them requires innovative approaches such as simultaneous data transmission across multiple Radio Access Technologies (RATs), e.g., WiFi and WiGig. However, existing transport layer multi-RAT traffic aggregation schemes, e.g., multi-path TCP, suffer from Head-of-Line (HoL) blocking and sub-optimal traffic splitting across the RATs that severely penalize their performance. In this paper, we investigate the design of FBDT, a novel multi-path transport layer solution that for the first time can achieve the sum of the throughput rates across the individual RATs network paths, despite their channel conditions’ dynamics. We have implemented FBDT in the Linux kernel and show substantial improvement in throughput relative to state-of-the-art schemes, e.g, 2.5x gain in a dual-RAT scenario (WiFi and WiGig) when the client is mobile. Second, we extend FBDT to more than two radios and demonstrate that its throughput performance scales linearly with the number of RATs, in contrast to multi-path TCP, whose performance degrades with an increase in the number of RATs. We evaluate the performance of FBDT on different traffic classes and demonstrate: (i) 2-3 times shorter file download times, (ii) up to 10 times shorter streaming times and 10 dB higher video quality for progressive download video applications, and (iii) up to 9 dB higher viewport quality for interactive mobile VR applications, when our viewport quality maximization framework is employed along with FBDT.},
  archive      = {J_TMC},
  author       = {Suresh Srinivasan and Sam Shippey and Ehsan Aryafar and Jacob Chakareski},
  doi          = {10.1109/TMC.2025.3569453},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10069-10084},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FBDT: Sum-throughput achieving transport layer solution for multi-RAT networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay-sensitive goods delivery and in-situ sensing using a multi-task drone. <em>TMC</em>, <em>24</em>(10), 10055-10068. (<a href='https://doi.org/10.1109/TMC.2025.3570437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drones are evolving into highly capable and adaptable devices, prompting the development of advanced control frameworks. This paper introduces a novel online control framework tailored for a multi-task drone, explicitly addressing the simultaneous execution of in-situ sensing and goods delivery. To tackle this complex scenario, a finite-horizon Markov decision process (FH-MDP) is formulated to ensure not only the prompt delivery of goods but also the minimization of energy consumption and the maximization of the drone's reward for in-situ sensing. A significant contribution lies in establishing the monotonicity and subadditivity of the FH-MDP. This mathematical foundation provides evidence for the existence of an optimal, monotone, deterministic Markovian policy. The crux of the optimal policy revolves around flight distance- and time-related thresholds, determining the precise points at which the drone should switch its optimal action. This unique feature empowers the multi-task drone to make real-time decisions, such as adjusting flight speed or engaging in in-situ sensing, by comparing its current state with these predefined thresholds. This process can be accomplished with a linear complexity, ensuring efficiency in decision-making. The optimality of our approach is rigorously demonstrated through numerical validation, where it is compared against a computationally expensive, dynamic programming-based alternative. Under the considered simulation settings, our approach reduces drone energy consumption by a substantial 19.8% compared to existing benchmarks. This not only highlights the practical effectiveness of the proposed framework but also underscores its potential for significant advancements in the field of drone operations and energy efficiency.},
  archive      = {J_TMC},
  author       = {Bin Liu and Wei Ni and Ren Ping Liu and Y. Jay Guo and Hongbo Zhu},
  doi          = {10.1109/TMC.2025.3570437},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10055-10068},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Delay-sensitive goods delivery and in-situ sensing using a multi-task drone},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Teaching to fish rather than giving a fish: The concentrator method of teaching classic congestion control with learning-based module. <em>TMC</em>, <em>24</em>(10), 10042-10054. (<a href='https://doi.org/10.1109/TMC.2025.3567582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Congestion Control (CC) algorithms are expected to satisfy the diverse demands of applications running over diverse networks. To achieve this goal, the combinations, aiming to inherit both the advantages of classic CC in terms of convergence, overhead, and explainability, and the advantages of learning-based CC on adapting to diverse networks and demands, become a hot topic. In this paper, we reveal the existing combination works are either giving a fish or teaching to fish. Based on the insight of their essential issues, we develop the Concentrator method of teaching to fish. According to this method, we propose Seagull. Specifically, Seagull captures the network characteristics and application demands in a coarse-grained manner via an online learning module. Moreover, this module guides to customize the rate adjustment rules of the classic CC module for fine-grained system evolution. Replacing the assumption on networks by the captured characteristics, the classic CC module of Seagull can fulfill the specified application demands. Real-world experimental results show Seagull respectively outperforms Orca, PCC-Vivace, and CUBIC by $49.3\%,\ 30.4\%$, and 24.9% in terms of throughput over the Internet, and improves the video quality of experience (QoE) by $12.9\sim 33.5\%$ compared to CUBIC over cellular links.},
  archive      = {J_TMC},
  author       = {Haoyang Li and Wanchun Jiang and Jie Wang and Ying Wang and Jiawei Huang and Danfeng Shan and Jianxin Wang},
  doi          = {10.1109/TMC.2025.3567582},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10042-10054},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Teaching to fish rather than giving a fish: The concentrator method of teaching classic congestion control with learning-based module},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CADEC: A combinatorial auction for dynamic distributed DNN inference scheduling in edge-cloud networks. <em>TMC</em>, <em>24</em>(10), 10024-10041. (<a href='https://doi.org/10.1109/TMC.2025.3567459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Network (DNN) Inference, as a key enabler of intelligent applications, is often computation-intensive and latency-sensitive. Combining the advantages of cloud computing (abundant computing resources) and edge computing (fast transmission), edge-cloud collaborative DNN inference is a powerful solution to these problems. However, in edge-cloud networks with heterogeneous resources, how to obtain reasonable decisions on server selection, model partition and resource allocation for efficient distributed DNN inference is a hard challenge. Furthermore, it is non-trivial to design suitable resource prices to maximize the social welfare. These challenges even escalate in dynamic edge-cloud networks where decisions should be generated as soon as each user arrives without future information. Therefore, we design a combinatorial auction for dynamic distributed DNN inference scheduling, named CADEC. CADEC first constructs a bid set for each user based on convex optimization theory for optimal solution searching. Next, prices of resources in the edge-cloud network are adjusted according to changes in supply-demand relationship, and whether to admit the request of each user is decided. Finally, the dynamic distributed inference scheduling decisions are generated through the primal-dual algorithm to maximize the social welfare. Theoretical analysis shows the good competitive ratio and polynomial time complexity of CADEC. Results of simulation experiments present that CADEC improves social welfare by up to 224% compared with state-of-the-art distributed DNN inference schemes.},
  archive      = {J_TMC},
  author       = {Xiaolong Xu and Yuhao Hu and Guangming Cui and Lianyong Qi and Wanchun Dou and Zhipeng Cai},
  doi          = {10.1109/TMC.2025.3567459},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10024-10041},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CADEC: A combinatorial auction for dynamic distributed DNN inference scheduling in edge-cloud networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unimodal training-multimodal prediction: Cross-modal federated learning with hierarchical aggregation. <em>TMC</em>, <em>24</em>(10), 10009-10023. (<a href='https://doi.org/10.1109/TMC.2025.3567535'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal learning has significantly advanced the extraction of features from varied data sources, enhancing model performance. Federated learning (FL) complements this by enabling collaborative training while maintaining data privacy. The fusion of these two fields, multimodal federated learning, offers considerable promise. Yet, standard methods often incorrectly assume that each node in the FL network has a full complement of multimodal data, which is rare in real-world applications. In our study, we present a novel architecture designed to surmount these challenges, termed the Unimodal Training - Multimodal Prediction (UTMP) framework, positioned within the multimodal federated learning paradigm. Our proposed model, the HA-Fedformer, is a transformer-based model crafted to facilitate unimodal training on the client-side using exclusively unimodal datasets and to execute multimodal inference by synthesizing insights from multiple clients. Our HA-Fedformer model effectively handles non-IID data through a novel uncertainty-aware aggregation technique and layer-wise Markov Chain Monte Carlo sampling in local encoders. It also resolves misaligned language sequences via cross-modal decoder aggregation, capturing correlations between decoders trained on different modalities. Our comprehensive evaluations conducted on widely recognized sentiment analysis benchmarks demonstrate the superiority of the HA-Fedformer. The results show that our model achieves a substantial uplift in performance.},
  archive      = {J_TMC},
  author       = {Rongyu Zhang and Xiaowei Chi and Wenyi Zhang and Guiliang Liu and Dan Wang and Fangxin Wang},
  doi          = {10.1109/TMC.2025.3567535},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {10009-10023},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Unimodal training-multimodal prediction: Cross-modal federated learning with hierarchical aggregation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint scheduling, computing, and load balancing for time sensitive traffic in SDN-enabled space-air-ground integrated 6G networks: A federated reinforcement learning approach. <em>TMC</em>, <em>24</em>(10), 9995-10008. (<a href='https://doi.org/10.1109/TMC.2025.3567289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) constellations and Unmanned Aerial Vehicle (UAV) networks enable wide coverage for the sixth generation (6G) mobile communication. However, it is a challenge to achieve high scheduling success rate, ultra-low latency, and efficient load balance in the Space-Air-Ground Integrated 6G Network (SAGGIN). This paper addresses the following issue: How to effectively and orderly transmit time-sensitive traffic in SAGGIN under strict deadlines, limited computational ability, and restrained link capacity? Specifically, this paper uses Software-Defined Networking (SDN) and designs a joint optimization method to enhance the traffic transmission ability of SAGGIN. Considering response time, computing cost, and link capacity in SAGGIN, the scheduling, computing, and load balance issues are modeled as a multi-objective optimization problem that minimizes the worst-case response time and computing cost of data frames while maximizing the network flow. Then, this paper leverages a Federated Reinforcement Learning (FRL) scheme to solve the problem. Results show that the FRL could achieve great scheduling, computing, and load balance performance. Specifically, our method can successfully schedule 80% of the traffic at most when the current network load is around 90%. Furthermore, the computational delay could reduce around 50%.},
  archive      = {J_TMC},
  author       = {Haitong Sun and Haijun Zhang and Hui Ma and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3567289},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9995-10008},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint scheduling, computing, and load balancing for time sensitive traffic in SDN-enabled space-air-ground integrated 6G networks: A federated reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-oriented mulsemedia communication using unified perceiver and conformal prediction in 6G wireless systems. <em>TMC</em>, <em>24</em>(10), 9980-9994. (<a href='https://doi.org/10.1109/TMC.2025.3567880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing prominence of eXtended Reality (XR), holographic-type communications, and metaverse demands truly immersive user experiences by using many sensory modalities, including sight, hearing, touch, smell, taste, etc. Additionally, the widespread deployment of sensors in areas such as agriculture, manufacturing, and smart homes is generating diverse sensory data. A new media format known as multisensory media (mulsemedia) has emerged, which incorporates many sensory modalities beyond the traditional visual and auditory media. 6G wireless systems are envisioned to support the Internet of Senses, making it crucial to explore effective data fusion and communication strategies for mulsemedia. In this paper, we introduce a task-oriented multi-task mulsemedia communication system named MuSeCo, which is developed using unified Perceiver models and Conformal Prediction. This unified model can accept any sensory input and efficiently extract latent semantic features, making it adaptable for deployment across various Artificial Intelligence of Things (AIoT) devices. Conformal Prediction is employed for modality selection and combination, enhancing task accuracy while minimizing data communication overhead. The model is trained using six sensory modalities across four classification tasks. Simulations and experiments demonstrate that it can effectively fuse sensory modalities, significantly reduce end-to-end communication latency and energy consumption, and maintain high accuracy in communication-constrained systems.},
  archive      = {J_TMC},
  author       = {Hongzhi Guo and Ian F. Akyildiz},
  doi          = {10.1109/TMC.2025.3567880},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9980-9994},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Task-oriented mulsemedia communication using unified perceiver and conformal prediction in 6G wireless systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative video processing of multiple cameras in smart transportation: Content analysis and resource allocation. <em>TMC</em>, <em>24</em>(10), 9965-9979. (<a href='https://doi.org/10.1109/TMC.2025.3567062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of smart transportation, the collaborative processing of video data sourced from multiple cameras plays a pivotal role in promoting efficient traffic management and augmenting safety measures. Nevertheless, the exponential surge in surveillance cameras deployment has concurrently engendered a rapid increase in the magnitude of video analysis tasks and data volume. To address these challenges, we propose a comprehensive framework for collaborative video processing. Primarily, a collaborative content analysis approach is proposed, and which employs a Transformer-based ReID (Re-identification) algorithm to construct key stickers. These key stickers are optimized with cross-cameras correlations and serve as the foundational structure for subsequent online video compression. Subsequently, we propose a collaborative resource allocation approach, and which involves the formulation of a queue model designed for the orchestration of online camera analysis tasks. In addition, we have devised an enhanced deep reinforcement learning algorithm to fine-tune the task scheduling configuration of multiple cameras, with guidance from the queue model. Extensive experiments and simulations were conducted to evaluate the proposed framework. The results demonstrate its effectiveness in achieving accurate and real-time analysis of video data in smart transportation scenarios.},
  archive      = {J_TMC},
  author       = {Lei Du and Ru Huo and Chuang Sun and Shuo Wang and Tao Huang},
  doi          = {10.1109/TMC.2025.3567062},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9965-9979},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative video processing of multiple cameras in smart transportation: Content analysis and resource allocation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Location and reward privacy-preserving based secure task allocation in mobile crowdsensing. <em>TMC</em>, <em>24</em>(10), 9951-9964. (<a href='https://doi.org/10.1109/TMC.2025.3564404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online multi-task allocation has become an essential research topic in Mobile Crowdsensing (MCS). Most existing studies merely focus on minimizing the total distance that workers need to travel, but ignore considering the total task rewards, which could lead to a reduction in the willingness of workers to complete tasks. In this paper, to incentivize workers to participate in tasks and protect their privacy, we propose a Location and Reward Privacy-Preserving based Secure Task Allocation(LRPP-STA) scheme. First, we design a secure distance computation method to obtain the distance from the workers to the tasks under location privacy preserving. Second, considering fixed reward for the task, we propose a Fixed Rewarding Secure Task Allocation(FR-STA) scheme, where a secure utility calculation method is proposed to calculate the encrypted utility of the worker upon completing tasks under rewards privacy preserving, along with the path planning for workers to maximize the total utility of the system through an Extended Maximum-Utility Flow model(EMUF). Third, considering the situation of dynamic task reward adjusted by requesters based on the supply and demand relationship as well as the urgency of the task, we propose a Dynamic Rewarding Secure Task Allocation(DR-STA) scheme to optimize the task allocation for workers while improving requesters satisfaction. Finally, we theoretically analyze the security of location and reward privacy-preserving scheme, and conduct extensive experiments with real-world datasets to verify that the secure task allocation scheme is effective in improving the total utility of workers compared to other baseline online tasking schemes.},
  archive      = {J_TMC},
  author       = {Zhetao Li and Weifan Shi and Young-June Choi and Hiroo Sekiya and Qingyong Deng},
  doi          = {10.1109/TMC.2025.3564404},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9951-9964},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Location and reward privacy-preserving based secure task allocation in mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compressed private aggregation for scalable and robust federated learning over massive networks. <em>TMC</em>, <em>24</em>(10), 9934-9950. (<a href='https://doi.org/10.1109/TMC.2025.3564390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging paradigm that allows a central server to train machine learning models using remote users’ data. Despite its growing popularity, Federated learning (FL) faces challenges in preserving the privacy of local datasets, its sensitivity to poisoning attacks by malicious users, and its communication overhead, especially in large-scale networks. These limitations are often individually mitigated by local differential privacy (LDP) mechanisms, robust aggregation, compression, and user selection techniques, which typically come at the cost of accuracy. In this work, we present compressed private aggregation (CPA), allowing massive deployments to simultaneously communicate at extremely low bit rates while achieving privacy, anonymity, and resilience to malicious users. CPA randomizes a codebook for compressing the data into a few bits using nested lattice quantizers, while ensuring anonymity and robustness, with a subsequent perturbation to hold LDP. CPA-aided FL is proven to converge in the same asymptotic rate as FL without privacy, compression, and robustness considerations, while satisfying both anonymity and LDP requirements. These analytical properties are empirically confirmed in a numerical study, where we demonstrate the performance gains of CPA compared with separate mechanisms for compression and privacy, as well as its robustness in mitigating the harmful effects of malicious users.},
  archive      = {J_TMC},
  author       = {Natalie Lang and Nir Shlezinger and Rafael G. L. D’Oliveira and Salim El Rouayheb},
  doi          = {10.1109/TMC.2025.3564390},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9934-9950},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Compressed private aggregation for scalable and robust federated learning over massive networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness-aware incentive mechanism for multi-server federated learning in edge-enabled wireless networks with differential privacy. <em>TMC</em>, <em>24</em>(10), 9919-9933. (<a href='https://doi.org/10.1109/TMC.2025.3564301'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a distributed machine learning method, federated learning (FL) can collaboratively train a global model with multiple devices without sharing the original data, thus protecting certain privacy. However, due to the strong heterogeneity of edge nodes (ENs) participating in FL, the quality of data uploaded to the parameter server (PS) varies significantly. Without an appropriate incentive mechanism, low-quality contributors may receive disproportionately high rewards, while high-quality contributors may lack sufficient motivation, leading to inefficient participation and suboptimal global model performance. Consequently, it is critical to develop an effective incentive mechanism to promote fairness for the FL process. To address the issues of existing FL incentive mechanisms lacking privacy protection performance analysis, we propose a fairness-aware incentive mechanism for multi-server FL in edge-enabled wireless differential privacy (DP) networks. Specifically, the wireless channel noise is used to provide DP protection for the local model gradients uploaded by ENs. Next, the interaction between the PSs and ENs is modeled as a Stackelberg game. Furthermore, we solve the Stackelberg game process using backward induction and theoretically propose optimal strategies for both the PSs and ENs. Finally, extensive numerical simulations using real datasets demonstrate the superior performance of our theoretical analysis of the proposed scheme.},
  archive      = {J_TMC},
  author       = {Yu Yang and Kai Peng and Shangguang Wang and Xiaolong Xu and Peiyun Xiao and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3564301},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9919-9933},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fairness-aware incentive mechanism for multi-server federated learning in edge-enabled wireless networks with differential privacy},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing convergence, privacy and fairness for wireless personalized federated learning: Quantization-assisted min-max fair scheduling. <em>TMC</em>, <em>24</em>(10), 9902-9918. (<a href='https://doi.org/10.1109/TMC.2025.3566421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized federated learning (PFL) offers a solution to balancing personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). Little attention has been given to wireless PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is another challenge resulting from communication bottlenecks in WPFL. This paper exploits quantization errors to enhance the privacy of WPFL and proposes a novel quantization-assisted Gaussian differential privacy (DP) mechanism. We analyze the convergence upper bounds of individual PL models by considering the impact of the mechanism (i.e., quantization errors and Gaussian DP noises) and imperfect communication channels on the FL of WPFL. By minimizing the maximum of the bounds, we design an optimal transmission scheduling strategy that yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by revealing the nested structure of this problem to decouple it into subproblems solved sequentially for the client selection, channel allocation, and power control, and for the learning rates and PL-FL weighting coefficients. Experiments validate our analysis and demonstrate that our approach substantially outperforms alternative scheduling strategies by 87.08%, 16.21%, and 38.37% in accuracy, the maximum test loss of participating clients, and fairness (Jain's index), respectively.},
  archive      = {J_TMC},
  author       = {Xiyu Zhao and Qimei Cui and Ziqiang Du and Wei Ni and Weicai Li and Xi Yu and Ji Zhang and Xiaofeng Tao and Ping Zhang},
  doi          = {10.1109/TMC.2025.3566421},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9902-9918},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing convergence, privacy and fairness for wireless personalized federated learning: Quantization-assisted min-max fair scheduling},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sonicumos: An enhanced active face liveness detection system via ultrasonic and video signals. <em>TMC</em>, <em>24</em>(10), 9883-9901. (<a href='https://doi.org/10.1109/TMC.2025.3565689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sonicumos is an enhanced behavior-based face liveness detection system that combines ultrasonic and video signals to sense the 3D head gestures. As face authentication becomes increasingly prevalent, the need for a reliable liveness detection system is paramount. Traditional behavior-based liveness detection methods (e.g., eye-blinking, nodding, etc.), which are widely deployed in mission-critical scenarios like finance and banking applications today, are prone to advanced media-based facial forgery attacks. Sonicumos aims to incorporate the traditional behavior-based method for active liveness detection without introducing extra user burden. By employing ultrasonic signals, Sonicumos capitalizes on the head gestures, significantly raising the security bar. Our approach utilizes the frequency-modulated continuous-wave (FMCW) ultrasonic radar for robust 3D gesture recognition compatible with face authentication. We also propose a new dual-feature fusion network that integrates audio and video features at the feature level to increase detection accuracy and resilience against numerous attacks. Our prototype has been tested on seven off-the-shelf Android/iOS smartphones, achieving an overall detection accuracy of 95.83% at an equal error rate (EER) of 4.96% when dealing with 3D impersonation attacks.},
  archive      = {J_TMC},
  author       = {Yihao Wu and Peipei Jiang and Jianhao Cheng and Lingchen Zhao and Chao Shen and Cong Wang and Qian Wang},
  doi          = {10.1109/TMC.2025.3565689},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9883-9901},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sonicumos: An enhanced active face liveness detection system via ultrasonic and video signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture-of-experts as continual knowledge adapter for mobile vision understanding. <em>TMC</em>, <em>24</em>(10), 9868-9882. (<a href='https://doi.org/10.1109/TMC.2025.3567179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual machine learning in the context of limited computational resources and data availability is critical in the connected digital world. Current intelligent applications predominantly rely on deep learning models requiring labor/computation-intensive training. These models often struggle to adapt effectively to new data while preserving performance on previously learned knowledge. In this paper, we introduce a lightweight method for continual knowledge adaptation that can address these challenges. To prevent disruption of the existing services, we propose a Mixture-of-Experts (MoE) adapter that integrates seamlessly with the existing vision model to encode new data. The weights of the original model are kept fixed during the adaptation process, ensuring the preservation of previously learned knowledge. The MoE technique enables scaling up the parameters of the adapter while maintaining a relatively low computation, making it fit for constrained devices in mobile computation scenarios. Furthermore, to enhance learning efficiency and accelerate convergence with new data, we implement a knowledge fusion mechanism that facilitates interaction between the existing knowledge and the information extracted from new data. The timing of employing the fusion module is further investigated. We find that it is conducive in scenarios where the task’s performance requirements are enhanced. The MoE adapter and knowledge fusion module are integrated at each stage with minimal trainable parameters, efficiently optimizing resource usage. Extensive experiments and ablation studies validate the effectiveness of the proposed method. Specifically, the proposed method prevents an accuracy drop of 43.02% on the previous data compared to the continual train method, while achieving an accuracy of 44.81% on the new data, which is even 0.34% higher than fully training a new model.},
  archive      = {J_TMC},
  author       = {Bicheng Guo and Conghao Zhou and Shibo He and Jiming Chen and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3567179},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9868-9882},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mixture-of-experts as continual knowledge adapter for mobile vision understanding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive search and collaborative offloading under device-to-device joint edge computing network. <em>TMC</em>, <em>24</em>(10), 9852-9867. (<a href='https://doi.org/10.1109/TMC.2025.3567549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) and Device-to-Device (D2D) peer offloading are two promising paradigms in the mobile Internet of Things (IoT). In this paper, we study the collaborative task offloading with redundant data and codes in large-scale IoT networks, where computing resource-starved IoT devices can offload their tasks to MEC servers via cellular links or to nearby peer devices (PDs) with idle resources through D2D links for execution. IoT tasks usually consist of a series of dependent and parallel subtasks, and the difficulties in current research are (i) how to eliminate redundancy in data or codes between subtasks, and (ii) how to leverage previous experience to adaptively search a set of collaborative MEC servers and PDs for matching offloading of dependent and parallel subtasks. From this, we propose a redundancy-aware adaptive search offloading (RASO) method based on the deep Q-network (DQN). Specifically, we first design a fine-grained task recombination scheme by judging the consistency of subtask data and codes. After that, we organize the global devices into a spatial index MP-tree to reduce the search solution space, and propose a fast adaptive search method based on the DQN combined with MP-tree, where optimal path-guiding parameters training of inner and outer layers is involved to efficiently help achieve collaborative devices to complete specific tasks with the same type. After finding the collaborative MEC servers and PDs along MP-tree for a certain task, a centralized stable matching algorithm is further developed to give a decision of offloading each of its divided dependent and parallel subtasks to the matched one, thereby optimizing offloading delay and energy consumption. Extensive simulation results show that compared to other counterpart solutions, our proposed method has improved task offloading performance in terms of delay and energy consumption.},
  archive      = {J_TMC},
  author       = {Jine Tang and Wentao Zhao and Jiahao Jin and Yong Xiang and Xiaofei Wang and Zhangbing Zhou},
  doi          = {10.1109/TMC.2025.3567549},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9852-9867},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive search and collaborative offloading under device-to-device joint edge computing network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical index retrieval-driven wireless network intent translation with LLM. <em>TMC</em>, <em>24</em>(10), 9837-9851. (<a href='https://doi.org/10.1109/TMC.2025.3564937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intent-Based Networking (IBN) represents an emerging network management concept that is designed to fulfill user service requirements through automation. At its core, IBN is capable of translating user intent into network policies, thereby enabling automated configuration and management. However, the application of IBN has been limited by challenges associated with automation and intelligence. The recent widespread adoption of Large Language Model (LLM) has partially mitigated these issues. Nonetheless, hardware heterogeneity and high dynamic networks remain significant challenges for IBN: (i) Devices from different vendors are challenging to manage uniformly; (ii) Aligning service demands with rapidly changing network status is difficult. To address these challenges, we propose LIT, a framework of LLM-empowered Intent Translation with manual guidance. LIT incorporates Retrieval-Augmented Generation (RAG) to reference hardware manuals and enhance the generation results of LLMs. To reduce noise from retrieval results, we optimized the general RAG process. Additionally, LIT introduces MoE (Mixture of Experts) to adjust parameter values according to network status by synthesizing results from multiple expert models. Experiments demonstrate that LIT alleviates the challenges faced by IBN, achieving a 57.5% improvement in F1 score compared to the baseline.},
  archive      = {J_TMC},
  author       = {Jingyu Wang and Lingqi Guo and Jianyu Wu and Caijun Yan and Haifeng Sun and Lei Zhang and Zirui Zhuang and Qi Qi and Jianxin Liao},
  doi          = {10.1109/TMC.2025.3564937},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9837-9851},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hierarchical index retrieval-driven wireless network intent translation with LLM},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-oriented semantic communication in large multimodal models-based vehicle networks. <em>TMC</em>, <em>24</em>(10), 9822-9836. (<a href='https://doi.org/10.1109/TMC.2025.3564543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-oriented semantic communication has emerged as a fundamental approach for enhancing performance in various communication scenarios. While recent advances in Generative Artificial Intelligence (GenAI), such as Large Language Models (LLMs), have been applied to semantic communication designs, the potential of Large Multimodal Models (LMMs) remains largely unexplored. In this paper, we investigate an LMM-based vehicle AI assistant using a Large Language and Vision Assistant (LLaVA) and propose a task-oriented semantic communication framework to facilitate efficient interaction between users and cloud servers. To reduce computational demands and shorten response time, we optimize LLaVA's image slicing to selectively focus on areas of utmost interest to users. Additionally, we assess the importance of image patches by combining objective and subjective user attention, adjusting energy usage for transmitting semantic information. This strategy optimizes resource utilization, ensuring precise transmission of critical information. We construct a Visual Question Answering (VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental results show that our semantic communication framework significantly increases accuracy in answering questions under the same channel conditions, performing particularly well in environments with poor Signal-to-Noise Ratios (SNR). Accuracy can be improved by 13.4% at an SNR of 12 dB and 33.1% at 10 dB, respectively.},
  archive      = {J_TMC},
  author       = {Baoxia Du and Hongyang Du and Dusit Niyato and Ruidong Li},
  doi          = {10.1109/TMC.2025.3564543},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9822-9836},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Task-oriented semantic communication in large multimodal models-based vehicle networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RadarODE: An ODE-embedded deep learning model for contactless ECG reconstruction from millimeter-wave radar. <em>TMC</em>, <em>24</em>(10), 9806-9821. (<a href='https://doi.org/10.1109/TMC.2025.3563945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radar-based cardiac monitoring has become a popular research direction recently, but the fine-grained electrocardiogram (ECG) signal is still hard to reconstruct from millimeter-wave radar signal. The key obstacle is to decouple cardiac activities in the electrical domain (i.e., ECG) from that in the mechanical domain (i.e., heartbeat), and most existing research only uses purely data-driven methods to map such domain transformation as a black box. Therefore, this work first proposes a signal model that considers the fine-grained cardiac feature sensed by radar, and a novel deep learning framework called radarODE is designed to extract both temporal and morphological features for generating ECG. In addition, ordinary differential equations are embedded in radarODE as a decoder to provide morphological prior, helping the convergence of the model training and improving the robustness under body movements. After being validated on the dataset, the proposed radarODE achieves better performance compared with the benchmark in terms of missed detection rate, root mean square error, Pearson correlation coefficient with improvements of 9%, 16% and 19%, respectively. The validation results imply that radarODE is capable of recovering ECG signals from radar signals with high fidelity and can potentially be implemented in real-life scenarios.},
  archive      = {J_TMC},
  author       = {Yuanyuan Zhang and Runwei Guan and Lingxiao Li and Rui Yang and Yutao Yue and Eng Gee Lim},
  doi          = {10.1109/TMC.2025.3563945},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9806-9821},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RadarODE: An ODE-embedded deep learning model for contactless ECG reconstruction from millimeter-wave radar},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AgentsCoMerge: Large language model empowered collaborative decision making for ramp merging. <em>TMC</em>, <em>24</em>(10), 9791-9805. (<a href='https://doi.org/10.1109/TMC.2025.3564163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ramp merging is one of the bottlenecks in traffic systems, which commonly cause traffic congestion, accidents, and severe carbon emissions. In order to address this essential issue and enhance the safety and efficiency of connected and autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel collaborative decision-making framework, named AgentsCoMerge, to leverage large language models (LLMs). Specifically, we first design a scene observation and understanding module to allow an agent to capture the traffic environment. Then we propose a hierarchical planning module to enable the agent to make decisions and plan trajectories based on the observation and the agent’s own state. In addition, in order to facilitate collaboration among multiple agents, we introduce a communication module to enable the surrounding agents to exchange necessary information and coordinate their actions. Finally, we develop a reinforcement reflection guided training paradigm to further enhance the decision-making capability of the framework. Extensive experiments are conducted to evaluate the performance of our proposed method, demonstrating its superior efficiency and effectiveness for multi-agent collaborative decision-making under various ramp merging scenarios.},
  archive      = {J_TMC},
  author       = {Senkang Hu and Zhengru Fang and Zihan Fang and Yiqin Deng and Xianhao Chen and Yuguang Fang and Sam Tak Wu Kwong},
  doi          = {10.1109/TMC.2025.3564163},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9791-9805},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AgentsCoMerge: Large language model empowered collaborative decision making for ramp merging},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive transport with high link utilization using opportunistic packets in cloud data centers. <em>TMC</em>, <em>24</em>(10), 9774-9790. (<a href='https://doi.org/10.1109/TMC.2025.3563182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To meet the stringent demanding low latency and high throughput of cloud datacenter applications, recent receiver-driven transport protocols transmit only one packet once receiving each credit packet from the receiver to achieve ultra-low queueing delay. However, the round-trip time variation and the highly dynamic background traffic significantly deteriorate the performance of receiver-driven transport protocols, resulting in under-utilized bandwidth. This article designs a simple yet effective solution called RPO, which retains the advantages of receiver-driven transmission while efficiently utilizing the available bandwidth. Specifically, RPO rationally uses low-priority opportunistic packets to ensure high network utilization without increasing the queueing delay of high-priority normal packets. Furthermore, to tackle the queueing buildup due to line-rate transmission in the first RTT, we design a selective dropping mechanism called SDM to help the majority of small flows complete within only one RTT by prioritizing the first-RTT bursty packets over the packets triggered by grants. We implement RPO in Linux hosts with DPDK. The experimental results show that RPO significantly improves the network utilization by up to 35% over the state-of-the-art schemes, without introducing additional queueing delay. Moreover, RPO integrated with SDM reduces the AFCT of small flows by up to 45% compared with RPO integrated with Aeolus.},
  archive      = {J_TMC},
  author       = {Jinbin Hu and Jiawei Huang and Zhaoyi Li and Yijun Li and Shuying Rao and Wenchao Jiang and Kai Chen and Jianxin Wang and Tian He},
  doi          = {10.1109/TMC.2025.3563182},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9774-9790},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Proactive transport with high link utilization using opportunistic packets in cloud data centers},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedACS: An adaptive client selection framework for communication-efficient federated graph learning. <em>TMC</em>, <em>24</em>(10), 9760-9773. (<a href='https://doi.org/10.1109/TMC.2025.3563404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated graph learning (FGL) has been proposed to collaboratively train the increasing graph data with graph neural networks (GNNs) in a recommendation system. Nevertheless, implementing an efficient recommendation system with FGL still faces two primary challenges, i.e., limited communication bandwidth and non-IID local graph data. Existing works typically reduce communication frequency or transmission amount, which may suffer significant performance degradation under non-IID settings. Furthermore, some researchers propose to share the underlying structure among clients, which brings massive communication cost. To this end, we propose an efficient FGL framework, named FedACS, which adaptively selects a subset of clients for model training, to alleviate communication overhead and non-IID issues simultaneously. In FedACS, the global GNN model learns significant hidden edges and the structure of graph data among selected clients, enhancing recommendation efficiency. This capability distinguishes it from the traditional FL client selection methods. To optimize the client selection process, we introduce a multi-armed bandit (MAB) based algorithm to select participating clients according to the resource budgets and the training performance (i.e., RMSE). Experimental results indicate that FedACS improves RMSE by 5.4% over baselines with the same resource budget and reduces communication costs by up to 70.7% to achieve the same RMSE performance.},
  archive      = {J_TMC},
  author       = {Hongli Xu and Xianjun Gao and Jianchun Liu and Qianpiao Ma and Liusheng Huang},
  doi          = {10.1109/TMC.2025.3563404},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9760-9773},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedACS: An adaptive client selection framework for communication-efficient federated graph learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid data-driven SSM for interpretable and label-free mmWave channel prediction. <em>TMC</em>, <em>24</em>(10), 9743-9759. (<a href='https://doi.org/10.1109/TMC.2025.3564260'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of mmWave time-varying channels is essential for mitigating the issue of channel aging in highly dynamic scenarios. Existing channel prediction methods have limitations: classical model-based methods often struggle to track highly nonlinear channel dynamics due to limited expert knowledge, while emerging data-driven methods typically require substantial labeled data for effective training and often lack interpretability. To address these issues, this paper proposes a novel hybrid method that integrates a data-driven neural network into a conventional model-based workflow based on a state-space model (SSM), implicitly tracking complex channel dynamics from data without requiring precise expert knowledge. Additionally, a novel unsupervised learning strategy is developed to train the embedded neural network solely with unlabeled data. Theoretical analyses and ablation studies are conducted to interpret the enhanced benefits gained from the hybrid integration. Numerical simulations based on the 3GPP mmWave channel model corroborate the superior prediction accuracy of the proposed method, compared to state-of-the-art methods that are either purely model-based or data-driven. Furthermore, extensive experiments validate its robustness against various challenging factors, including among others severe channel variations.},
  archive      = {J_TMC},
  author       = {Yiyong Sun and Jiajun He and Zhidi Lin and Wenqiang Pu and Feng Yin and Hing Cheung So},
  doi          = {10.1109/TMC.2025.3564260},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9743-9759},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hybrid data-driven SSM for interpretable and label-free mmWave channel prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trusted clustering based federated learning in edge networks. <em>TMC</em>, <em>24</em>(10), 9726-9742. (<a href='https://doi.org/10.1109/TMC.2025.3566492'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is integral to advancing edge intelligence by enabling collaborative machine learning. In FL-empowered edge networks, computing nodes first train local models and then send them to an or multiple aggregation node(s) for global model collaboration. However, the trustworthiness of both local and global models in conventional FL frameworks is compromised due to inadequate model security and transparency. Distributed ledger technique (DLT) can address this issue by leveraging multi-nodes trust capabilities to support distributed consensus. However, model training and consensus performance of DLT may significantly degrade due to instability and resource constraints of edge networks. Sharding technique provides an effective approach by dividing the ledger into smaller and manageable shards. In this paper, to improve model training and consensus performance, we propose a trusted FL framework by incorporating sharding DLT into FL frameworks. We construct a theoretical model to investigate the relationship between model training performance, consensus efficiency, and capacity of edge nodes regarding storage, computing and communications. Based on the theoretical model, we propose a trusted clustering scheme to aggregate local models. Numerical results show that our proposed scheme significantly improves network throughput for transmitting models while guaranteeing model learning performance in comparison with some classical baselines.},
  archive      = {J_TMC},
  author       = {Yi-Jing Liu and Long Zhang and Xiaoqian Li and Hongyang Du and Gang Feng and Shuang Qin and Jiacheng Wang},
  doi          = {10.1109/TMC.2025.3566492},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9726-9742},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Trusted clustering based federated learning in edge networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated unlearning with fast recovery. <em>TMC</em>, <em>24</em>(10), 9709-9725. (<a href='https://doi.org/10.1109/TMC.2025.3563265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent federated unlearning studies mainly focus on removing the target client's contributions from the global model permanently. However, the requirement for accommodating temporary user exits or additions in federated learning has been neglected. In this paper, we propose a novel recoverable federated unlearning scheme, named RFUL, which allows users to remove or add their local model to the global one at any time easily and quickly. It mainly consists of two main components, i.e., knowledge unlearning and knowledge recovery. In knowledge unlearning, the target contributions can be eliminated by training with mislabeled target data, while preserving the non-target contributions through distillation using the original model. In knowledge recovery, the forgotten contributions can be restored by training the target data using classification loss, while the non-target contributions are maintained through feature distillation and parameter freezing on the classifier. Both knowledge unlearning and recovery processes only require the participation of target data, guaranteeing the algorithm's practicality in federated learning systems. Extensive experiments demonstrate the significant efficacy of RFUL. For knowledge unlearning, RFUL matches state-of-the-art methods using only target data, achieving a runtime speedup of 3.3 to 8.7 times compared to retraining across various datasets. For knowledge recovery, RFUL exceeds state-of-the-art incremental learning methods by 5.02% to 29.97% in accuracy and achieves a runtime speedup of 1.8 to 4.4 times compared to retraining on different datasets.},
  archive      = {J_TMC},
  author       = {Changjun Zhou and Chenglin Pan and Minglu Li and Pengfei Wang},
  doi          = {10.1109/TMC.2025.3563265},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9709-9725},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated unlearning with fast recovery},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward universal personalization in federated learning via collaborative foundation generative models. <em>TMC</em>, <em>24</em>(10), 9695-9708. (<a href='https://doi.org/10.1109/TMC.2025.3564880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized federated learning (PFL) enhances the performance of customized client models through collaborative training without compromising data privacy and ownership. Some previous PFL methods rely on rich prior knowledge about the types of data heterogeneity (such as class imbalance or feature skew), which greatly limits their application ranges. In this paper, we study the Universal Personalization in Federated Learning (UniPFL), the problem that has no prior knowledge about the types of data heterogeneity. In real-world PFL scenarios, UniPFL is potential because the data distributions of clients are usually heterogeneous and unknown to the server, where quantity imbalance, class imbalance, feature skew, or hybrid heterogeneity are possible contingencies. To address UniPFL, we propose FedFD, a novel framework with local data augmentation and global concept fusion, which is based on the recent advances in the foundation generative models (e.g., diffusion models, BLIP-2). On the client side, FedFD utilizes a diffusion model to assist local training by generating augmented data samples, and is then efficiently fine-tuned to be personalized. On the server side, we customize the aggregation strategies based on model similarities to learn both personalized models and diverse feature concepts. Extensive experiments show that FedFD reaches the state-of-the-art on (1) CIFAR-10 and CIFAR-100 for class imbalance; (2) DomainNet and Office-10 for feature skew, and (3) hybrid heterogeneity with both class and feature shifts.},
  archive      = {J_TMC},
  author       = {Chenrui Wu and Zexi Li and Fangxin Wang and Hongyang Chen and Jiajun Bu and Haishuai Wang},
  doi          = {10.1109/TMC.2025.3564880},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9695-9708},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward universal personalization in federated learning via collaborative foundation generative models},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Salix-leaf: Find main veins of signal clusters for practical parallel decoding. <em>TMC</em>, <em>24</em>(10), 9683-9694. (<a href='https://doi.org/10.1109/TMC.2025.3562590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel decoding of backscatter improves communication throughput by enabling concurrent transmission of backscatter tags. In practical applications of parallel decoding, it is extremely difficult to distinguish collided signals in superclusters where multiple signal clusters overlap. Existing methods are usually effective for superclusters with uniformly distributed signals. Nevertheless, there are many more scenarios in which signals in superclusters tend to gather unevenly, and existing methods cannot work. Such uneven clustering of signals occurs due to the following two possible causes: (1) signal-strength-differences (SSDs) among tags; or (2) cluster drifting (CD) driven by interferences from other objects within communication environments. This paper proposes a novel scheme called Salix-Leaf, which aims to identify the main veins of signal clusters to address this problem of superclusters with unevenly distributed signals. Salix-Leaf identifies the main vein of each signal cluster for fine-grained clustering so that the direction of the main veins can be used to verify the accuracy of clustering. In addition, Salix-Leaf employs a supercluster decomposer that divides signals into different segments for clustering analysis, enhancing robustness and practicability. Experimental results show that Salix-Leaf achieves a 1.2-fold increase in throughput and a 25% reduction in bit error rate (BER) compared to the state-of-the-art.},
  archive      = {J_TMC},
  author       = {Yajun Li and Jumin Zhao and Dengao Li and Hejun Wu and Shuang Xu and Ruiqin Bai},
  doi          = {10.1109/TMC.2025.3562590},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9683-9694},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Salix-leaf: Find main veins of signal clusters for practical parallel decoding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoE-aware offloading and resource allocation for MEC-empowered AIGC services. <em>TMC</em>, <em>24</em>(10), 9664-9682. (<a href='https://doi.org/10.1109/TMC.2025.3563027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence-Generated Content (AIGC) has emerged as a transformative paradigm, enabling the autonomous creation of diverse content. By offloading model inference tasks to the network edge that is closer to mobile users (MUs), Mobile Edge Computing (MEC) has the potential to significantly enhance the performance of AIGC services. In practice, however, it is challenging to optimally manage MEC-empowered AIGC services, due to the lack of well-defined AIGC-specific metrics, as well as the dynamic workload and computation-intensive nature of AIGC services. In this paper, we first define a novel AIGC metric based on extensive real data experiments, and then study the joint task offloading and resource allocation problem in a generic MEC-empowered AIGC network, where MUs can offload model inference tasks to local or remote Base Stations (BSs), aiming at maximizing their Quality of Experience (QoE). The problem is challenging due to the fast and randomly changing of environments, as well as the necessity for real-time, asynchronous decision-making. To tackle these challenges, we propose two deep reinforcement learning algorithms based on the Proximal Policy Optimization (PPO) framework: Single-Layer PPO (SL-PPO) and Multi-Layer PPO (ML-PPO), designed for slow-changing and fast-changing environments, respectively. In the SL-PPO algorithm, both task offloading and resource allocation decisions are made simultaneously when tasks arrive. In the ML-PPO algorithm, the task offloading decision is made immediately when tasks arrive, while the resource allocation decision is deferred until tasks are scheduled for processing or transmission in the corresponding queues. Simulation results show that (i) both algorithms outperform existing methods in the literature, and can increase the average utility by up to 47% and 48.8%; (ii) both algorithms can effectively manage the trade-off between latency and energy consumption.},
  archive      = {J_TMC},
  author       = {Jiaqi Wu and Xinyi Zhuang and Ming Tang and Lin Gao},
  doi          = {10.1109/TMC.2025.3563027},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9664-9682},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoE-aware offloading and resource allocation for MEC-empowered AIGC services},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Participant recruitment of vehicular crowdsensing along freeways for traffic accident detection. <em>TMC</em>, <em>24</em>(10), 9650-9663. (<a href='https://doi.org/10.1109/TMC.2025.3562565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular crowdsensing provides a new approach for freeway traffic accident detection. However, the uncertainty on traffic accidents and Mobile Users (MUs) brings great challenges for participant recruitment in constructing the deterministic representation of sensing tasks and estimating the participants. To address the challenges, a participant recruitment method for freeway traffic accident detection is proposed. In the method, to deal with the non-deterministic sensing tasks and MUs, the temporal-spatial distribution of accident risk is estimated by optimal transport theory to represent sensing tasks, and the probability distributions of MUs’ trip distance and requested rewards are used to estimate MUs. Then the participant recruitment problem is converted into an optimal coverage problem for accident risk under the macro statistical characteristics of MUs. The participant recruitment model is established to determine the participants by maximizing the coverage rate of accident risk with the budget constraint. And a greedy heuristic strategy is used to solve the model. Simulation experiments are carried out to validate the proposed method. The results show the proposed method is effective and reliable in freeway traffic accident detection.},
  archive      = {J_TMC},
  author       = {Qian Cao and Zhihui Li and Haitao Li and Shirui Zhou and Yunxiang Zhang},
  doi          = {10.1109/TMC.2025.3562565},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9650-9663},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Participant recruitment of vehicular crowdsensing along freeways for traffic accident detection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoS-aware intelligence information sharing requests scheduling in IoV: CPO-based modeling and solution. <em>TMC</em>, <em>24</em>(10), 9636-9649. (<a href='https://doi.org/10.1109/TMC.2025.3565898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the accelerated development of autonomous driving and large language model, blockchain-supported data interaction and artificial intelligence (AI)-assisted performance optimization is the current mainstream research in the Internet of Vehicles (IoV). However, the trial-and-error behavior of the AI algorithm during the training process is a threat to road safety. Therefore, this paper proposes a general constrained policy optimization (CPO)-based modeling and solution for high-dimensional constrained optimization problems. We focus on intelligent driving information sharing in blockchain-enhanced IoV and optimize the service rewards in the sharing requests scheduling problem while ensuring the frequency resource limitation, service quality constraint, and road safety constraint. The constrained state space (CSS) is innovatively proposed to abstract the environment mathematically with the definition of constraint hyperplanes and distance. Accordingly, the constrained Markov Decision process (CMDP) and the optimization problem are formulated. With the practical implementation of the CPO theory, the constrained sharing requests scheduling (CSRS) algorithm is proposed. Ablation experiments are deep reinforcement learning-based methods without using the CSS-based constraint modeling or without using the CPO-based constrained problem solving process. Results show the effectiveness of CSS and CSRS algorithm in improving the policy training efficiency, and the testing results shows excellent generalization ability.},
  archive      = {J_TMC},
  author       = {Yang Gao and Wenjun Wu and Ao Sun and Yang Sun and Teng Sun and Pengbo Si},
  doi          = {10.1109/TMC.2025.3565898},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9636-9649},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoS-aware intelligence information sharing requests scheduling in IoV: CPO-based modeling and solution},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing edge-cloud collaboration with blockchain-assisted digital twin intelligence offloading scheme. <em>TMC</em>, <em>24</em>(10), 9619-9635. (<a href='https://doi.org/10.1109/TMC.2025.3562189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Edge-Cloud Collaborative (ECC) has emerged as an efficient and promising technique to empower various computation-intensive applications in Digital Twin Network (DTN). The integration of ECC with JointCloud and DTN serves to bridge the gap between data analysis and physical states. In ECC, a reliable and optimal task offloading scheme is required to maximize resource utilization and provide satisfying services to End Users (EU). However, existing offloading schemes still face significant challenges, such as the instability and complexity of network topologies, the intricacies of massive data, and the lack of trust among EU. In this paper, we propose an enhancinG edge-clOud collaboraTion wiTh blockchain-assistEd digital twin intelligence offloadiNg scheme (GOTTEN) which transmits large-scale tasks generated by DTs to Edge Station (ES) or Cloud Station (CS) in dynamic DTN scenarios. We first formulate this resource allocation and task offloading problem and provide an appropriate initial solution which guarantees that tasks generated by DTs can be accurately mapped to physical entities, while optimizing block allocation and reducing the decision space of task offloading. Then, we employ the Lagrange Multiplier based Distributed Island model-enhanced Genetic Algorithm (LM-DIGA) to transform our formulated problem into a convex form and achieve an optimal resource allocation under a specific scheme. Additionally, our proposed architecture also leverages blockchain verification mechanisms to enhance system stability, strengthening privacy protection for DT data as well. Finally, extensive simulation results demonstrate that, compared with seven baselines, our proposed scheme achieves a 10 percent the total system delay and privacy overhead with regard to other schemes in ECC.},
  archive      = {J_TMC},
  author       = {Tianyu Li and Xingwei Wang and Rongfei Zeng and Liang Zhao and Ammar Hawbani and Yuxin Zhang and Min Huang},
  doi          = {10.1109/TMC.2025.3562189},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9619-9635},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing edge-cloud collaboration with blockchain-assisted digital twin intelligence offloading scheme},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedRAV: Hierarchically federated region-learning for traffic object classification of personalized autonomous vehicles with guaranteed efficiency. <em>TMC</em>, <em>24</em>(10), 9599-9618. (<a href='https://doi.org/10.1109/TMC.2025.3564402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging federated learning enables distributed autonomous vehicles to train equipped deep learning models collaboratively without exposing their raw data, providing great potential for utilizing explosively growing autonomous driving data. However, considering the complicated traffic environments and driving scenarios, deploying federated learning for autonomous vehicles is inevitably challenged by non-independent and identically distributed (Non-IID) data of vehicles, which may lead to failed convergence and low training accuracy. In this paper, we propose a novel hierarchically Federated Region-learning framework of Autonomous Vehicles (FedRAV) that adaptively divides a large area containing vehicles into sub-regions based on the defined region-wise distance, and achieves personalized vehicular models and regional models. Specifically, the architecture employs a designated hypernetwork to learn personalized mask vectors per vehicle used in the linear combination of models shared by vehicles in the same region. This approach ensures that the updated vehicular model adopts the beneficial models while discarding the unprofitable ones. We validate our FedRAV framework against existing federated learning algorithms on four real-world autonomous driving datasets in various heterogeneous settings. Extensive experiment results demonstrate that FedRAV framework achieves superior performance than the state-of-the-art algorithms, and improves the accuracy by 9.36%.},
  archive      = {J_TMC},
  author       = {Pengzhan Zhou and Yijun Zhai and Yuepeng He and Fang Qu and Zhida Qin and Xianlong Jiao and Fulin Luo and Chao Chen and Songtao Guo},
  doi          = {10.1109/TMC.2025.3564402},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9599-9618},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedRAV: Hierarchically federated region-learning for traffic object classification of personalized autonomous vehicles with guaranteed efficiency},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint positioning and computation offloading in multi-UAV MEC for low latency applications: A proximal policy optimization approach. <em>TMC</em>, <em>24</em>(10), 9584-9598. (<a href='https://doi.org/10.1109/TMC.2025.3562806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) has emerged as a proven solution for reducing communication latency and enhancing user experience in delay-sensitive applications by offloading computation-intensive tasks to edge servers. In future networks, uncrewed aerial vehicles (UAVs), with their flexible deployment and reliable communication capabilities, have the potential to be deployed as aerial MEC servers in areas lacking cellular infrastructure. However, the joint optimization of UAV placement and task offloading poses significant challenges due to the interdependence between communication latency, computational demands, and the resource limitations of UAVs. In this paper, we propose a novel joint optimization framework utilizing proximal policy optimization (PPO) to simultaneously address UAV placement and computation offloading in UAV-enabled MEC networks. The framework dynamically adapts to changing network conditions, minimizing end-to-end latency while balancing computational loads and energy consumption. Extensive simulations demonstrate that the proposed PPO-based approach achieves superior performance compared to conventional optimization methods, with significant improvements in system latency, resource utilization, and network resilience. This work contributes scalable, adaptive solutions for UAV-assisted MEC networks in dynamic environments, enabling robust support for mission-critical and latency-sensitive applications.},
  archive      = {J_TMC},
  author       = {Yuhui Wang and Junaid Farooq and Hakim Ghazzai and Gianluca Setti},
  doi          = {10.1109/TMC.2025.3562806},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9584-9598},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint positioning and computation offloading in multi-UAV MEC for low latency applications: A proximal policy optimization approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HeadMon$^{+}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math>: Domain adaptive head dynamic-based riding maneuver prediction. <em>TMC</em>, <em>24</em>(10), 9570-9583. (<a href='https://doi.org/10.1109/TMC.2025.3562179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-mobility has become a vital means of transportation in recent years, however, it has also resulted in a rise in traffic incidents. Timely tracking and predicting riders’ maneuvers hold the potential to ensure active protection and allow for sufficient time to avert accidents by issuing timely warnings and interventions. We contend that the rider's head dynamics can provide valuable information regarding their subsequent maneuvers. Riders’ traveling habits, however diverse, not to mention the rapidly varying riding environment. The above factors contribute to significant disruptions in the data source, and various micro-mobility forms further exacerbate the issue. We accordingly present HeadMon$^{+}$, which predicts the rider's subsequent maneuver by examining their head dynamics, and it can effectively adapt to various riding conditions and individuals. The system incorporates a deep learning framework with an advanced domain adversarial network. By single-time pre-training, HeadMon$^{+}$ is capable of adapting to new data domains, including human subjects, and riding conditions for robust maneuver prediction. Based on our evaluation, we have found that the maneuver prediction of HeadMon$^{+}$ has an overall precision of 94% with a prediction time gap of 4 seconds. HeadMon$^{+}$'s low cost and rapid response capability make it easily deployed and then contribute to enhancing safe riding.},
  archive      = {J_TMC},
  author       = {Zengyi Han and En Wang and Mohan Yu and Jie Wang and Yuuki Nishiyama and Kaoru Sezaki},
  doi          = {10.1109/TMC.2025.3562179},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9570-9583},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HeadMon$^{+}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mo>+</mml:mo></mml:msup></mml:math>: Domain adaptive head dynamic-based riding maneuver prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tackling resource allocation for decentralized federated learning: A GNN-based approach. <em>TMC</em>, <em>24</em>(10), 9554-9569. (<a href='https://doi.org/10.1109/TMC.2025.3562834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized federated learning (DFL) enables clients to train a neural network model in a device-to-device (D2D) manner without central coordination. In practical systems, DFL faces challenges due to dynamic topology changes, time-varying channel conditions, and limited computational capability of the clients. These factors can affect the learning performance and efficiency of DFL. To address the aforementioned challenges, in this paper, we propose a graph neural network (GNN)–based algorithm to minimize the total delay and energy consumption on training and improve the learning performance of DFL in D2D wireless networks. In our proposed GNN, a multi-head graph attention mechanism is used to capture different features of clients and wireless channels. We design a neighbor selection module which enables each client to select a subset of its neighbors for the participation of model aggregation. We develop a decoder that enables each client to determine its transmit power and computational resource. Experimental results show that our proposed algorithm achieves a lower total delay and energy consumption on training when compared with five baseline schemes. Furthermore, by properly selecting a subset of neighbors for each client, our proposed algorithm achieves similar testing accuracy to the full participation scheme.},
  archive      = {J_TMC},
  author       = {Chuiyang Meng and Ming Tang and Mehdi Setayesh and Vincent W.S. Wong},
  doi          = {10.1109/TMC.2025.3562834},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9554-9569},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Tackling resource allocation for decentralized federated learning: A GNN-based approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of beamforming and trajectory for UAV-RIS-assisted MU-MISO systems using GNN and SD3. <em>TMC</em>, <em>24</em>(10), 9539-9553. (<a href='https://doi.org/10.1109/TMC.2025.3563072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In urban environments, direct communication links between a base station (BS) and user equipment (UEs) are often obstructed by buildings. To mitigate these blockages, we integrate uncrewed aerial vehicles (UAVs) and reconfigurable intelligent surfaces (RISs) to enhance system flexibility and improve transmission efficiency. This paper investigates an RIS-assisted multi-user multiple-input single-output (MU-MISO) downlink system, where the RIS is mounted on a UAV. To maximize the system rate while minimizing the UAV’s energy consumption and flight duration, we formulate a multi-objective optimization problem. To address this problem, we propose a hybrid algorithm that integrates the soft deep deterministic policy gradient (SD3) algorithm with a graph neural network (GNN) architecture, named SD3-GNN-RIS. The original problem is decomposed into two subproblems: joint active beamforming at the BS and passive beamforming at the RIS, optimized via a GNN-based approach, and three-dimensional (3D) UAV trajectory optimization, formulated as a Markov decision process and solved using the SD3 algorithm. Simulation results demonstrate the superior performance of the proposed algorithm compared to baseline methods in terms of system rate, energy efficiency, and UAV trajectory optimization.},
  archive      = {J_TMC},
  author       = {Shumo Wang and Xiaoqin Song and Tiecheng Song and Yang Yang},
  doi          = {10.1109/TMC.2025.3563072},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9539-9553},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of beamforming and trajectory for UAV-RIS-assisted MU-MISO systems using GNN and SD3},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiCG: In-body cardiac motion sensing based on a mix-medium wi-fi fresnel zone model. <em>TMC</em>, <em>24</em>(10), 9524-9538. (<a href='https://doi.org/10.1109/TMC.2025.3564843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cardiovascular diseases (CVDs) are a leading cause of mortality worldwide, highlighting the critical need for accurate and continuous heart health monitoring. Electrocardiograms (ECG), considered as the golden standard for diagnosing and monitoring heart-related conditions, offer precise measurements but require direct skin contact, limiting their practicality for long-term and everyday use. On the other hand, existing RF sensing techniques that analyze signals reflected off the skin struggle to distinguish micro cardiac motions of the heart due to weak motion amplitude and respiration interference at the chest wall. To overcome these limitations, we introduce WiCG, a novel contact-less cardiac motion monitoring system that employs 2.4 GHz Wi-Fi signals to penetrate the chest and detect subtle cardiac movements. A mix-medium Wi-Fi Fresnel zone model is developed to explain the enhanced phase sensitivity of in-body Wi-Fi signals, which is crucial for accurately detecting cardiac motions. By strategically positioning antennas near the heart, WiCG captures ventricular motions effectively. A novel cardiac Doppler method is proposed to suppress phase noise and interference from static paths and extract the time interval between the systole and diastole of the ventricular. Extensive experiments demonstrate that the proposed system can robustly estimate the R-R and Q-T intervals of human cardiac cycles across 21 subjects and different environments with an average accuracy of 99.22% and 92.8%, achieving performance comparable to ECG.},
  archive      = {J_TMC},
  author       = {Pei Wang and Anlan Yu and Xujun Ma and Rong Zheng and Jingfu Dong and Zhaoxin Chang and Duo Zhang and Djamal Zeghlache and Daqing Zhang},
  doi          = {10.1109/TMC.2025.3564843},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9524-9538},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WiCG: In-body cardiac motion sensing based on a mix-medium wi-fi fresnel zone model},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiCast: Parallel cross-technology transmission for connecting heterogeneous IoT devices. <em>TMC</em>, <em>24</em>(10), 9506-9523. (<a href='https://doi.org/10.1109/TMC.2025.3564340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-Technology Communication (CTC) is an emerging technique that enables direct interconnection among incompatible wireless technologies. However, for the downlink from WiFi to multiple IoT technologies, serially emulating and transmitting the data of each IoT technology has extremely low spectrum efficiency. In this paper, we propose WiCast, a parallel CTC that uses IEEE 802.11ax to emulate a composite signal that can be received by commodity BLE, ZigBee, and LoRa devices. By taking advantage of OFDMA in 802.11ax, WiCast uses a single Resource Unit (RU) for parallel CTC and sets other RUs free for high-rate WiFi users. But such a sophisticated composite signal is very easily distorted by emulation imperfections, dynamic channel noises, cyclic prefix, and center frequency offset. We propose a CTC link model that jointly models the emulation errors and channel distortions. Then we carve the emulated signal with elaborate compensations in both time and frequency domains. Based on the proposed CTC scheme, a unified Media Access Control approach is introduced to discover and synchronize the heterogeneous IoT devices. We implement a prototype of WiCast using USRP N210 platform along with commodity ZigBee, BLE, and LoRa devices. The extensive experiments demonstrate WiCast can achieve an efficient parallel transmission with the aggregated goodput up to $ 390.24 \;\text{kbps}$.},
  archive      = {J_TMC},
  author       = {Dan Xia and Xiaolong Zheng and Liang Liu and Shanguo Huang and Huadong Ma},
  doi          = {10.1109/TMC.2025.3564340},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9506-9523},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WiCast: Parallel cross-technology transmission for connecting heterogeneous IoT devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the robustness: Hierarchical federated learning framework for object detection of UAV cluster. <em>TMC</em>, <em>24</em>(10), 9489-9505. (<a href='https://doi.org/10.1109/TMC.2025.3562812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of Unmanned Aerial Vehicle (UAV) cluster is an available solution for object detection missions. In the harsh environment, UAV cluster could suffer from some significant threats (e.g., forest fire hazards, electromagnetic interference, and ground-to-air attacks), which could lead to the destruction of UAVs and loss of data. To this end, we propose a Hierarchical Federated Learning Framework for Object Detection (HFL-OD) to enhance the robustness of UAV cluster conducting object detection missions. In HFL-OD, UAVs are grouped through a Three-Dimensional (3D) graph coloring method, and an intragroup backup mechanism is provided to prevent the data loss caused by the destruction of UAVs. Besides, a dynamic server selection mechanism deals with the potential destruction of servers (cluster server and group servers) by adaptively reassigning the server roles. To further improve the robustness and mission efficiency of UAV cluster, a two-tier federated learning framework is introduced to make a proper trade-off between object detection accuracy and communication/computational overhead. This framework is built on the concept of hierarchical federated learning by implementing both intragroup parameter aggregation and global parameter aggregation. Extensive simulations and comparisons demonstrate the superior performance of our proposed HFL-OD, i.e., the robustness of UAV cluster conducting object detection missions can be significantly improved, and the communication/computational overhead is effectively reduced.},
  archive      = {J_TMC},
  author       = {Xingyu Li and Wenzhe Zhang and Linfeng Liu and Jia Xu},
  doi          = {10.1109/TMC.2025.3562812},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9489-9505},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring the robustness: Hierarchical federated learning framework for object detection of UAV cluster},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SWIPTNet: A unified deep learning framework for SWIPT based on GNN and transfer learning. <em>TMC</em>, <em>24</em>(10), 9477-9488. (<a href='https://doi.org/10.1109/TMC.2025.3563892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the deep learning based approaches for simultaneous wireless information and power transfer (SWIPT). The quality-of-service (QoS) constrained sum-rate maximization problems are, respectively, formulated for power-splitting (PS) receivers and time-switching (TS) receivers and solved by a unified graph neural network (GNN) based model termed SWIPT net (SWIPTNet). To improve the performance of SWIPTNet, we first propose a single-type output method to reduce the learning complexity and facilitate the satisfaction of QoS constraints, and then, utilize the Laplace transform to enhance input features with the structural information. Besides, we adopt the multi-head attention and layer connection to enhance feature extracting. Furthermore, we present the implementation of transfer learning to the SWIPTNet between PS and TS receivers. Ablation studies show the effectiveness of key components in the SWIPTNet. Numerical results also demonstrate the capability of SWIPTNet in achieving near-optimal performance with millisecond-level inference speed which is much faster than the traditional optimization algorithms. We also show the effectiveness of transfer learning via fast convergence and expressive capability improvement.},
  archive      = {J_TMC},
  author       = {Hong Han and Yang Lu and Zihan Song and Ruichen Zhang and Wei Chen and Bo Ai and Dusit Niyato and Dong In Kim},
  doi          = {10.1109/TMC.2025.3563892},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9477-9488},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SWIPTNet: A unified deep learning framework for SWIPT based on GNN and transfer learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EchoExpress: Facial expression recognition in the wild via acoustic sensing on smart glasses. <em>TMC</em>, <em>24</em>(10), 9458-9476. (<a href='https://doi.org/10.1109/TMC.2025.3566341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately recognizing facial expressions and emotions at any time and in any place can significantly improve people’s quality of life and mental well-being. However, existing methods lack the convenient capability for long-term monitoring in the wild environment. In this paper, we introduce EchoExpress, an in-the-wild emotion-related facial expression recognition system that works in an unobtrusive, low-power, and privacy-friendly way. EchoExpress uses two speakers and two microphones mounted on a glass-frame for transmitting and receiving mutually orthogonal wave signals. Concurrently, a unique attention mechanism dynamically extracts crucial features, enabling the capture of nuanced facial expressions and emotions. Furthermore, we introduce an open-set filtering mechanism with a specially designed loss function, which effectively filters out irrelevant actions, thereby reducing the risk of misidentification. Finally, a semi-supervised training method is employed to address the significant variability in wild expressions across different individuals. In extensive testing, EchoExpress achieves an accuracy of 84% in a laboratory environment and over 75% in real-world conditions. We believe that EchoExpress can serve as an unobtrusive and reliable way to monitor facial expressions.},
  archive      = {J_TMC},
  author       = {Kaiyi Guo and Qian Zhang and Dong Wang},
  doi          = {10.1109/TMC.2025.3566341},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9458-9476},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EchoExpress: Facial expression recognition in the wild via acoustic sensing on smart glasses},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AegisRAN: A fair and energy-efficient computing resource allocation framework for vRANs. <em>TMC</em>, <em>24</em>(10), 9441-9457. (<a href='https://doi.org/10.1109/TMC.2025.3564116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The virtualization of Radio Access Networks (vRAN) is rapidly becoming a reality, driven by the increasing need for flexible, scalable, and cost-effective mobile network solutions. To mitigate energy efficiency concerns in vRAN deployments, two approaches are gaining attention: ($i$) sharing computing infrastructure among multiple virtualized base stations (vBSs); and ($ii$) relying upon general-purpose, low-cost CPUs. However, effectively realizing these approaches poses several challenges. In this paper, we first conduct a comprehensive experimental campaign on a vRAN platform to characterize the impact of computing and radio resource allocation on energy consumption and performance across various network contexts. This analysis reveals several key issues. First, determining the optimal allocation of computing resources is difficult because it depends on the context of each vBS (e.g., traffic load, channel quality) in a non-trivial and non-linear manner. Second, suboptimal resource assignment can lead to increased energy consumption or, even worse, degradation of users’ Quality of Service. Third, the high dimensionality of the solution space hinders the effectiveness of traditional optimization or learning methods. To tackle these challenges, we propose AegisRAN, a framework for optimizing computing resource allocation in vRAN. AegisRAN addresses the dual objective of minimizing energy consumption while maintaining high system reliability. Moreover, when computing resources are overbooked, our solution ensures a fair resource partition based on vBS performance. AegisRAN leverages a discrete soft actor-critic algorithm combined with several techniques, including multi-step decision-making, action masking, digital twin-based training, and a tailored reward signal that mitigates feedback sparsity. Our evaluations demonstrate that AegisRAN achieves near-optimal performance and offers high flexibility across diverse network contexts and varying numbers of vBSs, with up to 25% improvement in energy savings compared to baseline solutions in medium-scale scenarios.},
  archive      = {J_TMC},
  author       = {Ethan Sanchez Hidalgo and Jose A. Ayala-Romero and Josep Xavier Salvat Lozano and Andres Garcia-Saavedra and Xavier Costa Perez},
  doi          = {10.1109/TMC.2025.3564116},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9441-9457},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AegisRAN: A fair and energy-efficient computing resource allocation framework for vRANs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REPLAY: Modeling time-varying temporal regularities of human mobility for location prediction over sparse trajectories. <em>TMC</em>, <em>24</em>(10), 9428-9440. (<a href='https://doi.org/10.1109/TMC.2025.3562669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-location prediction aims to forecast which location a user is most likely to visit given the user’s historical data. As a sequence modeling problem by nature, it has been widely addressed using Recurrent Neural Networks (RNNs). To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by integrating them into the RNN units as additional information, or utilizing them to search for informative historical hidden states to improve prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other time periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Under this circumstance, we propose REPLAY, learning to capture the time-varying temporal regularities for location prediction based on general RNN architecture. Specifically, REPLAY is designed on top of a flashback mechanism, where the spatiotemporal distances in sparse trajectories are used to search for the informative past hidden states; to accommodate the time-varying temporal regularities, REPLAY incorporates smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. We conduct a comprehensive evaluation, comparing REPLAY against a wide range of state-of-the-art methods. Experimental results show REPLAY significantly and consistently outperforms state-of-the-art methods by 7.7%–10.5% in the location prediction task, and the learnt bandwidths reveal interesting patterns of the time-varying temporal regularities.},
  archive      = {J_TMC},
  author       = {Bangchao Deng and Bingqing Qu and Pengyang Wang and Dingqi Yang and Benjamin Fankhauser and Philippe Cudre-Mauroux},
  doi          = {10.1109/TMC.2025.3562669},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9428-9440},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {REPLAY: Modeling time-varying temporal regularities of human mobility for location prediction over sparse trajectories},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-enhanced representation learning for road networks with temporal dynamics. <em>TMC</em>, <em>24</em>(10), 9413-9427. (<a href='https://doi.org/10.1109/TMC.2025.3562656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of mobile devices and positioning technology has resulted in the generation of massive urban data, offering great opportunities to improve analytical abilities for urban infrastructure components. In this study, we introduce a novel framework called Toast for learning general-purpose representations of road networks, along with its advanced counterpart DyToast, designed to enhance the integration of temporal dynamics to boost the performance of various time-sensitive downstream tasks. Specifically, we propose to encode two pivotal semantic characteristics intrinsic to road networks: traffic patterns and traveling semantics. To achieve this, we refine the skip-gram module by incorporating auxiliary objectives aimed at predicting the traffic context associated with a target road segment. Moreover, we leverage mobile trajectory data and design pre-training strategies based on Transformer to distill traveling semantics on road networks. DyToast further augments this framework by employing unified trigonometric functions characterized by their beneficial properties, enabling the capture of temporal evolution and dynamic nature of road networks more effectively. With these proposed techniques, we can obtain representations that encode multi-faceted aspects of knowledge within road networks, applicable across both road segment-based applications and trajectory-based applications. Extensive experiments on two real-world datasets across three tasks demonstrate that our proposed framework consistently outperforms the state-of-the-art baselines by a significant margin.},
  archive      = {J_TMC},
  author       = {Yile Chen and Xiucheng Li and Gao Cong and Zhifeng Bao and Cheng Long},
  doi          = {10.1109/TMC.2025.3562656},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9413-9427},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Semantic-enhanced representation learning for road networks with temporal dynamics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive dynamic scaling and request routing optimization in the multi-edge cluster collaboration. <em>TMC</em>, <em>24</em>(10), 9395-9412. (<a href='https://doi.org/10.1109/TMC.2025.3562209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid proliferation of mobile devices, a growing number of intelligent applications are being deployed at the network edge, placing immense strain on the processing capabilities of edge computing. Therefore, resource-constrained edge servers frequently experience overload due to highly dynamic workloads. To address this, one approach involves forwarding user requests to the cloud or other edge servers, albeit at the cost of increased transmission latency. Alternatively, dynamic scaling of edge clusters can be employed to enhance processing capacity, thereby mitigating latency but at the expense of additional service configuration and hosting expenses. By integrating their complementary benefits, we study the joint optimization problem of dynamic scaling and request routing within a multi-edge cluster collaborative framework, which fully exploits cluster resources to manage the temporal and spatial varying edge workloads. This collaborative framework aims to minimize overall request latency while satisfying an acceptable time-averaged budget cost. However, the complex coupling between scaling and routing decisions, along with the uncertainty of future system information (e.g., user request workloads) impedes the derivation of an optimal offline policy over the long term. Thus, considering the different decision granularities, we employ the two-timescale Lyapunov optimization technique to decouple the original problem into a series of independent online optimization problems with the current system state. In particular, we make cluster scaling decisions in each large timescale and request routing decisions in each small timescale. Given that the decoupled large-timescale subproblems involve NP-hard mixed-integer linear programming, we design an edge resource-aware greedy rounding algorithm to efficiently produce approximate optimal solutions. Finally, both rigorous theoretical analysis and extensive trace-driven evaluations demonstrate the superiority of our proposed algorithm over its counterparts.},
  archive      = {J_TMC},
  author       = {Pu Wang and Tao Ouyang and Jie Gong and Chao Hong and Xu Chen},
  doi          = {10.1109/TMC.2025.3562209},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9395-9412},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive dynamic scaling and request routing optimization in the multi-edge cluster collaboration},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SignCRF: Scalable channel-agnostic data-driven radio authentication system. <em>TMC</em>, <em>24</em>(10), 9383-9394. (<a href='https://doi.org/10.1109/TMC.2025.3564556'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio Frequency Fingerprinting through Deep Learning (RFFDL) is a data-driven IoT authentication technique that leverages the unique hardware-level manufacturing imperfections associated with a particular device to recognize (“fingerprint”) the device itself based on variations introduced in the transmitted waveform. Key impediments in developing robust and scalable Radio Frequency Fingerprinting through Deep Learning (RFFDL) techniques that are practical in dynamic and mobile environments are the non-stationary behavior of the wireless channel and other impairments introduced by the propagation conditions. To date, the existing RFFDL-based techniques have only been able to demonstrate a desirable performance when the training and testing environment remains the same, which makes the solutions impractical. SignCRF brings to the RFFDL landscape what it has been missing so far: a scalable, channel-agnostic data-driven radio authentication platform with unmatched precision in fingerprinting wireless devices based on their unique manufacturing impairments that is independent of the dynamic nature of the environment or channel irregularities caused by mobility. SignCRF consists of: (i) a classifier developed in a base-environment with minimum channel dynamics, and finely trained to authenticate devices with high accuracy and at scale; (ii) an environment translator that is carefully designed and trained to remove the dynamic channel impact from RF signals while maintaining the radio's specific “signature”; and (iii) a Max Rule module that selects the highest precision authentication technique between the baseline classifier and the environment translator per radio. We design, train, and validate the performance of SignCRF for multiple technologies in dynamic environments and at scale (100 LoRa and 20 WiFi devices, the largest datasets available in the literature). We assess the scalability of SignCRF across various testbed scales by validating our system using small, medium, and large-scale testbeds, with sizes of 5, 20, and 100 devices, respectively. We demonstrate that SignCRF can significantly improve the RFFDL performance by achieving as high as 100% correct authentication for WiFi devices and 80% correctly authenticated LoRa devices, a 5x and 8x improvement when compared to the state-of-the-art respectively. Furthermore, we show that SignCRF is resilient to adversarial actions by reducing the device recognition accuracy from 73% to 6%, which translates into zero mis-authentication of adversary radios that try to impersonate legitimate devices, which has not been achieved by any prior RFFDL techniques.},
  archive      = {J_TMC},
  author       = {Amani Al-Shawabka and Philip Pietraski and Sudhir B Pattar and Pedram Johari and Tommaso Melodia},
  doi          = {10.1109/TMC.2025.3564556},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9383-9394},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SignCRF: Scalable channel-agnostic data-driven radio authentication system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BAT: A versatile bipartite attention-based approach for comprehensive truth inference in mobile crowdsourcing. <em>TMC</em>, <em>24</em>(10), 9368-9382. (<a href='https://doi.org/10.1109/TMC.2025.3563345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of smart mobile devices has catalyzed the growth of Mobile CrowdSourcing (MCS) as a distributed problem-solving paradigm. MCS platforms heavily rely on advanced truth inference techniques to extract reliable information from diverse and potentially noisy crowd-contributed data. Existing truth inference models often made simplified assumptions about workers or tasks, employing complex Bayesian models or stringent data aggregation methods. These approaches tend to be task-specific, primarily limited to categorical labeling, making adaptations to other mobile computing scenarios labor-intensive. To address these limitations, we introduce the Bipartite Attention-driven Truth (BAT), a versatile approach tailored for mobile computing environments. BAT utilizes an Attributed Bipartite Graph (ABG) to holistically model the MCS process, with workers and tasks as nodes connected by edges representing answer-specific attributes. The approach employs a bipartite graph neural network with an innovative attention mechanism to assess the importance of different answers. BAT extends beyond categorical tasks to support numerical ones by incorporating novel feature representations and model extensions. Theoretical analyses clarify the link between answer similarity and worker expertise. Extensive experiments using diverse real-world datasets demonstrate BAT's superior performance compared to state-of-the-art categorical and numerical truth inference models, highlighting its effectiveness in mobile computing scenarios.},
  archive      = {J_TMC},
  author       = {Jiacheng Liu and Feilong Tang and Hao Liu and Long Chen and Yichuan Yu and Yanmin Zhu and Jiadi Yu and Xiaofeng Hou and Pheng-Ann Heng},
  doi          = {10.1109/TMC.2025.3563345},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9368-9382},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {BAT: A versatile bipartite attention-based approach for comprehensive truth inference in mobile crowdsourcing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical split federated learning: Convergence analysis and system optimization. <em>TMC</em>, <em>24</em>(10), 9352-9367. (<a href='https://doi.org/10.1109/TMC.2025.3565509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As AI models expand in size, it has become increasingly challenging to deploy federated learning (FL) on resource-constrained edge devices. To tackle this issue, split federated learning (SFL) has emerged as an FL framework with reduced workload on edge devices via model splitting; it has received extensive attention from the research community in recent years. Nevertheless, most prior works on SFL focus only on a two-tier architecture without harnessing multi-tier cloud-edge computing resources. In this paper, we intend to analyze and optimize the learning performance of SFL under multi-tier systems. Specifically, we propose the hierarchical SFL (HSFL) framework and derive its convergence bound. Based on the theoretical results, we formulate a joint optimization problem for model splitting (MS) and model aggregation (MA). To solve this rather hard problem, we then decompose it into MS and MA sub-problems that can be solved via an iterative descending algorithm. Simulation results demonstrate that the tailored algorithm can effectively optimize MS and MA in multi-tier systems and significantly outperform existing schemes.},
  archive      = {J_TMC},
  author       = {Zheng Lin and Wei Wei and Zhe Chen and Chan-Tong Lam and Xianhao Chen and Yue Gao and Jun Luo},
  doi          = {10.1109/TMC.2025.3565509},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9352-9367},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hierarchical split federated learning: Convergence analysis and system optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IvyCross: A privacy-preserving and concurrency control framework for blockchain interoperability. <em>TMC</em>, <em>24</em>(10), 9334-9351. (<a href='https://doi.org/10.1109/TMC.2025.3562875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interoperability is a fundamental challenge for long-envisioned blockchain applications. A mainstream approach is using Trusted Execution Environment (TEE) to support interoperable off-chain execution. However, this incurs multiple TEE configured with non-trivial storage capabilities running on fragile concurrent processing environments, rendering current strategies based on TEE far from being practical. This paper aims to fill this gap and design a practical interoperability mechanism with simplified TEE as the underlying architecture. Specifically, we present IvyCross, a TEE-based framework that achieves low-cost, privacy-preserving, and race-free blockchain interoperability. IvyCross allows running arbitrary smart contracts across heterogeneous blockchains atop two distributed TEE-powered hosts. We design an incentive scheme based on smart contracts to stimulate the honest behavior of two hosts, bypassing the requirement of the number of TEE and large memory need. We examine the conditions to guarantee the uniqueness of Nash Equilibrium via Game Theory. Furthermore, an extended optimistic concurrency control protocol is designed to ensure the correctness of concurrent contracts execution. We formally prove the security of IvyCross in the Universal Composability (UC) framework and implement a prototype atop Bitcoin, Ethereum, and FISCO BOCS. Extensive experimental results on end-to-end performance and concurrency control demonstrate the efficiency and practicality of IvyCross.},
  archive      = {J_TMC},
  author       = {Ming Li and Jian Weng and Jiasi Weng and Yi Li and Yongdong Wu and Dingcheng Li and Guowen Xu and Robert H. Deng},
  doi          = {10.1109/TMC.2025.3562875},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9334-9351},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {IvyCross: A privacy-preserving and concurrency control framework for blockchain interoperability},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergy: Towards on-body AI via tiny AI accelerator collaboration on wearables. <em>TMC</em>, <em>24</em>(10), 9319-9333. (<a href='https://doi.org/10.1109/TMC.2025.3564314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of tiny artificial intelligence (AI) accelerators enables AI to run at the extreme edge, offering reduced latency, lower power cost, and improved privacy. When integrated into wearable devices, these accelerators open exciting opportunities, allowing various AI apps to run directly on the body. We present Synergy that provides AI apps with best-effort performance via system-driven holistic collaboration over AI accelerator-equipped wearables. To achieve this, Synergy provides device-agnostic programming interfaces to AI apps, giving the system visibility and controllability over the app's resource use. Then, Synergy maximizes the inference throughput of concurrent AI models by creating various execution plans for each app considering AI accelerator availability and intelligently selecting the best set of execution plans. Synergy further improves throughput by leveraging parallelization opportunities over multiple computation units. Our evaluations with 7 baselines and 8 models demonstrate that, on average, Synergy achieves a 23.0× improvement in throughput, while reducing latency by 73.9% and power consumption by 15.8%, compared to the baselines.},
  archive      = {J_TMC},
  author       = {Taesik Gong and SiYoung Jang and Utku Günay Acer and Fahim Kawsar and Chulhong Min},
  doi          = {10.1109/TMC.2025.3564314},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9319-9333},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Synergy: Towards on-body AI via tiny AI accelerator collaboration on wearables},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPViT: Accelerate vision transformer inference on mobile devices via adaptive splitting and offloading. <em>TMC</em>, <em>24</em>(10), 9303-9318. (<a href='https://doi.org/10.1109/TMC.2025.3562721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Vision Transformer (ViT), which benefits from utilizing self-attention mechanisms, has demonstrated superior accuracy compared to CNNs. However, due to the expensive computational costs, deploying and inferring ViTs on resource-constrained mobile devices has become a challenge. To resolve this challenge, we conducted an empirical analysis to identify performance bottlenecks in deploying ViTs on mobile devices and explored viable solutions. In this paper, we propose SPViT, an adaptive split and offloading method that accelerates ViT inference on mobile devices. SPViT executes collaborative inference of ViT across available edge devices. We introduce a fine-grained splitting technique for the vision transformer structure. Furthermore, we propose an algorithm based on the Auto Regression model to predict partition latency and adaptive offload partitions. Finally, we design offline and online optimization methods to minimize the computational and communication overhead on each device. Based on real-world prototype experiments, SPViT effectively reduces inference latency by 2.2x to 3.3x across four state-of-the-art models.},
  archive      = {J_TMC},
  author       = {Sifan Zhao and Tongtong Liu and Hai Jin and Dezhong Yao},
  doi          = {10.1109/TMC.2025.3562721},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9303-9318},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SPViT: Accelerate vision transformer inference on mobile devices via adaptive splitting and offloading},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent reinforcement learning for task offloading in crowd-edge computing. <em>TMC</em>, <em>24</em>(10), 9289-9302. (<a href='https://doi.org/10.1109/TMC.2025.3531793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Crowd-edge (CE) computing paradigm facilitates the utilization of the computational resources through simultaneously relying the edge computing and the collaboration among various mobile devices (MDs). Most existing works, focusing on offloading tasks from device to edge servers by centralized solutions, are unable to distribute tasks to massive MDs in CE. Meanwhile, designing a decentralized task offloading solution enabling task subscribers to individually make offloading decisions can be challenging given the randomness of crowd resource provisioning and limited knowledge of global status variations. In this paper, we propose a decentralized crowd-edge task offloading solution that enables users to optimally offload tasks to the CE in a distributed manner. Specifically, we formulate the corresponding problem as a stochastic optimization with partially observable status. By observing network and process delays at the crowd side, we further reform the optimization forms and provide a novel approximation policy, enabling users to optimize their offloading strategy based on local observations without interaction with each other. We then solve this task offloading problem by developing a Mixed Multi-Agent Proxy Policy Optimization algorithm (mixed MAPPO). Extensive testing, including numerical and system-level simulations, was conducted to validate the performance of the proposed algorithm in terms of task delay (including the processing delay and transmission delay), load rate, and resource utilization.},
  archive      = {J_TMC},
  author       = {Su Yao and Mu Wang and Ju Ren and Tianyu Xia and Weiqiang Wang and Ke Xu and Mingwei Xu and Hongke Zhang},
  doi          = {10.1109/TMC.2025.3531793},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {10},
  number       = {10},
  pages        = {9289-9302},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-agent reinforcement learning for task offloading in crowd-edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward diverse tiny-model selection for microcontrollers. <em>TMC</em>, <em>24</em>(9), 9273-9288. (<a href='https://doi.org/10.1109/TMC.2025.3561778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabling efficient and accurate deep neural network (DNN) inference on microcontrollers is challenging due to their constrained on-chip resources. Existing approaches mainly focus on compressing larger models, often compromising model accuracy as a trade-off. In this paper, we rethink the problem from the inverse perspective by directly constructing small/weak models, then enhancing their accuracy. Thus, we propose DiTMoS, a novel DNN training and inference framework featuring a selector-classifiers architecture, where the selector routes each input sample to the appropriate classifier for classification. DiTMoS is built on a key insight: a combination of weak models can exhibit high diversity and the union of them can significantly raise the upper bound of overall accuracy. To approach the upper bound, DiTMoS introduces three strategies including diverse training data splitting to enhance the classifiers’ diversity, adversarial selector-classifiers training to ensure synergistic interactions thereby maximizing their complementarity, and heterogeneous feature aggregation to improve the capacity of classifiers. We further design a network slicing technique to eliminate the extra memory consumption incurred by feature aggregation. We deploy DiTMoS on the Nucleo STM32F767ZI board and evaluate its performance across three time-series datasets for human activity recognition, keyword spotting, and emotion recognition tasks. The experimental results show that: (a) DiTMoS improves accuracy by up to 13.4% compared to the best baseline; (b) network slicing successfully eliminates the memory overhead introduced by feature aggregation, with only a minimal increase in latency.},
  archive      = {J_TMC},
  author       = {Xiao Ma and Shengfeng He and Hezhe Qiao and Dong Ma},
  doi          = {10.1109/TMC.2025.3561778},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9273-9288},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward diverse tiny-model selection for microcontrollers},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrossTrace: Privacy-aware cross-system trajectory recovery via hybrid split and federated learning. <em>TMC</em>, <em>24</em>(9), 9255-9272. (<a href='https://doi.org/10.1109/TMC.2025.3561791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive urban-scale vehicle trajectories benefit various downstream applications. However, trajectories collected from existing sensing systems are often incomplete, necessitating the recovery of coarse-grained trajectories. Considering that mobility knowledge learned from a single system is less representative of all vehicles or covers only partial road segments, it becomes essential to combine diverse data from multiple systems to support trajectory recovery. Therefore, we learn the impacts of mobility intentions and dynamic traffic conditions on the movement of vehicles from trajectories aggregated across different systems to recover their travel routes on unobservable road intersections. Nonetheless, aggregating raw data across multiple systems raises privacy concerns. This data isolation compounds challenges in acquiring comprehensive mobility intentions and traffic conditions, thereby impairing recovery performance. In this paper, we propose CrossTrace, a two-stage framework for privacy-aware cross-system trajectory recovery: in the Traffic Condition Inference stage, a Split Learning pipeline with a multi-view graph neural network is utilized to infer complete traffic conditions for all road segments; in the Trajectory Recovery stage, a Federated Learning pipeline with dedicated modules is utilized to recover missing points by fusing inferred traffic conditions and mobility intentions. Extensive experiments on two large-scale trajectory datasets demonstrate that CrossTrace outperforms all alternative schemes.},
  archive      = {J_TMC},
  author       = {Zijian Cao and Dong Zhao and Qiyue Wang and Haitao Yuan and Huadong Ma and Shui Yu},
  doi          = {10.1109/TMC.2025.3561791},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9255-9272},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CrossTrace: Privacy-aware cross-system trajectory recovery via hybrid split and federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Baton: Compensate for missing wi-fi features for practical device-free tracking. <em>TMC</em>, <em>24</em>(9), 9238-9254. (<a href='https://doi.org/10.1109/TMC.2025.3559927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi contact-free sensing systems have attracted widespread attention due to their ubiquity and convenience. The integrated sensing and communication (ISAC) technology utilizes off-the-shelf Wi-Fi communication signals for sensing, which further promotes the deployment of intelligent sensing applications. However, current Wi-Fi sensing systems often require prolonged and unnecessary communication between transceivers, and brief communication interruptions will lead to significant performance degradation. This paper proposes Baton, the first system capable of accurately tracking targets even under severe Wi-Fi feature deficiencies. To be specific, we explore the relevance of the Wi-Fi feature matrix from both horizontal and vertical dimensions. The horizontal dimension reveals feature correlation across different Wi-Fi links, while the vertical dimension reveals feature correlation among different time slots. Based on the above principle, we propose the Simultaneous Tracking And Predicting (STAP) algorithm, which enables the seamless transfer of Wi-Fi features over time and across different links, akin to passing a baton. We implement the system on commercial devices, and the experimental results show that our system outperforms existing solutions with a median tracking error of $ 0.46$ m, even when the communication duty cycle is as low as 20.00%. Compared with the state-of-the-art, our system reduces the tracking error by 79.19% in scenarios with severe Wi-Fi feature deficiencies.},
  archive      = {J_TMC},
  author       = {Yiming Zhao and Xuanqi Meng and Xinyu Tong and Xiulong Liu and Xin Xie and Wenyu Qu},
  doi          = {10.1109/TMC.2025.3559927},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9238-9254},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Baton: Compensate for missing wi-fi features for practical device-free tracking},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicular edge computing networks optimization via DRL-based communication resource allocation and load balancing. <em>TMC</em>, <em>24</em>(9), 9222-9237. (<a href='https://doi.org/10.1109/TMC.2025.3559707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolution of the Internet of vehicles (IoV), the increasing demand for vehicular computation tasks presents significant challenges, particularly in the context of constrained local computation resources and high processing delays. To mitigate these challenges, multi-access edge computing (MEC) offers a potential solution by leveraging edge servers for low-latency processing. However, it also encounters issues such as sub-channel competition and workload imbalance owing to the uneven distribution of vehicle densities. This paper introduces a novel IoV architecture that incorporates multi-task and multi-roadside unit (RSU) capabilities, enabling edge-to-edge collaboration for efficient task offloading among RSUs. The optimization problem is formulated with the objective of minimizing the overall task delay, which is further divided into two sub-problems: communication resource allocation and load balancing. Considering the non-deterministic polynomial (NP)-hard nature of these sub-problems, we propose a two-stage deep reinforcement learning-based communication resource allocation and load balancing (DRLCL) algorithm to address them sequentially. Based on realistic vehicle trajectories, comprehensive evaluation results demonstrate the superiority of the proposed algorithm in reducing system delay compared to existing state-of-the-art baselines, offering an effective approach for optimizing the performance of vehicular edge computing (VEC) networks.},
  archive      = {J_TMC},
  author       = {Quan Chen and Xiaoqin Song and Tiecheng Song and Yang Yang},
  doi          = {10.1109/TMC.2025.3559707},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9222-9237},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Vehicular edge computing networks optimization via DRL-based communication resource allocation and load balancing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A secondary nondestructive detection method of liquid concentration for RFID tag array with mutual coupling. <em>TMC</em>, <em>24</em>(9), 9202-9221. (<a href='https://doi.org/10.1109/TMC.2025.3559487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio Frequency Identification (RFID) technology emerges as a crucial tool for passive sensing due to the lightweight, taggable, and easily deployable attributes of tags. However, RFID multi-tag systems face significant accuracy degradation due to tag mutual coupling interference. To address this problem, we propose the RF-Copy method, a tagged coupling interference suppression approach. The method constructs RSSI and phase models with coupled terms, simplifies and estimates these parameters based on the models, and develops an interference suppression algorithm to remove coupling effects and extract ‘clean’ signal fingerprints. Experimental results demonstrate that the decoupled outputs (0.318 dB RSSI, 0.623 radians phase) closely approach the interference-free baseline values (0.392 dB, 0.649 radians). We apply the RF-Copy method to evaluate the tag mutual coupling suppression effectiveness in nondestructive white wine and wine concentration recognition. This process generates ‘clean’ RSSI and phase values, which serve as input features for training a Gated Recurrent Unit (GRU) neural network integrated with an attention mechanism. Experimental results demonstrate that our method achieves unopened-package detection accuracies of 99.0% (white wine) and 99.6% (wine). Compared to other methods, the RF-Copy-GRU not only improves sensing accuracy but also demonstrates robustness against interference.},
  archive      = {J_TMC},
  author       = {Manman Zhang and Peng Li and Shanjun Bao and He Xu and Feng Zhu},
  doi          = {10.1109/TMC.2025.3559487},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9202-9221},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A secondary nondestructive detection method of liquid concentration for RFID tag array with mutual coupling},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CatUA: Catalyzing urban air quality intelligence through mobile crowd-sensing. <em>TMC</em>, <em>24</em>(9), 9184-9201. (<a href='https://doi.org/10.1109/TMC.2025.3560120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile air pollution sensing methods have emerged to collect air quality data with improved spatial and temporal resolutions. However, existing methodologies struggle to effectively process spatially mixed gas samples due to the highly dynamic fluctuations experienced by sensors, resulting in significant measurement deviations. We identify an opportunity to address this issue by exploring potential patterns within sensor measurements. To this end, we propose CatUA, a novel city-scale fine-grained air quality estimation system designed to deliver accurate mobile air quality data. First, we design AirBERT, a representation learning model specifically aimed at discerning mixed gas concentrations from sensor data. Second, we implement a Prompt-informed Training Strategy that leverages extensive unlabeled and minimal labeled city-scale data to enhance the performance of CatUA. Notably, the Auto-Prompt mechanism allows CatUA to conveniently acquire new knowledge tailored to specific downstream tasks. To ensure the practicality of CatUA, we have invested considerable effort in developing the software stack on our meticulously crafted Sensing Front-end, which has successfully gathered city-scale air quality data for over 1,200 hours. Experiments conducted on the collected data demonstrate that CatUA reduces sensing errors by 96.9% with a latency of only 44.9 ms, outperforming the state-of-the-art baseline by 42.6%.},
  archive      = {J_TMC},
  author       = {Nan Zhou and Yuxuan Liu and Haoyang Wang and Fanhang Man and Jingao Xu and Fan Dang and Chaopeng Hong and Yunhao Liu and Xiao-Ping Zhang and Yali Song and Qiuhua Wang and Xinlei Chen},
  doi          = {10.1109/TMC.2025.3560120},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9184-9201},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CatUA: Catalyzing urban air quality intelligence through mobile crowd-sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MHTrack: MmWave-based mobile hand tracking. <em>TMC</em>, <em>24</em>(9), 9168-9183. (<a href='https://doi.org/10.1109/TMC.2025.3560074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-intrusive hand tracking with mmWave radar technology is important in various Human-Computer Interaction (HCI) scenarios. However, existing mmWave-based solutions require users to be stationary and restrict a fixed hand motion area, which limits application flexibility and user experience. This paper proposes a novel mmWave-based Mobile Hand Tracking (MHTrack) system, which tracks user’s hand gestures during walking. MHTrack focuses on tracking both absolute hand trajectory in the global coordinate system and relative hand trajectory to the body. Specifically, we propose a wake-up mechanism for hand motion capture, in which hand point cloud can be recognized even under body interference and noise. We propose a hand tracking strategy named local spatial update, which overcomes the sparsity and instability of point clouds, to obtain absolute hand trajectory. Subsequently, we propose a hand anchor correction method to suppress anchor offset and remove the impact of body movement from absolute hand trajectory, thereby obtaining relative hand trajectory. As a case study, we project the relative hand trajectory onto a 2D image and feed it into a gesture recognition model to recognize the gestures. We conduct extensive experiments to evaluate the performance of MHTrack. Results demonstrate a 3D hand trajectory tracking error of $ 3.6$ cm in an area of $ 3.2\;{\rm m}\,\times\, 4.8\;{\rm m}$ and a gesture recognition accuracy of 99% with 30 gesture classes.},
  archive      = {J_TMC},
  author       = {Xiulong Liu and Hankai Liu and Yantao Han and Xin Xie and Xinyu Tong and Keqiu Li},
  doi          = {10.1109/TMC.2025.3560074},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9168-9183},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MHTrack: MmWave-based mobile hand tracking},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task reinforcement learning-based multiple access for dynamic wireless networks. <em>TMC</em>, <em>24</em>(9), 9153-9167. (<a href='https://doi.org/10.1109/TMC.2025.3559676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of emergent applications, wireless networks require the provision of high throughput. Meanwhile, wireless scenarios exhibit highly dynamic characteristics, involving frequent changes in the network scale and traffic. To satisfy the high demand for new applications in dynamic wireless scenarios, a novel medium access control (MAC) protocol is required to allow stations to access the channel with high efficiency and adaptability. Based on multi-agent reinforcement learning (MARL), we propose a new MAC protocol, Multi-task Transformer-based Multiple Access (MTMA). Multi-task learning is applied to train a single actor to adapt to multiple wireless environments simultaneously. To improve the scalability, we propose a transformer-based critic network, which can scale to different wireless scenarios. Moreover, a novel network called “Generalization for N (Gen-N)” network is proposed to enhance the generalization ability. We conduct simulation experiments to demonstrate that MTMA: 1) achieves over 95% of upper bound of throughput while maximizing the fairness performance; 2) outperforms classic MAC protocol and MARL-based baselines in scenarios with saturated and light traffic; 3) can adapt to environmental changes quickly in dynamic scenarios; 4) can generalize to unseen scenarios during training. Finally, the ablation experiments are conducted to evaluate the effectiveness of components used in MTMA.},
  archive      = {J_TMC},
  author       = {Zhenyu Chen and Xinghua Sun and Yili Jin and Fangxin Wang},
  doi          = {10.1109/TMC.2025.3559676},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9153-9167},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-task reinforcement learning-based multiple access for dynamic wireless networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient frame aggregation scheme for relay-aided internet of things networks with age of information constraints. <em>TMC</em>, <em>24</em>(9), 9141-9152. (<a href='https://doi.org/10.1109/TMC.2025.3560839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of Things (IoT) networks, monitoring information collection is critical for intelligent decision-making, which is a significant challenge for the sensors deployed at remote locations. Relay can effectively improve the transmission quality and transmission range of sensors by means of multi-hop transmission. It is an effective method for remote data collection in IoT networks. However, the lifetime of the relay may be dramatically reduced due to the heavy resource overhead for frequent short packet delivery. In this paper, we present an efficient relay transmission scheme for IoT networks, in which the frame aggregation technology is employed at the relay to reduce the resource overhead by sharing a common frame header and tail. Meanwhile, for the delay caused by frame aggregation, we analyze the freshness of the sensing data in terms of age of information (AoI) and take it as a constraint for the frame aggregation system. Besides, the optimal frame aggregation period is determined based on the closed-form expressions derived for the average AoI and transmission efficiency. Simulation results show that the theoretical analysis closely matches the simulations, and the proposed method significantly improves transmission efficiency compared to the traditional decode-and-forward method.},
  archive      = {J_TMC},
  author       = {Jiaxing Wang and Jingjing Wang and Jianrui Chen and Lin Bai and Jinho Choi},
  doi          = {10.1109/TMC.2025.3560839},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9141-9152},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An efficient frame aggregation scheme for relay-aided internet of things networks with age of information constraints},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E2EC: Edge-to-edge collaboration for efficient real-time video surveillance inference. <em>TMC</em>, <em>24</em>(9), 9126-9140. (<a href='https://doi.org/10.1109/TMC.2025.3559919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In smart cities, Multi-Camera Multi-Target pedestrian tracking and Re-identification (MCMT-ReID) is essential for effective surveillance, particularly in real-time scenarios, as it demands significant computational resources. Current edge-cloud collaboration methods encounter issues such as high latency and potential data leakage due to the physical distance between cloud servers and cameras. To address these issues, we propose a novel Edge-to-Edge Collaboration (E2EC) system that fully utilizes collaboration between heterogeneous edge devices. E2EC partitions the MCMT-ReID task into two modular applications: Tracking and Re-identification (ReID), and employs a customized Kafka communication protocol to optimize data exchange efficiency. Moreover, E2EC dynamically orchestrates intermediate inference flows and transmits features instead of pedestrian detection frames to avoid data leakage. To enhance ReID accuracy, we introduce a real-time ReID Loop Confirmation (ReLC) algorithm, which continuously validates identities to boost reliability and accuracy. E2EC has been deployed and tested in a real-world campus environment to validate its effectiveness. Experimental results demonstrate that E2EC enhances the Rank-1 accuracy and mAP of pedestrian ReID by 36.88% and 46.00%, respectively. Furthermore, it achieves an increase of about 6.35%-12.66% in throughput and reduces latency by 35.01%-57.83% compared to baselines, ensuring real-time performance under dynamic workloads.},
  archive      = {J_TMC},
  author       = {Guo Li and Jiandian Zeng and Zihao Peng and Yuzhu Liang and Xi Zheng and Tian Wang},
  doi          = {10.1109/TMC.2025.3559919},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9126-9140},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {E2EC: Edge-to-edge collaboration for efficient real-time video surveillance inference},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WebARNav: Mobile web AR indoor navigation with edge-assisted vision localization. <em>TMC</em>, <em>24</em>(9), 9110-9125. (<a href='https://doi.org/10.1109/TMC.2025.3560296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The gradual maturation of mobile augmented reality (AR) and localization technologies is enabling the development of immersive AR-enabled indoor localization and navigation systems. Existing indoor localization technologies (e.g., WiFi, infrared, Bluetooth) and navigation services do not provide intuitive 3D AR experiences and can be expensive to deploy. This paper introduces WebARNav, a cross-platform indoor localization system that provides user-friendly AR navigation services with low overhead and remarkable accuracy. First, we propose a lightweight location fusion framework for indoor navigation on the mobile web, which leverages accurate edge-supported vision localization to guide and correct lightweight pedestrian dead reckoning localization. Second, we improve the accuracy of localization using an attention-based feature extraction method and a dual-stream retrieval and co-visibility re-ranking technique for initial localization. Third, we significantly improve accuracy and speed up retrieval as users move by generating a topological map for traveling localization. We conducted extensive experiments on various indoor datasets to demonstrate localization accuracy and navigation experience. The study shows that WebARNav achieves a localization frequency of over 30 Hz and reduces the average trajectory error by 76% and 95% for single- and multi-floor office scenes, respectively, compared to the PDR-only method. The proposed traveling localization method also reduces the localization latency by 15.2%, 55.1%, and 98.6% in the baseline datasets, with an accuracy improvement of over 4%.},
  archive      = {J_TMC},
  author       = {Yakun Huang and Shengwei Meng and Yuanwei Zhu and Jun Wang and Jacky Cao and Xiuquan Qiao and Xiang Su},
  doi          = {10.1109/TMC.2025.3560296},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9110-9125},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WebARNav: Mobile web AR indoor navigation with edge-assisted vision localization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multi-user offloading of personalized diffusion models: A DRL-convex hybrid solution. <em>TMC</em>, <em>24</em>(9), 9092-9109. (<a href='https://doi.org/10.1109/TMC.2025.3560582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative diffusion models like Stable Diffusion are at the forefront of the thriving field of generative models today, celebrated for their robust training methodologies and high-quality photorealistic generation capabilities. These models excel in producing rich content, establishing them as essential tools in the industry. Building on this foundation, the field has seen the rise of personalized content synthesis as a particularly exciting application. However, the large model sizes and iterative nature of inference make it difficult to deploy personalized diffusion models broadly on local devices with heterogeneous computational power. To address this, we propose a novel framework for efficient multi-user offloading of personalized diffusion models. This framework accommodates a variable number of users, each with different computational capabilities, and adapts to the fluctuating computational resources available on edge servers. To enhance computational efficiency and alleviate the storage burden on edge servers, we propose a tailored multi-user hybrid inference approach. This method splits the inference process for each user into two phases, with an optimizable split point. Initially, a cluster-wide model processes low-level semantic information for each user’s prompt using batching techniques. Subsequently, users employ their personalized models to refine these details during the later phase of inference. Given the constraints on edge server computational resources and users’ preferences for low latency and high accuracy, we model the joint optimization of each user’s offloading request handling and split point as an extension of the Generalized Quadratic Assignment Problem (GQAP). Our objective is to maximize a comprehensive metric that balances both latency and accuracy across all users. To solve this NP-hard problem, we transform the GQAP into an adaptive decision sequence, model it as a Markov decision process, and develop a hybrid solution combining deep reinforcement learning with convex optimization techniques. Simulation results validate the effectiveness of our framework, demonstrating superior optimality and low complexity compared to traditional methods.},
  archive      = {J_TMC},
  author       = {Wanting Yang and Zehui Xiong and Song Guo and Shiwen Mao and Dong In Kim and Merouane Debbah},
  doi          = {10.1109/TMC.2025.3560582},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9092-9109},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient multi-user offloading of personalized diffusion models: A DRL-convex hybrid solution},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight and robust access control protocol for IoT-based e-healthcare network. <em>TMC</em>, <em>24</em>(9), 9080-9091. (<a href='https://doi.org/10.1109/TMC.2025.3561084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) devices are crucial components in e-healthcare networks. It enables remote patient health monitoring and facilitates seamless communication among medical sensors, wearable devices, and healthcare providers through public communication channels. Despite these advantages, the use of public communication among medical sensors in e-healthcare networks introduces critical challenges, such as vulnerability to impersonation, physical capture, and ephemeral secret leakage, particularly in resource-constrained environments. In recent years, various access control protocols have been developed to mitigate these risks. However, these protocols often fail to ensure robust security while incurring significant communication and computation overhead. To overcome these limitations, we propose a lightweight and robust access control protocol for IoT-based e-healthcare networks using chaotic maps. We propose a novel protocol that integrates a PUF-based mechanism to mitigate the challenges of physical tampering and cloning attacks in e-healthcare networks. It leverages the inherent uniqueness of PUF and enhances security through the high-entropy properties of chaotic maps. We analyze the proposed protocol informally, which confirms that it significantly bolsters efficiency and security. We also validate the security using the Random or Real (RoR) model. Moreover, we verify the security of the proposed protocol using Scyther. These analyses highlight that the proposed protocol offers robust resistance to numerous attacks, such as impersonation, physical capture, and ephemeral secret leakage. Moreover, we also compare it with existing and relevant protocols. The comparative analysis showcases its superior performance. Notably, the proposed authentication protocol significantly reduces 46.84% computational overhead and decreases 31.30% communication overhead, underscoring its enhanced performance and resource efficiency.},
  archive      = {J_TMC},
  author       = {Zahid Ghaffar and Wen-Chung Kuo and Khalid Mahmood and Tayyaba Tariq and Salman Shamshad and Ashok Kumar Das and Mohammed J. F. Alenazi},
  doi          = {10.1109/TMC.2025.3561084},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9080-9091},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A lightweight and robust access control protocol for IoT-based e-healthcare network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TypeFly: Low-latency drone planning with large language models. <em>TMC</em>, <em>24</em>(9), 9068-9079. (<a href='https://doi.org/10.1109/TMC.2025.3561282'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in robot planning using large language models (LLMs) have demonstrated significant potential, primarily due to LLMs’ capabilities to understand natural language commands and generate executable plans in various languages. However, in time-sensitive and interactive applications involving mobile robots, particularly drones, the sequential token generation process inherent to LLMs introduces substantial latency, i.e. response time, during the control plan generation. In this paper, we present a system called TypeFly that tackles this latency problem using a combination of a novel programming language called MiniSpec and its runtime to reduce both the response time and generation time for the robot plan. That is, instead of asking an LLM to write a program (robotic plan) in the popular but verbose Python, TypeFly gets it to do it in MiniSpec specially designed for token efficiency and stream interpreting. Using a set of challenging drone tasks, we show that design choices made by TypeFly can reduce the average response time to 74% compared to existing works and provide a more consistent user experience, enabling responsive and intelligent LLM-based drone control.},
  archive      = {J_TMC},
  author       = {Guojun Chen and Xiaojing Yu and Neiwen Ling and Lin Zhong},
  doi          = {10.1109/TMC.2025.3561282},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9068-9079},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TypeFly: Low-latency drone planning with large language models},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust dynamic broadcasting for multi-hop wireless networks under time-varying connectivity and dynamic SINR. <em>TMC</em>, <em>24</em>(9), 9050-9067. (<a href='https://doi.org/10.1109/TMC.2025.3558815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Throughput-optimal dynamic broadcasting is an essential cornerstone for the efficient operation of Multi-hop Wireless Networks (MWNs). Most existing algorithms for this problem were developed assuming static interference environments and network connectivity. However, wireless interference environments and network connectivity are inherently time-varying in real-world scenarios, primarily due to uncontrollable interference sources and unreliable links. Such time-varying characteristics make these existing algorithms less robust. In this paper, we study the robust throughput-optimal dynamic broadcasting for MWNs with multi-dimensional time-varying characteristics in terms of interference environments, network connectivity, and data arrival. We model the time-varying link existence states using a random process and characterize the time-varying interference environments through a dynamic variant of the classical Signal-to-Interference-plus-Noise-Ratio (SINR) model. In this variant, the SINR model parameters are dynamically adjusted over time by an adversary. Based on this, we first design a Robust Throughput-optimal Dynamic Broadcast (RTDB) algorithm which makes efficient slot-based max-weight link scheduling, power allocation, and data forwarding decisions in each time slot. We then prove its throughput-optimality in time-varying acyclic directed MWNs under the dynamic SINR model. The effectiveness of RTDB is validated via numerous simulations.},
  archive      = {J_TMC},
  author       = {Xiang Tian and Jiguo Yu and Chuanwen Luo and Dongxiao Yu and Guijuan Wang and Bin Feng},
  doi          = {10.1109/TMC.2025.3558815},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9050-9067},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust dynamic broadcasting for multi-hop wireless networks under time-varying connectivity and dynamic SINR},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward open-world-aware user authentication based on human bodies using mmWave signals. <em>TMC</em>, <em>24</em>(9), 9036-9049. (<a href='https://doi.org/10.1109/TMC.2025.3562151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User authentication is evolving with expanded applications and innovative techniques. New authentication approaches utilize RF signals to sense specific human characteristics, offering a contactless and nonintrusive solution. However, these RF signal-based methods struggle with challenges in open-world scenarios, i.e., dynamic environments, daily behaviors with unrestricted postures, and identification of unauthorized users with security threats. In this paper, we present an open-world user authentication system, OpenAuth, which leverages a commercial off-the-shelf (COTS) mmWave radar to sense unrestricted human postures and behaviors for identifying individuals. First, OpenAuth utilizes a MUSIC-based neural network imaging model to eliminate environmental clutter and generate environment-independent human silhouette images. Then, the human silhouette images are normalized to consistent topological structures of human postures, ensuring robustness against unrestricted human postures. Next, fine-grained body features are extracted from these environment-independent and posture-independent human silhouette images using a metric learning model. To eliminate potential security threats that arise from unauthorized users, OpenAuth synthesizes data placeholders for enhancing unauthorized user identification. Finally, a k-NN-based authentication model is constructed to authenticate users’ identities. Experiments in real environments show that the proposed OpenAuth achieves an average authentication accuracy of 93.4% and false acceptance rate (FAR) of 1.8% in open-world scenarios.},
  archive      = {J_TMC},
  author       = {Junlin Yang and Jiadi Yu and Linghe Kong and Yanmin Zhu and Hong-Ning Dai},
  doi          = {10.1109/TMC.2025.3562151},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9036-9049},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward open-world-aware user authentication based on human bodies using mmWave signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partially hidden policy attribute-based multi-keyword searchable encryption with verification and revocation. <em>TMC</em>, <em>24</em>(9), 9020-9035. (<a href='https://doi.org/10.1109/TMC.2025.3558955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data security is becoming increasingly critical as outsourced data services flourish. An effective solution for ensuring data confidentiality in the cloud is attribute-based searchable encryption (ABSE). However, most extant ABSE solutions have not been able to simultaneously achieve rich expressive queries, result validation, attribute revocation, and policy hiding, and there are problems such as inflexibility in query methods and access control capabilities, insufficient scalability and practicality, and privacy leakage. In particular, it should be noted that most ABSE schemes fail to promise the privacy security of access policies and are susceptible to offline keyword guessing attack. Hidden policy-ABSE (HP-ABSE) has been proposed to tackle the issue of explicit attribute values in access policies potentially compromising privacy. Nevertheless, in most existing HP-ABSE schemes, an adversary may initiate attribute values guessing attack to speculate on the values of attribute within access strategy. To this end, this paper proposes partially hidden policy attribute-based multi-keyword searchable encryption with verification and revocation (PHP-ABMKSE-VR). PHP-ABMKSE-VR scheme is practical, secure, as well as effective, making it appropriate for real-world scenarios like smart healthcare, as demonstrated by the experimental outcomes and safety analysis.},
  archive      = {J_TMC},
  author       = {Liqing Chen and Shiguo Xu and Chunhua Jin and Hao Zhang and Jian Weng},
  doi          = {10.1109/TMC.2025.3558955},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9020-9035},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Partially hidden policy attribute-based multi-keyword searchable encryption with verification and revocation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated domain-independent prototype learning with alignments of representation and parameter spaces for feature shift. <em>TMC</em>, <em>24</em>(9), 9004-9019. (<a href='https://doi.org/10.1109/TMC.2025.3560083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning provides a privacy-preserving modeling schema for distributed data, which coordinates multiple clients to collaboratively train a global model. However, data stored in different clients may be collected from diverse domains, and the resulting feature shift is prone to the degraded performance of global model. In this paper, we propose a Federated Domain-Independent Prototype Learning (FedDP) method with Alignments of Representation and Parameter Spaces for Feature Shift. Concretely, FedDP aims to eliminate the domain-specific information and explore the pure representations via information bottleneck, thus integrating the local and global domain-independent prototypes, respectively. To align the cross-domain representation spaces, the global domain-independent prototypes serve as the supervised signals to enable local intra-class representations to approach them. Further, to mitigate the divergences of optimization directions between multiple clients induced by the feature shift, the global representations are yielded by the global model on the client-side and guide the learning of local representations, thus unifying the parameter spaces of multiple local models. We derive the theoretical lower bound of the optimization objective based on mutual information, which is transformed into a computable loss. The proposed FedDP can be applied in the scenarios of homogeneous and heterogeneous models. Extensive experiments are conducted on three challenging multi-domain datasets. The experimental results illustrate the superiority of FedDP compared with state-of-the-art federated learning methods.},
  archive      = {J_TMC},
  author       = {Lele Fu and Sheng Huang and Yanyi Lai and Chuanfu Zhang and Hong-Ning Dai and Zibin Zheng and Chuan Chen},
  doi          = {10.1109/TMC.2025.3560083},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {9004-9019},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated domain-independent prototype learning with alignments of representation and parameter spaces for feature shift},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomous and incentivized wireless connection for robust mobile blockchain network. <em>TMC</em>, <em>24</em>(9), 8988-9003. (<a href='https://doi.org/10.1109/TMC.2025.3559007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain has been widely implemented as a trusted platform. Previous works mainly focus on the computing capacity of devices while communication factors play a vital role in blockchain performance during dynamic wireless environments. High-speed movement causes frequent wireless connection interruptions and leads to severe performance degradation of blockchain. Besides, resource-constrained mobile devices are unwilling to selflessly contribute their energy and bandwidth for blockchain, hindering applications in dynamic mobile networks. This paper proposes a reverse auction mechanism to incentivize mobile devices to provide robust wireless connections. Devices submit their connection provision and expected rewards as bids. The mobile blockchain system uses smart contracts to autonomously execute the reverse auction to determine winners and allocate payments based on actual connections. We prove that the reverse auction mechanism is Individual Rationality (IR), Incentive Compatibility (IC), and Computational Efficiency (CE), and derive the approximation ratio 2$\sigma$ of the mechanism. Extensive simulation results demonstrate that the proposed mechanism decreases up to half the energy and bandwidth consumption, but achieves a similar TPS and stale rate compared to the selfless scheme, where devices contribute all wireless connections for nothing in return. The proposed auction mechanism achieves more than 96% of the optimal social welfare.},
  archive      = {J_TMC},
  author       = {Weiyi Wang and Yutao Jiao and Jin Chen and Jiawen Kang and Yuhua Xu},
  doi          = {10.1109/TMC.2025.3559007},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8988-9003},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Autonomous and incentivized wireless connection for robust mobile blockchain network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foundation model enhanced multiple access in heterogeneous networks. <em>TMC</em>, <em>24</em>(9), 8974-8987. (<a href='https://doi.org/10.1109/TMC.2025.3558942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-generation multiple access techniques are crucial for providing low-latency and highly efficient data transmission services. Recently, Deep Reinforcement Learning (DRL) has emerged as a prevalent approach in the multiple access domain, aiming to facilitate user coordination and enhance transmission efficiency. However, current DRL approaches face challenges, including limited generalization ability, low sample efficiency, and the complexities associated with Partially Observable Markov Decision Processes (POMDP), which hinder their application in heterogeneous networks with varying numbers of nodes and configurations. In this paper, we propose a foundation model-based multiple access (FMA) algorithm. To address severe POMDP and sample inefficiency issues, we decompose the multiple access problem into two parts: a transmission decision part and a configuration estimation part. We leverage the strong generalization and inference capabilities of the foundation model, utilizing a Deep Learning (DL) approach instead of DRL for training, and adopt the Low-Rank Adaptation (LoRA) technique to fine-tune the foundation model for downstream multiple access tasks. Simulation results demonstrate that: 1) through the decomposition, the FMA approach exhibits sufficient generalization and inference abilities to adapt to various scenarios with various protocols, configurations, and numbers of heterogeneous nodes; 2) by incorporating expert knowledge, the FMA approach can significantly enhance network performance while ensuring certain fairness requirement for heterogeneous nodes.},
  archive      = {J_TMC},
  author       = {Mingqi Han and Xinghua Sun and Xijun Wang and Xiang Chen},
  doi          = {10.1109/TMC.2025.3558942},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8974-8987},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Foundation model enhanced multiple access in heterogeneous networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ANNS: An intelligent advanced non-convex non-smooth scheme for IRS-aided next generation mobile communication networks. <em>TMC</em>, <em>24</em>(9), 8959-8973. (<a href='https://doi.org/10.1109/TMC.2025.3559099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing the communication rate and quality has become the primary goal for the development of next-generation mobile communication networks, and traditional techniques such as MIMO and increasing the transmit power of the base station (BS) have not achieved a leapfrog effect. The emergence of Intelligent Reflective Surfaces (IRS) provides more reliable technical support for providing high-energy and high-rate communications. However, IRS-aided joint optimization with communication rate and quality of service constraints is a non-convex non-smooth optimization problem, and the optimal global solution cannot be obtained due to its computational complexity. In this paper, we propose an intelligent Advanced Non-convex Non-smooth Scheme (ANNS) in IRS-aided Next Generation Mobile Communication Networks for making the transmission rate of mission communication and communication quality effective. To ensure that the inequality constraints in the joint optimization problem are not violated and the equation constraints are satisfied, a hybrid deep reinforcement learning and data-experience-driven constraint security layer network is proposed, which maps the constraint violations into the safe feasible domain by mapping the constraint variables into the constraint variables mapping method, and the convergence of the algorithm is theoretically demonstrated. Experimental results show that the proposed ANNS performs superior optimization compared to SAC, DDPG, and A2C for solving non-convex non-smooth problems. The proposed ANNS has the potential to be generalized to other mobile computing applications with non-convex non-smooth characteristics.},
  archive      = {J_TMC},
  author       = {Miaojiang Chen and Anfeng Liu and Neal N. Xiong and Yingying Ren and Houbing Herbert Song},
  doi          = {10.1109/TMC.2025.3559099},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8959-8973},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ANNS: An intelligent advanced non-convex non-smooth scheme for IRS-aided next generation mobile communication networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedSTDN: A federated learning-enabled spatial-temporal prediction model for wireless traffic prediction. <em>TMC</em>, <em>24</em>(9), 8945-8958. (<a href='https://doi.org/10.1109/TMC.2025.3559066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Traffic Prediction (WTP) plays a significant role in achieving intelligent resource management forcommunication systems. However, WTP still faces challenges such as inaccurate prediction resulting from the complex spatial-temporal characteristics due to user mobility, high communication overhead caused by the complexity of the prediction model, and user privacy issues stemming from Centralized Learning (CL). To address the aforementioned issues, this paper proposes a WTP framework under the Federated Learning (FL) strategy called Federated Spatial-Temporal Dual-attention based Network (FedSTDN). Aiming at improving communication efficiency and simultaneously representing various wireless traffic patterns, a data augmentation-based clustering algorithm is adopted, which groups cells into different regions using a small augmented dataset, facilitating subsequent processing. To improve prediction performance, a local prediction model based on Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) is proposed to capture the short- and long-term dependencies of traffic. Additionally, a novel Kolmogorov-Arnold Network (KAN) layer is introduced to replace the traditional Multi-Layer Perceptron (MLP) layer, further enhancing prediction performance. Simulations on two different real-world datasets verify the effectiveness and efficiency of FedSTDN. Compared to the well-performing baseline, the proposed FedSTDN achieves up to 32.83% and 24.30% improvements in Mean Square Error (MSE) and Mean Absolute Error (MAE) on the Milan dataset, respectively. For the Trentino dataset, FedSTDN achieves up to 17.25% and 5.86% improvements in MSE and MAE, respectively.},
  archive      = {J_TMC},
  author       = {Zhenyu Li and Yuchuan Fu and Mengqiu Tian and Changle Li and F. Richard Yu and Nan Cheng},
  doi          = {10.1109/TMC.2025.3559066},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8945-8958},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedSTDN: A federated learning-enabled spatial-temporal prediction model for wireless traffic prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RepCaM++: Exploring transparent visual prompt with inference-time re-parameterization for neural video delivery. <em>TMC</em>, <em>24</em>(9), 8930-8944. (<a href='https://doi.org/10.1109/TMC.2025.3558259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, content-aware methods have been employed to reduce bandwidth and enhance the quality of Internet video delivery. These methods involve training distinct content-aware super-resolution (SR) models for each video chunk on the server, subsequently streaming the low-resolution (LR) video chunks with the SR models to the client. Prior research has incorporated additional partial parameters to customize the models for individual video chunks. However, this leads to parameter accumulation and can fail to adapt appropriately as video lengths increase, resulting in increased delivery costs and reduced performance. In this paper, we introduce RepCaM++, an innovative framework based on a novel Re-parameterization Content-aware Modulation (RepCaM) module that uniformly modulates video chunks. The RepCaM framework integrates extra parallel-cascade parameters during training to accommodate multiple chunks, subsequently eliminating these additional parameters through re-parameterization during inference. Furthermore, to enhance RepCaM’s performance, we propose the Transparent Visual Prompt (TVP), which includes a minimal set of zero-initialized image-level parameters (e.g., less than 0.1%) to capture fine details within video chunks. We conduct extensive experiments on the VSD4K dataset, encompassing six different video scenes, and achieve state-of-the-art results in video restoration quality and delivery bandwidth compression.},
  archive      = {J_TMC},
  author       = {Rongyu Zhang and Xize Duan and Jiaming Liu and Li Du and Yuan Du and Dan Wang and Shanghang Zhang and Fangxin Wang},
  doi          = {10.1109/TMC.2025.3558259},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8930-8944},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RepCaM++: Exploring transparent visual prompt with inference-time re-parameterization for neural video delivery},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeRelayL: Sustainable decentralized relay learning. <em>TMC</em>, <em>24</em>(9), 8913-8929. (<a href='https://doi.org/10.1109/TMC.2025.3558544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of Big Data, large-scale machine learning models have revolutionized various fields, driving significant advancements. However, large-scale model training demands high financial and computational resources, which are only affordable by a few technological giants and well-funded institutions. In this case, common users like mobile users, the real creators of valuable data, are often excluded from fully benefiting due to the barriers, while the current methods for accessing large-scale models either limit user ownership or lack sustainability. This growing gap highlights the urgent need for a collaborative model training approach, allowing common users to train and share models. However, existing collaborative model training paradigms, especially federated learning (FL), primarily focus on data privacy and group-based model aggregation. To this end, this paper intends to address this issue by proposing a novel training paradigm named decentralized relay learning (DeRelayL), a sustainable learning system where permissionless participants can contribute to model training in a relay-like manner and share the model. In detail, this paper presents the architecture and workflow of DeRelayL, designs incentive mechanisms to ensure sustainability, and conducts theoretical analysis and numerical simulations to demonstrate its effectiveness.},
  archive      = {J_TMC},
  author       = {Haihan Duan and Tengfei Ma and Yuyang Qin and Runhao Zeng and Wei Cai and Victor C. M. Leung and Xiping Hu},
  doi          = {10.1109/TMC.2025.3558544},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8913-8929},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeRelayL: Sustainable decentralized relay learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborate for real-time gain: Semantic-based robotic communication in 3D object tracking. <em>TMC</em>, <em>24</em>(9), 8899-8912. (<a href='https://doi.org/10.1109/TMC.2025.3559584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent collaborative perception has emerged as a promising solution to enhance autonomous vehicle perception capabilities, surpassing the limitations of single-agent systems. Utilizing vehicle-to-everything (V2X) communication, this approach inherently results in a trade-off between perception ability and communication costs. However, current research mainly focuses on optimizing this trade-off by selecting spatial critical information, often neglecting the temporal dimension. This paper proposes a novel collaborative perception framework that integrates semantic information across both spatial and temporal dimensions, thereby achieving reduced latency and enhanced perception performance. By leveraging a factorized hyperprior model for feature encoding and applying temporal fusion with Kalman filter predicting, the proposed approach enhances the accuracy and efficiency of object detection. The framework is evaluated on the OPV2V and Dair-V2X datasets, demonstrating significant improvements in the communication-perception trade-off compared to state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Junming Shao and Xiaoqi Qin and Jian Gao and Yanlin Li and Liang Xin and Ping Zhang},
  doi          = {10.1109/TMC.2025.3559584},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8899-8912},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborate for real-time gain: Semantic-based robotic communication in 3D object tracking},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent reinforcement learning in adversarial game environments: Personalized anti-interference strategies for heterogeneous UAV communication. <em>TMC</em>, <em>24</em>(9), 8886-8898. (<a href='https://doi.org/10.1109/TMC.2025.3559123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing anti-jamming strategies for unmanned aerial vehicle (UAV) networks largely assume homogeneity among UAVs, neglecting the differences in hardware configurations, task requirements, and environmental adaptability. In the face of such heterogeneity, these strategies often fail to effectively counter intelligent jamming and co-channel interference. To address this issue, this paper proposes an intelligent anti-jamming framework designed specifically for the heterogeneous UAV network, allowing each UAV to autonomously adjust its transmission channel and power based on its hardware capabilities and task requirements in a distributed environment. This aims to optimize communication efficiency and reduce energy consumption. We formulate the anti-jamming problem as an adversarial game and confirm the existence of a unique equilibrium point within this model. Moreover, we introduce the novel Personalized Federated Soft Actor-Critic (PFSAC) algorithm, which combines the global model with local models to customize personalized anti-jamming strategies for each UAV, significantly enhancing network performance in complex jamming environments. Simulation results indicate that compared to other methods, our proposed algorithm significantly enhances the anti-jamming capability of heterogeneous UAV networks and performs better than them.},
  archive      = {J_TMC},
  author       = {Yeguang Qin and Jie Tang and Fengxiao Tang and Ming Zhao and Nei Kato},
  doi          = {10.1109/TMC.2025.3559123},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8886-8898},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-agent reinforcement learning in adversarial game environments: Personalized anti-interference strategies for heterogeneous UAV communication},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parameter-efficient federated framework for streaming time series anomaly detection via lightweight adaptation. <em>TMC</em>, <em>24</em>(9), 8872-8885. (<a href='https://doi.org/10.1109/TMC.2025.3558964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of mobile sensing techniques, huge amounts of time series data are continuously generated and accumulated in various domains, fueling considerable real-world mobile computing applications. In this context, time series anomaly detection is practically important. It endeavors to identify deviant samples from the normal distribution in time series data. Existing approaches generally assume that all the time series data is available at a central location. However, with the increasing deployment of edge devices, we are witnessing a decentralized collection of time series data. To bridge the gap between decentralized data and centralized anomaly detection algorithms, we propose a Parameter-efficient Federated Anomaly Detection framework via Lightweight Adaptation (PeFAD-LA) that addresses growing privacy concerns. PeFAD-LA innovatively employs a pre-trained large language model (PLM or LLM) as the core of the client’s local model, which can benefit from its cross-modality knowledge transfer capability. To reduce the communication overhead and the local model adaptation cost, we propose a parameter-efficient federated training module that requires clients to fine-tune only small-scale parameters and transmit them to the server for updates. Further, to handle anomalies in streaming time series, a lightweight adaptation module is employed to overcome concept drift. PeFAD-LA utilizes an anomaly-driven mask selection strategy to mitigate the impact of neglected anomalies during training. A novel dual knowledge transfer mechanism is designed to harness the useful knowledge across clients and sequentially learned data. Extensive experiments on real data offer evidence of the effectiveness and efficiency of the proposed framework.},
  archive      = {J_TMC},
  author       = {Hao Miao and Ronghui Xu and Yan Zhao and Senzhang Wang and Jianxin Wang and Philip S. Yu and Christian S. Jensen},
  doi          = {10.1109/TMC.2025.3558964},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8872-8885},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A parameter-efficient federated framework for streaming time series anomaly detection via lightweight adaptation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent collaboration for vehicular task offloading using federated deep reinforcement learning. <em>TMC</em>, <em>24</em>(9), 8856-8871. (<a href='https://doi.org/10.1109/TMC.2025.3557898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) distributes resources such as computing, storage, and bandwidth to the side close to users, which can provide low-latency services to in-vehicle users, thus promising a more efficient and safer driving environment. However, due to the dynamic scale of vehicle and the variability of resource requirements, it is a significant challenge to quickly obtain effective task offloading in large-scale vehicle scenarios. The existing studies generally adopt the centralized decision-making method, with long decision-making time and high computational overhead, which cannot effectively achieve good offloading decisions in large-scale scenarios. To address these problems, we propose a Multi-agent Collaborative Method for vehicular task offloading using Federated Deep Reinforcement Learning called MCM-FDRL. First, each vehicle as an agent, independently makes offloading decisions based on local information. Next, the offloading decision model of each vehicle is obtained through federated reinforcement learning training. At runtime, an effective vehicle offloading plan can be gradually developed through multi-agent collaboration. Using two real-world datasets, experiments show that the MCM-FDRL has good adaptability and scalability. Moreover, compared to the state-of-the-art methods, the task's average response time of the MCM-FDRL is reduced by 9.75%-64.90%, respectively.},
  archive      = {J_TMC},
  author       = {Xing Chen and Bohuai Xiao and Xinyu Lin and Zheyi Chen and Geyong Min},
  doi          = {10.1109/TMC.2025.3557898},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8856-8871},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-agent collaboration for vehicular task offloading using federated deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-timescale approach for wireless federated learning with parameter freezing and power control. <em>TMC</em>, <em>24</em>(9), 8841-8855. (<a href='https://doi.org/10.1109/TMC.2025.3557838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables distributed devices to train a shared machine learning (ML) model collaboratively while protecting their data privacy. However, the resource-limited mobile devices suffer from intensive computation-and-communication costs of model parameters. In this paper, we observe the phenomenon that the model parameters tend to be stabilized long before convergence during training process. Based on this observation, we propose a two-timescale FL framework by joint optimization of freezing stabilized parameters and controlling transmit power for the unstable parameters to balance the energy consumption and convergence. First, we analyze the impact of model parameter freezing and unreliable transmission on the convergence rate. Next, we formulate a two-timescale optimization problem of parameter freezing percentage and transmit power to minimize the model convergence error subject to the energy budget. To solve this problem, we decompose it into parallel sub-problems and decompose each sub-problem into two different timescales problems using the Lyapunov optimization method. The optimal parameter freezing and power control strategies are derived in an online fashion. Experimental results demonstrate the superiority of the proposed scheme compared with the benchmark schemes.},
  archive      = {J_TMC},
  author       = {Jinhao Ouyang and Yuan Liu and Hang Liu},
  doi          = {10.1109/TMC.2025.3557838},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8841-8855},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A two-timescale approach for wireless federated learning with parameter freezing and power control},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient and privacy-preserving distributed learning: A double layer-based auction design. <em>TMC</em>, <em>24</em>(9), 8824-8840. (<a href='https://doi.org/10.1109/TMC.2025.3560550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of Artificial Intelligence of Things (AIoT) has enabled AI-powered services within wireless networks, relying on well-trained machine learning (ML) models. Distributed learning, such as federated learning (FL), allows smart devices (SDs) to collaborate on model training without sharing raw data, but privacy protection is still necessary to prevent potential information leakage from evolving attacks. Additionally, training efficiency is hampered by limited resources and selfishness of SDs. This paper considers a layered distributed learning scenario using a double-layer auction approach, where model users act as buyers, SDs act as data owners contributing their datasets, and edge layer nodes (ELNs) serve as model trainers providing computing resources. The differential privacy (DP) mechanism is utilized to add Gaussian noise to the trained models by the ELNs. Then, we formulate a joint optimization problem to optimize task assignment, data owners’ sensing durations, and model trainers’ local iterations and privacy budgets, aiming to maximize the utility of all participants while ensuring cost-effective and privacy-preserving distributed learning. We decompose the formulated problem into four sub-problems and design a layered algorithm to solve them and derive collaboration strategies. Simulation results validate the algorithm’s performance and demonstrate the advantages of our proposed approach compared to benchmark schemes.},
  archive      = {J_TMC},
  author       = {Yuxiao Song and Daojing He and Minghui Dai and Mohsen Guizani},
  doi          = {10.1109/TMC.2025.3560550},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8824-8840},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cost-efficient and privacy-preserving distributed learning: A double layer-based auction design},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedLFP: Communication-efficient personalized federated learning on non-IID data in mobile edge computing environments. <em>TMC</em>, <em>24</em>(9), 8811-8823. (<a href='https://doi.org/10.1109/TMC.2025.3558406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) facilitates computing and storage at edge nodes near user devices, reducing latency and optimizing bandwidth. Federated Learning (FL) complements MEC by enabling privacy-preserving collaborative model training across edge nodes without sharing raw data. However, in MEC environments, FL faces challenges such as communication inefficiency and data heterogeneity (Non-IID), which degrade model performance and hinder convergence. To address these issues, we propose FedLFP, a communication-efficient personalized federated learning approach using label-free prototypes for Non-IID data in MEC. FedLFP employs three key strategies: (1) a Label-Free Prototype strategy to reduce communication costs and mitigate privacy risks, (2) a centroid prototype and combined clustering weight strategy to improve global prototype quality by considering data quantity and confidence levels, and (3) a multifaceted weighted contrastive learning strategy to enhance local representation learning and global alignment. We evaluated FedLFP on Android malware recognition using the KronoDroid dataset and standard image classification tasks, with eight configurations representing practical Non-IID settings. Experimental results show that FedLFP consistently outperforms thirteen state-of-the-art FL methods in accuracy, communication and computational efficiency. Additionally, we provide theoretical guarantees for the convergence of FedLFP under Non-IID conditions.},
  archive      = {J_TMC},
  author       = {Yuxia Sun and Siyi Pan and Aoxiang Sun and Zhixiao Fu and Saiqin Long and Zhetao Li},
  doi          = {10.1109/TMC.2025.3558406},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8811-8823},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedLFP: Communication-efficient personalized federated learning on non-IID data in mobile edge computing environments},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentivized federated learning and unlearning. <em>TMC</em>, <em>24</em>(9), 8794-8810. (<a href='https://doi.org/10.1109/TMC.2025.3557857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To protect users’ right to be forgotten in federated learning, federated unlearning aims at eliminating the impact of leaving users’ data on the global learned model. The current research in federated unlearning mainly concentrates on developing effective and efficient unlearning techniques. However, the issue of incentivizing valuable users to remain engaged and preventing their data from being unlearned is still under-explored, yet important to the unlearned model performance. This paper focuses on the incentive issue and develops an incentive mechanism for federated learning and unlearning. We first characterize the leaving users’ impact on the global model accuracy and the required communication rounds for unlearning. Building on these results, we propose a four-stage game to capture the interaction and information updates during the learning and unlearning process. A key contribution is to summarize users’ multi-dimensional private information into one-dimensional metrics to guide the incentive design. Interestingly, we prove that allowing federated unlearning can result in reduced payoffs for both the server and users, compared to a scenario without unlearning. Numerical results demonstrate the necessity of unlearning incentives for retaining valuable leaving users, and also show that our proposed mechanisms decrease the server's cost by up to 53.91% compared to state-of-the-art benchmarks.},
  archive      = {J_TMC},
  author       = {Ningning Ding and Zhenyu Sun and Ermin Wei and Randall Berry},
  doi          = {10.1109/TMC.2025.3557857},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8794-8810},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Incentivized federated learning and unlearning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep graph reinforcement learning for UAV-enabled multi-user secure communications. <em>TMC</em>, <em>24</em>(9), 8780-8793. (<a href='https://doi.org/10.1109/TMC.2025.3558790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While unmanned aerial vehicles(UAVs) with flexible mobility are envisioned to enhance physical layer security in wireless communications, the efficient security design that adapts to such high network dynamics is rather challenging. The conventional approaches extended from optimization perspectives are usually quite involved, especially when jointly considering factors in different scales such as deployment and transmission in UAV-related scenarios. In this paper, we address the UAV-enabled multi-user secure communications by proposing a deep graph reinforcement learning framework. Specifically, we reinterpret the security beamforming as a graph neural network (GNN) learning task, where mutual interference among users is managed through the message-passing mechanism. Then, the UAV deployment is obtained through soft actor-critic reinforcement learning, where the GNN-based security beamforming is exploited to guide the deployment strategy update. Simulation results demonstrate that the proposed approach achieves near-optimal security performance and significantly enhances the efficiency of strategy determination. Moreover, the deep graph reinforcement learning framework offers a scalable solution, adaptable to various network scenarios and configurations, establishing a robust basis for information security in UAV-enabled communications.},
  archive      = {J_TMC},
  author       = {Xiao Tang and Kexin Zhao and Chao Shen and Qinghe Du and Yichen Wang and Dusit Niyato and Zhu Han},
  doi          = {10.1109/TMC.2025.3558790},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8780-8793},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Deep graph reinforcement learning for UAV-enabled multi-user secure communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relip: Reliable in-band parallel communication for magnetic MIMO wireless power transfer system. <em>TMC</em>, <em>24</em>(9), 8761-8779. (<a href='https://doi.org/10.1109/TMC.2025.3558524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In magnetic resonant coupling (MRC) based wireless power transfer (WPT) systems, receiver (RX) feedback communication is promising to enhance the capability and efficiency of the system. Although some studies have explored in-band implementations with low overhead costs, it has not been comprehensively investigated. In this paper, we propose $\mathtt {Relip}$, a Reliable layer-level in-band parallel feedback communication mechanism for MIMO MRC-WPT systems, which addresses the impact of RX-RX couplings (i.e., non-negligible interference from strong couplings and positive effects of relay phenomenon), and provides a theoretical analysis of communication reliability. Technically, we first devise an On-Off based two-phase modulation mechanism to achieve RX identification and dependency detection under relay phenomenon. Then, we utilize observed channel decomposability to collect group-level power transfer channel conditions for eliminating the interference caused by strong RX-RX couplings. Furthermore, we perform RX selection to optimize the trade-off between communication reliability and time overhead. We design and implement the $\mathtt {Relip}$ prototype and conduct extensive experiments. The results validate the effectiveness of our mechanism, i.e., $\mathtt {Relip}$ can provide $\geq$99% average decoding accuracy for concurrent feedback communication of 14 devices, achieving an 18.31% improvement compared to the state-of-the-art solution.},
  archive      = {J_TMC},
  author       = {Xinyu Wang and Wangqiu Zhou and Hao Zhou and Shenyao Jiang and Zhi Liu and Xiaoyan Wang and Yusheng Ji and Qi Song},
  doi          = {10.1109/TMC.2025.3558524},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8761-8779},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Relip: Reliable in-band parallel communication for magnetic MIMO wireless power transfer system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mm-fall: Practical and robust fall detection via mmWave signals. <em>TMC</em>, <em>24</em>(9), 8747-8760. (<a href='https://doi.org/10.1109/TMC.2025.3557504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Falls pose a significant risk to the health and well-being of older adults, driving the development of various fall detection systems. Existing solutions have explored wearable and vision sensors, while non-invasive RF-based approaches have raised a growing interest due to their convenience and privacy considerations. Despite major advancements in RF-based passive estimation, current approaches still face challenges in handling complex real-world scenarios. They often lack the ability to generalize to new domains (i.e., people, position, environment), and struggle to accurately detect and localize a fallen person in the presence of unknown activities from nearby objects (e.g., pet animal and robot vacuum cleaner) or persons. To address these challenges, we present mm-Fall, a novel mmWave-based non-invasive fall detection system that utilizes Range-Angle (RA) energy maps to separate and localize multiple moving targets, and further accurately estimate their states. Unlike previous approaches, mm-Fall is capable of working with new domains and effectively distinguishing falls from non-fall motions that may appear similar. Additionally, it performs well in challenging conditions, such as poor lighting and occluded scenarios. Our design of mm-Fall is evaluated in 13 environments with over 16 individuals performing 24+ types of motions. The results demonstrate an impressive average recall of 0.969 and precision of 0.996 in detecting falls, whether involving single or multiple moving targets simultaneously. The source codes and dataset of mmFall are available at https://github.com/iwantlatiao/mmFall.},
  archive      = {J_TMC},
  author       = {Cui Zhao and Qiumin Luo and Han Ding and Ge Wang and Kun Zhao and Zhi Wang and Wei Xi and Jizhong Zhao},
  doi          = {10.1109/TMC.2025.3557504},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8747-8760},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mm-fall: Practical and robust fall detection via mmWave signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and adaptive diffusion model inference through lookup table on mobile devices. <em>TMC</em>, <em>24</em>(9), 8729-8746. (<a href='https://doi.org/10.1109/TMC.2025.3558203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diffusion models have revolutionized image synthesis applications. Many studies focus on using approximate computation such as model quantization to reduce inference costs on mobile devices. However, due to their extensive model parameters and autoregressive inference fashion, the overhead of diffusion models remains high, which is challenging for mobile devices to handle. To reduce the inference overhead of diffusion models on mobile devices, we propose LUT-Diff, an algorithm-system co-design specifically tailored for mobile device diffusion model inference optimization. LUT-Diff optimizes using lookup tables and can efficiently generate a series of lookup table candidates for diffusion models without end-to-end training. During inference, LUT-Diff adaptively selects the best inference strategy based on the application/user’s latency budget. Additionally, LUT-Diff includes a parallel inference engine that rapidly completes model inference through CPU-GPU co-scheduling. Extensive experiments demonstrate that LUT-Diff can generate images comparable to the original model, with an up to 0.012 MSE in generated images. LUT-Diff can also achieve up to 9.1× inference acceleration and reduce the inference memory footprint by up to 70.9% compared to baseline methods. Moreover, LUT-Diff can save at least 3281× the learning cost of lookup tables.},
  archive      = {J_TMC},
  author       = {Qipeng Wang and Shiqi Jiang and Yifan Yang and Ruiqi Liu and Yuanchun Li and Ting Cao and Xuanzhe Liu},
  doi          = {10.1109/TMC.2025.3558203},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8729-8746},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient and adaptive diffusion model inference through lookup table on mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task scheduling mechanism for crowdsourcing in mobile social networks. <em>TMC</em>, <em>24</em>(9), 8714-8728. (<a href='https://doi.org/10.1109/TMC.2025.3558275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularization of smart phones, mobile crowdsourcing emerged and gained growing attention in the recent years. Mobile users are now able to conduct complex tasks with the communication between each other. In this paper, we study the task scheduling problem in the mobile crowdsourcing systems based on the spontaneously formed mobile social networks (MSNs). We introduce two crowdsourcing task scheduling problems under this system model, with one problem aiming to minimize the total cost of some crowdsourcing tasks and the other focusing on minimizing the final completion time of the tasks belonging to the same project. We introduce a unified framework to solve the problems and propose two approximation algorithms for these two problems in the offline versions respectively and prove Their approximation ratios accordingly. Based on the two algorithms, we further design two online algorithms to deal with the workers’ dynamism and also analyze their competitive ratios. Finally, we verify the effectiveness and efficiency of the proposed methods through numerical experiments on real and synthetic datasets.},
  archive      = {J_TMC},
  author       = {Jiale Zhang and Longhao Yi and Xiaofeng Gao and Shahzad Sarwar Bhatti and Ting Yuan and Guihai Chen},
  doi          = {10.1109/TMC.2025.3558275},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8714-8728},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Task scheduling mechanism for crowdsourcing in mobile social networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Container scheduling strategy based on image layer reuse and sequential arrangement in mobile edge computing. <em>TMC</em>, <em>24</em>(9), 8700-8713. (<a href='https://doi.org/10.1109/TMC.2025.3557160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Mobile Edge Computing (MEC) scenarios, computational tasks are popularly deployed using containerization to isolate the runtime environment. To complete the execution of the task, the edge server first pulls the image, then instantiates and runs the container. Since it takes a lot of time for the edge server to download the image from the cloud, image reuse reduces the pulling latency significantly. However, the limited storage capacity of edge servers hinders image reuse. Recent works have enhanced reuse efficiency by leveraging the hierarchical structure of images and caching high-value layers. However, their efficiency remains limited due to the lack of multi-container collaboration. This paper proposes a novel container scheduling strategy based on image layer reuse and sequence arrangement (ILR-SA) for MEC scenarios, which achieves efficient scheduling by collaborating multiple containers. First, containers are greedily deployed into the edge cluster. Then, the execution sequence of containers is modeled as an optimal Hamiltonian path problem, efficiently solved by our proposed decomposition algorithm. Finally, an efficient image layer update strategy is used to achieve layer reuse. We conduct rigorous experiments to demonstrate that our proposed container scheduling strategy reduces the computational task completion time by up to 91.3% compared to existing approaches.},
  archive      = {J_TMC},
  author       = {Haijie Wu and Weiwei Lin and Haotong Zhang and Fang Shi and Wangbo Shen and Keqin Li and Albert Y. Zomaya},
  doi          = {10.1109/TMC.2025.3557160},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8700-8713},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Container scheduling strategy based on image layer reuse and sequential arrangement in mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plugging and breathing on the air: A practical defense system for deep learning-based wireless semantic communications. <em>TMC</em>, <em>24</em>(9), 8683-8699. (<a href='https://doi.org/10.1109/TMC.2025.3558793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based semantic communications (DLSC) leverage deep neural networks in transmitters and receivers, pushing the boundaries beyond Shannon limit. However, DLSC is extremely vulnerable to malicious physical-layer adversarial attacks due to the openness of wireless channels. Meanwhile, existing defense approaches still suffer from two challenges for robust DLSC. First, most methods require offline DLSC retraining to defend against various attacks, causing interruptions of online service. Second, they struggle to achieve effective defense in real-world time-varying channels, thus limiting DLSC reliability. We propose PBNet, integrating a pluggable protector and an adaptive protector to respectively address the above two challenges. First, the pluggable protector utilizes a novel denoising module to safeguard the transmitted signals, enabling hot-pluggable deployment without interrupting communication. Second, the adaptive protector leverages a novel alternating adaption strategy to achieve effective defense in time-varying channels, ensuring robust performances under real-world dynamic conditions. Evaluations involving symbols, images, texts, and speeches show the efficacy of our PBNet, which has respectively achieved an impressive 72.22% and 73.71% accuracy improvement in defending against unknown $l_{0}$-norm and $l_{2}$-norm attacks on image-based DLSC. Furthermore, we developed two real-world radio systems of PBNet to perform over-the-air signal generation, integrating hardware and software such as FPGA chips and GNU radio. We also implemented an interactive UI of PBNet based on QT5, aiming to demonstrate the effect of attacks and defense visually. This work achieves robust DLSC performances under various attacks and time-varying channels, taking a significant step towards the practical defense scheme for robust DLSC.},
  archive      = {J_TMC},
  author       = {Chenyang Qiu and Guoshun Nan and Ruiwen Liang and Wendi Deng and Yufan Zhang and Yuchong Gao and Di Wang and Meng Qu and Zhuoran Duan and Qianlong Sun and Qimei Cui and Xiaodong Xu and Xiaofeng Tao and Tony Q.S. Quek},
  doi          = {10.1109/TMC.2025.3558793},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8683-8699},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Plugging and breathing on the air: A practical defense system for deep learning-based wireless semantic communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utilizing multipath effects for mobile charging. <em>TMC</em>, <em>24</em>(9), 8668-8682. (<a href='https://doi.org/10.1109/TMC.2025.3557899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Wireless Rechargeable Sensor Networks (WRSNs) have emerged as a promising solution to address the energy limitations of wireless sensor networks. In practical applications of WRSNs, environmental objects are ubiquitous, reflecting radio waves and causing them to reach sensors via multiple paths. These multipath effects significantly impact the power intensity received by sensors. In this paper, we study a fundamental issue of charGing schEduling with mulTipath effectS (GETS), that is, how to schedule a mobile charger by comprehensively considering the multipath effects to maximize the overall charging utility. To this end, we first establish a charging model with environmental objects to investigate the impact of multipath effects on power distribution. Then, we propose a charging scheduling scheme that not only selects a series of sojourn locations for the MC (Mobile Charger) to maximize the total power received by nearby sensors but also construct a charging path that avoids environmental objects. We conduct extensive simulations as well as indoor and outdoor field experiments to evaluate the performance of our scheme. The results demonstrate that, on average, our scheme outperforms baseline algorithms by 48.87% .},
  archive      = {J_TMC},
  author       = {Jing Gao and Die Wu and Linglin Zhang and Jingwen Li and Jin Yang and Jian Peng and Tang Liu},
  doi          = {10.1109/TMC.2025.3557899},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8668-8682},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Utilizing multipath effects for mobile charging},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From vision to motion: Translating large-scale knowledge for data-scarce IMU applications. <em>TMC</em>, <em>24</em>(9), 8656-8667. (<a href='https://doi.org/10.1109/TMC.2025.3556998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pre-training representations acquired via self-supervised learning could achieve high accuracy on even tasks with small training data. Unlike in vision and natural language processing domains, pre-training for IMU-based applications is challenging, as there are few public datasets with sufficient size and diversity to learn generalizable representations. To overcome this problem, we propose IMG2IMU that adapts pre-trained representation from large-scale images to diverse IMU sensing tasks. We convert the sensor data into visually interpretable spectrograms for the model to utilize the knowledge gained from vision. We further present a sensor-aware pre-training method for images that enables models to acquire particularly impactful knowledge for IMU sensing applications. This involves using contrastive learning on our augmentation set customized for the properties of sensor data. Our evaluation with four different IMU sensing tasks shows that IMG2IMU outperforms the baselines pre-trained on sensor data by an average of 9.6%p F1-score, illustrating that vision knowledge can be usefully incorporated into IMU sensing applications where only limited training data is available.},
  archive      = {J_TMC},
  author       = {Hyungjun Yoon and Hyeongheon Cha and Hoang C. Nguyen and Taesik Gong and Sung-Ju Lee},
  doi          = {10.1109/TMC.2025.3556998},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8656-8667},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {From vision to motion: Translating large-scale knowledge for data-scarce IMU applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PGVMatch: Privacy-preserving and fine-grained crowdsourcing task matching with lightweight on-chain public verifiability. <em>TMC</em>, <em>24</em>(9), 8642-8655. (<a href='https://doi.org/10.1109/TMC.2025.3556249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure task matching has been a crucial research problem in crowdsourcing, requiring the alignment of workers’ preferences and requesters’ task requirements while ensuring user privacy and matching integrity. Recently, some researchers applied blockchain to crowdsourcing, either replacing the platform for decentralization or recording proofs for public verification to defend against malicious platforms. However, they still suffer from unitary coarse-grained matching models or expensive on-chain costs. To address these limitations, we propose PGVMatch, a privacy-aware and fine-grained crowdsourcing task-matching scheme with lightweight on-chain public verifiability. Our scheme is constructed on our newly proposed cryptographic primitive–Multi-authority Attribute-Based Keyword Search with Public Verifiability (MABKS-PV), which avoids access policy leakage and key escrow risks on a single authority, meanwhile adding constant-size proof generation and lightweight verification algorithms to a basic ABKS construction. In PGVMatch, requesters can select workers with fine-grained attribute demands, and workers can pick interested tasks with multi-keyword search, preserving dual-side privacy. The matching process is conducted off-chain, while constant-size proofs are recorded on-chain for efficient and public verification of matching integrity. Security analysis and extensive experiments on the Hyperledger Fabric blockchain demonstrate both the security and our superior performance. PGVMatch outperforms the existing scheme with the fastest matching result verification, achieving a 29% improvement in throughput and a 33% reduction in latency.},
  archive      = {J_TMC},
  author       = {Liang Li and Haiqin Wu and Jiachen Shen and Zhenfu Cao and Xiaolei Dong},
  doi          = {10.1109/TMC.2025.3556249},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8642-8655},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PGVMatch: Privacy-preserving and fine-grained crowdsourcing task matching with lightweight on-chain public verifiability},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From earth to orbit: Launch sequence optimization for LEO mega-constellations. <em>TMC</em>, <em>24</em>(9), 8625-8641. (<a href='https://doi.org/10.1109/TMC.2025.3556227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent emergence of Low Earth Orbit (LEO) mega-constellations, designed for high-speed broadband connections with low latency, has introduced new deployment challenges. Efficient launch sequence planning is crucial for rapid service rollout, performance enhancement, and service promotion. However, existing research predominantly focuses on the design and performance analysis of fully-deployed constellations and overlooks the evolving process from a partially-deployed constellation to a fully-deployed one. This paper explores the launch sequence optimization problem for mega-constellations, tailored to expedite service delivery and adapt to changing performance demands. To this end, (1) we identify critical network performance metrics for the constellation evolving process and construct a simulation toolchain capable of simulating and evaluating these metrics for any potential partially-deployed constellation. (2) Drawing upon three key observations on network availability, the number of visible satellites, and latency, we propose an algorithm that can construct a launch sequence for an arbitrary mega-constellation topology. Evaluation results show that this algorithm enables the early provision of services and maximizes network performance gains at each launch batch while catering to different user demands. For instance, our algorithm can achieve network performance nearly equivalent to that of Starlink when it initiated its service, without losing redundancy, while using 55% fewer satellites.},
  archive      = {J_TMC},
  author       = {Tianze Huang and Qing Li and Chenren Xu and Mengwei Xu and Shangguang Wang and Gang Huang and Xuanzhe Liu},
  doi          = {10.1109/TMC.2025.3556227},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8625-8641},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {From earth to orbit: Launch sequence optimization for LEO mega-constellations},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computation offloading and resource allocation in mixed Cloud/Vehicular-fog computing systems. <em>TMC</em>, <em>24</em>(9), 8612-8624. (<a href='https://doi.org/10.1109/TMC.2025.3556315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of vehicular user equipment (V-UE) in the Internet-of-Vehicles (IoV) systems, cloud computing alone cannot process all V-UE tasks, especially those latency-sensitive ones. Although static roadside fog nodes have been employed to offload computation from V-UEs, mobile fog nodes carried by vehicles that have the potential to further improve the performance of computation offloading for vehicular tasks have not been sufficiently studied for IoV systems. In this paper, we consider a mixed cloud/vehicular-fog computing (VFC) system that employs vehicle-carried fog nodes (V-FNs) in addition to cloud servers to offload tasks from V-UEs. To minimise the maximum service delay (which includes the transmission delay, queueing delay, and processing delay) among all V-UEs, we jointly optimise the offloading decisions of all V-UEs, the computation resource allocation at all V-FNs, the allocation of resource block (RB) and transmission power for all V-UEs while considering the mobility of V-UEs and V-FNs. The joint optimisation is solved by devising a fireworks algorithm-based offloading decision optimisation scheme, in conjunction with a bisection method-based V-FN computation resource allocation scheme and a clustering-based communication resource allocation scheme. Simulation results show that our proposed schemes outperform the benchmarks in terms of service the maximum delay among all V-UEs.},
  archive      = {J_TMC},
  author       = {Bintao Hu and Jianbo Du and Jie Zhang and Xiaoli Chu},
  doi          = {10.1109/TMC.2025.3556315},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8612-8624},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Computation offloading and resource allocation in mixed Cloud/Vehicular-fog computing systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RIS-assisted seamless connectivity in wireless multi-hop relay networks. <em>TMC</em>, <em>24</em>(9), 8600-8611. (<a href='https://doi.org/10.1109/TMC.2025.3557676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, reconfigurable intelligent surfaces (RIS) have garnered significant attention for their ability to control the phase shifts in reflected signals. By intelligently adjusting these phases, RIS can establish seamless direct paths between communication devices obstructed by obstacles, eliminating the need for forwarding and significantly reducing system overhead associated with relaying. This capability is crucial in multi-hop ad hoc networks requiring multiple relay steps. Consequently, the concept of incorporating multi-hop RIS into wireless multi-hop relay networks has emerged. In this paper, we propose a novel network model where each UAV communication node is equipped with a RIS, facilitating seamless connections in multi-hop relay wireless networks. We analyze the performance of this model by integrating RIS-assisted physical layer modeling into the seamless connection network framework and conducting a detailed comparative analysis of RIS-assisted and conventional connections. At the medium access layer, we introduce a RIS-DCF MAC protocol based on the IEEE 802.11 distributed coordination function (DCF), modeling the medium access process as a two-hop access scenario. Our results demonstrate that the seamless connections and diversity gain provided by RIS significantly enhance the performance of multi-hop relay wireless networks.},
  archive      = {J_TMC},
  author       = {Peini Yi and Wenchi Cheng and Jingqing Wang and Wei Zhang},
  doi          = {10.1109/TMC.2025.3557676},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8600-8611},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RIS-assisted seamless connectivity in wireless multi-hop relay networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic hierarchical reinforcement learning framework for energy-efficient 5G base stations in urban environments. <em>TMC</em>, <em>24</em>(9), 8582-8599. (<a href='https://doi.org/10.1109/TMC.2025.3557280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The energy consumption of 5G base stations (BSs) is significantly higher than that of 4G BSs, creating challenges for operators due to increased costs and carbon emissions. Existing solutions address this issue by switching off BSs during specific periods or forming cooperation coalitions where some BSs deactivate while others serve users. However, these approaches often rely on fixed geographic configurations, making them unsuitable for urban areas with numerous BSs and mobile users. To tackle these challenges, we propose a hierarchical reinforcement learning (RL) framework for energy conservation in large-scale 5G networks. In the upper-layer, we propose a deep Q-network integrated with a graph convolutional network that dynamically groups BSs into coalitions from a macro perspective. This layer focuses on high-level coalition formation to optimize system-wide energy efficiency by considering the global state of the network. In the lower-layer, we combine attention mechanism with multi-agent RL and graph convolutional networks to design a scalable algorithm that maximizes local energy efficiency through optimizing the cooperation within each coalition. These two layers align global coalition dynamics with local intra-coalition cooperation to achieve system-wide energy optimization. Moreover, we accurately model large-scale urban 5G scenarios leveraging a high-fidelity network simulator, which enables our RL framework to learn from real-world feedback. Extensive experiments conducted with the simulator demonstrate that our proposed framework achieves remarkable energy savings of up to 75.6%, significantly outperforming baseline approaches. These findings highlight the effectiveness and superiority of our hierarchical RL optimization framework in addressing the energy consumption challenges faced by large-scale 5G networks.},
  archive      = {J_TMC},
  author       = {Dianlei Xu and Xiang Su and Gopika Premsankar and Huandong Wang and Sasu Tarkoma and Pan Hui},
  doi          = {10.1109/TMC.2025.3557280},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8582-8599},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic hierarchical reinforcement learning framework for energy-efficient 5G base stations in urban environments},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NBLoc: A narrowband RF localization system for wide-area indoor applications. <em>TMC</em>, <em>24</em>(9), 8564-8581. (<a href='https://doi.org/10.1109/TMC.2025.3556743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce NBLoc, a narrowband frequency hopping, long-range localization system designed for low-power Internet of Things (IoT) devices. Traditional high-accuracy localization systems typically require wide-bandwidth and power-demanding radio frequency (RF) circuits, leading to limitations such as short operational range and high power consumption to achieve decimeter-level accuracy. NBLoc overcomes these challenges by using narrowband symbols with a frequency-hopping mechanism across a wide bandwidth, enabling the localization of low-power tags over large areas. NBLoc features a novel custom-designed RF analog frontend (AFE) integrated circuit (IC), that eliminates the need for a conventional phase-locked loop, significantly reducing the cost and power consumption of the receiving tag. This advancement is enabled by NBLoc's thoughtful waveform design and specialized signal processing algorithms, which mitigate phase noise and uncertainty. Compared to previous solutions, NBLoc achieves lower power consumption and extended operational range due to its narrowband symbols while maintaining high localization accuracy by leveraging a 100 MHz localization bandwidth through frequency hopping. In NBLoc, system anchors transmit narrowband orthogonal symbols, hopping across the localization bandwidth in a predetermined pattern known to the tag. The tag, equipped with the custom low-power RF AFE IC, dynamically tunes its local oscillator (LO) frequency to match the hop pattern and capture these symbols, which are then used to estimate the channel impulse response (CIR). The tag calculates the time difference of arrival (TDOA) for each anchor pair from the CIRs, and determines its 2D location via multilateration. The system was implemented and tested using the low-power RF AFE IC in both line-of-sight (LOS) and non-line-of-sight (NLOS) environments, achieving decimeter-level accuracy across areas as large as 269 × 125 m$^{2}$.},
  archive      = {J_TMC},
  author       = {Demba Komma and Chien-Wei Tseng and Andrea Bejarano-Carbo and Mingyu Yang and David Blaauw and Hun-Seok Kim},
  doi          = {10.1109/TMC.2025.3556743},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8564-8581},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {NBLoc: A narrowband RF localization system for wide-area indoor applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient over-the-air computation in UAV-assisted IIoT networks. <em>TMC</em>, <em>24</em>(9), 8549-8563. (<a href='https://doi.org/10.1109/TMC.2025.3556382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In remote industrial Internet of Things (IIoT) monitoring systems, the uncrewed aerial vehicle (UAV) serves as supplementary infrastructure to aggregate data from a large number of distributed sensors, and achieve industrial operation intelligence. In the wireless data aggregation process, using conventional orthogonal multiple access techniques face challenges such as scarce bandwidth, high communication latency and energy consumption. To tackle these issues, the over-the-air computation (AirComp) technique has emerged. It allows concurrent data transmissions from sensors, as well as integrates communication and computation processes, ultimately enabling fast data aggregation. However, the energy consumption issue remains unresolved. In this paper, we exploit spatial correlations among sensor measurements, and design an energy-efficient AirComp in UAV-assisted IIoT networks, where only a subset of sensors transmit data instead of all sensors. Then, we derive a closed-form expression for the mean square error (MSE) of each combination under a specific number of sensor transmissions. By jointly optimizing the UAV deployment and pre-coding coefficients of sensors, we formulate the problem of minimizing MSE for each combination of transmitted sensors. Furthermore, the MSE optimization algorithm is developed to output the average MSE of all combinations. Finally, we evaluate the average MSE and network lifetime performance of proposed scheme.},
  archive      = {J_TMC},
  author       = {Yali Chen and Sheng Sun and Min Liu and Bo Ai and Yuwei Wang and Yunhao Liu},
  doi          = {10.1109/TMC.2025.3556382},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8549-8563},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy-efficient over-the-air computation in UAV-assisted IIoT networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User authentication on smart speakers leveraging acoustic imaging. <em>TMC</em>, <em>24</em>(9), 8532-8548. (<a href='https://doi.org/10.1109/TMC.2025.3556404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The user authentication has drawn increasingly attention as the smart speaker becomes more prevalent. For example, smart speakers that can verify who is sending voice commands can mitigate various types of attacks, such as replay attack or impersonation attack. Existing user authentication solutions either cannot be applicable to smart speakers directly or require certain additional user-device interaction or pre-installed infrastructure, which may severely affect the user experience and create extra burdens to users. In this work, we propose a user authentication system utilizing acoustic images, which are derived from the smart speaker by emitting beep signals and sensing echoes from the user’s body with its microphone array, as the proof for user authentication. Given the acoustic samplings of the reflected beep signal, our system designs a distance estimation component by applying a correlation based technique on the beamformed signal to estimate the distance between the user and microphone array. Our image construction component then constructs a virtual imaging plane using the estimated distance and steers the array towards each grid of the plane to generate an acoustic image of the user. We also propose a transfer learning-based method to derive efficient features from the constructed images, and employ SVM classifiers for accurate user authentication. Moreover, to ensure the accuracy of our system, our user direction estimation scheme could further estimate the DoA of the user’s voice command to make sure that the user stands in front of the smart speaker during the authentication process. Our extensive experiments demonstrate that our system is robust and accurate across various scenarios.},
  archive      = {J_TMC},
  author       = {Yanzhi Ren and Zhiliang Xia and Siyi Li and Hongbo Liu and Jiadi Yu and Shuai Li and Hongwei Li},
  doi          = {10.1109/TMC.2025.3556404},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8532-8548},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {User authentication on smart speakers leveraging acoustic imaging},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward real-time digital twin of physical reality via intelligent wireless resource allocation. <em>TMC</em>, <em>24</em>(9), 8520-8531. (<a href='https://doi.org/10.1109/TMC.2025.3557867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhanced Mobile Broadband (eMBB) and Ultra Reliable Low Latency Communication (URLLC) are two important wireless communication traffics to build a digital twin of physical reality. Therein, eMBB and URLLC traffics are to transmit high-quality sensed data and critical commands, respectively. To support these two important traffics, we develop an intelligent resource allocation mechanism. First, we model the time-frequency resource allocation as an optimization problem aiming to maximize the throughput for the eMBB traffics according to their urgency subject to the constraint on the successful transmission for the URLLC traffics. In this way, the amount of resources allocated to each traffic can be appropriately determined without causing waste in resource usage. Second, we propose a feasible low-complexity solution for the optimization problem by relaxing it and then applying linear programming. Third, to address the possible failure of the algorithm due to the relaxation, we propose a post-processing by puncturing the resource initially allocated to eMBB traffics and thereafter reallocating this resource to URLLC traffics. By such, the characteristic of the eMBB traffic, i.e., high throughput, and that of the URLLC traffic, i.e., low latency and ultra reliability, can be achieved. We perform system-level simulations on Matlab 5 G simulation platform to evaluate the performance of the proposed mechanism under different scenarios. Simulations show that the proposed mechanism achieves better performance compared to existing schemes regarding the total eMBB throughput and URLLC failure probability on all the scenarios.},
  archive      = {J_TMC},
  author       = {Yuhong Wang and Shaohan Feng and Yonghong Zeng and Sumei Sun and Peng Hui Tan},
  doi          = {10.1109/TMC.2025.3557867},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8520-8531},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward real-time digital twin of physical reality via intelligent wireless resource allocation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile parcels’ grasping detection system by the neuromorphic vision and efficient fusion network. <em>TMC</em>, <em>24</em>(9), 8506-8519. (<a href='https://doi.org/10.1109/TMC.2025.3556735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing popularity of online shopping has resulted in a surge of parcels that need to be sorted, which exerts great challenges to the sorting work. Robotic grasp can greatly improve the sorting efficiency. However, the dynamic grasp of moving parcels requires higher detection speed and grasping pose calculation accuracy. To address these requirements, this study proposes a new grasping system through the Neuromorphic vision (NeuroIV), which owns the advantages of low latency and lightweight computing. As a young field, Neuromorphic camera is rarely used in robotic grasp. In view of this, we present a novel parcel-grasping dataset. After that, a double channels’ down-sampling and grasping network (DCDG-Net) is designed, which can extract abundant features with ResNet and transformer branches, respectively. To mitigate the calculation burden introduced by the dual channels' network, we design a feature-vector multiplication to replace the dot-product multiplication, thereby reducing the computational load among different matrixes. Furthermore, channel and space attentions are fused to construct multidimensional network to suppress noisy features and highlight useful information. Finally, we have evaluated the proposed method in real-world scenarios. Together with qualitative and quantitative comparisons, this work provides a state-of-the-art grasping detection with the new NeuroIV dataset and network.},
  archive      = {J_TMC},
  author       = {Xiangyong Liu and Xuesong Sun and Zhiqiang Xu},
  doi          = {10.1109/TMC.2025.3556735},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8506-8519},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobile parcels’ grasping detection system by the neuromorphic vision and efficient fusion network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An incentive framework for task offloading in edge computing marketplaces under price competition. <em>TMC</em>, <em>24</em>(9), 8492-8505. (<a href='https://doi.org/10.1109/TMC.2025.3556516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To efficiently execute tasks, computation resource requesters (CRRs) with limited resources can offload their tasks to nearby computation resource providers (CRPs) with spare computing capacity. These CRPs require appropriate incentives to compensate for their incurred costs when helping process the offloaded tasks. Although several mechanisms have been designed to incentivize CRPs, none of them have investigated the incentive mechanism considering price-setting and price-taking CRPs simultaneously. In this work, we propose an incentive framework for task offloading in the edge computing marketplace that includes both price-setting and price-taking CRPs. We model the CRR's interactions with both types of CRPs as a three-stage Stackelberg game to maximize the profit for both the CRR and CRPs. We prove the existence of a unique subgame perfect equilibrium (SPE) of the formulated game and further develop iterative algorithms for the CRR and price-setting CRPs to achieve the equilibrium. Through the designed algorithms, each CRP does not require complete information about the CRR and other CRPs. Extensive simulations demonstrate that offloading tasks to both price-setting and price-taking CRPs achieves higher profits for the CRR and price-setting CRPs compared to offloading tasks solely to price-setting CRPs. Additionally, the obtained SPE can achieve near-optimal social welfare.},
  archive      = {J_TMC},
  author       = {Liantao Wu and Peng Sun and Zhibo Wang and Xiaoyi Pang and Jiahui Hu and Honglong Chen and Yang Yang},
  doi          = {10.1109/TMC.2025.3556516},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8492-8505},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An incentive framework for task offloading in edge computing marketplaces under price competition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge intelligence enhanced monte carlo tree search for virtually coupled train set optimal control. <em>TMC</em>, <em>24</em>(9), 8475-8491. (<a href='https://doi.org/10.1109/TMC.2025.3556143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtually Coupled Train Set (VCTS) is an advanced train control technology enabling multiple trains to operate closely through wireless communication, enhancing capacity and operational flexibility. Traditional VCTS control algorithms struggle with complex dynamic models and local optimality, hindering real-time, long-term optimization. This paper proposes an Edge Intelligence (EI) enhanced Monte Carlo Tree Search (MCTS) framework for VCTS Optimal Control (M-VOC). MCTS is a heuristic search algorithm that identifies optimal operational solutions efficiently, focusing on long-term stability over local optimums. EI supports MCTS for real-time decision-making, and we introduce a model-based reinforcement learning algorithm to manage VCTS's complex dynamics. Our framework addresses VCTS control issues in real-time while optimizing long-term benefits. To meet computational and real-time demands, we propose a train-to-edge cooperative computing strategy using multi-intelligence reinforcement learning. Simulations demonstrate that our EI-enhanced MCTS strategy effectively provides cooperative control, ensuring virtually coupled trains operate safely, stably, and punctually with reduced intervals.},
  archive      = {J_TMC},
  author       = {Taiyuan Gong and Li Zhu and Yang Li and Shuomei Ma and F. Richard Yu and Tao Tang},
  doi          = {10.1109/TMC.2025.3556143},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8475-8491},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge intelligence enhanced monte carlo tree search for virtually coupled train set optimal control},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acoustic sensing for multi-user heartbeat monitoring using dualforming. <em>TMC</em>, <em>24</em>(9), 8454-8474. (<a href='https://doi.org/10.1109/TMC.2025.3558418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic sensing for heartbeat monitoring has emerged as a prevailing research topic in wireless sensing. However, existing acoustic sensing systems face two limitations: a restricted sensing range and operation limited to a single user, impeding large-scale deployment of its applications. In this paper, we present DF-Sense, a Dual Forming based multi-user acoustic Sensing system for heartbeat monitoring in home settings. Specifically, we design a novel sensing signal-to-noise ratio (SSNR) enhancement model, namely Dualforming, which leverages constructive superposition across multiple subcarriers and microphones. To facilitate Dualforming, we propose a novel MUltiple Subtle SIgnal Classification (MUS$^{2}$<mml:math><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>IC) method and a 2-D peak identification scheme to locate and identify multiple subjects with subtle motions. Additionally, we propose a phase change-based method to promptly identify body leaning and adaptively re-localize subjects, thereby avoiding the high computational cost. Finally, we propose an enhanced recursive least squares (RLS) filter to effectively reconstruct high-quality heartbeat waveforms from Channel Frequency Response (CFR) signals affected by limb movements. Experimental results show that DF-Sense achieves high precision measurement of instantaneous heart rates within a range of 10 m, sufficient for most daily space requirements, and can monitor heartbeat for up to 6 subjects in a 2-D space.},
  archive      = {J_TMC},
  author       = {Lei Wang and Tao Gu and Jingyu Li and Haipeng Dai and Chenren Xu and Daqing Zhang},
  doi          = {10.1109/TMC.2025.3558418},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8454-8474},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Acoustic sensing for multi-user heartbeat monitoring using dualforming},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient registered attribute based access control with same sub-policies in mobile cloud computing. <em>TMC</em>, <em>24</em>(9), 8441-8453. (<a href='https://doi.org/10.1109/TMC.2025.3556279'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ciphertext-policy attribute-based encryption (CP-ABE) has long been considered as a promising access control technology for cloud storage. However, CP-ABE depends on a central trusted authority to generate and distribute decryption keys, resulting in the key escrow issue. Most existing solutions only mitigate this problem but fail to resolve it entirely. Registered attribute-based encryption (RABE), a new cryptographic primitive, fundamentally addresses the key escrow problem by modifying the trust model, but its high computational overhead limits its practical application. Inspired by this challenge, we present an efficient registered attribute-based access control scheme designed for data encrypted with access policies containing the same sub-policy. In our scheme, users generate their own keys, while a key manager, who does not hold keys, replaces the central authority in managing users. Additionally, for data encrypted with the same sub-policy, the user’s initial decryption stores the relevant parameters, which can be used for subsequent decryptions to reduce computational overhead. The proposed scheme is proven to achieve semantic security. Performance analysis demonstrates that our scheme enhances decryption efficiency by roughly 41.4$\%$ compared to existing RABE scheme, with a minimal storage trade-off, making it more practical for cloud storage application.},
  archive      = {J_TMC},
  author       = {Wuwei Weng and Jiguo Li and Yichen Zhang and Yang Lu and Jian Shen and Jinguang Han},
  doi          = {10.1109/TMC.2025.3556279},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8441-8453},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient registered attribute based access control with same sub-policies in mobile cloud computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FPSelector: A flexible path selector for mobile augmented reality offloading. <em>TMC</em>, <em>24</em>(9), 8423-8440. (<a href='https://doi.org/10.1109/TMC.2025.3556473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Augmented Reality (MAR) applications pose unique challenges due to computation intensity, constrained device resources, and high interactive rendering requirements. The emergence of 5G and edge computing offers opportunities to offload computation to the edge and cloud, indirectly enhancing the computing capability and usage duration of MAR devices. However, existing general task offloading and multipath transmission techniques do not address the challenges in offloading path selection with multiple edges, dynamic resource competition awareness, and spatial computation with strong task dependencies. This paper contributes FPSelector, a flexible path selector for MAR offloading. We present a two-tier MAR-specific offloading scheme with multiple edge nodes. In offloading decisions, we design a reinforcement learning model to generate the selection policy for each packet of an AR data stream. This model incorporates an action masking mechanism, a comprehensive reward function, and state features complemented by a resource prediction module, making FPSelector aware of dynamic heterogeneous environments. Moreover, we propose an online learning strategy to facilitate real-time selection. To validate its efficacy, we compare FPSelector’s performance against leading schedulers under various scenarios, demonstrating a notable reduction of 9.9% and 9.6% in overall completion time for 4 K and 8 K video-based MAR applications compared to its closest competitor.},
  archive      = {J_TMC},
  author       = {Yuanwei Zhu and Yakun Huang and Xiuquan Qiao and Xiaoli Liu and Xiang Su and Anna Brunström and Özgü Alay and Sasu Tarkoma},
  doi          = {10.1109/TMC.2025.3556473},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8423-8440},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FPSelector: A flexible path selector for mobile augmented reality offloading},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards bi-level Supply/Demand balanced charging systems via online power scheduling. <em>TMC</em>, <em>24</em>(9), 8405-8422. (<a href='https://doi.org/10.1109/TMC.2025.3558550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of transportation electrification, an increasing number of charging stations have been established, forming a city-scale charging system. These charging stations serve as intermediaries that connect supply and demand, drawing power from the grid and renewable energy sources to provide electricity to electric vehicles. Maintaining a delicate balance between supply and demand has emerged as a significant challenge for the charging system. On a macroscopic level, it impacts the power grid’s peak load and reliability, while locally, it influences electric vehicle detour events. To comprehensively model the spatio-temporal characteristics in the charging system, we partition the charging system by adopting a supply-demand-aware approach and propose OPS, an online power scheduling algorithm based on the regularization technique. OPS aims to achieve a bi-level balance between supply and demand while constraining the power output of the charging system. We substantiate the efficacy of OPS through rigorous theoretical proofs, demonstrating its comparability to the optimal solution. Furthermore, we conduct extensive evaluation experiments with real-world data sets to establish the feasibility of the proposed methodology in alleviating the supply-demand imbalance. The results indicate that OPS attains an empirical competitive ratio of less than 1.2.},
  archive      = {J_TMC},
  author       = {Xinyu Lu and Jiong Lou and Jie Li and Runhui Xu and Chentao Wu and Zhi Liu and Yuan Luo and Yang Yang},
  doi          = {10.1109/TMC.2025.3558550},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8405-8422},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards bi-level Supply/Demand balanced charging systems via online power scheduling},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Competitive multi-armed bandit games for resource sharing. <em>TMC</em>, <em>24</em>(9), 8393-8404. (<a href='https://doi.org/10.1109/TMC.2025.3555971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern resource-sharing systems, multiple agents access limited resources with unknown stochastic conditions to perform tasks. When multiple agents access the same resource (arm) simultaneously, they compete for successful usage, leading to contention and reduced rewards. This motivates our theoretical study of competitive multi-armed bandit (CMAB) games. In this paper, we study a new $N$-player $K$-arm competitive MAB game, where non-myopic players (agents) compete with each other to form diverse private estimations of unknown arms over time. Their possible collisions on the same arms and the time-varying nature of arm rewards make the policy analysis here more involved than the existing studies for myopic players. We explicitly analyze the threshold-based structures of the social optimum and the existing selfish policy, showing that the latter causes prolonged convergence times $\Omega (\frac{K}{\eta ^{2}}\ln ({\frac{KN}{\delta }}))$, while the socially optimal policy with coordinated communication reduces it to $\mathcal {O}(\frac{K}{N\eta ^{2}}\ln {(\frac{K}{\delta })})$. Based on the policy comparison, we prove that the competition among selfish players for the best arm can result in an infinite price of anarchy (PoA), indicating an arbitrarily large efficiency loss compared to the social optimum. We further prove that no informational (non-monetary) mechanism (including Bayesian persuasion) can reduce the infinite PoA, as strategic misreporting by non-myopic players undermines such approaches. To address this, we propose a Combined Informational and Side-Payment (CISP) mechanism, which provides socially optimal arm recommendations with proper informational and monetary incentives to players according to their diverse and time-varying private beliefs. Our CISP mechanism keeps ex-post budget balanced for the social planner and ensures truthful reporting from players, thereby achieving the minimum $\text{PoA}=1$ and the same convergence time as the social optimum.},
  archive      = {J_TMC},
  author       = {Hongbo Li and Lingjie Duan},
  doi          = {10.1109/TMC.2025.3555971},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8393-8404},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Competitive multi-armed bandit games for resource sharing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drone-assisted IRS system in 5G and beyond: Improving reliability and enhancing the network life span. <em>TMC</em>, <em>24</em>(9), 8379-8392. (<a href='https://doi.org/10.1109/TMC.2025.3556043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an drone-assisted intelligent reflecting surface (IRS) for device-to-device (D2D) communication in infrastructure-less scenarios. The aim of this paper is to enhance the reliability among D2D ground users (GUs) and extend the lifespan of 5G and beyond 5G (B5G) wireless communication system. This work may be applicable for packet delivery in bustling urban areas, especially where ground-to-ground (G2G) links are in deep fade. For modeling air-to-ground (A2G) links among GUs to IRS/drone and IRS/drone to GUs, we consider a height-dependent Nakagami-$m$ channel model for small-scale fading and height-dependent path-loss exponent for modeling large-scale fading. The lifespan of the network is improved by proposing height-dependent energy harvesting (EH) at drone. We derive the cumulative distribution function (CDF) of the signal-to-noise ratio (SNR) whenever the signal reaches the receiving node, either via drone or via IRS. We also develop the expression of spectral efficiency and derive a closed-form expression of an outage probability by taking the combined effect of the signal for the proposed scenario using decode-and-forward (DF) and amplify-and-forward (AF) relaying at the drone. Additionally, the statistical parameters such as mean, variance, and probability density function (PDF) of total noise are derived, which is useful at the receiver node for estimating the bit error rate (BER). The analytical result is validated with simulation results, and the work is compared with the existing state-of-the-art.},
  archive      = {J_TMC},
  author       = {Pankaj Kumar and Nikita Goel and Manoj Tolani},
  doi          = {10.1109/TMC.2025.3556043},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8379-8392},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Drone-assisted IRS system in 5G and beyond: Improving reliability and enhancing the network life span},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A CAV cooperative lane change protocol with CTH safety guarantee on dedicated highways. <em>TMC</em>, <em>24</em>(9), 8362-8378. (<a href='https://doi.org/10.1109/TMC.2025.3558922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autopiloting Connected and Autonomous Vehicles (CAVs) is an important application for mobile computing. A promising context to realize autopiloting CAVs is cooperative driving on dedicated highways. For such a context, an indispensable driving scenario is Cooperative Lane Change (CLC). Due to the safety concerns of this driving scenario, a verifiably safe solution is needed (at least, the solution design should be formally provably safe). However, this demand is complicated by the inherently unreliable wireless communications between the CAVs. In this paper, we focus on the well-adopted Constant Time Headway (CTH) safety rule. We propose a CLC protocol, and formally prove its guarantee of the CTH safety and liveness, even under arbitrary wireless packet losses. These theoretical claims are further confirmed by our simulations. The simulation results also show that our proposed protocol significantly improves lane change success rates (by $5.3\% \sim +\infty \%$) than other alternatives under adverse conditions. Furthermore, the sensitivity study results also show our protocol can tolerate reasonable disturbances.},
  archive      = {J_TMC},
  author       = {Xueli Fan and Jieming Chen and Qixin Wang and Edward Chung},
  doi          = {10.1109/TMC.2025.3558922},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8362-8378},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A CAV cooperative lane change protocol with CTH safety guarantee on dedicated highways},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-user behavioral privacy filtering for mmWave radar sensing. <em>TMC</em>, <em>24</em>(9), 8347-8361. (<a href='https://doi.org/10.1109/TMC.2025.3556674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an advanced technology for non-contact sensing, mmWave radar enables fine-grained measurement of a wide variety of user behaviors. While creating intelligence and convenience, it also concerns behavioral privacy and security, as radar signals contain a wealth of behavioral information. Existing solutions are either incapable of customizable privacy protections or cannot cope with multi-person scenarios. This paper presents a Multi-user behavioral privacy Filter, MuFilter, a data masking system centered on the idea of dimensional signal interference. It determines the sensing signatures that need to be preserved or interfered with based on the sensing services that users want to enable and disable, thereby making targeted tampering on the radar signal. On this basis, we introduce the multi-person tracking technology to allow MuFilter to determine the number of users in unknown scenarios. Moreover, a subspace tampering technique is proposed to ensure that each tampering only affects the target user and not other users, thus supporting personalized privacy protection for multiple users. Experiments show that MuFilter can interfere with targeted behavioral signatures with a 100% success rate, while the degree of impact on other users’ signatures ranges from 0% to 3.85%.},
  archive      = {J_TMC},
  author       = {Xiulong Liu and Hankai Liu and Jiaqi Zhang and Xin Xie and Keqiu Li},
  doi          = {10.1109/TMC.2025.3556674},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8347-8361},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-user behavioral privacy filtering for mmWave radar sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy minimization oriented hybrid semantic data transmission in air-ocean integrated networks: A resource allocation design. <em>TMC</em>, <em>24</em>(9), 8329-8346. (<a href='https://doi.org/10.1109/TMC.2025.3556476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of new generation communication technologies, the future maritime information networks pave the way to promote the exploration of ocean resources. Moreover, the underwater data center (UDC) is considered to be a significant data storage and computing unit in future maritime networks for providing ocean services. However, the current deployment of UDC faces the critical issues, i.e., the long-distance underwater transmission is unreliable and the energy consumption and resources of underwater transmission are overloaded. To address the two critical issues of unreliable data transmission and high resource overheads, in this paper, we present a hybrid semantic data transmission architecture in air-ocean integrated networks, which can perceive the sea surface data accurately and transmit it to the UDC for processing. Specifically, in surface layer, uncrewed aerial vehicles (UAVs) perceive ocean environment and send data to the buoy via non-orthogonal multiple-access (NOMA) transmission to improve the channel utilization. In underwater layer, the buoy sends the collected data to UDC via semantic transmission, while the semantic fidelity metric is utilized to improve the transmission efficiency. A resource allocation problem for energy minimization is formulated to jointly optimize the semantic scaling factor, the NOMA decoding order, the communication and computing resource allocations. We exploit a decomposition approach to transform the problem into two sub-problems, where the optimal resource allocations are obtained by proposing efficient algorithms. Finally, we provide simulations to verify the effectiveness and efficiency of our proposed scheme. The results demonstrate that our proposal has the advantages of lower energy consumption compared to several baseline schemes.},
  archive      = {J_TMC},
  author       = {Minghui Dai and Tianshun Wang and Shan Chang and Zhou Su and Yuan Wu},
  doi          = {10.1109/TMC.2025.3556476},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8329-8346},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy minimization oriented hybrid semantic data transmission in air-ocean integrated networks: A resource allocation design},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SatCooper: Enhancing cooperative inference analytics for satellite service via multi-exit DNNs. <em>TMC</em>, <em>24</em>(9), 8314-8328. (<a href='https://doi.org/10.1109/TMC.2025.3556457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a key technology of intelligent satellite-enabled services in B5G or 6G networks, deploying Deep Neural Networks (DNN) models on satellites has been a notable trend, catering to the daily demand for extensive computing-intensive and latency-sensitive tasks. The computing resources are strategically deployed on satellites where sensor data is generated or collected, facilitating the fine-grained computational inference of DNN-based tasks. However, no prior study has comprehensively explored the crucial inference challenges – e.g., the trade-off between the number of tasks completed and accuracy and partitioning models in multi-exit models – in the resource-constrained space environment. Effective scheduling frameworks cater to various streams of inference tasks are scarce because inference performance may deviate from the ideal situation due to changes in task system status, such as task profiles and network state. To this end, we first formulate a gain-aware in-orbit computing inference problem to strike a proper trade-off between inference latency and the number of tasks completed by dynamically selecting optimal early exit points and model partitioning points. We propose an offline dynamic programming-based algorithm that provides an effective solution when comprehensive system details are to be predicted. We have developed an online learning-based method to schedule inference tasks with uncertain and dynamic system statuses in real-world situations. Our evaluation shows that, compared to baseline methods, the online learning-based algorithm can improve task gain by an average of 87.3% across various tasks.},
  archive      = {J_TMC},
  author       = {Qiyang Zhang and Shangguang Wang and Jinglong Guan and Praveen Kumar Donta and Xiao Ma and RangaRao Venkatesha Prasad and Schahram Dustdar and Xuanzhe Liu},
  doi          = {10.1109/TMC.2025.3556457},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8314-8328},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SatCooper: Enhancing cooperative inference analytics for satellite service via multi-exit DNNs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TrackLet: Data-driven inertial tracking on your own IMU data. <em>TMC</em>, <em>24</em>(9), 8301-8313. (<a href='https://doi.org/10.1109/TMC.2025.3557190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empowered by deep learning, the data-driven smartphone inertial tracking has attracted much attention due to its high accuracy and robustness. However, training a one-size-fits-all model requires a significant amount of inertial measurement unit (IMU) data with accurate labels, which incurs high costs of annotation and computation. In this work, we propose a new solution for IMU tracking that trains a specific model for each individual on their own IMU data. Individual IMU data can be opportunistically collected and automatically annotated with coarse-grained locations provided by smartphones (lite labels). To learn from noisy lite labels, we propose a lightweight and noise-resistant framework TrackLet for personalized data-driven IMU tracking. Two effective techniques are designed to combat the noise in lite labels, namely CNAL (Chained-Noise Adaptation Layer) and CoAdapt (Cooperatively Adaptive small-loss selection and weighting). Extensive experiment results demonstrate that TrackLet achieves high accuracy yet at a low cost, outperforming its state-of-the-art counterparts.},
  archive      = {J_TMC},
  author       = {Jiankun Wang and Zenghua Zhao and Jiayang Cui and Jiafan Lu and Bin Wu},
  doi          = {10.1109/TMC.2025.3557190},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8301-8313},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TrackLet: Data-driven inertial tracking on your own IMU data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VR-fi: Positioning and recognizing hand gestures via VR-embedded wi-fi sensing. <em>TMC</em>, <em>24</em>(9), 8287-8300. (<a href='https://doi.org/10.1109/TMC.2025.3557561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate gesture-based interactions are crucial for enhancing the immersive experience in VR (virtual reality) systems; they in turn necessitate gesture positioning and recognition in physical world. However, existing VR gesture recognition methods are predominantly vision-based, incurring high computational demands and raising privacy concerns. Meanwhile, Wi-Fi-based gesture recognition methods, deemed as promising complement to vision-based ones, typically lack gesture positioning capabilities. To this end, we propose VR-Fi, a gesture positioning and recognition system leveraging VR(-headset)-embedded Wi-Fi. To position gestures across different areas, VR-Fi innovates in a frequency-hopping bandwidth expansion (FHBE) technique to improve spatial resolution for locating a target. Additionally, VR-Fi innovates in neural models to process the FHBE-enhanced Wi-Fi CSI (channel state information) and enable the multi-task requirements of the joint positioning and recognition of hand gestures. Extensive experimental results demonstrate that VR-Fi achieves a positioning accuracy of 94.47%, a recognition accuracy of 92.13%, and a joint accuracy of 89.47%.},
  archive      = {J_TMC},
  author       = {Hongbo Wang and Xin Li and Jiachun Li and Haojin Zhu and Jun Luo},
  doi          = {10.1109/TMC.2025.3557561},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8287-8300},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {VR-fi: Positioning and recognizing hand gestures via VR-embedded wi-fi sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin empowered mmWave multi-hop V2X routing scheme with UAV assistance. <em>TMC</em>, <em>24</em>(9), 8272-8286. (<a href='https://doi.org/10.1109/TMC.2025.3556091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In VANETs, millimeter wave (mmWave) is capable of providing ultra-high bandwidth, large data throughput, and very low transmission latency. However, the difficulty of mmWave transmission is further exacerbated by its limited transmission range and its physical properties of low diffraction and low penetration, coupled with real-time dynamic network topology. In future 6G networks, digital twins (DTs) are considered a promising enabling technology as it can provide seamless interaction between the virtual network world and the real world. In this paper, we propose a DT-empowered multi-hop V2X routing scheme using mmWave as the transmission link in urban scenarios. First, we propose a novel DT-assisted mmWave multi-hop relay network architecture, which uses the global traffic flow information possessed by the DTs to assist vehicles in selecting the optimal relay nodes. Second, the uncrewed aerial vehicles (UAVs) are deployed at intersections to improve the packet forwarding efficiency of the intersections. Finally, a reinforcement learning algorithm is used to make forwarding decisions between streets. In addition, we also consider vehicle density and network load as key indicators for optimal street selection. The experimental results indicate that our solution has significant advantages in terms of key indicators such as packet delivery rate and latency.},
  archive      = {J_TMC},
  author       = {Taolue Zhou and Xiaohan Wu and Xinming Zhang},
  doi          = {10.1109/TMC.2025.3556091},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8272-8286},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Digital twin empowered mmWave multi-hop V2X routing scheme with UAV assistance},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward multi-agent reinforcement learning based traffic signal control through spatio-temporal hypergraphs. <em>TMC</em>, <em>24</em>(9), 8258-8271. (<a href='https://doi.org/10.1109/TMC.2025.3556243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic signal control systems (TSCSs) are integral to intelligent traffic management, fostering efficient vehicle flow. Traditional approaches often simplify road networks into standard graphs, which results in a failure to consider the dynamic nature of traffic data at neighboring intersections, thereby neglecting higher-order interconnections necessary for real-time control. To address this, we propose a novel TSCS framework to realize intelligent traffic control. This framework collaborates with multiple neighboring edge computing servers to collect traffic information across the road network. To elevate the efficiency of traffic signal control, we have crafted a multi-agent soft actor-critic (MA-SAC) reinforcement learning algorithm. Within this algorithm, individual agents are deployed at each intersection with a mandate to optimize traffic flow across the road network collectively. Furthermore, we introduce hypergraph learning into the critic network of MA-SAC to enable the spatio-temporal interactions from multiple intersections in the road network. This method fuses hypergraph and spatio-temporal graph structures to encode traffic data and capture the complex spatio-temporal correlations between multiple intersections. Our empirical evaluation, tested on varied datasets, demonstrates the superiority of our framework in minimizing average vehicle travel times and sustaining high-throughput performance. This work facilitates the development of more intelligent urban traffic management solutions.},
  archive      = {J_TMC},
  author       = {Kang Wang and Zhishu Shen and Zhen Lei and Xianhui Liu and Tiehua Zhang},
  doi          = {10.1109/TMC.2025.3556243},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8258-8271},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward multi-agent reinforcement learning based traffic signal control through spatio-temporal hypergraphs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fed-NL: A federated learning approach to suppress noise in participant datasets to reduce communication rounds for convergence. <em>TMC</em>, <em>24</em>(9), 8245-8257. (<a href='https://doi.org/10.1109/TMC.2025.3558874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning enables multiple participants to collaboratively train machine learning models without the need to share their private and limited data, thereby preserving privacy. When datasets used in federated learning contain noisy labels, it can lead to degraded performance and an increased number of communication rounds needed to achieve convergence. This, in turn, requires more time and energy to train the model. This paper proposes a federated learning approach to suppress the unequal distribution of the noisy labels in the dataset of each participant. The approach first estimates the noise ratio of the dataset for each participant and normalizes it using the server dataset. Next, the approach considers the influence of each participant and calculates the optimal weighted contributions for each one. The approach also considers bias in the server dataset and minimizes its impact on the participants. Further, the paper provides an expression to estimate the number of communication rounds required for convergence. Results demonstrate the superiority of the proposed approach over baselines in terms of communication rounds and performance.},
  archive      = {J_TMC},
  author       = {Rahul Mishra and Hari Prabhat Gupta},
  doi          = {10.1109/TMC.2025.3558874},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8245-8257},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fed-NL: A federated learning approach to suppress noise in participant datasets to reduce communication rounds for convergence},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A near-optimal category information sampling in RFID systems. <em>TMC</em>, <em>24</em>(9), 8228-8244. (<a href='https://doi.org/10.1109/TMC.2025.3556869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many RFID-enabled applications, objects are classified into different categories, and the information associated with each object's category (called category information) is written into the attached tag, allowing the reader to access it later. The category information sampling in such RFID systems, which is to randomly choose (sample) a few tags from each category and collect their category information, is fundamental for providing real-time monitoring and analysis in RFID. However, to the best of our knowledge, two technical challenges, i.e., how to guarantee a minimized execution time and reduce collection failure caused by missing tags, remain unsolved for this problem. In this paper, we address these two limitations by considering how to use the shortest possible time to sample a different number of random tags from each category and collect their category information sequentially in small batches. In particular, we first obtain a lower bound on the execution time of any protocol that can solve this problem. Subsequently, we present a near-OPTimal Category information sampling protocol (OPT-C) that solves the problem with an execution time close to the lower bound. Finally, extensive simulation results demonstrate the superiority of OPT-C over existing protocols, while real-world experiments further validate its practicality.},
  archive      = {J_TMC},
  author       = {Xiujun Wang and Zhi Liu and Xiaokang Zhou and Yong Liao and Han Hu and Xiao Zheng and Jie Li},
  doi          = {10.1109/TMC.2025.3556869},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8228-8244},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A near-optimal category information sampling in RFID systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient twin migration in vehicular metaverses: Multi-agent split deep reinforcement learning with spatio-temporal trajectory generation. <em>TMC</em>, <em>24</em>(9), 8214-8227. (<a href='https://doi.org/10.1109/TMC.2025.3558918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle Twins (VTs) as digital representations of vehicles can provide users with immersive experiences in vehicular metaverse applications, e.g., Augmented Reality (AR) navigation and embodied intelligence. VT migration is an effective way that migrates the VT when the locations of physical entities keep changing to maintain seamless immersive VT services. However, an efficient VT migration is challenging due to the rapid movement of vehicles, dynamic workloads of Roadside Units (RSUs), and heterogeneous resources of the RSUs. To achieve efficient migration decisions and a minimum latency for the VT migration, we propose a multi-agent split Deep Reinforcement Learning (DRL) framework combined with spatio-temporal trajectory generation. In this framework, multiple split DRL agents utilize split architecture to efficiently determine VT migration decisions. Furthermore, we propose a spatio-temporal trajectory generation algorithm based on trajectory datasets and road network data to simulate vehicle trajectories, enhancing the generalization of the proposed scheme for managing VT migration in dynamic network environments. Finally, experimental results demonstrate that the proposed scheme not only enhances the Quality of Experience (QoE) by 29% but also reduces the computational parameter count by approximately 25% while maintaining similar performances, enhancing users’ immersive experiences in vehicular metaverses.},
  archive      = {J_TMC},
  author       = {Junlong Chen and Jiawen Kang and Minrui Xu and Fan Wu and Hongliang Zhang and Huawei Huang and Dusit Niyato and Shiwen Mao},
  doi          = {10.1109/TMC.2025.3558918},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8214-8227},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient twin migration in vehicular metaverses: Multi-agent split deep reinforcement learning with spatio-temporal trajectory generation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A digital twin-based intelligent network architecture for underwater acoustic sensor networks. <em>TMC</em>, <em>24</em>(9), 8196-8213. (<a href='https://doi.org/10.1109/TMC.2025.3555640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater acoustic sensor networks (UASNs) drive toward strong environmental adaptability, intelligence, and multifunctionality. However, due to unique UASN characteristics, such as long propagation delay, dynamic channel quality, and high attenuation, existing studies present untimeliness, inefficiency, and inflexibility in real practice. Digital twin (DT) technology is promising for UASNs to break the above bottlenecks by providing high-fidelity status prediction and exploring optimal schemes. In this article, we propose a Digital Twin-based Network Architecture (DTNA), enhancing UASNs’ environmental adaptability, intelligence, and multifunctionality. By extracting real UASN information from local (node) and global (network) levels, we first design a layered architecture to improve the DT replica fidelity and UASN control flexibility. In local DT, we develop a resource allocation paradigm (RAPD), which rapidly perceives performance variations and iteratively optimizes allocation schemes to improve real-time environmental adaptability of resource allocation algorithms. In global DT, we aggregate decentralized local DT data and propose a collaborative Multi-agent reinforcement learning framework (CMFD) and a task-oriented network slicing (TNSD). CMFD patches scarce real data and provides extensive DT data to accelerate AI model training. TNSD unifies heterogeneous tasks’ demand extraction and efficiently provides comprehensive network status, improving the flexibility of multi-task scheduling algorithms. Finally, practical and simulation experiments verify the high fidelity of DT. Compared with the original UASN architecture, experiment results demonstrate that DTNA can: (i) improve the timeliness and robustness of resource allocation; (ii) greatly reduce the training time of AI algorithms; (iii) more rapidly obtain network status for multi-task scheduling at a low cost.},
  archive      = {J_TMC},
  author       = {Shanshan Song and Bingwen Huangfu and Jiani Guo and Jun Liu and Junhong Cui and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3555640},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8196-8213},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A digital twin-based intelligent network architecture for underwater acoustic sensor networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evasion attacks and countermeasures in deep learning-based wi-fi gesture recognition. <em>TMC</em>, <em>24</em>(9), 8180-8195. (<a href='https://doi.org/10.1109/TMC.2025.3557757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based Wi-Fi sensing has received massive interest thanks to the prevalence of Wi-Fi technology. While deep learning techniques provide promising results in Wi-Fi sensing, there are only very few studies on the vulnerabilities against Wi-Fi ensing. In this paper, we studied evasion attacks against deep learning-based Wi-Fi sensing and the countermeasure and conducted an extensive experimental evaluation using two publicly available datasets, namely SignFi and Widar. Accordingly, we proposed three white-box and two black-box attacks and revealed that even with an undetectable power change, evasion attacks can achieve a remarkable attack success rate (ASR) of 97.0% and 95.6% in white-box and black-box settings, respectively. These results highlight the urgent need for countermeasures against evasion attacks in Wi-Fi sensing systems. We introduced adversarial training and randomised smoothing, which notably improved the robustness of the Wi-Fi sensing model. The ASRs for white-box and black-box attacks were reduced to a minimum of around 6% and 2%, respectively. Moreover, randomised smoothing also introduced certifiable robustness, achieving 70.1% of samples certified for our model. The certification method provides an additional layer of reliability, ensuring that the model’s performance remains consistent and predictable even under adversarial conditions.},
  archive      = {J_TMC},
  author       = {Guolin Yin and Junqing Zhang and Xinping Yi and Xuyu Wang},
  doi          = {10.1109/TMC.2025.3557757},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8180-8195},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Evasion attacks and countermeasures in deep learning-based wi-fi gesture recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergizing acoustic and wi-fi signals for device-free gesture recognition. <em>TMC</em>, <em>24</em>(9), 8167-8179. (<a href='https://doi.org/10.1109/TMC.2025.3558139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gesture recognition has significant applications in areas such as assisted living, e-health, and human-device interactions. Moving beyond conventional computer vision techniques, recent studies have increasingly adopted ubiquitous methods like Wi-Fi and acoustic signals, which provide a cost-effective solution for device deployment. In this paper, we explore these two ubiquitous techniques to enhance gesture recognition, focusing on overcoming challenges associated with multi-modal fusion. To harmonize information from these inherently different signal types, we propose a tailored fusion strategy specifically designed for Wi-Fi and acoustic signals. Traditional multi-modal fusion methods often lack a theoretical framework due to insufficient analysis of the fundamental characteristics of different signals. To address this gap, we introduce the concept of the Hybrid Zone, a novel theoretical framework that models the interaction and fusion of acoustic and Wi-Fi sensing signals. The Hybrid Zone offers a unified perspective on the interaction between acoustic and Wi-Fi sensing areas and delivers insights into the granular synthesis of their velocity profiles. Our experimental results demonstrate strong performance, achieving a gesture recognition accuracy rate of 94.69% .},
  archive      = {J_TMC},
  author       = {Mengning Li and Wenye Wang},
  doi          = {10.1109/TMC.2025.3558139},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8167-8179},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Synergizing acoustic and wi-fi signals for device-free gesture recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ACL: Adaptive edge-cloud collaborative learning for heterogeneous devices with unlabeled local data. <em>TMC</em>, <em>24</em>(9), 8152-8166. (<a href='https://doi.org/10.1109/TMC.2025.3553971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-cloud collaborative learning emerges as a promising paradigm for adapting pre-trained deep neural network (DNN) models to the ever-changing edge data environments and specific downstream tasks. However, the heterogeneity of edge devices and unlabeled local data hinder the effectiveness of existing collaborative learning approaches. To address the above issues, we propose ACL, a novel adaptive edge-cloud collaborative learning paradigm for heterogeneous devices with unlabeled local data. In ACL, we first use FedNAS, a neural architecture search algorithm designed for collaborative learning to generate a customized model on each participating device, and then a lightweight semi-supervised collaborative learning framework HSSCL is used to fine-tune the pre-trained DNN model. Compared with the SOTA collaborative learning approaches, ACL achieves significant accuracy improvement, averaging 31.5% for image classification and 15.5% for object detection. Furthermore, it reduces time overhead by 3.1-5.1× and memory overhead by 6.3-12.5×. We will release our models and tools.},
  archive      = {J_TMC},
  author       = {Zhengyuan Zhang and Dong Zhao and Renhao Liu and Yuxing Yao and Xiangyu Li and Huadong Ma},
  doi          = {10.1109/TMC.2025.3553971},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8152-8166},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ACL: Adaptive edge-cloud collaborative learning for heterogeneous devices with unlabeled local data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic AP clustering and power allocation for CF-mMIMO-enabled federated learning using multi-agent DRL. <em>TMC</em>, <em>24</em>(9), 8136-8151. (<a href='https://doi.org/10.1109/TMC.2025.3554081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is recognized as a pivotal paradigm for 6G, offering decentralized model training without compromising data privacy. Recent works have proposed deploying FL in cell-free massive MIMO (CF-mMIMO) networks for reliable model transmission between FL clients and the server. Nevertheless, the problem of simultaneous access point (AP) clustering (i.e., dynamically forming AP groups to facilitate client-server communication) and transmit power allocation has not been thoroughly investigated. Furthermore, most existing solutions do not simultaneously consider the fast decision-making requirements brought by user mobility and the scalability of solutions in large-scale networks. To address this gap, we propose DACPA, a multi-agent deep reinforcement learning (DRL)-based scheme that accounts for client mobility (walking speed) and heterogeneous computing capabilities. DACPA strategically assigns each client a customized AP cluster and corresponding transmit power configuration, thereby optimizing model update latency. Extensive simulation results demonstrate the superior performance of DACPA in terms of convergence stability, spectral efficiency, global model update latency, and average energy consumption.},
  archive      = {J_TMC},
  author       = {Ziqi Li and Jia Hu and Xi Li and Heli Zhang and Geyong Min},
  doi          = {10.1109/TMC.2025.3554081},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8136-8151},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic AP clustering and power allocation for CF-mMIMO-enabled federated learning using multi-agent DRL},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RaliSense: Extending WiFi respiratory detection range by rapid alignment of dynamic components. <em>TMC</em>, <em>24</em>(9), 8119-8135. (<a href='https://doi.org/10.1109/TMC.2025.3553924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi based respiratory detection has attracted increasing attentions due to its ubiquity and convenience. In Non-Line-of-Sight (NLoS) scenarios, WiFi signals reflected from human target are blocked by obstacles and become much weaker, thus limiting the sensing range and hindering the practical deployment. The existing best respiratory detection system extended the sensing range by scaling and aligning dynamic components in WiFi signals. However, its dynamic component scaling causes the amplification of noise, while its dynamic component alignment increases computation complexity due to the traversal on all possible rotation angles. To address the above issues, in this paper we first build WiFi sensing range models for respiratory detection in NLoS scenario, find factors that limit the sensing range, and then propose a new respiratory detection system named RaliSense which can further rapidly extend the sensing range in NLoS scenario. The main idea of RaliSense is rapidly aligning dynamic components without amplifying noise, based on change direction vector and CSI ratio sum polarity of dynamic components. The proposed change direction vector is obtained by calculating the direction on which the noisy dynamic components have the maximum variance, and CSI ratio sum polarity is then obtained by summing the dynamic components which have been rotated by the change direction vector. According to the CSI ratio sum polarity, the rotation angle is quickly adjusted for aligning dynamic components. Extensive simulation and experiment results verify the effectiveness of our proposed sensing range models. The results also demonstrate that our proposed system RaliSense can effectively extend sensing range in NLoS scenario, achieving a 22.7% improvement over the best existing work but spending only a quarter of its computation time.},
  archive      = {J_TMC},
  author       = {Linqing Gui and Siyi Zheng and Zhengxin Guo and Zhetao Li and Ming Gao and Schahram Dustdar and Fu Xiao},
  doi          = {10.1109/TMC.2025.3553924},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8119-8135},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {RaliSense: Extending WiFi respiratory detection range by rapid alignment of dynamic components},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A federated learning-based data augmentation method for privacy preservation under heterogeneous data. <em>TMC</em>, <em>24</em>(9), 8105-8118. (<a href='https://doi.org/10.1109/TMC.2025.3553501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning is an important distributed machine learning paradigm. This study proposes a privacy-preserving data augmentation model for federated learning of heterogeneous data, which is able to mitigate heterogeneity and augmenting the participant’s local data while protecting data privacy. First, to address the problem of global model bias due to heterogeneous data, this study proposes a distributed generative adversarial network FedEqGAN. The model introduces a multi-source data feature fusion mechanism, which can learn the features of each data source to generate synthetic data. Second, addressing the privacy leakage issue caused by the disclosure of data distribution information, this paper proposes an encryption algorithm for heterogeneous environments FedHE, which utilizes homomorphic encryption to protect local data distributions and aggregates local data information through KL dispersion in order to construct global data distributions. Finally, for the privacy leakage problem caused by uploading model parameters in federation training, this paper proposes a federation model parameter encryption algorithm DPFedMP. This algorithm dynamically injects Gaussian noise into the model parameters according to the difference of data distribution to realize differential privacy protection and update the global model. Experiments show that the method is applicable to heterogeneous data environment, significantly enhancing model performance while ensuring data security.},
  archive      = {J_TMC},
  author       = {Yunpeng Xiao and Dengke Zhao and Xufeng Li and Tun Li and Rong Wang and Guoyin Wang},
  doi          = {10.1109/TMC.2025.3553501},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8105-8118},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A federated learning-based data augmentation method for privacy preservation under heterogeneous data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ExpertDRL: Request dispatching and instance configuration for serverless edge inference with foundation models. <em>TMC</em>, <em>24</em>(9), 8089-8104. (<a href='https://doi.org/10.1109/TMC.2025.3553201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of the pre-training & fine-tuning paradigm enables machine learning models to quickly adapt to various downstream tasks by fine-tuning pre-trained foundation models (FMs), greatly facilitating various IoT applications that rely on model inference in dynamic edge serverless environments. Efficiently dispatching inference requests and configuring instances to batch inference requests can significantly enhance resource efficiency. However, existing serverless inference solutions are tailored for traditional models, make coarse-grained request dispatching and instance configuration decisions, fail to exploit the shared model backbone characteristics of the FM and capture delayed rewards in dynamic environments, and ignore communication latency between edge sites, resulting in high costs and constraint violations. In this paper, we leverage our insight that fine-grained batch inference requests can effectively exploit the shared model backbone feature of FM to save monetary costs. We propose an algorithm that incorporates deep reinforcement learning (DRL) and expert intervention for fine-grained request dispatching and instance configuration, where the DRL component outputs fractional solutions as guidance, while the expert intervention module integrates our insights—batching reduces monetary costs at the expense of increased inference latency, whereas higher configurations shorten inference latency. This module rounds fractional solutions and adjusts instance configurations to search for optimal solutions while satisfying constraints, with theoretical guarantees rigorously proved. Finally, we conducted our experiments on an OpenFaas-based platform and simulator, and extensive trace-driven evaluation results show that ExpertDRL can save costs by up to 85.14% and improve request acceptance ratio by up to 26.93%, compared to the state-of-the-art solution.},
  archive      = {J_TMC},
  author       = {Yue Zeng and Junlong Zhou and Baoliu Ye and Zhihao Qu and Song Guo and Tianjian Gong and Pan Li},
  doi          = {10.1109/TMC.2025.3553201},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8089-8104},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ExpertDRL: Request dispatching and instance configuration for serverless edge inference with foundation models},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security enhanced computation offloading for collaborative inference at semantic-communication-empowered edge. <em>TMC</em>, <em>24</em>(9), 8071-8088. (<a href='https://doi.org/10.1109/TMC.2025.3555298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic communication (SC) has emerged as a promising paradigm for upcoming intelligent applications, enabling mobile devices to collaboratively execute intelligent tasks with edge servers through computation offloading. However, few studies have addressed the problem of collaborative inference in SC networks. Traditional collaborative inference mechanisms may suffer performance decline in SC systems and are vulnerable to eavesdroppers. To address these issues, first, we present an encryptor that encrypts semantic information to avoid privacy leakage and a decryptor for restoration. Besides, we propose a novel SC-empowered edge computing framework enabling mobile devices to deploy a partial semantic encoder and offload the rest to edge servers. Based on this framework, we formulate the collaborative inference optimization problem, jointly optimizing delay, energy consumption, and privacy leakage. DNNPart is devised based on deep deterministic policy gradient to address the problem, which consists of a semantic attention mechanism that enables it to focus on important state variables, a hybrid action representation method that makes it adapt to mixed discrete and continuous action spaces, a dynamic model splitting algorithm that locates the optimal partition layer and adaptively splits the semantic coders. Integrated with these components, DNNPart iteratively optimizes the offloading strategy to find the optimal offloading strategy. Extensive simulations were conducted to verify the effectiveness of the proposed method by comparing it with baseline mechanisms.},
  archive      = {J_TMC},
  author       = {Jincheng Peng and Huanlai Xing and Xiangyi Chen and Yang Li and Yunhe Cui and Danyang Zheng and Laha Ale and Li Feng},
  doi          = {10.1109/TMC.2025.3555298},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8071-8088},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Security enhanced computation offloading for collaborative inference at semantic-communication-empowered edge},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic pricing based near-optimal resource allocation for elastic edge offloading. <em>TMC</em>, <em>24</em>(9), 8057-8070. (<a href='https://doi.org/10.1109/TMC.2025.3553188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing (MEC), task offloading can significantly reduce task execution latency and energy consumption of end user (EU). However, edge server (ES) resources are limited, necessitating efficient allocation to ensure the sustainable and healthy development for MEC system. In this paper, we propose a dynamic pricing mechanism based near-optimal resource allocation for elastic edge offloading. First, we construct a resource pricing model and accordingly develop the utility functions for both EU and ES, the optimal pricing model parameters are derived by optimizing the utility functions. In the meantime, our theoretical analysis reveals that the EU’s utility function reaches a local maximum within the search range, but exhibits barely growth with increased resource allocation beyond this point. To this end, we further propose the Dynamic Inertia and Speed-Constrained particle swarm optimization (DISC-PSO) algorithm, which efficiently identifies the near-optimal resource allocation. Comprehensive simulation results validate the effectiveness of DISC-PSO algorithm, demonstrating that it significantly outperforms existing schemes by reducing the average number of iterations to reach a near-optimal solution by 86.88%, increasing the EU utility function value by 0.13%, and decreasing the variance of results by 96.78%.},
  archive      = {J_TMC},
  author       = {Yun Xia and Hai Xue and Di Zhang and Shahid Mumtaz and Xiaolong Xu and Joel J. P. C. Rodrigues},
  doi          = {10.1109/TMC.2025.3553188},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8057-8070},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic pricing based near-optimal resource allocation for elastic edge offloading},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symmetry-informed MARL: A decentralized and cooperative UAV swarm control approach for communication coverage. <em>TMC</em>, <em>24</em>(9), 8039-8056. (<a href='https://doi.org/10.1109/TMC.2025.3553285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed aerial vehicle-mounted base stations (UAV-MBSs) provide flexible wireless connectivity, extending communication coverage in underserved areas. Recently, multi-agent reinforcement learning (MARL) has shown great potential for cooperative UAV swarm control to support efficient communication coverage in dynamic and complex environments. However, existing MARL-based methods often suffer from low sample efficiency due to its trial-and-error training characteristics, limiting its ability to control large UAV swarms with continuous state-action space and partial observation. We notice that UAV swarm systems in communication coverage tasks exhibit a spatial symmetry property, e.g., a rotation in the spatial observation of a UAV results in a same rotation in its optimal action. Exploiting this property, we formulate the task as a symmetric decentralized partially observable Markov decision process and introduce symmetry-informed MARL, featuring a novel network called the symmetry-informed graph neural network (SiGNN) to serve as the policy/value networks. SiGNN leverages the inherent symmetry in multi-UAV systems by embedding the symmetry into the network structure, thereby enhancing the training efficiency to handle large swarms with continuous control. Theoretical analysis shows that the SiGNN strictly preserves symmetry properties, which guarantees the effectiveness of the approach. Experiments in simulation were conducted to handle communication coverage using up to 20 UAVs with continuous control. Experimental results demonstrate that SiGNN-based MARL outperforms advanced baselines, verifying its superior sample efficiency, scalability and robustness.},
  archive      = {J_TMC},
  author       = {Rongye Shi and Xin Yu and Yandong Wang and Yongkai Tian and Zhenyu Liu and Wenjun Wu and Xiao-Ping Zhang and Manuela M. Veloso},
  doi          = {10.1109/TMC.2025.3553285},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8039-8056},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Symmetry-informed MARL: A decentralized and cooperative UAV swarm control approach for communication coverage},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient FEC scheme for time-sensitive multi-hop transmissions in overlay networks. <em>TMC</em>, <em>24</em>(9), 8025-8038. (<a href='https://doi.org/10.1109/TMC.2025.3553380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In pursuit of low latency, real-time communication (RTC) service providers usually use multi-hop overlay links worldwide to bypass congested links, especially for medium- and long-distance transmissions. In such multi-hop long-distance transmission scenarios, utilizing retransmission to recover lost packets can result in increased end-to-end latency. Therefore, Forward Error Correction (FEC) is viewed as a promising way to solve the loss problem. However, for multi-hop overlay transmission, existing FEC schemes either introduce a non-negligible processing delay at each hop or reduce the processing delay at the cost of a high coefficient overhead. In this work, we propose a multi-hop FEC scheme, i.e., FEC-OEM, which considers both processing delay and coefficient overhead. FEC-OEM is designed based on two observations we obtained from measurements. First, coefficient overhead can only be reduced through an implicit transmission way. Therefore, we design a modulation-based recoding module that enables implicit coefficient transmission and hop-by-hop recoding at the same time. Second, using on-the-fly computation is a promising way to reduce processing delay. Accordingly, we design an elimination method to make the modulation-based recoding can be carried out on-the-fly. Real-world experiments demonstrate that FEC-OEM can reduce the processing delay by up to 88% without increasing the coefficient overhead compared to state-of-the-art schemes. We also use FEC-OEM to transmit packets for applications with different loss tolerances, and the results show that FEC-OEM can improve the QoE more effectively than state-of-the-art coding schemes.},
  archive      = {J_TMC},
  author       = {Chao Xu and Jessie Hui Wang and Rui Li and Hao Wu and Jilong Wang and Jun Zhang and Kai Zheng},
  doi          = {10.1109/TMC.2025.3553380},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8025-8038},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cost-efficient FEC scheme for time-sensitive multi-hop transmissions in overlay networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-edge-end collaborative computing-enabled intelligent sharding blockchain for industrial IoT based on PPO approach. <em>TMC</em>, <em>24</em>(9), 8011-8024. (<a href='https://doi.org/10.1109/TMC.2025.3554568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security and reliability risks of industrial data have constrained the advancement of the Industrial Internet of Things (IIoT). Although blockchain can protect the security and reliability of industrial data through hash verification mechanisms, there are numerous challenges in the existing blockchain-enabled IIoT systems, such as the trilemma of scalability, decentralization and security, high computational power consumption of consensus protocols and limited computational resources of industrial devices. To address these problems, an intelligent sharding blockchain-enabled IIoT framework is proposed, in which the intelligent sharding based on the reputation mechanism and the adaptive switching for multi-consensus protocols are utilized to enhance the decentralization, security and scalability of blockchain. Considering higher requirement of computational power of the sharding blockchain, a cloud-edge-end collaborative computing framework is introduced, in which the parallel computational offloading and the Terahertz communication technology are utilized to enhance the cooperation of the cloud-edge-end networks. Furthermore, due to the highly dynamic nature of industrial devices and industrial data, we consider and design the optimization problem as a Markov decision process (MDP), which is solved via the Proximal Policy Optimization (PPO) algorithm. Simulation results show that our proposed scheme can minimize total delay and maximize transaction throughput while guaranteeing the safety as well as decentralization of blockchain-enabled IIoT systems.},
  archive      = {J_TMC},
  author       = {Xin Xiong and Meng Li and F. Richard Yu and Haijun Zhang and Kan Wang and Pengbo Si},
  doi          = {10.1109/TMC.2025.3554568},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {8011-8024},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cloud-edge-end collaborative computing-enabled intelligent sharding blockchain for industrial IoT based on PPO approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SmartSpr: A physics-informed mobile sprinkler scheduling system for reducing urban particulate matter pollution. <em>TMC</em>, <em>24</em>(9), 7994-8010. (<a href='https://doi.org/10.1109/TMC.2025.3555448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban particulate pollution presents considerable public health hazards, underscoring the need for effective control measures in various cities. This paper proposes SmartSpr, a physics-informed urban mobile sprinkler scheduling system designed for enhanced efficiency in reducing particulate pollution. SmartSpr incorporates a Physics-Informed Neural Network (PINN)-based model, enriched with Bayesian optimization, to accurately simulate the impact of mobile sprinklers on particulate matter (PM) dispersion. Building on this sprinkling effect model, a selective sprinkling strategy considering the replenish process is proposed. This strategy employs a sparsity-driven decoupling simulated annealing algorithm to refine sprinkler routes, prioritizing areas with substantial environmental benefits. Extensive field experiments and simulations have validated SmartSpr, demonstrating a 64.8% reduction in prediction error of SmartSpr's sprinkling model compared to the leading baseline and an 18% enhancement in pollutant reduction efficiency of the proposed scheduling algorithm.},
  archive      = {J_TMC},
  author       = {Ji Luo and Zijian Xiao and Zuxin Li and Xuecheng Chen and Chaopeng Hong and Xiao-Ping Zhang and Xinlei Chen},
  doi          = {10.1109/TMC.2025.3555448},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7994-8010},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SmartSpr: A physics-informed mobile sprinkler scheduling system for reducing urban particulate matter pollution},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PBN-CSMA/CA: A power back-off NOMA-based CSMA/CA protocol for ad hoc networks. <em>TMC</em>, <em>24</em>(9), 7980-7993. (<a href='https://doi.org/10.1109/TMC.2025.3551340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carrier-sense multiple access with collision avoidance (CSMA/CA) is one of the fundamental medium access control (MAC) protocols for ad hoc networks. As a network’s size increases, its throughput degrades substantially because of packet collisions. To reduce the collision probability, combining non-orthogonal multiple access (NOMA) with CSMA/CA is a promising solution. However, existing NOMA-CSMA/CA protocols adopt distributed power selection and channel inversion power control, resulting in a high power collision probability and limiting the number of power levels. To address these issues, we propose a power back-off NOMA-based CSMA/CA (PBN-CSMA/CA) protocol for ad hoc networks. The proposed protocol achieves centralized power allocation, avoiding power collisions by employing the Zadoff-Chu (ZC) sequence and the power level allocation (PLA) frame. Additionally, power back-off (PB) control is used to set the transmission power, which expands the number of power levels and gives full play to the performance advantages of NOMA. To analyze the performance comprehensively, the closed-form expressions of the average outage probability, normalized saturation throughput, average packet delay and transmission energy consumption are theoretically analyzed. Both the analytical and simulation results demonstrate that the PBN-CSMA/CA protocol outperforms the existing NOMA-CSMA/CA and traditional CSMA/CA protocols, with significant throughput gains and delay reductions.},
  archive      = {J_TMC},
  author       = {Ningbo Zhang and Guangqian Peng and Hao Chen and Caitong Tang},
  doi          = {10.1109/TMC.2025.3551340},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7980-7993},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PBN-CSMA/CA: A power back-off NOMA-based CSMA/CA protocol for ad hoc networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contextual bandits with non-stationary correlated rewards for user association in mmWave vehicular networks. <em>TMC</em>, <em>24</em>(9), 7965-7979. (<a href='https://doi.org/10.1109/TMC.2025.3552717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter wave (mmWave) communication has emerged as a key technology enabling ultra-low latency and high throughput in vehicular communication. Usually, an appropriate decision on user association requires timely channel information between vehicles and base stations (BSs), which is challenging given a fast-fading mmWave vehicular channel. In this paper, we propose a low-complexity semi-distributed contextual correlated upper confidence bound (SD-CC-UCB) algorithm to establish an up-to-date user association between vehicles and BSs without explicit measurement of channel state information (CSI). Under a contextual multi-arm bandits framework, SD-CC-UCB learns and predicts the transmission rate given the location and velocity of the vehicle, which can adequately capture the intricate channel condition for a prompt decision on user association. Further, SD-CC-UCB efficiently identifies the set of candidate BSs which probably support supreme transmission rates by leveraging the correlated distributions of transmission rates on different locations. To further refine the learning transmission rate to candidate BSs, each vehicle deploys the Thompson Sampling algorithm by taking the interference among vehicles and handover into consideration. Numerical results show that our proposed algorithm achieves the network throughput within 100%–103% of a benchmark algorithm which requires perfect instantaneous CSI, demonstrating the effectiveness of SD-CC-UCB in vehicular communications.},
  archive      = {J_TMC},
  author       = {Xiaoyang He and Xiaoxia Huang and Lanhua Li},
  doi          = {10.1109/TMC.2025.3552717},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7965-7979},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Contextual bandits with non-stationary correlated rewards for user association in mmWave vehicular networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge AI inference as a service via dynamic resources from repeated auctions. <em>TMC</em>, <em>24</em>(9), 7947-7964. (<a href='https://doi.org/10.1109/TMC.2025.3554816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enable edge AI providers to recruit edge devices and use them to deploy AI models and provision inference services, we conduct a comprehensive mathematical and algorithmic study on a novel incentive and optimization mechanism based on repeated auctions. We first model and formulate a time-cumulative social cost optimization problem to capture the challenges of the trade-off between cost and accuracy, the dependency between adjacent auctions, and the need of achieving desired economic properties. Then, to solve this intractable non-linear integer program in an online manner, we design a set of polynomial-time algorithms that work together. Our approach dynamically chooses and switches winning bids under careful control, incorporates online learning to overcome posterior inference accuracy and workload queue dynamics, and leverages randomization to strategically convert fractional decisions of model placement and query dispatch into integers. We also allocate payments to meet the necessary and sufficient conditions for the desired economic properties. Further, we rigorously prove the constant competitive ratio, the sub-linear regret and fit, and the truthfulness and individual rationality for our proposed approach. Finally, through extensive experiments using real devices, AI models, and data traces, we have validated the substantial advantages of our proposed approach compared to the baselines and the state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Mingtao Ji and Hehan Zhao and Lei Jiao and Sheng Zhang and Xin Li and Zhuzhong Qian and Baoliu Ye},
  doi          = {10.1109/TMC.2025.3554816},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7947-7964},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge AI inference as a service via dynamic resources from repeated auctions},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design optimization of NOMA aided multi-STAR-RIS for indoor environments: A convex approximation imitated reinforcement learning approach. <em>TMC</em>, <em>24</em>(9), 7929-7946. (<a href='https://doi.org/10.1109/TMC.2025.3552521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-orthogonal multiple access (NOMA) enables multiple users to share the same frequency band, and simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) provides 360-degree full-space coverage, optimizing both transmission and reflection for improved network performance and dynamic control of the indoor environment. However, deploying STAR-RIS indoors presents challenges in interference mitigation, power consumption, and real-time configuration. In this work, a novel network architecture utilizing multiple access points (APs), STAR-RISs, and NOMA is proposed for indoor communication. To address these, we formulate an optimization problem involving user assignment, access point (AP) beamforming, and STAR-RIS phase control. A decomposition approach is used to solve the complex problem efficiently, employing a many-to-one matching algorithm for user-AP assignment and K-means clustering for resource management. Additionally, multi-agent deep reinforcement learning (MADRL) is leveraged to optimize the control of the STAR-RIS. Within the proposed MADRL framework, a novel approach is introduced in which each decision variable acts as an independent agent, enabling collaborative learning and decision making. The MADRL framework is enhanced by incorporating convex approximation (CA), which accelerates policy learning through suboptimal solutions from successive convex approximation (SCA), leading to faster adaptation and convergence. Simulations demonstrate significant improvements in network utility compared to baseline approaches.},
  archive      = {J_TMC},
  author       = {Yu Min Park and Sheikh Salman Hassan and Yan Kyaw Tun and Eui-Nam Huh and Walid Saad and Choong Seon Hong},
  doi          = {10.1109/TMC.2025.3552521},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7929-7946},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Design optimization of NOMA aided multi-STAR-RIS for indoor environments: A convex approximation imitated reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HeadSonic: Usable bone conduction earphone authentication via head-conducted sounds. <em>TMC</em>, <em>24</em>(9), 7914-7928. (<a href='https://doi.org/10.1109/TMC.2025.3551272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Earables (ear wearables) are rapidly emerging as a new platform encompassing a diverse of personal applications, prompting the development of authentication schemes to protect user privacy. Existing earable authentication methods are all specifically designed for air-conduction earphones, which are not suited for bone conduction earphones (BCEs) that rely on bone conduction mechanisms. In this paper, we propose HeadSonic, a usable BCE authentication system based on the unique head-conducted sounds, which can be acquired when the user wears the BCE device. Specifically, the system emits a millisecond-level sound to initiate the authentication session. The signal captured by the BCE microphone is propagated through the user's head, which is unique in density, geometry, and bone-tissue ratio. It operates implicitly, while maintaining robustness across different behaviors. Extensive experiments involving 60 subjects demonstrate that HeadSonic achieves a commendable balanced accuracy of 96.59%, proving its efficacy and resilience against replay and synthesis attacks. Our dataset and source codes are available at https://anonymous.4open.science/r/HeadSonic-1CE4.},
  archive      = {J_TMC},
  author       = {Zhixiang He and Jing Chen and Kun He and Yangyang Gu and Qiyi Deng and Zijian Zhang and Ruiying Du and Qingchuan Zhao and Cong Wu},
  doi          = {10.1109/TMC.2025.3551272},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7914-7928},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HeadSonic: Usable bone conduction earphone authentication via head-conducted sounds},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying implementation flaws of SMS OTP authentication. <em>TMC</em>, <em>24</em>(9), 7899-7913. (<a href='https://doi.org/10.1109/TMC.2025.3550883'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the Short Message Service (SMS) One-Time Passwords (OTP) authentication is widely adopted in mobile applications. However, due to improper implementation by developers, significant security flaws exist in the SMS OTP authentication mechanisms of some apps. To provide a comprehensive and accurate assessment, we propose a new approach. First, we locate the SMS OTP authentication page through UI exploration. Then, using hooking technology, we conduct simulated attacks to verify the security of the SMS OTP authentication in the app, focusing on its susceptibility to brute-force attacks. This approach is applicable to apps with app-side or UI-layer protection measures, uncovering hidden implementation flaws beneath these protections. Technically, we employ dynamic analysis based on the ART virtual machine instrumentation to obtain runtime information of the app and generate vulnerability verification scripts, overcoming the challenges posed by code-packing in program analysis. We implemented a semi-automatic tool named AuthChecker and tested it on 950 popular apps, identifying 87 apps with security flaws that potentially allow attackers to achieve unauthorized account access. Our findings highlight the security issues in SMS OTP authentication of apps, promoting improvements in vulnerability patching and preventive strategies by developers.},
  archive      = {J_TMC},
  author       = {Jiayu Zhao and Fannv He and Yiyu Yang and Yuqing Zhang},
  doi          = {10.1109/TMC.2025.3550883},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7899-7913},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Identifying implementation flaws of SMS OTP authentication},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing resource block allocation for multicast in beyond 5G networks. <em>TMC</em>, <em>24</em>(9), 7880-7898. (<a href='https://doi.org/10.1109/TMC.2025.3549590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New radio (NR) and non-orthogonal multiple access (NOMA) offer scalable and efficient resource allocation in Beyond 5G (B5G) networks. NR implements mixed numerology with flexible frame structures for future compatibility, whereas NOMA allows users with different channel states to share an identical Physical Resource Block (PRB). Multi-connectivity enables a user to connect to multiple networks for reliability, and multicast conveys data to users simultaneously that request the same content. However, resource allocation in the NOMA-based mixed numerology system with multi-connectivity for multicast remains unexplored. The problem is challenging due to 1) the different shapes of PRBs in NR and 2) the shared locations of PRBs in a frame with NOMA. In this paper, we formulate a new optimization problem, named Multicast, Multi-connectivity, and Multi-Dimensional Resource Allocation Problem (M3DRAP), and prove its NP-hardness and inapproximability. We propose an approximation algorithm for general M3DRAP with the ideas of Multicast Inter-Numerology Relation, Layer Dissimilarity, Subgrouping Nonuniformity, and Segmentation Preference. To find the intrinsic properties of PRB allocation for multicast in NOMA-based networks, we consider a single B5G usage scenario (e.g., eMBB, URLLC, or mMTC) and propose another approximation algorithm. Simulations demonstrate our algorithms improve the weighted sum rate by over 50% and increase the user satisfaction ratio by 1.5x.},
  archive      = {J_TMC},
  author       = {Ru-Jun Wang and Chih-Hang Wang and De-Nian Yang and Guang-Siang Lee and Wen-Tsuen Chen and Jang-Ping Sheu},
  doi          = {10.1109/TMC.2025.3549590},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7880-7898},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing resource block allocation for multicast in beyond 5G networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight encoder-decoder framework for carpooling route planning. <em>TMC</em>, <em>24</em>(9), 7866-7879. (<a href='https://doi.org/10.1109/TMC.2025.3549757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Carpooling Route Planning (CRP) has become an important issue with the growth of low-carbon traffic systems. We investigate a novel, meaningful and challenging scenario for CRP in industry, called Multi-Candidate Carpooling Route Planning (MCRP) problem, where each passenger may have several potential positions to get on and off the car. We surprisingly notice that this problem can be easily generalized for similar services such as express, takeout, or crowdsensing services, which means MCRP is a new fundamental combinatorial optimization problem. Traditional graph search algorithms or indexing methods are usually time and space consuming or perform poorly, which are not suitable for solving the problem. In this paper, we propose an end-to-end encoder-decoder model to plan a route for each many-to-one carpooling order with various data-driven mechanisms such as graph partitioning and feature crossover. The encoder is a filter-integrated Graph Convolution Network with external information fusion combining a supervised pre-training classification task, while the latter mimics a pointer network with a rule-based mask mechanism and a domain feature crossover module. We validate the effectiveness and efficiency of our model based on both synthetic and real-world datasets.},
  archive      = {J_TMC},
  author       = {Yucen Gao and Li Ma and Zhemeng Yu and Songjian Zhang and Hui Gao and Jun Fang and Xiaofeng Gao},
  doi          = {10.1109/TMC.2025.3549757},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7866-7879},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A lightweight encoder-decoder framework for carpooling route planning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLiquID: Towards mobile liquid sensing with COTS RFIDs. <em>TMC</em>, <em>24</em>(9), 7851-7865. (<a href='https://doi.org/10.1109/TMC.2025.3536842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liquid sensing in ubiquitous contexts plays an essential role in various scenarios. Recently, some wireless sensing systems have been proposed for liquid identification. However, existing works usually require specific equipment or capture the signals penetrating a target, limiting the deployability of liquid sensing. In large-scale scenarios, multiple devices are usually required to expand the coverage area due to the RFID reader antenna's reading range limitation. To enlarge the sensing range and make the liquid sensing method can be adopted in real moving scenarios, in this paper, we present Mobile Liquid IDentification (MLiquID), a liquid sensing system that can recognize the type of liquid in a mobile manner with commercial off-the-shelf (COTS) RFID devices. This mobile process leads to continuous variation in location, so the major challenge in this paper is how to extract signal features from the superimposed information of movement and material. The key insight is to regard movement as an opportunity to acquire data from different perspectives instead of a challenge to hinder feature extraction. We construct a Phase-RSS model by analyzing the influence of moving and liquid on the phase and RSS signals. First, we propose a method to calculate the distance from the tag to the reader antenna. Second, we explore an identification method to identify liquid type by extracting signal features Phase-RSS coefficient $C_{P-R}$ and Maximum Response Distance (MRD). Experimental results demonstrate an average accuracy of 96.80% in identifying 10 common liquids, which shows the great potential of MLiquID for mobile liquid sensing.},
  archive      = {J_TMC},
  author       = {Zijuan Liu and Xiulong Liu and Xinyu Tong and Xin Xie and Jiancheng Chen and Ming Yang and Keqiu Li},
  doi          = {10.1109/TMC.2025.3536842},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7851-7865},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MLiquID: Towards mobile liquid sensing with COTS RFIDs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eve said yes: AirBone authentication for head-wearable smart voice assistant. <em>TMC</em>, <em>24</em>(9), 7836-7850. (<a href='https://doi.org/10.1109/TMC.2025.3530962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in speech and language processing have led to the rise of smart voice services like Alexa, Google Home, and Siri. However, these advancements also increase security risks due to sophisticated voice domain attacks. Instead of relying on acoustic clues to detect replayed or synthesized speech, we utilize microphones and motion sensors in head-wearable devices to authorize legitimate users through bone-conducted vibrations, enabling multi-factor authentication (MFA) for spoken voice. Our proposed two-stage authentication system, AirBone, captures air and bone conduction (AirBone) signals and exploits two authentication factors sequentially. The first stage, called temporal consistency scoring (TCS), employs signal processing to verify the recorded AC and BC signals are concurrent and originate from the same vocalization process. Statistical tools are employed to distinguish legitimate attempts against false-triggering or acoustic attacks. The second stage leverages deep learning to verify the user’s unique bone conduction patterns in the vibration domain. Specifically, we enhance the robustness through data augmentation with constant-Q transform and adversarial training, improving the model’s ability to detect impersonation and machine-induced vibrations. Thanks to these designs, AirBone authentication offers enhanced security via MFA with no extra cost of user effort. In addition, our experimental results demonstrate a $96.3\%$ overall accuracy, robustness against AirBone noise and room impulse responses, and $0.3\%$ Equal Error Rate (EER) against acoustic and cross-domain attacks.},
  archive      = {J_TMC},
  author       = {Chenpei Huang and Hui Zhong and Pavana Prakash and Dian Shi and Xu Yuan and Miao Pan},
  doi          = {10.1109/TMC.2025.3530962},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7836-7850},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Eve said yes: AirBone authentication for head-wearable smart voice assistant},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sums: Sniffing unknown multiband signals under low sampling rates. <em>TMC</em>, <em>24</em>(9), 7822-7835. (<a href='https://doi.org/10.1109/TMC.2024.3509861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to sophisticated deployments of all kinds of wireless networks (e.g., 5G, Wi-Fi, Bluetooth, LEO satellite, etc.), multiband signals distribute in a large bandwidth (e.g., from 70 MHz to 8 GHz). Consequently, for network monitoring and spectrum sharing applications, a sniffer for extracting physical layer information, such as structure of packet, with low sampling rate (especially, sub-Nyquist sampling) can significantly improve their cost- and energy-efficiency. However, to achieve a multiband signals sniffer is really a challenge. To this end, we propose Sums, a system that can sniff and analyze multiband signals in a blind manner. Our Sums takes advantage of hardware and algorithm co-design, multi-coset sub-Nyquist sampling hardware, and a multi-task deep learning framework. The hardware component breaks the Nyquist rule to sample GHz bandwidth, but only pays for a 50 MSPS sampling rate. Our multi-task learning framework directly tackles the sampling data to perform spectrum sensing, physical layer protocol recognition, and demodulation for deep inspection from multiband signals. Extensive experiments demonstrate that Sums achieves higher accuracy than the state-of-the-art baselines in spectrum sensing, modulation classification, and demodulation. As a result, our Sums can help researchers and end-users to diagnose or troubleshoot their problems of wireless infrastructures deployments in practice.},
  archive      = {J_TMC},
  author       = {Jinbo Peng and Zhe Chen and Zheng Lin and Haoxuan Yuan and Zihan Fang and Lingzhong Bao and Zihang Song and Ying Li and Jing Ren and Yue Gao},
  doi          = {10.1109/TMC.2024.3509861},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7822-7835},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sums: Sniffing unknown multiband signals under low sampling rates},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated sensing and communication enabled cooperative passive sensing using mobile communication system. <em>TMC</em>, <em>24</em>(9), 7805-7821. (<a href='https://doi.org/10.1109/TMC.2024.3514113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrated sensing and communication (ISAC) is a potential technology of the sixth-generation (6G) mobile communication system, which enables communication base station (BS) with sensing capability. However, the performance of single-BS sensing is limited, which can be overcome by multi-BS cooperative sensing. There are three types of multi-BS cooperative sensing, including cooperative active sensing, cooperative passive sensing, and cooperative active and passive sensing, where the multi-BS cooperative passive sensing has the advantages of low hardware modification cost and large sensing coverage. However, multi-BS cooperative passive sensing faces the challenges of synchronization offset mitigation and sensing information fusion. To address these challenges, a non-line of sight (NLoS) and line of sight (LoS) signal cross-correlation (NLCC) method is proposed to mitigate carrier frequency offset (CFO) and time offset (TO). Besides, a symbol-level fusion method of multi-BS sensing information is proposed. The discrete samplings of echo signals from multiple BSs are matched independently and coherently accumulated to improve sensing accuracy. Moreover, a low-complexity joint angle-of-arrival (AoA) and angle-of-departure (AoD) estimation method is proposed to reduce the computational complexity. Simulation results show that symbol-level multi-BS cooperative passive sensing scheme has an order of magnitude higher sensing accuracy than single-BS passive sensing. This work provides a reference for the research on multi-BS cooperative passive sensing.},
  archive      = {J_TMC},
  author       = {Zhiqing Wei and Haotian Liu and Hujun Li and Wangjun Jiang and Zhiyong Feng and Huici Wu and Ping Zhang},
  doi          = {10.1109/TMC.2024.3514113},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {9},
  number       = {9},
  pages        = {7805-7821},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Integrated sensing and communication enabled cooperative passive sensing using mobile communication system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scheduling drone and mobile charger via hybrid-action deep reinforcement learning. <em>TMC</em>, <em>24</em>(8), 7788-7804. (<a href='https://doi.org/10.1109/TMC.2025.3551386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been a growing interest in using chargers to extend the operational longevity of UAVs (drones). In this paper, we explore a charger-assisted drone application where a drone observes points of interest while a mobile charger moves to recharge its battery. We focus on the route and charging schedule of the drone and mobile charger to maximize observation utility in the shortest possible time, while ensuring continuous drone operation. In our problem, the drone and mobile charger cooperate to complete a task. Their discrete-continuous hybrid actions pose a major computational challenge. To address this issue, we present a hybrid-action deep reinforcement learning framework, called HaDMC, which uses a typical policy learning algorithm to generate latent continuous actions. We specifically design and train an action decoder. It involves two pipelines to convert the latent continuous actions into the original hybrid actions for the drone and mobile charger to directly interact with environment. We incorporate a mutual learning scheme into model training, emphasizing collaboration over individual actions. By extensive numerical experiments, we evaluate HaDMC and compare it with state-of-the-art approaches. The experimental results demonstrate the effectiveness and efficiency of our solution.},
  archive      = {J_TMC},
  author       = {Jizhe Dou and Haotian Zhang and Yang Luo and Guodong Sun},
  doi          = {10.1109/TMC.2025.3551386},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7788-7804},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Scheduling drone and mobile charger via hybrid-action deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging online learning for domain-adaptation in wi-fi-based device-free localization. <em>TMC</em>, <em>24</em>(8), 7773-7787. (<a href='https://doi.org/10.1109/TMC.2025.3552538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi-based device-free localization (DFL) will be an integral part of many emerging applications, such as smart healthcare and smart homes. One popular approach to DFL in Wi-Fi makes use of fingerprinting based on channel state information (CSI). Unfortunately, high-quality fingerprints cannot easily be obtained in many real-world environments due to the complicated, time-varying and multipath conditions which exist. Additionally, existing methods struggle to update the DFL models in a real-time manner to track changes of environment. To address these issues, an online data-driven modelling DFL framework is designed for robustness enhancement. Specifically, the raw CSI data is first augmented with the hidden layer parameters of an online deep neural network to strengthen the pair-to-pair mappings between signal variations and a target’s location. The radio map created with the augmented fingerprints can be updated with new sequential data collected from other domains, such as different times and layouts of the same environment. Subsequently, a novel online DFL model is established using these augmented fingerprints, which itself can be updated with new sequential data from other domains without the need for retraining. A forgetting mechanism is considered to mitigate the effects of outdated data on the localization performance. To validate our new framework, a comprehensive set of experiments have been performed in various environments for different scenarios. The experimental results verify the robustness and responsive tracking ability of the proposed online data-driven modelling DFL framework.},
  archive      = {J_TMC},
  author       = {Jie Zhang and Jianqiang Xue and Yanjiao Li and Simon L. Cotton},
  doi          = {10.1109/TMC.2025.3552538},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7773-7787},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Leveraging online learning for domain-adaptation in wi-fi-based device-free localization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AIChronoLens: AI/ML explainability for time series forecasting in mobile networks. <em>TMC</em>, <em>24</em>(8), 7757-7772. (<a href='https://doi.org/10.1109/TMC.2025.3554035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting is increasingly considered a fundamental enabler for the management of next-generation mobile networks. While deep neural networks excel at short- and long-term forecasting, their complexity hinders interpretability, a crucial factor for production deployment. The existing EXplainable Artificial Intelligence (XAI) techniques, primarily designed for computer vision and natural language processing, struggle with time series data due to their lack of understanding of temporal characteristics of the input data. In this paper, we take the research on EXplainable Artificial Intelligence (XAI) for time series forecasting one step further by proposing AIChronoLens, a new tool that links legacy XAI explanations with the temporal properties of the input. AIChronoLens allows diving deep into the behavior of time series predictors and spotting, among other aspects, the hidden causes of forecast errors. We show that AIChronoLens’s output can be utilized for meta-learning to predict when the original time series forecasting model makes errors and fix them in advance, thereby improving the accuracy of the predictors. Extensive evaluations with real-world mobile traffic traces pinpoint model behaviors that would not be possible to identify otherwise and show how model performance can be improved by 32 % upon re-training and by up to 39 % with meta-learning.},
  archive      = {J_TMC},
  author       = {Pablo Fernández Pérez and Claudio Fiandrino and Eloy Pérez Gómez and Hossein Mohammadalizadeh and Marco Fiore and Joerg Widmer},
  doi          = {10.1109/TMC.2025.3554035},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7757-7772},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AIChronoLens: AI/ML explainability for time series forecasting in mobile networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision language model-empowered contract theory for AIGC task allocation in teleoperation. <em>TMC</em>, <em>24</em>(8), 7742-7756. (<a href='https://doi.org/10.1109/TMC.2025.3551597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating low-light image enhancement techniques, in which diffusion-based AI-generated content (AIGC) models are promising, is necessary to enhance nighttime teleoperation. Remarkably, the AIGC model is computation-intensive, thus necessitating the allocation of AIGC tasks to edge servers with ample computational resources. Given the distinct cost of the AIGC model trained with varying-sized datasets and AIGC tasks possessing disparate demand, it is imperative to formulate a differential pricing strategy to optimize the utility of teleoperators and edge servers concurrently. Nonetheless, the pricing strategy formulation is under information asymmetry, i.e., the demand (e.g., the difficulty level of AIGC tasks and their distribution) of AIGC tasks is hidden information to edge servers. Additionally, manually assessing the difficulty level of AIGC tasks is tedious and unnecessary for teleoperators. To this end, we devise a framework of AIGC task allocation assisted by the Vision Language Model (VLM)-empowered contract theory, which includes two components: VLM-empowered difficulty assessment and contract theory-assisted AIGC task allocation. The first component enables automatic and accurate AIGC task difficulty assessment. The second component is capable of formulating the pricing strategy for edge servers under information asymmetry, thereby optimizing the utility of both edge servers and teleoperators. The simulation results demonstrated that our proposed framework can improve the average utility of teleoperators and edge servers by $10.88 \sim 12.43\%$ and $1.4\! \sim \!2.17\%$, respectively.},
  archive      = {J_TMC},
  author       = {Zijun Zhan and Yaxian Dong and Daniel Mawunyo Doe and Yuqing Hu and Shuai Li and Shaohua Cao and Zhu Han},
  doi          = {10.1109/TMC.2025.3551597},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7742-7756},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Vision language model-empowered contract theory for AIGC task allocation in teleoperation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint adaptation for mobile 360-degree video streaming and enhancement. <em>TMC</em>, <em>24</em>(8), 7726-7741. (<a href='https://doi.org/10.1109/TMC.2025.3555322'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tile-based streaming and super resolution (SR) are two representative technologies adopted to improve bandwidth efficiency of 360° video streaming. The former allows selective downloading of contents in the user viewport by splitting the video into multiple independently decodable tiles. The latter leverages client-side computation to enhance the received video to higher quality using advanced neural network models. In this work, we propose a Collaborated Streaming and Enhancement (CSE) adaptation framework for mobile 360° videos, which integrates super resolution with tile-based streaming to optimize the user experience with dynamic bandwidth and limited computing capability. To effectively enhance the tile-based video streaming through SR, we propose to adaptively group the tiles for quality enhancement adapting to the content similarity. We also identify and address several key design issues to integrate SR into tile-based video streaming including unified video quality assessment, computational complexity model for super resolution, and buffer analysis considering the interplay between transmission and enhancement. We further formulate the quality-of-experience (QoE) maximization problem for mobile 360° video streaming and propose a rate adaptation algorithm to make the best decisions for download and for enhancement based on the Lyapunov optimization theory. Extensive evaluation results validate the superiority of our proposed approach, which demonstrates stable performance with considerable QoE improvement, while enabling a trade-off between playback smoothness and video quality.},
  archive      = {J_TMC},
  author       = {Haotian Guo and Feng Wang and Wei Zhang and Yifei Zhu and Laizhong Cui and Jiangchuan Liu and Fei Richard Yu and Lei Zhang},
  doi          = {10.1109/TMC.2025.3555322},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7726-7741},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint adaptation for mobile 360-degree video streaming and enhancement},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reads: A personalized federated learning framework with fine-grained layer aggregation and decentralized clustering. <em>TMC</em>, <em>24</em>(8), 7709-7725. (<a href='https://doi.org/10.1109/TMC.2025.3552982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heterogeneity of local data and client performance, along with real-world system risks, is driving the evolution of federated learning (FL) towards personalized, model-heterogeneous, and decentralized approaches. However, due to the differing structures of heterogeneous models, it is hard to use them to identify clients with similar data distributions and further enhance the personalization of local models. Therefore, how to deal with data heterogeneity to obtain superior personalized local models for clients, while simultaneously addressing model heterogeneity and system risks is a challenging problem. In this paper, we propose a novel personalized FL framework with fine-gRained layEr aggregAtion and Decentralized cluStering (${\sf Reads}$), which integrates four key components: (1) deep mutual learning with privacy guarantee for model training and privacy preservation, (2) fine-grained layer similarity computation among heterogeneous model layers, (3) fully decentralized clustering for soft clustering of clients based on layer similarities, and (4) personalized layer aggregation for capturing common knowledge from other clients. Through ${\sf Reads}$, clients obtain personalized models that accommodate model heterogeneity, while the system ensures robustness against a single point of failure. Extensive experiments demonstrate the efficacy of ${\sf Reads}$ in achieving these goals.},
  archive      = {J_TMC},
  author       = {Haoyu Fu and Fengsen Tian and Guoqiang Deng and Lingyu Liang and Xinglin Zhang},
  doi          = {10.1109/TMC.2025.3552982},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7709-7725},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reads: A personalized federated learning framework with fine-grained layer aggregation and decentralized clustering},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acoustic eavesdropping from sound-induced vibrations with multi-antenna mmWave radar. <em>TMC</em>, <em>24</em>(8), 7693-7708. (<a href='https://doi.org/10.1109/TMC.2025.3551317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic eavesdropping against private or confidential spaces is a significant threat in the realm of privacy protection. While the presence of soundproof material would weaken such an attack, current eavesdropping technology may be able to bypass these protections. Fortunately, existing studies either inadequately cover the full spectrum of human speech due to low-frequency responses or rely heavily on the prior knowledge used to train a model. To address these challenges, this paper introduces mmEcho, a new acoustic eavesdropping method that utilizes millimeter-wave signals to sense vibration induced by sound precisely. Through signal processing techniques such as the intra-chirp scheme and phase calibration algorithm, mmEcho achieves micrometer-level vibration extraction without requiring target-related data. To improve the range of eavesdropping attacks while reducing noise, we optimize radar signals by leveraging the widespread availability of multiple antennas on commercial off-the-shelf radars. We comprehensively evaluate the performance of mmEcho in different real-world settings. Experimental results demonstrate that, with the aid of multi-antenna technology, mmEcho can more effectively reconstruct the audio from the target at various distances, directions, sound insulators, reverberating objects, sound levels, and languages. Compared to existing methods, our approach provides better effectiveness without prior knowledge, such as the speech data from the target.},
  archive      = {J_TMC},
  author       = {Wenhao Li and Riccardo Spolaor and Chuanwen Luo and Yuchao Sun and Huashan Chen and Guoming Zhang and Yanni Yang and Xiuzhen Cheng and Pengfei Hu},
  doi          = {10.1109/TMC.2025.3551317},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7693-7708},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Acoustic eavesdropping from sound-induced vibrations with multi-antenna mmWave radar},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ubicon-BP: Towards ubiquitous, contactless blood pressure detection using smartphone. <em>TMC</em>, <em>24</em>(8), 7680-7692. (<a href='https://doi.org/10.1109/TMC.2025.3551315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blood pressure (BP) is a critical physiological parameter closely associated with severe diseases such as heart failure and kidney damage. Current methods either require additional or dedicated hardware, or closing touching to the devices, causing discomfort and inconvenience. Therefore, a convenient, contactless BP measurement solution is highly desired. In this work, we present Ubicon-BP, a ubiquitous, device-free, and contactless BP detection application. Ubicon-BP calculates BP based on the pulse transit time (PTT), a key feature that is medically proven correlated with BP. However, using smartphone sensors to contactless calculate PTT is non-trivial since it requires a micro-second level precision for cardiac event detection. To address this issue, we propose leveraging the acoustic sensors in smartphone to detect vibrations caused by heart valve movements, as well as camera sensors to measure finger pulses. To accurately measure heartbeat signal that are susceptible to motion, we first improve the sensing granularity of acoustic signals and then introduce the IQ-MVED model to eliminate motion interference. Furthermore, when recovering pulse signals from video signals, issues such as poor generalization performance arise. Consequently, we propose the TS-CAN and meta-learning models to obtain personalized pulse signals. Finally, we transform the extracted time-frequency features from the recovered heartbeats and pulse signals to the corresponding BP. Comprehensive testing involving 50 subjects reveal a standard deviation error of $ 4.27 \; \text{mmHg}$ for diastolic pressure and $ 6.36 \; \text{mmHg}$ for systolic pressure, respectively.},
  archive      = {J_TMC},
  author       = {Yuan Wu and Shoudu Bai and Qingyong Hu and Bo Wang and Min Li and Xinrong Hu and Yanjiao Chen},
  doi          = {10.1109/TMC.2025.3551315},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7680-7692},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Ubicon-BP: Towards ubiquitous, contactless blood pressure detection using smartphone},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain-specific transport protocols for in-network processing at the edge: A case study of accelerating model synchronization. <em>TMC</em>, <em>24</em>(8), 7663-7679. (<a href='https://doi.org/10.1109/TMC.2025.3552220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, cross-device federated learning (FL) is the key to achieving personalization services for mobile users and has been widely employed by companies like Google, Microsoft, and Alibaba in production. With the explosive growth in the number of participants, the central FL server, which acts as the manager and aggregator of cross-device model training, would get overloaded, becoming the system bottlenecks. Inspired by the emerging wave of edge computing, an interesting question arises: Could edge clouds help cross-device FL systems overcome the bottleneck? This article provides a cautiously optimistic answer by proposing INP, a FL-specific In-Network Processing framework to achieve the goal. As in-network processing has broken the end-to-end principle of the involved communication and lacks the support of transport protocols, the key is to design domain-specific transport protocols for INP. To fill the gap, we propose the novel Model Download Protocol of mdp and Model Upload Protocol of mup. With mdp and mup, edge cloud nodes along the paths in INP can easily eliminate duplicated model downloads and pre-aggregate associated gradient uploads for the central FL server, thus alleviating its bottleneck effect, and further accelerating the entire training progress significantly.},
  archive      = {J_TMC},
  author       = {Shouxi Luo and Peidong Zhang and Xin Song and Pingzhi Fan and Huanlai Xing and Long Luo and Hongfang Yu},
  doi          = {10.1109/TMC.2025.3552220},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7663-7679},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Domain-specific transport protocols for in-network processing at the edge: A case study of accelerating model synchronization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UltraAdv: An ultrasonic adversarial attack on closed-box speech recognition systems. <em>TMC</em>, <em>24</em>(8), 7648-7662. (<a href='https://doi.org/10.1109/TMC.2025.3555680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attacks on speech recognition systems often use adversarial or inaudible commands. However, a challenge is that adversarial perturbations typically fall within the audible frequency range, making it difficult to achieve inaudibility. Additionally, the non-linear effects of loudspeakers often cause inaudible commands to become audible at higher power levels. Therefore, minimizing the power requirements of the attack is essential to maintain inaudibility. Another significant obstacle is the conversion of variable-length commands, especially longer ones, into shorter target commands. In this paper, we present UltraAdv, a method for generating long-range adversarial perturbations capable of compromising commands of arbitrary length in closed-box setting. By combining the ultrasonic signal with the normal one, rather than negating it as in DolphinAttack, we significantly improve the energy efficiency, thus enhancing its attack distance. We also propose a dynamically adjustable suppression-interference method based on automatic gain control to address the challenge of mismatched durations between long commands and target commands (length-independent). Experiments demonstrate that using a single perturbation, we achieve impressive success rates of 98.84% and 96.62% and 98.32% across a diverse set of 12,260 speeches on DeepSpeech, iFlytek, and Whisper. The attack range reaches up to 15 m, surpassing DolphinAttack's 5 m range at equivalent power.},
  archive      = {J_TMC},
  author       = {Guoming Zhang and Xiaohui Ma and Huiting Zhang and Riccardo Spolaor and Yanni Yang and Xiaoyu Ji and Xiuzhen Cheng and Pengfei Hu},
  doi          = {10.1109/TMC.2025.3555680},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7648-7662},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UltraAdv: An ultrasonic adversarial attack on closed-box speech recognition systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication-efficient federated learning by quantized variance reduction for heterogeneous wireless edge networks. <em>TMC</em>, <em>24</em>(8), 7632-7647. (<a href='https://doi.org/10.1109/TMC.2025.3551759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has been recognized as a viable solution for local-privacy-aware collaborative model training in wireless edge networks, but its practical deployment is hindered by the high communication overhead caused by frequent and costly server-device synchronization. Notably, most existing communication-efficient FL algorithms fail to reduce the significant inter-device variance resulting from the prevalent issue of device heterogeneity. This variance severely decelerates algorithm convergence, increasing communication overhead and making it more challenging to achieve a well-performed model. In this paper, we propose a novel communication-efficient FL algorithm, named FedQVR, which relies on a sophisticated variance-reduced scheme to achieve heterogeneity-robustness in the presence of quantized transmission and heterogeneous local updates among active edge devices. Comprehensive theoretical analysis justifies that FedQVR is inherently resilient to device heterogeneity and has a comparable convergence rate even with a small number of quantization bits, yielding significant communication savings. Besides, considering non-ideal wireless channels, we propose FedQVR-E which enhances the convergence of FedQVR by performing joint allocation of bandwidth and quantization bits across devices under constrained transmission delays. Extensive experimental results are also presented to demonstrate the superior performance of the proposed algorithms over their counterparts in terms of both communication efficiency and application performance.},
  archive      = {J_TMC},
  author       = {Shuai Wang and Yanqing Xu and Chaoqun You and Mingjie Shao and Tony Q. S. Quek},
  doi          = {10.1109/TMC.2025.3551759},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7632-7647},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Communication-efficient federated learning by quantized variance reduction for heterogeneous wireless edge networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrowdHMTware: A cross-level co-adaptation middleware for context-aware mobile DL deployment. <em>TMC</em>, <em>24</em>(8), 7615-7631. (<a href='https://doi.org/10.1109/TMC.2025.3549399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many deep learning (DL) powered mobile and wearable applications today continuously and unobtrusively sensing the ambient surroundings to enhance all aspects of human lives. To enable robust and private mobile sensing, DL models are often deployed locally on resource-constrained mobile devices using techniques such as model compression or offloading. However, existing methods, either front-end algorithm level (i.e. DL model compression/partitioning) or back-end scheduling level (i.e. operator/resource scheduling), cannot be locally online because they require offline retraining to ensure accuracy or rely on manually pre-defined strategies, struggle with dynamic adaptability. The primary challenge lies in feeding back runtime performance from the back-end level to the front-end level optimization decision. Moreover, the adaptive mobile DL model porting middleware with cross-level co-adaptation is less explored, particularly in mobile environments with diversity and dynamics. In response, we introduce CrowdHMTware, a dynamic context-adaptive DL model deployment middleware for heterogeneous mobile devices. It establishes an automated adaptation loop between cross-level functional components, i.e. elastic inference, scalable offloading, and model-adaptive engine, enhancing scalability and adaptability. Experiments with four typical tasks across 15 platforms and a real-world case study demonstrate that ${\sf CrowdHMTware}$ can effectively scale DL model, offloading, and engine actions across diverse platforms and tasks. It hides run-time system issues from developers, reducing the required developer expertise.},
  archive      = {J_TMC},
  author       = {Sicong Liu and Bin Guo and Shiyan Luo and Yuzhan Wang and Hao Luo and Cheng Fang and Yuan Xu and Ke Ma and Yao Li and Zhiwen Yu},
  doi          = {10.1109/TMC.2025.3549399},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7615-7631},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CrowdHMTware: A cross-level co-adaptation middleware for context-aware mobile DL deployment},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-UAV-assisted MEC in internet of vehicles with combined multi-modal semantic communication under jamming attacks. <em>TMC</em>, <em>24</em>(8), 7600-7614. (<a href='https://doi.org/10.1109/TMC.2025.3550965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic communication technology, which transmits only relevant semantic information, can significantly conserve communication resources and reduce service time. This technology is particularly promising for unpilotedaerial vehicle (UAV)-assisted mobile edge computing (MEC) in the internet of vehicles (IoV). However, integrating semantic communication with UAV-assisted vehicle MEC is susceptible to malicious jamming. This paper introduces a reliable communication method that combines multi-modal semantic communication with UAV-assisted vehicle MEC to minimize delays in communication and computation while maintaining semantic accuracy during jamming attacks. Our approach optimizes UAV trajectories, user associations, and channel selections, enabling the UAV to select optimal positions when associating with different modal users and reducing the impact of jammers during multi-modal task reception. Due to the non-convex nature of the optimization problem and the highly dynamic environment, we employ the semantic communication combined with the multi-agent twin delayed deep deterministic policy gradient (SC-MA-TD3) approach, a multi-agent deep reinforcement learning (DRL) strategy that fosters UAV cooperation for efficient resource allocation. Simulation results show that our approach outperforms existing approaches in reducing delays and enhancing semantic accuracy.},
  archive      = {J_TMC},
  author       = {Shuai Liu and Helin Yang and Mengting Zheng and Liang Xiao},
  doi          = {10.1109/TMC.2025.3550965},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7600-7614},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-UAV-assisted MEC in internet of vehicles with combined multi-modal semantic communication under jamming attacks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cybertwin-enabled multipath transmission scheme in cloud native networks. <em>TMC</em>, <em>24</em>(8), 7584-7599. (<a href='https://doi.org/10.1109/TMC.2025.3550129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address issues arising from the independence of the radio access network, the IP bearer network, and the data center network, a Cybertwin-based cloud native network (CCNN) is proposed. However, due to diverse application requirements and user demands, providing individualized transmission services that meet these requirements within CCNN remains a major challenge. This paper proposes a Cybertwin-enabled multipath transmission scheme (CMTS) to provide personalized quality-of-service (QoS)-guaranteed transmission services by dynamically creating network paths tailored to users’ demands. Specifically, by introducing Cybertwin, CMTS allows separate and dynamic scheduling of service and access network resources from different providers, decouples users from network providers, and enables more flexible use of heterogeneous network resources. Moreover, CMTS can make resource scheduling decisions on demand by leveraging the user’s personal information held by Cybertwin. To fully exploit the potential of multiple paths, we formulate network resource planning as an optimization problem aimed at minimizing link differences and propose a heuristic method for determining optimal policies. Finally, we present the first implementation of Cybertwin in CCNN, validate CMTS in both emulated and semi-physical environments, and conduct thorough evaluations with benchmarks. The results show that CMTS can deliver a personalized, QoS-guaranteed transmission service while achieving efficient resource utilization.},
  archive      = {J_TMC},
  author       = {Zhiyong Zeng and Meng Qin and Dandan Liang},
  doi          = {10.1109/TMC.2025.3550129},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7584-7599},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A cybertwin-enabled multipath transmission scheme in cloud native networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMRE: Adaptive multilevel redundancy elimination for multimodal mobile inference. <em>TMC</em>, <em>24</em>(8), 7568-7583. (<a href='https://doi.org/10.1109/TMC.2025.3549422'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given privacy and network load concerns, employing on-device multimodal neural networks (MNNs) for IoT data is a growing trend. However, the high computational demands of MNNs clash with limited on-device resources. MNNs involve input and model redundancies during inference, wasting resources to process redundant input components and run excess model parameters. Model Redundancy Elimination (MRE) reduces redundant parameters but cannot bypass inference for unnecessary input components. Input Redundancy Elimination (IRE) skips inference for redundant input components but cannot reduce computation for the remaining parts. MRE and IRE independently fail to meet the diverse computational needs of multimodal inference. To address these issues, we aim to combine the advantages of MRE and IRE to achieve a more efficient inference. We propose an adaptive multilevel redundancy elimination framework (AMRE), which supports both IRE and MRE. AMRE first establishes a collaborative inference mechanism for IRE and MRE. We then propose a multifunctional, lightweight policy model that adaptively controls the inference logic for each instance. Moreover, a three-stage training method is proposed to ensure the performance of collaborative inference in AMRE. We validate AMRE in three scenarios, achieving up to 52.91% lower latency, 56.79% lower energy cost, and a slight accuracy gain compared to state-of-the-art baselines.},
  archive      = {J_TMC},
  author       = {Qixuan Cai and Ruikai Chu and Kaixuan Zhang and Xiulong Liu and Xinyu Tong and Xin Xie and Jiancheng Chen and Keqiu Li},
  doi          = {10.1109/TMC.2025.3549422},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7568-7583},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AMRE: Adaptive multilevel redundancy elimination for multimodal mobile inference},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Streamlining data transfer in collaborative SLAM through bandwidth-aware map distillation. <em>TMC</em>, <em>24</em>(8), 7554-7567. (<a href='https://doi.org/10.1109/TMC.2025.3549367'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge intelligence offers a promising solution for Simultaneous Localization and Mapping (SLAM) in large-scale scenarios, where multiple robots collaboratively perceive the environment and upload their local maps to an edge server. However, maintaining mapping accuracy under constrained and dynamic communication resources remains a significant challenge for the practical deployment of robot swarms. Concurrent data uploads from multiple agents can exacerbate network congestion, leading to the loss of critical information, delayed updates, and, ultimately, the inconsistency of the generated maps. This paper presents Hermes, an edge-assisted collaborative mapping system designed for communication-constrained environments. Hermes streamlines data transfer through bandwidth-aware map distillation, ensuring only the most crucial messages are transmitted to the edge server. We quantify the importance of keyframes and landmarks based on their information entropy gain in pose estimation. By selectively sharing essential submaps, Hermes adaptively balances communication bandwidth and information richness during the mapping process. We implemented Hermes on heterogeneous platforms and conducted experiments using public datasets and self-collected campus data. Hermes exceeds SwarmMap by 50% in bandwidth utilization with similar accuracy and surpasses COVINS-G by 65% in trajectory error under highly constrained network resources.},
  archive      = {J_TMC},
  author       = {Rui Ge and Huanghuang Liang and Zheng Gong and Chuang Hu and Xiaobo Zhou and Dazhao Cheng},
  doi          = {10.1109/TMC.2025.3549367},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7554-7567},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Streamlining data transfer in collaborative SLAM through bandwidth-aware map distillation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LVMScissor: Split and schedule large vision model inference on mobile edges via salp swarm algorithm. <em>TMC</em>, <em>24</em>(8), 7538-7553. (<a href='https://doi.org/10.1109/TMC.2025.3550519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Computer Vision, the Large Vision Models (LVM) based on Vision Transformer (ViT) achieve advanced performance on general complex visual tasks. However, deploying these resource-intensive models on edge devices with limited computational power and memory is challenging, especially for those mobile edge devices. Existing model compression works downgrade the prediction accuracy and fail to adapt to dynamic network bandwidth or hardware changes. Besides, the split inference for typical Deep Neural Networks (DNN) is inefficient for LVM’s large intermediate result size. To address the computation and bandwidth limitation of edge devices of LVM, we design a new split inference acceleration LVMScissor by leveraging model parallelism and meta-heuristic algorithm. We first implement an inter-layer ViT parallelism strategy. After modeling the parallelized ViT split problem into a Multi-task three-processor scheduling (MTS) problem, we propose LVM-MSSA, a scheduling algorithm based on a famous meta-heuristic algorithm, the Multi-objective Salp Swarm Algorithm (MSSA) to schedule model parallelism and split strategy. The evaluation results show that compared to state-of-the-art inference acceleration approaches, our solution is faster from $1.6\times$ to $6.92\times$ under variant LVMs, edge devices, datasets, and network traces.},
  archive      = {J_TMC},
  author       = {Yanting Liu and Rui Lu and Dan Wang},
  doi          = {10.1109/TMC.2025.3550519},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7538-7553},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LVMScissor: Split and schedule large vision model inference on mobile edges via salp swarm algorithm},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Jointly-optimized trajectory generation and camera control for 3D coverage planning. <em>TMC</em>, <em>24</em>(8), 7519-7537. (<a href='https://doi.org/10.1109/TMC.2025.3551362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a jointly optimized trajectory generation and camera control approach, enabling an autonomous agent, such as an unmanned aerial vehicle (UAV) operating in 3D environments, to plan and execute coverage trajectories that maximally cover the surface area of a 3D object of interest. Specifically, the UAV's kinematic and camera control inputs are jointly optimized over a rolling planning horizon to achieve complete 3D coverage of the object. The proposed controller incorporates ray-tracing into the planning process to simulate the propagation of light rays, thereby determining the visible parts of the object through the UAV's camera. This integration enables the generation of precise look-ahead coverage trajectories. The coverage planning problem is formulated as a rolling finite-horizon optimal control problem and solved using mixed-integer programming techniques. Extensive real-world and synthetic experiments validate the performance of the proposed approach.},
  archive      = {J_TMC},
  author       = {Savvas Papaioannou and Panayiotis Kolios and Theocharis Theocharides and Christos G. Panayiotou and Marios M. Polycarpou},
  doi          = {10.1109/TMC.2025.3551362},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7519-7537},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Jointly-optimized trajectory generation and camera control for 3D coverage planning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and cost-effective vehicle recruitment for HD map crowdsourcing. <em>TMC</em>, <em>24</em>(8), 7505-7518. (<a href='https://doi.org/10.1109/TMC.2025.3552396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high-definition (HD) map is a cornerstone of autonomous driving. The crowdsourcing paradigm is a cost-effective way to keep an HD map up-to-date. Current HD map crowdsourcing mechanisms aim to enhance HD map freshness within recruitment budgets. However, many overlook unique and critical traits of crowdsourcing vehicles, such as random arrival and heterogeneity, leading to either compromised map freshness or excessive recruitment costs. Furthermore, these characteristics complicate the characterization of the feasible space of the optimal recruitment policy, necessitating a method to compute it efficiently in dynamic transportation scenarios. To overcome these challenges, we propose an efficient and cost-effective vehicle recruitment (ENTER) mechanism. Specifically, the ENTER mechanism has a threshold structure and balances freshness with recruitment costs while accounting for the vehicles’ random arrival and heterogeneity. It also integrates the bound-based relative value iteration (RVI) algorithm, which utilizes the threshold-type structure and upper bounds of thresholds to reduce the feasible space and expedite convergence. Numerical results show that the proposed ENTER mechanism increases the HD map company's payoff by 23.40$\%$ and 43.91$\%$ compared to state-of-the-art mechanisms that do not account for vehicle heterogeneity and random arrivals, respectively. Furthermore, the bound-based RVI algorithm in the ENTER mechanism reduces computation time by an average of 18.91% compared to the leading RVI-based algorithm.},
  archive      = {J_TMC},
  author       = {Wentao Ye and Yuan Luo and Bo Liu and Jianwei Huang},
  doi          = {10.1109/TMC.2025.3552396},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7505-7518},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient and cost-effective vehicle recruitment for HD map crowdsourcing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting and characterising mobile app metamorphosis in google play store. <em>TMC</em>, <em>24</em>(8), 7489-7504. (<a href='https://doi.org/10.1109/TMC.2025.3550121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {App markets have evolved into highly competitive and dynamic environments for developers. While the traditional app life cycle involves incremental updates for feature enhancements and issue resolution, some apps deviate from this norm by undergoing significant transformations in their use cases or market positioning. We define this previously unstudied phenomenon as ‘app metamorphosis'. In this paper, we propose a novel and efficient multi-modal search methodology to identify apps undergoing metamorphosis and apply it to analyse two snapshots of the Google Play Store taken five years apart. Our methodology uncovers various metamorphosis scenarios, including re-births, re-branding, re-purposing, and others, enabling comprehensive characterisation. Although these transformations may register as successful for app developers based on our defined success score metric (e.g., re-branded apps performing approximately 11.3% better than an average top app), we shed light on the concealed security and privacy risks that lurk within, potentially impacting even tech-savvy end-users.},
  archive      = {J_TMC},
  author       = {Dishanika Denipitiyage and Bhanuka Silva and Kavishka Gunathilaka and Suranga Seneviratne and Anirban Mahanti and Aruna Seneviratne and Sanjay Chawla},
  doi          = {10.1109/TMC.2025.3550121},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7489-7504},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Detecting and characterising mobile app metamorphosis in google play store},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimal reverse affine maximizer auction mechanism for task allocation in mobile crowdsensing. <em>TMC</em>, <em>24</em>(8), 7475-7488. (<a href='https://doi.org/10.1109/TMC.2025.3549504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowdsensing service (MCS) providers recruit users to complete data collection tasks with an incentive mechanism. How to maximize the utility of service providers has long been a popular topic in MCS research. Applying the existing reverse auction mechanism to an MCS may result in excessively high payments, thereby reducing the utility of the MCS provider. The affine maximizer auction (AMA) mechanism increases the revenue of service providers and meets dominant-strategy incentive-compatible (DSIC) characteristics. However, the AMA mechanism is a forward auction mechanism and cannot be applied to MCSs. Inspired by the AMA mechanism, this paper innovatively proposes a reverse affine maximizer auction (RAMA) mechanism to solve the task allocation problem of MCSs, effectively improving the MCS provider utility. Specifically, we construct a RAMA theoretical model and prove that the mechanism satisfies DSIC characteristics. For the discrete MCS task allocation problem, we use the reverse virtual valuation combinatorial auction (RVVCA) mechanism, a subclass of RAMA, to design a random mechanism RVVCA$^{t}$ and prove that the RVVCA$^{t}$ has a logarithmic approximate ratio. For the differentiable MCS task allocation problem, we use the deep learning transformer framework to design RAMANet, which can fit an exponential number of allocation solutions and output the optimal allocation and payment. We experimentally compare the algorithms of the RAMA family we propose, which use affine maximization, with existing state-of-the-art algorithms, demonstrating that the proposed algorithms significantly improve MCS provider utility.},
  archive      = {J_TMC},
  author       = {Jixian Zhang and Peng Chen and Xuelin Yang and Hao Wu and Weidong Li},
  doi          = {10.1109/TMC.2025.3549504},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7475-7488},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An optimal reverse affine maximizer auction mechanism for task allocation in mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A four-year retrospective of mobile access bandwidth evolution: The inspiring, the frustrating, and the fluctuating. <em>TMC</em>, <em>24</em>(8), 7458-7474. (<a href='https://doi.org/10.1109/TMC.2025.3551595'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in mobile technologies (like WiFi 6 and 5G) do not seem to deliver the promised access bandwidth. To effectively characterize mobile access bandwidth in the wild, we work with a major commercial mobile bandwidth testing app to conduct a long-term (2020-2023) and large-scale (involving 4.76M users) measurement study in China, based on coarse-grained general statistics and fine-grained sampling diagnostics. Our study presents distinct facts as to WiFi, 5G, and 4G: in the past few years, the average WiFi download bandwidth exhibits a considerable rise (by 119.7% ), the average 5G download bandwidth constantly decreases (by a total of 20.2% ) despite the enormous infrastructure investments, while the average 4G download bandwidth first declines (by 22.1% ) and then increases (by 22.5% ). The situations of upload bandwidths are generally similar to those of download bandwidths, except that 5G upload bandwidths manifest N-shaped $(\nearrow \searrow \nearrow )$ fluctuations. Our cross-layer and cross-technology analysis reveals a variety of impact factors as well as their complicated interplay as the root causes, such as the bottlenecks in underlying infrastructure (e.g., communication devices and wired Internet access), the traffic offloading from one access technology to another, the influence of the COVID-19 pandemic, and the side effects of aggressively migrating radio resources from 4G to 5G. With the longitudinal, holistic picture of today's mobile access bandwidth, we finally provide multifold practical implications on closing the technology gaps.},
  archive      = {J_TMC},
  author       = {Zhenhua Li and Ruoxuan Yang and Xinlei Yang and Jing Yang and Xingyao Li and Hao Lin and Feng Qian and Yunhao Liu and Zhi Liao and Daqiang Hu},
  doi          = {10.1109/TMC.2025.3551595},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7458-7474},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A four-year retrospective of mobile access bandwidth evolution: The inspiring, the frustrating, and the fluctuating},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint RIS and beamforming design for secure and energy-efficient two-way relay communications. <em>TMC</em>, <em>24</em>(8), 7440-7457. (<a href='https://doi.org/10.1109/TMC.2025.3549445'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the enhancement of secrecy energy efficiency (SEE) in a reconfigurable intelligent surface (RIS)-assisted two-way relay (TWR) system. We first establish a theoretical model for the system's secrecy rate, energy consumption, and SEE, and formulate the SEE maximization problem through the joint design of the RIS phase shifts and beamforming matrix. Using techniques such as weighted minimum mean square error (WMMSE), alternating optimization, and the augmented Lagrange method, we then develop a theoretical framework that identifies locally optimal solutions for the RIS and beamforming settings under unit-modulus and power constraints. The proposed framework is also shown to be applicable to solving the system's secrecy rate maximization problem. To address the computational complexity involved in optimizing the RIS phase shifts, we further propose a suboptimal scheme leveraging the Newton's method, which significantly reduces the computational burden while achieving performance close to the optimal SEE. Extensive numerical results validate the effectiveness of the proposed schemes, showing significant SEE improvements compared to traditional channel-capacity-based secure transmission scheme.},
  archive      = {J_TMC},
  author       = {Shuangrui Zhao and Xinghui Zhu and Yuanyu Zhang and Zhiwei Zhang and Yulong Shen},
  doi          = {10.1109/TMC.2025.3549445},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7440-7457},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint RIS and beamforming design for secure and energy-efficient two-way relay communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring communication-efficient federated learning via stateless in-network aggregation. <em>TMC</em>, <em>24</em>(8), 7423-7439. (<a href='https://doi.org/10.1109/TMC.2025.3551368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an ambitious training paradigm, federated learning has garnered increasing attention in recent years, which enables collaborative training of a global model without accessing users’ private data. However, due to the simultaneous and constant model updates gathering from massive distributed clients, the central server generally becomes a performance bottleneck. Additionally, the stateful aggregation (retaining all the updates from each client) conducted by the central server further poses potential threats to privacy, since it may recover the raw data based on such model updates inversely. The state-of-the-art methodologies, however, fail to address these two problems concurrently and efficiently. To this end, we propose GAIN, a secure aggregation acceleration service for federated learning. At its core, GAIN leverages programmable switches deployed at the edge network to aggregate model updates in a stateless manner before transmitting them to the central server. Consequently, GAIN can accelerate the transmission and aggregation of model updates while eliminating the chance of recovering private data. We evaluate the performance of GAIN through FPGA-based experiments and large-scale simulations. The results show that GAIN can effectively reduce bandwidth overhead and achieve up to 4.11× training throughput acceleration while prioritizing privacy protection.},
  archive      = {J_TMC},
  author       = {Junxu Xia and Geyao Cheng and Wenfei Wu and Lailong Luo and Deke Guo},
  doi          = {10.1109/TMC.2025.3551368},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7423-7439},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring communication-efficient federated learning via stateless in-network aggregation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedHMIR: Unified framework for federated human-machine synergy in personalization-generalization balancing identity recognition. <em>TMC</em>, <em>24</em>(8), 7406-7422. (<a href='https://doi.org/10.1109/TMC.2025.3549925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As device-free identity recognition (IR) gains popularity and the demand for the Internet of Things (IoT) continues to grow, a new-era IR system featuring multiple distributed recognition devices and edge servers faces two main challenges: model adaptability and balancing the personalization of devices with the generalization of the system. This research introduces FedHMIR, a federated framework designed to simultaneously address these challenges by harmonizing human-machine collaboration with personalization-generalization trade-offs. The proposed framework features a human-machine cooperative online internal update mechanism, leveraging reinforcement learning to maintain the adaptability of personalized local IR models. To counter overfitting and enhance the generalization of the overall IR system, an external update process incorporating a confidence index is introduced. Additionally, the framework employs asynchronous internal and external update procedures to effectively balance personalization and generalization between local and global models. Finally, extensive experiments on three diverse real-world datasets demonstrate the effectiveness and advantages of FedHMIR compared to state-of-the-art baselines.},
  archive      = {J_TMC},
  author       = {Qingyang Li and Yuanjiang Cao and Qianru Wang and Lina Yao and Zhiwen Yu and Jiangtao Cui},
  doi          = {10.1109/TMC.2025.3549925},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7406-7422},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedHMIR: Unified framework for federated human-machine synergy in personalization-generalization balancing identity recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contract-inspired contest theory for controllable image generation in mobile edge metaverse. <em>TMC</em>, <em>24</em>(8), 7389-7405. (<a href='https://doi.org/10.1109/TMC.2025.3550815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of immersive technologies has propelled the development of the Metaverse, where the convergence of virtual and physical realities necessitates the generation of high-quality, photorealistic images to enhance user experience. However, generating these images, especially through Generative Diffusion Models (GDMs), in mobile edge computing environments presents significant challenges due to the limited computing resources of edge devices and the dynamic nature of wireless networks. This paper proposes a novel framework that integrates contract-inspired contest theory, Deep Reinforcement Learning (DRL), and GDMs to optimize image generation in these resource-constrained environments. The framework addresses the critical challenges of resource allocation and semantic data transmission quality by incentivizing edge devices to efficiently transmit high-quality semantic data, which is essential for creating realistic and immersive images. The use of contest and contract theory ensures that edge devices are motivated to allocate resources effectively, while DRL dynamically adjusts to network conditions, optimizing the overall image generation process. Experimental results demonstrate that the proposed approach not only improves the quality of generated images but also achieves superior convergence speed and stability compared to traditional methods. This makes the framework particularly effective for optimizing complex resource allocation tasks in mobile edge Metaverse applications, offering enhanced performance and efficiency in creating immersive virtual environments.},
  archive      = {J_TMC},
  author       = {Guangyuan Liu and Hongyang Du and Jiacheng Wang and Dusit Niyato and Dong In Kim},
  doi          = {10.1109/TMC.2025.3550815},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7389-7405},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Contract-inspired contest theory for controllable image generation in mobile edge metaverse},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). H-STEP: Heuristic stable edge service entity placement for mobile virtual reality systems. <em>TMC</em>, <em>24</em>(8), 7377-7388. (<a href='https://doi.org/10.1109/TMC.2025.3548703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) technology, as a latency-sensitive application, can achieve real-time response to enhance the user’s quality of experience (QoE) on edge devices. However, edge servers, unlike internally managed cloud servers, are prone to hardware failures, software abnormalities, and network attacks. Most prior studies have focused on reducing service delay and improving user coverage through service entity (SE) placement, often neglecting the critical impact of edge server malfunctions on user QoE. In this work, we design a stable service entity placement framework that connects users on faulty servers to collaborative edge servers, ensuring seamless task completion. This framework presents two primary challenges: determining the grouping of collaborative edge services and the placement of SEs. To address these challenges, we introduce a heuristic stable service entity placement (H-STEP) scheme. This scheme first determines the grouping of collaborative edge servers using an iterative search algorithm and then places SEs on suitable edge servers via a fast non-dominated sorting genetic placement algorithm. This approach balances stability benefits with total cost, enhancing the system’s economic benefits. We theoretically analyze the performance of H-STEP and derive the performance gap between H-STEP and the optimal scheme. Extensive real-data-driven simulations demonstrate that H-STEP’s performance closely approximates that of the optimal scheme and surpasses existing schemes.},
  archive      = {J_TMC},
  author       = {Xuejian Chi and Honglong Chen and Zhichen Ni and Haiyang Sun and Peng Sun and Dongxiao Yu},
  doi          = {10.1109/TMC.2025.3548703},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7377-7388},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {H-STEP: Heuristic stable edge service entity placement for mobile virtual reality systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OACR$^{2}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>: Online admission control and resource reservation for 5G slice networks with deep reinforcement learning. <em>TMC</em>, <em>24</em>(8), 7360-7376. (<a href='https://doi.org/10.1109/TMC.2025.3548767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing architecture is expected to fulfill network applications with heterogeneous requirements through efficient slice admission control (SAC) policies. Existing SAC approaches entirely rely on current limited observations to make admission decisions, ignoring the potential impact of future demands. The short-sighted behaviors lead to poor service performance and infrastructure providers’ (InPs’) revenue in practice. In this paper, we propose OACR$^{2}$, an online SAC approach based on deep reinforcement learning (DRL) that can exploit predictable future requests to make more precise admission control decisions for the long-term revenue, and reserve proper resources accordingly. Specifically, we design three novel schemes: (i) a requirement predictor based on long short-term memory (LSTM) and a novel input-output way to predict future unforeseen requests, (ii) a DRL admission controller based on the partially observable Markov decision process model to make precise admission decisions without accurate future request information, with the convergence strictly proved, and (iii) a decision defender to guarantee decision reliability. Extensive experiments on real-world traces demonstrate that compared to the No-wait, Wait-queue, and Wait-earliest time approaches, OACR$^{2}$ improves InPs’ revenue and acceptance ratio by up to 40.9% and 16.7%, respectively, without sacrificing online inference time (within 0.9 milliseconds).},
  archive      = {J_TMC},
  author       = {Fang Li and Yijun Hao and Shusen Yang and Peng Zhao},
  doi          = {10.1109/TMC.2025.3548767},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7360-7376},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {OACR$^{2}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>: Online admission control and resource reservation for 5G slice networks with deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A joint secure mechanism of multi-task learning for a UAV team under FDI attacks. <em>TMC</em>, <em>24</em>(8), 7345-7359. (<a href='https://doi.org/10.1109/TMC.2025.3551537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A UAV team shows tremendous potential for various mobile scenarios. However, some evidences reveal their vulnerability to False Data Injection (FDI) attacks, which can significantly jeopardize the flight security or even lead to catastrophic incidents. Existing studies primarily focus on detecting or defending against FDI attacks at the trajectory control of individual UAVs, leaving a gap in a comprehensive secure mechanism that can simultaneously detect, localize, and compensate for such attacks across an entire UAV team. The complexity of developing such a solution is magnified by the multiple design goals, the inherent sophistication of UAV team, and practical attack assumptions. In this paper, we propose a joint secure framework based on multi-task deep learning to simultaneously detect FDI attacks, localize the compromised components, and compensate control signals to mitigate the impact of FDI attacks on promising UAV teams. Specifically, we design an all-in-one deep learning model framework with a temporal-spatial information extraction module and a hierarchical multi-task module to perform three tasks simultaneously. Moreover, we introduce an iterative learning method with experience replay to counteract knowledge decay during model training. Extensive experiments and real flight demonstrations are presented to validate the improved performance and the benefits of our proposed secure method.},
  archive      = {J_TMC},
  author       = {Rongfei Zeng and Chenyang Jiang and Xingwei Wang and Baochun Li},
  doi          = {10.1109/TMC.2025.3551537},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7345-7359},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A joint secure mechanism of multi-task learning for a UAV team under FDI attacks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASA: Multimodal federated learning through modality-aware and secure aggregation. <em>TMC</em>, <em>24</em>(8), 7328-7344. (<a href='https://doi.org/10.1109/TMC.2025.3548954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a promising paradigm, federated learning has been applied to multimodal sensing tasks due to its deployment convenience. However, the recent advances in multimodal federated learning emphasize learning a high-quality multimodal model but overlook the model usage requirements of massive unimodal clients. Moreover, the privacy risk in model sharing and client data heterogeneity impact the efficacy of federated learning. In this paper, we propose a novel multimodal federated learning system named MASA. As a departure from existing approaches, MASA simultaneously enhances the model learning efficiency of both multimodal and unimodal clients while ensuring their data privacy. First, we employ a gated cross-modal distillation scheme to achieve performance-aware knowledge transfer across modality-heterogeneous clients. To enhance the system security, MASA integrates a lightweight split-shuffle mechanism to realize the anonymization and encryption of model aggregation. Moreover, to reach personalized collaboration while protecting privacy, MASA features an attention-based spontaneous client clustering mechanism to form client cluster structures securely and distributedly. We evaluate our MASA on four public multimodal datasets for human activity recognition. The results show that our MASA outperforms leading multimodal federated learning methods on the model performance of both multimodal and unimodal clients.},
  archive      = {J_TMC},
  author       = {Jialin Guo and Yongjian Fu and Zhiwei Zhai and Xinyi Li and Yongheng Deng and Sheng Yue and Lili Chen and Hao Pan and Ju Ren},
  doi          = {10.1109/TMC.2025.3548954},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7328-7344},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MASA: Multimodal federated learning through modality-aware and secure aggregation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HarmonyPath: Fine-grained flexible multipath transmission for mobile differentiated services. <em>TMC</em>, <em>24</em>(8), 7312-7327. (<a href='https://doi.org/10.1109/TMC.2025.3549505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in mobile application services has led to diversified traffic and increased demands on network resources. Traditional multipath algorithms, designed for resource integration through subflow scheduling across paths, struggle with disharmonious transmission caused by terminal mobility and differentiated path resources. Especially when differentiated services are transmitted concurrently, disharmonious transmission can give rise to resource contention, causing a large number of subflows to congest a single path and leading to performance degradation. To mitigate these challenges, this paper introduces HarmonyPath, a fine-grained flexible multipath transmission mechanism that can ensure harmonious resource occupation. Specifically, HarmonyPath firstly employs an in-band telemetry protocol to gather path resource information, generating a network resource distribution map. Based on this map, it flexibly allocates path resources according to the network resource distribution and service requirements. Then, HarmonyPath establishes a collaborative matching model for service demands and path resources. Through matrix transformation and calculation, it rapidly generates and deploys the scheduling strategy. To further alleviate service contention, HarmonyPath employs heuristic algorithms to optimize the scheduling strategy and achieve precise multipath transmission. Experiments demonstrate that HarmonyPath surpasses traditional algorithms in the multipath transmission of differentiated services, offering flexible service resource guarantees and enhancing network resource utilization efficiency.},
  archive      = {J_TMC},
  author       = {Ziheng Xu and Wei Quan and Nan Cheng and Yuming Zhang and Mingyuan Liu and Xiaoting Ma and Xue Zhang and Hongke Zhang},
  doi          = {10.1109/TMC.2025.3549505},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7312-7327},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HarmonyPath: Fine-grained flexible multipath transmission for mobile differentiated services},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning adaptive multi-timescale scheduling for mobile edge computing. <em>TMC</em>, <em>24</em>(8), 7297-7311. (<a href='https://doi.org/10.1109/TMC.2025.3548533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing (MEC), resource scheduling is crucial to task requests’ performance and service providers’ cost, involving multi-layer heterogeneous scheduling decisions. Existing MEC schedulers typically adopt static-timescale scheduling, where scheduling decisions are updated regularly at fixed intervals for all layers. The inflexible updating timescales lead to poor performance in the production networks. In this paper, we propose EdgeTimer, an unprecedented approach that automatically and adaptively determines respective updating timescales of multiple scheduling layers to achieve a better trade-off between the operation cost and service performance. Specifically, we design (i) a three-layer hierarchical deep reinforcement learning (DRL) framework for efficient learning of tightly coupled policies, (ii) a tailored multi-agent DRL algorithm for decentralized scheduling, with the convergence strictly proved, and (iii) a lightweight system defender for deterministic reliability assurance. Furthermore, we apply EdgeTimer to a wide range of Kubernetes scheduling rules, and evaluate it using production traces with different workload patterns. Through extensive trace-driven experiments, we demonstrate that EdgeTimer can significantly decrease the operation cost for service providers without sacrificing the delay performance, thereby improving overall profits, compared with the state-of-the-art approaches.},
  archive      = {J_TMC},
  author       = {Yijun Hao and Shusen Yang and Fang Li and Yifan Zhang and Shibo Wang and Xuebin Ren},
  doi          = {10.1109/TMC.2025.3548533},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7297-7311},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Learning adaptive multi-timescale scheduling for mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated multi-source domain adaptation for mmWave-based human activity recognition. <em>TMC</em>, <em>24</em>(8), 7283-7296. (<a href='https://doi.org/10.1109/TMC.2025.3549705'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contactless mmWave-based human activity recognition (HAR) is essential for various applications, yet most existing approaches often assume consistent environments. Integrating domain adaptation offers a promising solution to this challenge. This prevailing paradigm works well when the source and target data are centralized on a single server while learning to adapt. However, in more universal and practical situations, such as personal health records, users’ biometric information, and financial issues, the raw data is typically protected by different privacy-preserving policies and is stored by multiple parties. Additionally, labeling RF signals in the target domain is a non-trivial and labor-intensive task for most end-users. To address these problems, this paper introduces FMDA, a federated multi-source domain adaptation framework for mmWave-based HAR. FMDA assesses the contribution of each source and performs weighted parameter aggregation for knowledge transfer. This facilitates unsupervised training of the target HAR model without requiring access to any source domain data. Moreover, the model is optimized by minimizing the generalization gaps between the source and target models, benefiting all participants during the learning process and enhancing overall performance. Extensive experiments demonstrate the effectiveness of FMDA. The results indicate that in the target domain, FMDA achieves comparable performance to supervised learning approaches, while also enhancing the efficacy of source domain models to varying degrees.},
  archive      = {J_TMC},
  author       = {Cui Zhao and Guotong Fang and Han Ding and Xinhui Liu and Fei Wang and Ge Wang and Kun Zhao and Zhi Wang and Wei Xi},
  doi          = {10.1109/TMC.2025.3549705},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7283-7296},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated multi-source domain adaptation for mmWave-based human activity recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint offloading decision, user association, and resource allocation in hierarchical aerial computing: Collaboration of UAVs and HAP. <em>TMC</em>, <em>24</em>(8), 7267-7282. (<a href='https://doi.org/10.1109/TMC.2025.3548668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, applications are becoming increasingly computation-intensive and delay-sensitive owing to the rapid growth of Internet of Things (IoT) devices among ground users (GUs). Mobile edge computing (MEC) presents crucial computational support, but conventional MEC services often fail in remote areas and in disaster scenarios. This study presents a hierarchical aerial computing platform leveraging uncrewed aerial vehicles (UAVs) and high-altitude platform (HAP) to meet the computation demands and latency requirements of various IoT applications for GUs. We propose a joint offloading decision, user association, and resource allocation (JOUR) scheme, utilizing binary offloading from GUs to UAVs and partial offloading from UAVs to HAP. The proposed scheme minimizes the energy consumption and latency while maximizing the load balancing. A matching game-based algorithm addresses the GUs offloading decision and GUs-UAVs association, followed by an enhanced soft actor-critic (ESAC) algorithm for UAV partial offloading decision, UAV computation resource allocation, and HAP computation resource allocation. Our simulation results demonstrate the effectiveness of the JOUR scheme in reducing the energy consumption and latency, while improving the load balancing and task completion rates. This demonstrates its potential for optimizing the hierarchical aerial computing platforms in dynamic IoT environments.},
  archive      = {J_TMC},
  author       = {Ahmadun Nabi and Sangman Moh},
  doi          = {10.1109/TMC.2025.3548668},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7267-7282},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint offloading decision, user association, and resource allocation in hierarchical aerial computing: Collaboration of UAVs and HAP},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An AI-assisted all-in-one integrated coronary artery disease diagnosis system using a portable heart sound sensor with an on-board executable lightweight model. <em>TMC</em>, <em>24</em>(8), 7252-7266. (<a href='https://doi.org/10.1109/TMC.2025.3547842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart sounds play a crucial role in assessing Coronary Artery Disease (CAD). The advancement of Artificial Intelligence (AI) technologies has given rise to Computer Audition (CA)-based methods for CAD detection. However, previous research has focused primarily on analyzing and modeling heart sound data, overlooking practical application scenarios. In this work, we design a pervasive heart sound collection device used for high-quality heart sound data acquisition. Moreover, we introduce an on-board executable lightweight network tailored for the designed portable device, referred to as TYKDModel. Further, heart sound data from 41 CAD patients and 22 non-CAD healthy controls are collected using the developed device. Experimental results show that the TYKDModel exhibits low-computational complexity, with 52.16 K parameters and 5.03 M Floating-Point Operations (FLOPs). When deployed on the board, it requires only 1.10 MB of Random Access Memory (RAM) and 236.27 KB of Read-Only Memory (ROM), and takes around 1.72 seconds to perform a classification. Despite the low computational and spatial complexity, the TYKDModel achieves a notable classification accuracy of 85.2%, specificity of 88.6%, and sensitivity of 82.8% on the board. These results indicate the promising potential of AI-assisted all-in-one integrated system for the diagnosis of heart sound-assisted CAD.},
  archive      = {J_TMC},
  author       = {Haojie Zhang and Fuze Tian and Yang Tan and Lin Shen and Jingyu Liu and Jie Liu and Kun Qian and Yalei Han and Gong Su and Bin Hu and Björn W. Schuller and Yoshiharu Yamamoto},
  doi          = {10.1109/TMC.2025.3547842},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7252-7266},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An AI-assisted all-in-one integrated coronary artery disease diagnosis system using a portable heart sound sensor with an on-board executable lightweight model},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockage-resilient integrated sensing and communication in mmWave networks: Multi-view collaboration and efficient task allocation. <em>TMC</em>, <em>24</em>(8), 7237-7251. (<a href='https://doi.org/10.1109/TMC.2025.3551099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrated sensing and communication (ISAC) has emerged as a promising technology for future millimeter wave (mmWave) networks. However, the susceptibility of mmWave signals to blockages poses considerable challenges for ISAC as it can result in unreliable links and disrupted sensing. As a result, this paper investigates the blockage-resilient ISAC design that leverages the robustness offered by multi-base station (BS) collaboration. Given the dynamic blockages and the fluctuation in the targets’ radar cross section (RCS), the blockage-resilient multi-BS collaborative ISAC design is cast as a chance constrained integer programming (CCIP) by jointly considering the diverse deadlines of different sensing tasks and the spatial/temporal user-target pairing for dual-functional radar and communication (DFRC) waveform scheduling. To facilitate efficient solution finding, we develop a group concatenating assisted reinforcement learning (GCRL) algorithm, where we linearize the chance constraints via variable grouping and concatenation, enabling the RL agent to understand the problem structure with bipartite graphs so as to develop an efficient branching policy. Extensive experiments demonstrate the resilience of the obtained ISAC scheme to dynamic blockages.},
  archive      = {J_TMC},
  author       = {Yue Cui and Haichuan Ding and Ying Ma and Xuanheng Li and Haixia Zhang and Yuguang Fang},
  doi          = {10.1109/TMC.2025.3551099},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7237-7251},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockage-resilient integrated sensing and communication in mmWave networks: Multi-view collaboration and efficient task allocation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent moth-flame reinforcement learning based broadcast beam optimization. <em>TMC</em>, <em>24</em>(8), 7223-7236. (<a href='https://doi.org/10.1109/TMC.2025.3547946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, beamforming antenna array technologies are of utmost importance in 5G communication systems. These technologies are essential for optimizing the coverage and signal quality of the cellular network. However, the optimization of broadcast beams presents significant challenges due to the complex strategy profile space. Each beam can be configured with different widths and heights, making it difficult for conventional algorithms to handle. To address this issue, we propose a novel approach called Multi-Agent Moth-Flame Reinforcement Learning (MAMF-RL) algorithm for broadcast beam optimization. MAMF-RL combines reinforcement learning and moth-flame optimization algorithms to interactively search for the optimal broadcast beams. By decomposing the problem into multiple single-sector antenna configuration problems, MAMF-RL effectively reduces the algorithm complexity. We conducted experiments utilizing real data in an 18-sector wireless coverage area. To evaluate the performance of our proposed method, we compared it with traditional methods such as the particle swarm algorithm. The results demonstrate that our MAMF-RL model achieves an average coverage rate of 1.82% higher and a 13.74% lower overlapping coverage rate compared to traditional methods.},
  archive      = {J_TMC},
  author       = {Shan Huang and Haipeng Yao and Tianle Mai and Di Wu and Jiaqi Xu and F. Richard Yu},
  doi          = {10.1109/TMC.2025.3547946},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7223-7236},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-agent moth-flame reinforcement learning based broadcast beam optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentive mechanism design for federated learning with dynamic network pricing. <em>TMC</em>, <em>24</em>(8), 7206-7222. (<a href='https://doi.org/10.1109/TMC.2025.3546977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning protects users’ data privacy by sharing users’ local model parameters (instead of raw data) with a server. However, when massive users train a large machine learning model through federated learning, the dynamically varying and often heavy communication overhead can put significant pressure on the network operator. The operator may choose to dynamically change the network prices in response, which will eventually affect the payoffs of the server and users. This paper considers the under-explored yet important issue of the joint design of participation incentives (for encouraging users’ contribution to federated learning) and network pricing (for managing network resources). Due to heterogeneous users’ private information and multi-dimensional decisions, the optimization problems in Stage I of multi-stage games are non-convex. Nevertheless, we are able to analytically derive the corresponding optimal contract and pricing mechanism through proper transformations of constraints, variables, and functions, under three interaction structures of the participants. We show that the coordinated structure is better than the two uncoordinated structures, as it avoids the selfish behaviors of the network operator and the server; the vertically uncoordinated structure is better than the horizontally uncoordinated structure, as it avoids the interests misalignment between the server and the network operator. We also propose multi-period network pricing to reduce the implementation complexity of dynamic pricing. Numerical results based on real-world datasets show that our proposed mechanisms decrease the server's cost by up to 24.87% and increase the network operator's profit by up to 1245.25%, compared with the state-of-the-art benchmarks.},
  archive      = {J_TMC},
  author       = {Ningning Ding and Lin Gao and Jianwei Huang},
  doi          = {10.1109/TMC.2025.3546977},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7206-7222},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Incentive mechanism design for federated learning with dynamic network pricing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Budget-feasible diffusion mechanisms for mobile crowdsourcing in social networks. <em>TMC</em>, <em>24</em>(8), 7189-7205. (<a href='https://doi.org/10.1109/TMC.2025.3549751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowdsourcing has emerged as a popular approach for organizations to leverage the collective intelligence of a crowd of users to obtain services. Considering users’ costs for providing services, it is vital for the requester to design incentive mechanisms to encourage users’ participation in crowdsourcing under the budget constraint. This aligns with the concept of budget-feasible mechanism design. Existing budget-feasible mechanisms often assume immediate user reachability and willingness of joining the crowdsourcing, which is unrealistic. To address this issue, a promising approach is to have participating users diffuse auction information to potential users in the social network. However, this brings another challenge in that participating users can be strategic and therefore hesitant to invite more potential competitors to join the crowdsourcing platform. In this paper, we focus on developing diffusion mechanisms that incentivize strategic users to actively diffuse auction information through the social network. This helps to attract more informed users and ultimately increases the value of the procured services. Specifically, we propose optimal budget-feasible diffusion mechanisms that simultaneously guarantee individual rationality, budget-feasibility, strong budget-balance, incentive-compatibility (i.e., users report real costs and diffuse auction information to all their neighbors) and approximation. Experiment results under real datasets further demonstrate the efficiency of proposed mechanisms.},
  archive      = {J_TMC},
  author       = {Xiang Liu and Weiwei Wu and Minming Li and Wanyuan Wang and Yifan Qin and Yingchao Zhao and Junzhou Luo},
  doi          = {10.1109/TMC.2025.3549751},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7189-7205},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Budget-feasible diffusion mechanisms for mobile crowdsourcing in social networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards privacy-enhanced and robust clustered federated learning. <em>TMC</em>, <em>24</em>(8), 7171-7188. (<a href='https://doi.org/10.1109/TMC.2025.3547149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustered federated learning (CFL) leverages data distribution similarities to cluster clients, facilitating personalized model training under data heterogeneity. However, most existing CFL schemes pose potential privacy risks for clients (e.g., gradient inversion attacks) as they rely on individual gradients for clustering. This also renders them incompatible with secure aggregation mechanisms that are widely employed in federated learning for privacy protection. Moreover, CFL introduces the risk of malicious clients dominating several clusters and conducting poisoning attacks therein, thereby threatening secure model training. To address these issues, we propose ProCFL, a Privacy-Enhanced and Robust CFL framework incorporating gradient-free clustering and peer validation. Specifically, we first design a new protocol for measuring data distribution similarity among clients without using their gradient information. Then, we transform the client clustering process into a weighted set covering problem and introduce a diversity-optimized clustering algorithm to achieve near-optimal clustering results while eliminating any need for prior knowledge. Furthermore, we develop a post-hoc detection mechanism that employs peer validation to identify and discard malicious client models. Extensive experimental evaluation of ProCFL validates its superior model robustness and accuracy performance compared to existing schemes.},
  archive      = {J_TMC},
  author       = {Yang Xu and Yunlin Tan and Cheng Zhang and Peng Sun and Yibang Zhang and Ju Ren and Hongbo Jiang and Yaoxue Zhang},
  doi          = {10.1109/TMC.2025.3547149},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7171-7188},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards privacy-enhanced and robust clustered federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trust online over-the-air computation for wireless federated learning. <em>TMC</em>, <em>24</em>(8), 7152-7170. (<a href='https://doi.org/10.1109/TMC.2025.3547148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using the wireless waveform superposition property, over-the-air computation (OAC) enables federated learning (FL) to achieve fast model aggregation. However, this computing paradigm is vulnerable to poisoning attacks due to the openness of a wireless channel over time, where malicious mobile devices can introduce cumulative errors for the global FL model in a time-varying wireless environment for each communication round. This article presents a trust online OAC (TO-OAC) scheme to minimize impacts on the global model introduced by malicious devices adjusting to dynamic attack and wireless channel fluctuations over time. TO-OAC achieves this by utilizing trustworthy security quantification of OAC for each FL training round. To optimize the cumulative training loss at the aggregation node with the long-term power and trust constraints of mobile devices, we propose a joint trust, power, and channel-aware algorithm to flexibly update local and global models in response to the dynamic changes in the wireless and secure environment. We analyze the performance limits for the aggregation of trust models, considering metrics for computation and communication through time. We then propose another trust online regularization over-the-air computation (TOR-OAC) as an improved version of the TO-OAC scheme to decrease convergence time while ensuring long-term trust and power limitation. Experimental results performed on real-life datasets show that the two proposed schemes (TO-OAC and TOR-OAC) outperform prior works, especially in noisy, time-varying wireless channels and malicious attacks.},
  archive      = {J_TMC},
  author       = {Mingjie Sun and Jie Zheng and Hongyang Du and Haijun Zhang and Dusit Niyato and Jiawen Kang and Jiacheng Wang and Jie Ren and Ling Gao and Zheng Wang},
  doi          = {10.1109/TMC.2025.3547148},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7152-7170},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Trust online over-the-air computation for wireless federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient, robust, and privacy-preserving incentives for crowdsensing via blockchain. <em>TMC</em>, <em>24</em>(8), 7136-7151. (<a href='https://doi.org/10.1109/TMC.2025.3546941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive development of mobile devices, mobile crowdsensing (MCS) has emerged as a promising approach for large-scale sensing data collection. In the research of MCS, blockchain technology has been widely adopted to decentralize the traditional mobile crowdsensing and tackle the problem of single point of failure. Incentive mechanisms are devised to boost participation with fairness and truthfulness. However, to better determine the incentive strategy, participants’ privacy can be disclosed on top of the blockchain and obtained by adversaries during the transmission and execution of user data, leading to serious security issues. In this paper, we propose a two-stage incentive scheme with efficiency, robustness and privacy preservation considered based on the combination of blockchain technology and Trusted Execution Environment (TEE). Detailedly, we design two kinds of smart contracts, where on-chain public contracts support the procedure of general crowdsensing interactions, and off-chain private ones enabled by TEE complete the privacy-preserving computations, including an online incentive mechanism for worker recruitment decisions and a truth discovery algorithm for data aggregation. Recovery mechanism and hash check mechanism are introduced to avoid TEE provider failures and TEE providers’ attacks, respectively. Our scheme is proved to be theoretically secure in terms of private information protection, worker participation anonymity, and data aggregation privacy. Experimental results also verify the feasibility and superiority of our incentive scheme.},
  archive      = {J_TMC},
  author       = {Yuanhang Zhou and Fei Tong and Chunming Kong and Shibo He and Guang Cheng},
  doi          = {10.1109/TMC.2025.3546941},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7136-7151},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards efficient, robust, and privacy-preserving incentives for crowdsensing via blockchain},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy preservation strategies for malware-infected edge intelligence systems: A bayesian stochastic game-based approach. <em>TMC</em>, <em>24</em>(8), 7121-7135. (<a href='https://doi.org/10.1109/TMC.2025.3546910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware in the Internet of Things (IoT) is prone to contaminating various IoT end-points through network communication and information transfer, leading to surreptitious privacy leakage and data theft. The existing privacy-preserving approaches including data masking, anonymization, and differential privacy always lack the consideration of strategic interactions among rational agents. Inspired by Bayesian games, we model incomplete stochastic games between IoT end-points and edge nodes in edge intelligence (EI)-enabled IoT systems to conduct probability analysis for predicting and defending privacy leakage caused by malware infection. It is notable that the posterior probability is defined based on the Bayes’ rule to reflect the statistical inference of incomplete privacy leakage information. Such a method can intrinsically characterize the actual situations of IoT end-points. Further, we propose a novel privacy preservation optimization approach named Bayesian advantage actor critic (BA2C) for the practical implementation of optimization decision in EI-enabled IoT privacy-preserving systems. Eventually, we conduct experimental simulations to understand the most effective parameters in decision-making among the successful detection rate, successful infection rate, and false alarm rate. We also compare traditional algorithms and validate the efficacy of the proposed approach.},
  archive      = {J_TMC},
  author       = {Yizhou Shen and Carlton Shepherd and Chuadhry Mujeeb Ahmed and Shigen Shen and Shui Yu},
  doi          = {10.1109/TMC.2025.3546910},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7121-7135},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy preservation strategies for malware-infected edge intelligence systems: A bayesian stochastic game-based approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensing metal coil vibration of headsets for eavesdropping on online conversations with out-of-vocabulary words using RFID. <em>TMC</em>, <em>24</em>(8), 7107-7120. (<a href='https://doi.org/10.1109/TMC.2025.3548980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most essential accessories, headsets have been widely used in common online conversations. The metal coil vibration patterns of headset speakers/microphones have been proven to be highly correlated with the speaker-produced/microphone-received sound. This paper presents an online conversation eavesdropping system, RFSpy, which uses only one RFID tag attached on a headset to alternately sense metal coil vibrations of headset speaker and microphone for eavesdropping on speaker-produced and microphone-received sound. In some accessible scenarios, assuming attackers secretly attach a small, battery-free RFID tag under one ear cushion of an eavesdropped user’s headset without being noticed. Meanwhile, RFID readers are camouflaged as decorations placed in/out of rooms to transmit and receive RF signals. When the eavesdropped user talks with other users online through the headset, RFSpy first activates the RFID tag to capture the metal coil vibration patterns of headset speaker and microphone upon RF signals. Then, RFSpy reconstructs sound spectrograms from the RF signal-based vibration patterns for not only trained words but also untrained (i.e., out-of-vocabulary) words utilizing designed SSR network. Finally, RFSpy converts the sound spectrograms to conversation content through sound recognition API. Extensive experiments demonstrate that RFSpy can eavesdrop on online conversations with out-of-vocabulary words effectively.},
  archive      = {J_TMC},
  author       = {Yunzhong Chen and Jiadi Yu and Yingying Chen and Linghe Kong and Yanmin Zhu and Yichao Chen},
  doi          = {10.1109/TMC.2025.3548980},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7107-7120},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sensing metal coil vibration of headsets for eavesdropping on online conversations with out-of-vocabulary words using RFID},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization for IRS-assisted self-powered IoT in 5G mmWave networks. <em>TMC</em>, <em>24</em>(8), 7092-7106. (<a href='https://doi.org/10.1109/TMC.2025.3547790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harvesting energy from ambient energy source is the key technology for self-powered Internet of Things (IoT) devices to maintain continuous operation without an external power supply. Motivated by the expansion and popularity of 5G networks, we propose a novel solution for IoT devices which are self-powered via harvesting energy from the millimeter-wave (mmWave) communications in 5G mmWave networks. For overcoming the high path loss in mmWave communications, directional narrow-beam transmission is adopted to provide sufficient link budget between transceivers through beamforming technology, which however makes IoT devices difficult to scavenge energy from the mmWave signals. Hence, we employ multiple intelligent reflecting surfaces (IRSs) to assist in energy harvesting at the IoT devices and data transmission at the 5G users. Considering beam codebook design for 5G mmWave networks, this paper jointly optimizes the Discrete Fourier transform (DFT) codebook-based transmit codevectors at the 5G base station (BS) and the phase shifts of IRS's reflective elements for minimizing BS's transmit power, while satisfying the Signal to Interference plus Noise Ratio (SINR) constraints at users and energy harvesting constraints of IoT devices. Nevertheless, owing to the intricate coupling of variables and discrete constraints, this joint optimization problem is extremely non-convex and non-linear. To address such challenges, we propose a penalty dual-decomposition (PDD)-based algorithm which combines the penalty-based augmented Lagrangian method and block coordinate descent method. It explores the structure of the mmWave channel and performs a double iterations in which the joint optimization problem is decomposed into several simplified subproblems. Simulation results reveal that the above algorithm enhances the energy efficiency as compared to other algorithms.},
  archive      = {J_TMC},
  author       = {Tao Liu and Suiwen Zhang and Xiaomei Qu and Lijun Yang and Chengjie Li and Yihong Chen},
  doi          = {10.1109/TMC.2025.3547790},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7092-7106},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization for IRS-assisted self-powered IoT in 5G mmWave networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wireless charging for uncertain location nodes. <em>TMC</em>, <em>24</em>(8), 7074-7091. (<a href='https://doi.org/10.1109/TMC.2025.3546316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from Wireless Power Transfer (WPT) technology, Wireless Rechargeable Sensor Networks (WRSNs) effectively address the lifetime bottleneck of sensor nodes, enabling them to work perpetually. Most state-of-the-art studies assume that all WRSNs’ information is known or precise in advance. However, sensor nodes may be deployed randomly in a large-scale area, and some critical information (such as node location) may be unavailable or difficult to obtain precisely. In this work, we eliminate the effect of uncertain or imprecise node location and formalize the Maximizing Charging Energy utility for uncertain location nodes problem (i.e., MCE problem). With magnetic resonance coupling and beamforming technologies, we propose a novel node localization method to determine precise node location information. In addition, we present a reinforcement learning framework and a charging path scheduling method to maximize charging energy. To validate the effectiveness of our proposed scheme in real-world scenarios, we conduct test-bed experiments. The results demonstrate that our approach significantly improves charging efficiency by an average of 20.9% in a large-scale network, even when the locations of sensors are entirely unknown.},
  archive      = {J_TMC},
  author       = {Chi Lin and Qiwei Wang and Hengyi Li and Shibo Hao and Yi Wang and Lei Wang and Xin Fan and Guowei Wu},
  doi          = {10.1109/TMC.2025.3546316},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7074-7091},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Wireless charging for uncertain location nodes},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeMoE: Empowering sparse large language models on mobile devices. <em>TMC</em>, <em>24</em>(8), 7059-7073. (<a href='https://doi.org/10.1109/TMC.2025.3546466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) such as GPTs and Mixtral-8x7B have revolutionized machine intelligence due to their exceptional abilities in generic ML tasks. Transiting LLMs from datacenters to edge devices brings benefits like better privacy and availability, but is challenged by their massive parameter size and thus unbearable runtime costs. To this end, we present EdgeMoE, an on-device inference engine for mixture-of-expert (MoE) LLMs – a popular form of sparse LLM that scales its parameter size with almost constant computing complexity. EdgeMoE achieves both memory- and compute-efficiency by partitioning the model into the storage hierarchy: non-expert weights are held in device memory; while expert weights are held on external storage and fetched to memory only when activated. This design is motivated by a key observation that expert weights are bulky but infrequently used due to sparse activation. To further reduce the expert I/O swapping overhead, EdgeMoE incorporates two novel techniques: (1) expert-wise bitwidth adaptation that reduces the expert sizes with tolerable accuracy loss; (2) expert preloading that predicts the activated experts ahead of time and preloads it with the compute-I/O pipeline. On popular MoE LLMs and edge devices, EdgeMoE showcase significant memory savings and speedup over competitive baselines.},
  archive      = {J_TMC},
  author       = {Rongjie Yi and Liwei Guo and Shiyun Wei and Ao Zhou and Shangguang Wang and Mengwei Xu},
  doi          = {10.1109/TMC.2025.3546466},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7059-7073},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EdgeMoE: Empowering sparse large language models on mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMIMO: Universal unsupervised learning for mmwave radar sensing with MIMO array synthesis. <em>TMC</em>, <em>24</em>(8), 7042-7058. (<a href='https://doi.org/10.1109/TMC.2025.3546757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter-wave (mmWave) radar sensing powered by deep learning is now emerging in numerous applications, which are predominantly trained in a supervised manner. However, due to the non-interpretable nature of mmWave signals, labeling the radar data has always been a difficult task. While there have been investigations on unsupervised pre-training for mmWave radar sensing, these methods are tailored to specific signal representations. In this paper, we propose UMIMO, an unsupervised learning framework combining the hardware nature of MIMO radar and deep learning techniques to resolve the challenge raised by the insufficient labeled data. UMIMO leverages the antenna arrays synthesized from multiple transmitting and receiving antennas in mmWave radar to construct positive samples for contrastive learning. To achieve this, we propose the constraints on angular resolution and grating lobes to generate effective signal representations with different synthetic arrays. We conduct experiments using UMIMO on three tasks: contactless ECG monitoring, 3D human pose estimation, and human silhouette generation. All experimental results demonstrate that UMIMO can effectively improve the performance of learning-based mmWave radar sensing in an unsupervised manner.},
  archive      = {J_TMC},
  author       = {Haoyu Zhang and Dongheng Zhang and Ruiyuan Song and Zhi Wu and Jinbo Chen and Liang Fang and Zhi Lu and Yang Hu and Hui Lin and Yan Chen},
  doi          = {10.1109/TMC.2025.3546757},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7042-7058},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UMIMO: Universal unsupervised learning for mmwave radar sensing with MIMO array synthesis},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FuseGrasp: Radar-camera fusion for robotic grasping of transparent objects. <em>TMC</em>, <em>24</em>(8), 7028-7041. (<a href='https://doi.org/10.1109/TMC.2025.3547371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transparent objects are prevalent in everyday environments, but their distinct physical properties pose significant challenges for camera-guided robotic arms. Current research is mainly dependent on camera-only approaches, which often falter in suboptimal conditions, such as low-light environments. In response to this challenge, we present FuseGrasp, the first radar-camera fusion system tailored to enhance the transparent objects manipulation. FuseGrasp exploits the weak penetrating property of millimeter-wave (mmWave) signals, which causes transparent materials to appear opaque, and combines it with the precise motion control of a robotic arm to acquire high-quality mmWave radar images of transparent objects. The system employs a carefully designed deep neural network to fuse radar and camera imagery, thereby improving depth completion and elevating the success rate of object grasping. Nevertheless, training FuseGrasp effectively is non-trivial, due to limited radar image datasets for transparent objects. We address this issue utilizing large RGB-D dataset, and propose an effective two-stage training approach: we first pre-train FuseGrasp on a large public RGB-D dataset of transparent objects, then fine-tune it on a self-built small RGB-D-Radar dataset. Furthermore, as a byproduct, FuseGrasp can determine the composition of transparent objects, such as glass or plastic, leveraging the material identification capability of mmWave radar. This identification result facilitates the robotic arm in modulating its grip force appropriately. Extensive testing reveals that FuseGrasp significantly improves the accuracy of depth reconstruction and material identification for transparent objects. Moreover, real-world robotic trials have confirmed that FuseGrasp markedly enhances the handling of transparent items.},
  archive      = {J_TMC},
  author       = {Hongyu Deng and Tianfan Xue and He Chen},
  doi          = {10.1109/TMC.2025.3547371},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7028-7041},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FuseGrasp: Radar-camera fusion for robotic grasping of transparent objects},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi–Task model personalization for federated supervised SVM in heterogeneous networks. <em>TMC</em>, <em>24</em>(8), 7012-7027. (<a href='https://doi.org/10.1109/TMC.2025.3546550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated systems enable collaborative training on highly heterogeneous, non-i.i.d. data through model personalization, which can be facilitated by employing multi-task learning. However, multi-task learning algorithms are often implemented using methods like stochastic gradient descent, which may suffer from slow convergence in a multi-task federated setting. To accelerate the training procedure, we design an efficient iterative distributed method based on the alternating direction method of multipliers (ADMM) for support vector machines (SVMs), which tackles federated classification and regression. The proposed method utilizes efficient computations and model exchange in a network of heterogeneous nodes and allows personalization of the learning model in the presence of non-i.i.d. data. To ensure data privacy, we introduce a randomization algorithm that helps avoid data inversion. Finally, we analyze the impact of the proposed privacy mechanisms and participant hardware and data heterogeneity on the system performance. Our experiments confirm the advantages of the proposed ADMM-based personalized federated multi-task learning.},
  archive      = {J_TMC},
  author       = {Aleksei Ponomarenko-Timofeev and Olga Galinina and Ravikumar Balakrishnan and Nageen Himayat and Sergey Andreev and Yevgeni Koucheryavy},
  doi          = {10.1109/TMC.2025.3546550},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {7012-7027},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi–Task model personalization for federated supervised SVM in heterogeneous networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of offloading and caching in full-duplex-enabled edge computing networks. <em>TMC</em>, <em>24</em>(8), 6996-7011. (<a href='https://doi.org/10.1109/TMC.2025.3546263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing (EC) reduces task processing and content download delay by providing computation and caching resources directly to task offloading (TO) users and content request (CR) users. However, existing studies often focus exclusively on either TO users or CR users within EC networks, neglecting the interaction between these two groups. To address this gap, we investigate the offloading and caching decision-making in scenarios where TO and CR users coexist. Furthermore, we employ full-duplex (FD) technology to enhance spectral utilization for edge-end transmissions. Specifically, we jointly optimize offloading and caching in FD-enabled EC networks. To accomplish this, we decompose the formulated optimization problem into three sub-problems using the alternating optimization (AO) method. We then propose a three-subproblem alternating iterative delay minimization algorithm to effectively tackle the challenges of offloading and caching. Additionally, we analyze the convergence and complexity of our proposed algorithm. Finally, we conduct extensive simulations to evaluate the effectiveness of our approach. The simulation results demonstrate that the delay reduction achieved by our algorithm is between 24.78% and 89.23% greater than that of comparative algorithms.},
  archive      = {J_TMC},
  author       = {Xingxia Dai and Shujuan Tian and Haolin Liu and Zhetao Li and Hongbo Jiang and Qingyong Deng},
  doi          = {10.1109/TMC.2025.3546263},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6996-7011},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of offloading and caching in full-duplex-enabled edge computing networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multimodal scale normalization framework for vision-radar small UAV positioning. <em>TMC</em>, <em>24</em>(8), 6978-6995. (<a href='https://doi.org/10.1109/TMC.2025.3549620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed aerial vehicles (UAVs) positioning is of crucial importance in diverse applications. However, it is extremely challenging to realize the precise UAVs positioning over long distances due to the small size and dramatic scale variations associated with the high mobility in the wide area. To tackle this issue, a multimodal scale normalization framework is proposed for the scale-robust precise pixel-level UAV positioning. The framework exploits our proposed distance-aware image slicing and distance-aware scale normalization module. Moreover, a modal fusion-based scale normalization network is proposed that can accept arbitrary low-resolution UAV patches and produce the consistent high-resolution images at a uniform UAV instance scale with a single learnable model. The proposed framework is generic and can be directly used in the existing pixel-level positioning pipelines to improve the positioning performance and scale robustness. To verify the proposed framework in the real application, a practical vision-radar UAV positioning system is developed. Experimental results on the real-world dataset demonstrate the generality and effectiveness of our framework. Moreover, the ablation experiments also confirm the contribution of each module in the framework.},
  archive      = {J_TMC},
  author       = {Yiyao Wan and Jiahuan Ji and Wenqing Xie and Guangyu Wu and Fuhui Zhou and Qihui Wu},
  doi          = {10.1109/TMC.2025.3549620},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6978-6995},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A multimodal scale normalization framework for vision-radar small UAV positioning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compromising rechargeable sensor networks in marine environment. <em>TMC</em>, <em>24</em>(8), 6961-6977. (<a href='https://doi.org/10.1109/TMC.2025.3546276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marine Wireless Rechargeable Sensor Networks (MWRSNs), enhanced by recent Wireless Power Transfer (WPT) technology, present a significant advancement in extending network life. Traditional methods improve network performance through algorithm optimization, but neglect charging security, exposing networks to potential attacks. This paper addresses this problem from an adversarial view and develops a novel attack for MWRSN through Denying of Charge (DoC) to maximize network destructiveness. We start by establishing a generalized on-demand charging model, essential for developing DoC tactics. Subsequently, we unveil the Collaborative DoC (CoDoC) algorithm, capable of manipulating and falsifying charging requests. Central to CoDoC is the Request Prediction Method (RPM), which forecasts the initiation of charging requests and facilitates rapid request surges to enhance the attack's efficacy. CoDoC is able to disguise the presence of the attack, which is able to escape from being detected by the base station. Theoretical analyses are provided to explore the features of the proposed scheme. To demonstrate the outperformed features of the proposed schemes, extensive simulations and test-bed experiments are conducted. Our analysis and extensive simulations demonstrate that CoDoC increases sensor node failures by 20% to 142% compared to traditional methods, highlighting its effectiveness in marine environments.},
  archive      = {J_TMC},
  author       = {Qiwei Wang and Chi Lin and Haipeng Dai and Mohammad S. Obaidat and Kuei-Fang Hsiao and Xin Fan},
  doi          = {10.1109/TMC.2025.3546276},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6961-6977},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Compromising rechargeable sensor networks in marine environment},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALANINE: A novel decentralized personalized federated learning for heterogeneous LEO satellite constellation. <em>TMC</em>, <em>24</em>(8), 6945-6960. (<a href='https://doi.org/10.1109/TMC.2025.3545429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) satellite constellations have seen significant growth and functional enhancement in recent years, which integrates various capabilities like communication, navigation, and remote sensing. However, the heterogeneity of data collected by different satellites and the problems of efficient inter-satellite collaborative computation pose significant obstacles to realizing the potential of these constellations. Existing approaches struggle with data heterogeneity, varing image resolutions, and the need for efficient on-orbit model training. To address these challenges, we propose a novel decentralized PFL framework, namely, A Novel DecentraLized PersonAlized Federated Learning for HeterogeNeous LEO SatellIte CoNstEllation (ALANINE). ALANINE incorporates decentralized FL (DFL) for satellite image Super Resolution (SR), which enhances input data quality. Then it utilizes PFL to implement a personalized approach that accounts for unique characteristics of satellite data. In addition, the framework employs advanced model pruning to optimize model complexity and transmission efficiency. The framework enables efficient data acquisition and processing while improving the accuracy of PFL image processing models. Simulation results demonstrate that ALANINE exhibits superior performance in on-orbit training of SR and PFL image processing models compared to traditional centralized approaches. This novel method shows significant improvements in data acquisition efficiency, process accuracy, and model adaptability to local satellite conditions.},
  archive      = {J_TMC},
  author       = {Liang Zhao and Shenglin Geng and Xiongyan Tang and Ammar Hawbani and Yunhe Sun and Lexi Xu and Daniele Tarchi},
  doi          = {10.1109/TMC.2025.3545429},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6945-6960},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ALANINE: A novel decentralized personalized federated learning for heterogeneous LEO satellite constellation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DT-RSSI: Digital twin-replica of sensing statistics for IRA in intelligent NG-HetNetIs. <em>TMC</em>, <em>24</em>(8), 6934-6944. (<a href='https://doi.org/10.1109/TMC.2025.3544918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent resource allocation maintains a better quality of service among devices in next-generation heterogeneous network infrastructures (NG-HetNetIs). NG-HetNetIs include industry 5.0 enabled infrastructures like Internet of Things (IoT), cognitive radio (CR) enabled B5G and 6G networks, unmanned aerial vehicles (UAVs), wireless sensor networks (WSNs) and autonomous vehicles (AVs). Digital twin (DT) joins hand with cognitive radio and resource aggregation technologies to provide the integrated framework for intelligent resource allocation in NG-HetNetIs. In NG-HetNetIs, the obtained statistics of measured radio activity as prior information play an instrumental role in enabling optimized resource allocation using context awareness. Unfortunately, the already available static approaches are inefficient to replicate (DT) the radio activity in a heterogeneous radio environment. To address the issue, static implementation framework is extended as dynamic radio activity characterization framework (DRAC) to have context awareness in NG-HetNetIs. The proposed DRAC replicates (DT) the wide sense stationarity of time and carrier aggregated radio activity due to its exploitation of more localized temporal and spectral information in NG-HetNets. The obtained localized statistics using DRAC can be exploited as appropriate prior knowledge and test statistics during the spectrum sensing phase of NG-HetNetIs for intelligent resource allocation instead of a single statistic obtained by the static approach.},
  archive      = {J_TMC},
  author       = {Muhammad Khurram Ehsan and Neelma Naz and Ali Hassan Sodhro and Shahid Mumtaz and Asad Mahmood},
  doi          = {10.1109/TMC.2025.3544918},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6934-6944},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DT-RSSI: Digital twin-replica of sensing statistics for IRA in intelligent NG-HetNetIs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time abnormal address detection for mobile devices in location-based services. <em>TMC</em>, <em>24</em>(8), 6918-6933. (<a href='https://doi.org/10.1109/TMC.2025.3545875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An address, a textual description of a geographical location, plays an important role in location-based services such as instant delivery. However, abnormal addresses (i.e., an address without detailed or accurate information) have led to significant costs. In real-world settings, abnormal address detection is not trivial because it needs to be completed in real-time to support massive online queries from mobile devices. In this study, we design FastAddr, a fast abnormal address detection framework, which detects abnormal addresses in real time. FastAddr consists of a novel contrastive address augmentation module and a lightweight multi-head attention model. We further design FastAddr+ to enhance FastAddr by utilizing large-scale spatial entities. A comprehensive three-phase evaluation is conducted. (i) We evaluate FastAddr on a real-world dataset and it yields the average F1 of 85.7% in 0.058 milliseconds, which outperforms the state-of-the-art models by 47.4% with a similar detection time. (ii) An offline A/B test shows that FastAddr outperforms the previous model significantly. (iii) We also conduct an online A/B test to compare FastAddr with the deployed model, which shows an improvement of F1 by more than 20%. Moreover, we conduct two case studies on real industry data, demonstrating both the efficiency and effectiveness of FastAddr.},
  archive      = {J_TMC},
  author       = {Zhiqing Hong and Heng Yang and Haotian Wang and Wenjun Lyu and Yu Yang and Guang Wang and Yunhuai Liu and Yang Wang and Desheng Zhang},
  doi          = {10.1109/TMC.2025.3545875},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6918-6933},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time abnormal address detection for mobile devices in location-based services},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Orchestrating joint offloading and scheduling for low-latency edge SLAM. <em>TMC</em>, <em>24</em>(8), 6901-6917. (<a href='https://doi.org/10.1109/TMC.2025.3547256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Simultaneous Localization and Mapping (vSLAM) is a prevailing technology for many emerging robotic applications. Achieving real-time SLAM on mobile robotic systems with limited computational resources is challenging because the complexity of SLAM algorithms increases over time. This restriction can be lifted by offloading computations to edge servers, forming the emerging paradigm of edge-assisted SLAM. Nevertheless, the exogenous and stochastic input processes affect the dynamics of the edge-assisted SLAM system. Moreover, the requirements of clients on SLAM metrics change over time, exerting implicit and time-varying effects on the system. In this paper, we aim to push the limit beyond existing edge-assist SLAM by proposing a new architecture that can handle the input-driven processes and also satisfy clients’ implicit and time-varying requirements. The key innovations of our work involve a regional feature prediction method for importance-aware local data processing, a configuration adaptation policy that integrates data compression/decompression and task offloading, and an input-dependent learning framework for task scheduling with constraint satisfaction. Extensive experiments prove that our architecture improves pose estimation accuracy and saves up to 47% of communication costs compared with a popular edge-assisted SLAM system, as well as effectively satisfies the clients’ requirements.},
  archive      = {J_TMC},
  author       = {Yao Zhang and Yuyi Mao and Hui Wang and Zhiwen Yu and Song Guo and Jun Zhang and Liang Wang and Bin Guo},
  doi          = {10.1109/TMC.2025.3547256},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6901-6917},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Orchestrating joint offloading and scheduling for low-latency edge SLAM},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIPair: Exploiting magnetic induction for laptop-phone pairing. <em>TMC</em>, <em>24</em>(8), 6886-6900. (<a href='https://doi.org/10.1109/TMC.2025.3546758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferring important files, photos, and other sensitive data between laptops and smartphones has become a routine necessity in daily life. Device pairing acts as the most fundamental need to secure the communication channel between two unconnected devices. Traditional pairing methods leveraging PINs or QR codes require tedious human efforts in the pairing procedures to establish a shared communication key. Nevertheless, these designs are vulnerable to shoulder-surfing attacks in which attackers might record and replay the pairing credentials. It is preferred to have more intuitive pairing designs that minimize users’ overhead in the pairing process while providing secure keys for communication purposes. In this paper, we propose MIPair for laptop-phone pairing by leveraging magnetic induction (MI) signals. MIPair is based on a key observation that changes in the CPU workload of a device cause variations in internal current, thereby inducing changes in surrounding magnetic fields. Moreover, the trends of MI signal variations are highly correlated with CPU workload trends. Thus, users simply need to place a smartphone on the keyboard of a laptop. By randomly altering the workload of the laptop through a stimulation program, the smartphone can capture MI signals with similar changing trends, thereby converting them into similar bit sequences that form the basis of a symmetric key. We propose essential techniques to overcome challenges such as time asynchronization between two devices, unfixed state transition time in MI signal, and shared key distribution from the two similar bit sequences. Our real-world experiments demonstrate reliable pairing as well as robustness against common attacks and high randomness of the generated keys. When generating a 128-bit key, MIPair achieves a successful pairing rate as high as 98% and a usable pairing time of 8.35 seconds.},
  archive      = {J_TMC},
  author       = {Yu Liu and Chenyu Huang and Kaiyi Wang and Zheng Qin and Wenqiang Jin},
  doi          = {10.1109/TMC.2025.3546758},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6886-6900},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MIPair: Exploiting magnetic induction for laptop-phone pairing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast UAV trajectory planning framework in RIS-assisted communication systems with accelerated learning via multithreading and federating. <em>TMC</em>, <em>24</em>(8), 6870-6885. (<a href='https://doi.org/10.1109/TMC.2025.3544903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable Intelligent Surface (RIS)-assisted uncrewed Aerial Vehicle (UAV) communications have been realized as essential to space-air-group system integration in the 6 G technology landscape. Trajectory planning plays a crucial role in RIS-assisted UAV communications to face the challenges of UAV’s limited power capacities and dynamic wireless channels. Existing solutions assume complete channel state information, focus on single-rotor UAVs, and rely heavily on time-consuming training processes for machine learning; thus, they lack applicability to deal with highly dynamic real-world scenarios. To fill these research gaps, we aim to characterize RIS-assisted UAV communications and design responsive and accurate UAV trajectory planning algorithms in this paper. We first develop a communication model with incomplete information and an energy consumption model for quadrotor UAVs. We then formulate UAV trajectory planning as an optimization problem to minimize UAV’s energy consumption while maintaining communication throughput. To solve this problem, we design an acceleration framework, FedX, for reinforcement learning (RL) solvers and present two fast trajectory planning algorithms, FedSAC and FedPPO, as instantiations of the FedX framework. Our evaluation results indicate that the proposed framework is effective and efficient–more than 3 times faster with 5 agents and 7 times faster with 10 agents than standard RL algorithms, making it suitable for using RL solvers within wireless networks and mobile computing environments. We also discuss and identify the pros and cons of our proposed framework.},
  archive      = {J_TMC},
  author       = {Jun Huang and Beining Wu and Qiang Duan and Liang Dong and Shui Yu},
  doi          = {10.1109/TMC.2025.3544903},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6870-6885},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A fast UAV trajectory planning framework in RIS-assisted communication systems with accelerated learning via multithreading and federating},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A traffic-aware graph neural network for user association in cellular networks. <em>TMC</em>, <em>24</em>(8), 6858-6869. (<a href='https://doi.org/10.1109/TMC.2025.3545464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we utilize a graph neural network to perform joint user association and access point activation/deactivation to optimize network blockage. Instead of using theoretical models to characterize the distribution of network traffic, we use a real-world network traffic dataset. In order to train the graph neural network, our method leverages reinforcement learning, specifically employing the deep deterministic policy gradient (DDPG) algorithm. This approach allows us to benefit from the advantages of both value-based and policy-based reinforcement learning methods. A simple but flexible reward function is defined to capture the trade-off between the fraction of active access points and network blockage. The graph neural network's awareness of the network topology gives the proposed method a clear performance advantage over the commonly used greedy heuristic optimization method, reducing blockage by up to more than 50% on real-world network traffic data while also reducing computational complexity from $\mathcal {O}(N^{3})$ to $\mathcal {O}(N)$. The proposed user association and access point activation method is not limited by the network architecture or technology, and can be applied to different generations of cellular networks.},
  archive      = {J_TMC},
  author       = {Saeed Jamshidiha and Vahid Pourahmadi and Abbas Mohammadi},
  doi          = {10.1109/TMC.2025.3545464},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6858-6869},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A traffic-aware graph neural network for user association in cellular networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual reinforcement learning for digital twin synchronization optimization. <em>TMC</em>, <em>24</em>(8), 6843-6857. (<a href='https://doi.org/10.1109/TMC.2025.3546507'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive resource allocation scheme for digital twin (DT) synchronization optimization over dynamic wireless networks. In our considered model, a base station (BS) continuously collects factory physical object state data from wireless devices to build a real-time virtual DT system for factory event analysis. Due to continuous data transmission, maintaining DT synchronization must use extensive wireless resources. To address this issue, a subset of devices is selected to transmit their sensing data, and resource block (RB) allocation is optimized. This problem is formulated as a constrained Markov process (CMDP) problem that minimizes the long-term mismatch between the physical and virtual systems. To solve this CMDP, we first transform the problem into a dual problem that refines RB constraint impacts on device scheduling strategies. We then propose a continual reinforcement learning (CRL) algorithm to solve the dual problem. The CRL algorithm learns a stable policy across historical experiences for quick adaptation to dynamics in physical states and network capacity. Simulation results show that the CRL can adapt quickly to network capacity changes and reduce normalized root mean square error (NRMSE) between physical and virtual states by up to 55.2%, using the same RB number as traditional methods.},
  archive      = {J_TMC},
  author       = {Haonan Tong and Mingzhe Chen and Jun Zhao and Ye Hu and Zhaohui Yang and Yuchen Liu and Changchuan Yin},
  doi          = {10.1109/TMC.2025.3546507},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6843-6857},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Continual reinforcement learning for digital twin synchronization optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental wavelet-capsules: A cross-environment solution for WiFi identification. <em>TMC</em>, <em>24</em>(8), 6827-6842. (<a href='https://doi.org/10.1109/TMC.2025.3544836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi-based identity recognition differs from traditional identification technologies as it is not limited by lighting conditions and does not require dense, specialized sensors or wearable devices. This makes it valuable in modern human–machine interactions. However, the diversity of real-world environmental conditions substantially limits the application of existing WiFi-based identity recognition algorithms, particularly when applied across different environments. As a solution, we introduce the incremental wavelet capsule (IWC) model, which combines a newly designed wavelet convolution layer with a capsule network to accelerate precise feature extraction. We adopt a hybrid incremental learning strategy, solving the catastrophic forgetting1 problem in cross-environment tasks and enabling the model to adapt to new environments in the data stream without forgetting the original environment. Furthermore, we developed a customized data augmentation method for WiFi signals, enhancing the model’s adaptability and stability across various environments. Experimental results show that the IWC model achieves an average recognition accuracy of 97.36% across five different environments and maintains an accuracy of 91.5% even when only 5% of the training data from a new environment is used. These findings demonstrate the model’s robust performance and practicality in cross-environment scenarios.},
  archive      = {J_TMC},
  author       = {Zhiyi Zhou and Xinxin Lu and Lei Wang and Yu Tian and Yunbo Chen and Bingxian Lu},
  doi          = {10.1109/TMC.2025.3544836},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6827-6842},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Incremental wavelet-capsules: A cross-environment solution for WiFi identification},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COLoRIS: Localization-agnostic smart surfaces enabling opportunistic ISAC in 6G networks. <em>TMC</em>, <em>24</em>(8), 6812-6826. (<a href='https://doi.org/10.1109/TMC.2025.3556326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Smart Surfaces in 6G communication networks, also dubbed as Reconfigurable Intelligent Surfaces (RISs), is a promising paradigm change gaining significant attention given its disruptive features. RISs are a key enabler in the realm of 6G Integrated Sensing and Communication (ISAC) systems where novel services can be offered together with the future mobile networks communication capabilities. This paper addresses the critical challenge of precisely localizing users within a communication network by leveraging the controlled-reflective properties of RIS elements without relying on more power-hungry traditional methods, e.g., GPS, adverting the need of deploying additional infrastructure and even avoiding interfering with communication efforts. Moreover, we go one step beyond: we build COLoRIS, an Opportunistic ISAC approach that leverages localization-agnostic RIS configurations to accurately position mobile users via trained learning models. Extensive experimental validation and simulations in large-scale synthetic scenarios show $\mathbf{5\%}$ positioning errors (with respect to field size) under different conditions. Further, we show that a low-complexity version running in a limited off-the-shelf (embedded, low-power) system achieves positioning errors in the $\mathbf{11\%}$ range at a negligible $\mathbf{+2.7\%}$ energy expense with respect to the classical RIS.},
  archive      = {J_TMC},
  author       = {Guillermo Encinas-Lago and Francesco Devoti and Marco Rossanese and Vincenzo Sciancalepore and Marco Di Renzo and Xavier Costa-Pérez},
  doi          = {10.1109/TMC.2025.3556326},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6812-6826},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {COLoRIS: Localization-agnostic smart surfaces enabling opportunistic ISAC in 6G networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ActivityMamba: A CNN-mamba hybrid neural network for efficient human activity recognition. <em>TMC</em>, <em>24</em>(8), 6797-6811. (<a href='https://doi.org/10.1109/TMC.2025.3544573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current research in human activity recognition primarily emphasizes enhancing accuracy, with limited exploration into computational efficiency and hardware compatibility. Recently, Mamba has sparked substantial interest within the realm of deep learning. Mamba is a hardware-aware algorithm enabling very efficient training and inference. Researchers are applying Mamba to various tasks, demonstrating significant promise in both language and vision tasks. It is worthwhile to investigate the use of Mamba for efficient human activity recognition. In this paper, we proposed a hybrid neural network that integrates CNN and visual Mamba, called ActivityMamba. The SE-Mamba block in ActivityMamba utilizes both CNN’s local and Mamba’s global context modeling while keeping computation and memory efficiency. We evaluated the ActivityMamba on five public benchmark datasets collected by using three different sensing techniques. ActivityMamba achieved higher performance than vision transformers, vision Mamba, and CNNs with fewer FLOPs and parameters. It sets a new SOTA on all five datasets, which are 91.78% OA and 89.13% F1 on the USC-HAD dataset, 99.19% OA and 98.64% F1 on the UT-HAR dataset, 99.82% OA and F1 on the DIAT dataset, 98.59% OA and 98.65% F1 on the UCI-HAR dataset, and 95.41% OA and 93.14% F1 on the UniMib dataset. Our work is the first to investigate the CNN-Mamba hybrid network for efficient human activity recognition.},
  archive      = {J_TMC},
  author       = {Fei Luo and Anna Li and Bin Jiang and Salabat Khan and Kaishun Wu and Lu Wang},
  doi          = {10.1109/TMC.2025.3544573},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6797-6811},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ActivityMamba: A CNN-mamba hybrid neural network for efficient human activity recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MoCo: Urban user mobile contact detection based on cellular signaling trace. <em>TMC</em>, <em>24</em>(8), 6780-6796. (<a href='https://doi.org/10.1109/TMC.2025.3545437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile contact exhibits user co-traveling events within the same transportation tool, which is crucial for resident profiling, face-to-face interaction detection, etc. In this paper, we investigate urban user mobile contact detection with cellular signaling traces, which is cost-efficient to enable large-scale detection. Specifically, we develop a data collection platform to collect substantial user signaling traces, covering different types of road scenarios within a city. With the collected traces, we perform systematic data analysis to reveal several technical challenges, which are sparsity of signaling trajectory, remote base station noise, and fuzzy matching difficulties. To address challenges, we propose a mobile contact detection method named MoCo. In MoCo framework, we first conduct data denoising to remove the noise from remote base stations. Then, we devise a spatio-temporal filter to eliminate unlikely mobile contact traces in both spatial and temporal domains, reducing the computational overhead. Finally, we design a detection network that integrates the submodules of data alignment, feature encoder, spatio-temporal representation learner, and user mobile contact detector. Extensive evaluation results demonstrate the superiority of MoCo in comparison with state-of-the-art baselines. Robust experiments show that MoCo can work efficiently in different transportation modes and urban densities.},
  archive      = {J_TMC},
  author       = {Sijing Duan and Feng Lyu and Jing Zhang and Huali Lu and Peng Yang and Huaqing Wu and Yaoxue Zhang and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3545437},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6780-6796},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MoCo: Urban user mobile contact detection based on cellular signaling trace},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AUTHFi: Cross-technology device authentication via commodity WiFi. <em>TMC</em>, <em>24</em>(8), 6765-6779. (<a href='https://doi.org/10.1109/TMC.2025.3547010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosive growth of the Internet of Things (IoT) has dramatically increased the demand for secure mechanisms to protect against unauthorized access and attacks. Traditionally, expensive Software-Defined Radios (SDRs) have been utilized to gather IoT physical features, which are critical for reliable authentication. However, the high cost of SDRs makes them impractical for widespread deployment across the vast and diverse IoT ecosystem. In contrast, this paper presents AUTHFi, a novel cross-technology device authentication framework that transforms the SDR approach for collecting and authenticating IoT device signals (e.g., ZigBee and Bluetooth) by utilizing commercial WiFi devices. Specifically, AUTHFi leverages the recent advances in Cross-Technology Communication (CTC) to reconstruct the partial waveform of IoT transmission, thus eliminating the requirement for expensive SDRs. AUTHFi requires us to address several unique challenges. First, AUTHFi compensates for signal losses of the partial waveform to get more signal information. Then, it introduces an enhanced Carrier Frequency Offset (CFO) estimation and a fusion neural network that combines CFO and the reconstructed waveform for accurate device authentication. We implement AUTHFi based on RTL8812au (commodity WiFi) and CC2652P (commodity ZigBee/Bluetooth). Our thorough evaluation confirms that AUTHFi offers reliable authentication under various settings, achieving a maximum accuracy of 94.2%.},
  archive      = {J_TMC},
  author       = {Weizheng Wang and Dusit Niyato and Zehui Xiong and Zhimeng Yin},
  doi          = {10.1109/TMC.2025.3547010},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6765-6779},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AUTHFi: Cross-technology device authentication via commodity WiFi},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FOOL: Addressing the downlink bottleneck in satellite computing with neural feature compression. <em>TMC</em>, <em>24</em>(8), 6747-6764. (<a href='https://doi.org/10.1109/TMC.2025.3544516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks. This work presents an OEC-native and task-agnostic feature compression method that preserves prediction performance and partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While the encoding prioritizes features for downstream tasks, we can reliably recover images with competitive scores on quality measures at lower bitrates. We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. Finally, we test the feasibility of our system for standardized nanosatellite form factors. We demonstrate that the proposed approach permits downlinking over 100× the data volume without relying on prior information on the downstream tasks.},
  archive      = {J_TMC},
  author       = {Alireza Furutanpey and Qiyang Zhang and Philipp Raith and Tobias Pfandzelter and Shangguang Wang and Schahram Dustdar},
  doi          = {10.1109/TMC.2025.3544516},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6747-6764},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FOOL: Addressing the downlink bottleneck in satellite computing with neural feature compression},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeFL: Nested model scaling for federated learning with system heterogeneous clients. <em>TMC</em>, <em>24</em>(8), 6734-6746. (<a href='https://doi.org/10.1109/TMC.2025.3549600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables distributed training while preserving data privacy, but stragglers—slow or incapable clients can significantly slow down the total training time and degrade performance. To mitigate the impact of stragglers, system heterogeneity, including heterogeneous computing and network bandwidth, has been addressed. While previous studies have addressed system heterogeneity by splitting models into submodels, they offer limited flexibility in model architecture design, without considering potential inconsistencies arising from training multiple submodel architectures. We propose nested federated learning (NeFL), a generalized framework that efficiently divides deep neural networks into submodels using both depthwise and widthwise scaling. To address the inconsistency arising from training multiple submodel architectures, NeFL decouples a subset of parameters from those being trained for each submodel. An averaging method is proposed to handle these decoupled parameters during aggregation. NeFL enables resource-constrained devices to effectively participate in the FL pipeline, facilitating larger datasets for model training. Experiments demonstrate that NeFL achieves performance gain, especially for the worst-case submodel compared to baseline approaches (7.63% improvement on CIFAR-100). Furthermore, NeFL aligns with recent advances in FL, such as leveraging pre-trained models and accounting for statistical heterogeneity.},
  archive      = {J_TMC},
  author       = {Honggu Kang and Seohyeon Cha and Jinwoo Shin and Jongmyeong Lee and Joonhyuk Kang},
  doi          = {10.1109/TMC.2025.3549600},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6734-6746},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {NeFL: Nested model scaling for federated learning with system heterogeneous clients},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive stabilization control by deep reinforcement learning for hovering drone surveillance. <em>TMC</em>, <em>24</em>(8), 6720-6733. (<a href='https://doi.org/10.1109/TMC.2025.3548421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an adaptive stabilization control mechanism by using deep reinforcement learning (DRL) for hovering drones that have to execute a surveillance task for a long time. For long-endurance flights, we design and implement a buoyancy-aided autonomous aerial vehicle (AAV) that can use buoyancy lift to decrease the weight and increase the battery capacity so that the flight time can be significantly extended. However, the balloons of the buoyancy-aided AAV can cause “an inverted pendulum effect” and an instability issue on the drone attitude because the increased surface is easily affected by the gusty wind. We propose a buoyancy-aided adaptive stabilization control (BAASC) method with the DRL to stabilize the attitude and extend the flight time of the quadrotor-based buoyancy-aided AAV. This proposed model can immediately control the speeds of all rotors to balance the attitude based on the current state of the drone. Therefore, the degree of swing can be stabilized, and the inverted pendulum effect can be eliminated. The experimental results reveal that the designed buoyancy-aided AAV with the proposed BAASC scheme can effectively stabilize the attitude to extend the flight time by 112.8% compared with a nonbuoyancy-aided AAV under a gusty wind disturbance.},
  archive      = {J_TMC},
  author       = {Chao-Yang Lee and Ang-Hsun Tsai and Li-Chun Wang},
  doi          = {10.1109/TMC.2025.3548421},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {8},
  number       = {8},
  pages        = {6720-6733},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive stabilization control by deep reinforcement learning for hovering drone surveillance},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correction to “CV-cast: Computer Vision–Oriented linear coding and transmission”. <em>TMC</em>, <em>24</em>(7), 6719. (<a href='https://doi.org/10.1109/TMC.2025.3565860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the above article [1], on page 1151, eq. (6), there is an error in the equation. The correct equation is: \begin{equation*} \min.\,\,D,\,\,\text{s.t.} \sum\limits_{k = 1}^K {{{\lambda }_k}\beta _k^2 \leqslant P.} \tag{6} \end{equation*} min.D,s.t.∑k=1Kλkβk2⩽P.(6)},
  archive      = {J_TMC},
  author       = {Jakub Žádník and Michel Kieffer and Anthony Trioux and Markku Mäkitalo and Pekka Jääskeläinen},
  doi          = {10.1109/TMC.2025.3565860},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6719},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Correction to “CV-cast: Computer Vision–Oriented linear coding and transmission”},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrections to “Learning domain-invariant model for WiFi-based indoor localization”. <em>TMC</em>, <em>24</em>(7), 6718. (<a href='https://doi.org/10.1109/TMC.2025.3539443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the above article [1], on page 13900, right column, there is an empty reference citation “[?]” in the sentence “By applying Model-Agnostic Meta-Learning (MAML) to fingerprint localization, MetaLoc [?] enables the model to quickly adapt to new environments based on the obtained meta-parameters, thus reducing human labor costs.” The missing reference is listed below as [2].},
  archive      = {J_TMC},
  author       = {Guanzhong Wang and Dongheng Zhang and Tianyu Zhang and Shuai Yang and Qibin Sun and Yan Chen},
  doi          = {10.1109/TMC.2025.3539443},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6718},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Corrections to “Learning domain-invariant model for WiFi-based indoor localization”},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential private data stream analytics in the local and shuffle models. <em>TMC</em>, <em>24</em>(7), 6701-6717. (<a href='https://doi.org/10.1109/TMC.2025.3559621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study online data analytics with differential privacy (DP) in decentralized settings. Specifically, online data analytics with local DP protection is widely adopted in real-world applications. Despite numerous endeavors in this field, significant gaps in utility and functionality remain when compared to its offline counterpart. We present an optimal, streamable mechanism: ExSub, for local DP sparse vector estimation. The mechanism enables a range of online analytics on streaming binary vectors, including multi-dimensional binary, categorical, or set-valued data. By leveraging the negative correlation of occurrence events in the sparse vector, we attain an optimal error rate under local privacy constraints, only requiring streamable computations. To surpass the error barrier of local privacy, we also study ExSub randomizer in the newly emerging (single-message) shuffle model of DP, and provide nearly-tight privacy amplification bounds therein. Additionally, we leverage the online shuffle model that independently permutes users’ messages at each timestamp, to design a simplified randomization strategy that can approximately reach Gaussian accuracy in central DP. Through experiments with both synthetic and real-world datasets, ExSub mechanism in the local model have been shown to reduce error by 40%–60% compared to SOTA approaches. The ExSub in the shuffle model can further reduce over 85% error, and the online shuffle protocol reduces over 99.7% error.},
  archive      = {J_TMC},
  author       = {Shaowei Wang and Jin Li and Yun Peng and Kongyang Chen and Wei Yang and Hui Jiang and Jin Li},
  doi          = {10.1109/TMC.2025.3559621},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6701-6717},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Differential private data stream analytics in the local and shuffle models},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicle-assisted service caching for task offloading in vehicular edge computing. <em>TMC</em>, <em>24</em>(7), 6688-6700. (<a href='https://doi.org/10.1109/TMC.2025.3545444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of artificial intelligence (AI) enables vehicular edge computing (VEC) servers to be able to provide more intelligent services. However, the limited storage resources of VEC servers constrain the deployment of intelligent service contents, which greatly restricts the intelligence level of the VEC network. To resolve this problem, we first design a novel vehicle-assisted VEC network architecture and further propose VaCo, a Vehicle-assisted Collaborative caching system. VaCo allows VEC servers to download the cached service content from any vehicle in the VEC network to support task offloading. VaCo mainly considers the real-time scheduling problem of vehicle storage resources under the dynamic VEC network and the benefit problem caused by invoking vehicle resources under the highly dynamic load environment. VaCo models the vehicle storage resources as an independent resource pool and deploys a cross-VEC server content retrieval mechanism to achieve unified and efficient management of the storage resources of the vehicle cluster and the VEC server cluster. Then, we propose a multi-swarm collaborative optimization scheme to jointly optimize the service failure rate and cost, and further propose a Pareto-based optimization scheme to ensuring that VaCo can correctly evaluate the benefits of invoking vehicle resources in a dynamic VEC network. Finally, we implement VaCo and conduct extensive evaluations on real-world dataset. The experimental results on the real trajectory dataset show that VaCo can effectively utilize vehicle resources and ensure the benefits of both vehicles and VEC servers simultaneously.},
  archive      = {J_TMC},
  author       = {Hongbo Jiang and Jianghao Cai and Zhu Xiao and Kehua Yang and Hongyang Chen and Jiangchuan Liu},
  doi          = {10.1109/TMC.2025.3545444},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6688-6700},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Vehicle-assisted service caching for task offloading in vehicular edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topology-compressed data delivery in large-scale heterogeneous satellite networks: An age-driven spatial-temporal graph neural network approach. <em>TMC</em>, <em>24</em>(7), 6673-6687. (<a href='https://doi.org/10.1109/TMC.2025.3544574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Large-Scale Heterogeneous Satellite Networks (LSHSNs) integrating Low Earth Orbit (LEO) and Medium Earth Orbit (MEO) satellites, high-timeliness data delivery confronts dynamical connectivity and obvious latency, which heavily challenges existing graph-dependable transmission strategies requiring to obtain global topological information with huge computational cost and signaling overhead. To address this issue, in this paper, we propose an Age-predicting Local Information Dependable Transmission (ALIDT) mechanism for the LSHSN by considering the impact of time-varying topology on the timeliness of data, in which a novel metric of data freshness called Forwarding-aware Age of Information (FAoI) is well-designed to evaluate the timeliness in data forwarding at node. In particular, we develop a satellite Coverage-based Local Information Sharing (CLIS)-assisted Spatial-Temporal Graph Neural Network (STGNN) to extract the topological features in both temporal and spatial dimensions and a Graph Matching Network (GMN)-based topology compression algorithm to improve computation efficiency. The simulation results indicate that the proposed mechanism performs better in improving the storage overhead, throughput and average FAoI compared with the conventional Open Shortest Path First (OSPF) routing algorithm with Time-Varying Graph (TVG) model, GNN-based Multipath Routing (GMR) algorithm, and Gated Recurrent Units (GRU) based metric prediction algorithm in hybrid satellite networks, respectively.},
  archive      = {J_TMC},
  author       = {Ronghao Gao and Bo Zhang and Qinyu Zhang and Zhihua Yang},
  doi          = {10.1109/TMC.2025.3544574},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6673-6687},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Topology-compressed data delivery in large-scale heterogeneous satellite networks: An age-driven spatial-temporal graph neural network approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental least-recently-used algorithm: Good, robust, and predictable performance. <em>TMC</em>, <em>24</em>(7), 6658-6672. (<a href='https://doi.org/10.1109/TMC.2025.3547066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a replacement algorithm for file caching in mobile edge computing (MEC) networks. While there are numerous schemes for file replacement, it remains a challenge to achieve good, robust, and predictable performance simultaneously. To address this challenge, we introduce a general scheme called Incremental Least-Recently-Used (iLRU), which builds on the classic Least-Recently-Used (LRU) algorithm. iLRU initially caches only a “portion” of the file upon the first request and incrementally caches more when there are more requests for the file. In this regard, the request frequency can be inferred from the cached size without incurring additional overhead, where a larger cached size represents a higher request frequency. We derive the theoretical hit ratio of iLRU based on the Time-to-Live (TTL) analysis. With the Time-to-Live (TTL) analysis, we can theoretically derive the hit ratio and properties of iLRU and notably show that iLRU allocates more cache space to popular files, resulting in a higher hit ratio than LRU. Simulation results demonstrate the superior performance of iLRU and validate the accuracy of the theoretical hit ratio. Furthermore, we conduct simulations over various real-world traces to show that iLRU outperforms existing schemes across various real-world traces, defenestrating the robustness of iLRU.},
  archive      = {J_TMC},
  author       = {Jinbei Zhang and Chunpeng Chen and Kechao Cai and John C. S. Lui},
  doi          = {10.1109/TMC.2025.3547066},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6658-6672},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Incremental least-recently-used algorithm: Good, robust, and predictable performance},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LCEFL: A lightweight contribution evaluation approach for federated learning. <em>TMC</em>, <em>24</em>(7), 6643-6657. (<a href='https://doi.org/10.1109/TMC.2025.3545140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prerequisite for implementing incentive mechanisms and reliable participant selection schemes in federated learning is to obtain the contribution of each participant. Available evaluation methods for participant contributions require the server to possess a test dataset, often impractical. Additionally, the excessively high complexity of these works is unacceptable when training complex models in large-scale federated learning system. To address these issues, we propose a lightweight contribution evaluation method for federated learning participants, named LCEFL, based on model projection theory, which does not require the server to provide a test dataset. In addition, a model compression method is designed to be used in LCEFL to reduce the computational complexity. Furthermore, a trusted aggregation method based on LCEFL is proposed, where the weight of each participant's local model is determined by its trust level, which can be calculated using its contribution evaluation result. Experimental results show that LCEFL can achieve nearly the same accuracy as schemes based on Shapley Value, while significantly reducing computational overhead by more than 50%. Compared to available aggregation methods, the proposed trusted aggregation scheme is able to accelerate the convergence speed of the global model and improve its accuracy by 2% to 45%.},
  archive      = {J_TMC},
  author       = {Jingjing Guo and Jiaxing Li and Zhiquan Liu and Yupeng Xiong and Yong Ma and Athanasios V. Vasilakos and Xinghua Li and Jianfeng Ma},
  doi          = {10.1109/TMC.2025.3545140},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6643-6657},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LCEFL: A lightweight contribution evaluation approach for federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Individualized data generation in personalized federated learning. <em>TMC</em>, <em>24</em>(7), 6628-6642. (<a href='https://doi.org/10.1109/TMC.2025.3545244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most Personalized Federated Learning (PFL) algorithms merge the model parameters of each client with other (similar or generic) model parameters to optimize the personalized model (PM). However, the merged model parameters in these algorithms may fit low relevance data, thereby limiting the performance of PM. In this paper, we generate similar data for each client through the collaboration of a generic model (GM) on the server, rather than merging model parameters. To train a generator capable of generating data for all classes on the server without real data, we employ the GM as the discriminator in adversarial training with the generator. Additionally, we introduce a similarity assessment metric, which allows for the assessment of the similarity between local data and data from other classes. Nevertheless, the presence of non-IID data among clients can weaken the performance of the GM, consequently impacting the training of the generator and similarity assessment. To address this issue, we design a directive mechanism so that GM can be optimized during adversarial training without the need for additional training. The experimental results validate the superiority of our algorithm over state-of-the-art algorithms in terms of accuracy, loss, and convergence speed.},
  archive      = {J_TMC},
  author       = {Yunyun Cai and Wei Xi and Yuhao Shen and Cerui Sun and Shuai Wang and Wei Gong and Jizhong Zhao},
  doi          = {10.1109/TMC.2025.3545244},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6628-6642},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Individualized data generation in personalized federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Device power saving with time-frequency adaptation: Joint BWP-DRX design with BWP switching delay considered. <em>TMC</em>, <em>24</em>(7), 6613-6627. (<a href='https://doi.org/10.1109/TMC.2025.3547978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today's ever-growing data traffic landscape, optimizing network power efficiency and performance has become crucial. Discontinuous Reception (DRX) and Bandwidth Parts (BWP) are two key technologies that fulfill this pursuit. DRX is a time-domain power-saving technology that allows user equipment (UE) to switch off their radio frequency module. BWP switching is a frequency domain operation that allows UE to operate on only partial bandwidth for power saving. Investigating the interaction and trade-off between DRX and BWP is a must to optimize network efficiency and enhance network performance. This work proposed a novel BWP-DRX joint mechanism and its analytical model that leverages the concept of “Detect time” with the consideration of BWP switching delay. The model reduces packet loss rate by 50%, packet delay by 36% and increases the energy efficiency rate by 50% when arrival rate is high with the trade-off of 12% power efficiency reduction when arrival rate is low compared to the model without Detect time. The influence of each parameter is further analyzed to reach the best network efficiency under different traffic conditions.},
  archive      = {J_TMC},
  author       = {Cheng-Wei Tsai and Kuang-Hsun Lin and Hung-Yu Wei},
  doi          = {10.1109/TMC.2025.3547978},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6613-6627},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Device power saving with time-frequency adaptation: Joint BWP-DRX design with BWP switching delay considered},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forward legal anonymous group pairing-onion routing for mobile opportunistic networks. <em>TMC</em>, <em>24</em>(7), 6595-6612. (<a href='https://doi.org/10.1109/TMC.2025.3544674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Opportunistic Networks (MONs) often experience frequent interruptions in end-to-end connections, which increases the likelihood of message loss during delivery and makes users more susceptible to various cyber attacks. However, most currently proposed anonymous routing protocols are primarily designed for networks with stable connections, making it challenging to protect user identities in MONs. To address these challenges, we propose FLAG-POR (Forward Legal Anonymous Group Pairing-Onion Routing), a novel anonymous routing protocol specifically tailored to enhance message delivery anonymity and security in MONs. Specifically, we abstract the mobile opportunistic network as a contact graph. By introducing the concept of “groups” into the pairing-onion routing protocol, which encrypts messages and relay nodes layer by layer, we develop a novel group-based pairing-onion routing protocol. This protocol ensures message confidentiality and relay node anonymity, while also improving message forwarding rates, as any node within a group can potentially act as a relay. To ensure message authenticity, we employ the efficient SM2 signing algorithm to generate signatures for the message source. Furthermore, by incorporating parameters such as the public key validity period and master key validity period into the group pairing-onion routing protocol, we achieve forward security in message delivery. We conduct a thorough theoretical analysis of the protocol’s security and performance. The experimental results demonstrate that our FLAG-POR protocol outperforms baseline anonymous protocols in terms of delivery success rate, traceability rate, path anonymity, and node anonymity. Additionally, the FLAG-POR scheme effectively resists three potential threats to the routing system: collusion attack threat, node identification threat, and path identification threat, in any situation.},
  archive      = {J_TMC},
  author       = {Xiuzhen Zhu and Limei Lin and Yanze Huang and Xiaoding Wang and Sun-Yuan Hsieh and Jie Wu},
  doi          = {10.1109/TMC.2025.3544674},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6595-6612},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Forward legal anonymous group pairing-onion routing for mobile opportunistic networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReflexGest: Recognizing hand gestures under VLC-capable lamps. <em>TMC</em>, <em>24</em>(7), 6583-6594. (<a href='https://doi.org/10.1109/TMC.2025.3545340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a main approach towards touch-free human-computer interaction, hand gesture recognition (HGR) has long been a research focus for both academia and industry. Meanwhile, visible light communication (VLC) has become increasingly popular with VLC-ready commercial products (e.g., Philips lamps) available on the market. These facts provoke us to ask: can we leverage a VLC-ready lamp to realize integrated sensing and communication (ISAC) by conducting both HGR and VLC simultaneously? To this end, we propose ReflexGest as our answer to this question. ReflexGest is implemented upon a table lamp for the sake of practicality; this VLC-ready lamp is equipped with a ring-shaped light-emitting diode (LED) array and a photodiode (PD, for light intensity sensing) originally aiming for up/down-link VLCs. Demanding hand gestures to be performed between the lamp and a table surface, ReflexGest exploits the variation of the reflection and their unique correlation with the corresponding hand gestures to achieve HGR. In particular, ReflexGest first handles the limited sensing ability of the PD by enhancing the LED lamp and thus diversifying the light emission patterns. Moreover, ReflexGest combats the reflection interference from varying table surfaces via an adversarial learning technique to distill only the features relevant to hand gestures. Our extensive evaluations demonstrate that ReflexGest is able to deliver accurate HGR under realistic VLC traffic.},
  archive      = {J_TMC},
  author       = {Ziwei Liu and Jifei Zhu and Jiaqi Yang and Yimao Sun and Yanbing Yang and Jun Luo},
  doi          = {10.1109/TMC.2025.3545340},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6583-6594},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ReflexGest: Recognizing hand gestures under VLC-capable lamps},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint design of power allocation and beamforming for IRS-assisted millimeter-wave communication system with imperfect CSI. <em>TMC</em>, <em>24</em>(7), 6566-6582. (<a href='https://doi.org/10.1109/TMC.2025.3545413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the joint power allocation (PA), passive beamforming (BF) and hybrid BF (HBF) including digital and analogue BFs are designed for an intelligent reflecting surface (IRS)-assisted millimeter-Wave (mmWave) communication system with imperfect channel state information (CSI) and multiple mobile users to optimize the weighted sum rate (WSR) and energy efficiency (EE). The achievable WSR and EE of the IRS-mmWave system are first derived based on imperfect cascaded CSI for performance optimization. Then, the non-convex constrained problem is formulated to maximize the WSR, where the PA, HBF, phase and amplitude of IRS elements are jointly optimized. Given PA and passive BF (PBF), closed-form suboptimal HBF is obtained for each iteration. Also, given HBF and PBF, using the block coordinate descent (BCD) methods, closed-form PA is derived. Moreover, the phase and amplitude of IRS elements are derived for PBF design during each iteration. With the obtained HBF, the digital and analogue BFs are also derived. Based on this, joint schemes of PA, HBF and PBF are developed. Besides, an efficient iterative algorithm based upon the alternating optimization (AO), weighted minimum mean-square error (WMMSE) and Dinkelbach methods are presented for EE maximization and the suboptimal solution is obtained. Correspondingly, the energy-efficient design for joint PA, HBF and PBF is provided. Simulation results verify the proposed solutions.},
  archive      = {J_TMC},
  author       = {Xiangbin Yu and Chenghong Yang and Jiawei Bai and Kezhi Wang and Yun Rui and Xiaoyu Dang},
  doi          = {10.1109/TMC.2025.3545413},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6566-6582},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint design of power allocation and beamforming for IRS-assisted millimeter-wave communication system with imperfect CSI},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward optimal broadcast mode in offline finding network. <em>TMC</em>, <em>24</em>(7), 6550-6565. (<a href='https://doi.org/10.1109/TMC.2025.3545561'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes ElastiCast, a novel Bluetooth Low Energy (BLE) broadcast mode that reduces the neighbor discovery latency in offline finding networks (OFNs). ElastiCast adapts the broadcast mode of the lost devices to the scan modes of the finder devices, considering their diversity. We start with an overview of OFNs, followed by a detailed analysis of the issues and challenges of existing solutions, which motivates the design of ElastiCast. Then we provide Blender, a simulator that models the neighbor discovery behavior of different broadcasters and scanners. By adopting Blender, ElastiCast can be implemented with three components: Local Optima Estimation, Common Interest Extraction, and Interval Multiplexing, in which we capture the key features of BLE neighbor discovery and globally optimize the broadcast mode interacting with diverse scan modes. Experimental evaluation results and commercial product deployment experience demonstrate that ElastiCast is effective in achieving stable and bounded neighbor discovery latency within the power budget.},
  archive      = {J_TMC},
  author       = {Tong Li and Yukuan Ding and Jiaxin Liang and Kai Zheng and Xu Zhang and Tian Pan and Dan Wang and Ke Xu},
  doi          = {10.1109/TMC.2025.3545561},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6550-6565},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward optimal broadcast mode in offline finding network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CHAR: Composite head-body activities recognition with a single earable device. <em>TMC</em>, <em>24</em>(7), 6532-6549. (<a href='https://doi.org/10.1109/TMC.2025.3548647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing popularity of earable devices stimulates great academic interest to design novel head gesture-based interaction technologies. But existing works simply consider it as a singular activity recognition problem. This is not in line with practice since users may have different body movements such as walking and jogging along with head gestures. It is also beneficial to recognize body movements during human-device interaction since it provides useful context information. As a result, it is significant to recognize such composite activities in which actions of different body parts happen simultaneously. In this paper, we propose a system called CHAR to recognize composite head-body activities with a single IMU sensor. The key idea of our solution is to make use of the inter-correlation of different activities and design a multi-task learning network to extract shared and specific representations. We implement a real-time prototype and conduct extensive experiments to evaluate it. The results show that CHAR can recognize 60 kinds of composite activities (12 head gestures and 5 body movements) with high accuracies of 89.7% and 85.1% in sufficient data and insufficient data cases, respectively.},
  archive      = {J_TMC},
  author       = {Peizhao Zhu and Yuzheng Zhu and Wenyuan Li and Yanbo He and Yongpan Zou and Kaishun Wu and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3548647},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6532-6549},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CHAR: Composite head-body activities recognition with a single earable device},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial offloading strategy based on deep reinforcement learning in the internet of vehicles. <em>TMC</em>, <em>24</em>(7), 6517-6531. (<a href='https://doi.org/10.1109/TMC.2025.3543976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the increasing demands of vehicular tasks, edge offloading has emerged as a promising paradigm to enhance quality of experience (QoE) in Internet of Vehicles (IoV) networks. This approach enables vehicles to offload computation-intensive tasks to edge servers, resulting in reduced computation delays and lower energy consumption. However, traditional binary offloading limits the efficiency of edge offloading. To address this gap, we propose a partial offloading strategy that jointly optimizes the offloading ratio, computation, and communication resources in IoV. Recognizing the varying priorities of vehicular tasks regarding task delay and energy consumption, we formulate two distinct scenarios: one focused on minimizing delay and the other on minimizing energy consumption. Furthermore, we employ a reinforcement learning approach to establish a multi-dimensional joint optimization function by setting different objectives for each scenario. Based on this framework, we introduce a multi-state iteration deep deterministic policy gradient algorithm (SIDDPG), which effectively determines task partitioning and resource allocation. Simulation results demonstrate that the proposed algorithm outperforms benchmark schemes in terms of task delay and energy consumption.},
  archive      = {J_TMC},
  author       = {Shujuan Tian and Xinjie Zhu and Bochao Feng and Zhirun Zheng and Haolin Liu and Zhetao Li},
  doi          = {10.1109/TMC.2025.3543976},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6517-6531},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Partial offloading strategy based on deep reinforcement learning in the internet of vehicles},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Build yourself before collaboration: Vertical federated learning with limited aligned samples. <em>TMC</em>, <em>24</em>(7), 6503-6516. (<a href='https://doi.org/10.1109/TMC.2025.3543923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical Federated Learning (VFL) has emerged as a crucial privacy-preserving learning paradigm that involves training models using distributed features from shared samples. However, the performance of VFL can be hindered when the number of shared or aligned samples is limited, a common issue in mobile environments where user data are diverse and unaligned across multiple devices. Existing approaches use feature generation and pseudo-label estimation for unaligned samples to address this issue, unavoidably introducing noise during the generation process. In this work, we propose Local Enhanced Effective Vertical Federated Learning (LEEF-VFL), which fully utilizes unaligned samples in the local learning before collaboration. Unlike previous methods that overlook private labels owned by each client, we leverage these private labels to learn from all local samples, constructing robust local models to serve as solid foundations for collaborative learning. Additionally, we reveal that the limited number of aligned samples introduces distribution bias from global data distribution. In this case, we propose to minimize the distribution discrepancies between the aligned samples and the global data distribution to enhance collaboration. Extensive experiments demonstrate the effectiveness of LEEF-VFL in addressing the challenges of limited aligned samples, making it suitable for VFL in mobile computing environments.},
  archive      = {J_TMC},
  author       = {Wei Shen and Mang Ye and Wei Yu and Pong C. Yuen},
  doi          = {10.1109/TMC.2025.3543923},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6503-6516},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Build yourself before collaboration: Vertical federated learning with limited aligned samples},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Truthful online combinatorial auction-based mechanisms for task offloading in mobile edge computing. <em>TMC</em>, <em>24</em>(7), 6488-6502. (<a href='https://doi.org/10.1109/TMC.2025.3542135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computation (MEC) is envisioned as a prospective approach for processing the computation-intensive and delay-sensitive tasks of smart mobile devices (SMDs) through offloading them to base stations (BSs) nearby. In fact, efficient task offloading mechanisms are crucial to accomplish an MEC system. The key challenge is to make on-spot decisions upon the arrival of each task and at the same time achieve truthfulness of each SMD. The challenge further escalates, when the unique characteristics of an MEC system, such as locality constraint, delay constraint, etc., are explicitly considered. To solve the challenge, we present a truthful online combinatorial auction-based mechanism (TOCA) for task offloading in an MEC system. Specifically, we first devise the candidate offloading scheme determination algorithm, aiming to determine the candidate offloading schemes of an SMD upon the arrival of its task. Next, we devise the winning offloading scheme selection and pricing algorithm based on the online primal-dual optimization framework, to decide the winning scheme among the SMD's candidate offloading schemes and calculate its payment. By solid theoretical analysis, we verify that TOCA achieves truthfulness, individual rationality and computational efficiency and a smaller competitive ratio. Trace-driven simulation studies validate the effectiveness and efficacy of TOCA.},
  archive      = {J_TMC},
  author       = {Xueyi Wang and Xingwei Wang and Chen Wang and Rongfei Zeng and Lianbo Ma and Qiang He and Min Huang},
  doi          = {10.1109/TMC.2025.3542135},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6488-6502},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Truthful online combinatorial auction-based mechanisms for task offloading in mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symbiotic resource pricing in the computing continuum era. <em>TMC</em>, <em>24</em>(7), 6474-6487. (<a href='https://doi.org/10.1109/TMC.2025.3542017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though extensive research efforts have been devoted to the problem of computing resource pricing, they mainly focus on single computing paradigms. In this paper, we provide a holistic approach to this problem, by treating the whole computing continuum, consisting of cloud, edge, and fog computing providers, simultaneously offering their resources to the users. Within such a complex setting, we establish the concept of symbiotic computing resource pricing and sharing, where the computing providers and the users coexist within a mutually beneficial ecosystem, sharing services and resources as a means of ensuring their business survival and service satisfaction. Under this prism, we introduce two key pricing families, namely the non-cooperative one which involves competition and is treated through game theoretic approaches, and the cooperative resource pricing (full or partial), which addresses complex scenarios through optimization and coalition. A thorough performance assessment is provided, through modeling and simulation, in order to highlight and quantify the key characteristics and tradeoffs of the various resource pricing approaches introduced.},
  archive      = {J_TMC},
  author       = {Aisha B Rahman and Panagiotis Charatsaris and Eirini Eleni Tsiropoulou and Symeon Papavassiliou},
  doi          = {10.1109/TMC.2025.3542017},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6474-6487},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Symbiotic resource pricing in the computing continuum era},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint mobile energy replenishment and data gathering in wireless sensor networks via federated deep reinforcement learning. <em>TMC</em>, <em>24</em>(7), 6460-6473. (<a href='https://doi.org/10.1109/TMC.2025.3543009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the proliferation of wireless energy transfer for Wireless Sensor Networks (WSNs), which are mainly used for data gathering in real-world applications. A number of studies have investigated mobile vehicle scheduling to charge sensor nodes via wireless Mobile Chargers (MCs). Unfortunately, most of them cannot parallelly charge all nodes in an intelligent manner with the global network attributes. Furthermore, the time-variable charging ignores the optimal data gathering, resulting in poor Joint Energy Replenishment and Data Gathering (JERDG). To fill this gap, this paper proposes a Federated Deep Reinforcement Learning (FDRL)-based JERDG (FERG) solution for WSNs. To this end, FERG first partitions the networks into a set of clusters to distribute the workload evenly among multiple MCs, and then designs an FDRL-based framework that incorporates various time-variant network attributes to determine the optimal schedule for charging and data gathering via multiple MCs and a base station (BS). The BS as the cloud server is responsible for global training of JERDG models, while multiple MCs will parallelly train local models to jointly charge energy-exhausted nodes and gather the data from all nodes in clusters. To reserve more personalized characteristics of each cluster, a density-based partial aggregation strategy is designed to train the global model. Furthermore, a reward-weighted update and selection solution is proposed to generate and exploit reference samples with high rewards. Simulation results obtained from various scenarios demonstrate that FERG significantly outperforms the state-of-the-art approaches in terms of network lifetime, energy efficiency and data collection latency.},
  archive      = {J_TMC},
  author       = {Haojun Huang and Junhao Zhang and Bang Wang and Wang Miao and Geyong Min},
  doi          = {10.1109/TMC.2025.3543009},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6460-6473},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint mobile energy replenishment and data gathering in wireless sensor networks via federated deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization-inspired graph neural network for cellular network optimization. <em>TMC</em>, <em>24</em>(7), 6446-6459. (<a href='https://doi.org/10.1109/TMC.2025.3542434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of wireless communications has driven the need for careful optimization of network parameters to improve network performance and reduce operational cost. Traditional methods, however, struggle with the vast number of tunable parameters and lack scalability in diverse network scenarios. To address these challenges, this paper introduces an optimization-inspired bipartite graph neural network (Bi-GNN) approach for scalable network optimization. Our approach leverages the bipartite structure of network topologies, and incorporates a message-passing mechanism by unfolding the Zeroth-Order Block Coordinate Projected Gradient Descent (ZO-BCPGD) algorithm, which ensures not only high-performance optimization but also manageable computational demand. We demonstrate the permutation and dimensionality equivariance property of the Bi-GNN, which significantly enhances the model’s generalizability across various network structures and sizes. Furthermore, we theoretically analyze the expressive power and generalization ability of the Bi-GNN, demonstrating its adeptness at complex network optimization tasks. The training process, parallel execution, and practical implementation techniques are also discussed to ensure the model’s applicability in real-world scenarios. Numerical results verify that the Bi-GNN outperforms existing methods in both coverage ratios and computational cost. Furthermore, our approach exhibits robust scalability across various network scenarios, making it a versatile tool for optimizing a wide range of wireless networks.},
  archive      = {J_TMC},
  author       = {Pengcheng He and Yijia Tang and Fan Xu and Qingjiang Shi},
  doi          = {10.1109/TMC.2025.3542434},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6446-6459},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimization-inspired graph neural network for cellular network optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed physical layer authentication framework exploiting array pattern feature for mmWave MIMO systems. <em>TMC</em>, <em>24</em>(7), 6430-6445. (<a href='https://doi.org/10.1109/TMC.2025.3541725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authentication in millimeter-Wave (mmWave) Multiple-Input Multiple-Output (MIMO) systems is a critical issue due to the unique characteristics of mmWave communication, such as highly directional beamforming and the ability to support massive device connectivity. To address this challenge, this paper proposes a novel low-complexity decision-level-based Distributed Physical Layer Authentication (DPLA) framework to combat identity-based impersonation attacks in mmWave MIMO systems. The DPLA framework leverages Beam Pattern (BP) deviation, which arises from hardware-specific gain errors, as a key authentication feature. A fusion center is introduced to make the final authentication decision by aggregating local decisions from multiple collaborative nodes, enabling multi-directional perception. Specifically, a low-complexity hybrid combining fusion rule is carefully designed to accommodate the fully connected structure of mmWave MIMO systems, balancing computational efficiency and authentication performance. A rigorous performance analysis is conducted by deriving closed-form analytical expressions for the probabilities of correct detection and false alarm. Furthermore, the asymptotic detection and discrimination performance are systematically analyzed in the large-scale antenna regime. To further enhance authentication accuracy, digital signaling matrices are designed using the deflection coefficient maximization principle. The feasibility of the proposed framework is validated through a comprehensive evaluation, demonstrating its superior robustness and efficiency compared to benchmark methods.},
  archive      = {J_TMC},
  author       = {Pinchang Zhang and Keshuang Han and Yuanyu Zhang and Yulong Shen and Fu Xiao and Xiaohong Jiang},
  doi          = {10.1109/TMC.2025.3541725},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6430-6445},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed physical layer authentication framework exploiting array pattern feature for mmWave MIMO systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Query-aware semantic encoder-based resource allocation in task-oriented communications. <em>TMC</em>, <em>24</em>(7), 6413-6429. (<a href='https://doi.org/10.1109/TMC.2025.3541636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-oriented communications with semantic encoders are promising to enhance the communication efficiency, by selecting and transmitting valuable data according to task requirements/queries. However, existing semantic encoders lack the capability to track the changing in queries, leading to biased data selection. This paper proposes a query-aware semantic encoder, i.e., Query-Data Cross (QDC) encoder for task-oriented communications. By consistently focusing on data features that are most relevant to the current query at the transmitter, QDC can adapt to changing queries. Based on the dynamic semantic relevance obtained by QDC, a relevance-based data selection and bandwidth allocation optimization (RDSBA) problem is formulated, considering a multi-device task-oriented communication system, where devices should transmit valuable data with high relevance to the queries broadcasted by the base station (BS). RDSBA aims to maximize the data profit of all devices, which is defined as the difference between the relevance of data selected for the BS and the cost of obtaining the data. Then, a DRL-based data selection and bandwidth allocation (DRL-DB) algorithm is proposed to solve the NP-hard optimization problem. Simulation results demonstrate that QDC can smartly track the changing in queries and achieve an accuracy of at least 85% in relevance evaluation, more than 8% higher than existing schemes. Based on the relevance provided by QDC, the proposed RDSBA scheme with DRL-DB can increase the data profit by at least 18%, comparing to existing schemes.},
  archive      = {J_TMC},
  author       = {Qing Cai and Yiqing Zhou and Ling Liu and Hanxiao Yu and Yihao Wu and Ningzhe Shi and Jinglin Shi},
  doi          = {10.1109/TMC.2025.3541636},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6413-6429},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Query-aware semantic encoder-based resource allocation in task-oriented communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and effort-efficient image-based indoor localization with generative features. <em>TMC</em>, <em>24</em>(7), 6394-6412. (<a href='https://doi.org/10.1109/TMC.2025.3541045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based indoor localization using smartphones has become popular, leveraging visual landmarks and fingerprint extraction for localization. Fingerprint density significantly affects accuracy, but collecting dense, high-resolution fingerprints during on-site surveys is labor-intensive and incurs high computation/storage costs during matching. Additionally, efficient fingerprint extraction often constrains users to specific shooting poses, with deviations markedly reducing localization accuracy. To address these challenges, we introduce ARGILS, an Automated Real-time Generative Image Localization System. The key idea is to use cross sparse sampling instead of dense sampling, generate fingerprint features for missing locations, and quickly match locations through feature orthogonal decomposition. Cross sparse sampling ensures full coverage of scene features and helps to generate missing fingerprints. To maintain high localization resolution with sparse sampling, we designed a distance-constrained generative adversarial network to generate fingerprints for unsampled locations. Additionally, we developed an orthogonal fingerprint extraction method to decompose image features into horizontal and vertical directions in 2D space. To improve robustness against obstacles, we implemented a scanning localization scheme using key frame filtering and clustering. We have implemented ARGILS and performed extensive real-world evaluations. Experiment results show that when reducing 75% site survey effort, the average location error of ARGILS is around 2.5m in a shopping mall, 48% higher than state-of-the-art methods. ARGILS can also efficiently speed up localization process, with the time consumption ranging from 0.1 to 0.3 seconds on smartphones of various configurations.},
  archive      = {J_TMC},
  author       = {Zhenhan Zhu and Yanchao Zhao and Maoxing Tang and Yanling Bu and Hao Han},
  doi          = {10.1109/TMC.2025.3541045},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6394-6412},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust and effort-efficient image-based indoor localization with generative features},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint view selection, multigroup multicast beamforming, and DIBR for RIS-aided multi-view videos. <em>TMC</em>, <em>24</em>(7), 6376-6393. (<a href='https://doi.org/10.1109/TMC.2025.3543297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of multi-view videos (MVV) transmission is an irresistible trend. Concurrently, reconfigurable intelligent surface (RIS)-assisted wireless communication has drawn significant attention. We observe that the view selection based on the base station and the view synthesis based on depth-image-based rendering (DIBR) can effectively reduce power consumption. Therefore, this paper studies the view selection and synthesis for RIS-aided MVV in multigroup multicast beamforming. To deal with this complicated scenario, we investigate a problem, named the joint View selection, Multicast beamforming, and DIBR (JVMD), to minimize the total multicast beamforming power, the view transmission operation power, and view synthesis, subject to quality-of-service (QoS), RIS phase shifts, view selection, and DIBR constraints. Unfortunately, the mathematical model is a complicated mixed discrete-continuous optimization problem. To tackle this challenging problem, we designed an algorithm, named View selection, Beamforming, RIS phase, and DIBR (VBRD) algorithm. First, we deal with the discrete optimization problem of selecting the view. VBRD uses the dual-based approximation methodology to round back a primal's integer solution. Then, in the continuous optimization problem, we apply the alternating optimization (AO) method to determine beamforming, RIS phase, and DIBR. Finally, simulation results show the performance of exploiting view synthesis for RIS-assisted wireless communication.},
  archive      = {J_TMC},
  author       = {Chi-Han Lee and De-Nian Yang and Guang-Siang Lee and Chih-Hang Wang and Wanjiun Liao},
  doi          = {10.1109/TMC.2025.3543297},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6376-6393},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint view selection, multigroup multicast beamforming, and DIBR for RIS-aided multi-view videos},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAAF: An NDN-based cache-aware adaptive forwarding strategy for reliable content delivery in VANETs. <em>TMC</em>, <em>24</em>(7), 6361-6375. (<a href='https://doi.org/10.1109/TMC.2025.3543458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high mobility in Vehicular Ad-hoc Networks (VANETs) significantly affects the reliability of data transmission. To solve this problem, Named Data Networking (NDN)-based VANETs are proposed, utilizing in-network caching and named-based forwarding to overcome the dual challenges of mobility and connectivity. Although in-network caching enhances content availability, a strategy that accurately locates and efficiently utilizes the cached content in VANETs with highly dynamic environments is still lacking. In this paper, we propose a novel NDN-based cache-aware adaptive forwarding (CAAF) strategy for VANETs. CAAF proactively predicts content locations and ensures reliable content retrieval by adaptively selecting forwarding nodes that prioritize fast delivery and stable transmission. Specifically, we design a content information table for each vehicle to record information about the Interest packets it receives. Furthermore, these tables are updated periodically across all vehicles and a prediction model is used to predict real-time in-network caching during the update interval. Subsequently, we execute a filter mechanism to sieve candidate forwarding vehicles that satisfy both the accessibility and stability requirements. These candidates are then evaluated using a multi-attribute decision-making method across diverse parameters to determine the optimal forwarding node. Our extensive simulation results demonstrate that the proposed CAAF outperforms the state-of-the-art forwarding strategy regarding content retrieval delay and Interest satisfaction ratio across diverse scenarios.},
  archive      = {J_TMC},
  author       = {Haodong Wang and Jiangping Han and Kaiping Xue and Jiayu Yang and Jian Li and Qibin Sun and Jun Lu},
  doi          = {10.1109/TMC.2025.3543458},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6361-6375},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CAAF: An NDN-based cache-aware adaptive forwarding strategy for reliable content delivery in VANETs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environment-tolerant trust opportunity routing based on reinforcement learning for internet of underwater things. <em>TMC</em>, <em>24</em>(7), 6348-6360. (<a href='https://doi.org/10.1109/TMC.2025.3540774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Underwater Things (IoUT) has garnered significant interest due to its potential applications in monitoring underwater environments. However, the unique characteristics of acoustic communication, such as long propagation delays and high attenuation, present considerable obstacles for achieving efficient and dependable data transmission. Opportunistic routing is a crucial technique for enhancing packet delivery ratios by selecting a set of forwarding nodes and utilizing their cooperative forwarding to boost network throughput. Nevertheless, choosing an excessive number of forwarding nodes can lead to wasteful energy usage and extended communication delays. Moreover, the overlooked trustworthiness of forwarded nodes in most research works can undermine the effectiveness of opportunistic routing. Therefore, this study presents a novel trust opportunistic routing scheme that employs reinforcement learning to achieve resilience in constantly changing underwater settings. The combination of reinforcement learning and trust management enables the proposed opportunistic routing scheme to adapt to the unstable underwater environment and unknown malicious attacks. Initially, a method is introduced for measuring environmental fitness by considering multiple trust factors, including communication success rate, data reliability, and location dynamics. The proposed scheme then uses reinforcement learning to develop a reliable opportunistic routing method based on quantified state information. This component employs the obtained state to formulate action strategies and obtains reward values from environmental inputs. The reward update equation integrates these qualities to optimize the deployment of superior action strategies, finally achieving trust opportunistic routing for underwater data collection. Fundamental experimental results demonstrate that the proposed protocol performs exceptionally well in demanding underwater conditions, outperforming existing methods in packet transmission rate, energy efficiency, and end-to-end delay.},
  archive      = {J_TMC},
  author       = {Yu He and Guangjie Han and Yun Hou and Chuan Lin},
  doi          = {10.1109/TMC.2025.3540774},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6348-6360},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Environment-tolerant trust opportunity routing based on reinforcement learning for internet of underwater things},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient two-stage networking topology design for mega-constellation of low earth orbit satellites. <em>TMC</em>, <em>24</em>(7), 6333-6347. (<a href='https://doi.org/10.1109/TMC.2025.3540671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) satellites play a crucial role in providing high-speed internet to remote areas and ensuring network resilience during outages. The design of efficient satellite constellations requires optimizing network topology, which is a complex task due to the large solution space and the need for fault tolerance. This paper presents the AlphaSat algorithm, a two-phase approach to improve latency and network robustness in LEO constellations. In the initialization phase, Monte Carlo Tree Search (MCTS) is used to generate an initial topology by selecting links from a vast search space. In the refinement phase, an edge-switching method is applied to enhance network resilience and performance. AlphaSat is evaluated on OneWeb, Starlink, and Telesat mega-constellations, demonstrating superior performance over existing algorithms. The results show significant reductions in latency ranging from 4.7% to 44.5% and improvements in network robustness, increasing by 3.3% to 28.3%. Furthermore, AlphaSat effectively balances network load and optimizes power consumption, offering a promising solution for efficient and resilient LEO satellite network design.},
  archive      = {J_TMC},
  author       = {Han Hu and Yifeng Lyu and Kaifeng Song and Rongfei Fan and Cheng Zhan and Jian Yang},
  doi          = {10.1109/TMC.2025.3540671},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6333-6347},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An efficient two-stage networking topology design for mega-constellation of low earth orbit satellites},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobility-aware seamless service migration and resource allocation in multi-edge IoV systems. <em>TMC</em>, <em>24</em>(7), 6315-6332. (<a href='https://doi.org/10.1109/TMC.2025.3540407'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) offers low-latency and high-bandwidth support for Internet-of-Vehicles (IoV) applications. However, due to high vehicle mobility and finite communication coverage of base stations, it is hard to maintain uninterrupted and high-quality services without proper service migration among MEC servers. Existing solutions commonly rely on prior knowledge and rarely consider efficient resource allocation during the service migration process, making it hard to reach optimal performance in dynamic IoV environments. To address these important challenges, we propose SR-CL, a novel mobility-aware seamless Service migration and Resource allocation framework via Convex-optimization-enabled deep reinforcement Learning in multi-edge IoV systems. First, we decouple the Mixed Integer Nonlinear Programming (MINLP) problem of service migration and resource allocation into two sub-problems. Next, we design a new actor-critic-based asynchronous-update deep reinforcement learning method to handle service migration, where the delayed-update actor makes migration decisions and the one-step-update critic evaluates the decisions to guide the policy update. Notably, we theoretically derive the optimal resource allocation with convex optimization for each MEC server, thereby further improving system performance. Using the real-world datasets of vehicle trajectories and testbed, extensive experiments are conducted to verify the effectiveness of the proposed SR-CL. Compared to benchmark methods, the SR-CL achieves superior convergence and delay performance under various scenarios.},
  archive      = {J_TMC},
  author       = {Zheyi Chen and Sijin Huang and Geyong Min and Zhaolong Ning and Jie Li and Yan Zhang},
  doi          = {10.1109/TMC.2025.3540407},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6315-6332},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobility-aware seamless service migration and resource allocation in multi-edge IoV systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic distributed model compression for efficient decentralized federated learning and incentive provisioning in edge computing networks. <em>TMC</em>, <em>24</em>(7), 6293-6314. (<a href='https://doi.org/10.1109/TMC.2025.3543295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study decentralized federated learning (DFL) in edge computing networks where edge nodes (ENs) collaboratively train their artificial intelligence (AI) models in a serverless manner without sharing local data. We consider the following critical DFL challenges: i) scarce bandwidth resources of ENs; ii) dynamic, heterogeneous edge environment; iii) incentive provisioning and complex tradeoffs between the DFL performance and training costs. To resolve these challenges, we develop a new model compression method where ENs utilize dynamic, non-identical compression rates to improve the communication efficiency of DFL under time-varying, heterogeneous resource constraints. We show that our method can be formulated as a graphical Markov potential game where ENs act as players deciding on their compression factors and the number of data samples used for model updates. Each EN is incentivized to participate in DFL through rewards based on the EN's contribution to training. We prove that our game has a dominant pure-strategy Nash equilibrium (NE) maximizing its potential function and propose a dynamic distributed compression algorithm in which each EN can find its dominant strategy independently. We show that this algorithm converges to the Pareto-optimal NE, representing the most efficient solution of our game enhancing the DFL performance with minimal costs.},
  archive      = {J_TMC},
  author       = {Alia Asheralieva and Dusit Niyato and Xuetao Wei},
  doi          = {10.1109/TMC.2025.3543295},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6293-6314},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic distributed model compression for efficient decentralized federated learning and incentive provisioning in edge computing networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced VR experience with edge computing: The impact of decoding latency. <em>TMC</em>, <em>24</em>(7), 6275-6292. (<a href='https://doi.org/10.1109/TMC.2025.3541741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) applications have revolutionized digital interaction by providing immersive experiences. 360$^{\circ }$ VR video streaming has experienced significant growth and popularity as a pivotal VR application. However, the combination of limited network bandwidth and the demand for high-quality videos frequently hinders the achievement of a satisfactory quality of experience (QoE). Although prior methods have enhanced QoE, the effects of decoding latency have been poorly studied. It is technically challenging to design a quality adaptation algorithm that can balance the pursuit of high-quality videos and the limitation of limited bandwidth resources. To address this challenge, we propose an edge-end architecture for 360$^{\circ }$ VR video streaming and aim to enhance overall QoE by solving a performance optimization problem. Specifically, our experiments on commercial mobile devices in real-world situations reveal that decoding latency significantly influences QoE. First, decoding latency plays a major role in contributing to end-to-end latency, which exceeds the transmission latency. Second, decoding latency can differ considerably between devices with varying computational capabilities. Building on this insight, we propose a novel latency-aware quality adaptation (LAQA) algorithm. LAQA lies in developing a solution that can allocate video quality in real-time and enhance overall QoE. LAQA involves not only the quality of the received content, the transmission latency and the quality variance, but also the decoding latency and the fairness of the user quality. Subsequently, we formulate a combinatorial optimization problem to maximize overall QoE. Through extensive validation with experimental data from real-world situations, LAQA offers a promising approach to enhance QoE and ensure fairness performance in different devices. In particular, LAQA achieves 16.77% and 10.66% enhancement over the state-of-the-art combinatorial optimization and reinforcement learning algorithm, respectively, in terms of QoE at 4K resolution. Furthermore, LAQA ensures excellent scalability by simulating the number of users ranging from 15 to 60, making it a robust solution for diverse and growing user scales.},
  archive      = {J_TMC},
  author       = {Liang Huang and Yuqi Li and Hongyuan Liang and Kaikai Chi and Yuan Wu},
  doi          = {10.1109/TMC.2025.3541741},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6275-6292},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhanced VR experience with edge computing: The impact of decoding latency},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CamTuner: Adaptive video analytics pipelines via real-time automated camera parameter tuning. <em>TMC</em>, <em>24</em>(7), 6259-6274. (<a href='https://doi.org/10.1109/TMC.2025.3540667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Video Analytics Pipelines (VAP), Analytics Units (AUs) such as object detection and face recognition operating on remote servers rely heavily on surveillance cameras to capture high-quality video streams to achieve high accuracy. Modern network cameras offer an array of parameters that directly influence video quality. While a few of such parameters, e.g., exposure, focus and white balance, are automatically adjusted by the camera internally, the others are not. We denote such camera parameters as non-automated (NAUTO) parameters. In this work, we first show that in a typical surveillance camera deployment, environmental condition changes can have significant adverse effect on the accuracy of insights from the AUs, but such adverse impact can potentially be mitigated by dynamically adjusting NAUTO camera parameters in response to changes in environmental conditions. Second, since most end-users lack the skill or understanding to appropriately configure these parameters and typically use a fixed parameter setting, we present CamTuner, to our knowledge, the first framework that dynamically adapts NAUTO camera parameters to optimize the accuracy of AUs in a VAP in response to adverse changes in environmental conditions. CamTuner is based on SARSA reinforcement learning and it incorporates two novel components: a light-weight analytics quality estimator and a virtual camera that drastically speed up offline RL training. Our controlled experiments and real-world VAP deployment show that compared to a VAP using the default camera setting, CamTuner enhances VAP accuracy by detecting 15.9% additional persons and 2.6% –4.2% additional cars (without any false positives) in a large enterprise parking lot. CamTuner opens up new avenues for elevating video analytics accuracy, transcending mere incremental enhancements achieved through refining deep-learning models.},
  archive      = {J_TMC},
  author       = {Sibendu Paul and Kunal Rao and Giuseppe Coviello and Murugan Sankaradas and Y. Charlie Hu and Srimat T. Chakradhar},
  doi          = {10.1109/TMC.2025.3540667},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6259-6274},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CamTuner: Adaptive video analytics pipelines via real-time automated camera parameter tuning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional training optimization for efficient federated synergy learning. <em>TMC</em>, <em>24</em>(7), 6243-6258. (<a href='https://doi.org/10.1109/TMC.2025.3540566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge learning (EL) is an end-to-edge collaborative learning paradigm enabling devices to participate in model training and data analysis, opening countless opportunities for edge intelligence. As a promising EL framework, federated synergy learning (FSyL) mitigates the computation and communication overhead on resource-constrained devices by offloading partial model layers to the edge server for synergistic training. Nevertheless, due to the system and statistical heterogeneity, naively using existing FSyL methods is significantly time-consuming and causes accuracy degradation. Motivated by this issue, this paper introduces a novel FSyL framework that integrates multi-dimensional training optimization and formulates the edge learning cost minimization (ELCM) problem. To tackle the ELCM efficiently, we design OL-MG, an OnLine Model Splitting and Resource Provisioning Game. Specifically, we first reformulate and decompose the original ELCM based on data quality evaluation. Then, given a model splitting decision, we determine the optimal resource provisioning in Sub-problem1, based on which optimal model splitting in Sub-problem2 is modeled as a potential game. Subsequently, we introduce a decentralized algorithm to find a Nash equilibrium (NE) solution. Furthermore, we further extend OL-MG to support a budget-aware multi-edge scenario. Extensive experiments demonstrate that the proposed mechanism significantly outperforms state-of-the-art methods in cost-saving and accuracy improvement.},
  archive      = {J_TMC},
  author       = {Shucun Fu and Fang Dong and Runze Chen and Dian Shen and Jinghui Zhang and Qiang He},
  doi          = {10.1109/TMC.2025.3540566},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6243-6258},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-dimensional training optimization for efficient federated synergy learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay-sensitive task offloading with edge caching through martingale-based deep reinforcement learning. <em>TMC</em>, <em>24</em>(7), 6225-6242. (<a href='https://doi.org/10.1109/TMC.2025.3540413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the forthcoming era of 6G networks, delay-sensitive applications for Internet of Things (IoT) are poised to become the prevailing services with ultra-reliable and low-latency (URLLC) requirements. Unlike traditional video caching, IoT-based edge caching faces unique challenges due to diverse data types, update frequencies, and computational needs, requiring integrated storage and computational resource management. To support the more stringent requirements for these innovative applications, mobile edge computing (MEC) is introduced to enhance the service reliability of delay-sensitive applications in the 6G era. However, task offloading, as an indispensable procedure in MEC, would encounter many challenges, such as network jitter and resource insufficiency, possibly leading to unpredictable queuing delays and other negative issues. To ensure reliable services in a dynamical MEC environment, the caching-enabled MEC network has emerged as a novel architecture, placing computing and storage resources in the edge network. In this paper, we investigate the caching-enabled MEC to support reliable task offloading for delay-sensitive applications, with a focus on IoT scenarios. In our system model, we formulate the task process as a two-hop tandem queuing system with limited capacity, including task transmission and computation queues. The Martingale theory is leveraged to analyze the delay violation probability in this system, demonstrating how the offloading and caching decisions affect the end-to-end (E2E) delay. Besides, task offloading and resource allocation policies are integrated to reduce high system costs, including energy consumption and cache resource rental costs. Based on the delay analysis of martingale theory, we propose an advanced deep reinforcement learning (DRL) algorithm called Dynamic Request Aware Soft Actor-Critic (DRA-SAC) algorithm to achieve minimal system costs by obtaining the optimal task offloading and resource allocation policies, including caching and computation resources. We conduct some illustrative studies to evaluate the proposed scheme. The algorithm we have put forward outperforms benchmark algorithms regarding both cache hit ratio and system cost.},
  archive      = {J_TMC},
  author       = {Chongwu Dong and Weidong Li and Zhi Zhou and Xu Chen and Zhihong Tian and Wushao Wen},
  doi          = {10.1109/TMC.2025.3540413},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6225-6242},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Delay-sensitive task offloading with edge caching through martingale-based deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Profit maximization of delay-sensitive, differential accuracy inference services in mobile edge computing. <em>TMC</em>, <em>24</em>(7), 6209-6224. (<a href='https://doi.org/10.1109/TMC.2025.3540017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Artificial Itelligence (AI) and edge computing has sparked significant interest in edge inference services. In this paper, we consider delay-sensitive, differential accuracy inference services in a Mobile Edge Computing (MEC) network while meeting user stringent delay and accuracy requirements. We formulate two novel profit maximization problems under static and dynamic settings of service request arrivals, with the aim of maximizing the accumulative profit of admitted requests. We assign differential accuracy service requests to the corresponding resolution instances of their requested service models, assuming that each resolution instance can serve up to $L\geq 1$ the same type of service requests. Since the profit maximization problem is NP-hard, we first formulate an Integer Linear Program (ILP) solution if the problem size is small or medium; otherwise, we devise a constant randomized algorithm with high probability. Then, we consider dynamic service request admissions without the knowledge of future request arrivals for a given finite time horizon, for which we develop a simple yet effective prediction mechanism to accurately predict the number of different resolution instances of each model needed, and pre-deploy the predicted number of resolution instances into cloudlets to reduce instantiating delays. We then devise an online algorithm with a provable competitive ratio for the dynamic profit maximization problem by leveraging the primal-dual dynamic updating technique. Finally, we evaluate the performance of the proposed algorithms by simulations. The simulation results demonstrate that the proposed algorithms are promising.},
  archive      = {J_TMC},
  author       = {Yuncan Zhang and Weifa Liang and Zichuan Xu and Xiaohua Jia and Yuanyuan Yang},
  doi          = {10.1109/TMC.2025.3540017},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6209-6224},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Profit maximization of delay-sensitive, differential accuracy inference services in mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust motion-guided frame sampler with interpretive evaluation for video action recognition. <em>TMC</em>, <em>24</em>(7), 6197-6208. (<a href='https://doi.org/10.1109/TMC.2025.3541580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the presence of redundancy and interference, frame sampling is a promising but challenging solution to mitigate the expensive computation of video action recognition. Although the motion prior has shown great potential for frame selection, existing motion-based strategies suffer from limitations in terms of robustness and interpretive evaluation. In this paper, we devise a robust frame sampling strategy called positive motion guided sampler (PMGSampler). It consists of two procedures, local motion capture and global motion statistics. At the local level, we propose two concepts about inter-frame motion amplitude and motion continuity, which helps to perceive the movement of subjects and identify abnormal events that may generate negative pseudo-motion information. Then, through a global analysis of the obtained local motions, the sampler becomes more sensitive to informative frames and robust to outliers. The proposed sampler can be applied to most existing models for improving recognition accuracy. We conduct extensive experiments on four widely-used benchmarks to demonstrate the superiority of our PMGSampler over other methods of the same type. In addition, to analyse how sampled frames influence action recognition, we present a visual interpretation method for video models, termed as spatio-temporal class activation map (STCAM). By introducing spatial and temporal branches, our STCAM is able to visualise the salience of spatio-temporal features. With the help of STCAM, we can further intuitively evaluate the performance of different sampling strategies.},
  archive      = {J_TMC},
  author       = {Jing Bai and Yuxiang Zhang and Yiran Wang and Zhu Xiao and Yong Xiong and Licheng Jiao},
  doi          = {10.1109/TMC.2025.3541580},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6197-6208},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust motion-guided frame sampler with interpretive evaluation for video action recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EarLock: Personal authentication system for hearables using sound leakage signals. <em>TMC</em>, <em>24</em>(7), 6183-6196. (<a href='https://doi.org/10.1109/TMC.2025.3540584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Earphone-type wearable devices, also known as “hearables,” will have many functions in the future. Some of those functions will require authentication of the wearer for access to the user's privacy information or settlement of payments. In this study, we propose a new personal authentication system for hearables called EarLock. EarLock authenticates the wearer by acquiring and analyzing ear canal and auricle shape information using sound leakage from the device. The system can be implemented using a speaker and external microphone that are highly compatible with hearables. We implemented three prototype devices and investigated EarLock's authentication performance under various practical scenarios, including walking conditions, noisy environments, and situations with object interference. Experimental results showed that the in-ear, open-ear, and bone-conduction devices achieved balanced accuracy (BAC) scores of 87.2–93.7%, 83.4–94.7%, and 85.9–90.0%.},
  archive      = {J_TMC},
  author       = {Takashi Amesaka and Hiroki Watanabe and Yuta Sugiura and Masanori Sugimoto and Buntarou Shizuki},
  doi          = {10.1109/TMC.2025.3540584},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6183-6196},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EarLock: Personal authentication system for hearables using sound leakage signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint frame drop and object detection task offloading for mobile devices via RL with lyapunov optimization. <em>TMC</em>, <em>24</em>(7), 6168-6182. (<a href='https://doi.org/10.1109/TMC.2025.3539356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection has become an increasingly important application for mobile devices. However, state-of-the-art object detection relies heavily on deep neural network, which is often burdensome to compute on mobile devices. To this end, we develop a layering framework for joint video frame drop and object detection task offloading. In the lower layer, by invoking Lyapunov optimization, we devise an algorithm for partitioning and offloading the computation tasks of deep neural networks. This algorithm also specifies the flow control for admitting the application traffic into the network. In the upper layer, we use the flow control as a form of guidance in the action space in order to develop a reinforcement learning (RL) algorithm that selectively drops video frames with object detection performance in consideration. By the nature of design, this Lyapunov-guided RL guarantees the network stability. We show through simulations that our Lyapunov-guided RL drops video frames with reasonable object detection performance and reduced latency while keeping the network stable. We also implemented our algorithm on the remote-controlled (RC) car equipped with microprocessor and GPU, and demonstrate the applicability of our algorithm to real-time object detection tasks from the video stream generated as the RC car moves.},
  archive      = {J_TMC},
  author       = {Vaughn Sohn and Suhwan Kim and Hyang-Won Lee},
  doi          = {10.1109/TMC.2025.3539356},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6168-6182},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint frame drop and object detection task offloading for mobile devices via RL with lyapunov optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust task offloading and resource allocation under imperfect computing capacity information in edge intelligence systems. <em>TMC</em>, <em>24</em>(7), 6154-6167. (<a href='https://doi.org/10.1109/TMC.2025.3539296'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge intelligence systems, task offloading and resource allocation policies critically depend on the required computing capacity of the task, which can only be accurately measured after execution, presenting significant design challenges. In this paper, we address the problem of robust task offloading and resource allocation under imperfect computing capacity information, where the exact value as well as distribution knowledge of the required computing capacity cannot be obtained in advance. Specifically, we formulate the energy-time cost (ETC) minimization problem using min-max robust optimization. To tackle this challenging issue, we propose a decoupling method. This method first assumes the offloading policy is predetermined and derives two independent subproblems: local ETC and edge ETC. Then, we provide a closed-form optimal solution for the local ETC problem. The edge ETC problem is equivalently transformed into a geometric programming (GP) problem, and we introduce an effective iterative algorithm to obtain a stationary point, utilizing successive convex approximation (SCA). Finally, we design a coordinate descent (CD)-based algorithm to optimize the offloading policy effectively. Extensive simulations demonstrate that the proposed policy significantly outperforms other benchmark methods, achieving near-optimal performance even in the presence of high estimation errors in computing capacity.},
  archive      = {J_TMC},
  author       = {Zhaojun Nan and Yunchu Han and Jintao Yan and Sheng Zhou and Zhisheng Niu},
  doi          = {10.1109/TMC.2025.3539296},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6154-6167},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust task offloading and resource allocation under imperfect computing capacity information in edge intelligence systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pushing the limits of WiFi-based gait recognition towards non-gait human behaviors. <em>TMC</em>, <em>24</em>(7), 6137-6153. (<a href='https://doi.org/10.1109/TMC.2025.3540863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi-based gait recognition technologies have seen significant advancements in recent years. However, most existing approaches rely on a critical assumption: users must walk continuously and maintain a consistent body posture. This poses a substantial challenge when users engage in non-periodic or discontinuous behaviors (e.g., stopping, starting, or turning mid-walk), which can disrupt the extraction of gait-related features and degrade recognition performance. To address this issue, we propose freeGait, a novel approach designed to mitigate the impact of non-gait behaviors in WiFi-based gait recognition systems. Our solution models this problem as domain adaptation, where we learn domain-independent representations to isolate gait features from behavior-dependent noise. We treat human behaviors with labeled user data as source domains and behaviors without user labels as target domains. However, applying domain adaptation directly is challenging due to the ambiguous classification boundaries in the target domains for WiFi signals. To overcome this, we align the posterior distributions between the source and target domains and constrain the conditional distribution within the target domains to enhance gait classification accuracy. Additionally, we implement a data augmentation module to generate data resembling the labeled data, while supervised learning ensures distinctiveness between users. Our experiments, conducted with 20 participants across 3 different scenarios, demonstrate that freeGait can accurately predict data across 15 domains by labeling only a small subset from 6 source domains, achieving up to a 45% improvement in user classification accuracy compared to existing methods.},
  archive      = {J_TMC},
  author       = {Dawei Yan and Panlong Yang and Fei Shang and Feiyu Han and Yubo Yan and Xiang-Yang Li},
  doi          = {10.1109/TMC.2025.3540863},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6137-6153},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Pushing the limits of WiFi-based gait recognition towards non-gait human behaviors},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D cooperative positioning via RIS and sidelink communications with zero access points. <em>TMC</em>, <em>24</em>(7), 6119-6136. (<a href='https://doi.org/10.1109/TMC.2025.3541575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable intelligent surfaces (RISs) are expected to be a main component of future 6G networks due to their capability to create a controllable wireless environment, achieve extended coverage, and improve localization accuracy. In this paper, we present a novel cooperative positioning use case of the RIS in mmWave frequencies and show that in the presence of RIS, together with sidelink communications, localization with zero access points (APs) is possible. We show that multiple (at least three) half-duplex single-antenna user equipments (UEs) can cooperatively estimate their positions through device-to-device communications with a single RIS as an anchor without the need for any APs. We start by formulating a three-dimensional positioning problem with Cramér-Rao lower bound (CRLB) derived for performance analysis. After that, we discuss the RIS profile design and the power allocation strategy between the UEs. Then, we propose low-complexity estimators for estimating the channel parameters and UEs’ positions. Finally, we evaluate the performance of the proposed estimators and RIS profiles in the considered scenario via extensive simulations and show that sub-meter level positioning accuracy can be achieved under multi-path propagation.},
  archive      = {J_TMC},
  author       = {Mustafa Ammous and Hui Chen and Henk Wymeersch and Shahrokh Valaee},
  doi          = {10.1109/TMC.2025.3541575},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6119-6136},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {3D cooperative positioning via RIS and sidelink communications with zero access points},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-channel analog beamforming transceiver for mmWave communications. <em>TMC</em>, <em>24</em>(7), 6106-6118. (<a href='https://doi.org/10.1109/TMC.2025.3539169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an analog multi-channel millimeter-wave transceiver architecture that offers advantages in terms of low hardware complexity and computational efficiency compared to digital beamforming and hybrid beamforming techniques. Also, it is known that analog beamforming with a single phase-shifter network faces limitations in maintaining consistent accuracy across a wideband spectrum. To this end, the proposed architecture leverages the inherent bandwidth-splitting property of the multi-channel transceiver. Thus, each sub-band signal is processed by its corresponding channel in the transceiver with an independent analog beamformer per channel. This approach can significantly improve the beamforming accuracy in a wideband communication system such as 5G and future 6G cellular networks. The simulation results demonstrate that increasing the channels in the multi-channel transceiver enables multi-channel analog beamforming to achieve a comparable bit-error-rate (BER) performance to digital beamforming when interference is not considered. Moreover, when interference is present, the proposed multi-channel analog beamforming exhibits enhanced resilience to high power interference compared with digital beamforming with limited analog-to-digital conversion resolution.},
  archive      = {J_TMC},
  author       = {Haotian Zhao and Kamran Entesari and Sebastian Hoyos},
  doi          = {10.1109/TMC.2025.3539169},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6106-6118},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-channel analog beamforming transceiver for mmWave communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topology-aware microservice architecture in edge networks: Deployment optimization and implementation. <em>TMC</em>, <em>24</em>(7), 6090-6105. (<a href='https://doi.org/10.1109/TMC.2025.3539312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a ubiquitous deployment paradigm, integrating microservice architecture (MSA) into edge networks promises to enhance the flexibility and scalability of services. However, it also presents significant challenges stemming from dispersed node locations and intricate network topologies. In this paper, we have proposed a topology-aware MSA characterized by a three-tier network traffic model encompassing the service, microservices, and edge node layers. This model meticulously characterizes the complex dependencies between edge network topologies and microservices, mapping microservice deployment onto link traffic to accurately estimate communication delay. Building upon this model, we have formulated a weighted sum communication delay optimization problem considering different types of services. Then, a novel topology-aware and individual-adaptive microservices deployment (TAIA-MD) scheme is proposed to solve the problem efficiently, which accurately senses the network topology and incorporates an individual-adaptive mechanism in a genetic algorithm to accelerate the convergence and avoid local optima. Extensive simulations show that, compared to the existing deployment schemes, TAIA-MD improves the communication delay performance by approximately 30% to 60% and effectively enhances the overall network performance. Furthermore, we implement the TAIA-MD scheme on a practical microservice physical platform. The experimental results demonstrate that TAIA-MD achieves superior robustness in withstanding link failures and network fluctuations.},
  archive      = {J_TMC},
  author       = {Yuang Chen and Chang Wu and Fangyu Zhang and Chengdi Lu and Yongsheng Huang and Hancheng Lu},
  doi          = {10.1109/TMC.2025.3539312},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6090-6105},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Topology-aware microservice architecture in edge networks: Deployment optimization and implementation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinating communication and computing for wireless VR in open radio access networks. <em>TMC</em>, <em>24</em>(7), 6075-6089. (<a href='https://doi.org/10.1109/TMC.2025.3540111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by diverse applications, radio access networks (RAN) are expected to embrace built-in computing and intelligence, forming a versatile wireless computing platform that closely integrates communication and computing. To fully unleash the potential of such a synergistic system, it is essential to coordinate communication and computing with intelligence unlocked by the radio intelligent controllers (RICs) in O-RAN. Building on the groundwork established by existing theoretical studies and simulations, we develop a platform that can emulate the events in the real-world system in more detail, bringing theoretical works closer to practical implementation. In this paper, we first introduce ns-GP-O-RAN, a software simulation platform developed over ns-3, enabling communication, computation task processing, large-scale data collection, and testing of system-level orchestration policies through user-level control. Taking virtual reality (VR) as an example, we formulate the computation offloading problem and develop a prediction-based computation offloading xAPP, which contains a prediction phase to predict users’ end-to-end (E2E) performance with the deep neural network and a system-level decision-making phase for global orchestration with the differential evolution algorithm. We evaluate the system capacity and E2E latency over the developed ns-GP-O-RAN, which is more effective than existing approaches.},
  archive      = {J_TMC},
  author       = {Hongtao Li and Ziqi Chen and Fengxian Guo and Nan Li and Yaohua Sun and Mugen Peng and Yuanwei Liu},
  doi          = {10.1109/TMC.2025.3540111},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6075-6089},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Coordinating communication and computing for wireless VR in open radio access networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCMM: Unsupervised convolutional networks for accurate and efficient map matching with mobile cellular data. <em>TMC</em>, <em>24</em>(7), 6062-6074. (<a href='https://doi.org/10.1109/TMC.2025.3540300'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The map matching of cellular data reconstructs real trajectories of users by exploiting the sequential connections between mobile devices and cell towers. The difficulty in obtaining paired cellular-GPS data and the cellular variation compromise the accuracy and reliability of existing map matching approaches. In this paper, we propose a novel unsupervised convolutional network for cellular map matching (UCMM) to address these challenges. UCMM employs a dual encoder-decoder network to capture a shared representation from both the cellular and GPS domains in an unsupervised manner. It leverages a dedicated convolutional architecture to tackle the varying lengths of output sequential data. An attention mechanism is specially introduced to deal with the cellular variation. The effectiveness of UCMM is demonstrated through comprehensive evaluations, which show that UCMM achieves a substantial improvement in matching accuracy and deduction of training time compared with the best-known prior works. These improvements make UCMM a significant advancement in the field of map matching.},
  archive      = {J_TMC},
  author       = {Mingxin Cai and Chen Ma and Yuchen Li and Zhonghao Lyu and Yutong Liu and Linghe Kong and Guihai Chen},
  doi          = {10.1109/TMC.2025.3540300},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6062-6074},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UCMM: Unsupervised convolutional networks for accurate and efficient map matching with mobile cellular data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmRotation: Unlocking versatility of a single mmWave radar via azimuth panning and elevation tilting. <em>TMC</em>, <em>24</em>(7), 6045-6061. (<a href='https://doi.org/10.1109/TMC.2025.3539985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor mmWave-based sensing technologies have garnered substantial interest from both the industrial and academic. Yet, the intrinsic challenge posed by the limited Field-of-View (FOV) of mmWave radars significantly restricts their coverage. This limitation necessitates careful selection of installation positions and orientations to optimize performance, thereby severely curtailing the versatility and widespread adoption of these systems. Traditionally, expanding coverage involved increasing the number of radar units. This paper introduces a novel approach to enhance the FOV by incorporating mobility, achieved by affixing the radar onto a pan-tilt unit capable of rotating along both the horizontal and azimuthal. Nevertheless, the disparity between the pan-tilt and the radar presents significant challenges for accurately rotating the radar's orientation. To mitigate this, we propose an automated calibration algorithm for radar and pan-tilt, ensuring precise calibration. Additionally, we have devised a radar orientation adjustment algorithm intended to automatically align the radar's FOV with the positions of detected objects to facilitate various applications. Through three case studies, we have demonstrated that mmRotation can greatly expand the sensing range, enabling support for multiple applications on a single radar, such as vital signs monitoring and fall detection. Comprehensive experimental results underscore that our system surpasses the current state-of-the-art (SOTA).},
  archive      = {J_TMC},
  author       = {Duo Zhang and Xusheng Zhang and Zhehui Yin and Yaxiong Xie and Hewen Wei and Zhaoxin Chang and Wenwei Li and Daqing Zhang},
  doi          = {10.1109/TMC.2025.3539985},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6045-6061},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmRotation: Unlocking versatility of a single mmWave radar via azimuth panning and elevation tilting},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring MEC server strategy in blockchain networks: Mining for mobile users or for self. <em>TMC</em>, <em>24</em>(7), 6030-6044. (<a href='https://doi.org/10.1109/TMC.2025.3544311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-based decentralized applications (DApps) offer enhanced security and decentralization features; nevertheless, their maintenance demands substantial computational resources and poses challenges for deployment in mobile networks. To address this, a number of studies have explored offloading blockchain mining tasks from mobile users to mobile edge computing (MEC) servers. However, the existing literature overlooks the fact that MEC servers can not only mine for mobile users but also mine for themselves, potentially explaining why MEC mining offloading has not gained broad acceptance within the industry. In this work, we exploit a more practical case and rethink the question of whether MEC servers lease computing power to mobile users by taking into account that MEC servers can mine for themselves. We establish a game model and apply backward induction to analytically characterize Nash equilibria for mining strategies adopted by MEC and mobile users. Our findings suggest that, if MEC can mine for self, MEC would mine for mobile users only under specific conditions where mobile users possess superior information gathering capability (at least better than the MEC server) or the whole blockchain system exhibits significant network value. We further provide a series of simulations to verify our conclusion and illustrate the impact of network parameters on the strategies of both sides.},
  archive      = {J_TMC},
  author       = {Xintong Ling and Rui Jiang and Weihang Cao and Mingkai Chen and Jiaheng Wang and Zhi Ding and Xiqi Gao},
  doi          = {10.1109/TMC.2025.3544311},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6030-6044},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring MEC server strategy in blockchain networks: Mining for mobile users or for self},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information freshness in multi-hop satellite IoT systems. <em>TMC</em>, <em>24</em>(7), 6014-6029. (<a href='https://doi.org/10.1109/TMC.2025.3540259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-air-ground integration has become paramount in the next generation of wireless communication systems in the era marked by the seamless integration of terrestrial and celestial domains. On the other hand, the age of information (AoI) has recently emerged as a vital metric for evaluating the timeliness and freshness of data in these multi-hop communication systems. This paper focuses on investigating the information freshness of multi-hop satellite IoT systems while considering several automatic repeat request (ARQ) and hybrid ARQ (HARQ) schemes over different hops to promise the reliability of data transmissions. Specifically, a group of remote sensors transmits their data to a terrestrial base station (B) via the code-division multiple access (CDMA) strategy to exploit CDMA’s natural merits, e.g., anti-jamming and simultaneous transmissions. Then, B sends these received data to a data destination (D) via a satellite (R) under the transparent forwarding strategy. We derive the closed form of the outage probability for the CDMA protocol considering multi-user interference (MUI) and the closed form of the end-to-end outage probability for the three kinds of ARQ and HARQ schemes on the dual-hop B-R-D transmission, and then finally derive the expression of the corresponding AoI. Finally, numerical results show that simulations match well with the theoretical results, proving the analysis’s correctness and the pros and cons of the different ARQ/HARQ schemes.},
  archive      = {J_TMC},
  author       = {Ying Ke and Zihan Ni and Di Zhang and Xiaqing Miao and Chee Yen Leow and Shuai Wang and Gaofeng Pan and Jianping An},
  doi          = {10.1109/TMC.2025.3540259},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {6014-6029},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Information freshness in multi-hop satellite IoT systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing monitoring utility of uncrewed aerial vehicles considering adverse effects. <em>TMC</em>, <em>24</em>(7), 5996-6013. (<a href='https://doi.org/10.1109/TMC.2025.3543399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Unmanned Aerial Vehicles (UAVs) monitoring tasks, capturing high quality images of target objects is important for subsequent recognition. Concerning the problem, many prior works study placement/trajectory planning for UAVs to maximize the quality of captured images. However, all of them overlook a fact that UAV monitoring may cause a huge risk/annoyance on living objects. In this paper, we investigate the novel problem of oPtimizing uncrewed aErial vehicles plAcement by Considering both monitoring utility and adverse Effects (PEACE). We propose an approach to solve PEACE, which is proved to be NP-hard. Overall, our approach achieves a $1- \frac{1}{e}-\varepsilon$ approximation ratio. First, we approximate the original problem of PEACE as a classical problem of Monotone Submodular function Maximization under a Uniform Matroid constraint (MSMUM) with a controlled gap. Then, for MSMUM, we propose a combination of algorithms achieving a $1-\frac{1}{e}$ approximation and $O(n\log n)$ time complexity considering the correlation among the UAV monitoring strategies. The proposed algorithms outperform existing algorithms for MSMUM through theoretical analysis and experimental results. Extensive simulations and field experiments demonstrate the effectiveness of our approach, achieving performance gains of 9.0% to 1434.5% compared to existing methods.},
  archive      = {J_TMC},
  author       = {Haihan Zhang and Haipeng Dai and Yu Qiu and Enze Yu and Ruiben Zhou and Weijun Wang and Jingwu Wang and Guihai Chen},
  doi          = {10.1109/TMC.2025.3543399},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5996-6013},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing monitoring utility of uncrewed aerial vehicles considering adverse effects},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aeacus: QUIC-powered low-latency and strong-consistency name resolution in 5G. <em>TMC</em>, <em>24</em>(7), 5981-5995. (<a href='https://doi.org/10.1109/TMC.2025.3539590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Domain Name System (DNS) serves as a foundational networking service, yet its inherent time-to-live (TTL)-based cache mechanism presents a conundrum—striving for both low query latency and robust cache consistency proves challenging. To address this, we introduce Aeacus: a middleware seamlessly integrated into the 5G core, engineered to optimize name resolution for QUIC. Aeacus adeptly fortifies DNS with substantial cache consistency by capitalizing on QUIC handshake states to detect cache inconsistency, without compromising query delay. Furthermore, Aeacus orchestrates the amalgamation of DNS queries and QUIC handshake messages, effectively truncating one round-trip of message exchange and reviving expired DNS cache to bolster cache hit rates. Our dual-pronged deployment, encompassing both commercial and test 5G networks, demonstrates Aeacus’ prowess. In direct comparison with DNS, Aeacus successfully truncates connection setup delays by a remarkable 8.9% to 71.8%, all while introducing a mere 5.9% overhead attributed to supplementary packet processing and forwarding expenses. Importantly, existing DNS-based systems reap the benefits of Aeacus without necessitating modifications. We demonstrate Aeacus’ seamless enhancement of DNS-based load balancers, extending QUIC's 0-RTT handshake to include 0-RTT connection setup and service migration.},
  archive      = {J_TMC},
  author       = {Xuebing Li and Byungjin Cho and Saimanoj Katta and Jose Costa Requena and Yu Xiao},
  doi          = {10.1109/TMC.2025.3539590},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5981-5995},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Aeacus: QUIC-powered low-latency and strong-consistency name resolution in 5G},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Realistic facial expression reconstruction using millimeter wave. <em>TMC</em>, <em>24</em>(7), 5964-5980. (<a href='https://doi.org/10.1109/TMC.2025.3540877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technology of facial expression reconstruction has paved the way for various face-centric applications such as virtual reality (VR) modeling, human-computer interaction, and affective computing. Existing vision-based solutions present challenges in privacy leakage and poor lighting conditions. In this paper, we introduce a nonintrusive facial expression reconstruction system, mm3DFace, which uses a millimeter wave (mmWave) radar to reconstruct facial expressions in a privacy-preserving and passive manner. mm3DFace first captures and pre-processes mmWave signals reflected by a human face, and extracts intricate facial geometric features using a ConvNeXt model integrated with triple loss embedding. Subsequently, mm3DFace derives pose-invariant facial representations utilizing region-divided affine transformation, and further generates individual facial shapes with 68 facial landmarks. Then, dynamic facial expressions with 3D facial avatars are reconstructed to exhibit realistic facial expressions. Finally, mm3DFace enables micro-expression recognition with mmWave signals, which ensures the capability of describing tiny facial changes. Through extensive real-world experiments involving 15 participants, mm3DFace achieves a normalized mean error of 3.94%, a mean absolute error of 2.30 mm, and a 3D-mean absolute error of 4.10 mm in tracking 68 facial landmarks, which demonstrates the efficacy and practicality of mm3DFace in real-world 3D facial reconstruction scenarios.},
  archive      = {J_TMC},
  author       = {Hao Kong and Jiahong Xie and Jiadi Yu and Yingying Chen and Linghe Kong and Yanmin Zhu and Feilong Tang},
  doi          = {10.1109/TMC.2025.3540877},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5964-5980},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Realistic facial expression reconstruction using millimeter wave},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual fine-grained authentication without trusted authority for data collection in TDT systems. <em>TMC</em>, <em>24</em>(7), 5951-5963. (<a href='https://doi.org/10.1109/TMC.2025.3539281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In transportation 5.0, digital twin (DT) is considered a promising paradigm to integrate physical entities into cyber physical systems by collecting massive data. However, the open collection process and key exposure issues bring critical security challenges. Furthermore, applying the existing authentication schemes to data collection in transportation DT (TDT) systems encounters three deficiencies: 1) forward security for collected data can only be achieved at a coarse-grained level; 2) one or more additional trusted authorities are introduced, causing the robustness of TDT systems to be downgraded; 3) dynamic attribute updating and revocability of physical entities are rarely considered. Therefore, we propose a dual fine-grained authentication scheme (DFAS) in this paper. Our DFAS can not only ensure data integrity and authenticity but also enable fine-grained access control, namely, only registered physical entities with authorized attributes can generate valid signatures. Meanwhile, DFAS provides the key puncturing for physical entities to guarantee fine-grained forward security without relying on any trusted authority. In addition, a non-interactive attribute updating and revocation of malicious entities are realized in DFAS. Finally, the security analysis indicates that DFAS can deal with various security challenges for data collection in TDT systems. The performance evaluation demonstrates that DFAS is efficient and practical.},
  archive      = {J_TMC},
  author       = {Chenhao Wang and Yang Ming and Hang Liu and Yutong Deng},
  doi          = {10.1109/TMC.2025.3539281},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5951-5963},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dual fine-grained authentication without trusted authority for data collection in TDT systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environment independent gait recognition based on wi-fi signals. <em>TMC</em>, <em>24</em>(7), 5934-5950. (<a href='https://doi.org/10.1109/TMC.2025.3540011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition plays a pivotal role in the area of mobile computing. While various research approaches leverage images, radar, RF signals, pressure sensors, wearables, and other methods, utilizing Wi-Fi signals for gait recognition offers distinct advantages such as a wide sensing range, simple deployment, and passive sensing capabilities. However, traditional gait recognition systems relying on Wi-Fi signals often suffer from performance degradation due to variations in walking directions and environmental conditions. To address this issue, in this paper we propose EIGait, a gait recognition system based on Wi-Fi signal time-frequency spectrograms. EIGait enhances the robustness and generalizability of extracted features through spectrogram augmentation, self-contrastive learning, and domain-adversarial training. Particularly, improvements to ResNet in EIGait yield a Spectrogram ResNet, which is better suited for time-frequency spectrograms. In addition, using merely a single pair of Wi-Fi transmitter and receiver, and by minimal signal denoising, we achieve the state-of-the-art performance. To evaluate the performance of EIGait, we conduct extensive experiments. In a typical indoor environment, EIGait achieves F1 scores ranging from 98.11% to 98.31% for four to eight individuals. In cross-direction gait recognition, we obtain F1 scores of 96.64% to 94.45% for four to eight individuals. Moreover, under the more challenging conditions of cross-room gait recognition, EIGait attains F1 scores of 92.09% to 89.61% for four to eight individuals. Additionally, we conduct experiments on the public dataset 3.0, and the results also demonstrate significant superiority.},
  archive      = {J_TMC},
  author       = {Wei Yang and Zhixiang Li and Sheng Chen},
  doi          = {10.1109/TMC.2025.3540011},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5934-5950},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Environment independent gait recognition based on wi-fi signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing federated semantic learning in distributed AIGC-enabled human digital twins: A multi-criteria and multi-shard user selection framework. <em>TMC</em>, <em>24</em>(7), 5916-5933. (<a href='https://doi.org/10.1109/TMC.2025.3541191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence-generated content (AIGC) has been proposed as a solution to meet the requirements of ultra-reliable, secure, and privacy-preserving connectivity in human digital twin (HDT) networks. In such an AIGC-enhanced HDT, contents representing the true statuses of physical twins are generated in the virtual environment for the immediate update and evolution of the corresponding virtual twins (VTs). However, adopting a distributed AIGC in HDT presents several challenges, including the need for personalized VTs, data privacy concerns, and insufficient contextual understanding. This paper introduces a multi-layer federated semantic learning framework to address these challenges, incorporating batch learning to meet the training requirements for semantic-channel encoders and decoders. Furthermore, we introduce a novel user association framework to maximize the overall system performance under shard formation constraints. We then formulate a long-term joint optimization problem for user selection over finite learning periods. A novel Lyapunov-based online optimization strategy was proposed to mitigate the impact of time-varying and unpredictable training conditions. Additionally, we introduce a multi-arm bandit-based method and a context-centric user selection approach to solve the optimization problem. The results demonstrate that the proposed user association framework addresses the limitations of existing approaches, thereby improving the overall performance of the multi-shard AIGC-enhanced HDT.},
  archive      = {J_TMC},
  author       = {Samuel D. Okegbile and Haoran Gao and Oluwasegun Talabi and Jun Cai and Dusit Niyato and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3541191},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5916-5933},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing federated semantic learning in distributed AIGC-enabled human digital twins: A multi-criteria and multi-shard user selection framework},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive 360-degree streaming: Optimizing with multi-window and stochastic viewport prediction. <em>TMC</em>, <em>24</em>(7), 5903-5915. (<a href='https://doi.org/10.1109/TMC.2025.3541748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tile-based approach is widely adopted in adaptive 360-degree video streaming systems, due to its efficiency in managing limited bandwidth resources. Recently, significant research efforts have been devoted to viewport-prediction-enabled bitrate adaptation for tile-based 360-degree Adaptive Bit-Rate (ABR) streaming, towards improving the average video quality while reducing rebuffering. However, the inherent uncertainty of users’ viewports has posed limitations on users’ Quality of Experience (QoE) for tile-based 360-degree ABR streaming. In this paper, we introduce a multi-window and stochastic viewport prediction approach to address the viewport uncertainty. In particular, considering our goal of maximizing the expectation of future QoE, we investigate a viewport distribution prediction model, to cope with the inherent randomness. Additionally, to accommodate the varying gap between the playback and the download process, we explore the multiple-window viewport prediction models to capture different prediction gaps. Even with the utilization of distributional prediction and multi-window models, predicting viewports far into the future is still inherently challenging. Accordingly, we propose a patience pattern temporarily suspending the download process, allowing for the accumulation of additional head movement trajectory data. Finally, we employ a model predictive control (MPC) approach for sequential decision-making, formulating the MPC problem as a mixed-integer non-linear programming (MINLP) task. To mitigate the computational burden associated with solving MINLP, we introduce a mixed-integer linear programming transformation to achieve efficient decision-making. Extensive experiments, utilizing real-world traces and user head movement trajectories, demonstrate that the proposed method outperforms state-of-the-art methods, improving overall QoE performance by 16.75% –18.91% .},
  archive      = {J_TMC},
  author       = {Weichao Feng and Shuoyao Wang and Yu Dai},
  doi          = {10.1109/TMC.2025.3541748},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5903-5915},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive 360-degree streaming: Optimizing with multi-window and stochastic viewport prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Average reliability-optimal offloading for mobile edge computing in low-latency industrial IoT networks. <em>TMC</em>, <em>24</em>(7), 5888-5902. (<a href='https://doi.org/10.1109/TMC.2025.3541661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a multi-access mobile edge computing (MEC) network with multiple sensors and one MEC server in industrial Internet of Things networks, where the MEC server provides a joint computation service (in the computation phase) for a set of sub-tasks offloaded by different sensors (in the communication phase). Due to the requirements of low latency and ultra reliability, we utilize finite blocklength information theory to characterize the reliability of the communication phase and exploit extreme value theory to investigate the delay violation probability in the computation phase. Following these characterizations, we derive the average end-to-end error probability of the entire service and provide two average end-to-end reliability-optimal design frameworks considering fixed frames structure and dynamic frames structure, in both of which the goal is to minimize the average end-to-end error probability by optimally allocating the total time length to each frame, as well as allocating each frame length to the communication phase and the computation phase. For the fixed frames structure, the original problem is decomposed, and the joint convexity of the decomposed sub-problems is rigorously proved, and the optimal solutions are obtained by the proposed optimal time allocation algorithm. Moreover, for the dynamic frames structure, we reformulate the optimization problem by introducing an average time constraint. By exploiting Lagrange multipliers, we transform the reformulated optimization problem into a dual problem with strong duality, the solutions of which can be obtained by the proposed time allocation algorithm. Via simulations, we validate the proven convexity and the approximation in our analytical model and evaluate the performance for both fixed frames length structure and dynamic frames length structure.},
  archive      = {J_TMC},
  author       = {Jie Wang and Yao Zhu and Yulin Hu and M. Cenk Gursoy and Anke Schmeink},
  doi          = {10.1109/TMC.2025.3541661},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5888-5902},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Average reliability-optimal offloading for mobile edge computing in low-latency industrial IoT networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PicaCAN: Reverse engineering physical semantics of signals in CAN messages using physically-induced causalities. <em>TMC</em>, <em>24</em>(7), 5871-5887. (<a href='https://doi.org/10.1109/TMC.2025.3541102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Connected and Autonomous Vehicles, In-Vehicle Network attacks have garnered heightened research scrutiny due to vehicles’ increasing connectivities to the external environment. The common characteristic among these attacks is to tamper with targeted powertrain-related signals in the Powertrain Controller Area Network (PT-CAN) and further physically threaten vehicles’ safety. These powertrain-related signals are encoded within CAN messages grounded by the syntax specification, which is proprietary to Original Equipment Manufacturers and publicly unavailable. Thus, to undertake comprehensive security analysis and strategies, reverse engineering PT-CAN to the semantic level is urgently needed. However, the existing methods rely on interactions (injecting challenge signals/actions) with the targeted vehicle, and certain manual efforts are required. To fill this gap, we propose PicaCAN, a novel framework to extract signals from CAN messages and reverse engineer their physical semantics based on physically induced causality. Once access to the CAN traffic, PicaCAN offers the researcher an eye on the vehicle’s powertrain system, decoding binaries flows into powertrain-related signals automatically. We experimentally evaluate PicaCAN on PT-CAN of three automobiles containing two power types. The experimental results show that PicaCAN could successfully extract physical signals representing all targeted semantics (pedals, engine speed, etc.) from two Internal Combustion Engine Vehicles and one Hybrid Electric Vehicle under EV mode.},
  archive      = {J_TMC},
  author       = {Yucheng Ruan and Chengcheng Zhao and Zeyu Yang and Yuanchao Shu and Peng Cheng and Jiming Chen},
  doi          = {10.1109/TMC.2025.3541102},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5871-5887},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PicaCAN: Reverse engineering physical semantics of signals in CAN messages using physically-induced causalities},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time distributed charging station recommendation for electric vehicles: A federated meta-RL approach. <em>TMC</em>, <em>24</em>(7), 5857-5870. (<a href='https://doi.org/10.1109/TMC.2025.3539496'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of Electric Vehicles (EVs) places an increasingly heavy burden on the limited charging infrastructure, necessitating an effective charging station recommendation strategy that assists EVs in finding the most suitable charging stations. Deep reinforcement learning is a promising technology that has been applied to optimize EVs’ charging recommendations. However, existing schemes have low scalability and high communication costs as they usually require collecting real-time information on both charging requests and charger availability at various stations during policy training or execution. To address this challenge, we develop a real-time distributed charging station recommendation approach, named ReDirect, to minimize the charging duration experienced by EVs, considering dynamic charging requests of EVs and fluctuating availability at charging stations. ReDirect employs federated meta-reinforcement learning (RL) to empower distributed stations to collaboratively learn effective recommendation strategies and make decisions without sharing their local information, yielding improved scalability, reduced communication overhead, and enhanced data privacy. Furthermore, we conduct a rigorous theoretical analysis of the convergence performance of ReDirect. Extensive experimental results on real-world datasets demonstrate that ReDirect performs closely to the centralized recommendation algorithm and outperforms several state-of-the-art distributed algorithms in EV charging duration while realizing a balanced distribution of charging requests across multiple stations.},
  archive      = {J_TMC},
  author       = {Yongchao Zhang and Jia Hu and Geyong Min and Jie Gao and Nektarios Georgalas},
  doi          = {10.1109/TMC.2025.3539496},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5857-5870},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time distributed charging station recommendation for electric vehicles: A federated meta-RL approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent deep reinforcement learning with trajectory prediction for task migration-assisted computation offloading. <em>TMC</em>, <em>24</em>(7), 5839-5856. (<a href='https://doi.org/10.1109/TMC.2025.3539945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing has become an effective paradigm to provide offloading services for computation-intensive and delay-sensitive tasks on vehicles. However, high mobility of vehicles usually incurs spatio-temporal load-imbalances among edge servers. Therefore, task migration is employed to maintain dynamic workload balancing by transmitting excessive tasks from overloaded to underloaded servers. Recent studies adopt deep reinforcement learning approaches to generate offloading and migration decisions based on current observations of systems. However, we argue that the migration direction is highly dependent on vehicular movements, and task migration towards the wrong direction could lead to additional delays. Therefore, we emphasize the importance of guiding task migration via exploring prospective trajectories of vehicles. We propose a Mobility-Aware Cooperative Multi-Agent (MCMA) deep reinforcement learning approach to make vehicle-by-vehicle decisions in multi-edge computation offloading scenarios. A two-stage decision framework is designed to solve the joint optimization problem of computation offloading and resource allocation. Additionally, an Informer-based multi-step vehicular trajectory prediction module is incorporated to enhance the capability of forecasting vehicular movements. Extensive experiments and analysis are conducted on synthetic and realistic scenarios, showing that our approach consistently outperforms both heuristic and DRL-based methods. The simulation scenarios and source codes are publicly available here.},
  archive      = {J_TMC},
  author       = {Xinyi Zhang and Chunyang Wang and Yanmin Zhu and Jian Cao and Tong Liu},
  doi          = {10.1109/TMC.2025.3539945},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5839-5856},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-agent deep reinforcement learning with trajectory prediction for task migration-assisted computation offloading},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A wearable PPG-based monitoring system for personalized free weight training. <em>TMC</em>, <em>24</em>(7), 5824-5838. (<a href='https://doi.org/10.1109/TMC.2025.3540165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free weight training (FWT) is of utmost importance for physical well-being. The success of FWT depends largely on choosing the suitable workload, as improper selections can lead to suboptimal outcomes or injury. Current workload estimation approaches rely on manual recording and specialized equipment with limited feedback. Therefore, we introduce PPGSpotter, a wearable PPG-based FWT monitoring system in a convenient, low-cost, and fine-grained manner. By characterizing the arterial geometry compressions caused by the deformation of distinct muscle groups, PPGSpotter can infer essential FWT factors such as current workload, repetitions, and exercise type and provide recommendations for workload adjustment. To remove pulse-related interference, we develop an arterial interference elimination approach based on adaptive filtering, effectively extracting the pure motion-derived signal (MDS). Furthermore, we explore 2D representations of MDS within the phase space to extract spatiotemporal information, enabling PPGSpotter to address the challenge of resisting sensor shifts. Finally, we leverage a multi-task CNN-based network and workload adjustment guidance to achieve personalized FWT monitoring. Extensive experiments with 15 participants confirm that PPGSpotter can achieve promising workload estimation (0.59 kg RMSE), repetitions estimation (0.96 reps RMSE), and exercise type recognition (91.57% F1-score) while providing valid workload adjustment recommendations (0.22 kg RMSE).},
  archive      = {J_TMC},
  author       = {Xiaochen Liu and Fan Li and Yetong Cao and Shengchun Zhai and Binghui Shi and Song Yang and Yu Wang},
  doi          = {10.1109/TMC.2025.3540165},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5824-5838},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A wearable PPG-based monitoring system for personalized free weight training},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How to prevent social media platforms from knowing the images you share with friends. <em>TMC</em>, <em>24</em>(7), 5808-5823. (<a href='https://doi.org/10.1109/TMC.2025.3538885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in image sharing on social media platforms escalates private information extraction for commercial use, increasing user demand for privacy protection. However, the dynamics of group communication within online social networks and the image compression imposed by platforms present significant challenges to secure key exchange and reliable image sharing in existing solutions. In this paper, we propose PrivSocial to prevent social media platforms from extracting private information in images shared within group communications. Specifically, we propose two frameworks, a server-based framework and a subscription-based framework, making PrivSocial applicable to different social media platforms and providing users with optional security levels, enhancing the flexibility and efficiency. To achieve intra-group key agreement and ensure image privacy protection, both frameworks integrate optimized continuous group key agreement and a novel image encryption scheme resisting compression. We implement an Android-based Priv-raster application and deploy a prototype on Twitter. Furthermore, we evaluate the proposed encryption scheme, and experimental results show that it has efficient encryption and decryption performance while being resistant to jigsaw puzzle solver attacks. The multi-user simulation experiments also demonstrate that the processing time of a single user is mere milliseconds, and the scheme can efficiently support tens of thousands of groups.},
  archive      = {J_TMC},
  author       = {Dawei Li and Yuxiao Guo and Di Liu and Qifan Liu and Song Bian and Zhenyu Guan},
  doi          = {10.1109/TMC.2025.3538885},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5808-5823},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {How to prevent social media platforms from knowing the images you share with friends},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GIRP: Energy-efficient QoS-oriented microservice resource provisioning via multi-objective multi-task reinforcement learning. <em>TMC</em>, <em>24</em>(7), 5793-5807. (<a href='https://doi.org/10.1109/TMC.2025.3547339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microservice architecture has revolutionized web service development by facilitating loosely coupled and independently developable components distributed as containers or virtual machines. While existing studies emphasize end-to-end latency, this paper investigates energy-efficient quality-of-service (QoS)-oriented microservice provisioning, focusing on both QoS satisfaction and power consumption (PC) conservation. We propose the Green and Intelligent Resource Provision (GIRP) architecture, integrating a data-driven energy-latency-aware resource allocation and scheduling manager to balance latency and PC. To reconcile the trade-offs involved, a dual-objective optimization problem is formulated to minimize latency and energy use by selecting proper servers, allocating CPU cores, and determining service replicas. To address challenges with discrete variables, dual objectives, and implicit mappings, we leverage a model-free deep deterministic policy gradient-based reinforcement learning algorithm. Specifically, we develop a multi-task agent via the Multi-gate Mixture-of-Experts model to simultaneously make two separate actions regarding CPU core numbers and service replica numbers, followed by a single-task agent to determine service scheduling. Extensive experiments on the DeathStarBenchmark testbed validate GIRP’s effectiveness, demonstrating approximately 52% resource savings and a 43% reduction in PC compared to leading methods like Sinan, Firm, and heuristic-based algorithms. These results highlight GIRP’s capability to optimize microservice orchestration by balancing end-to-end latency and power efficiency.},
  archive      = {J_TMC},
  author       = {Honggang Yuan and Ting Wang and Min Fu and Yuanming Shi},
  doi          = {10.1109/TMC.2025.3547339},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5793-5807},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {GIRP: Energy-efficient QoS-oriented microservice resource provisioning via multi-objective multi-task reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A tractable analysis model of information freshness for mobile edge computing assisted IoT system with layer-coded HARQ. <em>TMC</em>, <em>24</em>(7), 5779-5792. (<a href='https://doi.org/10.1109/TMC.2025.3539662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on an Internet of Things (IoT) status update system with the assistance of mobile edge computing (MEC) under short packet communications. The edge server includes a successive interference canceller (SIC) and a buffer pool. To simultaneously improve both information timeliness and throughput, a layer-coded hybrid automatic repeat request (L-HARQ) protocol is proposed especially considering backtrack decoding (BD) delay. In the MEC-assisted L-HARQ IoT status update system, the source successively transmits mixing updates to an access point (AP). The mixing update consists of the new generation and the part of the previously failed one. All the undecoded mixing packets are delivered to the edge server. Once a successful feedforward decoding (FD) occurs, the edge server attempts to recover undecoded mixing packets until a BD failure occurs or all buffers empty. The successful BD results are also delivered to the IR and the obtained prior information is fed back to the edge server for the next BD. By employing the stochastic hybrid system (SHS) model, the average age of informations (AoIs) are derived under circle-shift preemption (CS-P) and fixed preemption (F-P) policies. Considering the fact that the number of buffers is limited and the successful FD and BD do not always cause the reduction of system AoI, this work proposes a non-binary age evolution model as well as the state simplification mothed in SHS. The presented numerical results show that the CS-P and F-P policies obtain the enhanced information freshness compared with the non-preemptive blocking policy, and the CS-P policy outperforms the F-P one in terms of information freshness. The average AoI greatly depends on BD depth. When it is small, the average AoI is small. However, the two policies have the same average throughput that mainly depends on the size of buffer pool and the success probabilities of both FD and BD.},
  archive      = {J_TMC},
  author       = {Yue Li and Xiangdong Jia and Xiaoping Ma and Yuxin Guo and Hailong Tian},
  doi          = {10.1109/TMC.2025.3539662},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5779-5792},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A tractable analysis model of information freshness for mobile edge computing assisted IoT system with layer-coded HARQ},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Securing autonomous UAV cluster with blockchain-based threshold key management system utilizing crypto-asset and multisignature. <em>TMC</em>, <em>24</em>(7), 5765-5778. (<a href='https://doi.org/10.1109/TMC.2025.3538462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles deployed in remote locations rely on self-governed key management for their protection. However, conventional key management depends on a centralized ground-based station or single vehicle. Such a system is vulnerable to compromised certificate authority problems and single-points-of-failure. This paper proposed to resolve these vulnerabilities using a blockchain-based threshold key management system. The proposed system utilized blockchain’s concepts of crypto-asset and multisignature. Keys are defined as crypto-assets to improve their management in the blockchain network. Multisignature facilitates collaboration during key management based on a threshold value. The threshold value is also configurable to meet systems’ security and performance requirements. The proposed system secured the process of re-enforcement, sub-clustering, re-merging, and inter-cluster migration. Security analysis revealed that the proposed system complied with most key management security guidelines. The custom signature module used to authenticate intra-cluster communication was also verified as safe. Threats to the cluster were identified, assessed for risk, and mitigated accordingly. Performance analysis found that both AODV and DSDV routing protocols offer consistent performance but DSDV prevailed during the worst-case network scenario. The paper finally identified research gaps, including the requirement for an optimized mechanism for collecting consent signatures.},
  archive      = {J_TMC},
  author       = {Mebanjop Kharjana and Subhas Chandra Sahana and Goutam Saha},
  doi          = {10.1109/TMC.2025.3538462},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5765-5778},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Securing autonomous UAV cluster with blockchain-based threshold key management system utilizing crypto-asset and multisignature},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust device-free mmWave sensing with specular reflection interference mitigation. <em>TMC</em>, <em>24</em>(7), 5749-5764. (<a href='https://doi.org/10.1109/TMC.2025.3538112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device-Free mmWave Sensing (DFWS) could sense target state by analyzing how target activities influence the surrounding mmWave signals. It has emerged as a promising sensing technology. However, when employing DFWS indoors, specular reflection interference arises due to the specular reflectors. This interference often induces ghost targets, impacting the accurate estimation of the number and position of targets, resulting in degradation in sensing performance. To tackle this issue, we delve into the generation mechanism of specular reflection interference and analyze its multi-domain characteristics. Through exploration, we discern its temporal sparsity, spatial symmetry or collinearity, and frequency correlation characteristics, and propose four metrics to measure them, accordingly. Specifically, we propose a temporal characteristic quantitative evaluation metric based on identity matching, spatial symmetry and collinearity quantitative evaluation metrics based on geometric analysis, and a frequency correlation quantitative evaluation metric based on Doppler velocity correction, respectively. Based on these metrics, we design a novel Specular Reflection Interference Mitigation (SRIM) method and develop a robust SRIM-DFWS prototype system based on a 60 GHz mmWave radar to validate our proposed method. Experimental results demonstrate that our proposed method could achieve accurate and effective mitigation of specular reflection interference in device-free target tracking.},
  archive      = {J_TMC},
  author       = {Yulin Liu and Jie Wang and Qinghua Gao and Miao Pan and Yuguang Fang},
  doi          = {10.1109/TMC.2025.3538112},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5749-5764},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust device-free mmWave sensing with specular reflection interference mitigation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aerial reliable collaborative communications for terrestrial mobile users via evolutionary multi-objective deep reinforcement learning. <em>TMC</em>, <em>24</em>(7), 5731-5748. (<a href='https://doi.org/10.1109/TMC.2025.3536093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous aerial vehicles (AAVs) have emerged as the potential aerial base stations (BSs) to improve terrestrial communications. However, the limited onboard energy and antenna power of a AAV restrict its communication range and transmission capability. To address these limitations, this work employs collaborative beamforming through a AAV-enabled virtual antenna array to improve transmission performance from the AAV to terrestrial mobile users, under interference from non-associated BSs and dynamic channel conditions. Specifically, we introduce a memory-based random walk model to more accurately depict the mobility patterns of terrestrial mobile users. Following this, we formulate a multi-objective optimization problem (MOP) focused on maximizing the transmission rate while minimizing the flight energy consumption of the AAV swarm. Given the NP-hard nature of the formulated MOP and the highly dynamic environment, we transform this problem into a multi-objective Markov decision process and propose an improved evolutionary multi-objective reinforcement learning algorithm. Specifically, this algorithm introduces an evolutionary learning approach to obtain the approximate Pareto set for the formulated MOP. Moreover, the algorithm incorporates a long short-term memory network and hyper-sphere-based task selection method to discern the movement patterns of terrestrial mobile users and improve the diversity of the obtained Pareto set. Simulation results demonstrate that the proposed method effectively generates a diverse range of non-dominated policies and outperforms existing methods. Additional simulations demonstrate the scalability and robustness of the proposed CB-based method under different system parameters and various unexpected circumstances.},
  archive      = {J_TMC},
  author       = {Geng Sun and Jian Xiao and Jiahui Li and Jiacheng Wang and Jiawen Kang and Dusit Niyato and Shiwen Mao},
  doi          = {10.1109/TMC.2025.3536093},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5731-5748},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Aerial reliable collaborative communications for terrestrial mobile users via evolutionary multi-objective deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power optimization for low transmission delay in software defined data center networks. <em>TMC</em>, <em>24</em>(7), 5716-5730. (<a href='https://doi.org/10.1109/TMC.2025.3538791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Data Center Networks (SDDCNs) utilizes Software Defined Networking (SDN) as a network architecture to achieve highly flexible, programmable, and automated management of Data Center Networks (DCNs). The high energy consumption of DCNs remains a persistent and significant challenge. Thus, the energy saving is crucial and imperative for DCNs. Current energy-efficient solutions primarily rely on flow consolidation and dynamic device sleeping techniques to reduce energy consumption. However, these approaches often yield long Flow Completion Time (FCT), potentially resulting in violations of service-level agreements, particularly for delay-sensitive applications. In this paper, we formulate the problem of minimizing the power consumption in SDDCNs as key objective while ensuring timely FCT for delay-sensitive applications. To solve this problem, we first introduce the Active Network Generation (ANG) approach, which generates a minimal active subnet with the least number of active devices while meeting the current traffic demand. Subsequently, we propose two algorithms based on the type of applications: the Delay-Tolerant Flow Route (DTFR) algorithm for delay-tolerant applications and the Delay-Sensitive Flow Route (DSFR) algorithm for delay-sensitive applications. Simulation results demonstrate that our propose solution achieves an energy-saving rate of up to 67.77% and significantly reduces FCT compared to benchmark solutions.},
  archive      = {J_TMC},
  author       = {Yong Zhao and Jiannong Cao and Xingwei Wang and Fuliang Li and Qiang He and Xiaojie Liu},
  doi          = {10.1109/TMC.2025.3538791},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5716-5730},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Power optimization for low transmission delay in software defined data center networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S2A-P2FS: Secure storage auditing with privacy-preserving flexible data sharing in cloud-assisted industrial IoT. <em>TMC</em>, <em>24</em>(7), 5699-5715. (<a href='https://doi.org/10.1109/TMC.2025.3538057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of the Industrial Internet of Things (IIoT) has led to an explosion of industrial data. Due to computing and storage capacity limitations, IIoT devices often outsource the collected data to remote cloud servers. Unfortunately, cloud storage and cloud sharing services are not as reliable as they claim to be. Existing schemes aim to check data integrity in the cloud through cloud auditing. However, they suffer from a number of security and privacy vulnerabilities. The challenge of designing a secure storage auditing framework for industrial IoT comes from two aspects: 1) lack of physical protection of data owner IIoT devices; 2) privacy issues due to auditing of sensitive shared data. Inspired by the aforementioned challenges, we design the secure storage audit framework to support flexible cloud data sharing in IIoT: S2A-P2FS. The first contribution in our work is the Polynomial Prefix Message Authentication Code(P2MAC) design. We design an innovative P2MAC data structure as a label, which can simultaneously achieve efficient data verification in cloud data storage and privacy protection in flexible cloud data sharing for cloud auditing. The second contribution is the design of a unique Physical Unclonable Function(PUF) for IIoT. Harsh industrial conditions hinder the stable operation of PUFs. To protect the trustness of IIoT data owners, we propose a robust PUF-based physical protection mechanism for IIoT devices. The key point is that the required key is not stored in the memory of IIoT but hidden within its physical structure. A security analysis was conducted to demonstrate the robustness of S2A-P2FS against known vulnerabilities. A prototype was implemented in a real-world IIoT scenario. Experimental results indicate that, compared to state-of-the-art schemes, S2A-P2FS achieves over a 3x speedup in computational time and requires only 67.5% of the communication cost.},
  archive      = {J_TMC},
  author       = {Xiaohu Shan and Haiyang Yu and Yurun Chen and Yuwen Chen and Zhen Yang},
  doi          = {10.1109/TMC.2025.3538057},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5699-5715},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {S2A-P2FS: Secure storage auditing with privacy-preserving flexible data sharing in cloud-assisted industrial IoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint class-balanced client selection and bandwidth allocation for cost-efficient federated learning in mobile edge computing networks. <em>TMC</em>, <em>24</em>(7), 5681-5698. (<a href='https://doi.org/10.1109/TMC.2025.3539284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has significant potential to protect data privacy and mitigate network burden in mobile edge computing (MEC) networks. However, due to the system and data heterogeneity of mobile clients (MCs), client selection and bandwidth allocation is key for achieving cost-efficient FL in MEC networks with limited bandwidth. To address these challenges, we investigate the issue of joint client selection and bandwidth allocation for reducing the cost (i.e., latency and energy consumption) of FL training. We formulate the problem and decompose it into a holistic subproblem to reduce the number of rounds and a partial subproblem to reduce the costs of FL each round. We propose a joint class-balanced client selection and bandwidth allocation (CBCSBA) framework to address the whole problem. Specifically, for the holistic subproblem, CBCSBA combines MCs into groups, each having data distribution as close as possible to class-balanced distribution; For the partial subproblem, CBCSBA reduces costs by exploratively selecting a group and sequentially optimizing the latency and energy consumption of MCs within the group. Experimental results show that CBCSBA outperforms the baseline frameworks in reducing latency by 28.2% and energy consumption by 25.3% on average in the considered four datasets.},
  archive      = {J_TMC},
  author       = {Jian Tang and Xiuhua Li and Hui Li and Penghua Li and Xiaofei Wang and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3539284},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5681-5698},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint class-balanced client selection and bandwidth allocation for cost-efficient federated learning in mobile edge computing networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MWRS: A MAB-based worker recruitment scheme with tripartite stackelberg game for reliable mobile crowdsensing. <em>TMC</em>, <em>24</em>(7), 5665-5680. (<a href='https://doi.org/10.1109/TMC.2025.3535567'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Crowdsensing (MCS) has emerged as a compelling paradigm for data sensing and collection, leveraging the widespread adoption of mobile devices and the active participation of numerous users. Despite its potential, MCS faces critical challenges, particularly in recruiting reliable workers and acquiring high-quality sensing data. Most existing approaches assume prior information on worker quality and are vulnerable to collusion attacks, especially having not comprehensively considered workers’ reliability and stability. To address these problems, we propose a Multi-Armed Bandit (MAB) based Worker Recruitment Scheme (MWRS) integrated with the Tripartite Stackelberg Game (TSG) for MCS. Specifically, a trust evaluation and truth inference mechanism is introduced to assess the trustworthiness of workers through active truth detection. To enhance recruitment quality, we employ a trust-aware worker selection mechanism that utilizes a modified Upper Confidence Bound (UCB) algorithm, achieving an optimal balance between exploration and exploitation. Furthermore, the interactions among participants are modeled using a TSG framework, which formulates their respective payoffs to determine optimal decision-making strategies, thus achieving mutually beneficial outcomes. Extensive evaluations on real-world datasets demonstrate that our proposed scheme improves total quality by up to 30.8% and reduces regret by up to 80.3% compared to existing methods.},
  archive      = {J_TMC},
  author       = {Yan Ouyang and Feng Zeng and Neal N. Xiong and Anfeng Liu and Witold Pedrycz},
  doi          = {10.1109/TMC.2025.3535567},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5665-5680},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MWRS: A MAB-based worker recruitment scheme with tripartite stackelberg game for reliable mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint service caching and resource allocation over different timescales in satellite edge computing networks. <em>TMC</em>, <em>24</em>(7), 5649-5664. (<a href='https://doi.org/10.1109/TMC.2025.3534779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of edge computing into satellite networks offers a promising solution for extending computational services to remote and underserved areas. To effectively provide a variety of computing services, it is essential to cache the corresponding services on satellites. However, challenges exist such as dynamic computing requests that vary over time and space, energy constraints due to restricted power supply, as well as limited storage capacity on satellites and the impracticality of frequently adjusting service deployments. To tackle such challenges, this paper proposes a two-timescale joint optimization framework to minimize energy consumption in satellite edge computing networks while ensuring the delay requirements, by jointly optimizing service placement and task offloading, as well as computation resource and power allocation. On a larger timescale, we optimize service caching placement by strategically deploying services on satellites and ground devices (GDs) based on long-term service request statistics, aiming to minimize the total average delay over each time frame. We develop an efficient iterative algorithm by employing penalty-based methods and Lagrange duality techniques to achieve suboptimal service deployment. On a smaller timescale, we optimize task offloading and resource allocation in shorter time slots, adapting to dynamic traffic fluctuations to minimize energy consumption while meeting delay constraints. We utilize alternating optimization and quadratic transform methods to efficiently allocate resources and schedule tasks. Extensive simulations demonstrate the effectiveness and superiority of our framework over benchmark schemes, revealing significant reductions in delay and energy consumption. The results also highlight the trade-offs between task delay and energy consumption, as well as between transmit power and energy consumption.},
  archive      = {J_TMC},
  author       = {Han Hu and Kaifeng Song and Cheng Zhan and Rongfei Fan and Jian Yang},
  doi          = {10.1109/TMC.2025.3534779},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5649-5664},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint service caching and resource allocation over different timescales in satellite edge computing networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PAC-MC: An efficient password-based access control framework for time sequence aware media cloud. <em>TMC</em>, <em>24</em>(7), 5632-5648. (<a href='https://doi.org/10.1109/TMC.2025.3534861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud storage makes it easier for users to access and share data remotely, but it often requires integration with cryptographic technologies to address consumer-oriented applications, such as fine-grained data access, secure data sharing and retrieval. This paper focuses on the fine-grained access problem of media applications based on time sequence, that is, certain critical media applications based on time sequences should ideally be accessible only to authorized clients. The traditional keyword-based searchable encryption (SE) allows effective search and access over encrypted data while preserving data privacy, but most existing solutions do not support temporal access control (i.e., a mechanism that grants access permissions to users within a specified time range). In this paper, we propose PAC-MC, an efficient password-based access control framework for media cloud relying on content control with the time sequence attribute. PAC-MC not only supports multi-keyword search using any monotonic boolean formulas but also allows media owners to control content-encryption keys for different time periods with an updatable password. Furthermore, it supports the self-retrieval of content-encryption keys. In addition, PAC-MC is provably secure under the standard model. Finally, the detailed performance evaluation results and experimental comparisons indicate that PAC-MC is very efficient and outperforms the previous solutions in terms of computation, communication, and storage costs.},
  archive      = {J_TMC},
  author       = {Haiyan Wang and Penghui Liu and Xiaoxiong Zhong and Fucai Luo and Bin Xiao and Yuanyuan Yang},
  doi          = {10.1109/TMC.2025.3534861},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5632-5648},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PAC-MC: An efficient password-based access control framework for time sequence aware media cloud},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving stable data trading for unknown market based on blockchain. <em>TMC</em>, <em>24</em>(7), 5615-5631. (<a href='https://doi.org/10.1109/TMC.2025.3534201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsensing Data Trading (CDT) has emerged as a novel data trading paradigm, where market stability is crucial during the transaction matching process. However, most existing CDT systems usually assume that the preferences of both parties are known and the third-party trading platform is trustworthy, which is impractical in real-world scenarios and leads to significant challenges in reliability and privacy preservation. To address these challenges, we propose a Privacy-Preserving and Stable Data Trading for Unknown Market based on Blockchain and Bilateral Reputation (PPSDT-UMBBR) scheme in the decentralized CDT system. First, a privacy-preserving bilateral preference initialization method is designed to achieve the initial matching of buyers and sellers without exposing their location and attribute privacy. Then, a stable matching method based on dynamic bilateral preference updating is proposed, integrating Differential Privacy, Stable matching theory, and a strategy based on Asymmetric Bilateral Preferences with Multi-Armed Bandits (DPS-ABPMAB). Finally, we theoretically analyze the security and prove that the market outcome is $\delta$-stable. Furthermore, compared to other benchmark methods based on real datasets, our proposed DPS-ABPMAB algorithm improves the average accumulative reward by at least 4.22%, and reduces the average accumulative regret and the mean evaluation error rate by at least 66.86% and 7.35%, respectively.},
  archive      = {J_TMC},
  author       = {Qingyong Deng and Qinghua Zuo and Zhetao Li and Haolin Liu and Yong Xie},
  doi          = {10.1109/TMC.2025.3534201},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5615-5631},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-preserving stable data trading for unknown market based on blockchain},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Burst-sensitive traffic forecast via multi-property personalized fusion in federated learning. <em>TMC</em>, <em>24</em>(7), 5598-5614. (<a href='https://doi.org/10.1109/TMC.2025.3538871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For distributed network traffic prediction with data localization and privacy protection, Federated Learning (FL) enables collaborative training without raw data exchange across Base Stations (BSs). Nevertheless, traffic across BSs exhibit inherently heterogeneous trend burst and smooth fluctuation properties, but existing FL methods model single-scale series from only one view, which cannot simultaneously capture diverse trend and fluctuation properties, especially distinct burst distributions. In this paper, we propose Personalized Federated Forecasting with Multi-property Self-fusion (P2FMS), which can represent multi-scale traffic properties from different views. With precise multi-property representations, a fusion-level prediction decision is learned for each client in a personalized manner to promptly sense traffic bursts and improve forecasting performance in non-IID settings. Specifically, P2FMS decomposes the traffic series into distinct time scales, based on which, we effectively extract closeness, period, and trend properties from different views. The closeness and period are embedded through global-view representations with spatial correlations, while non-stationary trends are individually fitted from the client-side view. Furthermore, a personalized combiner is designed to accurately quantify the proportion of general fluctuation raws (i.e., closeness and period) and specific trend property in predictions, which enables multi-property self-fusion for each client to accommodate heterogeneous traffic patterns and enhance prediction accuracy. Besides, an alternant training mechanism is introduced to optimize property representation and fusion modules with the convergence guarantee. Extensive experiments on real-world datasets show that P2FMS outperforms status quo methods in both prediction performance and convergence time.},
  archive      = {J_TMC},
  author       = {Jingjing Xue and Sheng Sun and Min Liu and Yuwei Wang and Xuying Meng and Jingyuan Wang and JunBo Zhang and Ke Xu},
  doi          = {10.1109/TMC.2025.3538871},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {7},
  number       = {7},
  pages        = {5598-5614},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Burst-sensitive traffic forecast via multi-property personalized fusion in federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating federated codistillation via adaptive computation amount at network edge. <em>TMC</em>, <em>24</em>(6), 5584-5597. (<a href='https://doi.org/10.1109/TMC.2025.3533591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Federated Learning (FL) empowers IoT devices to collectively train a shared model without local data exposure. In order to address the issue of Non-IID that causes model performance degradation, the recently proposed federated codistillation framework has shown great potential. However, due to the system heterogeneity of devices, the federated codistillation framework still faces a synchronization barrier issue, resulting in a non-negligible waiting time with a fixed computation amount (epoch or batch size) assigned. In this paper, we propose Adaptive Computation Amount Allocation (ACAA) to accelerate federated codistillation. Specifically, we leverage a criterion, solution inexactness, to quantify the computation amount. We dynamically adjust the solution inexactness of devices based on their computing power and bandwidth to enable them nearly simultaneous completion of training, reducing synchronization waiting time without sacrificing the training performance. The minimum required computation amount is determined by the coefficient of the distillation term and the gradient dissimilarity bound of Non-IID. We theoretically analyze the convergence of ACAA. Extensive experiments show that, compared to benchmark algorithms, ACAA can accelerate training by up to 5×.},
  archive      = {J_TMC},
  author       = {Zhihao Zeng and Xiaoning Zhang and Yangming Zhao and Ahmed Zoha and Muhammad Ali Imran and Yan Zhang},
  doi          = {10.1109/TMC.2025.3533591},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5584-5597},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Accelerating federated codistillation via adaptive computation amount at network edge},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdapLDP-FL: An adaptive local differential privacy for federated learning. <em>TMC</em>, <em>24</em>(6), 5569-5583. (<a href='https://doi.org/10.1109/TMC.2025.3533090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a technique that allows multiple participants to co-train machine learning models, while also enhancing privacy by avoiding the exposure of local data. However, it is important to note that despite its effectiveness, there is still a potential risk of leaking users’ private information through weight analysis during FL updates. Local Differential Privacy (LDP) is a technique used to prevent individual information leakage by adding noise to the user's model parameters. However, FL based on LDP lacks dynamic optimization and adaptation considering privacy and data utility, especially regarding noise constraints. This paper investigates FL under the scenario of noise optimization with LDP. Specifically, given a certain privacy budget, we design the adaptive LDP method via a noise scaler, which adaptively optimizes the noise size of every client. Second, we dynamically tailor the model direction after adding noise by the designed a direction matrix, to overcome the model drift problem caused by adding noises to the client model. Finally, our method achieves higher accuracy than some existing works with the same privacy level and the convergence speed is significantly improved.},
  archive      = {J_TMC},
  author       = {Gaofeng Yue and Li Yan and Liuwang Kang and Chao Shen},
  doi          = {10.1109/TMC.2025.3533090},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5569-5583},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AdapLDP-FL: An adaptive local differential privacy for federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ESPD-LP: Edge service pre-deployment based on location prediction in MEC. <em>TMC</em>, <em>24</em>(6), 5551-5568. (<a href='https://doi.org/10.1109/TMC.2025.3533005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of real-time applications, services has made Multi-access Edge Computing (MEC) essential for delivering low-latency, high-performance computing. The effectiveness of MEC, however, is largely contingent on the efficient pre-deployment of services. Despite its importance, efficient service pre-deployment is challenged by the inherent unpredictability of user mobility, the fluctuating conditions of network environments. Accurately predicting user locations, dynamically optimizing resource allocation across geographically distributed MEC servers are complex tasks that are essential to minimizing latency, maximizing data transmission efficiency. The variability in user movement patterns, network bandwidth further exacerbates these challenges, often leading to increased latency, diminished performance, which can negate the advantages offered by MEC. To address these challenges, this paper introduces a novel edge service pre-deployment scheme based on location prediction (ESPD-LP). The ESPD-LP scheme leverages historical user trajectory data to predict future locations, facilitating proactive, strategic resource allocation via a user-centric bidirectional matching algorithm across multiple MEC servers. By pre-deploying services in anticipation of user needs, this approach optimizes data transmission rates, reduces pre-deployment latency, significantly enhancing the overall performance of MEC systems. A comprehensive analysis reveals that the ESPD-LP scheme consistently outperforms similar approaches, with a 41% increase in data transmission rates, a 31% reduction in pre-deployment latency compared to the JO-CDSD, MEC-RDESN schemes, demonstrating consistently superior performance.},
  archive      = {J_TMC},
  author       = {Liangjun Song and Gang Sun and Hongfang Yu and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3533005},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5551-5568},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ESPD-LP: Edge service pre-deployment based on location prediction in MEC},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Content-aware joint knob configuration and resource allocation for edge video analytics. <em>TMC</em>, <em>24</em>(6), 5536-5550. (<a href='https://doi.org/10.1109/TMC.2025.3533596'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Characterized by its ease of low-latency response, edge computing is capable of supporting real-time video analytics applications, constituting an edge video analytics paradigm, where the joint knob configuration and network scheduling design has drawn ever-escalating research attention. However, the potential of edge video analytics has not been fully exploited, owing to the limitations of the state-of-the-art as follows. i) The eminent impact of video content on accuracy performance has been ignored. ii) The variables that can be tuned are not fully considered in scheduling. iii) The heuristic algorithm-based solutions are far from the optimal. To fill in this gap, in this paper, we conceive a content-aware joint knob configuration and resource allocation scheme for edge video analytics. Concretely, fed with the features extracted from the video content, a deep neural network (DNN)-based predictor is proposed to predict the configuration-accuracy performance in a real-time manner. With an aid of the predictive results, we formulate an accuracy-maximization problem as an integer programming problem, by optimizing the variables, including resolution, frame rate, video analytic model, network bandwidth, and computational resource subject to the latency constraints. To solve this problem in an efficient manner, we devise a novel low-complexity dynamic programming method. Simulation results verify the efficiency of our content-aware joint knob configuration and resource allocation scheme. Quantitatively, a 3.3% gap is attained towards the upper bound in terms of the accuracy in an object detection scenario, relying on the scheme proposed.},
  archive      = {J_TMC},
  author       = {Tong Bai and Bo Hou and Zhipeng Wang and Dong Liu and Arumugam Nallanathan},
  doi          = {10.1109/TMC.2025.3533596},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5536-5550},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Content-aware joint knob configuration and resource allocation for edge video analytics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated deep reinforcement learning for ENDC optimization. <em>TMC</em>, <em>24</em>(6), 5525-5535. (<a href='https://doi.org/10.1109/TMC.2025.3534661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {5G New Radio (NR) network deployment in Non-Stand Alone (NSA) mode means that 5G networks rely on the control plane of existing Long Term Evolution (LTE) modules for control functions, while 5G modules are only dedicated to the user plane tasks, which could also be carried out by LTE modules simultaneously. The first deployments of 5G networks are essentially using this technology. These deployments enable what is known as E-UTRAN NR Dual Connectivity (ENDC), where a user establish a 5G connection simultaneously with a pre-existing LTE connection to boost their data rate. In this paper, a single Federated Deep Reinforcement Learning (FDRL) agent for the optimization of the event that triggers the dual connectivity between LTE and 5G is proposed. First, single Deep Reinforcement Learning (DRL) agents are trained in isolated cells. Later, these agents are merged into a unique global agent capable of optimizing the whole network with Federated Learning (FL). This scheme of training single agents and merging them also makes feasible the use of dynamic simulators for this type of learning algorithm and parameters related to mobility, by drastically reducing the number of possible combinations resulting in fewer simulations. The simulation results show that the final agent is capable of achieving a tradeoff between dropped calls and the user throughput to achieve global optimum without the need for interacting with all the cells for training.},
  archive      = {J_TMC},
  author       = {Adrian Martin and Isabel de-la-Bandera and Adriano Mendo and Jose Outes and Juan Ramiro and Raquel Barco},
  doi          = {10.1109/TMC.2025.3534661},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5525-5535},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Federated deep reinforcement learning for ENDC optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting mobile app usage with context-aware dynamic hypergraphs. <em>TMC</em>, <em>24</em>(6), 5511-5524. (<a href='https://doi.org/10.1109/TMC.2025.3532992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {App usage prediction aims to predict the next app most likely to be used based on historical behaviors, which is beneficial for smartphone system optimization, such as system resource management, battery energy optimization, and user experience enhancement. Existing studies have treated it as a simple time series prediction problem and overlooked the sessionization characteristic of mobile app usage, i.e., neglecting the intent context in which the user interacts with apps. In this paper, we explore the context of user intents and incorporate app sessionization features into prediction models to improve prediction accuracy. Specifically, we first extract the semantic meaning of spatio-temporal contextual information of app usage by constructing an urban knowledge graph. Second, we devise a hypergraph-based embedding model to extract the hyper-relations of intra-session apps. Third, we utilize a self-attention mechanism to fuse intra-session apps’ representations and combine spatio-temporal contextual embedding to form the session representation. We further leverage a transformer for inter-session intent transition modeling to extract users’ dynamic intent (i.e., the semantic meaning of sessions) for app usage. Finally, we jointly fuse dynamic intent and recently used app features using the MLP model for the prediction. The novelty of our method is that we are the first to leverage dynamic hypergraphs for modeling sessionization features, and we model both inter-session and intra-session relations. We evaluate our model based on two real-world datasets collected in Shanghai and Nanchang. In terms of prediction accuracy, mean reciprocal rank, and normalized discounted cumulative gain, our proposed framework outperforms state-of-the-art baselines by more than 30% in the Shanghai dataset and 20% in the Nanchang dataset, respectively.},
  archive      = {J_TMC},
  author       = {Zihan Huang and Tong Li and Xing Wang and Kexin Yang and Chao Deng and Junlan Feng and Yong Li},
  doi          = {10.1109/TMC.2025.3532992},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5511-5524},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Predicting mobile app usage with context-aware dynamic hypergraphs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online energy and interference management for dynamic target tracking with cellular-connected UAV. <em>TMC</em>, <em>24</em>(6), 5496-5510. (<a href='https://doi.org/10.1109/TMC.2025.3532276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cellular-connected Unmanned Aerial Vehicles (UAVs) have significant potential for target tracking in future cellular networks due to their broad coverage and operational flexibility. In this paper, we consider a multi-cell cellular network with a cellular-connected UAV for target tracking, which encounters challenges such as unpredictable flight energy consumption from the stochastic movements of the tracking target and severe uplink interference from ground devices (GDs). To tackle these challenges, we propose a multi-stage stochastic optimization framework focused on energy-efficient target tracking with interference coordination. Our objective is to optimize the long-term average uplink throughput of both aerial users and GDs by jointly optimizing the UAV's trajectory, power allocation, and cell association across multiple orthogonal communication resource blocks (RBs). The formulated stochastic non-convex problem is first transformed into a deterministic problem for each time slot by using the Lyapunov optimization framework. An online optimization strategy is proposed, utilizing the optimal structure, alternative optimization, and successive convex approximation (SCA) techniques. Simulation results show that the proposed approach significantly enhances network throughput and UAV energy queue stability compared to existing baseline schemes.},
  archive      = {J_TMC},
  author       = {Cheng Zhan and Huan Yan and Rongfei Fan and Han Hu and Shubin Xu and Jian Yang},
  doi          = {10.1109/TMC.2025.3532276},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5496-5510},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Online energy and interference management for dynamic target tracking with cellular-connected UAV},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid transformer based multi-agent reinforcement learning for multiple unpiloted aerial vehicle coordination in air corridors. <em>TMC</em>, <em>24</em>(6), 5482-5495. (<a href='https://doi.org/10.1109/TMC.2025.3532204'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced Air Mobility (AAM) seeks to establish a next-generation air transportation system by leveraging autonomous unpiloted aerial vehicles (UAVs) to transport passengers and cargo between locations previously underserved or unserved by traditional aviation. Achieving AAM at scale requires overcoming significant challenges in airspace management, classification, and traffic control to safely accommodate the increasing volume of UAV operations. This paper presents a comprehensive design for air corridors to facilitate efficient aerial transport and formulates a multi-UAV coordination problem within these corridors. The objective is to enable each UAV to autonomously make control decisions based on local observations gathered from onboard sensors. This decentralized control approach is modeled as a multi-agent partially observable Markov decision process (POMDP), aiming at minimizing UAV travel time while ensuring adherence to corridor boundaries and collision avoidance. To address the complexities posed by varying state dimensions and types, we propose a novel Hybrid Transformer-based Multi-agent Reinforcement Learning (HTransRL) architecture. HTransRL integrates a customized transformer model into an actor-critic network, effectively processing both sequential and non-sequential observed states of varying sizes while capturing their correlations. This enables safe and efficient UAV navigation. Simulation results show that in test environments similar to or simpler than training scenarios, HTransRL achieves a successful arrival rate exceeding 90% in worst-case test scenarios. In test environments more complex than training scenarios, HTransRL demonstrates superior scalability compared to two baseline methods, achieving higher arrival rates and comparable travel times.},
  archive      = {J_TMC},
  author       = {Liangkun Yu and Zhirun Li and Nirwan Ansari and Xiang Sun},
  doi          = {10.1109/TMC.2025.3532204},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5482-5495},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Hybrid transformer based multi-agent reinforcement learning for multiple unpiloted aerial vehicle coordination in air corridors},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bargaining approach for service placement in multi-access edge computing with information asymmetries. <em>TMC</em>, <em>24</em>(6), 5464-5481. (<a href='https://doi.org/10.1109/TMC.2025.3533045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC) refers to deploying computation resources, known as cloudlets or edge servers, near the edge of the mobile network. Services like augmented reality (AR) benefit from MEC by service placement, which refers to installing service-specific software and allocating resources on cloudlets. Service placement in MEC improves service quality and potentially reduces costs compared to centralized cloud computing approaches. The main stakeholders in MEC are infrastructure providers (IPs), who manage the MEC infrastructure, and service providers (SPs), who offer services to users. Both have unique technical and economic perspectives, such as resource demands, resource availability, and costs. Information asymmetries exist as only IPs have access to information about their resources, and only SPs have information about service usage and resource demands. This work addresses challenges of service placement in MEC from a multi-stakeholder, techno-economic perspective. We introduce a model including the stakeholders’ technical and economic goals and information asymmetries. To solve this problem efficiently, we propose a multi-stakeholder bargaining mechanism, termed Nash Backward Induction with Linear Equilibrium Strategies (NBI-LES). In a case study with 544 users and 16 SPs, we achieve $\text{79}{\%}$ of the optimal reduction in traffic given by a centralized optimal service placement strategy.},
  archive      = {J_TMC},
  author       = {Bernd Simon and Paul Adrian and Patrick Weber and Patrick Felka and Oliver Hinz and Anja Klein},
  doi          = {10.1109/TMC.2025.3533045},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5464-5481},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A bargaining approach for service placement in multi-access edge computing with information asymmetries},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards resilience 5G-V2N: Efficient and privacy-preserving authentication protocol for multi-service access and handover. <em>TMC</em>, <em>24</em>(6), 5446-5463. (<a href='https://doi.org/10.1109/TMC.2025.3532120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The booming 5G cellular networks sparked tremendous interest in supporting more sophisticated critical use cases through vehicle-to-network (V2N) communications. However, the inherent technical vulnerabilities and densification of 5G raise new security and efficiency challenges. The existing secondary authentication fails to support multi-service access. The random access process lacks authentication of the gNB, possibly leading to fake base station attacks (FBS). Moreover, related research extends key forward/backward secrecy (KF/BS) to require that it also applies to gNBs, thus invalidating most existing schemes. This paper introduces a comprehensive security framework for 5G-V2N that seamlessly integrates with existing standardized architecture to provide privacy-preserving mutual authentication and key agreement for the full service cycle. Specifically, we propose new secondary authentication involving gNBs and support single request access to multi-services. Second, incorporating the service migration idea, we design the g2g (gNB-to-gNB) channel establishment phase to promote secure context share. Finally, the proposed efficient handover phase achieves the security properties of enhanced KF/BS, known randomness secrecy and privacy-preserving, and avoids FBS. We verify the proposed protocol using three different formal techniques: provably secure, BAN-logic, and AVISPA tool. Extensive experimental results and comparison show that our scheme excels in computational and communication efficiencies, and detecting malicious events.},
  archive      = {J_TMC},
  author       = {Ye Bi and Chunfu Jia},
  doi          = {10.1109/TMC.2025.3532120},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5446-5463},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards resilience 5G-V2N: Efficient and privacy-preserving authentication protocol for multi-service access and handover},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey, design and evaluation of TGT-HC: A time-aware shaper MAC for wireless TSN. <em>TMC</em>, <em>24</em>(6), 5433-5445. (<a href='https://doi.org/10.1109/TMC.2025.3535413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-Reliable Low-Latency Communication (URLLC) and Time-Sensitive Networking (TSN) are essential for enhancing 5G and Wi-Fi 6/7 to support real-time industrial automation. However, our survey shows that existing Medium Access Control (MAC) schemes still face unresolved latency issues. This paper introduces the Transmission Gating Time Hyperchannel (TGT-HC), a novel contention-free Carrier-Sense Multiple Access (CSMA) scheme driven by a per-flow Time-Aware Shaper (TAS) scheduler. Our analytical results, simulations, and prototyping with the Universal Software Radio Peripheral (USRP) demonstrate that TGT-HC achieves latency performance comparable to a First-Come-First-Served (FCFS) single server for real-time cyclic traffic, even under high frame error rates (FERs). Given its promising performance, we advocate for reconsidering contention-free CSMA as a viable MAC scheme in next-generation URLLC/TSN.},
  archive      = {J_TMC},
  author       = {Raymond J. Jayabal and David Tung Chong Wong and Lee Kee Goh and Xiaojuan Zhang and Chin Ming Pang and Sumei Sun},
  doi          = {10.1109/TMC.2025.3535413},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5433-5445},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Survey, design and evaluation of TGT-HC: A time-aware shaper MAC for wireless TSN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service function chain deployment with intrinsic dynamic defense capability. <em>TMC</em>, <em>24</em>(6), 5418-5432. (<a href='https://doi.org/10.1109/TMC.2025.3532210'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Service Function Chain (SFC) leverages Network Function Virtualization (NFV) and Software-Defined Networking (SDN) for flexible deployment, creating customized service chains tailored to specific applications. As NFV and SDN technologies play crucial roles in the SFC implementation, any security risk that arises in an NFV/SDN network can potentially pose a threat to SFC. Thus, SFC becomes vulnerable to network security attacks. To address this, intrinsic security technologies, including moving target defense and mimic defense, offer proactive protection against both known and unknown threats. It is expected to break through traditional security protection mechanisms such as “enhanced”, “plug-in” and “passive” defense. This paper proposes an intrinsic dynamic defense architecture to equip SFC with active defense capabilities, shifting from passive reactive mechanism based on prior knowledge to an active defense against various attacks. The architecture comprises two models and five modules, including a sub-pool partitioning algorithm that enhances heterogeneity across sub-pools by splitting the heterogeneous replica pool into several sub-pools among replica VNFs. To meet Quality of Service (QoS) requirements like latency, cost, and security, we formulate a multi-objective optimization problem with three objectives: latency, cost, and defense success rate. Following that, we propose a dynamic Deep Reinforcement Learning (DRL)-based deployment algorithm. This algorithm selects appropriate VNFs based on heterogeneity and historical information, improving SFC and VNF security against external attacks. Extensive experiments validate that our architecture significantly enhances network security, provided that this improvement comes at the expense of limited cost and latency.},
  archive      = {J_TMC},
  author       = {Ran Wang and Lundan Cai and Qiang Wu and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3532210},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5418-5432},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Service function chain deployment with intrinsic dynamic defense capability},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimization of the training makespan in hybrid federated split learning. <em>TMC</em>, <em>24</em>(6), 5400-5417. (<a href='https://doi.org/10.1109/TMC.2025.3533033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel Split Learning (SL) allows resource-constrained devices that cannot participate in Federated Learning (FL) to train deep neural networks (NNs) by splitting the NN model into parts. In particular, such devices (clients) may offload the processing task of the largest model part to a computationally powerful helper, and multiple helpers may be employed and work in parallel. In hybrid federated and split learning (HFSL), on the other hand, devices can participate in the training process through any of the two protocols (SL and FL), depending on the system's characteristics. This could considerably reduce the maximum training time over all clients (makespan), especially in highly heterogeneous scenarios. In this paper, we study the joint problem of the training protocol selection, client-helper assignments, and scheduling decisions, to minimize the training makespan. We prove this problem is NP-hard and propose two solution methods: one based on the decomposition of the problem by leveraging its inherent symmetry, and a second fully scalable one. Through numerical evaluations using our testbed's measurements, we build a solution strategy comprising these methods. Moreover, this strategy finds a near-optimal solution and achieves a shorter makespan than the baseline schemes by up to 71%.},
  archive      = {J_TMC},
  author       = {Joana Tirana and Dimitra Tsigkari and George Iosifidis and Dimitris Chatzopoulos},
  doi          = {10.1109/TMC.2025.3533033},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5400-5417},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Minimization of the training makespan in hybrid federated split learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NCTX: A neural network-powered lossless compressive transmission using shared information. <em>TMC</em>, <em>24</em>(6), 5386-5399. (<a href='https://doi.org/10.1109/TMC.2025.3530950'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we explore the possibility of a new delivery method for lossless data, namely compressive transmission. It aims at minimizing the transmission data volume at runtime by exploiting the tailored information shared between the sender and the receiver. There are two approaches to leverage shared information for compression: 1) using a DNN-based codec as a proxy for shared information and 2) applying redundancy elimination using deduplication. However, these approaches have not been studied in depth to utilize the trade-off between the compression rate and the amount of shared information. Compared to these approaches, compressive transmission is unique as it fully leverages the abundance of information available on both sides, which is chosen and placed purposely. To bring the concept to reality, we propose nCTX, a neural network-powered Compressive Transmission System that adaptively exploits a generative model and matching blocks. nCTX extracts the optimal semantic data from the input data, exploiting shared information to closely imitate the original and compensate it with the offset (i.e., difference). Extensive evaluations in mobile platforms confirm that nCTX reduces the transmission volume significantly by 25.8% and 23.3% compared to FLIF and RC, the state-of-the-art image codecs, respectively, in comparable or shorter computation times.},
  archive      = {J_TMC},
  author       = {Wooseung Nam and Sungyong Lee and Joohyun Lee and Kyunghan Lee},
  doi          = {10.1109/TMC.2025.3530950},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5386-5399},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {NCTX: A neural network-powered lossless compressive transmission using shared information},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A trust-based computation offloading framework in mobile cloud-edge computing networks. <em>TMC</em>, <em>24</em>(6), 5370-5385. (<a href='https://doi.org/10.1109/TMC.2025.3530480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud service centers (CSCs) can purchase edge computation resources to improve service quality in mobile cloud-edge computing networks. However, edge servers (ESs) are owned by different entities, and dishonest entities may launch computational forgery attacks, i.e., the ES falsely reports its idle computation resources to win more tasks for increased revenue. Most existing approaches ignore the threat of dishonest ESs. To address the challenges, we design a Trust-based Computation Offloading (TCO) framework. First, we construct the problem for minimizing the difference between the CSC's cost and the expected revenue (DCER), which is a mixed-integer nonlinear programming problem. Second, we develop a trust-based computation offloading method that quickly finds a good solution by decomposing the problem. Finally, a two-tier trust evaluation method was proposed to obtain accurate trust values. Experimental results indicate that TCO's comprehensive performance surpasses the benchmarks and significantly enhances computation offloading reliability with a lower performance loss. Notably, tasks are preferentially offloaded to honest ESs to ensure their revenue and promote ESs’ honesty under the TCO framework. Additionally, compared with no trust mechanisms, TCO reduces the service timeout count in an interval by 34.37% - 73.80% with a performance loss of only 1.42% - 4.10%.},
  archive      = {J_TMC},
  author       = {Jianhui Wang and Zhetao Li and Haolin Liu and Tie Qiu and Hongbin Luo},
  doi          = {10.1109/TMC.2025.3530480},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5370-5385},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A trust-based computation offloading framework in mobile cloud-edge computing networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-effective edge data caching with failure tolerance and popularity awareness. <em>TMC</em>, <em>24</em>(6), 5357-5369. (<a href='https://doi.org/10.1109/TMC.2025.3531967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the mobile edge computing environment, caching data in edge storage systems can significantly reduce data retrieval latency for users while saving the costs incurred by cloud-edge data transmissions for app vendors. Existing edge data caching (EDC) methods prioritize popular data and aim to minimize users’ data retrieval latency and system storage costs jointly. However, these EDC methods often rely on the assumption that data popularity always follows certain distributions. As a result, they cannot properly adapt to the fluctuations in data popularity due to user mobility or unexpected increases in user demands. Meanwhile, unlike cloud data centers, complex and fragile edge servers are more likely to experience physical failures or network outages, presenting new challenges for EDC strategies. Specifically, when an edge server fails or experiences an outage, cached data may become temporarily unavailable, leading to increased latency as requests are redirected to alternative servers or the cloud. In this paper, to enable uncertainty-aware edge data caching (uEDC), we first model the problem as a robust optimization problem and propose an optimal algorithm named uEDC-B to find the optimal uEDC solution. To address the high computational complexity of uEDC-B, we introduce an approximate algorithm named uEDC-L based on linear decision rules. Theoretical analysis and extensive experiments on a real-world dataset demonstrate that the proposed methods outperform two state-of-the-art approaches in handling the uncertainties in data popularity and edge server failure with a significant performance improvement of 59.27% in data retrieval latency and 55.07% in data caching cost.},
  archive      = {J_TMC},
  author       = {Ruikun Luo and Zujia Zhang and Qiang He and Mengxi Xu and Feifei Chen and Xiaohai Dai and Song Wu and Hai Jin},
  doi          = {10.1109/TMC.2025.3531967},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5357-5369},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cost-effective edge data caching with failure tolerance and popularity awareness},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Faster convergence on heterogeneous federated edge learning: An adaptive clustered data sharing approach. <em>TMC</em>, <em>24</em>(6), 5342-5356. (<a href='https://doi.org/10.1109/TMC.2025.3533566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Edge Learning (FEL) emerges as a pioneering distributed machine learning paradigm for the 6 G Hyper-Connectivity, harnessing data from the IoT devices while upholding data privacy. However, current FEL algorithms struggle with non-independent and non-identically distributed (non-IID) data, leading to elevated communication costs and compromised model accuracy. To address these statistical imbalances, we introduce a clustered data sharing framework, mitigating data heterogeneity by selectively sharing partial data from cluster heads to trusted associates through sidelink-aided multicasting. The collective communication pattern is integral to FEL training, where both cluster formation and the efficiency of communication and computation impact training latency and accuracy simultaneously. To tackle the strictly coupled data sharing and resource optimization, we decompose the optimization problem into the clients clustering and effective data sharing subproblems. Specifically, a distribution-based adaptive clustering algorithm (DACA) is devised basing on three deductive cluster forming conditions, which ensures the maximum sharing yield. Meanwhile, we design a stochastic optimization based joint computed frequency and shared data volume optimization (JFVO) algorithm, determining the optimal resource allocation with an uncertain objective function. The experiments show that the proposed framework facilitates FEL on non-IID datasets with faster convergence rate and higher model accuracy in a resource-limited environment.},
  archive      = {J_TMC},
  author       = {Gang Hu and Yinglei Teng and Nan Wang and Zhu Han},
  doi          = {10.1109/TMC.2025.3533566},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5342-5356},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Faster convergence on heterogeneous federated edge learning: An adaptive clustered data sharing approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MADRL-based model partitioning, aggregation control, and resource allocation for cloud-edge-device collaborative split federated learning. <em>TMC</em>, <em>24</em>(6), 5324-5341. (<a href='https://doi.org/10.1109/TMC.2025.3530482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Split Federated Learning (SFL) has emerged as a promising paradigm to enhance FL by partitioning the Machine Learning (ML) model into parts and deploying them across clients and servers, effectively mitigating the workload on resource-constrained devices and preserving privacy. Compared to cloud-device-based and edge-device-based SFL, cloud-edge-device collaborative SFL offers both lower communication latency and wider network coverage. However, existing works adopt a uniform model partitioning strategy for different devices, ignoring the heterogeneous nature of device resources. This oversight leads to severe straggler problems, making the training process inefficient. Moreover, they do not consider joint optimization of model aggregation control and computing and communication resource allocation, and lack distributed algorithm design. To address these issues, we propose a joint resource management scheme for cloud-edge-device collaborative SFL to optimize the training latency and energy consumption of all devices. In our scheme, the partitioning strategy is optimized for each device based on resource heterogeneity. Meanwhile, we jointly optimize the aggregation frequency of ML models, computing resource allocation for all devices and edge servers, and transmit power allocation for all devices. We formulate a coordination game among all edge servers and then design a distributed optimization algorithm employing partially observable Multi-Agent Deep Reinforcement Learning (MADRL) with integrated numerical methods. Extensive experiments are conducted to validate the convergence of our algorithm and demonstrate the superiority of our scheme via evaluations under multiple scenarios and in comparison with four reference schemes.},
  archive      = {J_TMC},
  author       = {Wenhao Fan and Penghui Chen and Xiongfei Chun and Yuan’an Liu},
  doi          = {10.1109/TMC.2025.3530482},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5324-5341},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MADRL-based model partitioning, aggregation control, and resource allocation for cloud-edge-device collaborative split federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal analysis and risk assessment for batch crowdsourcing. <em>TMC</em>, <em>24</em>(6), 5312-5323. (<a href='https://doi.org/10.1109/TMC.2025.3532285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The way of task posting serves as the main pillar in achieving an efficient crowdsourcing market. Pioneer solutions on task posting can be categorized as retail task posting and batch task posting. Unlike retail task posting, which simply matches the most suitable worker to tasks, batch task posting considers the collaborations not only between workers and tasks but also among tasks, which brings high efficiency, low costs, and satisfactory task completion rates. However, the state of the arts on batch task posting leverage specific attributes to combine tasks as bundles for posting, leading to limited scalability. Hence, we propose a causal analysis framework for batch crowdsourcing to achieve an attribute-independent batch crowdsourcing solution that disentangles multi-factors to uncover the posting merits of tasks bundled at optimal prices, based on which an approximately optimal algorithm is further introduced to form reasonable bundles for posting. Since batch crowdsourcing may incur losses due to short-term profit fluctuation, a risk assessment method is proposed to encourage the requestor to act properly for loss mitigations. Our work explores the causal analysis and risk assessment in batch crowdsourcing for the first time, with the following highlights: 1) generality. It proposes a composite metric for gauging task bundles which avoids the issue of attribute dependence in the state of the arts, resulting in better universality; 2) synergy. By collaboratively considering the “value” and “relative position” of variables, our work derives results reflecting causal relationships rather than naive correlations; and 3) precision. We not only elucidate the probability of risk in batch crowdsourcing but also delineate the rate function governing its probability decay. This allows a requestor to know when and how fast to halt batch task posting.},
  archive      = {J_TMC},
  author       = {Ke Chao and Shengling Wang and Hongwei Shi and Jianhui Huang and Xiuzhen Cheng},
  doi          = {10.1109/TMC.2025.3532285},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5312-5323},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Causal analysis and risk assessment for batch crowdsourcing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Successive interference cancellation-enabled timely status update in linear multi-hop wireless networks. <em>TMC</em>, <em>24</em>(6), 5298-5311. (<a href='https://doi.org/10.1109/TMC.2025.3529462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the timely status update in linear multi-hop wireless networks, where a source tries to deliver status update packets to a destination through a sequence of half-duplex relays. Timeliness is measured by the age of information (AoI) metric. Maintaining a low AoI at the destination typically necessitates frequent transmission of update packets from the source. However, high packet transmission frequency in multi-hop scenarios can result in mutual wireless interference at intermediate relays. Specifically, when an intermediate relay receives wireless signals of a new packet from its previous node, simultaneous transmission of an old packet by its subsequent node to the next hop may cause wireless signals to interfere at the intermediate relay, conventionally leading to packet collision. A key motivation to solve this issue is that the intermediate relay has previously received the old packet (which can thus be forwarded to the subsequent node for further relaying). Hence, successive interference cancellation (SIC) can be employed to mitigate interference of the old packet and recover the new packet. This paper designs an SIC-enabled packet relaying scheme tailored to low AoI. Initially focusing on a three-hop network, we subsequently extend our approach to general multi-hop networks. We model the multi-hop relaying scheme using a Markov chain to derive the theoretical average AoI. Theoretical and simulation results indicate that the SIC-enabled packet relaying scheme significantly reduces the average AoI compared to the non-SIC approaches, owing to an increased packet transmission frequency at the source and the effectiveness of SIC techniques at the relays.},
  archive      = {J_TMC},
  author       = {Xinhui Han and Haoyuan Pan and Zhaorui Wang and Jianqiang Li},
  doi          = {10.1109/TMC.2025.3529462},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5298-5311},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Successive interference cancellation-enabled timely status update in linear multi-hop wireless networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Against mobile collusive eavesdroppers: Cooperative secure transmission and computation in UAV-assisted MEC networks. <em>TMC</em>, <em>24</em>(6), 5280-5297. (<a href='https://doi.org/10.1109/TMC.2025.3529929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Uncrewed Aerial Vehicle (UAV)-assisted Mobile Edge Computing (MEC) networks, the security of transmission faces significant challenges due to the vulnerabilities of line-of-sight links and potential eavesdropping on two-hop links. This paper addresses these challenges with an innovative Cooperative Secure Transmission and Computation strategy (CSTC), specifically engineered for time-slotted UAV-assisted MEC networks plagued by mobile collusive eavesdroppers. These eavesdroppers significantly bolster their interception capabilities through coordinated and optimized movements, escalating the security threats. To neutralize these risks, the proposed CSTC employs the UAV and remote devices as helper nodes to emit jamming signals, thereby thwarting eavesdropping activities, while simultaneously facilitating the efficient relay of users’ tasks to the base station for advanced processing. The CSTC aims to maximize the sum Secrecy Transmission Rate (STR) satisfying task latency constraints. It involves a joint optimization of UAV trajectory, jamming beamformers, transmit power, and data offloading strategy to expedite task transmission. Additionally, a real-time computation scheduling approach is developed based on a newly defined metric, the Urgency Degree of Users (UDoU), to enhance task processing efficiency. Our extensive simulations validate that the CSTC not only elevates the sum STR but also consistently meets latency constraints, demonstrating its robustness against advanced mobile eavesdropping techniques.},
  archive      = {J_TMC},
  author       = {Mingxiong Zhao and Zirui Wang and Kun Guo and Rongqian Zhang and Tony Q. S. Quek},
  doi          = {10.1109/TMC.2025.3529929},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5280-5297},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Against mobile collusive eavesdroppers: Cooperative secure transmission and computation in UAV-assisted MEC networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utility-enhanced personalized privacy preservation in hierarchical federated learning. <em>TMC</em>, <em>24</em>(6), 5264-5279. (<a href='https://doi.org/10.1109/TMC.2025.3531919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a distributed learning framework that allows clients to jointly train a model by uploading parameter updates rather than sharing local data. FL deployed on a client-edge-cloud hierarchical architecture, named Hierarchical Federated Learning (HFL), can accelerate model training and accommodate more clients with reduced communication cost via edge aggregation. Unfortunately, HFL suffers from privacy risks since the submitted parameters from clients are vulnerable to privacy attacks. To address this issue, we propose a novel Differential Privacy (DP) definition tailored for HFL, i.e., Group Local Differential Privacy (GLDP). We design the Sampling-Randomizing-Shuffling (SRS) mechanism to implement GLDP in HFL, where the sampling process is employed to achieve a stronger level of privacy protection with less noise added. By combining the randomized response and the shuffling mechanism, our proposed SRS mechanism can achieve client-level personalization within $\rho _{k}$-GLDP for privacy preservation while balancing model performance and privacy protection in HFL. Privacy analysis and convergence analysis are conducted to provide theoretical performance guarantees. Experimental results based on real-world datasets verify the effectiveness of SRS.},
  archive      = {J_TMC},
  author       = {Jianan Chen and Honglu Jiang and Qin Hu},
  doi          = {10.1109/TMC.2025.3531919},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5264-5279},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Utility-enhanced personalized privacy preservation in hierarchical federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeExp: Revealing model vulnerabilities for spatio-temporal mobile traffic forecasting with explainable AI. <em>TMC</em>, <em>24</em>(6), 5245-5263. (<a href='https://doi.org/10.1109/TMC.2025.3531544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to perform mobile traffic forecasting effectively with Deep Neural Networks (DNN) is instrumental to optimize resource management in 5G and beyond generation mobile networks. However, despite their capabilities, these Deep Neural Networks (DNN)s often act as complex opaque-boxes with decisions that are difficult to interpret. Even worse, they have proven vulnerable to adversarial attacks which undermine their applicability in production networks. Unfortunately, although existing state-of-the-art EXplainable Artificial Intelligence (XAI) techniques are often demonstrated in computer vision and Natural Language Processing (NLP), they may not fully address the unique challenges posed by spatio-temporal time-series forecasting models. To address these challenges, we introduce DeExp in this paper, a tool that flexibly builds upon legacy EXplainable Artificial Intelligence (XAI) techniques to synthesize compact explanations by making it possible to understand which Base Stations (BSs) are more influential for forecasting from a spatio-temporal perspective. Armed with such knowledge, we run state-of-the-art Adversarial Machine Learning (AML) techniques on those BSs to measure the accuracy degradation of the predictors under adversarial attacks. Our comprehensive evaluation uses real-world mobile traffic datasets and demonstrates that legacy XAI techniques spot different types of vulnerabilities. While Gradient-weighted Class Activation Mapping (GC) is suitable to spot BSs sensitive to moderate/low traffic injection, LayeR-wise backPropagation (LRP) is suitable to identify BSs sensitive to high traffic injection. Under moderate adversarial attacks, the prediction error of the BSs identified as vulnerable can increase by more than 250%.},
  archive      = {J_TMC},
  author       = {Serly Moghadas Gholian and Claudio Fiandrino and Narseo Vallina-Rodríguez and Marco Fiore and Joerg Widmer},
  doi          = {10.1109/TMC.2025.3531544},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5245-5263},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeExp: Revealing model vulnerabilities for spatio-temporal mobile traffic forecasting with explainable AI},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open-set occluded person identification with mmWave radar. <em>TMC</em>, <em>24</em>(6), 5229-5244. (<a href='https://doi.org/10.1109/TMC.2025.3529735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio frequency sensors can penetrate non-metal objects and provide complementary information to vision sensors for person identification (PID) purposes. However, there is a lack of research on millimeter wave (mmWave) radar for PID under occlusions, particularly in addressing the open-set recognition problem. Thus, we propose an open-set occluded PID (OSO-PID) framework that can deal with various obstacle and occlusion scenarios with open-set recognition capability. We first introduce a new dataset, mmWave-ocPID, comprising mmWave radar measurements and RGB-depth images, collected from 23 human subjects. We next design a novel neural network, mm-PIDNet, for occluded person identification using mmWave radar measurements. mm-PIDNet incorporates a transformer encoder, a bidirectional long short-term memory module, and a novel supervised contrastive learning module to improve PID performance. For open-set recognition, we enhance the mmWave radar-based PID method by integrating supervised contrastive learning with the Weibull models, which can identify out-of-distribution samples. We perform extensive indoor experiments with a variety of obstacles and occlusion scenarios. Our experimental results show that mm-PIDNet achieves an F1-score of 0.93 on average, outperforming state-of-the-art methods by up to 13.41% for occluded cases. For open-set PID, the OSO-PID framework achieves an F1-score above 0.8 when the openness is less than 14.36%.},
  archive      = {J_TMC},
  author       = {Tao Wang and Yang Zhao and Ming-Ching Chang and Jie Liu},
  doi          = {10.1109/TMC.2025.3529735},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5229-5244},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Open-set occluded person identification with mmWave radar},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SnapCFL: A pre-clustering-based clustered federated learning framework for data and system heterogeneities. <em>TMC</em>, <em>24</em>(6), 5214-5228. (<a href='https://doi.org/10.1109/TMC.2025.3529487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as a promising framework to address data privacy concerns associated with mobile devices, in contrast to conventional Machine Learning (ML). However, traditional FL encounters significant challenges due to the heterogeneities among different clients. Clustered Federated Learning (CFL) has demonstrated effectiveness in mitigating the data heterogeneity challenge, which significantly limits a broader application of FL. Nevertheless, existing CFL approaches often tightly couple the clustering process with the main FL process, affecting the flexibility and performance of CFL. In this paper, we propose a pre-clustering-based CFL approach, named SnapCFL, which decouples the CFL process into pre-clustering and main FL stages, considering both the impact of heterogeneity on CFL accuracy and the framework's flexibility. The pre-clustering stage models the measurement of data similarity as a two-sample hypothesis testing problem to more accurately group clients and alleviate data heterogeneity. In the main FL stage, a constraint-based client selection method is employed to address the system heterogeneity problem. We conduct extensive experiments using popular datasets with various heterogeneity settings. The results demonstrate that SnapCFL achieves excellent performance in terms of accuracy and efficiency. Compared to five other state-of-the-art approaches, SnapCFL can improve model accuracy by 0.7%$\sim$36.4%, and achieve the same level of accuracy with at least 0.08× the convergence time.},
  archive      = {J_TMC},
  author       = {Yujun Cheng and Weiting Zhang and Zhewei Zhang and Jiawen Kang and Qi Xu and Shengjin Wang and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3529487},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5214-5228},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SnapCFL: A pre-clustering-based clustered federated learning framework for data and system heterogeneities},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DualRec: A collaborative training framework for device and cloud recommendation models. <em>TMC</em>, <em>24</em>(6), 5202-5213. (<a href='https://doi.org/10.1109/TMC.2025.3528967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommendation systems (RS) play a vital role in various domains. However, under recent data regulations like General Data Protection Regulation (GDPR), traditional RS that rely on collecting user's interaction data centrally face significant challenges. Federated learning (FL) enables collaborative model training among users while keeping their private data locally. Yet, the constrained resources of devices often limit the size of the learned model, resulting in suboptimal recommendation performance. To overcome the dilemma of data accessibility and model size, we propose DualRec, a novel collaborative training framework for device and cloud recommendation models. In DualRec, users train lightweight models on devices to harness their local private data, while a larger model is simultaneously trained on the cloud server to exploit its substantial resources. Devices and the cloud server collaboratively train their models, compensating for individual limitations of model size and data availability, enabling mutual empowerment and benefits. Specifically, we introduce an efficient aggregation mechanism for recommendation models to boost the collaborative training performance of device models. With the learned device models, we propose to generate pseudo user interaction data to train the server model. To enhance the training performance of the server model, we design an automated denoising mechanism to mitigate the negative impact of noisy samples in the generated pseudo dataset. Finally, the learned knowledge of the server model is distilled to device models for enhanced on-device recommendation performance. Extensive experiments demonstrate the superior performance of DualRec compared to state-of-the-art baselines.},
  archive      = {J_TMC},
  author       = {Ye Zhang and Yongheng Deng and Sheng Yue and Qiushi Li and Ju Ren},
  doi          = {10.1109/TMC.2025.3528967},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5202-5213},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DualRec: A collaborative training framework for device and cloud recommendation models},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLVS: A self-learning approach to achieve near-second low-latency video streaming under highly variable networks. <em>TMC</em>, <em>24</em>(6), 5189-5201. (<a href='https://doi.org/10.1109/TMC.2025.3528635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fueled by the rapid advances in high-speed mobile networks, live video streaming has seen explosive growth in recent years and some DASH-based algorithms were specifically proposed for low-latency video delivery. We conducted a measurement study for the state-of-the-art algorithms with large-scale network traces. It reveals that these algorithms are susceptible to network condition changes due to the use of solo universal adaptation logics, resulting in the playback latency that has substantial variations across highly fluctuating networks. To tackle this challenge, this paper proposes Stateful Live Video Streaming (SLVS), which is a novel self-learning approach that learns the various network features and optimizes the adaptation logic separately for different network conditions, then dynamically tunes the logic at runtime, so that bitrate decision can better match the changing networks. Moreover, we further generalize SLVS to complement the streaming platform already in service to make it compatible with any live streaming services. Extensive evaluations based on real system prototypes show that SLVS can control playback latency down to 1 s while improving Quality-of-Experience (QoE) by 17.7% to 31.8%. Moreover, it has strong robustness to maintain near-second latency over highly fluctuating networks as well as long periods of video viewing.},
  archive      = {J_TMC},
  author       = {Guanghui Zhang and Ke Liu and Mengbai Xiao and Bingshu Wang and Dongxiao Yu and Xiuzhen Cheng},
  doi          = {10.1109/TMC.2025.3528635},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5189-5201},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SLVS: A self-learning approach to achieve near-second low-latency video streaming under highly variable networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AAV swarm cooperative search based on scalable multiagent deep reinforcement learning with digital twin-enabled sim-to-real transfer. <em>TMC</em>, <em>24</em>(6), 5173-5188. (<a href='https://doi.org/10.1109/TMC.2025.3530438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative target search (CTS) technology is highly desirable in various multi-autonomous aerial vehicle (AAV) applications. However, searching for unknown targets in a dynamic threatening environment is a challenging problem, especially for AAVs with limited sensing range and communication capabilities. Besides, traditional searching methods lack scalability and efficient collaboration among the AAV swarm in dynamic environments. In this work, a digital twin (DT)-enabled distributed CTS approach was presented for AAV swarms and achieving sim-to-real transfer. Specifically, a new scalable multi-agent reinforcement learning (MARL) based algorithm called SAMARL is adopted to improve effectiveness and adaptability, combining a multi-head attention mechanism. In SAMARL, a scalable observation space with graph representation and an environmental cognition map is designed to thoroughly consider the target search rate, area coverage, and safety assurance. Then, a DT-driven training framework is proposed to facilitate the continuous evolution of MARL models and address the tradeoff between training speed and environment fidelity. Furthermore, we innovatively develop a distributed AAV swarm digital twin cooperative target search validation system, including real flight control, communication simulation tools, and a 3D physics engine. Extensive simulations validate its superiority compared to state-of-the-art strategies. More importantly, we also conduct real-world flight experiments on different scale mission areas and AAV swarms, further demonstrating the generalization and scalability of trained models.},
  archive      = {J_TMC},
  author       = {Pan Cao and Lei Lei and Gaoqing Shen and Shengsuo Cai and Xiaojiao Liu and Xiaochang Liu},
  doi          = {10.1109/TMC.2025.3530438},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5173-5188},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AAV swarm cooperative search based on scalable multiagent deep reinforcement learning with digital twin-enabled sim-to-real transfer},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ML-track: Passive human tracking using WiFi multi-link round-trip CSI and particle filter. <em>TMC</em>, <em>24</em>(6), 5155-5172. (<a href='https://doi.org/10.1109/TMC.2025.3529897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present ML-Track, an innovative uncooperative passive tracking system leveraging WiFi communication signals between multiple devices. Our approach is realized with three pivotal techniques. First, we introduce a novel protocol termed multi-link round-trip CSI, which enables multi-link bistatic Doppler detection within a WiFi network. Second, a phase error cancellation method is developed, and we demonstrate a 0.92 rad reduction in error (0.96 to 0.04 rad) experimentally. Lastly, we propose a particle-filter-based back-end to track a moving human in the room passively without the need for the participant to carry any type of cooperative or active device. A prototype system is constructed using four Raspberry Pi CM4 units and subjected to real-world evaluations. Experimental results indicate a median error of approximately 0.23 m for tracking, which corresponds to a relative error of 5.8% based on the 4 m side length of the experimental field. Compared to existing studies, a distinct advantage of our system is it can run with non-MIMO (single-antenna) WiFi devices, making it particularly suitable for budget or low-profile WiFi hardware. This compatibility makes it an ideal fit for real-world Internet-of-Things (IoT) devices. Moreover, in terms of computational demands, our solution excels, delivering real-time performance on the Raspberry Pi CM4 while utilizing just 20% of its CPU capability and drawing a modest 2.5 watts of power.},
  archive      = {J_TMC},
  author       = {Fangzhan Shi and Wenda Li and Chong Tang and Yuan Fang and Paul V. Brennan and Kevin Chetty},
  doi          = {10.1109/TMC.2025.3529897},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5155-5172},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ML-track: Passive human tracking using WiFi multi-link round-trip CSI and particle filter},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MC-2PF: A multi-edge cooperative universal framework for load prediction with personalized federated deep learning. <em>TMC</em>, <em>24</em>(6), 5138-5154. (<a href='https://doi.org/10.1109/TMC.2025.3528404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging load prediction techniques support up-front and rational resource provisioning in edge systems to enhance system efficiency and Quality-of-Service (QoS). Classic prediction methods may handle loads with apparent trends, but they cannot achieve accurate prediction for highly-variable edge loads. With the advantage of sequential data analysis, recurrent neural networks (RNNs) are often used for load prediction but reveal limited generalization ability and low training efficiency. Moreover, it is hard to obtain a well-performed prediction model by discrete single-edge training with insufficient historical data. To address these important challenges, we propose a novel Multi-edge Cooperative universal framework for load Prediction with Personalized Federated deep learning (MC-2PF), enabling multi-edge cooperative training of load prediction models. Specifically, to solve the client-drift issue in federated learning (FL) caused by distinct data distribution, we customize personalized models for each edge by independent control parameters and theoretically analyze the model convergence improvement. Meanwhile, we prove the generalization bound of the MC-2PF and its universality to RNN-based prediction models through a practical example. Using the real-world testbed and load datasets, extensive experiments verify the effectiveness and practicality of the MC-2PF for different RNN-based prediction models. Compared to state-of-the-art frameworks, the MC-2PF achieves higher prediction accuracy, faster convergence, and stronger adaptiveness.},
  archive      = {J_TMC},
  author       = {Zheyi Chen and Qingnan Jiang and Lixian Chen and Xing Chen and Jie Li and Geyong Min},
  doi          = {10.1109/TMC.2025.3528404},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5138-5154},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MC-2PF: A multi-edge cooperative universal framework for load prediction with personalized federated deep learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time cross-domain gesture and user identification via COTS WiFi. <em>TMC</em>, <em>24</em>(6), 5124-5137. (<a href='https://doi.org/10.1109/TMC.2025.3532295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi-based gesture recognition has emerged as a promising alternative to computer vision, enabling seamless integration and enhanced interaction in human-computer interaction systems. Simultaneously identifying users during gesture recognition is vital for improving security and personalization. However, existing WiFi-based dual-task recognition approaches often rely on handcrafted features, which hinder precision and introduce delays in cross-domain scenarios. To address these challenges, we propose WiDual, a real-time system for cross-domain gesture recognition and user identification using WiFi signals. By integrating spatial and channel attention mechanisms, WiDual adaptively extracts crucial features for dual-task recognition. The system employs Channel State Information (CSI) visualization to convert WiFi signals into images, facilitating efficient feature extraction and minimizing information loss and latency. Furthermore, a collaborative module fuses gesture and user identity features, enhancing recognition performance. Experimental evaluations on a public dataset with six gestures and six users across diverse environments demonstrate WiDual's effectiveness. It achieves 96% accuracy in cross-domain gesture recognition and 91.27% in user identification. Compared to state-of-the-art methods, WiDual improves user identification accuracy by 26%, gesture recognition by 8%, and reduces processing time sixfold, showcasing its potential for real-time applications.},
  archive      = {J_TMC},
  author       = {Chenhong Cao and Yue Ding and Miaoling Dai and Wei Gong and Xibin Zhao},
  doi          = {10.1109/TMC.2025.3532295},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5124-5137},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time cross-domain gesture and user identification via COTS WiFi},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepVLP: A graph neural network-based denoising and signals optimization framework for visible light positioning. <em>TMC</em>, <em>24</em>(6), 5106-5123. (<a href='https://doi.org/10.1109/TMC.2025.3528440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible Light Positioning (VLP) has emerged as a promising technique in the Internet of Things landscape and gained increasing attention worldwide due to its widely existing infrastructure, high precision, and cost-effectiveness. Recently, ratio and difference-based VLP systems have been used to reduce errors from environmental noise, ambient light, and device differences. However, there may be intricate interference patterns that simple ratios and differences struggle to address. Moreover, a single LED often has limited capability to achieve self-diagnosis and self-correction. In fact, the information from other LEDs can be used to refine the signal and suppress interference. Thus, we propose to organize the VLP system in a graph and use the Graph Neural Network to model the interrelationships among LED lamps. This allows us to optimize the signals and further efficiently suppress interferences by simultaneously considering multiple LED lamps. In addition, the precisions of LEDs’ measurements is different due to various factors (e.g., distances and powers), and low-precision measurements may reduce the performance of the VLP system. To address this issue, we incorporate an attention layer to allow our model to give higher weights to high-precision measurements. Finally, the long short-term memory network is used to model the temporal dependencies between adjacent positions in a trajectory. Taking these modules together, we develop a robust VLP system called DeepVLP. The comprehensive experiments demonstrate that DeepVLP achieves better performance than state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Xiansheng Yang and Yuan Zhuang and Min Shi and Qian Meng and Jun Xiong and Yue Cao},
  doi          = {10.1109/TMC.2025.3528440},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5106-5123},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeepVLP: A graph neural network-based denoising and signals optimization framework for visible light positioning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking the effect of sparse data completion on sparse mobile crowdsensing tasks. <em>TMC</em>, <em>24</em>(6), 5094-5105. (<a href='https://doi.org/10.1109/TMC.2025.3531362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowdsensing (MCS) is a powerful technique that enables a variety of urban tasks, including temperature monitoring, location-based services, and urban path recommendations. However, these tasks often face the challenge of sparse and incomplete sensing data, undermining their effectiveness and reliability. Sparse data completion (SDC) methods have been developed to infer missing or unobserved data by leveraging spatio-temporal correlations to tackle this issue. This forms the core concept of the sparse mobile crowdsensing problem (SMCS), which aims to improve the performance of downstream tasks through inferred data. Despite the potential benefits, most existing SMCS methods fail to consider the trade-off between the cost of SDC and the benefits for downstream tasks. These methods often treat SDC and downstream tasks as independent modules, resulting in suboptimal outcomes. In this paper, we investigate the impact of SDC on the SMCS paradigm, both qualitatively and quantitatively. We establish the upper bound of performance achievable when applying SDC in SMCS under different levels of sensing data sparsity. Based on these studies and findings, we propose a practical and flexible framework called SDC-EVA, Sensing Data Completion EVAluation framework. This framework allows for applying different SDC methods in SMCS, considering factors such as computing complexity, storage space, and associated costs. Our proposed framework allows researchers to assess the necessity and feasibility of integrating SDC into SMCS systems before designing and deploying them in real-world scenarios. This assessment can be tailored to specific data sparsity and contextual information. To validate the effectiveness of our proposed evaluation framework, we conduct experiments in various real-world scenarios involving different combinations of SDC and downstream tasks. The results demonstrate the superiority of our framework in improving the performance of SMCS. By presenting these findings, we aim to contribute to developing SMCS techniques and provide valuable insights for researchers and practitioners.},
  archive      = {J_TMC},
  author       = {Yuanbo Xu and Jiawei Liu and En Wang and Bo Yang and Dongming Luan and Yongjian Yang and Jing Deng},
  doi          = {10.1109/TMC.2025.3531362},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5094-5105},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Rethinking the effect of sparse data completion on sparse mobile crowdsensing tasks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient subcarrier-level OFDM backscatter communications. <em>TMC</em>, <em>24</em>(6), 5078-5093. (<a href='https://doi.org/10.1109/TMC.2025.3529265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the existing OFDM backscatter systems adopt phase-modulated schemes to embed tag data, suffering from symbol-level modulation limitation, heavy synchronization accuracy reliance, and small tolerability to symbol time offset (STO) / carrier frequency (CFO) offset. We introduce SubScatter, the first subcarrier-level frequency-modulated OFDM backscatter which is able to tolerate bigger synchronization errors, STO, and CFO. The unique feature of SubScatter is our subcarrier shift keying (SSK) modulation. This method pushes the modulation granularity to the subcarrier by encoding and mapping tag data into different subcarrier patterns. We also design a tandem frequency shift (TFS) scheme that enables SSK with low cost and low power. Furthermore, we design SubScatter+ that shows these advantages while providing an even higher throughput without requiring more subcarrier patterns. We prototype and test SubScatter and SubScatter+, and the results show that our systems outperforms prior works in terms of effectiveness and robustness. Specifically, SubScatter has 743 kbps throughput that is 3.1 times and 14.9 times higher than RapidRider and MOXcatter, respectively. It also has a lower BER under noise and interferences which is over 6 times better than RapidRider or MOXcatter. Moreover, our proposed SubScatter+ could increase the throughput of SubScatter by 30%.},
  archive      = {J_TMC},
  author       = {Caihui Du and Jihong Yu and Zhenyu Yan and Ju Ren and Rongrong Zhang and Yun Li},
  doi          = {10.1109/TMC.2025.3529265},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5078-5093},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient subcarrier-level OFDM backscatter communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy efficient AAV-assisted bidirectional relaying system for multi-pair user devices. <em>TMC</em>, <em>24</em>(6), 5061-5077. (<a href='https://doi.org/10.1109/TMC.2025.3526981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomousaerial vehicles (AAVs), or drones, are garnering considerable focus in the realm of wireless communications research because to their notable characteristics, including exceptional mobility, versatile deployment capabilities, and robustness in maintaining line-of-sight (LoS) links. This paper studies a AAV-assisted bidirectional relaying system for multi-pair user devices (UDs), where a rotary-wing AAV is used to serve as a mobile relay for providing information transmission between UDs belonging to a pair on the ground. The UDs in a pair communicate with each other via the AAV relay employing the physical-layer network coding (PNC) technique. To trade off fair communications with system energy consumption, we jointly optimize transmission scheduling and association, AAV relay and UD transmission power, and AAV trajectory to maximize system energy efficiency during AAV relay communications. Due to the formulated problem being mixed-integer and nonconvex programming, it proves to be excessively complex to solve. For ease of solution, this problem is initially decomposed into three sub-problems. Next, by adopting the block coordinate descent (BCD) method, the successive convex approximation (SCA) method, and the Dinkelbach method, an efficient iterative algorithm is proposed that alternately solves variables of each sub-problem while fixing others. The numerical results demonstrate that our designed scheme is capable of substantially improving the system energy efficiency in comparison with other baseline schemes and benchmark schemes.},
  archive      = {J_TMC},
  author       = {Na Lin and Cong Peng and Ammar Hawbani and Cunqian Yu and Yanbo Fan and Liang Zhao},
  doi          = {10.1109/TMC.2025.3526981},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5061-5077},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Energy efficient AAV-assisted bidirectional relaying system for multi-pair user devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task offloading in internet of vehicles: A DRL-based approach with representation learning for DAG scheduling. <em>TMC</em>, <em>24</em>(6), 5045-5060. (<a href='https://doi.org/10.1109/TMC.2025.3531887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of the Internet-of-Vehicles (IoV) has amplified the need for mobile computing resources, driving the shift toward offloading tasks to edge servers or vehicles with idle resources to optimize computational efficiency. To this end, an approach based on Deep Reinforcement Learning (DRL) is presented in this paper, termed DVTP, which integrates Variational Graph Attention Networks (VGAT) and Transformer models to optimize Directed Acyclic Graph (DAG) task scheduling in vehicular networks. DVTP effectively captures both the spatiotemporal information and task dependencies, enabling more accurate and efficient task offloading decisions. Extensive simulation experiments demonstrate that DVTP outperforms traditional methods in reducing task completion times across various multi-vehicle and multi-edge server scenarios, showcasing its potential for real-world IoV applications.},
  archive      = {J_TMC},
  author       = {Xiaoheng Deng and Haoyu Yang and Jingjing Zhang and Jinsong Gui and Siyu Lin and Xin Wang and Geyong Min},
  doi          = {10.1109/TMC.2025.3531887},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5045-5060},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Task offloading in internet of vehicles: A DRL-based approach with representation learning for DAG scheduling},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-based machine unlearning for vertical federated learning in IoT networks. <em>TMC</em>, <em>24</em>(6), 5031-5044. (<a href='https://doi.org/10.1109/TMC.2025.3530529'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of the Internet of Things (IoT), managing the deluge of data generated by distributed devices presents unique challenges, particularly concerning privacy and the efficient use of computational resources. Vertical Federated Learning (VFL) offers a promising avenue for collaborative machine learning without centralizing data, thereby addressing privacy concerns inherent in traditional approaches. However, as data privacy laws and personal data deletion requests become more prevalent, the necessity for effective machine unlearning strategies within VFL frameworks grows increasingly important. To this end, this paper introduces a novel approach to feature-based machine unlearning tailored specifically for VFL systems in IoT networks. Our methodology enables the selective removal of data influence from trained models without the need for full retraining, thus preserving model utility while ensuring compliance with privacy requirements. By integrating a combination of feature relevance measuring techniques and efficient communication protocols, our solution minimizes the data footprint on network nodes, reduces bandwidth consumption, and maintains the integrity and performance of the learning models. To the best of our knowledge, our proposed framework represents the first practical approach to enable machine unlearning within vertical federated learning environments. We demonstrate the effectiveness of our approach through rigorous evaluation using several IoT datasets, highlighting significant improvements in unlearning efficiency and model robustness compared to existing techniques. Our work not only furthers the development of sustainable and compliant machine learning models in IoT but also sets a foundational framework for future research in secure and efficient data management within federated environments.},
  archive      = {J_TMC},
  author       = {Zijie Pan and Zuobin Ying and Yajie Wang and Chuan Zhang and Weiting Zhang and Wanlei Zhou and Liehuang Zhu},
  doi          = {10.1109/TMC.2025.3530529},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5031-5044},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Feature-based machine unlearning for vertical federated learning in IoT networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cooperative path planning with asynchronous multiagent reinforcement learning. <em>TMC</em>, <em>24</em>(6), 5016-5030. (<a href='https://doi.org/10.1109/TMC.2025.3526979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the number of vehicles grows in urban cities, planning vehicle routes to avoid congestion and decrease commuting time is important. In this paper, we study the shortest path problem (SPP) with multiple source-destination pairs, namely MSD-SPP, to minimize the average travel time of all routing paths. The asynchronous setting in MSD-SPP, i.e., vehicles may not simultaneously complete routing actions, makes it challenging for cooperative route planning among multiple agents and leads to ineffective route planning. To tackle this issue, in this paper, we propose a two-stage framework of inter-region and intra-region route planning by dividing an entire road network into multiple sub-graph regions. Next, the proposed asyn-MARL model allows efficient asynchronous multi-agent learning by three key techniques. First, the model adopts a low-dimensional global state to implicitly represent the high-dimensional joint observations and actions of multi-agents. Second, by a novel trajectory collection mechanism, the model can decrease the redundancy in training trajectories. Additionally, with a novel actor network, the model facilitates the cooperation among vehicles towards the same or close destinations, and a reachability graph can prevent infinite loops in routing paths. On both synthetic and real road networks, the evaluation result demonstrates that asyn-MARL outperforms state-of-the-art planning approaches.},
  archive      = {J_TMC},
  author       = {Jiaming Yin and Weixiong Rao and Yu Xiao and Keshuang Tang},
  doi          = {10.1109/TMC.2025.3526979},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5016-5030},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Cooperative path planning with asynchronous multiagent reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards understanding the impact of participant and its wearable devices in federated learning. <em>TMC</em>, <em>24</em>(6), 5003-5015. (<a href='https://doi.org/10.1109/TMC.2025.3530818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of wearable smart devices has increased due to their seamless monitoring of vital signs during daily activities. Federated learning leverages these devices along with participants’ smartphones to fine-tune pre-trained models. Moreover, calibrating the differences between wearables and smartphones in terms of sampling rates, orientations, activity correlation, battery power, and other factors is challenging. Thus, the paper introduces a participant and wearable selection cross-device federated learning approach. It leverages criteria such as the activity wearable(s) relationship, data quality, battery life, sampling rate, and so on to perform the wearable selection. The server evaluates and estimates the utility of each participant and selects those with higher utility in each communication round. We then figure out the optimal weighted contribution of each participant to perform robust aggregation. We also use knowledge distillation techniques to develop a high-performing and lightweight wearable model. Finally, we conduct simulation and real-world experiments on existing datasets and compare our approach with state-of-the-art. The result shows an improvement of $3\!\!-\!\!4\%$ in accuracy via fine-tuning from selected wearable data.},
  archive      = {J_TMC},
  author       = {Rahul Mishra and Hari Prabhat Gupta},
  doi          = {10.1109/TMC.2025.3530818},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {5003-5015},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards understanding the impact of participant and its wearable devices in federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Field evaluation of a softwarized modem from the perspective of 5G cell search. <em>TMC</em>, <em>24</em>(6), 4987-5002. (<a href='https://doi.org/10.1109/TMC.2025.3526753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modem softwarization, where baseband signals are fully processed using software on a general-purpose CPU, is a promising technology in mobile communications due to its simplicity and flexibility in realizing various features. On the other hand, many still question the effectiveness of a softwarized modem in commercial environments concerning performance and complexity. Motivated by this perspective, this paper presents the design and implementation of a softwarized modem with the specific feature of 5G cell search for field evaluation. Based on the baseline algorithms of 5G cell search in Open Air Interface (OAI), we propose a new software architecture which can efficiently manage a 5G cell search procedure and decompose the overall 5G cell search into sub-algorithms. We also design and implement novel sub-algorithms that enhance the detection of Synchronization Signal Blocks (SSBs). Our softwarized modem utilizes dual-rate sampling to significantly reduce computation complexity during timing offset estimation. It also adaptively detects synchronization signals or cell identities based on the presence of inter-cell interference or multi-path fading. The performance evaluation through field experiments concludes that our softwarized modem outperforms the baseline, and the proposed sub-algorithms are effective in enhancing cell search performance. The detection probability and time consumption results for our softwarized modem confirm that it is feasible for commercial uses.},
  archive      = {J_TMC},
  author       = {Dawoon Lim and Subin Jeong and Bitna Kim and Yelan Lee and Hye Jin Shin and Juyeop Kim},
  doi          = {10.1109/TMC.2025.3526753},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4987-5002},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Field evaluation of a softwarized modem from the perspective of 5G cell search},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Through-wall mobile charging: Theory, methodology, and implementation. <em>TMC</em>, <em>24</em>(6), 4971-4986. (<a href='https://doi.org/10.1109/TMC.2025.3527440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Power Transfer (WPT) has revolutionized the field of Wireless Rechargeable Sensor Networks (WRSNs), enabling sustainable operation of sensor nodes. Traditional mobile charging methods often require sensors to be within line-of-sight or physically accessed by the mobile charger, which may potentially lead to user safety or privacy concerns. Addressing this concern, this work is the first to introduce and validate the feasibility of Through-Wall charging. We formulate the Wireless charging thrOugh Walls (WOW) problem to simultaneously enhance user safety and maximize charging utility. Our approach leverages fundamental principles of electromagnetics to construct an accurate charging model for Magnetic Resonance Coupling-based WPT systems. Additionally, we thoroughly analyze the impact of wall obstruction and provide a generalized framework for through-wall charging. By employing discretization techniques and approximation algorithms, we derive a near-optimal solution to the WOW problem. Extensive simulations and test-bed experiments demonstrate that our proposed approach reduces the reliance on physical access to devices, simplifies deployment in complex environments, and thereby optimizes the travel paths of mobile chargers and enhances the overall performance and lifetime of WRSNs. Compared to conventional methods, our method benefits from more reasonable scheduling order and path construction, achieving an average energy efficiency improvement of 27.8%.},
  archive      = {J_TMC},
  author       = {Yu Sun and Chi Lin and Wei Yang and Haipeng Dai and Jiankang Ren and Lei Wang and Guowei Wu},
  doi          = {10.1109/TMC.2025.3527440},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4971-4986},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Through-wall mobile charging: Theory, methodology, and implementation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRFusion: Fine-grained object identification using RF-image modality fusion. <em>TMC</em>, <em>24</em>(6), 4957-4970. (<a href='https://doi.org/10.1109/TMC.2025.3527872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object identification is a pivotal enabling technique for smart home and manufacturing applications. Traditional methodologies for object identification predominantly rely on a singular sensor modality, which inherently limits their ability to furnish a detailed characterization of the target object. Addressing this deficiency, in this paper, we fill this gap by introducing CRFusion, the first-of-its-kind system that integrates the object RGB image and the radio frequency (RF) signal reflected by the object for fine-grained object identification. CRFusion leverages the complementary characteristics between visible light and radio frequency modalities to simultaneously determine the category and material of target objects. We design a multifaceted object feature from the RF signal, called the Energy Reflection Factor (ERF), which not only reveals the object texture but complements the image modality for identifying the object category. By integrating the characteristics of radar, we obtain radar feature maps based on the ERF of target objects. Additionally, we have developed a modality fusion network to comprehensively integrate the image and ERF features. We conducted a comprehensive evaluation of CRFusion using a commercial mmWave radar development board and camera. The results show that CRFusion achieves a classification accuracy of over 96%, demonstrating its robustness, and potential for application.},
  archive      = {J_TMC},
  author       = {Liyang Xiao and Yanni Yang and Zhe Chen and Yue Gao and Prasant Mohapatra and Pengfei Hu},
  doi          = {10.1109/TMC.2025.3527872},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4957-4970},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CRFusion: Fine-grained object identification using RF-image modality fusion},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal knowledge driven diffusion model for mobile traffic generation. <em>TMC</em>, <em>24</em>(6), 4939-4956. (<a href='https://doi.org/10.1109/TMC.2025.3527966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating mobile traffic in urban environments is important for network planning and optimization. However, existing models show weakness in capturing spatial-temporal dynamics between mobile traffic and urban environments. This makes it difficult for the models to generate high-fidelity traffic data and control the generation process across different regions in large-scale urban environments, ultimately affecting the effectiveness of optimization strategies. In this paper, we propose a Spatio-Temporal Knowledge-driven Diffusion model (STK-Diff) for controllable mobile traffic generation. We construct an Urban Knowledge Graph (UKG) to fully characterize the urban features, which incorporates both the spatial and semantic relations of different entities, such as base stations, business areas, and functional regions. Based on the constructed UKG, we design the denoising network of diffusion model with a temporal extraction module and a spatial connection module. These two modules capture the correlations of mobile traffic and environment features via a frequency attention mechanism and spatial graph learning scheme, so as to make a strong controllability on the generated mobile traffic. Extensive experiments on three real-world datasets show that the proposed framework not only improves generation fidelity by up to 19%, but also enhances the controllability to generate specific patterns, with a gain of surpassing 15%.},
  archive      = {J_TMC},
  author       = {Haoye Chai and Xiaoqian Qi and Yong Li},
  doi          = {10.1109/TMC.2025.3527966},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4939-4956},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Spatio-temporal knowledge driven diffusion model for mobile traffic generation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security-enhanced data sharing via efficient sanitization for VANETs. <em>TMC</em>, <em>24</em>(6), 4925-4938. (<a href='https://doi.org/10.1109/TMC.2025.3527935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread deployment of vehicular ad-hoc networks (VANETs), data sharing has garnered considerable attention as a core feature of VANETs. Attribute-based proxy re-encryption (ABPRE) enables fine-grained access control and provides flexible ciphertext updates. The initially authorized vehicle generates the re-encryption key to enable ciphertext-to-ciphertext conversion in the cloud, allowing ciphertext to be shared with new recipients. However, initially authorized vehicles may not always be trustworthy and could share data with malicious receivers. In addition, the computation and communication overhead of ABPRE hinders its widespread application in VANETs. To address these issues, we propose a lightweight sanitizable scheme for edge-assisted VANETs based on ABPRE. In this scheme, the re-encryption key is verified by a sanitizer, to prevent the data from being shared with malicious data receivers. In addition, key-splitting techniques and edge computing are employed to reduce the communication and computation overhead of re-encryption. A comprehensive security analysis and performance evaluation demonstrate that the proposed scheme is efficient and practical.},
  archive      = {J_TMC},
  author       = {Hong Zhong and Dan Zhou and Jie Cui and Li Wang and Jing Zhang and Irina Bolodurina and Debiao He},
  doi          = {10.1109/TMC.2025.3527935},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4925-4938},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Security-enhanced data sharing via efficient sanitization for VANETs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring impacts of age of information on data accuracy for wireless sensing systems: An information entropy perspective. <em>TMC</em>, <em>24</em>(6), 4907-4924. (<a href='https://doi.org/10.1109/TMC.2025.3527587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensing systems have been employed in the field of healthcare, environment monitoring, and smart agriculture, etc. Since the freshness and accuracy indicators of the sensing data are critical to wireless sensing systems, it is of great significance to ensure their performances simultaneously, i.e., the Age of Information (AoI) and information entropy of the sensing data should be jointly optimized. In this regard, we first establish the wireless sensing system models, including AoI and information entropy expressions. Next, from the information entropy viewpoint, we theoretically analyze an impact of the AoI on data accuracy. Then, we formulate the joint optimization problem of AoI, information entropy, and sensing energy consumption. Furthermore, we propose two numerical algorithms to solve the formulated problem in the known or unknown transmission environment, respectively. Finally, we evaluate the correctness and effectiveness of our proposals under various parameter settings, where the proposed scheme can obtain a better sum-weighted performance on AoI, information entropy, and sensing energy consumption than baselines in the literature.},
  archive      = {J_TMC},
  author       = {Yaoqi Yang and Hongyang Du and Zehui Xiong and Renhui Xu and Dusit Niyato and Zhu Han},
  doi          = {10.1109/TMC.2025.3527587},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4907-4924},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring impacts of age of information on data accuracy for wireless sensing systems: An information entropy perspective},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Average AoI minimization with directional charging for wireless-powered network edge. <em>TMC</em>, <em>24</em>(6), 4889-4906. (<a href='https://doi.org/10.1109/TMC.2025.3527559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age of Information (AoI) has been proposed as a new performance metric to capture the freshness of data. At wireless-powered network edge, the source nodes first need to be charged ready for update transmissions, which means the system AoI is not only decided by the scheduling of update transmissions but also by the designing of charging plan. However, the existing works either only focused on the point of scheduling update transmissions or have a rigid assumption that only one source node can be charged per time. Aiming at making the work more practical and general, we investigate the average AoI optimization problem at wireless-powered network edge with directional charging. First, the theoretical bound of the weighted sum of average AoI of the entire network with a directional charger is analyzed, which is proved to be related to nodes’ maximum transmitting interval and the charging strategy. An optimal charging time computation algorithm is proposed to obtain the maximum transmitting interval of each source node by considering the overlapped areas of different charging orientations. After then, an AoI-aware periodical charging scheduling algorithm is proposed to compute a periodical charging schedule while the average AoI is bounded, including a charging period $T$ and the charging orientations assigned to each time slot within $T$. The proposed algorithm is proved to have an approximation ratio of up to 1.5625. Furthermore, several approximate algorithms are also proposed for average AoI optimization with multiple chargers and network bandwidth constraint. Finally, the extensive simulations demonstrate the high performance of the proposed algorithm in terms of AoI.},
  archive      = {J_TMC},
  author       = {Quan Chen and Song Guo and Wenchao Xu and Jing Li and Tuo Shi and Hong Gao and Zhipeng Cai},
  doi          = {10.1109/TMC.2025.3527559},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4889-4906},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Average AoI minimization with directional charging for wireless-powered network edge},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JSFBA: Joint segment and frame bitrate adaptation for real-time video analytics. <em>TMC</em>, <em>24</em>(6), 4874-4888. (<a href='https://doi.org/10.1109/TMC.2025.3526867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the intensive computing resource requirements, real-time video analytics applications typically need to transmit video to a server. However, the transmission inevitably suffers from network bandwidth limitations and fluctuations, making it challenging to guarantee video analytics performance. In this paper, we aim to maximize video analytics accuracy while maintaining low latency and frame loss rate, and propose a joint segment and frame bitrate adaptation (JSFBA) framework for real-time video analytics, which incorporates two reinforcement learning-based algorithms to adapt to bandwidth at both the segment and frame levels. Initially, considering the effect of video encoding on video analytics, we employ a bitrate control method to design a segment-level bitrate adaptation (SLBA) algorithm with a unique reward function. Based on the historical information of the video segments, SLBA selects the appropriate bitrate for each segment. Subsequently, by leveraging the ability to generate multiple bitrates in scalable video coding (SVC), we design a frame-level bitrate adaptation (FLBA) algorithm, which adapts to bandwidth in a more fine-grained manner by determining the number of layers sent for each frame. Extensive experiments on large-scale network traces reveal that JSFBA effectively balances various video analytics performance metrics and achieves maximum utility compared to state-of-the-art solutions.},
  archive      = {J_TMC},
  author       = {Yiming Wang and Shuang Cheng and NianZhen Gao and Ting Bi and Tao Jiang},
  doi          = {10.1109/TMC.2025.3526867},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4874-4888},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {JSFBA: Joint segment and frame bitrate adaptation for real-time video analytics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling ultralow-latency services with ubiquitous mobility by means of a compact network architecture. <em>TMC</em>, <em>24</em>(6), 4858-4873. (<a href='https://doi.org/10.1109/TMC.2025.3526971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of emerging services such as cellular vehicle-to-everything and immersive video service, network connections have further evolved from tangible physical connections to intangible virtual connections such as content, services, and computing resources, and the application scenarios have become more abundant. The mobile ultra-service, which is characterized by ultra-low latency, ultra-high reliability, and ubiquitous mobility, is becoming one of the most representative traffic types. However, the existing mobile network architecture has not evolved sufficiently to meet the specific requirements of these mobile ultra-services, the mobility anchors introduce unnecessary node and link latency, leaving space for further optimization. A compact network architecture (ComArch) is proposed in this paper for ultralow-latency services with ubiquitous mobility. ComArch is designed with a mapping control plane and a generalized forwarding plane to collaboratively implement packet forwarding in mobile scenarios. The generalized forwarding plane handles packet forwarding, while the mapping control plane manages terminals’ identifier and locator mapping entries. The node latency introduced by mobility anchors is eliminated, and an efficient routing scheme is proposed to find the optimal mandatory nodes in the forwarding path, thereby reducing unnecessary link latency. Experimental results show that ComArch can effectively reduce end-to-end delay while saving resources.},
  archive      = {J_TMC},
  author       = {Guiliang Cai and Qiang Wu and Ran Wang and Lianyi Zhi and Xiaoming Fu and Hongke Zhang},
  doi          = {10.1109/TMC.2025.3526971},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4858-4873},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling ultralow-latency services with ubiquitous mobility by means of a compact network architecture},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Indirect-communication federated learning via mobile transporters. <em>TMC</em>, <em>24</em>(6), 4845-4857. (<a href='https://doi.org/10.1109/TMC.2025.3527405'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a distributed machine learning framework that efficiently reduces communication and preserves privacy. Existing FL algorithms typically rely on the assumption of direct communication between the server and clients for model data exchange. However, this assumption does not apply in many real-world scenarios where appropriate communication infrastructure is lacking, such as in remote smart sensing. To overcome this challenge, we propose a new framework, FedEx (Federated Learning via Model Express Delivery). FedEx employs mobile transporters, such as Unmanned Aerial Vehicles (UAVs), to establish indirect communication channels between the server and clients. We have developed two algorithms under this framework: FedEx-Sync and FedEx-Async, which differ based on whether the transporters operate on a synchronized or asynchronized schedule. Although indirect communication introduces variable delays in global model dissemination and local model collection, we demonstrate the convergence of both FedEx versions. Additionally, we explore the energy consumption of transporters, integrating it with the convergence bounds and proposing a bi-level optimization algorithm for efficient client assignment and route planning. Our experiments, conducted on two public datasets in a simulated environment, further demonstrate the efficacy of FedEx.},
  archive      = {J_TMC},
  author       = {Jieming Bian and Cong Shen and Mingzhe Chen and Jie Xu},
  doi          = {10.1109/TMC.2025.3527405},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4845-4857},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Indirect-communication federated learning via mobile transporters},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LaserKey: Eavesdropping keyboard typing leveraging vibrational emanations via laser sensing. <em>TMC</em>, <em>24</em>(6), 4829-4844. (<a href='https://doi.org/10.1109/TMC.2025.3529919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconstructing keyboard input through side-channel attacks has posed significant threats to user security. While conventional keystroke eavesdropping attacks have demonstrated effectiveness using side channels such as acoustic signals, they are usually shorter in range and can be significantly affected by environmental noises. In this paper, we propose LaserKey, a novel keystroke eavesdropping technique that leverages the long-range and noise-resistant nature of lasers to achieve a more stealthy side-channel attack. We utilize laser sensors to accurately capture the subtle vibrations induced on laptop screens by keystrokes, and innovatively design a laser-driven deep learning-based keystroke recognition model with the inputs being the Mel-frequency Cepstral Coefficien (MFCC), Time Difference of Arrival (TDoA), and amplitude features extracted from such vibration signals. Through systematic experiments, we demonstrate that LaserKey achieves a 92.2% single-key recognition accuracy. By combining multiple single-key recognition capabilities based on this, we then realize the end-to-end word-level recognition. Moreover, to mitigate the recognition errors caused by the changes in keystroke positions, we introduce a meta-learning based domain generalization approach for achieving robust laser position calibration. Results show that LaserKey achieves as low as 3% character error rate (CER) for word-level recognition, proving its effectiveness for long-range and high-accuracy keystroke eavesdropping, and highlighting the necessity for countermeasures in the future.},
  archive      = {J_TMC},
  author       = {Chengwen Luo and Zhuoqing Xie and Yuhan Huang and Gecheng Chen and Haiyi Yao and Jin Zhang and Long Cheng and Weitao Xu and Jianqiang Li},
  doi          = {10.1109/TMC.2025.3529919},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4829-4844},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LaserKey: Eavesdropping keyboard typing leveraging vibrational emanations via laser sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). O-RAN intelligence orchestration framework for quality-driven xApp deployment and sharing. <em>TMC</em>, <em>24</em>(6), 4811-4828. (<a href='https://doi.org/10.1109/TMC.2025.3527707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of 5G networks, with diverse traffic classes and demanding services, highlights the importance of Open Radio Access Networks (O-RAN) for enabling RAN intelligence and performance optimization. Machine Learning-powered xApps offer novel network control opportunities, but their resource demands necessitate efficient orchestration. To address these issues, we present OREO, an O-RAN xApp orchestrator that, using a multi-layer graph model, aims to maximize the number of RAN services concurrently deployed while minimizing their overall energy consumption. OREO's key innovation lies in the concept of sharing xApps across RAN services when they include semantically equivalent functions and meet quality requirements. Despite the NP-hard nature of the problem, numerical results show that OREO offers a lightweight and scalable solution that closely and swiftly approximates the optimum in several different scenarios. Also, OREO outperforms state-of-the-art benchmarks by enabling the co-existence of more RAN services (14.3% more on average and up to 22%), while reducing resource expenditure (by 48.7% less on average and up to 123% for computing resources). Moreover, using an experimental prototype deployed on the Colosseum network emulator and using real-world RAN services, we show that OREO leads to substantial resource savings (up to 66.7% of computing resources) while its xApp sharing policy can significantly enhance quality of service.},
  archive      = {J_TMC},
  author       = {Federico Mungari and Corrado Puligheddu and Andres Garcia-Saavedra and Carla Fabiana Chiasserini},
  doi          = {10.1109/TMC.2025.3527707},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4811-4828},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {O-RAN intelligence orchestration framework for quality-driven xApp deployment and sharing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FireExpert: Fire event identification and assessment leveraging cross-domain knowledge and large language model. <em>TMC</em>, <em>24</em>(6), 4794-4810. (<a href='https://doi.org/10.1109/TMC.2025.3528413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fire events threaten the safety of residents and the health of ecosystems in affected areas, and post-disaster recovery efforts also require a large investment of resources and time. In recent years, the rising frequency of fire events has motivated local governments to strengthen their monitoring and emergency response efforts. However, current fire event identification methods can only identify the presence of a fire, without the ability to distinguish its specific category. In addition, when a fire occurs, the lack of information about the affected areas makes it challenging for emergency management authorities to take timely and effective rescue measures. To address these issues, we propose a two-stage framework for fire event identification and assessment. Specifically, in the first stage, based on multi-band fused remote sensing images and heterogeneous environmental images, the proposed framework not only identifies various fire events but also accurately identifies the boundaries of the fire events. In the second stage, integrating the results of fire event identification with social media data and domain knowledge, we present a real-time assessment agent for fire events based on the large language model. This agent enables timely and accurate analysis of the impact of fires on the affected areas. We evaluate our method on a real-world authority dataset, and results show that our framework identifies fire events with an F1-score of 61.0$\%$ and a mAP of 57.7$\%$, which outperforms state-of-the-art baseline methods. In addition, the assessment results of fire events in real cases indicate that the proposed fire event assessment agent can assist emergency responders in obtaining timely and accurate information.},
  archive      = {J_TMC},
  author       = {Guofeng Luo and Lijuan Weng and Yunqian Li and Yilu Sun and Yayao Hong and Yongyi Wu and Ruixiang Luo and Leye Wang and Cheng Wang and Longbiao Chen},
  doi          = {10.1109/TMC.2025.3528413},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4794-4810},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FireExpert: Fire event identification and assessment leveraging cross-domain knowledge and large language model},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning-based tracking-before-detect for unconstrained indoor human tracking using RF signal. <em>TMC</em>, <em>24</em>(6), 4777-4793. (<a href='https://doi.org/10.1109/TMC.2025.3529501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human tracking plays a crucial role in various wireless sensing applications. However, recent advancements have primarily focused on constrained experimental scenarios with less interference, often involving a few individuals performing actions in an empty space without obstacles. In empirical unconstrained scenarios, such as daily office scenes, severe interference and attenuation caused by chaotic environments is inevitable which results in dramatic performance degradation. In this paper, we introduce TBDNet, which incorporates tracking-before-detect (TBD) from conventional signal processing into learning-based models, achieving impressive tracking performance in unconstrained scenarios. TBDNet follows first-track-then-detect pipeline. It maps input heatmap sequence into high-level frame-wise features to adapt the time-varying intensity distribution and motion pattern of targets. After that, the temporal information is accumulated in feature space to obtain trace proposals. We then predict the accurate positions and probability of traces at each timestamp. To assess the efficiency of TBDNet, we collect and release the first RF-UNIT (RF-based Unconstrained Indoor Tracking) dataset, which comprises 4,030,880 radar heatmaps and the corresponding tracking annotations under 6 different scenarios. To our knowledge, RF-UNIT is the first dataset for RF-based human tracking in unconstrained scenes. We anticipate that TBDNet and the RF-UNIT dataset will significantly contribute to the advancement of RF-based sensing technologies.},
  archive      = {J_TMC},
  author       = {Zhi Wu and Dongheng Zhang and Zixin Shang and Yuqin Yuan and Hanqin Gong and Binquan Wang and Zhi Lu and Yadong Li and Yang Hu and Qibin Sun and Yan Chen},
  doi          = {10.1109/TMC.2025.3529501},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4777-4793},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Learning-based tracking-before-detect for unconstrained indoor human tracking using RF signal},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness-aware budgeted edge server placement for connected autonomous vehicles. <em>TMC</em>, <em>24</em>(6), 4762-4776. (<a href='https://doi.org/10.1109/TMC.2025.3526873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) considerably enhances the capabilities and performance of connected autonomous vehicles (CAVs) by deploying edge servers (ESs) on roadside units (RSUs) near CAVs, thereby ensuring low-latency services. Given the constrained and costly nature of ES resources (computing, storage, and bandwidth), equitable ES utilization is critical for CAV operations. However, fairness considerations are often overlooked in current budgeted edge server placement (ESP) strategies, potentially worsening resource imbalances and compromising user experience. This paper investigates the fairness-aware budgeted edge server placement (FESP) problem within RSUs, proving its NP-hardness. To address FESP, we first propose FESP-O, an integer programming-based optimal approach for small-scale problems, followed by FESP-APX, an approximation approach for large-scale scenarios that provides near-optimal solutions. We analyze the time complexity and approximation ratio of our proposed algorithms and validate their efficacy through experiments on real-world datasets. Extensive experimental results demonstrate significant performance improvements over baseline and state-of-the-art methods, indicating practical suitability and efficiency.},
  archive      = {J_TMC},
  author       = {Jintao Wu and Xiaolong Xu and Guangming Cui and Yiwen Zhang and Lianyong Qi and Wanchun Dou and Zhipeng Cai},
  doi          = {10.1109/TMC.2025.3526873},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4762-4776},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fairness-aware budgeted edge server placement for connected autonomous vehicles},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FastTuner: Fast resolution and model tuning for multi-object tracking in edge video analytics. <em>TMC</em>, <em>24</em>(6), 4747-4761. (<a href='https://doi.org/10.1109/TMC.2025.3526573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) is the “killer app” of edge video analytics. Deploying MOT pipelines for live video analytics poses a significant system challenge due to their computation-intensive nature. In this paper, we propose FastTuner, a model-agnostic framework that aims to accelerate MOT pipelines by adapting frame resolutions and backbone models. Unlike prior works that utilize a separate and time-consuming online profiling procedure to identify the optimal configuration, FastTuner incorporates multi-task learning to perform configuration selection and object tracking through a shared model. Multi-resolution training is employed to further improve the tracking accuracy across different resolutions. Furthermore, two workload placement schemes are designed for the practical deployment of FastTuner in edge video analytics systems. Extensive experiments demonstrate that FastTuner can achieve 1.1%–9.2% higher tracking accuracy and 2.5%–25.5% higher speed compared to the state-of-the-art methods, and accelerate end-to-end processing by 1.7%–22.5% in a real-world testbed consisting of an embedded device and an edge server.},
  archive      = {J_TMC},
  author       = {Renjie Xu and Keivan Nalaie and Rong Zheng},
  doi          = {10.1109/TMC.2025.3526573},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4747-4761},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FastTuner: Fast resolution and model tuning for multi-object tracking in edge video analytics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate 3D wireless charging. <em>TMC</em>, <em>24</em>(6), 4733-4746. (<a href='https://doi.org/10.1109/TMC.2025.3526945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Rechargeable Sensor Networks (WRSNs) have become an important research issue as they can overcome the energy bottleneck problem of wireless sensor networks. However, existing 2D charging methods suffer from significant errors in 3D scenarios, which leads to a huge gap between theoretical results and practical applications, hindering the widespread adoption of WRSNs. In this paper, we address the chargIng utility maximizatioN problem In 3D environmenT (INIT) and provide a general solution suitable for any type of transceiver antenna. Specifically, we first establish an accurate 3D charging model to quantify the received power of sensors in 3D environments. Second, we design an angle-distance discretization scheme to determine appropriate charging spots for the Mobile Charger (MC). Then, we transform the mobile charging problem into a submodular function maximization problem and propose an approximation algorithm with guaranteed performance to solve it. Finally, our method has been extensively evaluated through experiments and simulations and has demonstrated considerable advantages over other comparison algorithms in real-world 3D environments. On average, it has achieved an impressive 34.8% improvement in charging utility and a remarkable 56.1% reduction in the number of dead sensors.},
  archive      = {J_TMC},
  author       = {Wei Yang and Chi Lin and Yu Sun and Haipeng Dai and Jiankang Ren and Lei Wang and Guowei Wu},
  doi          = {10.1109/TMC.2025.3526945},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4733-4746},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Accurate 3D wireless charging},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving wireless security with phase-tag physical-layer authentication. <em>TMC</em>, <em>24</em>(6), 4716-4732. (<a href='https://doi.org/10.1109/TMC.2025.3527220'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authentication is a fundamental requirement and a crucial topic in wireless communications. This paper focuses on improving the security of the prior tag-based Physical-Layer Authentication (PLA) schemes. We propose two phase-tag-based PLA schemes to overcome the limitations of prior tag-based PLA schemes. First, we propose the Phase-Tag-based PLA (PT-PLA) scheme, which superimposes a tag to the phase of the transmitted signal rather than the amplitude. Second, we propose the Adaptive Phase-Tag-based PLA (APT-PLA) scheme, where an appropriate parameter of the PT-PLA scheme is adaptively set for achieving a better trade-off among robustness, security, and compatibility. Rigorous theoretical analyses of the proposed schemes are conducted in high-order modulation systems, such as $M$-PSK and ${M}$-QAM systems, with a focus on evaluating their robustness, security, and compatibility. We also discuss advantages and disadvantages of these schemes and offer helpful recommendations according to different scenarios. Furthermore, we implement the proposed schemes and perform extensive simulations to compare their performance comprehensively. Simulation results demonstrate a perfect match with the theoretical results, verifying the superiority of the proposed schemes over the prior schemes.},
  archive      = {J_TMC},
  author       = {Zikai Chang and Wei Xiong and Ning Xie and Dusit Niyato},
  doi          = {10.1109/TMC.2025.3527220},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4716-4732},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Improving wireless security with phase-tag physical-layer authentication},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed MAC for RIS-assisted multiuser networks: CSMA/CA protocol design and statistical optimization. <em>TMC</em>, <em>24</em>(6), 4698-4715. (<a href='https://doi.org/10.1109/TMC.2025.3530851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research focuses on the challenges of distributed Medium Access Control (MAC) protocols involving Reconfigurable Intelligent Surfaces (RISs), which are still in early development. The study explores optimal channel access for multiple source-destination pairs in distributed networks with the assistance of multiple RISs. Three key issues are addressed: joint scheme for channel contention, RISs’ channel state information (CSI) acquisition, and RIS-assisted channel access; tradeoff between overhead and effective data transmission; and low-complexity distributed network operation. To achieve maximum network throughput, the paper proposes an optimal distributed Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) strategy with opportunistic RIS assistance based on statistical optimization. The proposed MAC strategy's optimality in terms of average network throughput is rigorously derived. Closed-form expressions for threshold functions of rewards for making decisions are derived, and an easy-to-implement distributed channel access algorithm is provided with online complexity $\mathcal {O}(2^{L})$, where $L$ denotes the number of distributed RISs. The proposed MAC strategy is then refined, and a low-complexity distributed algorithm is developed with complexity $\mathcal {O}(L)$. Numerical simulations verify the theoretical results, demonstrating the efficiency of the proposed strategy. This work introduces innovative solutions and analytical frameworks for the distributed MAC problem with RIS assistance, significantly advancing existing research.},
  archive      = {J_TMC},
  author       = {Zhou Zhang and Saman Atapattu and Yizhu Wang and Marco Di Renzo},
  doi          = {10.1109/TMC.2025.3530851},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4698-4715},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed MAC for RIS-assisted multiuser networks: CSMA/CA protocol design and statistical optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bilateral pricing for dynamic association in federated edge learning. <em>TMC</em>, <em>24</em>(6), 4684-4697. (<a href='https://doi.org/10.1109/TMC.2025.3527048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Devices and servers in Federated Edge Learning (FEL) are self-interested and resource-constrained, making it critical to design incentives to improve model performance. However, dynamic network conditions raise energy consumption, while data heterogeneity undermines device cooperation. Current research overlooks the interplay between system efficiency and device clustering, resulting in suboptimal updates. To address these challenges, we develop BENCH, a bilateral pricing mechanism consisting of three core rules aimed at incentivizing participation from both devices and servers. Specifically, we first design a reward allocation rule, based on the Rubinstein bargaining model, which dynamically allocates rewards. Theoretically, we derive a closed-form solution for this rule, demonstrating BENCH achieves Nash equilibrium. Secondly, we design a device partitioning rule that leverages modularity to group similar devices, facilitating personalized edge aggregation to accelerate local data adaptation. Thirdly, we design an edge matching rule that employs the Kuhn-Munkres algorithm to balance the load at edge servers, thus minimizing the congestion. Together, these three rules enable hierarchical optimization of pricing and associations, effectively mitigating the impact of dynamic costs and device heterogeneity. Extensive experiments demonstrate BENCH’s effectiveness in increasing device participation by 28.81% and improving model performance by 2.66% compared to state-of-the-art baselines.},
  archive      = {J_TMC},
  author       = {Bangqi Pan and Jianfeng Lu and Shuqin Cao and Jing Liu and Wenlong Tian and Minglu Li},
  doi          = {10.1109/TMC.2025.3527048},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4684-4697},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Bilateral pricing for dynamic association in federated edge learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple edge data integrity verification with multi-vendors and multi-servers in mobile edge computing. <em>TMC</em>, <em>24</em>(6), 4668-4683. (<a href='https://doi.org/10.1109/TMC.2025.3528016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring Edge Data Integrity (EDI) is imperative in providing reliable and low-latency services in mobile edge computing. Existing EDI schemes typically address single-vendor (App Vendor, AV) single-server (Edge Server, ES), single-vendor multi-server, and multi-vendor multi-server scenarios, which consider a single data replica cached by an ES from the AVs. However, the most practical scenario of Multi-Vendors and Multi-Servers with Multiple Data (MVMS-MD) cached by an ES from different AVs remains unexplored. Current solutions struggle when applied to this scenario due to increased computation and communication costs in the verification process across all ESs using the classical challenge-response per-data multi-round strategy. To tackle this issue, we propose a Multiple EDI-Verification (MEDI-V) approach in this paper. In particular, our MEDI-V utilizes an adaptive Merkle Hash Tree (ad-MHT) to efficiently generate a tree of multiple data replicas within each AV. Next, the dynamic mechanism computes minimal verification information using ad-MHT to create a challenge for individual ESs to produce EDI proofs. The ES then leverages its ad-MHT and the ES’s proof to send the reconstructed ad-MHT root to the AV for verification. Theoretical insights into MEDI-V’s correctness, efficiency, security, and comprehensive evaluations demonstrate its superiority in addressing MEDI issues in the MVMS-MD scenario.},
  archive      = {J_TMC},
  author       = {Md Rashedul Islam and Yong Xiang and Md Palash Uddin and Yao Zhao and Jonathan Kua and Longxiang Gao},
  doi          = {10.1109/TMC.2025.3528016},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4668-4683},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multiple edge data integrity verification with multi-vendors and multi-servers in mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online flow scheduling in virtualized time- sensitive networks: A joint admission control and VNF embedding approach. <em>TMC</em>, <em>24</em>(6), 4651-4667. (<a href='https://doi.org/10.1109/TMC.2025.3527749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-sensitive networking (TSN) is proposed to satisfy the increasingly stringent demands of Industrial 4.0 for deterministic transmission. This is achieved by generating a series of centrally configured gate control lists to strictly restrict the forwarding time of arriving flows. However, such a centralized scheme requires prior information of all flows, severely impeding TSN from providing an online response to dynamic industrial applications. To solve this problem, we innovatively propose to use admission control (AC) to realize deterministic transmission in the virtualized TSN network. In this approach, AC is distributively executed on each node and link, whereby flows of applications are served by passing through a series of virtual network functions (VNFs). This distributed AC execution is regarded as a VNF embedding (VNE) process. Specifically, we propose a two-stage online framework, Smart Admission Control (SmartAC), to cater to dynamic applications. The first stage, referred to as the static stage, obtains a deterministic VNE solution by synthesizing AC decisions of individual TSN nodes and links. The second stage, referred to as the dynamic stage, fine-tunes VNE solutions obtained from the static stage to adapt to the harsh environment with insufficient resources or limited VNF migration budgets. Simulation results demonstrate the effectiveness of SmartAC in improving response rate and resource utilization ratio. Notably, SmartAC reduces runtime by 90% compared to existing algorithms and exhibits robustness across different network topologies.},
  archive      = {J_TMC},
  author       = {Yajing Zhang and Cailian Chen and Mingyan Li and Chaoqun You and Xinping Guan and Tony Q. S. Quek},
  doi          = {10.1109/TMC.2025.3527749},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4651-4667},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Online flow scheduling in virtualized time- sensitive networks: A joint admission control and VNF embedding approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AccEmo: Accelerometer based human emotion recognition for eyewear devices. <em>TMC</em>, <em>24</em>(6), 4639-4650. (<a href='https://doi.org/10.1109/TMC.2025.3529662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of virtual reality applications, there is an increasing demand for more interactive entertainment, learning, social interactions, and other activities on eyewear devices. Recognizing users’ emotion and providing reliable feedback can significantly improve the immersive experience for users. However, previous works in emotion recognition required modifications to existing eyewear devices and the integration of additional sensors, or relied on specialized sensors in expensive commercial-grade eyewear devices, making direct deployment on existing consumer-grade eyewear devices challenging. In this paper, we propose AccEmo, the first system that analyzes the data from the built-in accelerometer sensor on eyewear devices to accurately recognize human emotion. AccEmo first employs signal processing technologies to process raw accelerometer data, and then uses a binary classification network to determine whether the accelerometer data is influenced by emotional changes. Subsequently, AccEmo proposes a network architecture based on residual neural network and channel-wise attention mechanism as a universal feature extractor to extract complex features related to human emotions from the accelerometer data. Finally, AccEmo uses personalized classifiers to achieve emotion recognition for different users. Extensive performance evaluation of AccEmo across diverse users demonstrates an exceptional average accuracy of 94.3%. Additionally, the robustness of AccEmo is validated through evaluations in various scenarios, yielding promising results.},
  archive      = {J_TMC},
  author       = {Hui Zhuang and Yihang Zhang and Yanni Yang and Guoming Zhang and Zhe Chen and Riccardo Spolaor and Xiuzhen Cheng and Prasant Mohapatra and Pengfei Hu},
  doi          = {10.1109/TMC.2025.3529662},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4639-4650},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AccEmo: Accelerometer based human emotion recognition for eyewear devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Usability and security analysis of the compare-and-confirm method in mobile push-based two-factor authentication. <em>TMC</em>, <em>24</em>(6), 4623-4638. (<a href='https://doi.org/10.1109/TMC.2024.3524093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Push-based two-factor authentication (2FA) methods, such as the ”Just-Confirm” approach, are popular due to their user-friendly design, requiring users to simply approve or deny a push notification on their mobile device. However, these methods are vulnerable to ”concurrency attacks,” where an attacker attempts to log in immediately after the legitimate user, causing multiple push notifications that may lead to users inadvertently approving fraudulent access. This vulnerability arises because the login notifications are not uniquely bound to individual login attempts. To address this issue, Push-Compare-and-Confirm 2FA method enhances security by associating each login notification with a unique code displayed on both the authentication terminal and the push notification. Users are required to match these codes before confirming access, thereby binding the notification to a specific login attempt. Recognizing the ubiquity of mobile devices in daily life, we conducted a comprehensive user study with 65 participants to evaluate the usability and security of Push-Compare-and-Confirm. The study considered two scenarios: one where the user’s second-factor device (phone) is physically separate from the authentication terminal (e.g., logging in on a PC and confirming on the phone), and another where the phone serves as both the authentication terminal and the second-factor device. Participants completed 24 login trials, including both benign and attack scenarios, with varying code lengths (four characters and six characters). Our results indicate that while Push-Compare-and-Confirm maintains high usability in benign scenarios, with True Positive Rates (TPR) exceeding 95%, it presents significant challenges in attack detection. Participants correctly identified only about 50% of fraudulent login attempts, indicating a substantial vulnerability remains. These findings suggest that although Push-Compare-and-Confirm enhances security over standard push-based 2FA methods, additional measures—such as more intuitive interface designs, clearer visual cues, and user education on the importance of code verification—are necessary to improve attack detection rates without compromising usability.},
  archive      = {J_TMC},
  author       = {Mohammed Jubur and Nitesh Saxena and Faheem A. Reegu},
  doi          = {10.1109/TMC.2024.3524093},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4623-4638},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Usability and security analysis of the compare-and-confirm method in mobile push-based two-factor authentication},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRT and PUF-based Self/Mutual-healing key distribution protocol with collusion resistance and revocation capability. <em>TMC</em>, <em>24</em>(6), 4607-4622. (<a href='https://doi.org/10.1109/TMC.2025.3528491'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-healing group key distribution (SGKD) protocols guarantee the security of group communications by allowing authorized users to independently recover missed previous session keys from the current broadcast without retransmission. However, existing SGKD protocols have flaws: (1) collusion resistance and revocable nodes are both upper-bounded by the degree of polynomials used, (2) the disclosure of personal secrets enables the recovery of group key, (3) temporary revocation of a group member is not possible, and (4) a revoked node may obtain the session key when initiating mutual healing, moreover, a malicious node may cause the recovery of false group keys. To address these limitations, we propose an SGKD protocol using the Chinese remainder theorem (CRT) and Physical Unclonable Function (PUF). Our proposed SGKD protocol generates a PUF-based dynamic secret by stimulating nodes’ PUF using a polynomial-based encrypted challenge. This secret is then employed to retrieve a CRT-based encrypted group key. By combining PUF and CRT, we can generate dynamic secrets on the fly and reduce computation time significantly. Utilizing such a technique, our protocol achieves superior security goals, including resistance to any coalition of group nodes even if nodes’ personal secrets were disclosed. Furthermore, the proposed protocol provides an unlimited number of revocable nodes. Additionally, a revoked node can rejoin its group in later sessions without affecting backward secrecy. Moreover, the protocol provides a backward secrecy guaranteed mutual-healing feature free from desynchronization. Our performance and security analyses (i.e., theorem-based formal analysis, NS3-based experiment, and formal verification using the AVIPSA tool) show that our proposed protocol achieves stronger security goals and better efficiency in terms of computation, communication, and storage costs compared to existing SGKD schemes.},
  archive      = {J_TMC},
  author       = {Wajdy Othman and Zhong Hong and Miao Fuyou and Kaiping Xue and Ammar Hawbani and Ruhul Amin and Liang Zhao and Tao Li},
  doi          = {10.1109/TMC.2025.3528491},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4607-4622},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CRT and PUF-based Self/Mutual-healing key distribution protocol with collusion resistance and revocation capability},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIDDLE: A mobility-driven device-edge-cloud federated learning framework. <em>TMC</em>, <em>24</em>(6), 4589-4606. (<a href='https://doi.org/10.1109/TMC.2025.3543723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) can be implemented in large-scale wireless networks in a hierarchical way, introducing edge servers as relays between the cloud server and devices. These devices are dispersed within multiple clusters coordinated by edges. However, the devices are typically mobile users with unpredictable trajectories, and the impact of their mobility on the model training process is not well-studied. In this work, we propose a new MobIlity-Driven feDerated LEarning framework, namely MIDDLE. MIDDLE addresses unbalanced model updates by capitalizing on model aggregation opportunities on mobile devices due to their mobility across edges. It consists of two components: on-device model aggregation, which aggregates models from different edges carried by mobile devices as they move across edges, and in-edge device selection, adjusting the current edge optimization direction through careful device selection. Theoretical analysis emphasizes that on-device model aggregation can reduce bias in model updating on edges and the cloud, thereby accelerating the FL model convergence. Building on this analysis, we introduce on-device global control averaging, modifying the training process on mobile devices and extending MIDDLE into $\text{MIDDLE}^{+}$. Extensive experimental results validate that MIDDLE and $\text{MIDDLE}^{+}$ can reduce the time steps to reach the target accuracy by 19.44% and 20.37% at least, respectively.},
  archive      = {J_TMC},
  author       = {Songli Zhang and Zhenzhe Zheng and Fan Wu and Bingshuai Li and Yunfeng Shao and Guihai Chen},
  doi          = {10.1109/TMC.2025.3543723},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {6},
  number       = {6},
  pages        = {4589-4606},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MIDDLE: A mobility-driven device-edge-cloud federated learning framework},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative diffusion-based contract design for efficient AI twin migration in vehicular embodied AI networks. <em>TMC</em>, <em>24</em>(5), 4573-4588. (<a href='https://doi.org/10.1109/TMC.2025.3526230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied Artificial Intelligence (AI) bridges the cyberspace and the physical space, driving advancements in autonomous systems like the Vehicular Embodied AI NETwork (VEANET). VEANET integrates advanced AI capabilities into vehicular systems to enhance autonomous operations and decision-making. Embodied agents, such as Autonomous Vehicles (AVs), are autonomous entities that can perceive their environment and take actions to achieve specific goals, actively interacting with the physical world. Embodied Agent Twins (EATs) are digital models of these embodied agents, with various Embodied Agent AI Twins (EAATs) for intelligent applications in cyberspace. In VEANETs, EAATs act as in-vehicle AI assistants to perform diverse tasks supporting autonomous driving using generative AI models. Due to limited onboard computational resources, AVs offload EAATs to nearby RoadSide Units (RSUs). However, the mobility of AVs and limited RSU coverage necessitates dynamic migrations of EAATs, posing challenges in selecting suitable RSUs under information asymmetry. To address this, we construct a multi-dimensional contract theoretical model between AVs and alternative RSUs. Considering that AVs may exhibit irrational behavior, we utilize prospect theory instead of expected utility theory to model the actual utilities of AVs. Finally, we employ a Generative Diffusion Model (GDM)-based algorithm to identify the optimal contract designs, thus enhancing the efficiency of EAAT migrations. Numerical results demonstrate the superior efficiency of the proposed GDM-based scheme in facilitating EAAT migrations compared with traditional deep reinforcement learning methods.},
  archive      = {J_TMC},
  author       = {Yue Zhong and Jiawen Kang and Jinbo Wen and Dongdong Ye and Jiangtian Nie and Dusit Niyato and Xiaozheng Gao and Shengli Xie},
  doi          = {10.1109/TMC.2025.3526230},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4573-4588},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Generative diffusion-based contract design for efficient AI twin migration in vehicular embodied AI networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCSE: Privacy-preserving collaborative searchable encryption for group data sharing in cloud computing. <em>TMC</em>, <em>24</em>(5), 4558-4572. (<a href='https://doi.org/10.1109/TMC.2025.3526232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative searchable encryption for group data sharing enables a consortium of authorized users to collectively generate trapdoors and decrypt search results. However, existing countermeasures may be vulnerable to a keyword guessing attack (KGA) initiated by malicious insiders, compromising the confidentiality of keywords. Simultaneously, these solutions often fail to guard against hostile manufacturers embedding backdoors, leading to potential information leakage. To address these challenges, we propose a novel privacy-preserving collaborative searchable encryption (PCSE) scheme tailored for group data sharing. This scheme introduces a dedicated keyword server to export server-derived keywords, thereby withstanding KGA attempts. Based on this, PCSE deploys cryptographic reverse firewalls to thwart subversion attacks. To overcome the single point of failure inherent in a single keyword server, the export of server-derived keywords is collaboratively performed by multiple keyword servers. Furthermore, PCSE extends its capabilities to support efficient multi-keyword searches and result verification and incorporates a rate-limiting mechanism to effectively slow down adversaries’ online KGA attempts. Security analysis demonstrates that our scheme can resist KGA and subversion attack. Theoretical analyses and experimental results show that PCSE is significantly more practical for group data sharing systems compared with state-of-the-art works.},
  archive      = {J_TMC},
  author       = {Yongliang Xu and Hang Cheng and Ximeng Liu and Changsong Jiang and Xinpeng Zhang and Meiqing Wang},
  doi          = {10.1109/TMC.2025.3526232},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4558-4572},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PCSE: Privacy-preserving collaborative searchable encryption for group data sharing in cloud computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved ultra-lightweight anonymous authenticated key agreement protocol for wearable devices. <em>TMC</em>, <em>24</em>(5), 4543-4557. (<a href='https://doi.org/10.1109/TMC.2025.3526076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For wearable devices with constrained computational resources, it is typically required to offload processing tasks to more capable servers. However, this practice introduces vulnerabilities to data confidentiality and integrity due to potential malicious network attacks, unreliable servers, and insecure communication channels. A robust mechanism that ensures anonymous authentication and key agreement is therefore imperative for safeguarding the authenticity of computing entities and securing data during transmission. Recently, Guo et al. proposed an anonymous authentication key agreement and group proof protocol specifically designed for wearable devices. This protocol, benefiting from the strengths of previous research, is designed to thwart a variety of cyber threats. However, inaccuracies in their protocol lead to issues with authenticity verification, ultimately preventing the establishment of secure session keys between communication entities. To address these design flaws, an improved ultra-lightweight protocol was proposed, employing cryptographic hash functions to ensure authentication and privacy during data transmission in wearable devices. Supported by rigorous security validations and analyses, the proposed protocol significantly boosts both security and efficiency, marking a substantial advancement over prior methodologies.},
  archive      = {J_TMC},
  author       = {Xin Ai and Akhtar Badshah and Shanshan Tu and Muhammad Waqas and Iftekhar Ahmad},
  doi          = {10.1109/TMC.2025.3526076},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4543-4557},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An improved ultra-lightweight anonymous authenticated key agreement protocol for wearable devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A security-enhanced ultra-lightweight and anonymous user authentication protocol for telehealthcare information systems. <em>TMC</em>, <em>24</em>(5), 4529-4542. (<a href='https://doi.org/10.1109/TMC.2025.3526519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in smartphone and wearable device usage has propelled the advancement of the Internet of Things (IoT) applications. Among these, e-healthcare stands out as a fundamental service, enabling the remote access and storage of patient-related data on a centralized medical server (MS), and facilitating connections between authorized individuals such as doctors, patients, and nurses over the public Internet. However, the inherent vulnerability of the public Internet to diverse security threats underscores the critical need for a robust and secure user authentication protocol to safeguard these essential services. This research presents a novel, resource-efficient user authentication protocol specifically designed for healthcare systems. Our proposed protocol leverages the lightweight authenticated encryption with associated data (AEAD) primitive Ascon combined with hash functions and XoR, specifically tailored for encrypted communication in resource-constrained IoT devices, emphasizing resource efficiency. Additionally, the proposed protocol establishes secure session keys between users and MS, facilitating future encrypted communications and preventing unauthorized attackers from illegally obtaining users’ private data. Furthermore, comprehensive security validation, including informal security analyses, demonstrates the protocol's resilience against a spectrum of security threats. Extensive analysis reveals that our proposed protocol significantly reduces computational and communication resource requirements during the authentication phase in comparison to similar authentication protocols, underscoring its efficiency and suitability for deployment in healthcare systems.},
  archive      = {J_TMC},
  author       = {Dake Zeng and Akhtar Badshah and Shanshan Tu and Muhammad Waqas and Zhu Han},
  doi          = {10.1109/TMC.2025.3526519},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4529-4542},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A security-enhanced ultra-lightweight and anonymous user authentication protocol for telehealthcare information systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AgileDART: An agile and scalable edge stream processing engine. <em>TMC</em>, <em>24</em>(5), 4510-4528. (<a href='https://doi.org/10.1109/TMC.2025.3526143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge applications generate a large influx of sensor data on massive scales, and these massive data streams must be processed shortly to derive actionable intelligence. However, traditional data processing systems are not well-suited for these edge applications as they often do not scale well with a large number of concurrent stream queries, do not support low-latency processing under limited edge computing resources, and do not adapt to the level of heterogeneity and dynamicity commonly present in edge computing environments. As such, we present AgileDart, an agile and scalable edge stream processing engine that enables fast stream processing of many concurrently running low-latency edge applications’ queries at scale in dynamic, heterogeneous edge environments. The novelty of our work lies in a dynamic dataflow abstraction that leverages distributed hash table-based peer-to-peer overlay networks to autonomously place, chain, and scale stream operators to reduce query latencies, adapt to workload variations, and recover from failures and a bandit-based path planning model that re-plans the data shuffling paths to adapt to unreliable and heterogeneous edge networks. We show that AgileDart outperforms Storm and EdgeWise on query latency and significantly improves scalability and adaptability when processing many real-world edge stream applications’ queries.},
  archive      = {J_TMC},
  author       = {Cheng-Wei Ching and Xin Chen and Chaeeun Kim and Tongze Wang and Dong Chen and Dilma Da Silva and Liting Hu},
  doi          = {10.1109/TMC.2025.3526143},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4510-4528},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AgileDART: An agile and scalable edge stream processing engine},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing remote sensing image scene classification with satellite-terrestrial collaboration and attention-aware transmission policy. <em>TMC</em>, <em>24</em>(5), 4496-4509. (<a href='https://doi.org/10.1109/TMC.2025.3526142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in Earth observation sensors on low Earth orbit (LEO) satellites have significantly increased the volume of remote sensing images. This growth has led to challenges such as higher storage demands, downlink bandwidth stress, and transmission delays, particularly for real-time remote sensing image scene classification (RSISC). To address this, we propose a novel Satellite-Terrestrial Collaborative Scene Classification (STCSC) framework that integrates transmission and computation. The framework employs an attention-aware policy on the satellite, which adaptively determines the sequence of images and selection of image blocks for transmission, as well as these blocks’ sampling rates. This policy is based on image complexity and the real-time data transmission rate, prioritizing blocks crucial for downstream tasks. On the ground, a classification model processes the received image blocks, balancing classification accuracy and transmission delay. Moreover, we have developed a comprehensive simulation system to validate the performance of our framework, including simulations of the satellite, transmission, and ground modules. Simulation results demonstrate that our STCSC framework can reduce transmission delay by 76.6% while enhancing classification accuracy on the ground by 0.6%. Additionally, our attention-aware policy is compatible with any ground classification model.},
  archive      = {J_TMC},
  author       = {Anqi Lu and Youbing Hu and Zhiqiang Cao and Jie Liu and Lingzhi Li and Zhijun Li},
  doi          = {10.1109/TMC.2025.3526142},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4496-4509},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing remote sensing image scene classification with satellite-terrestrial collaboration and attention-aware transmission policy},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based indoor 3D pedestrian location tracking with inertial-only perception. <em>TMC</em>, <em>24</em>(5), 4481-4495. (<a href='https://doi.org/10.1109/TMC.2025.3526196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian location tracking in emergency responses and environmental surveys of indoor scenarios tend to rely only on their own mobile devices, reducing the usage of external services. Low-cost and small-sized inertial measurement units (IMU) have been widely distributed in mobile devices. However, they suffer from high-level noises, leading to drift in position estimation over time. In this work, we present a graph-based indoor 3D pedestrian location tracking with inertial-only perception. The proposed method uses onboard inertial sensors in mobile devices alone for pedestrian state estimation in a simultaneous localization and mapping (SLAM) mode. It starts with a deep vertical odometry-aided 3D pedestrian dead reckoning (PDR) to predict the position in 3D space. Environment-induced behaviors, such as corner-turning and stair-taking, are regarded as landmarks. Multi-hypothesis loop closures are formed using statistical methods to handle ambiguous data association. A factor graph optimization fuses 3D PDR and behavior loop closures for state estimation. Experiments in different scenarios are performed using a smartphone to evaluate the performance of the proposed method, which can achieve better location tracking than current learning-based and filtering-based methods. Moreover, the proposed method is also discussed in different aspects, including the accuracy of offline optimization and proposed height regression, and the reliability of the multi-hypothesis behavior loop closures. The video (YouTube) or (BiliBili) is also shared to display our research.},
  archive      = {J_TMC},
  author       = {Shiyu Bai and Weisong Wen and Dongzhe Su and Li-Ta Hsu},
  doi          = {10.1109/TMC.2025.3526196},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4481-4495},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Graph-based indoor 3D pedestrian location tracking with inertial-only perception},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight configuration adaptation with multi-teacher reinforcement learning for live video analytics. <em>TMC</em>, <em>24</em>(5), 4466-4480. (<a href='https://doi.org/10.1109/TMC.2025.3526359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of video data and advancements in Deep Neural Networks (DNNs) have greatly boosted live video analytics, driven by the growing video capture capabilities of mobile devices. However, resource limitations necessitate the transmission of endpoint-collected videos to servers for inference. To meet real-time requirements and ensure accurate inference, it is essential to adjust video configurations at the endpoint. Traditional methods rely on deterministic strategies, posing difficulties in adapting to dynamic networks and video content. Meanwhile, emerging learning-based schemes suffer from trial-and-error exploration mechanisms, resulting in a concerning long-tail effect on upload latency. In this paper, we propose a novel lightweight and robust configuration adaptation policy (LCA), which fuses heuristic and RL-based agents using multi-teacher knowledge distillation (MKD) theory. First, we propose a content-sensitive and bandwidth-adaptive RL agent and introduce a Lyapunov-based optimization agent for ensuring latency robustness. To leverage both agents’ strengths, we design a feature-guided multi-teacher distillation network to transfer their advantages to the student. The experimental results across two vision tasks (pose estimation and semantic segmentation) demonstrate that LCA significantly reduces transmission latency compared to prior work (average reduction of 47.11%-89.55%, 95-percentile reduction of 27.63%-88.78%) and computational overhead while maintaining comparable inference accuracy.},
  archive      = {J_TMC},
  author       = {Yuanhong Zhang and Weizhan Zhang and Muyao Yuan and Liang Xu and Caixia Yan and Tieliang Gong and Haipeng Du},
  doi          = {10.1109/TMC.2025.3526359},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4466-4480},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Lightweight configuration adaptation with multi-teacher reinforcement learning for live video analytics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can we enhance the quality of mobile crowdsensing data without ground truth?. <em>TMC</em>, <em>24</em>(5), 4451-4465. (<a href='https://doi.org/10.1109/TMC.2025.3526277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowdsensing (MCS) has emerged as a prominent trend across various domains. However, ensuring the quality of the sensing data submitted by mobile users (MUs) remains a complex and challenging problem. To address this challenge, an advanced method is needed to detect low-quality sensing data and identify malicious MUs that may disrupt the normal operations of an MCS system. Therefore, this article proposes a prediction- and reputation-based truth discovery (PRBTD) framework, which can separate low-quality data from high-quality data in sensing tasks. First, we apply a correlation-focused spatio-temporal Transformer network that learns from the historical sensing data and predicts the ground truth of the data submitted by MUs. However, due to the noise in historical data for training and the bursty values within sensing data, the prediction results can be inaccurate. To address this issue, we use the implications among the sensing data, which are learned from the prediction results but are stable and less affected by inaccurate predictions, to evaluate the quality of the data. Finally, we design a reputation-based truth discovery (TD) module for identifying low-quality data with their implications. Given the sensing data submitted by MUs, PRBTD can eliminate the data with heavy noise and identify malicious MUs with high accuracy. Extensive experimental results demonstrate that the PRBTD method outperforms existing methods in terms of identification accuracy and data quality enhancement.},
  archive      = {J_TMC},
  author       = {Jiajie Li and Bo Gu and Shimin Gong and Zhou Su and Mohsen Guizani},
  doi          = {10.1109/TMC.2025.3526277},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4451-4465},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Can we enhance the quality of mobile crowdsensing data without ground truth?},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high reliable routing protocol based on spatial-temporal graph model for multiple unmanned underwater vehicles network. <em>TMC</em>, <em>24</em>(5), 4434-4450. (<a href='https://doi.org/10.1109/TMC.2025.3526158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing demands for versatile applications have spurred the rapid development of Unmanned Underwater Vehicle (UUV) networks. Nevertheless, multi-UUV movements exacerbates the spatial-temporal variability, leading to serious intermittent connectivity of underwater acoustic channel. Such phenomena challenge the identification of reliable paths for high-dynamic network routing. Existing routing protocols overlook the effects of UUV movements on forwarding path, typically selecting forwarders based solely on the current network state, which lead to instability in packet transmission. To address these challenges, we propose a Routing protocol based on Spatial-Temporal Graph model with Q-learning for multi-UUV networks (STGR), achieving high reliable and energy effective transmission. Specifically, a distributed Spatial-Temporal Graph model (STG) is proposed to depict the evolving variation characteristics (neighbor relationships, link quality, and connectivity duration) among underwater nodes over periodic intervals. Then we design a Q-learning-based forwarder selection algorithm integrated with STG to calculate reward function, ensuring adaptability to the ever-changing conditions. We have performed extensive simulations of STGR on the Aqua-Sim-tg platform and compared with the state-of-the-art routing protocols in terms of Packet Delivery Rate (PDR), latency, energy consumption and energy balance with different network settings. The results show that STGR yields 24.32 percent higher PDR on average than them in multi-UUV networks.},
  archive      = {J_TMC},
  author       = {Cangzhu Xu and Shanshan Song and Xiujuan Wu and Guangjie Han and Miao Pan and Gaochao Xu and Jun-Hong Cui},
  doi          = {10.1109/TMC.2025.3526158},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4434-4450},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A high reliable routing protocol based on spatial-temporal graph model for multiple unmanned underwater vehicles network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-DeepViT: Binarized transformer for efficient sensor-based human activity recognition. <em>TMC</em>, <em>24</em>(5), 4419-4433. (<a href='https://doi.org/10.1109/TMC.2025.3526166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer architectures are popularized in both vision and natural language processing tasks, and they have achieved new performance benchmarks because of their long-term dependencies modeling, efficient parallel processing, and increased model capacity. While transformers offer powerful capabilities, their demanding computational requirements clash with the real-time and energy-efficient needs of edge-oriented human activity recognition. It is necessary to compress the transformer to reduce its memory consumption and accelerate the inference. In this paper, we investigated the binarization of a transformer-DeepViT for efficient human activity recognition. For feeding sensor signals into DeepViT, we first processed sensor signals to spectrograms by using wavelet transform. Then we applied three methods to binarize DeepViT and evaluated it on three public benchmark datasets for sensor-based human activity recognition. Compared to the full-precision DeepViT, the fully binarized one (Bi-DeepViT) reduced about 96.7% model size and 99% BOPs (Bit Operations) with only a little accuracy compromised. Furthermore, we explored the effects of binarizing various components and latent binarization of DeepViT to understand their impact on the model. We also validated the performance of Bi-DeepViTs on two wireless sensing datasets. The result shows that a certain partial binarization can improve the performance of DeepViT. Our work is the first to apply a binarized transformer in HAR.},
  archive      = {J_TMC},
  author       = {Fei Luo and Anna Li and Salabat Khan and Kaishun Wu and Lu Wang},
  doi          = {10.1109/TMC.2025.3526166},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4419-4433},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Bi-DeepViT: Binarized transformer for efficient sensor-based human activity recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MagicWrite: One-dimensional acoustic tracking-based air writing system. <em>TMC</em>, <em>24</em>(5), 4403-4418. (<a href='https://doi.org/10.1109/TMC.2025.3526185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air writing technology enhances text input for IoT, VR, and AR devices, offering a spatially flexible alternative to physical keyboards. Addressing the demand for such innovation, this paper presents MagicWrite, a novel system utilizing acoustic-based 1D tracking, which is suitable for mobile devices with existing speaker and microphone infrastructure. Compared to 2D or 3D tracking of the finger, 1D tracking eliminates the need for multiple microphones and/or speakers and is more universally applicable. However, challenges emerge when using 1D tracking for recognizing handwritten letters due to trajectory loss and inter-user writing variability. To address this, we develop a general conversion technique that transforms image-based text datasets (e.g., MNIST) into 1D tracking trajectory data, generating artificial datasets of tracking traces (referred to as TrackMNISTs) to bolster system robustness and scalability. These tracking datasets facilitate the creation of personalized user databases that align with individual writing habits. Combined with a kNN classifier, our proposed MagicWrite ensures high accuracy and robustness in text input recognition while simultaneously reducing computational load and energy consumption. Extensive experiments validate that our proposed MagicWrite achieves exceptional classification accuracy for unseen users and inputs in five languages, marking it as a robust solution for air writing.},
  archive      = {J_TMC},
  author       = {Hao Pan and Yongjian Fu and Ye Qi and Yi-Chao Chen and Ju Ren},
  doi          = {10.1109/TMC.2025.3526185},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4403-4418},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MagicWrite: One-dimensional acoustic tracking-based air writing system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A game-theoretical approach for distributed computation offloading in LEO satellite-terrestrial edge computing systems. <em>TMC</em>, <em>24</em>(5), 4389-4402. (<a href='https://doi.org/10.1109/TMC.2025.3526200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limitations of computing resources and battery capacity, the computation tasks of ground devices can be offloaded to edge servers for processing. Moreover, with the development of the low earth orbit (LEO) satellite technology, LEO satellite-terrestrial edge computing can realize a global coverage network to provide seamless computing services beyond the regional restrictions compared to the conventional terrestrial edge computing networks. In this paper, we study the computation offloading problem in the LEO satellite-terrestrial edge computing systems. Ground devices can offload their computation tasks to terrestrial base stations (BSs) or LEO satellites deployed on edge servers for remote processing. We formulate the computation offloading problem to minimize the cost of devices while satisfying resource and LEO satellite communication time constraints. Since each ground device competes for transmission and computing resources to reduce its own offloading cost, we reformulate this problem as the LEO satellite-terrestrial computation offloading game (LSTCO-Game). It is derived that there is an upper bound on transmission interference and computing resource competition among devices. Then, we theoretically prove that at least one Nash equilibrium (NE) offloading strategy exists in the LSTCO-Game. We propose the game-theoretical distributed computation offloading (GDCO) algorithm to find the NE offloading strategy. Next, we analyze the cost obtained by GDCO's NE offloading strategy in the worst case. Experiments are conducted by comparing the proposed GDCO algorithm with other computation offloading methods. The results show that the GDCO algorithm can effectively reduce the offloading cost.},
  archive      = {J_TMC},
  author       = {Ying Chen and Yaozong Yang and Jintao Hu and Yuan Wu and Jiwei Huang},
  doi          = {10.1109/TMC.2025.3526200},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4389-4402},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A game-theoretical approach for distributed computation offloading in LEO satellite-terrestrial edge computing systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task semantic communication with graph attention-based feature correlation extraction. <em>TMC</em>, <em>24</em>(5), 4371-4388. (<a href='https://doi.org/10.1109/TMC.2025.3525477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task semantic communication can serve multiple learning tasks using a shared encoder model. Existing models have overlooked the intricate relationships between features extracted during an encoding process of tasks. This paper presents a new graph attention inter-block (GAI) module to the encoder/ transmitter of a multi-task semantic communication system, which enriches the features for multiple tasks by embedding the intermediate outputs of encoding in the features, compared to the existing techniques. The key idea is that we interpret the outputs of the intermediate feature extraction blocks of the encoder as the nodes of a graph to capture the correlations of the intermediate features. Another important aspect is that we refine the node representation using a graph attention mechanism to extract the correlations and a multi-layer perceptron network to associate the node representations with different tasks. Consequently, the intermediate features are weighted and embedded into the features transmitted for executing multiple tasks at the receiver. Experiments demonstrate that the proposed model surpasses the most competitive and publicly available models by 11.4% on the CityScapes 2Task dataset and outperforms the established state-of-the-art by 3.97% on the NYU V2 3Task dataset, respectively, when the bandwidth ratio of the communication channel (i.e., compression level for transmission over the channel) is as constrained as $\frac{1}{12}$.},
  archive      = {J_TMC},
  author       = {Xi Yu and Tiejun Lv and Weicai Li and Wei Ni and Dusit Niyato and Ekram Hossain},
  doi          = {10.1109/TMC.2025.3525477},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4371-4388},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-task semantic communication with graph attention-based feature correlation extraction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AugSSO: Secure threshold single-sign-on authentication with popular password collection. <em>TMC</em>, <em>24</em>(5), 4355-4370. (<a href='https://doi.org/10.1109/TMC.2024.3525453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-sign-on authentication is widely deployed in mobile systems, which allows an identity server to authenticate a mobile user and issue her/him with a token, such that the user can access diverse mobile services. To address the single-point-of-failure problem, threshold single-sign-on authentication (PbTA) is a feasible solution, where multiple identity servers perform user authentication and token issuance in a threshold way. However, existing PbTA schemes confront critical drawbacks. Specifically, these schemes are vulnerable to perpetual secret leakage attacks (PSLA): an adversary perpetually compromises secrets of identity servers (e.g., secret key shares or credentials) to break security. Besides, they fail to achieve popular password collection, which is an effective means of enhancing system security. In this paper, we propose a secure PbTA scheme with popular password collection, dubbed AugSSO. In AugSSO, we conceive an efficient key renewal mechanism that allows identity servers to periodically update secret key shares in batches, and require storage of hardened password-derived public keys in credentials for user authentication, thereby resisting PSLA. We also present a popular password collection mechanism, where an aggregation server is introduced to identify popular passwords without disclosing unpopular ones. We provide security analysis and performance evaluation to demonstrate security and efficiency of AugSSO.},
  archive      = {J_TMC},
  author       = {Changsong Jiang and Chunxiang Xu and Guomin Yang},
  doi          = {10.1109/TMC.2024.3525453},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4355-4370},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AugSSO: Secure threshold single-sign-on authentication with popular password collection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NeuroBalancer: Balancing system frequencies with punctual laziness for timely and energy-efficient DNN inferences. <em>TMC</em>, <em>24</em>(5), 4339-4354. (<a href='https://doi.org/10.1109/TMC.2024.3524628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On-device deep neural network (DNN) inference is often desirable for user experience and privacy. Existing solutions have fully utilized resources to minimize inference latency. However, they result in severe energy inefficiency by completing DNN inference much earlier than the required service interval. It poses a new challenge of how to make DNN inferences in a punctual and energy-efficient manner. To tackle this challenge, we propose a new resource allocation strategy for DNN processing, namely punctual laziness that disperses its workload as efficiently as possible over time within its strict delay constraint. This strategy is particularly beneficial for neural workloads since a DNN comprises a set of popular operators whose latency and energy consumption are predictable. Through this understanding, we propose NeuroBalancer, an operator-aware core and memory frequency scaling framework that balances those frequencies as efficiently as possible while making timely inferences. We implement and evaluate NeuroBalancer on off-the-shelf Android devices with various state-of-the-art DNN models. Our results show that NeuroBalancer successfully meets a given inference latency requirements while saving energy consumption up to 43.9% and 21.1% compared to the Android’s default governor and up to 42.1% and 18.6% compared to SysScale, the state-of-the-art mobile governor on CPU and GPU, respectively.},
  archive      = {J_TMC},
  author       = {Kyungmin Bin and Seyeon Kim and Sangtae Ha and Song Chong and Kyunghan Lee},
  doi          = {10.1109/TMC.2024.3524628},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4339-4354},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {NeuroBalancer: Balancing system frequencies with punctual laziness for timely and energy-efficient DNN inferences},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource allocation for the uplink of a multi-user massive MIMO system. <em>TMC</em>, <em>24</em>(5), 4326-4338. (<a href='https://doi.org/10.1109/TMC.2024.3522207'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the uplink resource management of a multi-user multiple-input-multiple-output single cell for Zero-Forcing receive combining transmission. We consider jointly power allocation, user selection and modulation and coding scheme selection over multiple subchannels. Our contributions are twofold: we first propose a quasi-optimal offline algorithm that provides a target performance and then design and validate an efficient online proportional fair algorithm that performs the above steps. Due to user power constraints, the offline optimization is conducted jointly for all subchannels within a time slot, a computationally intensive task, prompting the proposal of a greedy offline algorithm that we validate in two ways: 1) for a small number of users, by solving the general problem to quasi-optimality and 2) for a larger number of users, by solving again to quasi-optimality a transformed version of the general problem when the channels are assumed flat. From the offline study, we find that, given the right user selection, equal power allocation can be employed without much degradation in performance. We also see that the number of channels allocated to users varies widely depending upon their channel gains. Using these insights, we propose our efficient real-time online algorithm that has runtime competitiveness with a state-of-the-art benchmark.},
  archive      = {J_TMC},
  author       = {Haseen Rahman and Catherine Rosenberg},
  doi          = {10.1109/TMC.2024.3522207},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4326-4338},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resource allocation for the uplink of a multi-user massive MIMO system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent task offloading and resource allocation in knowledge defined edge computing networks. <em>TMC</em>, <em>24</em>(5), 4312-4325. (<a href='https://doi.org/10.1109/TMC.2024.3522253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging architecture, edge computing enables resource limited terminal devices to offload their computation tasks to edge servers in the vicinity, to efficiently reduce delay and energy consumption. However, the continuous expansion of network scale and rapid growth of network traffic in recent years have brought huge challenges to task offloading and resource allocation. To tackle the challenges, by integrating Knowledge Defined Networking (KDN) and edge computing technologies, we design a novel Knowledge defined Edge Computing (KEC) architecture, to achieve intelligent resource allocation and task offloading in dynamic large-scale edge computing networks. We formulate the task offloading and resource allocation optimization problem, to minimize delay and energy consumption, by considering resource requirements and controller deployment. To solve it, we present an intelligent Resource Allocation based Task Offloading (TORA) mechanism, where a Multi-Agent SD3 based resource allocation (MASD3) algorithm is devised to perform efficient resource allocation. To adapt to the rapid expansion of network scale, we design a resource Allocation based Controller Deployment and task offloading Decision (DACD) algorithm, to perform the optimal controller deployment and task offloading. Extensive simulation experiments demonstrate the effectiveness and efficiency of our proposed solution, and TORA mechanism outperforms comparison mechanisms on delay and energy consumption.},
  archive      = {J_TMC},
  author       = {Chuangchuang Zhang and Qiang He and Fuliang Li and Keping Yu},
  doi          = {10.1109/TMC.2024.3522253},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4312-4325},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Intelligent task offloading and resource allocation in knowledge defined edge computing networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-AUV cooperative underwater multi-target tracking based on dynamic-switching-enabled multi-agent reinforcement learning. <em>TMC</em>, <em>24</em>(5), 4296-4311. (<a href='https://doi.org/10.1109/TMC.2024.3521889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, autonomous underwater vehicle (AUV) swarms are gradually becoming popular and have been widely promoted in ocean exploration or underwater tracking, etc. In this paper, we propose a multi-AUV cooperative underwater multi-target tracking algorithm especially when the real underwater factors are taken into account. We first give normally modelling approach for the underwater sonar-based detection and the ocean current interference on the target tracking process. Then, based on software-defined networking (SDN), we regard the AUV swarm as a underwater ad-hoc network and propose a hierarchical software-defined multi-AUV reinforcement learning (HSARL) architecture. Based on the proposed HSARL architecture, we propose the “Dynamic-Switching” mechanism, it includes “Dynamic-Switching Attention” and “Dynamic-Switching Resampling” mechanisms which accelerate the HSARL algorithm's convergence speed and effectively prevents it from getting stuck in a local optimum state. Additionally, we introduce the reward reshaping mechanism for further accelerating the convergence speed of the proposed HSARL algorithm in early phase. Finally, based on a proposed AUV classification method, we propose a cooperative tracking algorithm called Dynamic-Switching-Based MARL (DSBM)-driven tracking algorithm. Evaluation results demonstrate that our proposed DSBM tracking algorithm can perform precise underwater multi-target tracking, comparing with many of recent research products in terms of various important metrics.},
  archive      = {J_TMC},
  author       = {Shengbo Wang and Chuan Lin and Guangjie Han and Shengchao Zhu and Zhixian Li and Zhenyu Wang and Yunpeng Ma},
  doi          = {10.1109/TMC.2024.3521889},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4296-4311},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-AUV cooperative underwater multi-target tracking based on dynamic-switching-enabled multi-agent reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Laser-powered UAV trajectory and charging optimization for sustainable data-gathering in the internet of things. <em>TMC</em>, <em>24</em>(5), 4278-4295. (<a href='https://doi.org/10.1109/TMC.2024.3523281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work examines the trajectory design and energy charging strategy of a data-gathering unmanned aerial vehicle (UAV). The UAV utilizes laser charging from high-altitude platforms (HAPs) to replenish its battery, enabling sustained travel across multiple data-gathering points. The trajectory is determined by a sequence of hovering positions at which the UAV stays to perform both data collection and energy charging. The UAV's hovering positions affect both the sensors’ transmission rates and the laser-charging efficiency. To minimize the total task completion time, it is necessary to choose hovering positions that consider both data upload and energy charging times. In this work, we first propose the Minimum Completion Time Trajectory and Charging Optimization (MinTime-TCO) algorithm, where the hovering positions and charging energies are optimized in turn using a block coordinate descent approach. Given the UAV's hovering positions, we propose the Minimum Charge Rate Search (MCRS) algorithm to optimize the charging energies at these positions. We show that MCRS is optimal in terms of minimizing the total task completion time. Then, given the charging energies, we propose the Hovering Position Optimization (HPO) algorithm, employing successive convex approximation to address the non-convexity of the optimization problem. We also propose a low-complexity alternative based on dynamic programming to further reduce computational complexity. Simulation results demonstrate the effectiveness of the proposed algorithms against several baseline strategies.},
  archive      = {J_TMC},
  author       = {Yue-Shiuan Liau and Y.-W. Peter Hong and Jang-Ping Sheu},
  doi          = {10.1109/TMC.2024.3523281},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4278-4295},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Laser-powered UAV trajectory and charging optimization for sustainable data-gathering in the internet of things},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A delay-oriented joint optimization approach for RIS-assisted MEC-MIMO system. <em>TMC</em>, <em>24</em>(5), 4263-4277. (<a href='https://doi.org/10.1109/TMC.2024.3521012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, we propose a joint optimization algorithm based on the block coordinate descent (JOABCD) algorithm for reflective intelligent surface (RIS) assisted MEC-MIMO systems. First, we define the delay minimization function for both single user with multi-antenna and multiple users with single-antenna scenarios. Since the optimization function is an NP-hard problem, we decompose it into two subproblems: computing setting and communication setting using the block coordinate descent (BCD) iterative algorithm. The subproblem of resource allocation is solved using a bisection method, while the subproblem of transmit power and phase shift matrix is solved alternately. The optimal simulation results show that the JOABCD algorithm can realize a lower time latency and a higher sum achievable rate compared with the existing methods.},
  archive      = {J_TMC},
  author       = {Shanshan Jiang and Xue Wang and Junhao Lin and Chongwen Huang and Zhihong Qian and Zhu Han},
  doi          = {10.1109/TMC.2024.3521012},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4263-4277},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A delay-oriented joint optimization approach for RIS-assisted MEC-MIMO system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HBF MU-MIMO with interference-aware beam pair link allocation for beyond-5G mm-wave networks. <em>TMC</em>, <em>24</em>(5), 4248-4262. (<a href='https://doi.org/10.1109/TMC.2025.3526547'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid beamforming (HBF) multi-user multiple-input multiple-output (MU-MIMO) is a key technology for unlocking the directional millimeter-wave (mm-wave) nature for spatial multiplexing beyond current codebook-based 5G-NR networks. In order to suppress co-scheduled users’ interference, HBF MU-MIMO is predicated on having sufficient radio frequency chains and accurate channel state information (CSI), which can otherwise lead to performance losses due to imperfect interference cancellation. In this work, we propose IABA, a 5G-NR standard-compliant beam pair link (BPL) allocation scheme for mitigating spatial interference in practical HBF MU-MIMO networks. IABA solves the network sum throughput optimization via either a distributed or a centralized BPL allocation using dedicated CSI reference signals for candidate BPL monitoring. We present a comprehensive study of practical multi-cell mm-wave networks and demonstrate that HBF MU-MIMO without interference-aware BPL allocation experiences strong residual interference which limits the achievable network performance. Our results show that IABA offers significant performance gains over the default interference-agnostic 5G-NR BPL allocation, and even allows HBF MU-MIMO to outperform the fully digital MU-MIMO baseline, by facilitating allocation of secondary BPLs other than the strongest BPL found during initial access. We further demonstrate the scalability of IABA with increased gNB antennas and densification for beyond-5G mm-wave networks.},
  archive      = {J_TMC},
  author       = {Aleksandar Ichkov and Alexander Wietfeld and Marina Petrova and Ljiljana Simić},
  doi          = {10.1109/TMC.2025.3526547},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4248-4262},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HBF MU-MIMO with interference-aware beam pair link allocation for beyond-5G mm-wave networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-factor authentication based on acoustic fingerprinting in modulation domain. <em>TMC</em>, <em>24</em>(5), 4235-4247. (<a href='https://doi.org/10.1109/TMC.2024.3522077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-factor authentication (2FA) has been increasingly used with the popularity of mobile devices. Currently, many existing 2FA schemes extract the devices’ acoustic fingerprints as the second factor. Nevertheless, they mainly consider deriving fingerprints from the raw acoustic waveforms for authentication, which are susceptible to the fingerprint variations caused by the environmental noise or the varying distance between devices. To address these vulnerabilities, we propose a robust system utilizing the distortions of modulated signals, which are incurred by the acoustic elements of mobile devices, as the proof for 2FA. Specifically, our system first designs a channel delay estimation scheme to accurately estimate the propagation delay from the speaker to the microphone by deriving the phase change of the received sinusoidal signal. To perform a robust authentication, we design a new acoustic fingerprinting scheme to remove the impacts of the varying distance and environmental noise from the demodulated PSK signals for fingerprint extraction. Moreover, our device authentication component designs a transfer learning-based scheme to capture the subtle differences in devices’ fingerprints for accurate device authentication. To the best of our knowledge, this is the first 2FA system that could extract acoustic fingerprints in modulation domain and can effectively withstand the impacts of channel distortions. We also confirm the accuracy and security of our system through extensive user experiments.},
  archive      = {J_TMC},
  author       = {Yanzhi Ren and Tingyuan Yang and Yufei Zhou and Hongbo Liu and Jiadi Yu and Haomiao Yang and Hongwei Li},
  doi          = {10.1109/TMC.2024.3522077},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4235-4247},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Two-factor authentication based on acoustic fingerprinting in modulation domain},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability-optimal UAV-assisted mobile edge computing: Joint resource allocation, data transmission scheduling and motion control. <em>TMC</em>, <em>24</em>(5), 4217-4234. (<a href='https://doi.org/10.1109/TMC.2024.3521934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed aerial vehicles (UAVs) play a crucial role in mobile edge computing (MEC) within space-air-ground integrated networks. They serve as aerial cloudlets, enabling task processing in close proximity to ground users. While numerous joint trajectory design and resource allocation schemes aim to enhance energy efficiency or computation rate, few focus on improving system reliability, which is often challenged by stochastic channels and node mobility. This paper presents a stochastic modeling perspective to derive a system reliability expression. Our reliability formulation incorporates the impacts of stochastic Line-of-Sight (LoS) and Non-Line-of-Sight (NLoS) air-to-ground communication channels, application data load, available bandwidth, offloading time, and transmission power. This comprehensive approach leads to a reliability-oriented joint optimization model that considers not only resource allocation and user data transmission scheduling but also the motion of UAVs. To solve this problem, we propose a low-complexity algorithm. By utilizing augmented Lagrangian multipliers, the algorithm transforms nonlinear constraints into a tractable formulation, enabling the utilization of legacy unconstrained optimization techniques. We provide a proof of convergence for this algorithm. Through simulations, we demonstrate that our proposed method guarantees convergence within finite iterations and improves the average communication reliability in comparison with several other joint optimization schemes.},
  archive      = {J_TMC},
  author       = {Jianshan Zhou and Mingqian Wang and Daxin Tian and Kaige Qu and Guixian Qu and Xuting Duan and Xuemin Shen},
  doi          = {10.1109/TMC.2024.3521934},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4217-4234},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reliability-optimal UAV-assisted mobile edge computing: Joint resource allocation, data transmission scheduling and motion control},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taming event cameras with bio-inspired architecture and algorithm: A case for drone obstacle avoidance. <em>TMC</em>, <em>24</em>(5), 4202-4216. (<a href='https://doi.org/10.1109/TMC.2024.3521044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast and accurate obstacle avoidance is crucial to drone safety. Yet existing on-board sensor modules such as frame cameras and radars are ill-suited for doing so due to their low temporal resolution or limited field of view. This paper presents BioDrone, a new design paradigm for drone obstacle avoidance using stereo event cameras. At the heart of BioDrone are three simple yet effective system designs inspired by the mammalian visual system, namely, a chiasm-inspired event filtering, a lateral geniculate nucleus (LGN)-inspired event matching, and a dorsal stream-inspired obstacle tracking. We implement BioDrone on FPGA through software-hardware co-design and deploy it on an industrial drone. In comparative experiments against two state-of-the-art event-based systems, BioDrone consistently achieves an obstacle detection rate of $&gt; $90%, and an obstacle tracking error of $&lt;$5.8 cm across all flight modes with an end-to-end latency of $&lt;$6.4 ms, outperforming both baselines by over 44%.},
  archive      = {J_TMC},
  author       = {Danyang Li and Jingao Xu and Zheng Yang and Yishujie Zhao and Hao Cao and Yunhao Liu and Longfei Shangguan},
  doi          = {10.1109/TMC.2024.3521044},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4202-4216},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Taming event cameras with bio-inspired architecture and algorithm: A case for drone obstacle avoidance},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PLRQ: Practical and less leakage range query over encrypted mobile cloud data. <em>TMC</em>, <em>24</em>(5), 4183-4201. (<a href='https://doi.org/10.1109/TMC.2024.3521366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a fundamental service in mobile cloud computing, range query has attracted extensive attention. But the existing secure range query schemes not only leak data privacy but also have low query efficiency. To address those issues, we first design a novel range-matched code to convert the range query into code set matching, which aims to hide the order relationship of outsourced data as well as the index of most significant different bit. Based on the designed range-matched code, we propose a Practical and Less Leakage Range Query scheme over encrypted mobile cloud data (PLRQ) by integrating XOR filter and multiset hash function. Security analysis shows that PLRQ achieves semantic security and avoids data privacy leakage. Extensive experiments using real datasets demonstrate that, compared with two state-of-the-art solutions-RngMatch and LSRQ, our proposed PLRQ improves the query efficiency both by 2 orders of magnitude, and reduces the storage cost on Cloud Service Provider by about 79.5% and 73.6% respectively.},
  archive      = {J_TMC},
  author       = {Yunwei Wang and Xinghua Li and Yinbin Miao and Qiuyun Tong and Ximeng Liu and Robert H. Deng},
  doi          = {10.1109/TMC.2024.3521366},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4183-4201},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PLRQ: Practical and less leakage range query over encrypted mobile cloud data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater multiple AUV cooperative target tracking based on minimal reward participation-embedded MARL. <em>TMC</em>, <em>24</em>(5), 4169-4182. (<a href='https://doi.org/10.1109/TMC.2024.3521028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the rapid advancement of Multi-Agent Reinforcement Learning (MARL) has introduced a new paradigm for intelligent underwater target tracking within Autonomous Underwater Vehicle (AUV) cluster networks, enabling these networks to intelligently collaborate in target tracking. However, the limited scalability of MARL poses significant challenges to the performance of AUV cluster networks in tracking tasks. Specifically, MARL models trained on a fixed agents lose their effectiveness when the agent count changes, underscoring the critical need to enhance MARL’s scalability to accommodate an arbitrary number of agents. This paper addresses the pressing issue of MARL’s scalability in the context of AUV cluster network-based target tracking. Specifically, we propose an Elastic Software-Defined Multi-Agent Reinforcement Learning (ESD-MARL) architecture to enhance the scalability of AUV cluster networks. Moreover, we propose an Incremental Multi-Agent Reinforcement Learning algorithm based on Minimal Reward Participation (IMARL-MRP) that allows for the expansion of the agents without retraining. By integrating the ESD-MARL with the IMARL-MRP, we propose an elastic underwater target tracking scheme, achieving high-performance target tracking with enhanced scalability. Evaluation results demonstrate that the proposed approach effectively enhances the scalability of MARL, enabling the arbitrary expansion of the AUV cluster network, thus supporting scalable and efficient underwater target tracking.},
  archive      = {J_TMC},
  author       = {Shengchao Zhu and Guangjie Han and Chuan Lin and Fan Zhang},
  doi          = {10.1109/TMC.2024.3521028},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4169-4182},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Underwater multiple AUV cooperative target tracking based on minimal reward participation-embedded MARL},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FairSTG: Countering performance heterogeneity via collaborative sample-level optimization. <em>TMC</em>, <em>24</em>(5), 4153-4168. (<a href='https://doi.org/10.1109/TMC.2024.3522476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatiotemporal learning plays a crucial role in mobile computing techniques to empower smart cites. While existing research has made great efforts to achieve accurate predictions on the overall dataset, they still neglect the significant performance heterogeneity across samples. In this work, we designate the performance heterogeneity as the reason for unfair spatiotemporal learning, which not only degrades the practical functions of models, but also brings serious potential risks to real-world urban applications. To fix this gap, we propose a model-independent Fairness-aware framework for SpatioTemporal Graph learning (FairSTG), which inherits the idea of exploiting advantages of well-learned samples to challenging ones with collaborative mix-up. Specifically, FairSTG consists of a spatiotemporal feature extractor for model initialization, a collaborative representation enhancement for knowledge transfer between well-learned samples and challenging ones, and fairness objectives for immediately suppressing sample-level performance heterogeneity. Experiments on four spatiotemporal datasets demonstrate that our FairSTG significantly improves the fairness quality while maintaining comparable forecasting accuracy. Case studies show FairSTG can counter both spatial and temporal performance heterogeneity by our sample-level retrieval and compensation, and our work can potentially alleviate the risks on spatiotemporal resource allocation for underrepresented urban regions.},
  archive      = {J_TMC},
  author       = {Gengyu Lin and Zhengyang Zhou and Qihe Huang and Kuo Yang and Shifen Cheng and Yang Wang},
  doi          = {10.1109/TMC.2024.3522476},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4153-4168},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FairSTG: Countering performance heterogeneity via collaborative sample-level optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Industrial internet of things with large language models (LLMs): An intelligence-based reinforcement learning approach. <em>TMC</em>, <em>24</em>(5), 4136-4152. (<a href='https://doi.org/10.1109/TMC.2024.3522130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs), as advanced AI technologies for processing and generating natural language text, bring substantial benefits to the Industrial Internet of Things (IIoT) by enhancing efficiency, decision-making, and automation. Nevertheless, their deployment faces significant obstacles due to high computational and energy demands, which often exceed the capabilities of many industrial devices. To overcome these challenges, edge-cloud collaboration has become increasingly essential, assisting in offloading LLMs tasks to reduce the computational load. However, traditional reinforcement learning (RL)-based strategies for LLMs task offloading encounter difficulties with generalization ability and defining explicit, appropriate reward functions. Therefore, in this paper, we propose a novel framework for offloading LLMs inference tasks in IIoT, utilizing a Decentralized Identifier (DID)-based identity management system for trusted task offloading. Furthermore, we introduce an intelligence-based RL (IRL) approach, which sidesteps the need for defining specific reward functions. Instead, it uses “intelligence” as a metric to evaluate cognitive improvements and adapt to varying environmental preferences, significantly improving generalizability. In our experiments, we employ the GPT-J-6B model and utilize the Human Eval dataset to assess its ability to tackle programming challenges, demonstrating the superior performance of our proposed solution compared to existing methods.},
  archive      = {J_TMC},
  author       = {Yuzheng Ren and Haijun Zhang and Fei Richard Yu and Wei Li and Pincan Zhao and Ying He},
  doi          = {10.1109/TMC.2024.3522130},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4136-4152},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Industrial internet of things with large language models (LLMs): An intelligence-based reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive joint routing and caching in knowledge-defined networking: An actor-critic deep reinforcement learning approach. <em>TMC</em>, <em>24</em>(5), 4118-4135. (<a href='https://doi.org/10.1109/TMC.2024.3521247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By integrating the software-defined networking (SDN) architecture with the machine learning-based knowledge plane, knowledge-defined networking (KDN) is revolutionizing established traffic engineering (TE) methodologies. This paper investigates the challenging joint routing and caching problem in KDN-based networks, managing multiple traffic flows to improve long-term quality-of-service (QoS) performance. This challenge is formulated as a computationally expensive non-convex mixed-integer non-linear programming (MINLP) problem, which exceeds the capacity of heuristic methods to achieve near-optimal solutions. To address this issue, we present DRL-JRC, an actor-critic deep reinforcement learning (DRL) algorithm for adaptive joint routing and caching in KDN-based networks. DRL-JRC orchestrates the optimization of multiple QoS metrics, including end-to-end delay, packet loss rate, load balancing index, and hop count. During offline training, DRL-JRC employs proximal policy optimization (PPO) to smooth the policy optimization process. In addition, the learned policy can be seamlessly integrated with conventional caching solutions during online execution. Extensive experiments demonstrate the comprehensive superiority of DRL-JRC over baseline methods in various scenarios. Meanwhile, DRL-JRC consistently outperforms the heuristic baseline under partial policy deployment during execution. Compared to the average performance of the baseline methods, DRL-JRC reduces the end-to-end delay by 51.14% and the packet loss rate by 40.78%.},
  archive      = {J_TMC},
  author       = {Yang Xiao and Huihan Yu and Ying Yang and Yixing Wang and Jun Liu and Nirwan Ansari},
  doi          = {10.1109/TMC.2024.3521247},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4118-4135},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive joint routing and caching in knowledge-defined networking: An actor-critic deep reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal federated learning based resources convergence for satellite-ground twin networks. <em>TMC</em>, <em>24</em>(5), 4104-4117. (<a href='https://doi.org/10.1109/TMC.2024.3521399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite-ground twin networks (SGTNs) are regarded as a promising service paradigm, which can provide mega access services and powerful computation offloading capabilities via cloud-fog automation functions. Specifically, cloud-fog automation technologies are collaboratively leveraged to enable dense connectivity, pervasive computing, and intelligent control in terrestrial industrial cyber-physical systems, whose system-level privacy security can be strengthened via blockchain based consensus protocol. Moreover, digital twin (DT) can shorten the gap between physical unities and digital space to enable instant data mapping in SGTNs environments. However, complex multi-modal network environments, such as stochastic task size, dynamic low earth orbit location, and time-varying channel gains, hinder better performance metrics in terms of energy consumption, throughput and privacy overhead. Hence, we establish a SGTN integrated cloud-fog automation model to transfer task data to low earth orbit satellites, and then execute broad communication access, powerful computation offloading, and efficient twin control. Next, we propose a Lyapunov stability theory based multi-modal federated learning (LST-MMFL) method to optimize the battery energy, the size of block, computation frequency, and the number of twin control for minimizing the total energy consumption and privacy overhead. Furthermore, we design a novel blockchain based transaction verification protocol to strengthen privacy security, derive performance upper bounds of SGTN model, and fulfill the long-term average task as well as energy queue constraints. Finally, massive simulation results show that the proposed LST-MMFL algorithm outperforms existing state-of-the-art benchmarks in line with energy consumption, available battery level, networked control and privacy protection overhead.},
  archive      = {J_TMC},
  author       = {Yongkang Gong and Haipeng Yao and Zehui Xiong and Dongxiao Yu and Xiuzhen Cheng and Chau Yuen and Mehdi Bennis and Mérouane Debbah},
  doi          = {10.1109/TMC.2024.3521399},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4104-4117},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-modal federated learning based resources convergence for satellite-ground twin networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learning-based dual-identity double auction in personalized federated learning. <em>TMC</em>, <em>24</em>(5), 4086-4103. (<a href='https://doi.org/10.1109/TMC.2024.3521304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning participants have two identities: model trainers and model users. As model users, participants care most about the performance of the final model on their own distributions, which is called Personal Model Performance (PMP). This makes training a single global model to accommodate all participants impractical because the data distributions of participants are heterogeneous. As model trainers, due to high training costs, participants are reluctant to contribute models if incentives are not enough. With the combination of the above two reasons, we propose a dual-identity double auction as an incentive mechanism in personalized federated learning, allowing directional selection between model users and model trainers, both of which are served by FL participants. Within the double auction framework, we devise a reinforcement learning-based model selection method. This method selects a set of models for each buyer to bid on. The bought models are aggregated to be a personalized model to achieve higher PMP. Additionally, we implement a transaction partition-based approach for determining clearing prices and winning pairs. We address the challenge of the unavailability of private yet essential data distribution information, the coupled influence of model selection and auction results on PMP, and more utility improvement ways of multi-demand dual-identity participants. Finally, our double auction optimizes the PMP of all participants and ensures the truthfulness of multi-demand dual-identity participants, which is harder compared with single-demand single-identity participants.},
  archive      = {J_TMC},
  author       = {Juan Li and Zishang Chen and Tianzi Zang and Tong Liu and Jie Wu and Yanmin Zhu},
  doi          = {10.1109/TMC.2024.3521304},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4086-4103},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Reinforcement learning-based dual-identity double auction in personalized federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STTF: A spatiotemporal transformer framework for multi-task mobile network prediction. <em>TMC</em>, <em>24</em>(5), 4072-4085. (<a href='https://doi.org/10.1109/TMC.2024.3521245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately predicting mobile traffic and accessed user amount is of great importance to network resource allocation, energy saving, etc. However, due to the complicated environmental contexts and complex interaction between mobile traffic and connected users, mobile network prediction is still challenging. Besides, the existing works could not be applied to large-scale networks because of the limited hardware resources and unacceptable time cost. In this work, we propose the spatiotemporal transformer framework for the multi-task mobile network prediction. Our proposed model contains three key parts. First, to capture the complex interaction between mobile traffic and connected users, we propose the temporal cross-attention encoder. Then, to identify and extract the most relevant information from various semantic relationships, we propose the hierarchical spatial encoder. This information is then used to create a more comprehensive representation of the network. Finally, the subgraph sampling method could significantly reduce the amount of computing power required and have comparable performance to the methods that input the whole network, enabling the model for real-world applications. Extensive experiments demonstrate that our proposed model significantly outperforms the state-of-the-art models by over 17% in both mobile traffic prediction and connected user prediction.},
  archive      = {J_TMC},
  author       = {Jiahui Gong and Yu Liu and Tong Li and Jingtao Ding and Zhaocheng Wang and Depeng Jin},
  doi          = {10.1109/TMC.2024.3521245},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4072-4085},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {STTF: A spatiotemporal transformer framework for multi-task mobile network prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise-robust federated learning with model heterogeneous clients. <em>TMC</em>, <em>24</em>(5), 4053-4071. (<a href='https://doi.org/10.1109/TMC.2024.3522573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables multiple devices to collaboratively train models without sharing their raw data. Considering that clients may prefer to design their own models independently, model heterogeneous FL has emerged. Additionally, due to the annotation uncertainty, the collected data usually contain unavoidable and varying noise, which cannot be effectively addressed by existing FL algorithms. This paper presents a novel solution that simultaneously handles model heterogeneity and label noise in a single framework. It is featured in three aspects: (1) For the communication between heterogeneous models, we directly align the model feedback by utilizing the easily-accessible public data, which does not require additional global models or relevant data for collaboration. (2) For internal label noise in each client, we design a dynamic label refinement strategy to mitigate the negative effects. (3) For challenging noisy feedback from other participants, we design an enhanced client confidence re-weighting scheme, which adaptively assigns corresponding weights to each client in the collaborative learning stage. Extensive experiments validate the effectiveness of our approach in mitigating the negative effects of various noise rates and types under both model homogeneous and heterogeneous FL settings.},
  archive      = {J_TMC},
  author       = {Xiuwen Fang and Mang Ye},
  doi          = {10.1109/TMC.2024.3522573},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4053-4071},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Noise-robust federated learning with model heterogeneous clients},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On user scheduling for fixed wireless access via channel statistics. <em>TMC</em>, <em>24</em>(5), 4040-4052. (<a href='https://doi.org/10.1109/TMC.2024.3524565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional multi-user scheduling in cellular networks are required to make a decision every transmission time interval (TTI) of at most several milliseconds. Only quite simple schemes can be implemented under the stringent time constraint, resulting in far-from-optimum performance. In this paper, we focus on the case of scheduling multiple users in a fixed wireless access (FWA) network with stable channel characteristics. We propose a scheduling approach by which a high-quality scheduling decision based on statistical channel state information (CSI) is made across all TTIs instead of making simple TTI-level decisions. The proposed design is essentially a mixed-integer non-smooth non-convex stochastic problem. We first replace the indicator functions in the formulation by smooth sigmoid functions to tackle nonsmoothness. By leveraging deterministic equivalents (D.E.), we then convert the original stochastic problem into an approximated deterministic one, followed by linear relaxation of the integer constraints. However, the converted problem is still nonconvex due to implicit equation constraints formerly introduced by D.E. Therefore, we employ implicit optimization technique to compute the gradient explicitly, with which we further propose an algorithm design based on a modified version of Frank–Wolfe method. Numerical results verify the effectiveness of our proposed scheme.},
  archive      = {J_TMC},
  author       = {Xin Guan and Zhixing Chen and Yibin Kang and Qi Yan and Qingjiang Shi},
  doi          = {10.1109/TMC.2024.3524565},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4040-4052},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {On user scheduling for fixed wireless access via channel statistics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmFruit: A contactless and non-destructive approach for fine-grained fruit moisture sensing using millimeter-wave technology. <em>TMC</em>, <em>24</em>(5), 4022-4039. (<a href='https://doi.org/10.1109/TMC.2024.3520914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless sensing offers a promising approach for non-destructive and contactless identification of the moisture content in fruits. Traditional methods assess fruit quality based on external features, such as color, shape, size, and texture. However, fruits often appear perfect externally while being rotten inside. Thus, accurately measuring internal conditions is crucial. This paper introduces mmFruit, a non-destructive and ubiquitous system that employs mmWave signals for precise and robust moisture level sensing in thin and thick pericarp fruits. We propose a novel dual incidence moisture estimation model for regular moisture monitoring to achieve high granularity and eliminate fruit type and size dependency. Additionally, we leverage unique reflection responses across different mmWave frequencies to provide discriminative information about fruit moisture levels. Our comprehensive theoretical model demonstrates how fruits’ refractive index, attenuation factor, and elasticity can be estimated by eliminating fruit type dependency. We developed an electric field distribution model utilizing two receiving antennas to address the challenge of varying fruit sizes through a differential approach, aiming to improve overall robustness. mmFruit integrates a customized Spatial-invariant network (SpI-Net) to eliminate interference from different frequencies and locations, ensuring stable moisture monitoring regardless of target displacement. Extensive experiments were conducted over a month in varied environments on seven types of fruits with thin and thick pericarps (apple, pear, peach, mango, orange, dragon fruit, and watermelon). The results demonstrate that mmFruit achieves a commendable RMSE of 0.276 in moisture estimation. It accurately distinguishes fruits with minor moisture level differences (0% to 7%) with 93.6% accuracy and higher moisture differences (45% to 65%) with over 95.1% accuracy, even in scenarios involving diverse displacements and rotations.},
  archive      = {J_TMC},
  author       = {Fahim Niaz and Jian Zhang and Muhammad Khalid and Muhammad Younas and Ashfaq Niaz},
  doi          = {10.1109/TMC.2024.3520914},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4022-4039},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmFruit: A contactless and non-destructive approach for fine-grained fruit moisture sensing using millimeter-wave technology},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Membership inference attacks against incremental learning in IoT devices. <em>TMC</em>, <em>24</em>(5), 4006-4021. (<a href='https://doi.org/10.1109/TMC.2024.3521216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) devices are frequently deployed in highly dynamic environments and need to continuously learn new classes from data streams. Incremental Learning (IL) has gained popularity in IoT as it enables devices to learn new classes efficiently without retraining model entirely. IL involves fine-tuning the model using two sources of data: a small amount of representative samples from the original training dataset and samples from the new classes. However, both data sources are vulnerable to Membership Inference Attack (MIA). Fortunately, the existing MIAs result in poor performance against IL, because they ignore features such as the similarity between old and new models at the old classification layer. This paper presents the first MIA against IL, capable of determining not only whether a sample was used for training/fine-tuning but also distinguishing whether it belongs to the representative dataset or the new classes (unique in IL). Extensive experiments validate the effectiveness of our attack across four real-world datasets. Our attack achieves an average attack success rate of 74.03% in the white-box setting (model structure and parameters are known) and 70.08% in the black-box setting. Importantly, our attack is not sensitive to the IL hyper-parameters (e.g., distillation temperature), confirming its accurate, robust, and practical.},
  archive      = {J_TMC},
  author       = {Xianglong Zhang and Huanle Zhang and Guoming Zhang and Yanni Yang and Feng Li and Lisheng Fan and Zhijian Huang and Xiuzhen Cheng and Pengfei Hu},
  doi          = {10.1109/TMC.2024.3521216},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {4006-4021},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Membership inference attacks against incremental learning in IoT devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PriFairFed: A local differentially private federated learning algorithm for client-level fairness. <em>TMC</em>, <em>24</em>(5), 3993-4005. (<a href='https://doi.org/10.1109/TMC.2024.3516813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local Differential Privacy (LDP) is a mechanism used to protect training privacy in Federated Learning (FL) systems, typically by introducing noise to data and local models. However, in real-world distributed edge systems, the non-independent and identically distributed nature of data means that clients in FL systems experience varying sensitivities to LDP-introduced noise. This disparity leads to fairness issues, potentially discouraging marginal clients from contributing further. In this paper, we explore how to enhance client-level performance fairness under LDP conditions. We model an FL system with LDP and formulate the problem PriFair using regularization, which assigns varied noise amplitudes to clients based on federated analytics. Additionally, we develop PriFairFed, a Tikhonov regularization-based algorithm that eliminates variable dependencies and optimizes variables alternately, while also offering a theoretical privacy guarantee. We further experimented with the algorithm on a real-world system with 20 Raspberry Pi clients, showing up to a 73.2% improvement in client-level fairness compared to existing state-of-the-art approaches, while maintaining a comparable level of privacy.},
  archive      = {J_TMC},
  author       = {Chuang Hu and Nanxi Wu and Siping Shi and Xuan Liu and Bing Luo and Kanye Ye Wang and Jiawei Jiang and Dazhao Cheng},
  doi          = {10.1109/TMC.2024.3516813},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3993-4005},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PriFairFed: A local differentially private federated learning algorithm for client-level fairness},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-distributed network fault diagnosis based on digital twin network in highly dynamic heterogeneous networks. <em>TMC</em>, <em>24</em>(5), 3979-3992. (<a href='https://doi.org/10.1109/TMC.2024.3519576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Highly dynamic heterogeneous networks (HDHNs), characterized by high node mobility and heterogeneity, frequently experience complex and recurrent network faults. Conventional centralized fault diagnosis methods demand real-time collection of extensive network-wide data, while distributed approaches often exhibit limited fault detection capabilities. Additionally, machine learning-based fault diagnosis methods are challenged by the scarcity of labeled fault samples required for training. To address these limitations, this study proposes a semi-distributed network fault diagnosis architecture based on a digital twin network (DTN). The proposed architecture facilitates the extraction of a comprehensive labeled fault dataset that closely replicates real-world network conditions. Using this dataset, we perform centralized training of an enhanced anomaly detection model, FTS-LSTM, to infer fault types at the node level. To overcome the drawbacks of both centralized and distributed approaches, we further introduce a semi-distributed fault diagnosis algorithm (SDFD) that integrates fault types and severity levels identified by nodes to infer overall network faults. The proposed fault diagnosis scheme is validated on a semi-physical DTN simulation platform, demonstrating its effectiveness in realistic scenarios.},
  archive      = {J_TMC},
  author       = {Fengxiao Tang and Linfeng Luo and Zhiqi Guo and Yangfan Li and Ming Zhao and Nei Kato},
  doi          = {10.1109/TMC.2024.3519576},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3979-3992},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Semi-distributed network fault diagnosis based on digital twin network in highly dynamic heterogeneous networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning-driven swarm intelligence: Enabling deterministic flows scheduling in LEO satellite networks. <em>TMC</em>, <em>24</em>(5), 3962-3978. (<a href='https://doi.org/10.1109/TMC.2024.3517618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, low-Earth-orbit (LEO) satellite networks have emerged as a critical infrastructure in communication systems, providing wide coverage, high reliability, and global connectivity. Recently, the development of 6G technologies has challenged the LEO satellite networks to guarantee deterministic scheduling for time-sensitive services. However, traditional deterministic networking techniques fall short for LEO satellite networks. First, these techniques impose strict time constraints, but in LEO satellite networks, delay and jitter typically range in the tens of milliseconds, which exceed these limits and render them infeasible. Second, the dynamic topologies of LEO satellite networks challenge the inflexible scheduling strategies generated by these techniques, leading to sub-optimal performance and potential strategy failures. To tackle the first problem, we propose a Cycle Specified Queuing and Forwarding (CSQF) based deterministic flows scheduling mechanism. It relaxes strict time constraints by employing cyclic multi-queue scheduling, enabling more flexible and reliable long-distance transmission. For the second problem, we propose a learning-based swarm intelligence method for deterministic flows scheduling in dynamic LEO satellite networks. It includes an algorithm that combines a Dynamic Graph Convolutional Network (DGCN) with an Adaptive Ant Colony Optimization (ACO) algorithm, referred to as the DGCN-ACO algorithm. The DGCN captures the dynamic feature of the network and generates the heuristic information. The Adaptive ACO utilizes the heuristic information and considers each flow's attribute to generate multi-path scheduling strategies for each deterministic flow, as well as updates the DGCN. The experiment results demonstrate the effectiveness of our proposed algorithm.},
  archive      = {J_TMC},
  author       = {Zunliang Wang and Haipeng Yao and Tianle Mai and Zhipei Li and C. L. Philip Chen},
  doi          = {10.1109/TMC.2024.3517618},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3962-3978},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Learning-driven swarm intelligence: Enabling deterministic flows scheduling in LEO satellite networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized data integrity inspection offloading in edge computing systems using potential games. <em>TMC</em>, <em>24</em>(5), 3950-3961. (<a href='https://doi.org/10.1109/TMC.2024.3516583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge storage is becoming an increasingly appealing alternative for data owners (DOs), offering benefits like decreased latency and minimized bandwidth usage compared to traditional cloud storage solutions. Nonetheless, stored data within edge servers (ESs) remains vulnerable to disruptions. Existing data integrity auditing schemes face challenges such as the costs of third-party auditors (TPA), unreliable and delayed audit results, and effective management of data inspection concerning time and energy consumption. To tackle these challenges, we introduce DIVO, a decentralized data inspection approach. DIVO leverages ESs as each others’ auditors, removing the necessity for a centralized party, thereby mitigating collision risks and potential biases in audit results. We propose a game-theoretic technique to efficiently manage data inspection and verification offloading to ESs. By formulating the decision-making issue as a strategic game for optimally allocating verification tasks among multiple ESs, we establish the presence of Nash equilibrium and design a strategy to attain it. Through comprehensive security and performance evaluations, DIVO has been shown to operate securely within the random oracle model while delivering notable efficiency improvements over recent methods. Our analysis highlights that DIVO surpasses a wide range of recent approaches in both communication and computation efficiency.},
  archive      = {J_TMC},
  author       = {Zahra Seyedi and Farhad Rahmati and Mohammad Ali and Ximeng Liu},
  doi          = {10.1109/TMC.2024.3516583},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3950-3961},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Decentralized data integrity inspection offloading in edge computing systems using potential games},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Near-field spot beamfocusing: A correlation-aware transfer learning approach. <em>TMC</em>, <em>24</em>(5), 3935-3949. (<a href='https://doi.org/10.1109/TMC.2024.3519382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional (3D) spot beamfocusing (SBF), in contrast to conventional angular-domain beamforming, concentrates radiating power within a very small volume in both radial and angular domains in the near-field zone. Recently the implementation of channel-state-information (CSI)-independent machine learning (ML)-based approaches have been developed for effective SBF using extremely large-scale programmable metasurface (ELPMs). These methods involve dividing the ELPMs into subarrays and independently training them with Deep Reinforcement Learning to jointly focus the beam at the desired focal point (DFP). This paper explores near-field SBF using ELPMs, addressing challenges associated with lengthy training times resulting from independent training of subarrays. To achieve a faster CSI-independent solution, inspired by the correlation between the beamfocusing matrices of the subarrays, we leverage transfer learning techniques. First, we introduce a novel similarity criterion based on the phase distribution image (PDI) of subarray apertures. Then we devise a subarray policy propagation scheme that transfers the knowledge from trained to untrained subarrays. We further enhance learning by introducing quasi-liquid layers as a revised version of the adaptive policy reuse technique. We show through simulations that the proposed scheme improves the training speed about 5 times. Furthermore, for dynamic DFP management, we devised a DFP policy blending process, which augments the convergence rate up to 8-fold.},
  archive      = {J_TMC},
  author       = {Mohammad Amir Fallah and Mehdi Monemi and Mehdi Rasti and Matti Latva-aho},
  doi          = {10.1109/TMC.2024.3519382},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3935-3949},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Near-field spot beamfocusing: A correlation-aware transfer learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A verifiable privacy-preserving federated learning framework against collusion attacks. <em>TMC</em>, <em>24</em>(5), 3918-3934. (<a href='https://doi.org/10.1109/TMC.2024.3516119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the current federated learning schemes aimed at safeguarding privacy exhibit vulnerability to collusion attacks and lack a verification mechanism for participants to consolidate the aggregation results of the parameter server, leading to privacy breaches for users and inaccurate model training outcomes. In order to address these issues, we propose a verifiable privacy-preserving federated learning framework against collusion attacks. Primarily, the federated learning scheme is reconfigured utilizing the ElGamal encryption algorithm, which effectively safeguards the data privacy of participants in scenarios involving collusion between certain participants and servers. Additionally, the introduction of the assistant server can realize the joint decryption of the gradient ciphertext by the non-collusive parameter server and assistant server, which can effectively resist the internal attack of a single parameter server model in the process of data upload. Third, this scheme designs a verification mechanism that enables participants to effectively verify the accuracy and integrality of the parameter server's aggregated results, preventing the parameter server from returning incorrect aggregation results to participants. Experimental results and performance analysis demonstrate that our proposed scheme not only fortifies security measures but also upholds the precision of model training, surpassing the security and correctness of many existing methodologies.},
  archive      = {J_TMC},
  author       = {Yange Chen and Suyu He and Baocang Wang and Zhanshen Feng and Guanghui Zhu and Zhihong Tian},
  doi          = {10.1109/TMC.2024.3516119},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3918-3934},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A verifiable privacy-preserving federated learning framework against collusion attacks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault-tolerant wireless charger placement. <em>TMC</em>, <em>24</em>(5), 3903-3917. (<a href='https://doi.org/10.1109/TMC.2024.3517544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-life applications, wireless chargers are deployed outdoor or in public area or even unattended environment such as hotels, restaurants, retail stores. They are exposed to various risks and malicious attacks that may break them down and further incur significant cost (e.g., battery replacement and maintenance) or performance degradation. Hence, we consider the problem of Fault-tolerant wIreless chaRger placeMent (FIRM): given a set of wireless chargers and a set of tasks to be collaboratively conducted by a set of rechargeable devices, determining where to deploy the chargers to maximize the worst-cast overall task charging utility subject to the constraint that up to $\tau$ chargers may break down. FIRM is a non-linear combinatorial two-level optimization problem. We first consider a relaxed version of FIRM (FIRM-R for short) corresponding to the inner optimization problem in FIRM. To address FIRM-R, we first propose an area discretization scheme to convert the infinite solution space into finite candidate positions. We then devise a power allocation method, based on which we prove that FIRM-R falls into the realm of maximizing a monotone submodular function under a uniform constraint. We then propose a constant-factor approximation algorithm to solve FIRM-R. Taking the above approximation algorithm as a subroutine, we further develop an approximation algorithm that solves FIRM with a constant-factor approximation ratio. Our extensive simulations and field experiments demonstrate that the overall charging utility of our proposed algorithm FIRM considering fault tolerance by greedy removal of $\tau$ chargers outperforms the that of FIRM-R without considering fault tolerance by greedy removal of $\tau$ chargers by at least 119.89%.},
  archive      = {J_TMC},
  author       = {Nan Yu and Haipeng Dai and Lin Chen and Xiaoyu Wang and Shuai Wang and Guihai Chen and Bin Dong and Mingwei Zhang},
  doi          = {10.1109/TMC.2024.3517544},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3903-3917},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fault-tolerant wireless charger placement},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Determining task assignments for candidate workers based on trajectory prediction. <em>TMC</em>, <em>24</em>(5), 3890-3902. (<a href='https://doi.org/10.1109/TMC.2024.3518534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of sensor-equipped mobile devices, Mobile Crowd Sensing (MCS) has emerged as an efficient method for information gathering. In smart city environmental sensing, workers can acquire data by merely being within the sensing area. Currently, most studies select opportunistic workers based on the workers’ prior preferences and ignore the effect of movement trajectories on potential opportunistic workers. This may result in the selected opportunistic workers being less-than-ideal, or even ignoring the failure of some tasks to be accomplished, thus resulting in a waste of resources. Therefore, this paper proposes a Recruitment Framework for judging Opportunistic Workers based on Movement Trajectories (RFOW-MT), a two-phase framework for worker recruitment. In the offline phase, combining the neural network model Long Short-Term Memory (LSTM) and Geohash algorithm, an algorithm to detect the set of candidate opportunistic workers is proposed, solving the problems of location privacy and search efficiency. In the online phase, in order to maximize the task spatial coverage under the task budget constraint, a task allocation algorithm based on geographic location packed grouping is proposed. Finally, RFOW-MT outperforms other methods in terms of task spatial coverage and runtime as verified by experiments on real datasets.},
  archive      = {J_TMC},
  author       = {Yahong Li and Yingjie Wang and Gang Li and Xiangrong Tong and Zhipeng Cai},
  doi          = {10.1109/TMC.2024.3518534},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3890-3902},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Determining task assignments for candidate workers based on trajectory prediction},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum-assisted online task offloading and resource allocation in MEC-enabled satellite-aerial-terrestrial integrated networks. <em>TMC</em>, <em>24</em>(5), 3878-3889. (<a href='https://doi.org/10.1109/TMC.2024.3519060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of Internet of Things (IoT), multi-access edge computing (MEC)-enabled satellite-aerial-terrestrial integrated network (SATIN) has emerged as a promising technology to provide massive IoT devices with seamless and reliable communication and computation services. This paper investigates the cooperation of low Earth orbit (LEO) satellites, high altitude platforms (HAPs), and terrestrial base stations (BSs) to provide relaying and computation services for vastly distributed IoT devices. Considering the uncertainty in dynamic SATIN systems, we formulate a stochastic optimization problem to minimize the time-average expected service delay by jointly optimizing resource allocation and task offloading while satisfying the energy constraints. To solve the formulated problem, we first develop a Lyapunov-based online control algorithm to decompose it into multiple one-slot problems. Since each one-slot problem is a large-scale mixed-integer nonlinear program (MINLP) that is intractable for classical computers, we further propose novel hybrid quantum-classical generalized Benders’ decomposition (HQCGBD) algorithms to solve the problem efficiently by leveraging quantum advantages in parallel computing. Numerical results validate the effectiveness of the proposed MEC-enabled SATIN schemes.},
  archive      = {J_TMC},
  author       = {Yu Zhang and Yanmin Gong and Lei Fan and Yu Wang and Zhu Han and Yuanxiong Guo},
  doi          = {10.1109/TMC.2024.3519060},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3878-3889},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Quantum-assisted online task offloading and resource allocation in MEC-enabled satellite-aerial-terrestrial integrated networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin-empowered federated incremental learning for non-IID privacy data. <em>TMC</em>, <em>24</em>(5), 3860-3877. (<a href='https://doi.org/10.1109/TMC.2024.3517592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has emerged as a compelling distributed learning paradigm without sharing local original data. However, with ubiquitous non-independent and identically distributed (non-IID) privacy data, the FL suffers from severe performance loss and the privacy leakage by inference attacks. Existing solutions lack a cohesive framework with theoretical support, and their performance optimization and privacy protection are inter-inhibitive or high-cost. In this paper, we propose a digital twin (DT)-empowered federated incremental learning method to tackle the above challenges. First, we construct a DT-empowered federated incremental learning model to achieve cooperative awareness of performance and privacy-preservation. Second, a diffusion model-based selective data synthesis method is designed to provide auxiliary data for FL, it can avoid unnecessary overhead while ensuring the quality of synthetic samples under non-IID. Besides, it alleviates the negative impact of non-IID by allocating a class-balanced sub-dataset to each DT with IID setting. Third, we develop a DT-empowered alternating incremental learning method initiatively, under the premise of ensuring the confidentiality of original dataset, it can achieve efficient FL performance under non-IID with a small amount of synthetic samples. Furthermore, in order to estimate the contribution of each local model accurately, we investigate a comentropy-based federated aggregation strategy, which can obtain a superior global model. By sufficient theoretical analysis, we prove that the proposed methodology can achieve consistent enhancement of performance and privacy-preservation. Simultaneously, the experiments demonstrate that our methodology has efficient privacy-preserving property, it also outperforms other benchmarks on the accuracy and stability of the global model, especially in highly heterogeneous scenarios.},
  archive      = {J_TMC},
  author       = {Qian Wang and Siguang Chen and Meng Wu and Xue Li},
  doi          = {10.1109/TMC.2024.3517592},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3860-3877},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Digital twin-empowered federated incremental learning for non-IID privacy data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hypergraph approach to deep learning based routing in software-defined vehicular networks. <em>TMC</em>, <em>24</em>(5), 3844-3859. (<a href='https://doi.org/10.1109/TMC.2024.3520657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Vehicular Networks (SDVNs) revolutionize modern transportation by enabling dynamic and adaptable communication infrastructures. However, accurately capturing the dynamic communication patterns in vehicular networks, characterized by intricate spatio-temporal dynamics, remains a challenge with traditional graph-based models. Hypergraphs, due to their ability to represent multi-way relationships, provide a more nuanced representation of these dynamics. Building on this hypergraph foundation, we introduce a novel hypergraph-based routing algorithm. We jointly train a model that incorporates Convolutional Neural Networks (CNN) and Gated Recurrent Units (GRU) using a Deep Deterministic Policy Gradient (DDPG) approach. This model carefully extracts spatial and temporal traffic matrices, capturing elements such as location, time, velocity, inter-dependencies, and distance. An integrated attention mechanism refines these matrices, ensuring precision in capturing vehicular dynamics. The culmination of these components results in routing decisions that are both responsive and anticipatory. Through detailed empirical experiments using a testbed, simulations with OMNeT++, and theoretical assessments grounded in real-world datasets, we demonstrate the distinct advantages of our methodology. Furthermore, when benchmarked against existing solutions, our technique performs better in model interpretability, delay minimization, rapid convergence, reducing complexity, and minimizing memory footprint.},
  archive      = {J_TMC},
  author       = {Ankur Nahar and Nishit Bhardwaj and Debasis Das and Sajal K. Das},
  doi          = {10.1109/TMC.2024.3520657},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3844-3859},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A hypergraph approach to deep learning based routing in software-defined vehicular networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain assisted trust management for data-parallel distributed learning. <em>TMC</em>, <em>24</em>(5), 3826-3843. (<a href='https://doi.org/10.1109/TMC.2024.3521443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models can support decision-making in mobile terminals (MTs) deployments, but their training generally requires massive datasets and abundant computation resources. This is challenging in practice due to the resource constraints of many MTs. To address this issue, data-parallel distributed learning can be conducted by offloading computation tasks from MTs to the edge-layer nodes. To facilitate the establishment of trust, one can leverage trust management, say to use trust values derived from local model quality and evaluations by other nodes as access criteria. Nonetheless, security and performance considerations remain unsolved. In this paper, we propose a blockchain-assisted dynamic trust management scheme for distributed learning, which comprises nodes attributes registration, trust calculation, information saving, and block writing. The proof of stake (PoS) consensus mechanism is leveraged to enable efficient consensus among the nodes using trust values as stakes. The incentive mechanism and corresponding dynamic optimization are then proposed to further improve system performance and security. The reinforcement-learning approach is leveraged to provide the optimal strategy for nodes’ local iterations and selection. Simulations and security analysis demonstrate that our proposed scheme can achieve an optimal trade-off between efficiency and quality of distributed learning while maintaining system security.},
  archive      = {J_TMC},
  author       = {Yuxiao Song and Daojing He and Minghui Dai and Sammy Chan and Kim-Kwang Raymond Choo and Mohsen Guizani},
  doi          = {10.1109/TMC.2024.3521443},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3826-3843},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain assisted trust management for data-parallel distributed learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PIM-LLN: Protocol independent multicast for low-power and lossy networks. <em>TMC</em>, <em>24</em>(5), 3809-3825. (<a href='https://doi.org/10.1109/TMC.2024.3514942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In resource-constrained Internet of Things (IoT) environments like Low-power and Lossy Networks (LLNs), efficient communication protocols are essential. In this context, IP multicast protocols play a crucial role, facilitating the transmission of data packets from a single source to multiple recipients, thereby conserving bandwidth, power, and time for numerous LLN applications, such as over-the-air programming, information dissemination, and device configuration. Despite their usefulness, existing multicast solutions face several challenges, including scalability, energy efficiency, and reliability. To tackle such issues, this paper introduces Protocol Independent Multicast for LLNs (PIM-LLN). PIM-LLN employs a multicast distribution tree anchored at the border router, a multi-path data dissemination mechanism, and an efficient retransmission technique to route streams exclusively to regions with group members reducing energy consumption and bandwidth usage while improving response times and reliability. Through comprehensive simulations and public testbed experiments, we meticulously assess PIM-LLN’s performance, benchmarking it against state-of-the-art solutions under different scenarios. Our findings underscore the scalability, reliability, reduced latency, and efficient resource utilization of PIM-LLN in terms of memory, bandwidth, and energy. Notably, PIM-LLN, as compared to state-of-the-art solutions, achieves a similar level of reliability while reducing overhead by up to 50%.},
  archive      = {J_TMC},
  author       = {Issam Eddine Lakhlef and Badis Djamaa and Mustapha Reda Senouci and Abbas Bradai and Yahia Mohamed Cherif},
  doi          = {10.1109/TMC.2024.3514942},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3809-3825},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PIM-LLN: Protocol independent multicast for low-power and lossy networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint UAV deployment and resource allocation in THz-assisted MEC-enabled integrated space-air-ground networks. <em>TMC</em>, <em>24</em>(5), 3794-3808. (<a href='https://doi.org/10.1109/TMC.2024.3516655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing (MEC)-enabled integrated space-air-ground (SAG) networks have drawn much attention recently, as they can provide communication and computing services to wireless devices in areas that lack terrestrial base stations (TBSs). Leveraging the ample bandwidth in the terahertz (THz) spectrum, in this paper, we propose MEC-enabled integrated SAG networks with collaboration among unmanned aerial vehicles (UAVs). We then formulate the problem of minimizing the energy consumption of devices and UAVs in the proposed MEC-enabled integrated SAG networks by optimizing tasks offloading decisions, THz sub-bands assignment, transmit power control, and UAVs deployment. The formulated problem is a mixed-integer nonlinear programming (MILP) problem with a non-convex structure, which is challenging to solve. We thus propose a block coordinate descent (BCD) approach to decompose the problem into four sub-problems: 1) device task offloading decision problem, 2) THz sub-band assignment and power control problem, 3) UAV deployment problem, and 4) UAV task offloading decision problem. We then propose to use a matching game, concave-convex procedure (CCP) method, successive convex approximation (SCA), and block successive upper-bound minimization (BSUM) approaches for solving the individual subproblems. Finally, extensive simulations are performed to demonstrate the effectiveness of our proposed algorithm.},
  archive      = {J_TMC},
  author       = {Yan Kyaw Tun and György Dán and Yu Min Park and Choong Seon Hong},
  doi          = {10.1109/TMC.2024.3516655},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3794-3808},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint UAV deployment and resource allocation in THz-assisted MEC-enabled integrated space-air-ground networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised learning for complex activity recognition through motif identification learning. <em>TMC</em>, <em>24</em>(5), 3779-3793. (<a href='https://doi.org/10.1109/TMC.2024.3514736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to the cost of collecting labeled sensor data, self-supervised learning (SSL) methods for human activity recognition (HAR) that effectively use unlabeled data for pretraining have attracted attention. However, applying prior SSL to COMPLEX activities in real industrial settings poses challenges. Despite the consistency of work procedures, varying circumstances, such as different sizes of packages and contents in a packing process, introduce significant variability within the same activity class. In this study, we focus on sensor data corresponding to characteristic and necessary actions (sensor data motifs) in a specific activity such as a stretching packing tape action in an assembling a box activity, and propose to train a neural network in self-supervised learning so that it identifies occurrences of the characteristic actions, i.e., Motif Identification Learning (MoIL). The feature extractor in the network is subsequently employed in the downstream activity recognition task, enabling accurate recognition of activities containing these characteristic actions, even with limited labeled training data. The MoIL approach was evaluated on real-world industrial activity data, encompassing the state-of-the-art SSL tasks with an improvement of up to 23.85% under limited training labels.},
  archive      = {J_TMC},
  author       = {Qingxin Xia and Jaime Morales and Yongzhi Huang and Takahiro Hara and Kaishun Wu and Hirotomo Oshima and Masamitsu Fukuda and Yasuo Namioka and Takuya Maekawa},
  doi          = {10.1109/TMC.2024.3514736},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3779-3793},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Self-supervised learning for complex activity recognition through motif identification learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alleviating data sparsity to enhance AI models robustness in IoT network security context. <em>TMC</em>, <em>24</em>(5), 3764-3778. (<a href='https://doi.org/10.1109/TMC.2025.3525463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Internet of Things (IoT) networks, the IoT sensors collect valuable raw data required to sustain Artificial Intelligence (AI) based networks operation. AI models are data-driven as they use the data to make accurate network security, management, and operational decisions. Unfortunately, the sensors are deployed in harsh environments which affects the sensor behaviour and eventually the networks’ operations. Further, IoT devices are typically vulnerable to a range of malicious events. Therefore, IoT sensor's correct operation including resilience to failure is essential for sustained operations. Naturally, the state variables of time-series data can be changed, i.e., the data streams generated in these situations can be incorrect, incomplete or missing, and sparse presenting a significant challenge for real-time decision-making ability of AI models to make explainable and intelligent management and control decisions. In this paper, we aim to alleviate this fundamental problem to predict the missing and faulty reading correctly so that the decision-making ability of the AI models should not deteriorate in the presence of incorrect, missing, and highly imbalanced data sets. We use a novel approach using fuzzy-based information decomposition to recover the missed data values. We use three data sets, and our preliminary results show that our approach effectively recovers the missed or compromised data samples and help AI models in making accurate decision. Finally, the limitations and future work of this research have been discussed.},
  archive      = {J_TMC},
  author       = {Keshav Sood and Shigang Liu and Dinh Duc Nha Nguyen and Neeraj Kumar and Bohao Feng and Shui Yu},
  doi          = {10.1109/TMC.2025.3525463},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3764-3778},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Alleviating data sparsity to enhance AI models robustness in IoT network security context},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIQA: An application agent for immersive content delivery over millimeter waves. <em>TMC</em>, <em>24</em>(5), 3750-3763. (<a href='https://doi.org/10.1109/TMC.2024.3514973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The highly directional nature of the millimeter wave (mmWave) beams causes several challenges in using that spectrum to meet the communication demands of immersive applications. The mmWave beams are especially susceptible to misalignments and blockages caused by user movements. As a result, mmWave channels are vulnerable to large quality fluctuations, which in turn, degrades the end-to-end performance of immersive applications. In this paper, we propose a reinforcement learning (RL) based application-layer plugin that works in conjunction with the QUIC protocol to combat the challenges of mmWave networks. The plug-in called Millimeter wave based Immersive QUIC Agent (MIQA) uses the RL model to help modulate the sending rate along with the congestion control scheme of QUIC. To evaluate the effectiveness of MIQA, we conduct experiments on a mmWave augmented immersive testbed. The evaluation results show that MIQA significantly improves the immersive experience by increasing the end-to-end throughput and by decreasing the end-to-end latency.},
  archive      = {J_TMC},
  author       = {Zongshen Wu and Chin-Ya Huang and Parameswaran Ramanathan},
  doi          = {10.1109/TMC.2024.3514973},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3750-3763},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MIQA: An application agent for immersive content delivery over millimeter waves},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentially private weighted graphs publication under continuous monitoring. <em>TMC</em>, <em>24</em>(5), 3735-3749. (<a href='https://doi.org/10.1109/TMC.2024.3514153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph data analysis has been used in various real-world applications to improve services or scientific research, which, however, may expose sensitive personal information. Differential privacy (DP) has become the gold standard for publishing graph data while still protecting personal privacy. However, most existing studies over differentially private graph data publication mainly focus on static unweighted graphs. As interactions between entities in real systems are often dynamically changing and associated with weights, it is desirable to consider the more general scenario of continuous weighted graph publication under DP in the temporal dimension. Therefore, we investigate the problem of publishing weighted graphs satisfying DP under continuous monitoring. Specifically, we consider a server that continuously monitors user data and publishes a sequence of weighted graph snapshots. We propose SwgDP, a novel framework that leverages historical graph data to guide current snapshot generation. SwgDP consists of four key components: node adaptive sampling, dynamic weight optimization, prediction-based community detection and weighted graph generation. We demonstrate that SwgDP satisfies DP, and comprehensive experiments on four real-world datasets and four commonly used graph metrics show that SwgDP can effectively synthesize weighted graph at any time step.},
  archive      = {J_TMC},
  author       = {Wen Xu and Zhetao Li and Haolin Liu and Yunjun Gao and Xiaofei Liao and Kenli Li},
  doi          = {10.1109/TMC.2024.3514153},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3735-3749},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Differentially private weighted graphs publication under continuous monitoring},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User-driven privacy-preserving data streams release for multi-task assignment in mobile crowdsensing. <em>TMC</em>, <em>24</em>(5), 3719-3734. (<a href='https://doi.org/10.1109/TMC.2024.3516885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task assignment is widely used in mobile crowdsensing (MCS) to efficiently utilize limited resources such as shared user pool, user capability constraints and so on. In MCS, users need to submit data streams to perform sensing tasks, which involve a large amount of private information. However, the privacy leakage when users perform tasks across different types and submit multimodal data streams in multi-task assignment has not been fully addressed in current works. Privacy requirements vary for users with different activity levels in multi-task assignment. Specifically, users with higher activity levels tend to handle more task types and submit more data types, which poses more serious consequences of privacy leakage. Meanwhile, the privacy requirements of users are dynamic due to the user’s changing activity. In this work, we propose a user-driven local differential privacy framework for multi-task assignment called UD-LDP. First, we design a flexible privacy model called $w$-adjacent-event privacy to provide accurate privacy protection for users with different activity levels. Then, we introduce information entropy to quantify privacy requirements of user’s activity in real-time. After that, we propose a privacy-aware budget allocation method to dynamically allocate personalized privacy budgets for each user. At last, we design a variance-optimized selection method that chooses rational privacy budgets and users for release to improve data utility. The effectiveness of our framework is supported by experiments conducted on both real-world and synthetic datasets.},
  archive      = {J_TMC},
  author       = {Zhetao Li and Junru Wu and Saiqin Long and Zhirun Zheng and Chengxin Li and Mianxiong Dong},
  doi          = {10.1109/TMC.2024.3516885},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3719-3734},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {User-driven privacy-preserving data streams release for multi-task assignment in mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FinerSense: A fine-grained respiration sensing system based on precise separation of wi-fi signals. <em>TMC</em>, <em>24</em>(5), 3703-3718. (<a href='https://doi.org/10.1109/TMC.2024.3514311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel approach for preventing overexertion in home fitness through fine-grained detection of respiratory parameters. To overcome the robustness limitation associated with using a composite signal for wireless sensing, we introduce an optimization-based signal separation model. This model effectively disentangles composite signals into static and dynamic components, while preserving the intricate details of target movements or activities. Specifically, by constructing a reference signal derived from the dominant static component, we eliminate time-varying phase shifts and leverage the invariant property of the dynamic component’s amplitude for precise separation. A system called FinerSense is developed, which is able to accurately and robustly detect fine-grained respiratory parameters such as respiration rate, depth, and inhalation-to-exhalation ratio with accuracy rates exceeding 97%, 95%, and 91%, respectively. Extensive experiments show that the developed system outperforms state-of-the-art baselines significantly, empowering users to optimize exercise intensity and duration while mitigating the risk of overexertion. We believe that this work is able to facilitate the seamless transition of wireless sensing systems from laboratory prototypes to practical and user-friendly applications.},
  archive      = {J_TMC},
  author       = {Wenchao Song and Zhu Wang and Yifan Guo and Zhuo Sun and Zhihui Ren and Chao Chen and Bin Guo and Zhiwen Yu and Xingshe Zhou and Daqing Zhang},
  doi          = {10.1109/TMC.2024.3514311},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3703-3718},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FinerSense: A fine-grained respiration sensing system based on precise separation of wi-fi signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service function chain deployment with VNF-dependent software migration in multi-domain networks. <em>TMC</em>, <em>24</em>(5), 3685-3702. (<a href='https://doi.org/10.1109/TMC.2024.3514173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the 6G era, user demand for low-latency, cost-effective extreme services such as extended reality (XR) and holographic communications has significantly increased. Multi-domain networks, known for their vast capacity and coverage, are essential in fulfilling the growing demand for high-performance services. Despite their potential, these networks face challenges with domain isolation, requiring a software defined network (SDN) controller for inter-domain communication. Network function virtualization (NFV) enhances flexibility of service delivery with customizable service function chain (SFC), yet prior research falls short in delivering low-latency, cost-efficient services in multi-domain NFV networks alongside an unreasonable assumption that software on physical nodes can support the execution of all virtualization network functions (VNFs). In this paper, we study the problem of SFC deployment with VNF-dependent software migration (SD-VDSM) in multi-domain networks. Particularly, we first formulate the problem by setting an objective to minimize the end-to-end communication delay and the associated costs of service provisioning, while simultaneously ensuring load balancing across multi-domain networks. However, complexity of the issue escalates to an intractable level due to the intertwined nature of SFC deployment strategies and VNF-dependent software migration tactics, which mutually influence each other intricately. To tackle this issue, we propose an innovative heuristic algorithm, designated as the Joint SFC Deployment with VNF-Dependent Software Migration Algorithm (JSD-VDSMA). Comprising three fundamental steps, this algorithm is crafted to adeptly resolve the complexities of service provisioning across multi-domain networks. A suite of rigorous experimental assessments is detailed, demonstrating the capability of our proposed JSD-VDSMA. Through these comparative analyses, we demonstrate its effectiveness not only to increase the service acceptance rate but also to diminish both the end-to-end communication delay and resource utilization costs in comparison to its counterparts.},
  archive      = {J_TMC},
  author       = {Yuhan Zhang and Ran Wang and Jie Hao and Qiang Wu and Yidan Teng and Ping Wang and Dusit Niyato},
  doi          = {10.1109/TMC.2024.3514173},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3685-3702},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Service function chain deployment with VNF-dependent software migration in multi-domain networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HyperRegion: Integrating graph and hypergraph contrastive learning for region embeddings. <em>TMC</em>, <em>24</em>(5), 3667-3684. (<a href='https://doi.org/10.1109/TMC.2024.3515154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Region representations (also called embeddings) are useful for various urban computing tasks. While graph-based region representation learning methods have shown outstanding performance, they encounter two major challenges: 1) the pervasive data noise and missing data can affect the quality of the constructed region graphs; and 2) high-order relationships (i.e., group-wise relationships) among regions are often insufficiently modeled and sometimes entirely overlooked. To this end, we propose HyperRegion, an unsupervised region representation learning framework that integrates graph and hypergraph contrastive learning to learn comprehensive region embeddings from multi-modal data. Built upon a region hybrid graph network, this framework models both pair-wise and group-wise dependencies involving POI semantics, mobility patterns, geographic neighbors, and visual semantics. To mitigate the impact of data noise and missing data, graph and hypergraph contrastive learning are performed in parallel, and a cross-module contrast is further introduced to facilitate information exchange and collaboration. Extensive experiments on real-world datasets across three downstream tasks demonstrate that HyperRegion outperforms all baselines, particularly improving check-in prediction by reducing MAE and RMSE by approximately 8.5% and 8.2%, respectively, and increasing $R^{2}$ by about 7%.},
  archive      = {J_TMC},
  author       = {Mingyu Deng and Chao Chen and Wanyi Zhang and Jie Zhao and Wei Yang and Suiming Guo and Huayan Pu and Jun Luo},
  doi          = {10.1109/TMC.2024.3515154},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3667-3684},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HyperRegion: Integrating graph and hypergraph contrastive learning for region embeddings},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource-efficient DNN inference with early exiting in serverless edge computing. <em>TMC</em>, <em>24</em>(5), 3650-3666. (<a href='https://doi.org/10.1109/TMC.2024.3514993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless Edge Computing (SEC) has gained widespread adoption in improving resource utilization due to its triggered event-driven model. However, deploying deep neural network (DNN) inference services directly in SEC leads to resource inefficiencies, which stem from two key factors. First, existing methods adopt model-wise function encapsulation, which requires the entire DNN model to occupy memory throughout its execution lifecycle. This increases both memory footprint and occupancy time. Second, uniform DNN inference for diversity input leads to redundant computations and additional inference time. To this end, we propose REDI, a novel framework that leverages fine-grained block-wise function encapsulation and progressive inference to provide resource-efficient DNN inference while ensuring latency requirements. REDI enables the release of memory from already inferred shallow networks and allows each request to exit early based on input data complexity, eliminating redundant computations. To fully unleash the potential, REDI jointly considers resource heterogeneity, data diversity, and environment dynamics to investigate the block-wise function placement problem. We introduce an uncertainty-aware online learning-driven algorithm with bounded regret. Finally, we conduct extensive trace-driven experiments to evaluate our methods, demonstrating that REDI achieves a significant speedup of up to $6.52\times$ in terms of resource usage cost compared to state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Xiaolin Guo and Fang Dong and Dian Shen and Zhaowu Huang and Jinghui Zhang},
  doi          = {10.1109/TMC.2024.3514993},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3650-3666},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resource-efficient DNN inference with early exiting in serverless edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiCamera: Vortex electromagnetic wave-based WiFi imaging. <em>TMC</em>, <em>24</em>(5), 3633-3649. (<a href='https://doi.org/10.1109/TMC.2024.3519623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current WiFi imaging approaches focus on monitoring dynamic targets to facilitate easy object distinction and capture rich signal reflections for image construction. In static object imaging, massive antenna array or emulated antenna array is often necessary. We propose WiCamera, a novel WiFi imaging prototype that utilizes vortex electromagnetic waves (VEMWs) to monitor stationary human postures using commodity WiFi, by generating human silhouettes with only $3 \times 3$ MIMO. VEMWs possess a helical wavefront with different phase variations, enabling the imaging of stationary objects through different OAM (Orbital Angular Momentum) modes with time-division multiplexing. WiCamera emits three OAM modes waves from WiFi devices and utilizes their phase variations for imaging. By ray tracing the received signals to a target image plane, WiCamera generates a wavefront image. A generative adversarial network (GAN)-based model is further utilized to refine the wavefront image and create a high-resolution human silhouette. The system's output images are evaluated using metrics such as structural similarity index measure (SSIM) and Szymkiewicz-Simpson coefficient (SSC), comparing them to ground truth images captured by cameras. The evaluation shows that WiCamera performs consistently well in various environments and with different users, with an SSIM reaching up to 0.89 and an SSC reaching up to 0.93.},
  archive      = {J_TMC},
  author       = {Leiyang Xu and Xiaolong Zheng and Xinrun Du and Liang Liu and Huadong Ma},
  doi          = {10.1109/TMC.2024.3519623},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3633-3649},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WiCamera: Vortex electromagnetic wave-based WiFi imaging},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mystique: User-level adaptation for real-time video analytics in edge networks via meta-RL. <em>TMC</em>, <em>24</em>(5), 3615-3632. (<a href='https://doi.org/10.1109/TMC.2024.3514088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN)-based real-time video analytics service, as a core module for numerous crucial applications such as augmented reality (AR), has garnered increasing research attention, where mobile edge computing (MEC) is often leveraged to mitigate its real-time processing burden on resource-constrained user devices. For Quality of Experience (QoE) optimization, latest works employ reinforcement learning (RL)-based methods to adaptively adjust configurations (e.g., resolution and frame rate), yet still presenting significant challenges. Firstly, we observe a substantial diversity in QoE patterns among users. Given that existing methods integrate a fixed QoE pattern in parameter training, it is intuitive to customize a policy network for each user. However, this necessitates significant training investment, failing to support on-the-fly deployment for new users. Secondly, given the dual dynamics from both the network and video content in edge video analytics system, existing methods often fall into the dilemma of fitting newly emerged and diverse system states with offline-trained fixed parameters. While it is promising to employ online learning algorithms, most of them struggle to catch up with the high dynamics. We hence propose Mystique. In real-time edge video analytics domain, it is the first meta-RL-based user-level configuration adaptation framework. Mystique establishes an initial model in offline meta training with model-agnostic meta-learning (MAML), enabling swift online adaptation to new users and system states through limited gradient updates from initial parameters. Comprehensive experiments illustrate that Mystique can improve QoE by 42% on average compared to prior works.},
  archive      = {J_TMC},
  author       = {Xiaohang Shi and Sheng Zhang and Meizhao Liu and Lingkun Meng and Liu Wei and Yingcheng Gu and Kai Liu and Huanyu Cheng and Yu Song and Lei Tang and Andong Zhu and Ning Chen and Zhuzhong Qian},
  doi          = {10.1109/TMC.2024.3514088},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3615-3632},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mystique: User-level adaptation for real-time video analytics in edge networks via meta-RL},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring context generalizability in citywide crowd mobility prediction: An analytic framework and benchmark. <em>TMC</em>, <em>24</em>(5), 3597-3614. (<a href='https://doi.org/10.1109/TMC.2024.3517332'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contextual features are important data sources for building citywide crowd mobility prediction models. However, the difficulty of applying context lies in the unknown generalizability of contextual features (e.g., weather, holiday, and points of interests) and context modeling techniques across different scenarios. In this paper, we present a unified analytic framework and a large-scale benchmark for evaluating context generalizability. The benchmark includes crowd mobility data, contextual data, and advanced prediction models. We conduct comprehensive experiments in several crowd mobility prediction tasks such as bike flow, metro passenger flow, and electric vehicle charging demand. Our results reveal several important observations: (1) Using more contextual features may not always result in better prediction with existing context modeling techniques; in particular, the combination of holiday and temporal position can provide more generalizable beneficial information than other contextual feature combinations. (2) In context modeling techniques, using a gated unit to incorporate raw contextual features into the deep prediction model has good generalizability. Besides, we offer several suggestions about incorporating contextual factors for building crowd mobility prediction applications. From our findings, we call for future research efforts devoted to developing new context modeling solutions.},
  archive      = {J_TMC},
  author       = {Liyue Chen and Xiaoxiang Wang and Leye Wang},
  doi          = {10.1109/TMC.2024.3517332},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3597-3614},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring context generalizability in citywide crowd mobility prediction: An analytic framework and benchmark},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based joint client clustering and resource allocation for wireless distributed learning: A new hierarchical federated learning framework with non-IID data. <em>TMC</em>, <em>24</em>(5), 3579-3596. (<a href='https://doi.org/10.1109/TMC.2024.3515037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical federated learning (HFL) is a key technology enabling distributed learning with reduced communication overhead. However, practical HFL systems encounter two major challenges: limited resources and data heterogeneity. In particular, limited resources can result in intolerable system latency, while heterogeneous data across clients can significantly degrade model accuracy and convergence rates. To address these issues and fully leverage the potential of HFL, we propose a novel framework called graph-based joint client and resource orchestration. This framework addresses the challenges of practical networks through joint client clustering and resource allocation. First, we propose a learning process where edge servers employ hypernetworks to achieve edge aggregation. This method can generate personalized client models and extract data distributions without directly exposing data distributions. Then, to characterize the joint effects of limited resources and data heterogeneity, we propose a graph-based modeling method and formulate a joint optimization problem that aims to balance data distributions and minimize latency. Subsequently, we propose a graph neural network-based algorithm to tackle the formulated problem with low-complexity optimization. Numerical results demonstrate significant benefits over existing algorithms in terms of convergence latency, model accuracy, scalability, and adaptability to new distributions.},
  archive      = {J_TMC},
  author       = {Ercong Yu and Shanyun Liu and Qiang Li and Hongyang Chen and H. Vincent Poor and Shlomo Shamai},
  doi          = {10.1109/TMC.2024.3515037},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3579-3596},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Graph-based joint client clustering and resource allocation for wireless distributed learning: A new hierarchical federated learning framework with non-IID data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minimizing age of processed information over unreliable wireless network channels. <em>TMC</em>, <em>24</em>(5), 3567-3578. (<a href='https://doi.org/10.1109/TMC.2024.3520913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The freshness of real-time status processing of time-sensitive information is crucial for many applications, including flight control, image processing, and autonomous vehicles. In this paper, unprocessed information is sent from sensors to a base station over a shared, unreliable wireless network. The base station has a set of dedicated non-preemptive processors with constant processing times to process information from each sensor. The age of processed information is the time elapsed since the generation of the packet that the processor most recently processed. Our objective is to minimize the expected weighted sum of this age over an infinite time horizon. Here, the challenge is the coupling between a scheduling problem under unreliable communications and the processing times. We first break the coupling by tracking the age of information during processing and derive a lower performance bound of the objective. We then design a stationary randomized policy and a Max-Weight policy for two queueing disciplines: no queues and single-packet queues to achieve our objective. We prove that these policies achieve performance within a factor of two from the optimal. In addition, we prove queues are useful to the stationary randomized policies in highly unreliable or large network settings. Our analytical results are further validated by numerical experiments.},
  archive      = {J_TMC},
  author       = {Wasin Meesena and Chanikarn Nikunram and Stephen John Turner and Sucha Supittayapornpong},
  doi          = {10.1109/TMC.2024.3520913},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3567-3578},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Minimizing age of processed information over unreliable wireless network channels},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). R2Pricing: A MARL-based pricing strategy to maximize revenue in MoD systems with ridesharing and repositioning. <em>TMC</em>, <em>24</em>(5), 3552-3566. (<a href='https://doi.org/10.1109/TMC.2024.3514124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pricing strategy is crucial for improving the revenue of mobility on-demand (MoD) systems by achieving supply-demand equilibrium across different city zones. Modern MoD systems commonly utilize order ridesharing and vehicle repositioning to improve the order completion rate while supporting this equilibrium, thereby improving the revenue. However, most existing pricing strategies overlook the effects of ridesharing and repositioning, resulting in supply-demand mismatch and revenue decline. To fill this gap, we propose a multi-agent reinforcement learning (MARL) based pricing strategy via a mutual attention mechanism, named R2Pricing, where the impact of ridesharing and repositioning is considered. First, we formulate the pricing with ridesharing and repositioning as an optimization problem toward maximum overall revenue. Then, we transform it into a MARL model, where the agent makes coupled decisions about order fare with ridesharing and vehicle income with repositioning for each zone. Next, the agents are clustered based on supply-demand observation and reward to train more efficiently. The pricing messages between agents are generated based on mutual information theory, which is then aggregated with an attention mechanism to estimate the impact of price differences among zones. Finally, simulations based on real-world data are conducted to demonstrate the superiority of R2Pricing over the benchmarks.},
  archive      = {J_TMC},
  author       = {Shuxin Ge and Xiaobo Zhou and Tie Qiu},
  doi          = {10.1109/TMC.2024.3514124},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3552-3566},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {R2Pricing: A MARL-based pricing strategy to maximize revenue in MoD systems with ridesharing and repositioning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmFinger: Talk to smart devices with finger tapping gesture. <em>TMC</em>, <em>24</em>(5), 3537-3551. (<a href='https://doi.org/10.1109/TMC.2024.3515044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contact-free finger gesture recognition unlocks plenty of applications in smart Human-Computer Interaction (HCI). However, existing solutions either require users to wear sensors on their fingers or use continuously monitored cameras, raising concerns regarding user comfort and privacy. In this paper, we propose mmFinger, an accurate and robust mmWave-based finger gesture recognition system that can extend the range of available custom commands. The core idea is that mmFinger leverages the finger tapping pattern as a basic gesture and encodes different number combinations of the basic gesture like Morse code. To enable reliable recognition across different locations and for various users, we carefully design a robust feature Dop-profile to effectively characterize finger movements. Furthermore, by leveraging the multi-views provided by multiple antennas of radar, we develop an adaptive weighted feature fusion network to enhance the system's robustness. Finally, we devise a novel sequence prediction network to enable the system to recognize new gestures without retraining. Comprehensive experiments demonstrate that mmFinger can achieve an average recognition accuracy of 92% for 36 predefined gestures and 88% for 5 new user-defined commands, and is robust against finger location and user diversity.},
  archive      = {J_TMC},
  author       = {Xuan Wang and Xuerong Zhao and Chao Feng and Dingyi Fang and Xiaojiang Chen},
  doi          = {10.1109/TMC.2024.3515044},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3537-3551},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmFinger: Talk to smart devices with finger tapping gesture},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling large scale LoRa parallel decoding with high-dimensional and high-accuracy features. <em>TMC</em>, <em>24</em>(5), 3520-3536. (<a href='https://doi.org/10.1109/TMC.2024.3517343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRaWAN is a prominent technology for Low Power Wide Area Networks (LPWAN). However, the increasing network size has introduced a significant challenge: packet collisions resulting from concurrent transmissions in LoRaWAN. Previous studies either overlooked the issue by examining limited features or tackled it with intricate receivers employing up to eight antennas. To achieve a more favorable balance between implementation cost and system performance, we introduce $\text{Hi}^{2}\text{LoRa}$—a solution utilizing highly dimensional and accurate features for LoRa concurrent decoding, implemented with only two receiving antennas. The feature dimensions are expanded through an exploration of various hardware imperfections and inherent channel state information specific to each transceiver pair. To enhance feature accuracy, low pass filters and BiLSTM networks are applied to capture and learn their temporal patterns. Additionally, an efficient collision suppression strategy is introduced to mitigate feature corruption from concurrently transmitted packets. Extensive real-world testbed evaluations demonstrate that the achievable concurrency in $\text{Hi}^{2}\text{LoRa}$ approaches that of state-of-the-art approaches with significantly higher complexity (e.g., utilizing eight antennas) or exceeds prior work by a factor of 2.7 with comparable complexity (e.g., using two antennas).},
  archive      = {J_TMC},
  author       = {Weiwei Chen and Xianjin Xia and Shuai Wang and Tian He and Shuai Wang and Gang Liu and Caishi Huang},
  doi          = {10.1109/TMC.2024.3517343},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3520-3536},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling large scale LoRa parallel decoding with high-dimensional and high-accuracy features},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multicast scheduling over multiple channels: A distribution-embedding deep reinforcement learning method. <em>TMC</em>, <em>24</em>(5), 3502-3519. (<a href='https://doi.org/10.1109/TMC.2024.3514169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multicasting is an efficient technique for simultaneously transmitting common messages from the base station (BS) to multiple mobile users (MUs). Multicast scheduling over multiple channels, which aims to jointly minimize the energy consumption of the BS and the latency of serving asynchronized requests from the MUs, is formulated as an infinite-horizon Markov decision process (MDP) problem with a large discrete action space, multiple time-varying constraints, and multiple time-invariant constraints. To address these challenges, this paper proposes a novel distribution-embedding multi-agent proximal policy optimization (DE-MAPPO) algorithm, which consists of one modified MAPPO and one distribution-embedding module. The former one handles the large discrete action space and time-varying constraints by modifying the structure of the actor networks and the training kernel of the conventional MAPPO; and the latter one iteratively adjusts the action distribution to satisfy the time-invariant constraints. Moreover, a performance upper bound of the considered MDP is derived by solving a two-step optimization problem. Finally, numerical results demonstrate that our proposed algorithm outperforms the existing ones in terms of applicability, effectiveness, and robustness, and achieves comparable performance to the derived upper bound.},
  archive      = {J_TMC},
  author       = {Ran Li and Chuan Huang and Xiaoqi Qin and Dong Yang and Xinyao Nie},
  doi          = {10.1109/TMC.2024.3514169},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3502-3519},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multicast scheduling over multiple channels: A distribution-embedding deep reinforcement learning method},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distortion resilience for goal-oriented semantic communication. <em>TMC</em>, <em>24</em>(5), 3489-3501. (<a href='https://doi.org/10.1109/TMC.2024.3456856'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research efforts on Semantic Communication (SemCom) have mostly considered accuracy as a main problem for optimizing goal-oriented communication systems. However, these approaches introduce a paradox: the accuracy of Artificial Intelligence (AI) tasks should naturally emerge through training rather than being dictated by network constraints. Acknowledging this dilemma, this work introduces an innovative approach that leverages the rate distortion theory to analyze distortions induced by communication and compression, thereby analyzing the learning process. Specifically, we examine the distribution shift between the original data and the distorted data, thus assessing its impact on the AI model's performance. Founding upon this analysis, we can preemptively estimate the empirical accuracy of AI tasks, making the goal-oriented SemCom problem feasible. To achieve this objective, we present the theoretical foundation of our approach, accompanied by simulations and experiments that demonstrate its effectiveness. The experimental results indicate that our proposed method enables accurate AI task performance while adhering to network constraints, establishing it as a valuable contribution to the field of signal processing. Furthermore, this work advances research in goal-oriented SemCom and highlights the significance of data-driven approaches in optimizing the performance of intelligent systems.},
  archive      = {J_TMC},
  author       = {Minh-Duong Nguyen and Quang Vinh Do and Zhaohui Yang and Quoc-Viet Pham and Won-Joo Hwang},
  doi          = {10.1109/TMC.2024.3456856},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {5},
  number       = {5},
  pages        = {3489-3501},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distortion resilience for goal-oriented semantic communication},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource allocation for metaverse experience optimization: A multi-objective multi-agent evolutionary reinforcement learning approach. <em>TMC</em>, <em>24</em>(4), 3473-3488. (<a href='https://doi.org/10.1109/TMC.2024.3509680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Metaverse, real-time, concurrent services such as virtual classrooms and immersive gaming require local graphic rendering to maintain low latency. However, the limited processing power and battery capacity of user devices make it challenging to balance Quality of Experience (QoE) and terminal energy consumption. In this paper, we investigate a multi-objective optimization problem (MOP) regarding power control and rendering capacity allocation by formulating it as a multi-objective optimization problem. This problem aims to minimize energy consumption while maximizing Meta-Immersion (MI), a metric that integrates objective network performance with subjective user perception. To solve this problem, we propose a Multi-Objective Multi-Agent Evolutionary Reinforcement Learning with User-Object-Attention (M2ERL-UOA) algorithm. The algorithm employs a prediction-driven evolutionary learning mechanism for multi-agents, coupled with optimized rendering capacity decisions for virtual objects. The algorithm can yield a superior Pareto front that attains the Nash equilibrium. Simulation results demonstrate that the proposed algorithm can generate Pareto fronts, effectively adapts to dynamic user preferences, and significantly reduces decision-making time compared to several benchmarks.},
  archive      = {J_TMC},
  author       = {Lei Feng and Xiaoyi Jiang and Yao Sun and Dusit Niyato and Yu Zhou and Shiyi Gu and Zhixiang Yang and Yang Yang and Fanqin Zhou},
  doi          = {10.1109/TMC.2024.3509680},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3473-3488},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resource allocation for metaverse experience optimization: A multi-objective multi-agent evolutionary reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do as the romans do: Location imitation-based edge task offloading for privacy protection. <em>TMC</em>, <em>24</em>(4), 3456-3472. (<a href='https://doi.org/10.1109/TMC.2024.3509418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge computing, a user prefers offloading his/her task to nearby edge servers to maximize the offloading utility. However, this inevitably exposes the user's location privacy information when suffering from the side-channel attacks based on offloading decision behaviors and Received Signal Strength Indicators (RSSI). Existing works only consider the scenario with one untrusted edge server or defend only against one of the attacks. In this paper, we first study the edge task offloading problem with comprehensive privacy protection against these side-channel attacks from multiple edge servers. To address this problem while ensuring satisfactory offloading utility, we develop a Location Imitation-based Edge Task Offloading approach LITO. Specifically, we first determine a suitable perturbation region centered at the user's real location for a balance between offloading utility and privacy protection, and then propose a modified Laplace mechanism to generate a fake location meeting geo-indistinguishability within the region. Subsequently, to mislead the side-channel attacks to the fake location, we design an approximate algorithm and a transmit power control strategy to imitate the offloading decisions and RSSIs at the fake location, respectively. Theoretical analysis and experimental evaluations demonstrate the performance of LITO in improving privacy protection and guaranteeing offloading utility.},
  archive      = {J_TMC},
  author       = {Jiahao Zhu and Lu Zhao and Jian Zhou and Hui Cai and Fu Xiao},
  doi          = {10.1109/TMC.2024.3509418},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3456-3472},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Do as the romans do: Location imitation-based edge task offloading for privacy protection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing fault-tolerant time-aware flow scheduling in TSN-5G networks. <em>TMC</em>, <em>24</em>(4), 3441-3455. (<a href='https://doi.org/10.1109/TMC.2024.3510604'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of time-sensitive networking (TSN) and fifth-generation (5G) offers a promising solution for real-time and reliable data transmission in the Industrial Internet of Things (IIoT). However, current research focuses on traffic scheduling in TSN-5G networks to support low latency. New challenges arise when TSN-5G networks leverage time-aware shaper (TAS) and frame replication and elimination for reliability (FRER) to achieve low latency and high reliability. Simply combining TAS and FRER (SCTF) requires scheduling all time-triggered (TT) flows and their replica flows, which substantially increases the computational complexity of gate control lists (GCLs) and severely weakens scheduling capabilities. Moreover, the packet elimination function (PEF) in FRER may induce packet misordering. In this paper, we propose an efficient and fault-tolerant time-aware shaper (EF-TAS) mechanism for TSN-5G networks. EF-TAS only allocates timeslots for TT flows, while replica TT (RT) flows are delivered using a best-effort strategy. Due to the potential violation of deadlines in RT flows, we design an adaptive cyclic GCL window (ACGW)-based hybrid scheduling (AHS) algorithm to schedule TT and RT flows differentially. The AHS algorithm utilizes network calculus to ensure the timely arrival of RT flows without affecting the deterministic transmission of TT flows. In particular, we provide upper bounds on the amount of reordering to quantify the disorder caused by PEF and analyze the impact of introducing the packet ordering function (POF) on EF-TAS performance. The evaluation results show that EF-TAS not only meets the reliability and deadline requirements but also significantly reduces the total number of GCL entries and the computation time of GCLs compared to state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Guizhen Li and Shuo Wang and Yudong Huang and Tao Huang and Yuanhao Cui and Zehui Xiong},
  doi          = {10.1109/TMC.2024.3510604},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3441-3455},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing fault-tolerant time-aware flow scheduling in TSN-5G networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile tile-based 360$^\circ$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math> video multicast with cybersickness alleviation. <em>TMC</em>, <em>24</em>(4), 3423-3440. (<a href='https://doi.org/10.1109/TMC.2024.3514852'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) imaging is 360°, which requires a large bandwidth for video transmission. To address this challenge, tile-based streaming has been proposed to deliver only the focused part of the video instead of the entire one. However, the impact of cybersickness, akin to motion sickness, on tile selection in VR has not been explored. In this paper, we investigate Multi-user Tile Streaming with Cybersickness Control (MTSCC) in an adaptive 360$^\circ$ video streaming system with multicast and cybersickness alleviation. We propose a novel $m^{2}$-competitive online algorithm that utilizes Individual Sickness Indicator (ISI) and Bitrate Restriction Indicator (BRI) to evaluate user cybersickness tendency and network bandwidth efficiency. Moreover, we introduce the Video Loss Indicator (VLI) and Quality Variance Indicator (QVI) to assess video quality loss and quality difference between tiles. We also propose a multi-armed bandit (MAB) algorithm with confidence bound-based reward (video quality) and cost (cybersickness) estimation. The algorithm learns the weighting factor of each user's cost to slow down cybersickness accumulation for users with high cybersickness tendencies. We prove that the algorithm converges to an optimal solution over time. According to simulation with real network settings, our proposed algorithms outperform baselines in terms of video quality and cybersickness accumulation.},
  archive      = {J_TMC},
  author       = {Chiao-Wen Lin and De-Nian Yang and Wanjiun Liao},
  doi          = {10.1109/TMC.2024.3514852},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3423-3440},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobile tile-based 360$^\circ$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mo>∘</mml:mo></mml:msup></mml:math> video multicast with cybersickness alleviation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing RFID technology for virtual boundary detection. <em>TMC</em>, <em>24</em>(4), 3407-3422. (<a href='https://doi.org/10.1109/TMC.2024.3514895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A boundary is a physical or virtual line that marks the edge or limit of a specific region, which has been widely used in many applications, such as autonomous driving, virtual wall, and robotic lawn mowers. However, none of existing work can well balance the deployability and the scalability of a boundary. In this paper, we propose a brand new RFID-based virtual boundary scheme together with its detection algorithm called RF-Boundary, which has the competitive advantages of being battery-free and easy-to-maintain. We develop two technologies of phase gradient and dual-antenna AoA to address the key challenges posed by RF-boundary, in terms of lack of calibration information and multi-edge interference. Besides, we consider the presence of multipath in the real world applications, model the effect on signals in the dynamic scenarios, and demonstrate the robustness of our phase gradient-based scheme under multipath. We implement a prototype of RF-Boundary with commercial RFID systems and a mobile robot. Extensive experiments verify the feasibility as well as the good performance of RF-Boundary, with a mean detection error of only 8.6 cm.},
  archive      = {J_TMC},
  author       = {Xiaoyu Li and Jia Liu and Zihao Lin and Xuan Liu and Yanyan Wang and Shigeng Zhang and Baoliu Ye},
  doi          = {10.1109/TMC.2024.3514895},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3407-3422},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Advancing RFID technology for virtual boundary detection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASMAFL: Adaptive staleness-aware momentum asynchronous federated learning in edge computing. <em>TMC</em>, <em>24</em>(4), 3390-3406. (<a href='https://doi.org/10.1109/TMC.2024.3510135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with synchronous federated learning (FL), asynchronous FL (AFL) has attracted more and more attention in edge computing (EC) fields because of its strong adaptability to heterogeneous application scenarios. However, the non-independent and identically distributed (Non-IID) data across devices and the staleness-aware estimation of unreliable wireless connections and limited edge resources make it much more difficult to achieve better AFL-related applications. To handle this problem, we propose an Adaptive Staleness-aware Momentum Accelerated AFL (ASMAFL) algorithm to reduce the resources consumption of heterogeneous wireless communication EC (WCEC) scenarios, as well as decrease the negative impact of Non-IID data for model training. Specifically, we first introduce the staleness-aware parameter and a unified momentum gradient descent (GD) framework to reformulate AFL. Then, we establish global convergence properties of AFL, derive an upper bound on AFL convergence rate, and find that the bound is related to the staleness-aware parameter and Non-IIDness. Next, we formulate the bound into a minimization problem of resource consumption under given model accuracy, and the corresponding staleness-aware parameter of devices will be recomputed after each asynchronous aggregation to eliminate the differences of local models’ contribution to global model aggregation. Finally, extensive experiments are carried out to validate the superiority of ASMAFL in model accuracy, convergence rate, resources consumption, Non-IID issue, etc.},
  archive      = {J_TMC},
  author       = {Dewen Qiao and Songtao Guo and Jun Zhao and Junqing Le and Pengzhan Zhou and Mingyan Li and Xuetao Chen},
  doi          = {10.1109/TMC.2024.3510135},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3390-3406},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ASMAFL: Adaptive staleness-aware momentum asynchronous federated learning in edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of models and strategies for computation offloading in the internet of vehicles: Efficiency and trust. <em>TMC</em>, <em>24</em>(4), 3372-3389. (<a href='https://doi.org/10.1109/TMC.2024.3509542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet of Vehicles (IoV), vehicles will generate massive data and computation demands, necessitating computation offloading at the edge. However, existing research faces challenges in efficiency and trust. In this paper, we explore the IoV computation offloading from both user and edge facility provider perspectives, working to optimize the quality of experience (QoE), load balancing, and success rate based on challenges to efficiency and trust. First, two vehicle interconnection models are constructed to extend the linkable range of intra-road and inter-road vehicles while considering the maximum link time constraint. Then, a dynamic planning method is proposed, combining the reputation and feedback mechanisms, which can schedule edge resources online based on the cumulative computation latency of each service side, reliability value, and historical behavior. These two phases further improve the efficiency of edge services. Subsequently, blockchain is combined to optimize the trust problem of edge collaboration, and an edge-limited Byzantine fault tolerance local consensus mechanism is proposed to optimize consensus efficiency and ensure the reliability of edge services. Finally, this paper conducts dynamic experiments on real-world datasets, verifying the effectiveness of the proposed algorithm and models in multiple vehicle density datasets and experimental scenarios.},
  archive      = {J_TMC},
  author       = {Qinghang Gao and Jianmao Xiao and Zhiyong Feng and Jingyu Li and Yang Yu and Hongqi Chen and Qiaoyun Yin},
  doi          = {10.1109/TMC.2024.3509542},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3372-3389},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimization of models and strategies for computation offloading in the internet of vehicles: Efficiency and trust},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient hybrid transmission for cell-free systems via NOMA and multiuser diversity. <em>TMC</em>, <em>24</em>(4), 3359-3371. (<a href='https://doi.org/10.1109/TMC.2024.3514165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell-free technology is considered a pivotal advancement for next-generation mobile communications, which can effectively enhance the quality of service for user equipments (UEs) located at the cell edge. For cell-free systems, in this paper, we propose a hybrid downlink transmission method that combines non-orthogonal multiple access (NOMA) and multiuser diversity (MUD). To evaluate the communication performance of the system, we derive closed-form expressions for both instantaneous and average sum rates of UEs using the NOMA and MUD transmission methods. Furthermore, we comprehensively investigate the spectrum efficiency of the NOMA and MUD transmission methods to provide a basis for selecting the hybrid transmission strategy. On the basis of the proposed hybrid transmission strategy, we can derive an optimal hybrid transmission strategy for the scenarios with two access points (APs) and two UEs. Particularly, we extend the aforementioned strategy to the scenarios with multiple UEs, and formulate an optimization problem to maximize the system spectrum efficiency subject to the transmission strategy and power allocation. Furthermore, we propose a low-complexity user selection strategy and power allocation algorithm to solve the problem. Numerical results demonstrate that the hybrid transmission method and power allocation strategy can achieve higher system spectrum efficiency. Our results reveal the influence of key parameters on the downlink spectrum efficiency, analytically and numerically.},
  archive      = {J_TMC},
  author       = {Lin Bai and Jinpeng Xu and Jiaxing Wang and Rui Han and Jinho Choi},
  doi          = {10.1109/TMC.2024.3514165},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3359-3371},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient hybrid transmission for cell-free systems via NOMA and multiuser diversity},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge assisted low-latency cooperative BEV perception with progressive state estimation. <em>TMC</em>, <em>24</em>(4), 3346-3358. (<a href='https://doi.org/10.1109/TMC.2024.3509716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern intelligent vehicles (IVs) are equipped with a variety of sensors and communication modules, empowering Advanced Driver Assistance Systems (ADAS) and enabling inter-vehicle connectivity. This paper focuses on multi-vehicle cooperative perception, with a primary objective of achieving low latency. The task involves nearby cooperative vehicles sending their camera data to an edge server, which then merges the local views to create a global traffic view. While multi-camera perception has been actively researched, existing solutions often rely on deep learning models, resulting in excessive processing latency. In contrast, we propose leveraging the state estimation technique from the robotics field for this task. We explicitly model and solve for the system state, addressing additional challenges brought by object mobility and vision obstruction. Furthermore, we introduce a progressive state estimation pipeline to further accelerate system state notifications, supported by a motion prediction method that optimizes position accuracy and perception smoothness. Experimental results demonstrate the superiority of our approach over the deep learning method, with 12.0 × to 27.4 × reductions in server processing delay, while maintaining mean absolute errors below 1 m.},
  archive      = {J_TMC},
  author       = {Yuhan Lin and Haoran Xu and Zhimeng Yin and Guang Tan},
  doi          = {10.1109/TMC.2024.3509716},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3346-3358},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge assisted low-latency cooperative BEV perception with progressive state estimation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint encoding and enhancement for low-light video analytics in mobile edge networks. <em>TMC</em>, <em>24</em>(4), 3330-3345. (<a href='https://doi.org/10.1109/TMC.2024.3514214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present our design and analysis of a Joint Encoding and Enhancement (JEE) system for low-light video analytics in mobile edge networks. First, it is observed that, relying solely on a single pipeline for encoding and enhancement of mobile videos proves insufficient, because of the fluctuations in end-edge bandwidth and computing resources. Therefore, two distinct pipelines are introduced in the JEE system, namely, the encode-decode-enhance pipeline and the enhance-encode-decode pipeline. We then characterize the relationship of accuracy, transmission overhead, and computing overhead of these two pipelines through extensive experiments. Considering the significant demands of transmission and computing for low-light videos, we formulate an optimization problem to strike a balance between accuracy and delay, where the available end-edge bandwidth and computing resources are unknown in advance. To solve this mixed-integer nonlinear programming problem, we propose an algorithm based on online gradient descent, enabling adaptive pipeline selection and joint encoding and enhancement configuration. Theoretical analysis indicates that the proposed algorithm achieves sub-linear dynamic regret, highlighting its capability to the accuracy improvement and delay reduction in online environments. Experimental comparison against baselines demonstrates that, JEE can achieve up to a 27.32% increase in accuracy and a 26.18% reduction in delay.},
  archive      = {J_TMC},
  author       = {Yuanyi He and Peng Yang and Tian Qin and Jiawei Hou and Ning Zhang},
  doi          = {10.1109/TMC.2024.3514214},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3330-3345},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint encoding and enhancement for low-light video analytics in mobile edge networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Offloading game for mobile edge computing with random access in IoT. <em>TMC</em>, <em>24</em>(4), 3316-3329. (<a href='https://doi.org/10.1109/TMC.2024.3514204'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Internet of Things (IoT), numerous devices and sensors are deployed to collect data sets. Although some IoT devices can process data locally, most devices may have limited power and computational capability. Since mobile edge computing (MEC) is a new paradigm to provide strong computing capability at the edge of networks close to users, these devices can offload their tasks to MEC servers. Therefore, designing an efficient computation offloading strategy to decide whether the tasks to be offloaded to MEC servers becomes crucial. In this paper, we study the computation offloading for IoT devices based on a non-cooperative game with one-shot random access, where users’ offloading decisions can be made independently to realize distributed offloading. In particular, we discuss the offloading game with and without sharing information among devices and find the Nash equilibrium (NE). Besides, we analyze the effective bandwidth as a performance metric from a device perspective, which considering the Quality of Service (QoS) of network layer while analyzing users’ offloading strategies. Simulation results show the effectiveness of proposed strategies and the impact of offloading tasks to users’ strategies in time-varying channel based on effective bandwidth.},
  archive      = {J_TMC},
  author       = {Rui Han and Yue Yu and Qingzhe Zeng and Jiaxing Wang and Lin Bai and Jinho Choi and Wei Zhang},
  doi          = {10.1109/TMC.2024.3514204},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3316-3329},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Offloading game for mobile edge computing with random access in IoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedMTPP: Federated multivariate temporal point processes for distributed event sequence forecasting. <em>TMC</em>, <em>24</em>(4), 3302-3315. (<a href='https://doi.org/10.1109/TMC.2024.3509915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile network technology and wearable mobile devices, user-scenario interactions generate a large amount of user behavioral data in the form of multivariate event sequences. Due to data isolation, these multi-scenario events need to be jointly trained to achieve better prediction results. However, traditional federated learning methods face significant challenges when handling distributed event sequences. And the effectiveness of existing modeling approaches for event sequences in federated contexts has not been thoroughly explored. To this end, we propose Federated Multivariate Temporal Point Processes (FedMTPP), which enables learning from distributed event sequences within a novel federated learning framework and leverages efficient event modeling technology, MTPP, to forecast future events. Specifically, FedMTPP restores the temporal structure of the original event sequence by rearranging event embeddings and redesigns the autoregressive-based hidden representation computation in traditional MTPP, making it more suitable for federated prediction tasks. Additionally, FedMTPP incorporates advanced encryption techniques to effectively safeguard user privacy and security. Experimental results on both synthetic and real datasets demonstrate that FedMTPP substantially improves the performance of local models and achieves results comparable to state-of-the-art centralized MTPP methods.},
  archive      = {J_TMC},
  author       = {Houxin Gong and Haishuai Wang and Peng Zhang and Sheng Zhou and Hongyang Chen and Jiajun Bu},
  doi          = {10.1109/TMC.2024.3509915},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3302-3315},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedMTPP: Federated multivariate temporal point processes for distributed event sequence forecasting},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel insights from a cross-layer analysis of TCP and UDP traffic over full-duplex WLANs. <em>TMC</em>, <em>24</em>(4), 3288-3301. (<a href='https://doi.org/10.1109/TMC.2024.3510099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Full-duplex (FD) communication is a promising new technology that enables simultaneous transmission and reception in wireless local area networks (WLANs). The benefits of FD on the medium access control (MAC) layer throughput in IEEE 802.11 WLANs are well-documented. However, cross-layer interactions between the FD MAC protocol and transport layer protocols such as Transmission Control Protocol (TCP) and User Datagram Protocol (UDP) are less explored. We consider a WLAN with uplink and downlink TCP flows as well as UDP flows between stations (STAs) and a server via an FD access point (AP). We study an STA-initiated FD MAC protocol in which the AP can transmit on the downlink while receiving on the uplink. Using a novel FD-specific STA saturation approximation, Markov renewal theory, and fixed-point analysis, we derive novel expressions for the uplink and downlink TCP and UDP saturation throughputs. Our analysis shows that the AP is no longer a bottleneck and may be unsaturated unlike in conventional half-duplex (HD) WLANs. Despite greater contention and cross-link interference between STAs, FD achieves a higher TCP throughput than HD. FD causes a significant degradation in the UDP throughput. In the unsaturated regime, FD achieves a lower average downlink TCP packet delay than HD.},
  archive      = {J_TMC},
  author       = {Vinay U. Pai and Neelesh B. Mehta and Chandramani Singh},
  doi          = {10.1109/TMC.2024.3510099},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3288-3301},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Novel insights from a cross-layer analysis of TCP and UDP traffic over full-duplex WLANs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QoS-driven contextual MAB for MPQUIC supporting video streaming in mobile networks. <em>TMC</em>, <em>24</em>(4), 3274-3287. (<a href='https://doi.org/10.1109/TMC.2024.3507051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video streaming performance may degrade substantially in a mobile environment due to fast-changing wireless links. On the other hand, to provide ubiquitous services, heterogeneous static and mobile access and backbone networks will be integrated in the sixth-generation (6G) systems, so mobile users can take advantage of multiple access options for better services. Multi-path transport-layer protocols like Multi-Path QUIC (MPQUIC) show promise in utilizing multiple access links to address the impact of mobility. However, the optimal link selection that aims to provide statistical QoS guarantee for video streaming in a mobile environment with both user mobility and network mobility remains an open issue. In this paper, based on a lightweight Multi-Armed Bandit (MAB) technique, we develop a QoS-driven Contextual MAB (QC-MAB) framework for MPQUIC, which makes an intelligent access network selection and adaptively enables FEC coding to trade off delay, reliability and goodput. Extensive simulation results with ns-3 show that the proposed QC-MAB framework can outperform the state-of-the-art solutions. It achieves up to ten times lower video interruption ratio and three times higher goodput in highly dynamic mobile environments.},
  archive      = {J_TMC},
  author       = {Wenjun Yang and Lin Cai and Shengjie Shu and Amir Sepahi and Zhiming Huang and Jianping Pan},
  doi          = {10.1109/TMC.2024.3507051},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3274-3287},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {QoS-driven contextual MAB for MPQUIC supporting video streaming in mobile networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeLLM: Fast on-device LLM inference with speculative decoding. <em>TMC</em>, <em>24</em>(4), 3256-3273. (<a href='https://doi.org/10.1109/TMC.2024.3513457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative tasks, such as text generation and question answering, are essential for mobile applications. Given their inherent privacy sensitivity, executing them on devices is demanded. Nowadays, the execution of these generative tasks heavily relies on the Large Language Models (LLMs). However, the scarce device memory severely hinders the scalability of these models. We present EdgeLLM, an efficient on-device LLM inference system for models whose sizes exceed the device's memory capacity. EdgeLLM is built atop speculative decoding, which delegates most tokens to a smaller, memory-resident (draft) LLM. EdgeLLM integrates three novel techniques: (1) Instead of generating a fixed width and depth token tree, EdgeLLM proposes compute-efficient branch navigation and verification to pace the progress of different branches according to their accepted probability to prevent the wasteful allocation of computing resources to the wrong branch and to verify them all at once efficiently. (2) It uses a self-adaptive fallback strategy that promptly initiates the verification process when the smaller LLM generates an incorrect token. (3) To not block the generation, EdgeLLM proposes speculatively generating tokens during large LLM verification with the compute-IO pipeline. Through extensive experiments, EdgeLLM exhibits impressive token generation speed which is up to 9.3× faster than existing engines.},
  archive      = {J_TMC},
  author       = {Daliang Xu and Wangsong Yin and Hao Zhang and Xin Jin and Ying Zhang and Shiyun Wei and Mengwei Xu and Xuanzhe Liu},
  doi          = {10.1109/TMC.2024.3513457},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3256-3273},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EdgeLLM: Fast on-device LLM inference with speculative decoding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-enabled multiple sensitive task-offloading mechanism for MEC applications. <em>TMC</em>, <em>24</em>(4), 3241-3255. (<a href='https://doi.org/10.1109/TMC.2024.3507153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As mobile devices proliferate and mobile applications diversify, Mobile Edge Computing (MEC) has become widely adopted to efficiently allocate computing resources at the network edge and alleviate network congestion. In the MEC initial phase, the absence of vital information presents challenges in devising task-offloading policies, and identifying malicious devices responsible for providing inaccurate feedback is complex. To fill in such gaps, we introduce a consortium blockchain-enabled Committee Voting based Task Offloading Model (CVTOM) to collaboratively formulate resource allocation policies and establish deterrence against malicious servers producing erroneous results intentionally. Different voting principle mechanisms of each committee member are first designed in a Blockchain-enabled system which helps to represent the system's resource status. Additionally, we propose a Multi-armed Bandits related Thompson Sampling based Adaptive Preference Optimization (TSAPO) algorithm for task-offloading policy, enhancing the timely identification of potent edge servers to improve computing resource utilization which first considers dynamic edge server space and parallel computing scenarios. The solid proof process greatly contributes to the theoretical analysis of the TSAPO. The simulation experiments demonstrate the delay and budget can be reduced by around 25% and 10% respectively, showcasing the superior performance of our approach.},
  archive      = {J_TMC},
  author       = {Yang Xu and Hangfan Li and Cheng Zhang and Zhiqing Tang and Xiaoxiong Zhong and Ju Ren and Hongbo Jiang and Yaoxue Zhang},
  doi          = {10.1109/TMC.2024.3507153},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3241-3255},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-enabled multiple sensitive task-offloading mechanism for MEC applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PandORA: Automated design and comprehensive evaluation of deep reinforcement learning agents for open RAN. <em>TMC</em>, <em>24</em>(4), 3223-3240. (<a href='https://doi.org/10.1109/TMC.2024.3505781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The highly heterogeneous ecosystem of Next Generation (NextG) wireless communication systems calls for novel networking paradigms where functionalities and operations can be dynamically and optimally reconfigured in real time to adapt to changing traffic conditions and satisfy stringent and diverse Quality of Service (QoS) demands. Open Radio Access Network (RAN) technologies, and specifically those being standardized by the O-RAN Alliance, make it possible to integrate network intelligence into the once monolithic RAN via intelligent applications, namely, xApps and rApps. These applications enable flexible control of the network resources and functionalities, network management, and orchestration through data-driven intelligent control loops. Recent work has showed how Deep Reinforcement Learning (DRL) is effective in dynamically controlling O-RAN systems. However, how to design these solutions in a way that manages heterogeneous optimization goals and prevents unfair resource allocation is still an open challenge, with the logic within DRL agents often considered as a opaque system. In this paper, we introduce PandORA, a framework to automatically design and train DRL agents for Open RAN applications, package them as xApps and evaluate them in the Colosseum wireless network emulator. We benchmark 23 xApps that embed DRL agents trained using different architectures, reward design, action spaces, and decision-making timescales, and with the ability to hierarchically control different network parameters. We test these agents on the Colosseum testbed under diverse traffic and channel conditions, in static and mobile setups. Our experimental results indicate how suitable fine-tuning of the RAN control timers, as well as proper selection of reward designs and DRL architectures can boost network performance according to the network conditions and demand. Notably, finer decision-making granularities can improve Massive Machine-Type Communications (mMTC)’s performance by $\sim\! 56\%$ and even increase Enhanced Mobile Broadband (eMBB) Throughput by $\sim\! 99\%$.},
  archive      = {J_TMC},
  author       = {Maria Tsampazi and Salvatore D'Oro and Michele Polese and Leonardo Bonati and Gwenael Poitau and Michael Healy and Mohammad Alavirad and Tommaso Melodia},
  doi          = {10.1109/TMC.2024.3505781},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3223-3240},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PandORA: Automated design and comprehensive evaluation of deep reinforcement learning agents for open RAN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How to collaborate: Towards maximizing the generalization performance in cross-silo federated learning. <em>TMC</em>, <em>24</em>(4), 3211-3222. (<a href='https://doi.org/10.1109/TMC.2024.3509852'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) has attracted vivid attention as a privacy-preserving distributed learning framework. In this work, we focus on cross-silo FL, where clients become the model owners after training and are only concerned about the model's generalization performance on their local data. Due to the data heterogeneity issue, asking all the clients to join a single FL training process may result in model performance degradation. To investigate the effectiveness of collaboration, we first derive a generalization bound for each client when collaborating with others or when training independently. We show that the generalization performance of a client can be improved by collaborating with other clients that have more training data and similar data distributions. Our analysis allows us to formulate a client utility maximization problem by partitioning clients into multiple collaborating groups. A hierarchical clustering-based collaborative training (HCCT) scheme is then proposed, which does not need to fix in advance the number of groups. We further analyze the convergence of HCCT for general non-convex loss functions which unveils the effect of data similarity among clients. Extensive simulations show that HCCT achieves better generalization performance than baseline schemes, whereas it degenerates to independent training and conventional FL in specific scenarios.},
  archive      = {J_TMC},
  author       = {Yuchang Sun and Marios Kountouris and Jun Zhang},
  doi          = {10.1109/TMC.2024.3509852},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3211-3222},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {How to collaborate: Towards maximizing the generalization performance in cross-silo federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint trajectory planning and task offloading for MIMO AAV-aided mobile edge computing. <em>TMC</em>, <em>24</em>(4), 3196-3210. (<a href='https://doi.org/10.1109/TMC.2024.3510272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing is conducive to reducing service response time and improving service quality by pushing cloud functions to a network's edges. Most existing works in edge computing focus on utility maximization of task offloading on static edges with a single antenna. Besides, trajectory planning of mobile edges, e.g., autonomous aerial vehicles (AAVs) is also rarely discussed. In this paper, we are the first to jointly discuss the deadline-ware task offloading and AAV trajectory planning problem in a multi-input multi-output (MIMO) AAV-aided mobile edge computing system. Due to discrete variables and highly coupling nonconvex constraints, we equivalently convert the original problem into a more solvable form by introducing auxiliary variables. Next, a penalty dual decomposition-based algorithm is developed to achieve a global optimal solution to the problem. Besides, we proposed a profit-based fireworks algorithm in a relatively lower time to reduce the execution time for large-scale networks. Extensive evaluation results reveal that our proposed optimal algorithms could significantly outperform static offloading algorithms and other algorithms by 25% on average.},
  archive      = {J_TMC},
  author       = {Xuewen Dong and Shuangrui Zhao and Ximeng Liu and Zijie Di and Yuzhen Zhang and Yulong Shen},
  doi          = {10.1109/TMC.2024.3510272},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3196-3210},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint trajectory planning and task offloading for MIMO AAV-aided mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wireless eavesdropping on wired audio with radio-frequency retroreflector attack. <em>TMC</em>, <em>24</em>(4), 3178-3195. (<a href='https://doi.org/10.1109/TMC.2024.3505268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated the feasibility of eavesdropping on audio via radio frequency signals or videos, which capture physical surface vibrations from surrounding objects. However, these methods are inadequate for intercepting internally transmitted audio through wired media. In this work, we introduce radio-frequency retroreflector attack (RFRA) and bridge this gap by proposing an RFRA-based eavesdropping system, RF-Parrot${}^{\mathbf {2}}$, capable of wirelessly capturing audio signals transmitted through earphone wires. Our system entails embedding a tiny field-effect transistor within the wire to establish a battery-free retroreflector, whose reflective efficiency is correlated with the amplitude of the audio signal. To preserve the details of audio signals, we designed a unique retroreflector using a depletion-mode MOSFET (D-MOSFET). This MOSFET can be triggered by any voltage level present in the audio signals, thus guaranteeing no information loss during activation. However, the D-MOSFET introduces a nonlinear convolution operation on the original audio, resulting in distorted audio eavesdropping. Thus, we devised an engineering solution which utilized a novel convolutional neural network in conjunction with an efficient Parallel WaveGAN vocoder to reconstruct the original audio. Our comprehensive experiments demonstrate a strong similarity between the reconstructed audio and the original, achieving an impressive 95% accuracy in speech command recognition.},
  archive      = {J_TMC},
  author       = {Genglin Wang and Zheng Shi and Yanni Yang and Zhenlin An and Guoming Zhang and Pengfei Hu and Xiuzhen Cheng and Jiannong Cao},
  doi          = {10.1109/TMC.2024.3505268},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3178-3195},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Wireless eavesdropping on wired audio with radio-frequency retroreflector attack},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HearLoc: Locating unknown sound sources in 3D with a small-sized microphone array. <em>TMC</em>, <em>24</em>(4), 3163-3177. (<a href='https://doi.org/10.1109/TMC.2024.3507035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor Sound Source Localization (ISSL) is under growing focus with the rapid development of smart IOT intelligence. The predominant approaches typically involve constructing large microphone (Mic) array systems or extracting multiple angles of arrival (AOAs). However, the performance of these solutions is often constrained by the physical size of the array. Besides, there has been limited focus on 3D localization with a single small-sized Mic array. In this paper, we propose HearLoc, an ISSL system that can directly locate 3D sources with a ten-$cm$ Mic array. We demonstrate that the localization ability and dimensional capability can be significantly enhanced by incorporating the time differences of arrival (TDOAs) between the line-of-sight (LOS) and ECHO signals from nearby reflective surfaces. Our approach involves a localization method that selectively sums the correlation powers at useful TDOAs induced by each location. We also design a data processing pipeline with interpolation, normalization and pruning techniques to improve system accuracy and efficiency. To further enhance scalability, we design an iterative algorithm for the ISSL problem with multiple sources and an array location calibration scheme. Experiments demonstrate that the HearLoc can effectively locate sound sources, exhibiting $2\times$/$3.7\times$ improvements in accuracy for 2D and 3D localization, respectively, and a $4\times$ increase in efficiency compared to the existing AOA-based ISSL solutions.},
  archive      = {J_TMC},
  author       = {Zhaohui Li and Yongmin Zhang and Lin Cai and Yaoxue Zhang},
  doi          = {10.1109/TMC.2024.3507035},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3163-3177},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HearLoc: Locating unknown sound sources in 3D with a small-sized microphone array},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CL-shield: A continuous learning system for protecting user privacy. <em>TMC</em>, <em>24</em>(4), 3148-3162. (<a href='https://doi.org/10.1109/TMC.2024.3504721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The video analytics system utilizes deep learning models (DNN) to perform inference on the videos captured by cameras. Continuous learning algorithms are used to address the data drift problem in video analytics systems. However, uploading images from deployment environments and processing on the cloud carry the risk of privacy leakage. In this paper, we have designed a system called CL-Shield to protect user’s privacy. First, we review the causes of privacy leakage in a continuous learning system and propose the objective of full privacy protection. Second, we design an online training mechanism based on a scene library to avoid direct uploading of user’s frames to the cloud server. Lastly, we design a fast training set search algorithm based on a novel Ebv-List, which effectively improves the speed of model updates. We collect various real-world scenario data to build our scene library and validate our system on a dataset of over 10 hours. The experiments demonstrate that our privacy-aware continuous learning system achieves an F1-score of over 92% compared to the conventional systems without protecting privacy and has long-term stability in analytic F1-score.},
  archive      = {J_TMC},
  author       = {Tianyu Li and Hanling Wang and Qing Li and Yong Jiang and Zhenhui Yuan},
  doi          = {10.1109/TMC.2024.3504721},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3148-3162},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CL-shield: A continuous learning system for protecting user privacy},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TJCCT: A two-timescale approach for UAV-assisted mobile edge computing. <em>TMC</em>, <em>24</em>(4), 3130-3147. (<a href='https://doi.org/10.1109/TMC.2024.3505155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV)-assisted mobile edge computing (MEC) is emerging as a promising paradigm to provide aerial-terrestrial computing services in close proximity to mobile devices (MDs). However, meeting the demands of computation-intensive and delay-sensitive tasks for MDs poses several challenges, including the demand-supply contradiction between MDs and MEC servers, the demand-supply discrepancy between MDs and MEC servers, the trajectory control requirements on energy efficiency and timeliness, and the different time-scale dynamics of the network. To address these issues, we first present a hierarchical architecture by incorporating terrestrial-aerial computing capabilities and leveraging UAV flexibility. Furthermore, we formulate a joint computing resource allocation, computation offloading, and trajectory control problem to maximize the system utility. Since the problem is a non-convex and NP-hard mixed integer nonlinear programming (MINLP), we propose a two-timescale joint computing resource allocation, computation offloading, and trajectory control (TJCCT) approach for solving the problem. In the short timescale, we propose a price-incentive model for on-demand computing resource allocation and a matching mechanism-based method for computation offloading. In the long timescale, we propose a convex optimization-based method for UAV trajectory control. Besides, we theoretically prove the stability and polynomial complexity of TJCCT. Extensive simulation results demonstrate that the proposed TJCCT is able to achieve superior performances in terms of the system utility, average processing rate, average completion delay, average completion ratio, and average cost, while meeting the energy constraints despite the trade-off of the increased energy consumption.},
  archive      = {J_TMC},
  author       = {Zemin Sun and Geng Sun and Qingqing Wu and Long He and Shuang Liang and Hongyang Pan and Dusit Niyato and Chau Yuen and Victor C. M. Leung},
  doi          = {10.1109/TMC.2024.3505155},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3130-3147},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TJCCT: A two-timescale approach for UAV-assisted mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSTAR-FL: Stochastic client selection for tree all-reduce federated learning. <em>TMC</em>, <em>24</em>(4), 3110-3129. (<a href='https://doi.org/10.1109/TMC.2024.3507381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is widely applied in privacy-sensitive domains, such as healthcare, finance, and education, due to its privacy-preserving properties. However, implementing FL in dynamic wireless networks poses substantial communication challenges. Central to these challenges is the need for efficient communication strategies that can adapt to fluctuating network conditions and the growing number of participating devices, which can lead to unacceptable communication delays. In this article, we propose Stochastic Client Selection for Tree All-Reduce Federated Learning (CSTAR-FL), a novel approach that combines a probabilistic User Device (UD) selection strategy with a tree-based communication architecture to enhance communication efficiency in FL within densely populated wireless networks. By optimizing UD selection for effective model aggregation and employing an efficient data transmission structure, CSTAR-FL significantly reduces communication time and improves FL efficiency. Additionally, our approach ensures high global model accuracy under scenarios where data distribution is heterogeneous from User Device (UD)s. Extensive simulations in dynamic wireless network scenarios demonstrate that CSTAR-FL outperforms existing state-of-the-art methods, reducing model convergence time by up to 40% without losing the global model accuracy. This makes CSTAR-FL a robust solution for efficient and scalable FL deployments in high-density environments.},
  archive      = {J_TMC},
  author       = {Zimu Xu and Antonio Di Maio and Eric Samikwa and Torsten Braun},
  doi          = {10.1109/TMC.2024.3507381},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3110-3129},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CSTAR-FL: Stochastic client selection for tree all-reduce federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User association and channel allocation in 5G mobile asymmetric multi-band heterogeneous networks. <em>TMC</em>, <em>24</em>(4), 3092-3109. (<a href='https://doi.org/10.1109/TMC.2024.3503632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of mobile terminals, the continuous upgrading of services, 4G LTE networks are showing signs of weakness. To enhance the capacity of wireless networks, millimeter waves are introduced to drive the evolution of networks towards multi-band 5G heterogeneous networks. The distinct propagation characteristics of mmWaves, microwaves, as well as the vastly different hardware configurations of heterogeneous base stations, make traditional access strategies no longer effective. Therefore, to narrowing the gap between theory, practice, we investigate the access strategy in multi-band 5G heterogeneous networks, taking into account the characteristics of mobile users, asynchronous switching between uplink, downlink of pico base stations, asymmetric service requirements, user communication continuity. We formulate the problem as integer nonlinear programming, prove its intractability. Thereby, we decouple it into three subproblems: user association, switch point selection, subchannel allocation, design an algorithm based on optimal matching, spectral clustering to solve it efficiently. The simulation results show that the proposed algorithm outperforms the comparison methods in terms of overall data rate, effective data rate, number of satisfied users.},
  archive      = {J_TMC},
  author       = {Miao Dai and Gang Sun and Hongfang Yu and Sheng Wang and Dusit Niyato},
  doi          = {10.1109/TMC.2024.3503632},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3092-3109},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {User association and channel allocation in 5G mobile asymmetric multi-band heterogeneous networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparsified random partial model update for personalized federated learning. <em>TMC</em>, <em>24</em>(4), 3076-3091. (<a href='https://doi.org/10.1109/TMC.2024.3507286'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) stands as a privacy-preserving machine learning paradigm that enables collaborative training of a global model across multiple clients. However, the practical implementation of FL models often confronts challenges arising from data heterogeneity and limited communication resources. To address the aforementioned issues simultaneously, we develop a Sparsified Random Partial Update framework for personalized Federated Learning (SRP-pFed), which builds upon the foundation of dynamic partial model updates. Specifically, we decouple the local model into personal and shared parts to achieve personalization. For each client, the ratio of its personal part associated with the local model, referred to as the update rate, is regularly renewed over the training procedure via a random walk process endowed with reinforced memory. In each global iteration, clients are clustered into different groups where the ones in the same group share a common update rate. Benefiting from such design, SRP-pFed realizes model personalization while substantially reducing communication costs in the uplink transmissions. We conduct extensive experiments on various training tasks with diverse heterogeneous data settings. The results demonstrate that the SRP-pFed consistently outperforms the state-of-the-art methods in test accuracy and communication efficiency.},
  archive      = {J_TMC},
  author       = {Xinyi Hu and Zihan Chen and Chenyuan Feng and Geyong Min and Tony Q. S. Quek and Howard H. Yang},
  doi          = {10.1109/TMC.2024.3507286},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3076-3091},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sparsified random partial model update for personalized federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentive mechanism design for cross-device federated learning: A reinforcement auction approach. <em>TMC</em>, <em>24</em>(4), 3059-3075. (<a href='https://doi.org/10.1109/TMC.2024.3508260'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the operational context of a cross-device federated learning (FL), the efficient allocation of resources, such as transmission powers, channels, and computation resources, significantly impacts overall performance. Existing research in cross-device FL has predominantly concentrated on either resource allocation to enhance training accuracy or incentivizing participation, while ignoring their integrated designs for further improving the performance in cross-device FL. Different from existing work, in this paper, we jointly integrate the power allocation, channel assignment, user selection, and allocation of computation frequency into the design of incentive mechanism, where each mobile user plays a dual role as both a buyer and a seller. Because of complex resource allocation, truthfulness guarantee in a dual role scenario, and unavailable prior information, the considered mechanism design problem is challenging. To tackle such combinatorial problem, we propose a Reinforcement Auction Mechanism (RAM), comprising two layers. The upper layer features a Hybrid Action Reinforcement Learning scheme to learn the outcomes of user selection and payments. In the lower layer, each selected mobile user optimizes its resources to maximize its utility. Theoretical analyses affirm that our proposed RAM ensures individual rationality and truthfulness. Extensive simulations have been conducted to validate the effectiveness of the proposed RAM.},
  archive      = {J_TMC},
  author       = {Gang Li and Jun Cai and Jianfeng Lu and Hongming Chen},
  doi          = {10.1109/TMC.2024.3508260},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3059-3075},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Incentive mechanism design for cross-device federated learning: A reinforcement auction approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective aerial collaborative secure communication optimization via generative diffusion model-enabled deep reinforcement learning. <em>TMC</em>, <em>24</em>(4), 3041-3058. (<a href='https://doi.org/10.1109/TMC.2024.3502685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to flexibility and low-cost, unmanned aerial vehicles (UAVs) are increasingly crucial for enhancing coverage and functionality of wireless networks. However, incorporating UAVs into next-generation wireless communication systems poses significant challenges, particularly in sustaining high-rate and long-range secure communications against eavesdropping attacks. In this work, we consider a UAV swarm-enabled secure surveillance network system, where a UAV swarm forms a virtual antenna array to transmit sensitive surveillance data to a remote base station (RBS) via collaborative beamforming (CB) so as to resist mobile eavesdroppers. Specifically, we formulate an aerial secure communication and energy efficiency multi-objective optimization problem (ASCEE-MOP) to maximize the secrecy rate of the system and to minimize the flight energy consumption of the UAV swarm. To address the non-convex, NP-hard and dynamic ASCEE-MOP, we propose a generative diffusion model-enabled twin delayed deep deterministic policy gradient (GDMTD3) method. Specifically, GDMTD3 leverages an innovative application of diffusion models to determine optimal excitation current weights and position decisions of UAVs. The diffusion models can better capture the complex dynamics and the trade-off of the ASCEE-MOP, thereby yielding promising solutions. Simulation results highlight the superior performance of the proposed approach compared with traditional deployment strategies and some other deep reinforcement learning (DRL) benchmarks. Moreover, performance analysis under various parameter settings of GDMTD3 and different numbers of UAVs verifies the robustness of the proposed approach.},
  archive      = {J_TMC},
  author       = {Chuang Zhang and Geng Sun and Jiahui Li and Qingqing Wu and Jiacheng Wang and Dusit Niyato and Yuanwei Liu},
  doi          = {10.1109/TMC.2024.3502685},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3041-3058},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-objective aerial collaborative secure communication optimization via generative diffusion model-enabled deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A coverage-aware high-quality sensing data collection method in mobile crowd sensing. <em>TMC</em>, <em>24</em>(4), 3025-3040. (<a href='https://doi.org/10.1109/TMC.2024.3502158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we leverage unmanned aerial vehicles (UAVs) to enhance mobile crowd sensing (MCS) by addressing two critical challenges: uncontrollable data quality and inevitable unsensed points of interest (PoIs). We introduce a UAV-assisted method to deal with these challenges. To ensure the accuracy of sensing data contributed by human participants, the proposed truth discovery method utilizes UAV-collected sensing data as few-shot samples to train the truth discovery model, which is then employed to calibrate sensing data solely collected by human participants. Additionally, to meet the sensing coverage requirement, we present a method that predicts data values for unsensed PoIs by utilizing their historical sensing data and the sensed neighboring PoIs information. The method employs a graph neural network to capture spatio-temporal relationships of the sensing data, facilitating accurate estimation of unsensed PoIs. Through extensive simulations, our approaches demonstrate superior performance compared to existing methods, showcasing the potential of UAV-assisted MCS for overcoming challenges and enhancing data collection efficiency in various domains.},
  archive      = {J_TMC},
  author       = {Ye Wang and Hui Gao and Edith C. H. Ngai and Kun Niu and Tan Yang and Bo Zhang and Wendong Wang},
  doi          = {10.1109/TMC.2024.3502158},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3025-3040},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A coverage-aware high-quality sensing data collection method in mobile crowd sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed learn-to-optimize: Limited communications optimization over networks via deep unfolded distributed ADMM. <em>TMC</em>, <em>24</em>(4), 3012-3024. (<a href='https://doi.org/10.1109/TMC.2024.3502574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed optimization is a fundamental framework for collaborative inference over networks. The operation is modeled as the joint minimization of a shared objective which typically depends on local observations. Distributed optimization algorithms, such as the distributed alternating direction method of multipliers (D-ADMM), iteratively combine local computations and message exchanges. A main challenge associated with distributed optimization, and particularly with D-ADMM, is that it requires a large number of communications to reach consensus. In this work we propose unfolded D-ADMM, which follows the emerging deep unfolding methodology to enable D-ADMM to operate reliably with a predefined and small number of messages exchanged by each agent. Unfolded D-ADMM fully preserves the operation of D-ADMM, while leveraging data to tune the hyperparameters of each iteration. These hyperparameters can either be agent-specific, aiming at achieving the best performance within a fixed number of iterations over a given network, or shared among the agents, allowing to learn to distributedly optimize over different networks. We specialize unfolded D-ADMM for two representative settings: a distributed sparse recovery setup, and a distributed machine learning learning scenario. Our numerical results demonstrate that the proposed approach dramatically reduces the number of communications utilized by D-ADMM, without compromising on its performance.},
  archive      = {J_TMC},
  author       = {Yoav Noah and Nir Shlezinger},
  doi          = {10.1109/TMC.2024.3502574},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {3012-3024},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed learn-to-optimize: Limited communications optimization over networks via deep unfolded distributed ADMM},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart shield: Prevent aerial eavesdropping via cooperative intelligent jamming based on multi-agent reinforcement learning. <em>TMC</em>, <em>24</em>(4), 2995-3011. (<a href='https://doi.org/10.1109/TMC.2024.3505206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spotlight on autonomous aerial vehicles (AAVs) is to enhance wireless communications while ignoring the potential risk of AAVs acting as adversaries. Due to their mobility and flexibility, AAV eavesdroppers pose an immeasurable threat to legitimate wireless transmissions. However, the existing fixed jamming scheme without cooperation cannot counter the flexible and dynamic AAV eavesdropping. In this article, a cooperative intelligent jamming scheme is proposed, authorizing ground jammers (GJs) to interfere with AAV eavesdroppers, generating specific jamming shields between AAV eavesdroppers and legitimate users. Toward this end, we formulate a secrecy capacity maximization problem and model the problem as a decentralized partially observable Markov decision process (Dec-POMDP). To address the challenge of the huge state space and action space with network dynamics, we leverage a deep reinforcement learning (DRL) algorithm with a dueling network and double-Q learning (i.e., dueling double deep Q-network) to train policy networks. Then, we propose a multi-agent mixing network framework (QMIX)-based collaborative jamming algorithm to enable GJs to independently make decisions without sharing local information. Additionally, we perform extensive simulations to validate the superiority of our proposed scheme and present useful insights into practical implementation by elucidating the relationship between the deployment settings of GJs and the instantaneous secrecy capacity.},
  archive      = {J_TMC},
  author       = {Qubeijian Wang and Shiyue Tang and Wen Sun and Yin Zhang and Geng Sun and Hong-Ning Dai and Mohsen Guizani},
  doi          = {10.1109/TMC.2024.3505206},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2995-3011},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Smart shield: Prevent aerial eavesdropping via cooperative intelligent jamming based on multi-agent reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobility-aware dependent task offloading in edge computing: A digital twin-assisted reinforcement learning approach. <em>TMC</em>, <em>24</em>(4), 2979-2994. (<a href='https://doi.org/10.1109/TMC.2024.3506221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute tasks from end devices. Task offloading is a fundamental problem in CEC that decides when and where tasks are executed upon the arrival of tasks. However, the mobility of users often results in unstable connections, leading to network failures and resource underutilization. Existing works have not adequately addressed joint mobility-aware dependent task offloading and network flow scheduling, resulting in network congestion and suboptimal performance. To address this, we formulate an online joint mobility-aware dependent task offloading and bandwidth allocation problem, to improve the quality of service by reducing task completion time and energy consumption. We introduce a Mobility-aware Digital Twin-assisted Deep Reinforcement Learning (MDT-DRL) algorithm. Our digital twin model equips the reinforcement learning process by providing future states of mobile users, enabling efficient offloading plans for adapting to the mobile CEC system. Experimental results on real-world and synthetic datasets show that MDT-DRL surpasses state-of-the-art baselines on average task completion time and energy consumption.},
  archive      = {J_TMC},
  author       = {Xiangchun Chen and Jiannong Cao and Yuvraj Sahni and Mingjin Zhang and Zhixuan Liang and Lei Yang},
  doi          = {10.1109/TMC.2024.3506221},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2979-2994},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobility-aware dependent task offloading in edge computing: A digital twin-assisted reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating update conflict in non-IID federated learning via orthogonal class gradients. <em>TMC</em>, <em>24</em>(4), 2967-2978. (<a href='https://doi.org/10.1109/TMC.2024.3503682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasingly popular federated learning still faces the practical challenge of non-independent and identically distributed data. Most efforts to address this issue focus on limiting local updates or enhancing model aggregation. However, these methods either restrict the learning capacity of local models or overlook the negative knowledge transfer caused by local objective divergences. In contrast, we observe that the global update can be re-expressed as a weighted sum of the gradients of samples from different classes. Therefore, we hypothesize that the competition among local updates may arise from the conflict between the gradients of samples belonging to different classes. Inspired by this insight, we introduce the novel perspective of orthogonal class gradients, aimed at eliminating interference between updates from different classes without the aforementioned drawbacks. To this end, this paper presents FedOCF, which implements orthogonal class gradient constraints by encouraging orthogonality among features of different classes. Specifically, FedOCF maintains a generator to learn features that are orthogonal for different classes and utilizes it to regularize features learned in local learning. Theoretically, we also demonstrate that FedOCF can improve generalization performance through feature conditional distribution alignment during local learning. Extensive experiments validate the excellent performance of FedOCF in various heterogeneous scenarios.},
  archive      = {J_TMC},
  author       = {Siyang Guo and Yaming Guo and Hui Zhang and Junbo Wang},
  doi          = {10.1109/TMC.2024.3503682},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2967-2978},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mitigating update conflict in non-IID federated learning via orthogonal class gradients},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TapWristband: A wearable keypad system based on wrist vibration sensing. <em>TMC</em>, <em>24</em>(4), 2949-2966. (<a href='https://doi.org/10.1109/TMC.2024.3503417'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained human motion detection has become increasingly important with the growing popularity of human computer interaction (HCI). However, traditional gesture-based HCI systems often require the design of new operation modes rather than conforming to user habits, thus increasing system learning costs. In this paper, we present TapWristband, a novel wearable sensor-based vibration sensing system that detects finger tapping by measuring wrist vibrations. We first perform real-world experiments to collect measurements for modeling the effects of the tapping motion on wearable wristband sensors including piezoelectric transducer (PZT) and inertial measurement unit (IMU). We find that a damped vibration model can be used to represent the relaxing phase of a vibration response due to tapping motion. Thus, we propose a mutual cross-correlation-based event segmentation algorithm to extract the vibration signal during the relaxing phase. After that, we develop feature extraction and classification algorithms to recognize the tapping patterns of five fingers across twelve key locations of a keypad system. Finally, we performed extensive experiments with thirteen participants to evaluate our system. Experimental results show that our low-cost vibration sensing system can achieve an average accuracy of over 93% with a tapping speed of over 100 taps per minute in real-world tapping scenarios.},
  archive      = {J_TMC},
  author       = {Jialiang Yan and Siyao Cheng and Yang Zhao and Jie Liu},
  doi          = {10.1109/TMC.2024.3503417},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2949-2966},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TapWristband: A wearable keypad system based on wrist vibration sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high-reliability small-area task offloading mechanism with trust evaluation and fuzzy logic in power IoTs. <em>TMC</em>, <em>24</em>(4), 2935-2948. (<a href='https://doi.org/10.1109/TMC.2024.3502167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to solve the problem that high-priority tasks can not be processed timely and reliably due to the disorder of multi-task and dynamicity in Power Internet of Things(PIoTs), a high-reliability small-area task offloading mechanism with trust evaluation and fuzzy logic(HRSATF) is proposed. First, considering task priority, preemptive priority queue is introduced to ensure high-priority tasks processed preferentially, and minimum resource allocation coefficients(MRACs) of tasks are solved to ensure the effectiveness of offloading. Second, the trust model between smart device(SD) and edge server(ES) is established, and ESs are divided into three priorities based on trust value and computing power by fast non-dominated sorting. Thirdly, fuzzy logic is applied to select target ES when the priorities of task and ES do not match or the ES is offline, and MRAC is used to schedule tasks between SD and ES. Finally, NSGA2 is modified (MNSGA2) to verify the effectiveness of HRSATF in terms of success rate, time, power consumption and load balancing, where success rate is increased by $102.3\%$, and time and power consumption are decreased by $90.7\%$, $89.3\%$ at most, respectively.},
  archive      = {J_TMC},
  author       = {Suhong Wang and Tuanfa Qin and Tingting Chen and Wenhao Guo and Yongle Hu and Hongmin Sun},
  doi          = {10.1109/TMC.2024.3502167},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2935-2948},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A high-reliability small-area task offloading mechanism with trust evaluation and fuzzy logic in power IoTs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). G3R: Generating rich and fine-grained mmWave radar data from 2D videos for generalized gesture recognition. <em>TMC</em>, <em>24</em>(4), 2917-2934. (<a href='https://doi.org/10.1109/TMC.2024.3502668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter wave radar is gaining traction recently as a promising modality for enabling pervasive and privacy-preserving gesture recognition. However, the lack of rich and fine-grained radar datasets hinders progress in developing generalized deep learning models for gesture recognition across various user postures (e.g., standing, sitting), positions, and scenes. To remedy this, we resort to designing a software pipeline that exploits wealthy 2D videos to generate realistic radar data, but it needs to address the challenge of simulating diversified and fine-grained reflection properties of user gestures. To this end, we design G3R with three key components: i) a gesture reflection point generator expands the arm's skeleton points to form human reflection points; ii) a signal simulation model simulates the multipath reflection and attenuation of radar signals to output the human intensity map; iii) an encoder-decoder model combines a sampling module and a fitting module to address the differences in number and distribution of points between generated and real-world radar data for generating realistic radar data. We implement and evaluate G3R using 2D videos from public data sources and self-collected real-world radar data, demonstrating its superiority over other state-of-the-art approaches for gesture recognition.},
  archive      = {J_TMC},
  author       = {Kaikai Deng and Dong Zhao and Wenxin Zheng and Yue Ling and Kangwen Yin and Huadong Ma},
  doi          = {10.1109/TMC.2024.3502668},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2917-2934},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {G3R: Generating rich and fine-grained mmWave radar data from 2D videos for generalized gesture recognition},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visualizing the smart environment in AR: An approach based on visual geometry matching. <em>TMC</em>, <em>24</em>(4), 2900-2916. (<a href='https://doi.org/10.1109/TMC.2024.3504960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents Insight, an AR system for visualizing the IoT-enabled smart environment without relying on the unique appearances, barcodes, world coordinates, or wireless signals of IoT infrastructures. The system analyzes the camera video and motion data taken by mobile AR equipment to extract the self and cross visual geometries describing the poses and geographic distribution of nearby IoT devices. To recognize IoT devices using the extracted geometries, Insight operates in two phases. At deployment time, it learns pairwise mappings from the visual geometries to the corresponding device identities. After that, it leverages the geometries scanned at run time to look for a partial assignment to the recorded geometries, allowing it to automatically recognize the IoT devices in AR view. As such, our system turns the IoT device recognition task into a geometry matching problem, which is further formalized as to perform Subset, Incomplete, and Duplicated Point Cloud Registration (SID-PCR) in this work. We design a deep neural network paying specific edge- and spectral-wise graph attention to solve SID-PCR, and implement a prototype that adaptively requests visual geometry scan and registration operations for accurate recognition. The performance of Insight is validated using both synthetic data and a real-world testbed.},
  archive      = {J_TMC},
  author       = {Ming Xia and Min Huang and Qiuqi Pan and Yunhan Wang and Xiaoyan Wang and Kaikai Chi},
  doi          = {10.1109/TMC.2024.3504960},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2900-2916},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Visualizing the smart environment in AR: An approach based on visual geometry matching},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). To optimize human-in-the-loop learning in repeated routing games. <em>TMC</em>, <em>24</em>(4), 2889-2899. (<a href='https://doi.org/10.1109/TMC.2024.3502076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today navigation applications (e.g., Waze and Google Maps) enable human users to learn and share the latest traffic observations, yet such information sharing simply aids selfish users to predict and choose the shortest paths to jam each other. Prior routing game studies focus on myopic users in oversimplified one-shot scenarios to regulate selfish routing via information hiding or pricing mechanisms. For practical human-in-the-loop learning (HILL) in repeated routing games, we face non-myopic users of differential past observations and need new mechanisms (preferably non-monetary) to persuade users to adhere to the optimal path recommendations. We model the repeated routing game in a typical parallel transportation network, which generally contains one deterministic path and $N$ stochastic paths. We first prove that no matter under the information sharing mechanism in use or the latest routing literature’s hiding mechanism, the resultant price of anarchy (PoA) for measuring the efficiency loss from social optimum can approach infinity, telling arbitrarily poor exploration-exploitation tradeoff over time. Then we propose a novel user-differential probabilistic recommendation (UPR) mechanism to differentiate and randomize path recommendations for users with differential learning histories. We prove that our UPR mechanism ensures interim individual rationality for all users and significantly reduces $\text{PoA}=\infty$ to close-to-optimal $\text{PoA}=1+\frac{1}{4N+3}$, which cannot be further reduced by any other non-monetary mechanism. In addition to theoretical analysis, we conduct extensive experiments using real-world datasets to generalize our routing graphs and validate the close-to-optimal performance of UPR mechanism.},
  archive      = {J_TMC},
  author       = {Hongbo Li and Lingjie Duan},
  doi          = {10.1109/TMC.2024.3502076},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2889-2899},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {To optimize human-in-the-loop learning in repeated routing games},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiSDA: Subdomain adaptation human activity recognition method using wi-fi signals. <em>TMC</em>, <em>24</em>(4), 2876-2888. (<a href='https://doi.org/10.1109/TMC.2024.3501299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition based on Wi-Fi signals has become one part of integrated sensing and communications, which has promising application prospects. Detecting activities across different domains is an important and challenging problem. To reduce model complexity and improve recognition accuracy, we propose a novel approach to realize activity recognition across domains, named WiSDA. The proposed WiSDA contains two parts: data augmentation and a deep learning model. The recursive plots method is employed as the data augmentation to transform Wi-Fi channel state information into images, which can take advantage of the image recognition ability of the latter deep learning model. The proposed learning model utilizes weighted cosine similarity to align feature distributions among sub-domains activated by a deep network layer across different domains, thereby a domain-independent feature representation is generated. Based on this representation, WiSDA can make the recognition decision independent of domains, then the cross-domain recognition accuracy is increased. The numerical results illustrate that WiSDA achieves higher recognition accuracy and has lower complexity. The cross-domain recognition accuracy ranges from 89% to 93% with offline pre-training. Enhancing the pre-trained WiSDA with limited samples boosts cross-domain recognition accuracy to 97%.},
  archive      = {J_TMC},
  author       = {Wanguo Jiao and Changsheng Zhang and Wei Du and Shuai Ma},
  doi          = {10.1109/TMC.2024.3501299},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2876-2888},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WiSDA: Subdomain adaptation human activity recognition method using wi-fi signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative cloud-edge approach for robust edge workload forecasting. <em>TMC</em>, <em>24</em>(4), 2861-2875. (<a href='https://doi.org/10.1109/TMC.2024.3502683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of edge computing in the post-COVID19 pandemic period, precise workload forecasting is considered the basis for making full use of the edge-limited resources, and both edge service providers (ESPs) and edge service consumers (ESCs) can benefit significantly from it. Existing paradigms of workload forecasting (i.e., edge-only or cloud-only) are improper, due to failing to consider the inter-site correlations and might suffer from significant data transmission delays. With the increasing adoption of edge platforms by web services, it is critical to balance both accuracy and efficiency in workload forecasting. In this paper, we propose XELASTIC, which offers three key improvements over the conference version. First, we redesigned the aggregation and disaggregation layers using GCNs to capture more complex relationships among workload series. Second, we introduced a supervised contrastive loss to enhance robustness against outliers, particularly for handling missing or abnormal data in real-world scenarios. Finally, we expanded the evaluation with additional baselines and larger datasets. Extensive experiments on realistic edge workload datasets collected from China’s largest edge service provider (Alibaba ENS) show that XELASTIC outperforms state-of-the-art methods, decreases time consumption, and reduces communication costs.},
  archive      = {J_TMC},
  author       = {Yanan Li and Penghong Zhao and Xiao Ma and Haitao Yuan and Zhe Fu and Mengwei Xu and Shangguang Wang},
  doi          = {10.1109/TMC.2024.3502683},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2861-2875},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A collaborative cloud-edge approach for robust edge workload forecasting},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preference-aware vehicle repositioning recommendation for MoD systems: A coulomb force directed perspective. <em>TMC</em>, <em>24</em>(4), 2847-2860. (<a href='https://doi.org/10.1109/TMC.2024.3502235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle repositioning is widely used in Mobility on-Demand (MoD) systems to address supply-demand imbalances and improve order completion rates. Existing methods typically offer repositioning recommendations focused on enhancing vehicle coordination toward supply-demand re-balance. However, these methods often overlook the possibility that drivers may not follow these recommendations due to their personal preferences, leading to recommendation-decision inconsistency and further disrupting the supply-demand balance. To address this issue, we propose a preference-aware vehicle repositioning recommendation strategy for MoD systems, named FREE, which is based on a Coulomb Force directed approach. The core idea is to strike a balance between vehicle coordination and consistency between recommendations and driver decisions. First, we introduce a Coulomb force-based representation (CFR) to model coordination among vehicles. In this model, the interactions between vehicles and orders are represented as forces that drive the repositioning of vehicles. Next, we develop a driver preference learning model that accurately captures drivers’ preferences using triplet and consistency loss. We then integrate these preferences with the CFR into a multi-agent deep reinforcement learning (MADRL) based repositioning algorithm to generate optimal recommendations. Finally, we validate the effectiveness of FREE through simulations using real-world data, demonstrating its superiority over existing benchmarks.},
  archive      = {J_TMC},
  author       = {Xiaobo Zhou and Shuxin Ge and Tie Qiu and Xingwei Wang},
  doi          = {10.1109/TMC.2024.3502235},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2847-2860},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Preference-aware vehicle repositioning recommendation for MoD systems: A coulomb force directed perspective},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards effective transportation mode-aware trajectory recovery: Heterogeneity, personalization and efficiency. <em>TMC</em>, <em>24</em>(4), 2832-2846. (<a href='https://doi.org/10.1109/TMC.2024.3501280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on the transportation-aware trajectory recovery problem, which is distinct from the conventional vehicle-based trajectory recovery, facing three major challenges: heterogeneity, personalization and efficiency. For the heterogeneity, the velocity of the mobile object is intrinsically correlated with the specific transportation mode, containing inherent heterogeneity. For the personalization, the trajectory data is complicated by substantial variations in users, which are different in personalized behaviors. For the efficiency, previous works mostly employ sequence-to-sequence framework which limits their efficiency due to the auto-regressive inference pattern. To address these challenges, we design a novel efficient and effective multi-modal deep model, coined as PTrajRec, for transportation-aware trajectory recovery. Specifically, we initially embed location, behavior, and transportation mode modalities in distinct channels, which not only reflect spatio-temporal information encapsulated in location sequences but also introduce the heterogeneity and personalization characteristics associated with mode and behavior sequences. For further modeling these modalities, we employ the auto-correlation mechanism to learn periodic dependencies on the temporal dimension and the graph attention mechanism to learn road network dependencies on the spatial dimension. At last, we propose a dual-view constraint mechanism to assist the efficient trajectory recovery framework and design three auxiliary tasks to address the inherent heterogeneity and efficiency design. Extensive experimental results on two real-world datasets demonstrate the superiority of our proposed method compared to state-of-the-art baselines with reduced computation cost and excellent performance.},
  archive      = {J_TMC},
  author       = {Chenxing Wang and Fang Zhao and Haiyong Luo and Yuchen Fang and Haichao Zhang and Haoyu Xiong},
  doi          = {10.1109/TMC.2024.3501280},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2832-2846},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards effective transportation mode-aware trajectory recovery: Heterogeneity, personalization and efficiency},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HT-FL: Hybrid training federated learning for heterogeneous edge-based IoT networks. <em>TMC</em>, <em>24</em>(4), 2817-2831. (<a href='https://doi.org/10.1109/TMC.2024.3502686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous rolling-out of edge computing, Federated Learning (FL) has become a promising solution for intelligent Internet-of-things (IoT). In addition to resource constraints, deploying FL schemes in IoT networks is greatly challenged by heterogeneity in multiple dimensions. While heterogeneity in data distribution and computation capability has been extensively studied, the impact of distinct, even hybrid training paradigms on FL performances remains largely unknown. To answer this open question in the IoT context, we propose a Hybrid-Training Federated Learning (HT-FL) algorithm for the power-constrained IoT networks, incorporating both sequential and parallel training that naturally adapts to various sub-network topologies, while greatly reducing the energy consumption during the training stage. We demonstrate through analysis that the convergence of HT-FL is theoretically guaranteed, achieving $O (\frac{1}{\sqrt{K}})$ for carefully chosen learning rates. Experiments on multiple datasets show that, the proposed HT-FL outperforms existing FL schemes on multiple training tasks under various data distribution settings, while reducing an average of 20% energy consumption. In a more practical sense, a self-adaptive parameter-tuning strategy is also designed for HT-FL deployment, which can be easily extended to other multi-layer FL schemes in complex application scenarios.},
  archive      = {J_TMC},
  author       = {Yixun Gu and Jie Wang and Shengjie Zhao},
  doi          = {10.1109/TMC.2024.3502686},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2817-2831},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HT-FL: Hybrid training federated learning for heterogeneous edge-based IoT networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LBDT: A lightweight blockchain-based data trading scheme in internet of vehicles using proof-of-reputation. <em>TMC</em>, <em>24</em>(4), 2800-2816. (<a href='https://doi.org/10.1109/TMC.2024.3497934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth of data in the Internet of Vehicles (IoV) has created opportunities to improve traffic safety and efficiency through data trading. However, establishing trust among highly mobile and resource-constrained vehicles poses significant challenges for effective data trading in IoV. To address this issue, we propose a lightweight blockchain-based data trading scheme (LBDT), which ensures secure and efficient data trading in IoV. We introduce a proof-of-reputation (PoR) consensus mechanism to establish trustworthiness for data trading. Specifically, we use a progressive reputation mechainism to support the PoR consensus. LBDT utilizes a parallel-chain structure for the PoR consensus to minimize communication and storage costs while reducing transaction confirmation latency. Additionally, we adopt a double auction mechanism as an incentivizing strategy to encourage vehicle participation in data trading. We evaluate the performance of LBDT through extensive experiments. The experimental results demonstrate that LBDT is highly effective and secure, achieving a transaction latency of approximately 4 seconds. Moreover, LBDT successfully mitigates communication and storage overheads by over 90%, thus establishing its superiority over state-of-the-art solutions under comparable conditions.},
  archive      = {J_TMC},
  author       = {Weilin Chen and Wei Yang and Mingjun Xiao and Lide Xue and Shaowei Wang},
  doi          = {10.1109/TMC.2024.3497934},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2800-2816},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LBDT: A lightweight blockchain-based data trading scheme in internet of vehicles using proof-of-reputation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Co-sense: Exploiting cooperative dark pixels in radio sensing for non-stationary target. <em>TMC</em>, <em>24</em>(4), 2783-2799. (<a href='https://doi.org/10.1109/TMC.2024.3498048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio sensing has emerged as a promising solution for monitoring vital signs in a contactless manner. However, most of the existing designs focus on stationary target and struggle with body motion interference. While some efforts have been made to address this issue, the lack of a physical explanation for the motion elimination principle makes them work as a blind signal separation way and thus leaves the body motion elimination problem still as an open challenge. In this paper, we reveal for the first time the existence of “dark pixels”–specific points on the same rigid body parts that share the same body movement but exhibit varying physiological motions, with these variations still preserving the physiological rhythm. By exploiting the inherent relationship between the dark pixels, we propose a cooperative sensing framework, Co-Sense, that can achieve robust radio sensing for non-stationary targets in an explainable way. Through extensive experiments, Co-Sense demonstrates its superiority over existing methods, achieving effective motion cancellation and breath sensing with a median absolute respiratory rate (RR) error of 0.36 respiration per minute (RPM) and breath wave correlation of 0.61 under non-stationary scenarios. The results indicate the great potential of Co-Sense in enhancing the accuracy of vital sign sensing with radio signals, especially in real-world environments where targets are rarely stationary.},
  archive      = {J_TMC},
  author       = {Jinbo Chen and Dongheng Zhang and Ganlin Zhang and Haoyu Wang and Qibin Sun and Yan Chen},
  doi          = {10.1109/TMC.2024.3498048},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2783-2799},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Co-sense: Exploiting cooperative dark pixels in radio sensing for non-stationary target},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Similarity caching in dynamic cooperative edge networks: An adversarial bandit approach. <em>TMC</em>, <em>24</em>(4), 2769-2782. (<a href='https://doi.org/10.1109/TMC.2024.3500132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike traditional edge caching paradigms, similarity edge caching enables the retrieval of similar content from local caches to fulfill user requests, reducing reliance on remote data centers and improving system performance. Although several pioneering works have contributed to similarity edge caching, most focus on single-edge nodes and/or static environment settings, which are impractical for real-world applications. To address this gap, we investigate the similarity caching problem in dynamic cooperative edge networks, where a set of edge nodes cooperatively serve requests generated from arbitrary distributions with similar content over fluctuating transmission links. This presents a significant challenge, as it requires balancing content similarity with delivery latency over the transmission network and learning the environment in real-time to optimize caching policies. We frame this problem within an adversarial Multi-Armed Bandit framework to accommodate the continuously changing operational environment. To solve this, we propose an online learning-based approach named MABSCP, which dynamically updates caching policies based on real-time feedback to minimize the service cost of edge caching networks. To enhance implementation efficiency, we devise both an offline compact strategy construction method and an online Gibbs sampling method. Finally, trace-driven simulation results demonstrate that our proposed approach outperforms several existing methods in terms of system performance.},
  archive      = {J_TMC},
  author       = {Liang Wang and Yaru Wang and Zhiwen Yu and Fei Xiong and Lianbo Ma and Huan Zhou and Bin Guo},
  doi          = {10.1109/TMC.2024.3500132},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2769-2782},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Similarity caching in dynamic cooperative edge networks: An adversarial bandit approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-aware gradient compression for FL in communication-constrained mobile computing. <em>TMC</em>, <em>24</em>(4), 2755-2768. (<a href='https://doi.org/10.1109/TMC.2024.3504284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) in mobile environments faces significant communication bottlenecks. Gradient compression has proven as an effective solution to this issue, offering substantial benefits in environments with limited bandwidth and metered data. Yet, it encounters severe performance drops in non-IID environments due to a one-size-fits-all compression approach, which does not account for the varying data volumes across workers. Assigning varying compression ratios to workers with distinct data distributions and volumes is therefore a promising solution. This work derives the convergence rate of distributed SGD with non-uniform compression, which reveals the intricate relationship between model convergence and the compression ratios applied to individual workers. Accordingly, we frame the relative compression ratio assignment as an $n$-variable chi-squared nonlinear optimization problem, constrained by a limited communication budget. We propose DAGC-R, which assigns conservative compression to workers handling larger data volumes. Recognizing the computational limitations of mobile devices, we propose the DAGC-A, which is computationally less demanding and enhances the robustness of compression in non-IID scenarios. Our experiments confirm that the DAGC-R and DAGC-A can speed up the training speed by up to 25.43% and 16.65% compared to the uniform compression respectively, when dealing with highly imbalanced data volume distribution and restricted communication.},
  archive      = {J_TMC},
  author       = {Rongwei Lu and Yutong Jiang and Yinan Mao and Chen Tang and Bin Chen and Laizhong Cui and Zhi Wang},
  doi          = {10.1109/TMC.2024.3504284},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2755-2768},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Data-aware gradient compression for FL in communication-constrained mobile computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Device selection and resource allocation with semi-supervised method for federated edge learning. <em>TMC</em>, <em>24</em>(4), 2740-2754. (<a href='https://doi.org/10.1109/TMC.2024.3504271'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of distributed learning and workflow orchestration, Federated Edge Learning has emerged as a solution, enabling multiple edge devices to collaboratively train a large model without the need for sharing raw data. Beyond considering bandwidth and computational resource limitations in the Internet of Things (IoT) environment, it is crucial to address the issue of IoT devices often collecting data that lacks timely annotations, which can lead to latency and label deficiency issues. In most Federated Edge Learning mechanisms, clients’ weights are selected for offloading to the server. In this paper, we propose a solution for dynamic edge selection and wireless network allocation under semi-supervised and privacy protection settings, termed Semi-supervised Scheduling and Allocation Optimization for Federated Edge Learning (SSAFL). SSAFL is designed to adapt to various scenarios, including channel state variations, device heterogeneity, resource incentives, deadline control, label deficiencies, and Non-IID data distributions. This adaptability is achieved through the utilization of an Incentive Optimization framework that encompasses bandwidth allocation and device scheduling policies. Within SSAFL, we introduce the concept of a weighted bipartite graph network to tackle the Incentive Optimization problem and achieve a balance in large-scale optimization of device selection. Additionally, to address the label deficiency issue, we devise a Dynamic Timer for deadline control for each client. Comprehensive and confidential results demonstrate that our proposed approach significantly outperforms other Federated Edge Learning baselines.},
  archive      = {J_TMC},
  author       = {Ruihan Hu and Haochen Yuan and Daimin Tan and Zhongjie Wang},
  doi          = {10.1109/TMC.2024.3504271},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2740-2754},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Device selection and resource allocation with semi-supervised method for federated edge learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive task assignment in spatial crowdsourcing: A human-in-the-loop approach. <em>TMC</em>, <em>24</em>(4), 2726-2739. (<a href='https://doi.org/10.1109/TMC.2024.3501734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, adaptive task assignment has been explored in spatial crowdsourcing. The challenge lies in how to adaptively partition the task stream to achieve the best utility for task assignment. A number of existing works have attempted to solve this challenge and achieve better performance by utilizing learning-based methods. Specifically, they mainly employ reinforcement learning to divide the task stream into a series of suitable batches and then perform task assignment in a batch fashion. Drawing inspiration from the effectiveness of human-machine collaborative decision-making, we aim to investigate human-in-the-loop methods to further enhance the performance of adaptive task assignment. In this paper, we propose a novel framework called Human-in-the-Loop Adaptive Partition (HLAP), which consists of two primary modules: Reinforcement Learning Partition Decision (RL-PD) and Human Supervision and Guidance (HSG). In the RL-PD module, we develop an RL agent, referred to as the decision-maker, by integrating the dual attention network into the Deep Q-Network (DQN) algorithm to capture cross-dimensional contextual information and long-range dependencies for a better understanding of the environment. In the HSG module, we design a human-in-the-loop mechanism to optimize the performance of the decision-maker, focusing on addressing two key issues: when and how humans interact with the decision-maker. Furthermore, to alleviate the heavy workload on humans, we construct a supervisor based on RL to oversee the decision-maker's partition process and adaptively determine when human intervention is necessary. We conduct extensive experiments on two real-world datasets, and the results demonstrate the efficiency and effectiveness of the HLAP framework.},
  archive      = {J_TMC},
  author       = {Qingshun Wu and Yafei Li and Jinxing Yan and Mei Zhang and Jianliang Xu and Mingliang Xu},
  doi          = {10.1109/TMC.2024.3501734},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2726-2739},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive task assignment in spatial crowdsourcing: A human-in-the-loop approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Inter-stream adaptive bitrate streaming for short-video services. <em>TMC</em>, <em>24</em>(4), 2708-2725. (<a href='https://doi.org/10.1109/TMC.2024.3497954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short-video services have seen explosive growth in recent years. Streaming over mobile networks is inherently challenging due to the latter's bandwidth fluctuations, motivating researchers to develop many sophisticated adaptive bitrate (ABR) algorithms to compensate. While ABR, together with prefetching, has been proposed for playlist streaming, its application to non-playlist streaming has received little attention. This work fills this gap by first exploring the efficacy of directly applying ABR to non-playlist streaming. Observing their limitations motivates the development of a new class of inter-stream bitrate adaptation (ISA) algorithms. Unlike ABR, ISA adapts bitrate on a per-video basis, which is not only simpler to implement and deploy but can even outperform ABR algorithms by up to 66.71% across a wide range of networks. Moreover, ISA and ABR are complementary such that they can be combined into Integrated Bitrate Adaptation (IBA) algorithms to raise performance gains further by up to 77.03%. In addition, this work develops a novel adaptive rebuffering duration (ARD) algorithm specifically designed for frame-based playback common in short-video services to further improve their performance under challenging network conditions. Together, ISA and ARD offer a new set of tools with progressive complexity-performance tradeoffs for enhancing the performance of short-video services.},
  archive      = {J_TMC},
  author       = {Yuming Zhang and Shengtong Zhu and Yan Liu and Lingfeng Guo and Ji Li and Jack Y. B. Lee},
  doi          = {10.1109/TMC.2024.3497954},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2708-2725},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Inter-stream adaptive bitrate streaming for short-video services},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeTA: Neuron-grained scaling of foundation models in edge-side retraining. <em>TMC</em>, <em>24</em>(4), 2690-2707. (<a href='https://doi.org/10.1109/TMC.2024.3504859'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foundation models (FMs) such as large language models are becoming the backbone technology for artificial intelligence systems. It is particularly challenging to deploy multiple FMs on edge devices, which not only have limited computational resources, but also encounter unseen input data from evolving domains or learning tasks. When new data arrives, existing prior art of FM mainly focuses on retraining compressed models of predetermined network architectures, limiting the feasibility of edge devices to efficiently achieve high accuracy for FMs. In this paper, we propose EdgeTA, a neuron-grained FM scaling system to maximize the overall accuracy of FMs promptly in response to their data dynamics. EdgeTA's key design features in scaling are (i) proxy mechanism, which adaptively transforms a FM into a compact architecture retaining the most important neurons to the input data, and (ii) neuron-grained scheduler, which jointly optimizes model sizes and resource allocation for all FMs on edge devices. Under tight retraining window and limited device resources, the design of EdgeTA can achieve most of the original FM's accuracy with much smaller retraining costs. We implement EdgeTA on FMs of natural language processing, computer vision and multimodal applications. Comparison results against state-of-the-art techniques show that our approach improves accuracy by 21.88% and reduces memory footprint and energy consumptions by 27.14% and 65.65%, while further achieving 15.96% overall accuracy improvement via neuron-grained scheduling.},
  archive      = {J_TMC},
  author       = {Qinglong Zhang and Rui Han and Chi Harold Liu and Guoren Wang and Song Guo and Lydia Y. Chen},
  doi          = {10.1109/TMC.2024.3504859},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2690-2707},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EdgeTA: Neuron-grained scaling of foundation models in edge-side retraining},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed computation offloading for energy provision minimization in WP-MEC networks with multiple HAPs. <em>TMC</em>, <em>24</em>(4), 2673-2689. (<a href='https://doi.org/10.1109/TMC.2024.3502004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a wireless powered mobile edge computing (WP-MEC) network with multiple hybrid access points (HAPs) in a dynamic environment, where wireless devices (WDs) harvest energy from radio frequency (RF) signals of HAPs, and then compute their computation data locally (i.e., local computing mode) or offload it to the chosen HAPs (i.e., edge computing mode). In order to pursue a green computing design, we formulate an optimization problem that minimizes the long-term energy provision of the WP-MEC network subject to the energy, computing delay and computation data demand constraints. The transmit power of HAPs, the duration of the wireless power transfer (WPT) phase, the offloading decisions of WDs, the time allocation for offloading and the CPU frequency for local computing are jointly optimized adapting to the time-varying generated computation data and wireless channels of WDs. To efficiently address the formulated non-convex mixed integer programming (MIP) problem in a distributed manner, we propose a Two-stage Multi-Agent deep reinforcement learning-based Distributed computation Offloading (TMADO) framework, which consists of a high-level agent and multiple low-level agents. The high-level agent residing in all HAPs optimizes the transmit power of HAPs and the duration of the WPT phase, while each low-level agent residing in each WD optimizes its offloading decision, time allocation for offloading and CPU frequency for local computing. Simulation results show the superiority of the proposed TMADO framework in terms of the energy provision minimization.},
  archive      = {J_TMC},
  author       = {Xiaoying Liu and Anping Chen and Kechen Zheng and Kaikai Chi and Bin Yang and Tarik Taleb},
  doi          = {10.1109/TMC.2024.3502004},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2673-2689},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed computation offloading for energy provision minimization in WP-MEC networks with multiple HAPs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and implementation of a light-weight channel vector classifier based on support vector machine for real-time 5G beam index detection. <em>TMC</em>, <em>24</em>(4), 2660-2672. (<a href='https://doi.org/10.1109/TMC.2024.3494757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) is recently considered a key technology for bringing outstanding performance to wireless communications. Conventional research has highlighted the potential of Support Vector Machines (SVMs), which train their model based on optimization theory, to enhance the performance of wireless communications. However, there are practical issues that makes SVM difficult to apply to a wireless communication system. SVM generally entails a heavy training process with high computational complexity, and the model requires a significant amount of time for training. Also, the entire dataset needs to be trained at once, requiring a substantial amount of memory for data storage. To enable SVM in wireless communications, we propose Real-Time Channel Vector Classifier (RTCVC), which employs a light-weight SVM model capable of training and processing incoming data in real-time. A novel input data pre-processing technique is implemented to reduce the computational overhead associated with calculating non-linear functions. The rearranged formulation of the original problem also allows each SVM sub-model to be trained distributively over time based on incremental parameters. For performance evaluation, we implement the RTCVC inter-operating with 5G beam index detection, whose detection probability has been theoretically proven to be significantly enhanced by SVM. The software modules of the RTCVC are based on LibSVM, a well-known open-source library for implementing SVM sub-models. The experimental results confirm that RTCVC significantly reduces training time while maintaining suitable performance for 5G beam index detection.},
  archive      = {J_TMC},
  author       = {Juyeop Kim and Soomin Kwon and Jiyoon Han and Taegyeom Lee and Ohyun Jo},
  doi          = {10.1109/TMC.2024.3494757},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2660-2672},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Design and implementation of a light-weight channel vector classifier based on support vector machine for real-time 5G beam index detection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latency-energy efficient task offloading in the satellite network-assisted edge computing via deep reinforcement learning. <em>TMC</em>, <em>24</em>(4), 2644-2659. (<a href='https://doi.org/10.1109/TMC.2024.3502643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the demand for global computing coverage continues to surge, satellite edge computing emerges as a pivotal technology for the next generation of networks. Unlike ground-based edge computing, Low Earth Orbit (LEO) satellites face distinctive challenges, including high-speed mobility and resource limitations, etc. Therefore, effectively utilizing LEO satellites for global coverage services is crucial but challenging due to their dynamic coverage areas and diverse task requirements. To address these challenges, we introduce a novel dual-cloud edge collaborative task offloading architecture in the satellite network-assisted edge computing environment, namely, Satellite-Ground Task Offloading (SGTO). The architecture employs a Geostationary Earth Orbit (GEO) satellite and a ground cloud computing center as satellite cloud and ground cloud, respectively, and LEO satellites as edge nodes. We formally define the task offloading problem in the SGTO with the aim of minimizing the average latency and average energy consumption. We then propose an adaptive approach named SGTO-A from the perspective of satellites to adaptively solve the problem leveraging deep reinforcement learning. Specifically, we transform the task offloading problem into a Markov decision process and adopt the generalized proximal policy optimization (GePPO) algorithm to solve the problem. Finally, experimental results demonstrate that SGTO architecture and SGTO-A outperform the representative approaches in terms of average latency, average energy consumption and running time.},
  archive      = {J_TMC},
  author       = {Jian Zhou and Juewen Liang and Lu Zhao and Shaohua Wan and Hui Cai and Fu Xiao},
  doi          = {10.1109/TMC.2024.3502643},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2644-2659},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Latency-energy efficient task offloading in the satellite network-assisted edge computing via deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint order dispatching and vehicle repositioning for dynamic ridesharing. <em>TMC</em>, <em>24</em>(4), 2628-2643. (<a href='https://doi.org/10.1109/TMC.2024.3493974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic ridesharing has gained significant attention in recent years. However, existing ridesharing studies often focus on optimizing order dispatching and vehicle repositioning separately, leading to short-sighted decisions and underutilization of the ridesharing potential. In this paper, we propose a novel joint optimization framework called $\mathtt {JODR}$. By coordinating order dispatching and vehicle repositioning, $\mathtt {JODR}$ enhances ridesharing efficiency while ensuring high-quality service. The core idea of $\mathtt {JODR}$ is to dispatch ride orders with high demand in specific mobility directions to vehicles with sufficient available capacity, effectively balancing future supply and demand in those directions. To achieve this, we introduce a novel mobility value function that can predict the long-term mobility value of matching an order with its travel direction. By considering orders’ directional mobility values, service quality assessments, and available vehicle capacities, $\mathtt {JODR}$ formulates the order dispatching as a minimum-cost maximum-flow problem to derive the optimal order-vehicle assignments. Furthermore, the value function helps the intelligent repositioning of idle vehicles. Extensive experiments conducted on a large real-world dataset demonstrate the superiority of $\mathtt {JODR}$ over state-of-the-art methods across various performance metrics. These experimental results validate the effectiveness of $\mathtt {JODR}$ in improving the ridesharing efficiency and experience.},
  archive      = {J_TMC},
  author       = {Zhidan Liu and Guofeng Ouyang and Bolin Zhang and Bo Du and Chao Chen and Kaishun Wu},
  doi          = {10.1109/TMC.2024.3493974},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2628-2643},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint order dispatching and vehicle repositioning for dynamic ridesharing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleashing the potential of self-supervised RF learning with group shuffle. <em>TMC</em>, <em>24</em>(4), 2612-2627. (<a href='https://doi.org/10.1109/TMC.2024.3497972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning (SSL) is a powerful approach that learns general semantic representations from large-scale unlabeled data to make downstream tasks solve easier, offering significant potential in enhancing downstream performance and alleviating the appetite for large-scale annotated data. However, existing SSL techniques, predominantly designed for natural images, may be prone to shortcuts when applied to RF signals. This study presents surprising empirical findings showing that SSL can indeed learn meaningful RF representations by employing simple group shuffle (GS) and asymmetry augmentation techniques. The GS augmentation is inspired by blind calibration tasks in Time-Interleaved Analog-to-Digital Converters (TIADC). By treating the original RF signal as a composite output from sub-ADCs, GS augmentation enriches RF signals while preserving their global semantics. We also provide a theoretical validation of the GS augmentation’s singular value consistency. Notably, we observe that the shortcut is essentially a domain gap between the pre-trained and the downstream task models. This issue can be mitigated by an asymmetry augmentation technique, which maximizes the similarity between an original RF signal and its augmented version, rather than between two augmentations of the same RF signal. By integrating group shuffle and asymmetry augmentation (GSAA) into an existing contrastive learning framework, we develop an effective contrastive learning approach for RF signals. Our evaluations, spanning seven downstream RF sensing tasks across two general RF devices (WiFi and radar), strongly demonstrate that GSAA plays a significant role in advancing SSL-based solutions in RF sensing.},
  archive      = {J_TMC},
  author       = {Ruiyuan Song and Zhi Lu and Dongheng Zhang and Liang Fang and Zhi Wu and Yang Hu and Qibin Sun and Yan Chen},
  doi          = {10.1109/TMC.2024.3497972},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2612-2627},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Unleashing the potential of self-supervised RF learning with group shuffle},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative error detection and correction scheme for safety message in V2X. <em>TMC</em>, <em>24</em>(4), 2594-2611. (<a href='https://doi.org/10.1109/TMC.2024.3494713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle-to-Everything (V2X) technology plays a pivotal role in enabling real-time traffic coordination and safety, warning, and decision support. Within V2X, the Basic Safety Message (BSM) serves as the core to transmit critical vehicle status, location, and intention information to provide a foundation for ensuring reliable traffic safety and coordination mechanisms. Data accuracy stands as a key to the effectiveness and reliability of the V2X system, in which the transmission of error data can potentially result in severe traffic accidents. During vehicular operation, sensors may generate error data owing to looseness or external conditions. However, immediate sensor replacement is often impractical or infeasible. Therefore, this paper introduces a collaborative scheme involving vehicles, Road Side Units (RSUs), and Data Center (DC) to jointly enhance the accuracy of vehicle-transmitted BSMs. Our scheme involves analyzing statistical features of vehicle driving information to detect error BSMs. Subsequently, these detected errors are corrected by leveraging historical data from the vehicle and its relative relationship with surrounding vehicles. In addition, we propose a time optimization method to reduce the average processing time of each data by RSUs. The extensive experimental results demonstrate that the proposed scheme can accurately detect error BSMs and effectively correct error BSMs. The entire scheme also meets the requisite computational latency requirements.},
  archive      = {J_TMC},
  author       = {Hui Qian and Hongmei Chai and Ammar Hawbani and Yuanguo Bi and Na Lin and Liang Zhao},
  doi          = {10.1109/TMC.2024.3494713},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2594-2611},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A collaborative error detection and correction scheme for safety message in V2X},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantum reinforcement learning approach for joint resource allocation and task offloading in mobile edge computing. <em>TMC</em>, <em>24</em>(4), 2580-2593. (<a href='https://doi.org/10.1109/TMC.2024.3496918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) has revolutionized the way computational tasks are offloaded and latency is reduced by leveraging edge servers close to end devices. Efficient resource allocation and task offloading are crucial for enhancing system performance in MEC environments. Traditional reinforcement learning (RL) approaches have shown promise in optimizing resource allocation and task offloading problems. However, they often face challenges such as high computational complexity and the need for extensive training data. Quantum reinforcement learning (QRL) emerges as a promising solution to overcome these limitations by leveraging quantum computing principles to enhance efficiency and scalability. In this paper, we propose a hybrid quantum-classical non-sequential model for joint resource allocation and task offloading in MEC systems. Our model combines the advantages of RL in handling environmental dynamics and quantum computing in reducing adjustable parameters and accelerating the training process. Extensive experiments demonstrate that our proposed algorithm can achieve higher training and inference performance under various parameter settings compared to traditional RL models and previous QRL models.},
  archive      = {J_TMC},
  author       = {Xinliang Wei and Xitong Gao and Kejiang Ye and Cheng-Zhong Xu and Yu Wang},
  doi          = {10.1109/TMC.2024.3496918},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2580-2593},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A quantum reinforcement learning approach for joint resource allocation and task offloading in mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay- and resource-aware satellite UPF service optimization. <em>TMC</em>, <em>24</em>(4), 2564-2579. (<a href='https://doi.org/10.1109/TMC.2024.3494043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Executing 5G core network functions on satellites has become crucial to enhance satellite network management and service capabilities. The User Plane Function (UPF) is responsible for efficient data traffic forwarding and is envisioned as a key and pioneering core network function that will be deployed on satellites. However, managing and providing services with satellite UPFs face dual challenges. Limited satellite resources constrain the user scale that a satellite UPF can service, resulting in an unguaranteed service delay. Moreover, the extremely rapid mobility of satellites renders it difficult for satellite UPFs to provide seamless services. To address the above challenges, this paper presents the first-of-its-kind service optimization scheme for satellite UPFs in terms of switch control, state migration, and traffic routing. To provide guaranteed service delay, we provide a theoretical analysis based on the M/G/1 queue model, demonstrating the service delay-resource consumption trade-off. A satellite UPF switch control scheme is integrated into the service optimization process, which can decrease satellite UPF service delay while saving satellite resources by adjusting the switch control parameters. To provide seamless services, we propose a satellite UPF-oriented state-aware service migration and traffic routing (UPF service optimization) algorithm. A policy network-based reinforcement learning approach is employed to dynamically perceive the satellite network’s state as well as the satellite UPF switch state. Building upon the optimization of service delay through satellite UPF switch control, the processes of state-aware state migration and traffic routing are further employed to reduce delay, ensuring seamless service effectively. Experiments reveal that the proposed algorithm outperforms other benchmark algorithms under different metrics. The service delay is reduced by an average of 23.2% compared with other algorithms.},
  archive      = {J_TMC},
  author       = {Chao Wang and Xiao Ma and Ruolin Xing and Sisi Li and Ao Zhou and Shangguang Wang},
  doi          = {10.1109/TMC.2024.3494043},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2564-2579},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Delay- and resource-aware satellite UPF service optimization},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards communication-efficient cooperative perception via planning-oriented feature sharing. <em>TMC</em>, <em>24</em>(4), 2551-2563. (<a href='https://doi.org/10.1109/TMC.2024.3496856'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving systems are fundamentally composed of sequential modular tasks, i.e., perception, prediction, and planning. For connected autonomous vehicles (CAVs), cooperative perception offers a promising solution to surpass their perception limitations, such as occlusion, by sharing sensing data with each other through wireless communication. Existing works typically prioritize sharing data from potential object-containing areas to maximize object detection accuracy under constrained communication resources. However, such detection-oriented approaches ignore a crucial fact that more accurate detection does not equal safer planning. Sharing large amounts of sensing data for detection accuracy can lead to communication resource wastage and performance degradation of subsequent driving tasks. To address this, we introduce Plan2comm, a communication-efficient cooperative perception framework via planning-oriented feature sharing, which shares only sensing data around planned trajectories to enable safer planning rather than mere detection accuracy. Specifically, a planning-oriented communication mechanism is designed to select and transmit the most valuable features from the perspective of the planning task. Moreover, an uncertainty-aware spatial-temporal feature fusion strategy is proposed to enhance high-quality information aggregation. Comprehensive experiments demonstrate that Plan2comm outperforms all other cooperative perception methods on motion prediction performance, and is more communication-efficient.},
  archive      = {J_TMC},
  author       = {Qi Xie and Xiaobo Zhou and Tianyu Hong and Wenkai Hu and Wenyu Qu and Tie Qiu},
  doi          = {10.1109/TMC.2024.3496856},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2551-2563},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards communication-efficient cooperative perception via planning-oriented feature sharing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of task offloading and resource allocation of fog network by considering matching externalities and dynamics. <em>TMC</em>, <em>24</em>(4), 2534-2550. (<a href='https://doi.org/10.1109/TMC.2024.3494793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to jointly optimize task offloading and resource allocation to minimize the task failure rate and task payments remains an unresolved challenge in fog networks. Focusing on this problem, this research formulates a novel task offloading and resource allocation model with two offloading modes and on-demand virtual resource units (VRUs). This model is decomposed into two sub-problems to solve: a joint task offloading and resource allocation optimization problem and a matching problem with externalities and dynamics. First, for a given terminal node (TN) and fog node (FN), this research theoretically derives the optimal offloading ratio and resource allocation strategy to minimize the payment of TNs for two offloading modes, i.e., immediate and queued offloading. Second, in the multi-TNs and multi-FNs scenario, the problem of making the task offloading decision is transformed into a many-to-one matching game by considering externalities and dynamics. Finally, a Deferred acceptance-based Loss ratio and Payment Minimized task Offloading and resource Allocation optimization (DLPMOA) algorithm is proposed to derive a stable and Pareto-optimal match. The simulation results show that the proposed DLPMOA has better performance in terms of task failure rate, task average payment, fog computing resource utilization, and fairness than the state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Jiahui Xu and Yingbiao Yao and Xin Xu and Wei Feng and Pei Li},
  doi          = {10.1109/TMC.2024.3494793},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2534-2550},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of task offloading and resource allocation of fog network by considering matching externalities and dynamics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TaPIN: Reinforcing PIN authentication on smartphones with tap biometrics. <em>TMC</em>, <em>24</em>(4), 2519-2533. (<a href='https://doi.org/10.1109/TMC.2024.3502902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {PIN authentication is the first line of defense for protecting private data on many smartphone applications, such as lock screens, messengers, and banking apps. However, existing PIN authentication systems have several constraints regarding security, usability, and robustness. To go beyond their limitations, this paper presents TaPIN, a reliable system that authenticates smartphone users with the collaborative use of PINs and tap biometrics. A user is first instructed to enter her PIN by tapping a smartphone screen for authentication. During the PIN entry, the user's fingertip collides with the screen, producing user-specific vibration and sound signals. TaPIN then senses the tap-induced signals and the collision properties, e.g., pressures and sizes, using the smartphone's built-in sensors and leverages them as biometric features. That is, it authenticates the user by verifying not only the entered PIN but also the collected features. Our experiments with 20 real-world users demonstrate that this two-factor authentication system is easy to use, more secure than existing methods, and deployable without dedicated hardware. For example, it accurately authenticates users with an average EER of 1.9% in stationary environments and maintains a reasonable level of security regardless of devices, tap styles, and noise.},
  archive      = {J_TMC},
  author       = {Junhyub Lee and Insu Kim and Sangeun Oh and Hyosu Kim},
  doi          = {10.1109/TMC.2024.3502902},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2519-2533},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TaPIN: Reinforcing PIN authentication on smartphones with tap biometrics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent end-to-end deterministic scheduling across converged networks. <em>TMC</em>, <em>24</em>(4), 2504-2518. (<a href='https://doi.org/10.1109/TMC.2025.3530486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deterministic network services play a vital role for supporting emerging real-time applications with bounded low latency, jitter, and high reliability. The deterministic guarantee is penetrated into various types of networks, such as 5G, WiFi, satellite, and edge computing networks. From the user’s perspective, the real-time applications require end-to-end deterministic guarantee across the converged network. In this paper, we investigate the end-to-end deterministic guarantee problem across the whole converged network, aiming to provide a scalable method for different kinds of converged networks to meet the bounded end-to-end latency, jitter, and high reliability demands of each flow, while improving the network scheduling QoS. Particularly, we set up the global end-to-end control plane to abstract the deterministic-related resources from converged network, and model the deterministic flow transmission by using the abstracted resources. With the resource abstraction, our model can work well for different underlying technologies. Given large amounts of abstracted resources in our model, it is difficult for traditional algorithms to fully utilize the resources. Thus, we propose a deep reinforcement learning based end-to-end deterministic-related resource scheduling (E2eDRS) algorithm to schedule the network resources from end to end. By setting the action groups, the E2eDRS can support varying network dimensions both in horizontal and vertical end-to-end deterministic-related network architectures. Experimental results show that E2eDRS can averagely increase 1.33x and 6.01x schedulable flow number for horizontal scheduling compared with MultiDRS and MultiNaive algorithms, respectively. The E2eDRS can also optimize 2.65x and 3.87x server load balance than MultiDRS and MultiNaive algorithms, respectively. For vertical scheduling, the E2eDRS can still perform better on schedulable flow number and server load balance.},
  archive      = {J_TMC},
  author       = {Zongrong Cheng and Weiting Zhang and Dong Yang and Chuan Huang and Hongke Zhang and Xuemin Sherman Shen},
  doi          = {10.1109/TMC.2025.3530486},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2504-2518},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Intelligent end-to-end deterministic scheduling across converged networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaEvo: Edge-assisted continuous and timely DNN model evolution for mobile devices. <em>TMC</em>, <em>24</em>(4), 2485-2503. (<a href='https://doi.org/10.1109/TMC.2023.3316388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile video applications today have attracted significant attention. Deep learning model (e.g., deep neural network, DNN) compression is widely used to enable on-device inference for facilitating robust and private mobile video applications. The compressed DNN, however, is vulnerable to the agnostic data drift of the live video captured from the dynamically changing mobile scenarios. To combat the data drift, mobile ends rely on edge servers to continuously evolve and re-compress the DNN with freshly collected data. We design a framework, ${\sf AdaEvo}$, that efficiently supports the resource-limited edge server handling mobile DNN evolution tasks from multiple mobile ends. The key goal of ${\sf AdaEvo}$ is to maximize the average quality of experience (QoE), i.e., the proportion of high-quality DNN service time to the entire life cycle, for all mobile ends. Specifically, it estimates the DNN accuracy drops at the mobile end without labels and performs a dedicated video frame sampling strategy to control the size of retraining data. In addition, it balances the limited computing and memory resources on the edge server and the competition between asynchronous tasks initiated by different mobile users. With an extensive evaluation of real-world videos from mobile scenarios and across four diverse mobile tasks, experimental results show that ${\sf AdaEvo}$ enables up to 34% accuracy improvement and 32% average QoE improvement.},
  archive      = {J_TMC},
  author       = {Lehao Wang and Zhiwen Yu and Haoyi Yu and Sicong Liu and Yaxiong Xie and Bin Guo and Yunxin Liu},
  doi          = {10.1109/TMC.2023.3316388},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {4},
  number       = {4},
  pages        = {2485-2503},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AdaEvo: Edge-assisted continuous and timely DNN model evolution for mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MagSpy: Revealing user privacy leakage via magnetometer on mobile devices. <em>TMC</em>, <em>24</em>(3), 2455-2469. (<a href='https://doi.org/10.1109/TMC.2024.3495506'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various characteristics of mobile applications (apps) and associated in-app services can reveal potentially-sensitive user information; however, privacy concerns have prompted third-party apps to restrict access to data related to mobile app usage. This paper outlines a novel approach to extracting detailed app usage information by analyzing electromagnetic (EM) signals emitted from mobile devices during app-related tasks. The proposed system, MagSpy, recovers user privacy information from magnetometer readings that do not require access permissions. This EM leakage becomes complex when multiple apps are used simultaneously and is subject to interference from geomagnetic signals generated by device movement. To address these challenges, MagSpy employs multiple techniques to extract and identify signals related to app usage. Specifically, the geomagnetic offset signal is canceled using accelerometer and gyroscope sensor data, and a Cascade-LSTM algorithm is used to classify apps and in-app services. MagSpy also uses CWT-based peak detection and a Random Forest classifier to detect PIN inputs. A prototype system was evaluated on over 50 popular mobile apps with 30 devices. Extensive evaluation results demonstrate the efficacy of MagSpy in identifying in-app services (96% accuracy), apps (93.5% accuracy), and extracting PIN input information (96% top-3 accuracy).},
  archive      = {J_TMC},
  author       = {Yongjian Fu and Lanqing Yang and Hao Pan and Yi-Chao Chen and Guangtao Xue and Ju Ren},
  doi          = {10.1109/TMC.2024.3495506},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2455-2469},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MagSpy: Revealing user privacy leakage via magnetometer on mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling feedback-free MIMO transmission for FD-RAN: A data-driven approach. <em>TMC</em>, <em>24</em>(3), 2437-2454. (<a href='https://doi.org/10.1109/TMC.2024.3495719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To enhance flexibility and facilitate resource cooperation, a novel fully-decoupled radio access network (FD-RAN) architecture is proposed for 6G. However, the decoupling of uplink (UL) and downlink (DL) in FD-RAN makes the existing feedback mechanism ineffective. To this end, we propose an end-to-end data-driven MIMO solution without the conventional channel feedback procedure. Data-driven MIMO can alleviate the drawbacks of feedback including overheads and delay, and can provide customized precoding design for different BSs based on their historical channel data. It essentially learns a mapping from geolocation to MIMO transmission parameters. We first present a codebook-based approach, which selects transmission parameters from the statistics of discrete channel state information (CSI) values and utilizes nearest neighbor interpolation for spatial inference. We further present a non-codebook-based approach, which 1) derives the optimal precoder from the singular value decomposition (SVD) of the channel; 2) utilizes variational autoencoder (VAE) to select the representative precoder from the latent Gaussian representations; and 3) exploits Gaussian process regression (GPR) to predict unknown precoders in the space domain. Extensive simulations are performed on a link-level 5G simulator using realistic ray-tracing channel data. The results demonstrate the effectiveness of data-driven MIMO, showcasing its potential for application in FD-RAN and 6G.},
  archive      = {J_TMC},
  author       = {Jingbo Liu and Jiacheng Chen and Zongxi Liu and Haibo Zhou},
  doi          = {10.1109/TMC.2024.3495719},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2437-2454},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enabling feedback-free MIMO transmission for FD-RAN: A data-driven approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPUF: An entropy-derived latency-based DRAM physical unclonable function for lightweight authentication in internet of things. <em>TMC</em>, <em>24</em>(3), 2422-2436. (<a href='https://doi.org/10.1109/TMC.2024.3494612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical Unclonable Functions (PUFs) are hardware-based mechanisms that exploit inherent manufacturing variations to generate unique identifiers for devices. Dynamic Random Access Memory (DRAM) has emerged as a promising medium for implementing PUFs, providing a cost-effective solution without the need for additional circuitry. This makes DRAM PUFs ideal for use in resource-constrained environments such as Internet of Things (IoT) networks. However, current DRAM PUF implementations often either disrupt host system functions or produce unreliable responses due to environmental sensitivity. In this paper, we present EPUF, a novel approach to extracting random and unique features from DRAM cells to generate reliable PUF responses. We leverage bitmap images of binary DRAM values and their entropy features to enhance the robustness of our PUF. Through extensive real-world experiments, we demonstrate that EPUF is approximately 1.7 times faster than existing solutions, achieves 100% reliability, produces features with 47.79% uniqueness, and supports a substantial set of Challenge-Response Pairs (CRPs). These capabilities make EPUF a powerful tool for DRAM PUF-based authentication. Based on EPUF, we then propose a lightweight authentication protocol that not only offers superior security features but also surpasses state-of-the-art authentication schemes in terms of communication overhead and computational efficiency.},
  archive      = {J_TMC},
  author       = {Fatemeh Najafi and Masoud Kaveh and Mohammad Reza Mosavi and Alessandro Brighente and Mauro Conti},
  doi          = {10.1109/TMC.2024.3494612},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2422-2436},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EPUF: An entropy-derived latency-based DRAM physical unclonable function for lightweight authentication in internet of things},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biasing federated learning with a new adversarial graph attention network. <em>TMC</em>, <em>24</em>(3), 2407-2421. (<a href='https://doi.org/10.1109/TMC.2024.3499371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairness in Federated Learning (FL) is imperative not only for the ethical utilization of technology but also for ensuring that models provide accurate, equitable, and beneficial outcomes across varied user demographics and equipment. This paper proposes a new adversarial architecture, referred to as Adversarial Graph Attention Network (AGAT), which deliberately instigates fairness attacks with an aim to bias the learning process across the FL. The proposed AGAT is developed to synthesize malicious, biasing model updates, where the minimum of Kullback-Leibler (KL) divergence between the user's model update and the global model is maximized. Due to a limited set of labeled input-output biasing data samples, a surrogate model is created, which presents the behavior of a complex malicious model update. Moreover, a graph autoencoder (GAE) is designed within the AGAT architecture, which is trained together with sub-gradient descent to reconstruct manipulatively the correlations of the model updates, and maximize the reconstruction loss while keeping the malicious, biasing model updates undetectable. The proposed AGAT attack is implemented in PyTorch, showing experimentally that AGAT successfully increases the minimum value of KL divergence of benign model updates by 60.9% and bypasses the detection of existing defense models. The source code of the AGAT attack is released on GitHub.},
  archive      = {J_TMC},
  author       = {Kai Li and Jingjing Zheng and Wei Ni and Hailong Huang and Pietro Liò and Falko Dressler and Ozgur B. Akan},
  doi          = {10.1109/TMC.2024.3499371},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2407-2421},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Biasing federated learning with a new adversarial graph attention network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligence-based reinforcement learning for dynamic resource optimization in edge computing-enabled vehicular networks. <em>TMC</em>, <em>24</em>(3), 2394-2406. (<a href='https://doi.org/10.1109/TMC.2024.3506161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent transportation systems demand efficient resource allocation and task offloading to ensure low-latency, high-bandwidth vehicular services. The dynamic nature of vehicular environments, characterized by high mobility and extensive interactions among vehicles, necessitates considering time-varying statistical regularities, especially in scenarios with sharp variations. Despite the widespread use of traditional reinforcement learning for resource allocation, its limitations in generalization and interpretability are evident. To overcome these challenges, we propose an Intelligence-based Reinforcement Learning (IRL) algorithm. This algorithm utilizes active inference to infer the real world and maintain an internal model by minimizing free energy. Enhancing the efficiency of active inference, we incorporate prior knowledge as macro guidance, ensuring more accurate and efficient training. By constructing an intelligence-based model, we eliminate the need for designing reward functions, aligning better with human thinking, and providing a method to reflect the learning, information transmission and intelligence accumulation processes. This approach also allows for quantifying intelligence to a certain extent. Considering the dynamic and uncertain nature of vehicular scenarios, we apply the IRL algorithm to environments with constantly changing parameters. Extensive simulations confirm the effectiveness of IRL, significantly improving the generalization and interpretability of intelligent models in vehicular networks.},
  archive      = {J_TMC},
  author       = {Yuhang Wang and Ying He and F. Richard Yu and Kaishun Wu and Shanzhi Chen},
  doi          = {10.1109/TMC.2024.3506161},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2394-2406},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Intelligence-based reinforcement learning for dynamic resource optimization in edge computing-enabled vehicular networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring long-term commensalism: Throughput maximization for symbiotic radio networks. <em>TMC</em>, <em>24</em>(3), 2376-2393. (<a href='https://doi.org/10.1109/TMC.2024.3495015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Symbiotic radio (SR), combining the advantages of cognitive radio and ambient backscatter communication (AmBC), stands as a promising solution for spectrum-and-energy-efficient wireless communications. In an SR network, backscatter devices (BDs) share the spectrum resources with the primary transmitter (PT) by utilizing the incident radio frequency (RF) signal from PT for uplink non-orthogonal multiple access (NOMA) transmission. The primary receiver (PR) decodes the signals of PT and BDs via the successive interference cancellation (SIC) technique. Our goal is to establish a long-term commensalistic relationship between PT and BDs. We address the problem of maximizing the long-term average sum rate of BDs while ensuring a minimum average rate for the PT by optimizing the power reflection coefficients of the BDs. We explicitly consider practical constraints such as the required power difference among signals for SIC decoding and the unknown future channel state information (CSI). We prove the NP-hardness of the offline version of the problem and subsequently employ the Lyapunov optimization technique to convert the original problem into a series of sub-problems in each individual time slot that can be solved in an online manner without relying on future CSI. We then utilize the successive convex optimization (SCO) technique to solve the non-convex sub-problems. Extensive simulations validate that our proposed Lyapunov-SCO algorithm achieves superior performance in terms of the average sum rate of BDs while ensuring PT’s required average rate. In addition, we provide discussions on extending the proposed solution to SR networks with multiple PT-PR pairs, high-mobility BDs, and enhancing fairness among BDs.},
  archive      = {J_TMC},
  author       = {Yuzhe Chen and Yanjun Li and Chung Shue Chen and Kaikai Chi},
  doi          = {10.1109/TMC.2024.3495015},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2376-2393},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring long-term commensalism: Throughput maximization for symbiotic radio networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAiMo: Motif density enhances topology robustness for highly dynamic scale-free IoT. <em>TMC</em>, <em>24</em>(3), 2360-2375. (<a href='https://doi.org/10.1109/TMC.2024.3492002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust Topology is a key prerequisite to providing consistent connectivity for highly dynamic Internet-of-Things (IoT) applications that are suffering node failures. In this paper, we present a two-step approach to organizing the most robust IoT topology. First, we propose a novel robustness metric denoted as $I$, which is based on network motifs and is specifically designed to sensitively analyze the dynamic changes in topology resulting from node failures. Second, we introduce a Distributed duAl-layer collaborative competition optimization strategy based on Motif density (DAiMo). This strategy significantly expands the search space for optimal solutions and facilitates the identification of the optimal IoT topology. We utilize the motif density concept in the collaborative optimization process to efficiently search for the optimal topology. To support our approach, extensive mathematical proofs are provided to demonstrate the advantages of the metric $I$ in effectively perceiving changes in IoT topology and to establish the convergence of the DAiMo algorithm. Finally, we conduct comprehensive performance evaluations of DAiMo and investigate the influence of network motifs on the resilience and reliability of IoT topologies. Experimental results clearly indicate that the proposed method outperforms existing state-of-the-art topology optimization methods in terms of enhancing network robustness.},
  archive      = {J_TMC},
  author       = {Ning Chen and Tie Qiu and Weisheng Si and Dapeng Oliver Wu},
  doi          = {10.1109/TMC.2024.3492002},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2360-2375},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DAiMo: Motif density enhances topology robustness for highly dynamic scale-free IoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative task offloading and resource allocation in small-cell MEC: A multi-agent PPO-based scheme. <em>TMC</em>, <em>24</em>(3), 2346-2359. (<a href='https://doi.org/10.1109/TMC.2024.3496536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small-cell mobile edge computing (SE-MEC) networks amalgamate the virtues of MEC and small-cell networks, enhancing data processing capabilities of user devices (UDs). Nevertheless, time-varying wireless channels, dynamic UD requirements, and severe interference among UDs make it difficult to fully exploit the limited network resources and stably provide computing services for UDs. Therefore, efficient task offloading and resource allocation (TORA) is essential. Moreover, since multiple small cells are deployed, decentralized TORA schemes are preferred in practice. Thus, this paper aims to design distributed adaptive TORA schemes for SE-MEC networks. In pursuit of an eco-friendly design, an optimization problem is formulated to minimize the total energy consumption (TEC) of UDs subject to delay constraints. To effectively deal with network's dynamic characteristics, the reinforce learning framework is applied, where the TEC minimization problem is first modeled as a partially observable Markov decision process (POMDP), and then an efficient multi-agent proximal policy optimization (MAPPO)-based scheme is presented to solve it. In the presented scheme, each small-cell base station (SBS) serves as an agent and is capable of making TORA decisions only with its own local information. To promote collaboration among multiple agents, a global reward function is designed. A state normalization mechanism is also introduced into the presented scheme for enhancing learning performance. Simulation results show that although the proposed MAPPO-based scheme works in a distributed manner, it achieves very similar performance to the centralized one. In addition, it is demonstrated that the state normalization mechanism has a significant effect on reducing TEC.},
  archive      = {J_TMC},
  author       = {Han Li and Ke Xiong and Yuping Lu and Wei Chen and Pingyi Fan and Khaled Ben Letaief},
  doi          = {10.1109/TMC.2024.3496536},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2346-2359},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative task offloading and resource allocation in small-cell MEC: A multi-agent PPO-based scheme},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAST: Efficient traffic scenario inpainting in cellular vehicle-to-everything systems. <em>TMC</em>, <em>24</em>(3), 2331-2345. (<a href='https://doi.org/10.1109/TMC.2024.3492148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a promising vehicular communication technology, Cellular Vehicle-to-Everything (C-V2X) is expected to ensure the safety and convenience of Intelligent Transportation Systems (ITS) by providing global road information. However, it is difficult to obtain global road information in practical scenarios since there will still be many vehicles on the road without onboard units (OBUs) in the near future. Specifically, although C-V2X vehicles have sensors that can perceive their surroundings and broadcast their perceived information to the C-V2X system, their line-of-sight (LoS) is limited and obscured by the environment, such as other vehicles and terrain. Besides, vehicles without OBUs cannot share their perceived information. These two problems cause extensive areas with unperceived information in the C-V2X system, and whether vehicles are in these areas is unknown. Thus, extending the perceivable range of the limited scenario for C-V2X applications that require global road information is necessary. To this end, this paper pioneers investigating the scenario inpainting task problem in C-V2X. To solve this challenging problem, we propose an effiCient trAffic Scenario inpainTing (CAST) solution consisting of a generative architecture and knowledge distillation, simultaneously considering the inpainting precision and computation efficiency. Extensive experiments have been conducted to demonstrate the effectiveness of CAST in terms of Precise Inpaint Rate (PIR), Rough Inpaint Rate (RIR), Lane-Level Inpaint Rate (LLIR), and Inpaint Confidence Error (ICE), paving the way for novel solutions for the inpainting problem in more complex road scenarios.},
  archive      = {J_TMC},
  author       = {Liang Zhao and Chaojin Mao and Shaohua Wan and Ammar Hawbani and Ahmed Y. Al-Dubai and Geyong Min and Albert Y. Zomaya},
  doi          = {10.1109/TMC.2024.3492148},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2331-2345},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CAST: Efficient traffic scenario inpainting in cellular vehicle-to-everything systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing semi-supervised federated learning with progressive training in heterogeneous edge computing. <em>TMC</em>, <em>24</em>(3), 2315-2330. (<a href='https://doi.org/10.1109/TMC.2024.3492140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an efficient distributed learning method that facilitates collaborative model training among multiple edge devices (or clients). However, current research always assumes that clients have access to ground-truth data for training, which is unrealistic in practice because of a lack of expertise. Semi-supervised federated learning (SSFL) has been proposed in many existing works to address this problem, which always adopts a fixed model architecture for training, bringing two main problems with varying amounts of pseudo-labeled data. First, the shallow model cannot have the capability to fit the increasing pseudo-labeled data, leading to poor training performance. Second, the large model suffers from an overfitting problem when exploiting a few labeled data samples in SSFL, and also requires tremendous resource (e.g., computation and communication) costs. To tackle these problems, we propose a novel framework, called star, which adopts progressive training to enhance model training in SSFL. Specifically, star gradually increases the model depth through adding the sub-module (e.g., one or several layers) from a shallow model, and performs pseudo-labeling for unlabeled data with a specialized confidence threshold simultaneously. Then, we propose an efficient algorithm to determine the appropriate model depth for each client with varied resource budgets and the proper confidence threshold for pseudo-labeling in SSFL. The experimental results demonstrate the high effectiveness of STAR. For instance, star can reduce the bandwidth consumption by about 40%, and achieve an average accuracy improvement of around 9.8% compared with the baselines, on CIFAR10.},
  archive      = {J_TMC},
  author       = {Jianchun Liu and Jun Liu and Hongli Xu and Yunming Liao and Zhiwei Yao and Min Chen and Chen Qian},
  doi          = {10.1109/TMC.2024.3492140},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2315-2330},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing semi-supervised federated learning with progressive training in heterogeneous edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise distribution decomposition based multi-agent distributional reinforcement learning. <em>TMC</em>, <em>24</em>(3), 2301-2314. (<a href='https://doi.org/10.1109/TMC.2024.3492272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generally, Reinforcement Learning (RL) agent updates its policy by repetitively interacting with the environment, contingent on the received rewards to observed states and undertaken actions. However, the environmental disturbance, commonly leading to noisy observations (e.g., rewards and states), could significantly shape the performance of agent. Furthermore, the learning performance of Multi-Agent Reinforcement Learning (MARL) is more susceptible to noise due to the interference among intelligent agents. Therefore, it becomes imperative to revolutionize the design of MARL, so as to capably ameliorate the annoying impact of noisy rewards. In this paper, we propose a novel decomposition-based multi-agent distributional RL method by approximating the globally shared noisy reward by a Gaussian Mixture Model (GMM) and decomposing it into the combination of individual distributional local rewards, with which each agent can be updated locally through distributional RL. Moreover, a Diffusion Model (DM) is leveraged for reward generation in order to mitigate the issue of costly interaction expenditure for learning distributions. Furthermore, the monotonicity of the reward distribution decomposition is theoretically validated under nonnegative weights and increasing distortion risk function, while the design of the loss function is carefully calibrated to avoid decomposition ambiguity. We also verify the effectiveness of the proposed method through extensive simulation experiments with noisy rewards. Besides, different risk-sensitive policies are evaluated in order to demonstrate the superiority of distributional RL in different MARL tasks.},
  archive      = {J_TMC},
  author       = {Wei Geng and Baidi Xiao and Rongpeng Li and Ning Wei and Dong Wang and Zhifeng Zhao},
  doi          = {10.1109/TMC.2024.3492272},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2301-2314},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Noise distribution decomposition based multi-agent distributional reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harvesting physical-layer randomness in millimeter wave bands. <em>TMC</em>, <em>24</em>(3), 2285-2300. (<a href='https://doi.org/10.1109/TMC.2024.3499876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unpredictability of the wireless channel has been used as a natural source of randomness to build physical-layer security primitives for shared key generation, authentication, access control, proximity verification, and other security properties. Compared to pseudo-random generators, it has the potential to achieve information-theoretic security. In sub-6 GHz frequencies, the randomness is harvested from the small-scale fading effects of RF signal propagation in rich scattering environments. However, the RF propagation characteristics follow sparse models with clustered paths when devices operate in millimeter-wave (mmWave) bands (5G and Next-Generation networks, Wi-Fi in 60GHz). Millimeter-wave transmissions are typically directional to increase the gain and combat high signal attenuation, leading to stable and more predictable channels. In this paper, we first demonstrate that state-of-the-art methods relying on channel state information or received signal strength measurements fail to produce high randomness. Accounting for the unique features of mmWave propagation, we propose a novel randomness extraction mechanism that exploits the random timing of channel blockage to harvest random bits. Compared with the prior art in CSI-based and context-based randomness extraction, our protocol remains secure against passive and active Man-in-the-Middle adversaries co-located with the legitimate devices. We demonstrate the security properties of our method in a 28 GHz mmWave testbed in an indoor setting.},
  archive      = {J_TMC},
  author       = {Ziqi Xu and Jingcheng Li and Yanjun Pan and Ming Li and Loukas Lazos},
  doi          = {10.1109/TMC.2024.3499876},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2285-2300},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Harvesting physical-layer randomness in millimeter wave bands},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual network computation offloading based on DRL for satellite-terrestrial integrated networks. <em>TMC</em>, <em>24</em>(3), 2270-2284. (<a href='https://doi.org/10.1109/TMC.2024.3493388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite-terrestrial integrated networks based on edge computing can provide computation offloading service to terminal devices in remote areas. However, it faces various limitations, including satellite energy consumption, computation delay, and environmental dynamics, etc. In this paper, we propose a satellite-terrestrial integrated cloud and edge computing network (STCECN) architecture, including satellite layer, terrestrial layer and cloud center, where computing resources exist in multi-layer heterogeneous edge computing clusters. Optimization of system delay and energy consumption is defined as a mixed-integer programming problem. Moreover, we present a deep reinforcement learning-based computation offloading decision algorithm that can adapt to the dynamics and variability of satellite networks. A dual network computation offloading decision method is proposed for delay and energy consumption based on deep reinforcement learning offloading (DRLO), including deep convolutional network update method, quantization strategy, and bandwidth resource allocation. Meanwhile, the proposed method is based on previous experience and integrates deviation adjustment strategies for decision making to solve the problem of pseudo-patch loss caused by satellite network switching. The simulation results indicate that the proposed method performs almost consistently with traditional heuristic algorithms, with only 20% of the time consumption of the latter, and the number of pseudo packet loss also decreases to the original 10–20%.},
  archive      = {J_TMC},
  author       = {Dongbo Li and Yuchen Sun and Jielun Peng and Siyao Cheng and Zhisheng Yin and Nan Cheng and Jie Liu and Zhijun Li and Chenren Xu},
  doi          = {10.1109/TMC.2024.3493388},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2270-2284},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dual network computation offloading based on DRL for satellite-terrestrial integrated networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient missing key tag identification in large-scale RFID systems: An iterative verification and selection method. <em>TMC</em>, <em>24</em>(3), 2253-2269. (<a href='https://doi.org/10.1109/TMC.2024.3493597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio frequency identification (RFID) system has been extensively employed to track missing items by affixing them with RFID tags. Many practical applications require to efficiently identify missing events for a specific subset of system tags (called key tags) due to their elevated importance. Existing methods primarily aim to identify all tags, which makes it challenging to specifically identify key tags because of interference from other non-key tags (called ordinary tags). In light of this, several key tag identification methods follow a two-step scheme that filters ordinary tags first and then identifies key tags. Nevertheless, this wastes too much time on tag filtering, resulting in low time efficiency. This paper presents a novel missing key tag identification protocol with two creative designs to gain high efficiency. First, we develop a novel verification technique that can rapidly determine the presence or absence of key tags amid the scenarios with both key tags and ordinary ones. By combining the ON-OFF Keying modulation, we could verify multiple key tags in a single slot, thereby reducing the total slots required. Second, we design a new selection technique that efficiently selects the unverified key tags for further verification, while filtering out the verified key tags and irrelevant ordinary tags to avoid redundant data transmission. Additionally, we present an enhancement protocol that leverages a preselection technique to avoid collecting useless tag responses, further boosting efficiency. We carry out rigorous theoretical analysis to optimize the performance of the proposed protocols. Both simulations and practical experiments demonstrate that our method is markedly superior to state-of-the-art solutions.},
  archive      = {J_TMC},
  author       = {Jiangjin Yin and Xin Xie and Hangyu Mao and Song Guo},
  doi          = {10.1109/TMC.2024.3493597},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2253-2269},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient missing key tag identification in large-scale RFID systems: An iterative verification and selection method},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving federated neural architecture search with enhanced robustness for edge computing. <em>TMC</em>, <em>24</em>(3), 2234-2252. (<a href='https://doi.org/10.1109/TMC.2024.3490835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of large-scale artificial intelligence services, edge devices are becoming essential providers of data and computing power. However, these edge devices are not immune to malicious attacks. Federated learning (FL), while protecting privacy of decentralized data through secure aggregation, struggles to trace adversaries and lacks optimization for heterogeneity. We discover that FL augmented with Differentiable Architecture Search (DARTS) can improve resilience against backdoor attacks while compatible with secure aggregation. Based on this, we propose a federated neural architecture search (NAS) framwork named SLNAS. The architecture of SLNAS is built on three pivotal components: a server-side search space generation method that employs an evolutionary algorithm with dual encodings, a federated NAS process based on DARTS, and client-side architecture tuning that utilizes Gumbel softmax combined with knowledge distillation. To validate robustness, we adapt a framework that includes backdoor attacks based on trigger optimization, data poisoning, and model poisoning, targeting both model weights and architecture parameters. Extensive experiments demonstrate that SLNAS not only effectively counters advanced backdoor attacks but also handles heterogeneity, outperforming defense baselines across a wide range of backdoor attack scenarios.},
  archive      = {J_TMC},
  author       = {Feifei Zhang and Mao Li and Jidong Ge and Fenghui Tang and Sheng Zhang and Jie Wu and Bin Luo},
  doi          = {10.1109/TMC.2024.3490835},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2234-2252},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-preserving federated neural architecture search with enhanced robustness for edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security-sensitive task offloading in integrated satellite-terrestrial networks. <em>TMC</em>, <em>24</em>(3), 2220-2233. (<a href='https://doi.org/10.1109/TMC.2024.3489619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of sixth-generation (6G) communication technology, global communication networks are moving towards the goal of comprehensive and seamless coverage. In particular, low earth orbit (LEO) satellites have become a critical component of satellite communication networks. The emergence of LEO satellites has brought about new computational resources known as the LEO satellite edge, enabling ground users (GU) to offload computing tasks to the resource-rich LEO satellite edge. However, existing LEO satellite computational offloading solutions primarily focus on optimizing system performance, neglecting the potential issue of malicious satellite attacks during task offloading. In this paper, we propose the deployment of LEO satellite edge in an integrated satellite-terrestrial networks (ISTN) structure to support security-sensitive computing task offloading. We model the task allocation and offloading order problem as a joint optimization problem to minimize task offloading delay, energy consumption, and the number of attacks while satisfying reliability constraints. To achieve this objective, we model the task offloading process as a Markov decision process (MDP) and propose a security-sensitive task offloading strategy optimization algorithm based on proximal policy optimization (PPO). Experimental results demonstrate that our algorithm significantly outperforms other benchmark methods in terms of performance.},
  archive      = {J_TMC},
  author       = {Wenjun Lan and Kongyang Chen and Jiannong Cao and Yikai Li and Ning Li and Qi Chen and Yuvraj Sahni},
  doi          = {10.1109/TMC.2024.3489619},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2220-2233},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Security-sensitive task offloading in integrated satellite-terrestrial networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ETBP-TD: An efficient and trusted bilateral privacy-preserving truth discovery scheme for mobile crowdsensing. <em>TMC</em>, <em>24</em>(3), 2203-2219. (<a href='https://doi.org/10.1109/TMC.2024.3489717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Crowdsensing (MCS) has emerged as a promising sensing paradigm for accomplishing large-scale tasks by leveraging ubiquitously distributed mobile workers. Due to the variability in sensory data provided by different workers, identifying truth values from them has garnered wide attention. However, existing truth discovery schemes either offer limited privacy protection or incur high participation costs and lower data aggregation quality due to malicious workers. In this paper, we propose an Efficient and Trusted Bilateral Privacy-preserving Truth Discovery scheme (ETBP-TD) to obtain high-quality truth values while preventing privacy leakage from both workers and the data requester. Specifically, a matrix encryption-based protocol is introduced to the whole truth discovery process, which keeps locations and data related to tasks and workers secret from other entries. Additionally, trust-based worker recruitment and trust update mechanisms are first integrated within a privacy-preserving truth discovery scheme to enhance truth value accuracy and reduce unnecessary participation costs. Our theoretical analyses on the security and regret of ETBP-TD, along with extensive simulations on real-world datasets, demonstrate that ETBP-TD effectively preserves workers’ and tasks’ privacy while reducing the estimated error by up to 84.40% and participation cost by 54.72%.},
  archive      = {J_TMC},
  author       = {Jing Bai and Jinsong Gui and Tian Wang and Houbing Song and Anfeng Liu and Neal N. Xiong},
  doi          = {10.1109/TMC.2024.3489717},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2203-2219},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ETBP-TD: An efficient and trusted bilateral privacy-preserving truth discovery scheme for mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive scheme for protecting source location privacy in underwater acoustic sensor networks. <em>TMC</em>, <em>24</em>(3), 2193-2202. (<a href='https://doi.org/10.1109/TMC.2024.3489722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the source location privacy (SLP) becomes a hot research interest in network security of Underwater Acoustic Sensor Networks (UASNs), and existing schemes are mostly proposed for a given scenario. Introducing source location privacy technologies inevitably increase the energy consumption of nodes, while they are widely deployed in available studies, resulting in massive energy wastage. Therefore, an adaptive scheme for protecting source location privacy (APSLP) in UASNs is proposed. The APSLP scheme first analyzes the possible locations of the adversary by trust method. Then, considering the lagging nature of the trust method, which means that the adversary may not stay in locations given by trust method, a hidden Markov-based backtracking method is proposed and location privacy methods are functioned according to the backtracking result. The simulation shows that even though the security level of the APSLP scheme is not the largest, the efficiency is the highest, approximately an increase of 69.1$\%$ and 10.3$\%$ compared with two comparison algorithms, respectively.},
  archive      = {J_TMC},
  author       = {Hao Wang and Huijuan Zheng and Guangjie Han and Dong Tang},
  doi          = {10.1109/TMC.2024.3489722},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2193-2202},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An adaptive scheme for protecting source location privacy in underwater acoustic sensor networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel method to solve the maximum weight clique problem for instantly decodable network coding. <em>TMC</em>, <em>24</em>(3), 2181-2192. (<a href='https://doi.org/10.1109/TMC.2024.3489724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Minimizing the decoding delay, the completion time, or the delivery time of instantly decodable network coding (IDNC) can all be approximated to a maximum weight clique (MWC) problem, which is well known to be NP hard. Due to its good tradeoff between performance and computational complexity, a heuristic approach named as maximum weight vertex (MWV) search is widely employed to select MWC for IDNC. However, in MWV, when there are few coding connection edges among the adjacent vertices of a vertex, its modified vertex weight cannot well reflect the weight of the MWC containing the vertex, which leads to incorrect selection of MWC. This paper proposes a new method to calculate the modified weight of a vertex by summing the weights of the vertices in the approximate maximum weight path (A-MWP) generated by this vertex. Since the vertices in an A-MWP can form a maximal clique, the proposed modified vertex weight may well indicate the weight of the MWC containing the vertex. The proposed algorithm has the same computational complexity as the MWV algorithm. Simulation results show that when employing any of the three performance metrics of IDNC, our proposed algorithm can achieve better system performance than the MWV algorithm.},
  archive      = {J_TMC},
  author       = {Zhonghui Mei},
  doi          = {10.1109/TMC.2024.3489724},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2181-2192},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A novel method to solve the maximum weight clique problem for instantly decodable network coding},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IFNet: Deep imaging and focusing for handheld SAR with millimeter-wave signals. <em>TMC</em>, <em>24</em>(3), 2166-2180. (<a href='https://doi.org/10.1109/TMC.2024.3489641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements have showcased the potential of handheld millimeter-wave (mmWave) imaging, which applies synthetic aperture radar (SAR) principles in portable settings. However, existing studies addressing handheld motion errors either rely on costly tracking devices or employ simplified imaging models, leading to impractical deployment or limited performance. In this paper, we present IFNet, a novel deep unfolding network that combines the strengths of signal processing models and deep neural networks to achieve robust imaging and focusing for handheld mmWave systems. We first formulate the handheld imaging model by integrating multiple priors about mmWave images and handheld phase errors. Furthermore, we transform the optimization processes into an iterative network structure for improved and efficient imaging performance. Extensive experiments demonstrate that IFNet effectively compensates for handheld phase errors and recovers high-fidelity images from severely distorted signals. In comparison with existing methods, IFNet can achieve at least 11.89 dB improvement in average peak signal-to-noise ratio (PSNR) and 64.91% improvement in average structural similarity index measure (SSIM) on a real-world dataset.},
  archive      = {J_TMC},
  author       = {Yadong Li and Dongheng Zhang and Ruixu Geng and Jincheng Wu and Yang Hu and Qibin Sun and Yan Chen},
  doi          = {10.1109/TMC.2024.3489641},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2166-2180},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {IFNet: Deep imaging and focusing for handheld SAR with millimeter-wave signals},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing IEEE 802.11ax network performance: An investigation and modeling into multi-user transmission. <em>TMC</em>, <em>24</em>(3), 2151-2165. (<a href='https://doi.org/10.1109/TMC.2024.3493032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the performance optimization of uplink orthogonal frequency division multiple access (OFDMA)-based random access (UORA) in IEEE 802.11ax networks. UORA supports multi-user transmission via two methods, where users transmit either fixed-size or variable-size aggregated MAC protocol data units. However, three critical issues arise. 1 Existing studies only focus on the fixed-size method with low practicality, and overlook the impact of traffic load which leads to inaccurate evaluation of the network performance. 2 The variable-size method has never been studied due to a complex scenario, where user frames append padding bits to fulfill the transmission opportunity constraint. 3 In realistic networks, the variable-size method sacrifices throughput to achieve high practicality and low latency. To address the first two issues, we proposed two novel models based on queueing theory that accurately capture the impact of these transmission methods and various parameters (e.g., the traffic load and padding bits) on throughput, packet loss rate, and latency. To address Issue 3, we design a Dynamic Selection Algorithm of Transmission Methods (DSATM), which dynamically switches between the two transmission methods to enhance practicality, maximize throughput, and minimize latency. Finally, we conducted extensive simulations to verify the accuracy of our models and DSATM.},
  archive      = {J_TMC},
  author       = {Jin Meng and Qinglin Zhao and Weimin Wu and Minghao Jin and Penghui Song and Yingzhuang Liu},
  doi          = {10.1109/TMC.2024.3493032},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2151-2165},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing IEEE 802.11ax network performance: An investigation and modeling into multi-user transmission},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A wireless AI-generated content (AIGC) provisioning framework empowered by semantic communication. <em>TMC</em>, <em>24</em>(3), 2137-2150. (<a href='https://doi.org/10.1109/TMC.2024.3493375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the significant advances in AI-generated content (AIGC) and the proliferation of mobile devices, providing high-quality AIGC services via wireless networks is becoming the future direction. However, the primary challenges of AIGC services provisioning in wireless networks lie in unstable channels, limited bandwidth resources, and unevenly distributed computational resources. To this end, this paper proposes a semantic communication (SemCom)-empowered AIGC (SemAIGC) generation and transmission framework, where only semantic information of the content rather than all the binary bits should be generated and transmitted by using SemCom. Specifically, SemAIGC integrates diffusion models within the semantic encoder and decoder to design a workload-adjustable transceiver thereby allowing adjustment of computational resource utilization in edge and local. In addition, a resource-aware workload trade-off (ROOT) scheme is devised to intelligently make workload adaptation decisions for the transceiver, thus efficiently generating, transmitting, and fine-tuning content as per dynamic wireless channel conditions and service requirements. Simulations verify the superiority of our proposed SemAIGC framework in terms of latency and content quality compared to conventional approaches.},
  archive      = {J_TMC},
  author       = {Runze Cheng and Yao Sun and Dusit Niyato and Lan Zhang and Lei Zhang and Muhammad Ali Imran},
  doi          = {10.1109/TMC.2024.3493375},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2137-2150},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A wireless AI-generated content (AIGC) provisioning framework empowered by semantic communication},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Underwater target tracking based on interrupted software-defined multi-AUV reinforcement learning: A multi-AUV time-saving MARL approach. <em>TMC</em>, <em>24</em>(3), 2124-2136. (<a href='https://doi.org/10.1109/TMC.2024.3490545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of underwater materials technology and underwater robot technology, human exploitation of marine resources has been increasingly advanced, which has given rise to various application scenarios for Autonomous Underwater Vehicle (AUV) cluster networks, such as cooperative data collection and target tracking. In this paper, we aim to explore how to utilize networking and swarm intelligence to improve the AUV cluster network’s target tracking performance in a time-saving manner. Specifically, on account of our previous work, we introduce an underwater interrupted mechanism and propose an Interrupted Software-Defined Multi-AUV Reinforcement Learning (ISD-MARL) architecture. For MARL algorithm in ISD-MARL, we propose a time-saving MARL algorithm, S-MADDPG, integrating our proposed action optimization model and action network loss function, to accelerate the convergence of the MARL algorithm. Furthermore, to further improve the AUV cluster network’s path planning performance during the target tracking, we propose an Interrupted Tracking Path Planning Scheme (ITPPS) for the AUV cluster network based on the proposed ISD-MARL and S-MADDPG. The evaluation results showcase that our proposed scheme can effectively plan the underwater target tracking path for the AUV cluster network in a shorter time and outperform various mainstream strategies in terms of convergence speed and training time, etc.},
  archive      = {J_TMC},
  author       = {Shengchao Zhu and Guangjie Han and Chuan Lin and Yu Zhang},
  doi          = {10.1109/TMC.2024.3490545},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2124-2136},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Underwater target tracking based on interrupted software-defined multi-AUV reinforcement learning: A multi-AUV time-saving MARL approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive sampling for age of information in non-stationary network traffic. <em>TMC</em>, <em>24</em>(3), 2110-2123. (<a href='https://doi.org/10.1109/TMC.2024.3493592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time status updates play an important role in low-latency cyber-physical systems, in which the real network traffic statistics (i.e., transmission delay and/or error rate) are often unknown and non-stationary. In such cases, short-time age-of-information (ST-AoI) is more crucial than long-term average AoI, because instantaneous high ST-AoI could lead to system failures even if the long-term average AoI is low. In this paper, we propose an adaptive sampling control (ASC) scheme to ensure a low ST-AoI outage probability, defined as the probability of the average AoI in each control cycle, i.e., over a limited number of packets, exceeding a given threshold. This ASC scheme does not rely on an explicit statistical model for the non-stationary traffic behaviors. It establishes a dynamic linearization data model with a pseudo-partial derivative (PPD) parameter to capture the unknown and non-stationary traffic statistics. By estimating the PPD parameter in each control cycle, ASC can determine the sampling rates to ensure an extremely low ST-AoI outage probability. Both numerical simulation and real-world experiment show that the proposed ASC scheme significantly outperforms existing methods, reducing the ST-AoI outage probability almost by half.},
  archive      = {J_TMC},
  author       = {Yifan Gu and Zhi Quan},
  doi          = {10.1109/TMC.2024.3493592},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2110-2123},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive sampling for age of information in non-stationary network traffic},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient satellite-ground interconnection design for low-orbit mega-constellation topology. <em>TMC</em>, <em>24</em>(3), 2098-2109. (<a href='https://doi.org/10.1109/TMC.2024.3490575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The low-orbit mega-constellation network (LMCN) is an important part of the space-air-ground integrated network system. An effective satellite-ground interconnection design can result in a stable constellation topology for LMCNs. A naïve solution is accessing the satellite with the longest remaining service time (LRST), which is widely used in previous designs. The Coordinated Satellite-Ground Interconnecting (CSGI), the state-of-the-art algorithm, coordinates the establishment of ground-satellite links (GSLs). Compared with existing solutions, it reduces latency by 19% and jitter by 70% on average. However, CSGI only supports the scenario where terminals access only one satellite, and cannot fully utilize the multi-access capabilities of terminals. Additionally, CSGI's high computational complexity poses deployment challenges. To overcome these problems, we propose the Classification-based Longest Remaining Service Time (C-LRST) algorithm. C-LRST supports the actual scenario with multi-access capabilities. It adds optional paths during routing with low computational complexity, improving end-to-end communications quality. We conduct our 1000 s simulation from Brazil to Lithuania on the open-source platform Hypatia. Experiment results show that compared with CSGI, C-LRST reduces the latency and increases the throughput by approximately 60% and 40%, respectively. In addition, C-LRST's GSL switchings number is 14, whereas CSGI is 23. C-LRST has better link stability than CSGI.},
  archive      = {J_TMC},
  author       = {Wenhao Liu and Jiazhi Wu and Quanwei Lin and Handong Luo and Qi Zhang and Kun Qiu and Zhe Chen and Yue Gao},
  doi          = {10.1109/TMC.2024.3490575},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2098-2109},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient satellite-ground interconnection design for low-orbit mega-constellation topology},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REM: Enabling real-time neural-enhanced video streaming on mobile devices using macroblock-aware lookup table. <em>TMC</em>, <em>24</em>(3), 2085-2097. (<a href='https://doi.org/10.1109/TMC.2024.3496443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for mobile video streaming has seen a substantial surge in recent years. However, current platforms heavily depend on network capacity to ensure the delivery of high-quality video streams. The emergence of neural-enhanced video streaming presents a promising solution to address this challenge by leveraging client-side computation, thereby reducing bandwidth consumption. Nonetheless, deploying advanced super-resolution (SR) models on mobile devices is hindered by the computational demands of existing SR models. In this paper, we propose REM, a novel neural-enhanced mobile video streaming framework. REM utilizes a customized lookup table to facilitate real-time neural-enhanced video streaming on mobile devices. Initially, we conduct a series of measurements to identify abundant macroblock redundancies across frames in a video stream. Subsequently, we introduce a dynamic macroblock selection algorithm that prioritizes important macroblocks for neural enhancement. The SR-enhanced results are stored in the lookup table and efficiently reused to meet real-time requirements and minimize resource overhead. By considering macroblock-level characteristics of the video frames, the lookup table enables efficient and fast processing. Additionally, we design a lightweight macroblock-aware SR module to expedite inference. Finally, we perform extensive experiments on various mobile devices. The results demonstrate that REM enhances overall processing throughput by up to 10.2 times and reduces power consumption by up to 58.6% compared to state-of-the-art methods. Consequently, this leads to a 38.06% improvement in the quality of experience for mobile users.},
  archive      = {J_TMC},
  author       = {Baili Chai and Di Wu and Jinyu Chen and Mengyu Yang and Zelong Wang and Miao Hu},
  doi          = {10.1109/TMC.2024.3496443},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2085-2097},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {REM: Enabling real-time neural-enhanced video streaming on mobile devices using macroblock-aware lookup table},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneity-aware cooperative federated edge learning with adaptive computation and communication compression. <em>TMC</em>, <em>24</em>(3), 2073-2084. (<a href='https://doi.org/10.1109/TMC.2024.3492916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the drawbacks of cloud-based federated learning (FL), cooperative federated edge learning (CFEL) has been proposed to improve efficiency for FL over mobile edge networks, where multiple edge servers collaboratively coordinate the distributed model training across a large number of edge devices. However, CFEL faces critical challenges arising from dynamic and heterogeneous device properties, which slow down the convergence and increase resource consumption. This paper proposes a heterogeneity-aware CFEL scheme called Heterogeneity-Aware Cooperative Edge-based Federated Averaging (HCEF) that aims to maximize the model accuracy while minimizing the training time and energy consumption via adaptive computation and communication compression in CFEL. By theoretically analyzing how local update frequency and gradient compression affect the convergence error bound in CFEL, we develop an efficient online control algorithm for HCEF to dynamically determine local update frequencies and compression ratios for heterogeneous devices. Experimental results show that compared with prior schemes, the proposed HCEF scheme can maintain higher model accuracy while reducing training latency and improving energy efficiency simultaneously.},
  archive      = {J_TMC},
  author       = {Zhenxiao Zhang and Zhidong Gao and Yuanxiong Guo and Yanmin Gong},
  doi          = {10.1109/TMC.2024.3492916},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2073-2084},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Heterogeneity-aware cooperative federated edge learning with adaptive computation and communication compression},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A “Breathing” mobile communication network. <em>TMC</em>, <em>24</em>(3), 2056-2072. (<a href='https://doi.org/10.1109/TMC.2024.3487213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The frequent migration of large-scale users leads to the load imbalance of mobile communication networks, which causes resource waste and decreases user experience. To address the load balancing problem, this paper proposes a dynamic optimization framework for mobile communication networks inspired by the average consensus in multi-agent systems. In this framework, all antennas cooperatively optimize their CPICH (Common Pilot Channel) transmit power in real-time to balance their busy-degrees. Then, the coverage area of each antenna would change accordingly, and we call this framework a “breathing” mobile communication network. To solve this optimization problem, two algorithms named BDBA (Busy-degree Dynamic Balancing Algorithm) and BFDBA (Busy-degree Fast Dynamic Balancing Algorithm) are proposed. Moreover, a fast network coverage calculation method is introduced, by which each antenna's minimum CPICH transmit power is determined under the premise of meeting the network coverage requirements. Besides, we present the theoretical analysis of the two proposed algorithms’ performance, which prove that all antennas’ busy-degrees will reach consensus under certain assumptions. Furthermore, simulations carried out on three large datasets demonstrate that our cooperative optimization can significantly reduce the unbalance among antennas as well as the proportion of over-busy antennas.},
  archive      = {J_TMC},
  author       = {Chao Ge and Ge Chen and Zhipeng Jiang},
  doi          = {10.1109/TMC.2024.3487213},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2056-2072},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A “Breathing” mobile communication network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). V2I-coop: Accurate object detection for connected automated vehicles at accident black spots with V2I cross-modality cooperation. <em>TMC</em>, <em>24</em>(3), 2043-2055. (<a href='https://doi.org/10.1109/TMC.2024.3486758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate object detection with on-board LiDAR sensors is crucial for ensuring driving safety of Connected Automated Vehicles (CAVs), especially at accident black spots with more occlusions. Fortunately, road-side infrastructure equipped with traffic cameras is usually available at these places, offers an extensive field of view and encounters fewer occlusions, and thus can provide sustained assistance to CAVs to improve their object detection performance. However, vehicle-to-infrastructure (V2I) cooperative object detection is quite challenging due to modality heterogeneity, agent heterogeneity, and bandwidth limitations. To address these challenges, in this paper, we propose V2I-Coop, an accurate object detection approach with V2I cross-modality cooperation for CAVs to improve perception performance at accident black spots. In V2I-Coop, first, we extract bird-eye-view (BEV) features from both multi-view 2D images and 3D point clouds, which facilitates the feature fusion of different modalities. Next, the most valuable features from the images are adaptively selected according to available bandwidth and then transmitted to CAVs. Then, a cross-modality feature fusion algorithm is adopted at CAVs to mitigate the modality difference and improve the feature fusion efficiency. Finally, extensive experiments demonstrate that V2I-Coop significantly improves the 3D object detection performance of CAVs at accident black spots.},
  archive      = {J_TMC},
  author       = {Xiaobo Zhou and Chuanan Wang and Qi Xie and Tie Qiu},
  doi          = {10.1109/TMC.2024.3486758},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2043-2055},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {V2I-coop: Accurate object detection for connected automated vehicles at accident black spots with V2I cross-modality cooperation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explanation-guided backdoor attacks against model-agnostic RF fingerprinting systems. <em>TMC</em>, <em>24</em>(3), 2029-2042. (<a href='https://doi.org/10.1109/TMC.2024.3487967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the proven capabilities of deep neural networks (DNNs) in identifying devices through radio frequency (RF) fingerprinting, the security vulnerabilities of these deep learning models have been largely overlooked. While the threat of backdoor attacks is well-studied in the image domain, few works have explored this threat in the context of RF signals. In this paper, we thoroughly analyze the susceptibility of DNN-based RF fingerprinting to backdoor attacks, focusing on a more practical scenario where attackers lack access to control model gradients and training processes. We propose leveraging explainable machine learning techniques and autoencoders to guide the selection of trigger positions and values, allowing for the creation of effective backdoor triggers in a model-agnostic manner. To comprehensively evaluate this backdoor attack, we employ four diverse datasets with two protocols (Wi-Fi and LoRa) across various DNN architectures. Given that RF signals are often transformed into the frequency or time-frequency domains, this study also assesses attack efficacy in the time-frequency domain. Furthermore, we experiment with potential detection and defense methods, demonstrating the difficulty of fully safeguarding against our proposed backdoor attack. Additionally, we consider the attack performance in the domain shift case.},
  archive      = {J_TMC},
  author       = {Tianya Zhao and Junqing Zhang and Shiwen Mao and Xuyu Wang},
  doi          = {10.1109/TMC.2024.3487967},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2029-2042},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Explanation-guided backdoor attacks against model-agnostic RF fingerprinting systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ClassTer: Mobile shift-robust personalized federated learning via class-wise clustering. <em>TMC</em>, <em>24</em>(3), 2014-2028. (<a href='https://doi.org/10.1109/TMC.2024.3487294'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of mobile devices with abundant sensor data and computing power has driven the trend of federated learning (FL) on them. Personalized FL (PFL) aims to train tailored models for each device, addressing data heterogeneity from diverse user behaviors and preferences. However, due to dynamic mobile environments, PFL faces challenges in test-time data shifts, i.e., variations between training and testing. While this issue is well studied in generic deep learning through model generalization or adaptation, this issue remains less explored in PFL, where models often overfit local data. To address this, we introduce ${\sf ClassTer}$, a shift-robust PFL framework. We observe that class-wise clustering of clients in cluster-based PFL (CFL) can avoid class-specific biases by decoupling the training of classes. Thus, we propose a paradigm shift from traditional client-wise clustering to class-wise clustering, which allows effective aggregation of cluster models into a generalized one via knowledge distillation. Additionally, we extend ClassTer to asynchronous mobile clients to optimize wall clock time by leveraging critical learning periods and both intra- and inter-device scheduling. Experiments show that compared to status quo approaches, ${\sf ClassTer}$ achieves a reduction of up to 91% in convergence time, and an improvement of up to 50.45% in accuracy.},
  archive      = {J_TMC},
  author       = {Xiaochen Li and Sicong Liu and Zimu Zhou and Yuan Xu and Bin Guo and Zhiwen Yu},
  doi          = {10.1109/TMC.2024.3487294},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {2014-2028},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ClassTer: Mobile shift-robust personalized federated learning via class-wise clustering},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing in-situ structural health monitoring through RF energy-powered sensor nodes and mobile platform. <em>TMC</em>, <em>24</em>(3), 1999-2013. (<a href='https://doi.org/10.1109/TMC.2024.3491574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research contributes to long-term structural health monitoring (SHM) by exploring radio frequency energy-powered sensor nodes (RF-SNs) embedded in concrete. The RF-SN captures radio energy from a mobile radio transmitter for sensing and communication, which offers a cost-effective solution for consistent in-situ perception. To optimize the system performance across various situations, we’ve explored both active and passive communication methods. For the active RF-SN, we implement a specialized control circuit enabling the node to transmit data through ZigBee protocol at low incident power. For the passive RF-SN, radio energy is not only for power but also as a carrier signal, with data conveyed by modulating the amplitude of the backscattered radio wave. To address the challenge of significant attenuation of the backscattering signal in concrete, we utilize a square chirp-based modulation scheme for passive communication. This scheme allows the receiver to successfully decode the data even under a negative signal-to-noise ratio (SNR) condition. Performance modeling and optimization for both active and passive RF-SNs are provided in this study. The experimental results verify that an active RF-SN embedded in concrete at a depth of 13.5 cm can be effectively powered by a 915 MHz mobile radio transmitter with an effective isotropic radiated power (EIRP) of 32.5 dBm. This setup allows the RF-SN to send over 1 kB of data within 10 seconds, with an additional 1.7 kilobytes every 1.6 seconds of extra charging. For the passive RF-SN buried at the same depth, continuous data transmission at a rate of 224 bps with a 3% bit error rate (BER) is achieved when the EIRP of the transmitter is 23.6 dBm.},
  archive      = {J_TMC},
  author       = {Yu Luo and Lina Pu and Jun Wang and Isaac L. Howard},
  doi          = {10.1109/TMC.2024.3491574},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1999-2013},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhancing in-situ structural health monitoring through RF energy-powered sensor nodes and mobile platform},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of trajectory, offloading, caching, and migration for UAV-assisted MEC. <em>TMC</em>, <em>24</em>(3), 1981-1998. (<a href='https://doi.org/10.1109/TMC.2024.3486995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV-assisted MEC revolutionizes edge computing by deploying UAVs for real-time data processing in areas lacking infrastructure, supporting a wide range of applications from emergency responses to smart cities. Unlike edge servers, UAVs face substantial computational constraints, necessitating a comprehensive strategy that integrates UAV trajectory with task offloading, caching, and migration. Existing studies often overlook the synergy among these strategies, impacting their overall effectiveness. Furthermore, the focus on content pre-caching overlooks task caching’s critical role in addressing high computational demands with limited UAV resources. This research aims to jointly optimize UAV trajectories and task management strategies, including offloading, caching, and migration. Utilizing the Lyapunov optimization framework, we break down the complex optimization problem into manageable subproblems: UAV placement, user-UAV association, task offloading, scheduling, and bandwidth allocation, addressed iteratively using the Block Coordinate Descent method. Specifically, the scheduling subproblem is transformed into a non-convex quadratically constrained quadratic programming problem, managed effectively through semidefinite relaxation and a probabilistic mapping approach. Our simulations show that this integrated approach significantly boosts system throughput and reduces execution times compared to conventional methods. This study enhances the understanding of the interplay between UAV trajectory planning and task management, offering vital theoretical insights for advancing UAV-assisted MEC systems.},
  archive      = {J_TMC},
  author       = {Mingxiong Zhao and Rongqian Zhang and Zhenli He and Keqin Li},
  doi          = {10.1109/TMC.2024.3486995},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1981-1998},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of trajectory, offloading, caching, and migration for UAV-assisted MEC},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient resource management for massive MIMO in high-density massive IoT networks. <em>TMC</em>, <em>24</em>(3), 1963-1980. (<a href='https://doi.org/10.1109/TMC.2024.3486712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive MIMO technology offers a promising solution for supporting the simultaneous connectivity of a large number of ultra high-density massive IoT devices. However, due to limited resources, effective channel estimation becomes a challenge. One approach is to reuse the orthogonal reference signal (ORS) in a repetitive manner, while another option is to employ random non-orthogonal reference signals (NORS) for distributed massive IoT devices. In order to enhance performance in the face of high RS congestion from massive IoT devices, the strategic utilization of several critical resources becomes imperative. In this paper, we delve into the methodology of resource management to effectively address the severe high-density scenario of massive IoT devices. Specifically, we examine the system bandwidth, which proves particularly advantageous in bandwidth-limited environments. Exploiting the spatial domain, we leverage the distinctive features of massive MIMO to enable simultaneous parallel transmission by increasing the number of service antennas at the base station (BS). Furthermore, we explore the potential benefits of sectorization, a technique that involves dividing a circular cell into multiple sectors, thereby reducing RS congestion. Nevertheless, it is crucial to acknowledge that increasing these resources may entail certain trade-offs and could potentially have adverse effects on overall system performance. To gain comprehensive insights, we conduct a thorough performance analysis under various scenarios, aiming to identify key characteristics that can facilitate the optimal operation of massive MIMO in high-density IoT environments. Building upon our findings, we devise an algorithm that efficiently manages resources, ultimately leading to improved system performance.},
  archive      = {J_TMC},
  author       = {Byung Moo Lee},
  doi          = {10.1109/TMC.2024.3486712},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1963-1980},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient resource management for massive MIMO in high-density massive IoT networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNN partitioning, task offloading, and resource allocation in dynamic vehicular networks: A lyapunov-guided diffusion-based reinforcement learning approach. <em>TMC</em>, <em>24</em>(3), 1945-1962. (<a href='https://doi.org/10.1109/TMC.2024.3486728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of Artificial Intelligence (AI) has introduced Deep Neural Network (DNN)-based tasks to the ecosystem of vehicular networks. These tasks are often computation-intensive, requiring substantial computation resources, which are beyond the capability of a single vehicle. To address this challenge, Vehicular Edge Computing (VEC) has emerged as a solution, offering computing services for DNN-based tasks through resource pooling via Vehicle-to-Vehicle/Infrastructure (V2V/V2I) communications. In this paper, we formulate the problem of joint DNN partitioning, task offloading, and resource allocation in VEC as a dynamic long-term optimization. Our objective is to minimize the DNN-based task completion time while guaranteeing the system stability over time. To this end, we first leverage a Lyapunov optimization technique to decouple the original long-term optimization with stability constraints into a per-slot deterministic problem. Afterwards, we propose a Multi-Agent Diffusion-based Deep Reinforcement Learning (MAD2RL) algorithm, incorporating the innovative use of diffusion models to determine the optimal DNN partitioning and task offloading decisions. Furthermore, we integrate convex optimization techniques into MAD2RL as a subroutine to allocate computation resources, enhancing the learning efficiency. Through simulations under real-world movement traces of vehicles, we demonstrate the superior performance of our proposed algorithm compared to existing benchmark solutions.},
  archive      = {J_TMC},
  author       = {Zhang Liu and Hongyang Du and Junzhe Lin and Zhibin Gao and Lianfen Huang and Seyyedali Hosseinalipour and Dusit Niyato},
  doi          = {10.1109/TMC.2024.3486728},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1945-1962},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DNN partitioning, task offloading, and resource allocation in dynamic vehicular networks: A lyapunov-guided diffusion-based reinforcement learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiteChain: A lightweight blockchain for verifiable and scalable federated learning in massive edge networks. <em>TMC</em>, <em>24</em>(3), 1928-1944. (<a href='https://doi.org/10.1109/TMC.2024.3488746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging blockchain in Federated Learning (FL) emerges as a new paradigm for secure collaborative learning on Massive Edge Networks (MENs). As the scale of MENs increases, it becomes more difficult to implement and manage a blockchain among edge devices due to complex communication topologies, heterogeneous computation capabilities, and limited storage capacities. Moreover, the lack of a standard metric for blockchain security becomes a significant issue. To address these challenges, we propose a lightweight blockchain for verifiable and scalable FL, namely LiteChain, to provide efficient and secure services in MENs. Specifically, we develop a distributed clustering algorithm to reorganize MENs into a two-level structure to improve communication and computing efficiency under security requirements. Moreover, we introduce a Comprehensive Byzantine Fault Tolerance (CBFT) consensus mechanism and a secure update mechanism to ensure the security of model transactions through LiteChain. Our experiments based on Hyperledger Fabric demonstrate that LiteChain presents the lowest end-to-end latency and on-chain storage overheads across various network scales, outperforming the other two benchmarks. In addition, LiteChain exhibits a high level of robustness against replay and data poisoning attacks.},
  archive      = {J_TMC},
  author       = {Handi Chen and Rui Zhou and Yun-Hin Chan and Zhihan Jiang and Xianhao Chen and Edith C. H. Ngai},
  doi          = {10.1109/TMC.2024.3488746},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1928-1944},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LiteChain: A lightweight blockchain for verifiable and scalable federated learning in massive edge networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing power and battery charging management for solar energy powered edge computing. <em>TMC</em>, <em>24</em>(3), 1913-1927. (<a href='https://doi.org/10.1109/TMC.2024.3489028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of energy harvesting capabilities into mobile edge computing (MEC) edge servers enables their deployment beyond the reach of electrical grids, expanding MEC services to isolated regions and geographically challenging terrains. However, the fluctuating nature of renewable energy sources, such as solar and wind, necessitates dynamic management of server computing power in response to variable energy harvesting rates. Unlike conventional models that assume predetermined amounts of harvested energy per time period, this study illustrates the complex interdependencies between server power consumption and variable energy harvesting rates due to battery charging characteristics. To address this, we introduce a novel energy harvesting model that comprehensively accounts for the interaction between computing power management and energy harvesting rates. We develop both offline and online offline optimal computing power management strategies aimed at maximizing the average computational capacity of edge servers. An analytical solution to the resulting nonlinear optimization problem is provided to determine the optimal computing power configurations. Simulation results indicate that the proposed strategy effectively balances energy harvesting rates and energy utilization, thereby enhancing computational performance in dynamic energy environments.},
  archive      = {J_TMC},
  author       = {Yu Luo and Lina Pu and Chun-Hung Liu},
  doi          = {10.1109/TMC.2024.3489028},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1913-1927},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Computing power and battery charging management for solar energy powered edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SigCan: Toward reliable ToF estimation leveraging multipath signal cancellation on commodity WiFi devices. <em>TMC</em>, <em>24</em>(3), 1895-1912. (<a href='https://doi.org/10.1109/TMC.2024.3491337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread deployment of WiFi infrastructure has facilitated the development of Time-of-Flight (ToF) based sensing applications. ToF estimation, however, is a challenging task due to the complexity of multipath effect. In this paper, we propose a phase difference based method for ToF estimation and uncover the potential of signal cancellation to mitigate the impact of multipath and noise on phase differences among subcarriers. To separate the moving target path from the complex multipath for ToF estimation, we suggest employing specific elimination methods tailored to the characteristics of different signal components. For dynamic multipath, we observe that when a given subcarrier propagates along two paths to the receiver, with path lengths differing by half a wavelength, the phase difference introduced by these two paths cancels each other out. Therefore, we propose two metrics to identify signals that satisfy this condition, utilizing both frequency diversity and spatial diversity. Additionally, we propose leveraging time diversity to eliminate the static multipath component and reduce the impact of noise. We implemented the methods with off-the-shelf WiFi devices and achieved mean errors of 15.36 cm and 21.05 cm for distance estimation in outdoor and indoor scenarios, outperforming state-of-the-art ToF estimation method by 50% error reduction.},
  archive      = {J_TMC},
  author       = {Yang Li and Dan Wu and Jiahe Chen and Weiyan Shi and Leye Wang and Lu Su and Wenwei Li and Daqing Zhang},
  doi          = {10.1109/TMC.2024.3491337},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1895-1912},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SigCan: Toward reliable ToF estimation leveraging multipath signal cancellation on commodity WiFi devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HGExplainer: Heterogeneous graph explainer for IoT device identification. <em>TMC</em>, <em>24</em>(3), 1877-1894. (<a href='https://doi.org/10.1109/TMC.2024.3486717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoT device identification is vital for network asset and security management. However, existing methods use statistical features that can not identify IoT devices accurately in complex network environments. GraphIoT proposes using non-statistical features and building a heterogeneous graph neural network to identify IoT devices accurately. However, heterogeneous graph neural networks lack interpretability, which reduces trust in the model. Besides, it is difficult to deploy on resource-constrained devices, limiting the broad application of IoT device identification. To make IoT device identification interpretable, easy to deploy, and with high accuracy, we get the interpretation results of GraphIoT through interpretability and further build the rule set based on the interpretation results. Considering there is no suitable interpreter for GraphIoT with many nodes and edges, we propose HGExplainer, which reduces the time complexity by splitting the interpretation target into important relation solving and edge solving and uses a novel solution method, ExpandTree. Then, we also designed a rule extractor, which can build rule sets based on the interpretation results. Experimental results on Yourthings and UNSW datasets show that HGExplainer can build high fidelity, concise sample-level explanations in less than 3 seconds, and the established rule set can precisely identify IoT devices.},
  archive      = {J_TMC},
  author       = {Linna Fan and Bo Wu and Xuan Shen and Jun He and Guanglei Song and Gang Yang and Chaocan Xiang and Duohe Ma and Yongfeng Huang},
  doi          = {10.1109/TMC.2024.3486717},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1877-1894},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {HGExplainer: Heterogeneous graph explainer for IoT device identification},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defending data poisoning attacks in DP-based crowdsensing: A game-theoretic approach. <em>TMC</em>, <em>24</em>(3), 1859-1876. (<a href='https://doi.org/10.1109/TMC.2024.3486689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential privacy (DP) is widely used for protecting privacy in crowdsensing by adding noises. However, malicious attackers can exploit noise to launch covert data poisoning attacks. In this paper, we propose a game-based defense approach to resist such data poisoning attacks in DP-based crowdsensing systems. In this approach, attackers are believed to be powerful as they can refine their attack strategy based on the observations of deployed defenders’ defense strategy. Specifically, the defenders formulate the defense as a functional minimization problem (which cannot be directly solved by numerical optimization algorithms because its decision variable is a set of functions), resisting data poisoning attacks by deleting data shared by identified malicious workers through the log-likelihood ratio test. To obtain a current defense strategy, the decision variable of the problem is relaxed into the coefficients of basis-based linear combinations through the variable-basis approximation, and then solved using the simulated annealing genetic algorithm. Correspondingly, the attackers formulate their attack strategy as a bi-level maximization problem (which is an NP-hard problem), biasing crowdsensing results as much as possible while remaining undetected. Since the attackers can know the defense strategy, they may bypass the defenders by constraining the expected log-likelihood ratio test. Additionally, the attackers can evade truth discovery methods deployed in crowdsensing using DP noise. To determine a current attack strategy, the bi-level problem is decomposed into upper-level and lower-level sub-problems, wherein the upper-level sub-problem is solved by the variational methods, and then these sub-problems are alternately optimized. Finally, we propose a local minimax points calculating algorithm to obtain an equilibrium point in the defenders-attackers game, thereby finding an optimal defense strategy to resist the powerful data poisoning attack. Extensive experiments on real-world and synthetic datasets show that the proposed game-based defense approach can effectively defend powerful and covert attackers.},
  archive      = {J_TMC},
  author       = {Zhirun Zheng and Zhetao Li and Cheng Huang and Saiqin Long and Xuemin Shen},
  doi          = {10.1109/TMC.2024.3486689},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1859-1876},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Defending data poisoning attacks in DP-based crowdsensing: A game-theoretic approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking cost-efficient VM scheduling on public edge platforms: A service provider’s perspective. <em>TMC</em>, <em>24</em>(3), 1846-1858. (<a href='https://doi.org/10.1109/TMC.2024.3488082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior studies on traditional centralized clouds independently optimizing static resource utilization and dynamic bandwidth cost are not applicable to edge scenarios, where edge sites are interconnected by wide area networks (WAN) rather than local area networks (LAN) as within clouds. Due to the lack of knowledge about the actual status of public edge platforms and real-world edge datasets, existing influential literature on edge scenarios demonstrates significant disparities in optimization objectives and perspectives. To bridge this gap, we collaborate with a public edge platform and perform a comprehensive measurement, which reveals limitations of the status quo VM scheduling schemes and potential opportunities for improvement. However, resolving VM scheduling considering static resource utilization, dynamic bandwidth cost, and end users’ QoE in a cost-efficient manner faces several challenges, including coupled objectives, exponentially increased complexity, and spatiotemporal dynamics. To address the above challenges, in this work, we propose a holistic online framework that integrates combinatorial bandit-based VM migration and seasonality-aware VM request allocation at two distinct time granularities. Large-scale experiments based on a real-world dataset confirm that our online framework achieves near-offline bandwidth cost and resource utilization while significantly lowering time consumption.},
  archive      = {J_TMC},
  author       = {Yanan Li and Xiao Ma and Zhe Fu and Ao Zhou and Mengwei Xu and Shangguang Wang},
  doi          = {10.1109/TMC.2024.3488082},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1846-1858},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Rethinking cost-efficient VM scheduling on public edge platforms: A service provider’s perspective},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mighty: Towards long-range and high-throughput backscatter for drones. <em>TMC</em>, <em>24</em>(3), 1833-1845. (<a href='https://doi.org/10.1109/TMC.2024.3486993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While small drone video streaming systems create unprecedented video content, they also place a power burden exceeding 20% on the drone's battery, limiting flight endurance. We present ${\sf Mighty}$, a hardware-software solution to minimize the power consumption of a drone's video streaming system by offloading power overheads associated with both video compression and transmission to a ground controller. ${\sf Mighty}$ innovates a high performance co-design among: (1) a ring oscillator-based, ultra-low power backscatter radio; (2) a spectrally-efficient, non-linear, low-power physical layer modulation and multi-chain radio architecture; and (3) a lightweight video compression codec-bypassing software design. Our co-design exploits synergies among these components, resulting in joint throughput and range performance that pushes the known envelope. We prototype ${\sf Mighty}$ on PCB board and conduct extensive field studies both indoors and outdoors. The power efficiency of ${\sf Mighty}$ is about 16.6 nJ/bit. A head-to-head comparison with a DJI Mini2 drone's default video streaming system shows that ${\sf Mighty}$ achieves similar throughput at a drone-to-controller distance of up to 150 meters, with 34–55× improvement of power efficiency than WiFi-based video streaming solutions.},
  archive      = {J_TMC},
  author       = {Xiuzhen Guo and Yuan He and Longfei Shangguan and Yande Chen and Chaojie Gu and Yuanchao Shu and Kyle Jamieson and Jiming Chen},
  doi          = {10.1109/TMC.2024.3486993},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1833-1845},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mighty: Towards long-range and high-throughput backscatter for drones},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving contact query processing over trajectory data in mobile cloud computing. <em>TMC</em>, <em>24</em>(3), 1818-1832. (<a href='https://doi.org/10.1109/TMC.2024.3488728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the expansion of mobile devices and cloud computing, massive spatial trajectory data is generated and outsourced to the cloud for storage and analysis, enabling location-based mobile computing services. However, due to the sensitivity of the trajectory data, sharing it in plaintext could lead to privacy risks, especially in operations like contact queries. Thus, achieving secure and efficient contact queries based on the trajectory data in the cloud is a significant challenge. In this paper, we propose a privacy-preserving contact query processing over trajectory data in mobile cloud computing. The projection-based secure trajectory encoding is designed to convert trajectories into secure codes such that the comparison between the distance of two moving objects and the contact distance threshold is transformed into a problem of secure code matching. Adopting the secure code matching method, a baseline privacy-preserving contact query processing is proposed. To improve the query accuracy and efficiency, an amplification factor, an HTG-index and a filter table are designed for query processing optimization, based on which an enhanced privacy-preserving contact query processing is proposed. The game stimulation-based security analysis and experimental results show that the proposed query scheme is secure and performs well in query accuracy and efficiency.},
  archive      = {J_TMC},
  author       = {Qu Lu and Hua Dai and Pengyue Li and Shuyan Wan and Geng Yang and Yang Xiang and Fu Xiao},
  doi          = {10.1109/TMC.2024.3488728},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1818-1832},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-preserving contact query processing over trajectory data in mobile cloud computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving single-sign-on with fine-grained access control for IoT devices. <em>TMC</em>, <em>24</em>(3), 1805-1817. (<a href='https://doi.org/10.1109/TMC.2024.3486719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoT-based sharing economy is a win-win business model, where a transferor owns idle IoT devices and transfers the right to use a device to a user for a fee. Considering usage of multiple devices and privacy preservation, anonymous single-sign-on (ASSO) is a feasible solution for authentication. ASSO allows a user to access multiple devices with one token issued by the transferor and prevents the transferor from identifying the user. We also observe that in the scenario of IoT-based sharing economy, the token should (i) support attributes since a device should be available only to users with specific attributes (e.g., age) and (ii) avoid incurring significant communication/computation overhead as IoT devices are resource-constrained. In this paper, we proposed PILOT, a privacy-preserving single-sign-on with fine-grained access control for IoT devices. When a user attempts to access a device, he/she requests a token from the transferor. The token is actually a blind signature that cannot be tracked, and contains the user’s attributes which facilitate fine-grained access control on the device. Besides, the token consists of only four group elements and verification of the token involves only several exponentiation operations. This renders PILOT superior in terms of communication/computation overhead and suitable for IoT devices.},
  archive      = {J_TMC},
  author       = {Zhao Zhang and Chunxiang Xu and Man Ho Allen Au and Changsong Jiang},
  doi          = {10.1109/TMC.2024.3486719},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1805-1817},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Privacy-preserving single-sign-on with fine-grained access control for IoT devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A long-term-planning learning strategy to coordinate viewport prediction and video transmission in 360° video streaming. <em>TMC</em>, <em>24</em>(3), 1792-1804. (<a href='https://doi.org/10.1109/TMC.2024.3487998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fueled by Metaverse, 360° video streaming has seen tremendous growth in the past years. However, our measurement reveals that current 360° streaming systems suffer from a dilemma that severely limits QoE. On the one hand, viewport prediction requires the shortest possible prediction distance for high predicting accuracy; On the other hand, video transmission requires more buffered data to compensate for bandwidth fluctuations otherwise substantial playback rebuffering would be incurred. There is so far no existing method that can break this dilemma so the QoE optimization for 360° video streaming was naturally bottlenecked. This work is the first attempt to tackle this challenge by developing QUTA – a novel learning-based streaming system. Specifically, according to our measurement, three kinds of internal streaming parameters have significant impacts on the prediction distance, namely, download pause, data rate threshold, and playback rate. On top of this, we design a new long-term-planning (LTP) continuous control deep reinforcement learning method that tunes the parameters dynamically based on the network condition and the streaming context. Extensive evaluations based on real system prototypes show that QUTA not only improves the prediction accuracy and QoE performance by up to 68.4% but also exhibits strong temporal and spatial robustness.},
  archive      = {J_TMC},
  author       = {Guanghui Zhang and Jing Guo and Mengbai Xiao and Dongxiao Yu and Vaneet Aggarwal and Xiuzhen Cheng},
  doi          = {10.1109/TMC.2024.3487998},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1792-1804},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A long-term-planning learning strategy to coordinate viewport prediction and video transmission in 360° video streaming},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PipeSFL: A fine-grained parallelization framework for split federated learning on heterogeneous clients. <em>TMC</em>, <em>24</em>(3), 1774-1791. (<a href='https://doi.org/10.1109/TMC.2024.3489642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Split Federated Learning (SFL) improves scalability of Split Learning (SL) by enabling parallel computing of the learning tasks on multiple clients. However, state-of-the-art SFL schemes neglect the effects of heterogeneity in the clients’ computation and communication performance as well as the computation time for the tasks offloaded to the cloud server. In this paper, we propose a fine-grained parallelization framework, called PipeSFL, to accelerate SFL on heterogeneous clients. PipeSFL is based on two key novel ideas. First, we design a server-side priority scheduling mechanism to minimize per-iteration time. Second, we propose a hybrid training mode to reduce per-round time, which employs asynchronous training within rounds and synchronous training between rounds. We theoretically prove the optimality of the proposed priority scheduling mechanism within one round and analyze the total time per round for PipeSFL, SFL and SL. We implement PipeSFL on PyTorch. Extensive experiments on seven 64-client clusters with different heterogeneity demonstrate that at training speed, PipeSFL achieves up to 1.65x and 1.93x speedup compared to EPSL and SFL, respectively. At energy consumption, PipeSFL saves up to 30.8% and 43.4% of the energy consumed within each training round compared to EPSL and SFL, respectively.},
  archive      = {J_TMC},
  author       = {Yunqi Gao and Bing Hu and Mahdi Boloursaz Mashhadi and Wei Wang and Mehdi Bennis},
  doi          = {10.1109/TMC.2024.3489642},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1774-1791},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PipeSFL: A fine-grained parallelization framework for split federated learning on heterogeneous clients},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepSelector: A deep learning-based virtual network function placement approach in SDN/NFV-enabled networks. <em>TMC</em>, <em>24</em>(3), 1759-1773. (<a href='https://doi.org/10.1109/TMC.2024.3483779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of Software-Defined Networks (SDN) and Network Function Virtualization (NFV) has popularized the adoption of the Service Function Chain (SFC) paradigm for efficient network service delivery. This paradigm leverages the flexibility and cost-effectiveness of deploying Virtual Network Functions (VNFs) as software entities or virtual machines on off-the-shelf servers. Chaining VNFs together allows traffic to be directed through the network as required. However, existing algorithms for traffic steering and routing path computation in SFC suffer from many challenges, including complexity, lack of scalability, and low time efficiency. This paper focuses on addressing the challenges associated with VNF placement and SFC chaining in SDN/NFV-enabled networks. Our objective is to identify an optimal solution for VNF placement that maximizes the utilization of network resources. We formulate the problem as a Binary Integer Programming (BIP) model to accomplish this. Additionally, we propose a novel algorithm called DeepSelector, which incorporates deep learning techniques and an intelligent node selection network to determine the optimal placement of VNFs for SFC requests. Through performance evaluation, we demonstrate that DeepSelector achieves high network resource utilization and offers efficient VNF placement computation, significantly improving overall network performance.},
  archive      = {J_TMC},
  author       = {Yi Yue and Xiongyan Tang and Ying-Chang Liang and Chang Cao and Lexi Xu and Wencong Yang and Zhiyan Zhang},
  doi          = {10.1109/TMC.2024.3483779},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1759-1773},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeepSelector: A deep learning-based virtual network function placement approach in SDN/NFV-enabled networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepRP: Bottleneck theory guided relay placement for 6G mesh backhaul augmentation. <em>TMC</em>, <em>24</em>(3), 1744-1758. (<a href='https://doi.org/10.1109/TMC.2024.3487020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backhaul mesh networks are critical for ensuring coverage and connectivity of high-frequency 6G networks. To maintain high throughput, its architecture needs to be augmented by adding relays. However, how to place relays at appropriate sites poses two challenges: 1) there lacks a theory to capture the relationship between a certain change of network architecture and its throughput gain; 2) selecting the best sites for relays is a complicated combinatorial problem. To tackle the first challenge, this paper first establishes a clique-based bottleneck theory, through which a clique-based bottleneck structure of a given network architecture is constructed to determine the network throughput. Based on this bottleneck structure, clique gradients are then computed to quantify the impact of each clique on the overall network throughput. With the clique-based bottleneck theory, the second challenge is resolved by embedding clique gradients into a deep reinforcement learning (DRL) scheme. Specifically, the DRL actions are masked such that only the relay sites that match the highest clique gradients are selected. This DRL-based relay placement (DeepRP) scheme is evaluated via extensive simulations, and performance results show that it can boost network throughput by more than 50%, which is $\text{10.4} \!-\! \text{32.1}\% $ higher than those of baseline schemes.},
  archive      = {J_TMC},
  author       = {Tianxin Wang and Xudong Wang},
  doi          = {10.1109/TMC.2024.3487020},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1744-1758},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DeepRP: Bottleneck theory guided relay placement for 6G mesh backhaul augmentation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FCER: A federated cloud-edge recommendation framework with cluster-based edge selection. <em>TMC</em>, <em>24</em>(3), 1731-1743. (<a href='https://doi.org/10.1109/TMC.2024.3484493'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional recommendation system provides web services by modeling user behavior characteristics, which also faces the risk of leaking user privacy. To mitigate the rising concern on privacy leakage in recommender systems, federated learning (FL) based recommendation has received tremendous attention, which can preserve data privacy by conducting local model training on clients. However, devices (e.g., mobile phones) used by clients in a recommender system may have limited capacity for computation and communication, which can severely deteriorate FL training efficiency. Besides, offloading local training tasks to the cloud can lead to privacy leakage and excessive pressure to the cloud. To overcome this deficiency, we propose a novel federated cloud-edge recommendation framework, which is called FCER, by offloading local training tasks to powerful and trusted edge servers. The challenge of FCER lies in the heterogeneity of edge servers, which makes the parameter server (PS) deployed in the cloud face difficulty in judiciously selecting edge servers for model training. To address this challenge, we divide the FCER framework into two stages. In the first pre-training stage, edge servers expose their data statistical features protected by local differential privacy (LDP) to the PS so that edge servers can be grouped into clusters. In the second training stage, FCER activates a single cluster in each communication round, ensuring that edge servers with statistical homogenization are not repeatedly involved in FL. The PS only selects a certain number of edge servers with the highest data quality in each cluster for FL. Effective metrics are proposed to dynamically evaluate the data quality of each edge server. Convergence rate analysis is conducted to show the convergence of recommendation algorithms in FCER. We also perform extensive experiments to demonstrate that FCER remarkably outperforms competitive baselines by $3.85\%-9.14\%$ on HR@10 and $1.46\%-11.77\%$ on NDCG@10.},
  archive      = {J_TMC},
  author       = {Jiang Wu and Yunchao Yang and Miao Hu and Yipeng Zhou and Di Wu},
  doi          = {10.1109/TMC.2024.3484493},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1731-1743},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FCER: A federated cloud-edge recommendation framework with cluster-based edge selection},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model poisoning attack against neural network interpreters in IoT devices. <em>TMC</em>, <em>24</em>(3), 1715-1730. (<a href='https://doi.org/10.1109/TMC.2024.3486218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network models have become integral to Internet of Things (IoT) systems, with applications spanning from industrial automation to critical infrastructure management. Despite their prevalence, the deployment of these models within IoT systems introduces distinctive security vulnerabilities. In particular, adversaries may execute model poisoning attacks, which aim to alter the decision-making processes of embedded models, leading to erroneous outcomes. Existing model poisoning attacks necessitate access to extensive auxiliary datasets, such as the training dataset itself or one with same distribution. These requirements often render such attacks impractical in IoT contexts, given the constrained storage and computational resources of IoT devices. This paper proposes the first model poisoning attack against interpreters without auxiliary datasets to manipulate the model’s behavior. We evaluate the attack on three real-world datasets, and results indicate that this attack can successfully coerce the targeted interpreters to produce outcomes aligned with an adversary’s intentions, while maintaining nearly indistinguishable performance from the original model, thereby ensuring its stealthiness. Furthermore, beyond directly affected interpreters, our experiments reveal that four additional interpreters coupled to the poisoned model are indirectly influenced, underscoring the attack’s transferability.},
  archive      = {J_TMC},
  author       = {Xianglong Zhang and Feng Li and Huanle Zhang and Haoxin Zhang and Zhijian Huang and Lisheng Fan and Xiuzhen Cheng and Pengfei Hu},
  doi          = {10.1109/TMC.2024.3486218},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1715-1730},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Model poisoning attack against neural network interpreters in IoT devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LoRadar: An efficient LoRa channel occupancy acquirer based on cross-channel scanning. <em>TMC</em>, <em>24</em>(3), 1699-1714. (<a href='https://doi.org/10.1109/TMC.2024.3487835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LoRa is widely deployed for various applications. Though the knowledge of the channel occupancy is the prerequisite of many aspects of network management, acquiring the channel occupancy for LoRa is challenging due to the large number of possible channels. In this paper, we propose ${\sf LoRadar}$, a novel LoRa channel occupancy acquirer based on cross-channel scanning. Our in-depth study finds that Channel Activity Detection (CAD) in a narrow band can indicate the channel activities of wide bands because they have the same slope in the time-frequency domain. Based on this finding, we design a cross-channel scanning mechanism that infers the channel occupancy states of all the overlapping channels by the distribution of CAD results. We elaborately select and adjust the CAD settings to enhance the distribution features and design a pattern correction method to cope with distribution distortions. We also design a CAD scheduler to deal with the low duty-cycle LoRa operations. We implement ${\sf LoRadar}$ on commercial LoRa platforms and evaluate its performance in the indoor testbed and two outdoor deployed networks. The experimental results show that ${\sf LoRadar}$ can achieve a detection accuracy of 0.99 and reduce the acquisition overhead by up to 90%, compared to the traversal-based methods.},
  archive      = {J_TMC},
  author       = {Xiaolong Zheng and Fu Yu and Liang Liu and Huadong Ma},
  doi          = {10.1109/TMC.2024.3487835},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1699-1714},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LoRadar: An efficient LoRa channel occupancy acquirer based on cross-channel scanning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time continuous activity recognition with a commercial mmWave radar. <em>TMC</em>, <em>24</em>(3), 1684-1698. (<a href='https://doi.org/10.1109/TMC.2024.3483813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {mmWave-based activity recognition technology has attracted widespread attention as it provides the ability of device-free, ubiquitous and accurate sensing. Recognition of human activities intrinsically demands to be real-time and continuous, but the state of the arts is still far limited with the capacity in this regard. The main obstacle lies in activity sequence segmentation, i.e., locating the boundaries between consecutive activities in an activity sequence. This is a daunting task, due to the unclear activity boundaries and the variable activity duration. In this paper, we propose ZuMa, the first mmWave-based approach to real-time continuous activity recognition. When resorting to a machine learning model for activity recognition, our insight is that the recognition confidence of the recognition model is highly correlated to the accuracy of activity sequence segmentation, so that the former can be utilized as a feedback metric to finely adjust the segmentation boundaries. Based on this insight, ZuMa is a coarse-to-fine grained approach, which includes the fast coarse-grained activity chunk extraction and the find-grained explicit segmentation adjustment and recognition. We have implemented ZuMa with the commercial mmWave radar and evaluated its performance under various settings. The results demonstrate that ZuMa achieves an average recognition error of 12.67%, which is 65.08% and 71.87% lower than that of the two baseline methods. The average recognition delay of ZuMa is only 1.86 s.},
  archive      = {J_TMC},
  author       = {Yunhao Liu and Jia Zhang and Yande Chen and Weiguo Wang and Songzhou Yang and Xin Na and Yimiao Sun and Yuan He},
  doi          = {10.1109/TMC.2024.3483813},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1684-1698},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time continuous activity recognition with a commercial mmWave radar},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin backed closed-loops for energy-aware and open RAN-based fixed wireless access serving rural areas. <em>TMC</em>, <em>24</em>(3), 1669-1683. (<a href='https://doi.org/10.1109/TMC.2024.3482985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet access in rural areas should be improved to support digital inclusion and 5G services. Due to the high deployment costs of fiber optics in these areas, Fixed Wireless Access (FWA) has become a preferable alternative. Additionally, the Open Radio Access Network (O-RAN) can facilitate the interoperability of FWA elements, allowing some FWA functions to be deployed at the edge cloud. However, deploying edge clouds in rural areas can increase network and energy costs. To address these challenges, we propose a closed-loop system assisted by a Digital Twin (DT) to automate energy-aware O-RAN based FWA resource management in rural areas. We consider the FWA and edge cloud as the Physical Twin (PT) and design a closed-loop that distributes radio resources to edge cloud instances for scheduling. We develop another closed-loop for intra-slice resource allocation to houses. We design an energy model that integrates radio resource allocation and formulate ultra-small and small-timescale optimizations for the PT to maximize slice requirement satisfaction while minimizing energy costs. We then design a reinforcement learning approach and successive convex approximation to address the formulated problems. We present a DT that replicates the PT by incorporating solution experiences into future states. The results show that our approach efficiently uses radio and energy resources.},
  archive      = {J_TMC},
  author       = {Anselme Ndikumana and Kim Khoa Nguyen and Mohamed Cheriet},
  doi          = {10.1109/TMC.2024.3482985},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1669-1683},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Digital twin backed closed-loops for energy-aware and open RAN-based fixed wireless access serving rural areas},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MANSY: Generalizing neural adaptive immersive video streaming with ensemble and representation learning. <em>TMC</em>, <em>24</em>(3), 1654-1668. (<a href='https://doi.org/10.1109/TMC.2024.3487175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity of immersive videos has prompted extensive research into neural adaptive tile-based streaming to optimize video transmission over networks with limited bandwidth. However, the diversity of users’ viewing patterns and Quality of Experience (QoE) preferences has not been fully addressed yet by existing neural adaptive approaches for viewport prediction and bitrate selection. Their performance can significantly deteriorate when users’ actual viewing patterns and QoE preferences differ considerably from those observed during the training phase, resulting in poor generalization. In this paper, we propose MANSY, a novel streaming system that embraces user diversity to improve generalization. Specifically, to accommodate users’ diverse viewing patterns, we design a Transformer-based viewport prediction model with an efficient multi-viewport trajectory input output architecture based on implicit ensemble learning. Besides, we for the first time combine the advanced representation learning and deep reinforcement learning to train the bitrate selection model to maximize diverse QoE objectives, enabling the model to generalize across users with diverse preferences. Extensive experiments demonstrate that MANSY outperforms state-of-the-art approaches in viewport prediction accuracy and QoE improvement on both trained and unseen viewing patterns and QoE preferences, achieving better generalization.},
  archive      = {J_TMC},
  author       = {Duo Wu and Panlong Wu and Miao Zhang and Fangxin Wang},
  doi          = {10.1109/TMC.2024.3487175},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1654-1668},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MANSY: Generalizing neural adaptive immersive video streaming with ensemble and representation learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SANSee: A physical-layer semantic-aware networking framework for distributed wireless sensing. <em>TMC</em>, <em>24</em>(3), 1636-1653. (<a href='https://doi.org/10.1109/TMC.2024.3483272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contactless device-free wireless sensing has recently attracted significant interest due to its potential to support a wide range of immersive human-machine interactive applications using ubiquitously available radio frequency (RF) signals. Traditional approaches focus on developing a single global model based on a combined dataset collected from different locations. However, wireless signals are known to be location and environment specific. Thus, a global model results in inconsistent and unreliable sensing results. It is also unrealistic to construct individual models for all the possible locations and environmental scenarios. Motivated by the observation that signals recorded at different locations are closely related to a set of physical-layer semantic features, in this paper we propose SANSee, a semantic-aware networking-based framework for distributed wireless sensing. SANSee allows models constructed in one or a limited number of locations to be transferred to new locations without requiring any locally labeled data or model training. SANSee is built on the concept of physical-layer semantic-aware network (pSAN), which characterizes the semantic similarity and the correlations of sensed data across different locations. A pSAN-based zero-shot transfer learning solution is introduced to allow receivers in new locations to obtain location-specific models by directly aggregating the models trained by other receivers. We theoretically prove that models obtained by SANSee can approach the locally optimal models. Experimental results based on real-world datasets are used to verify that the accuracy of the transferred models obtained by SANSee matches that of the models trained by the locally labeled data based on supervised learning approaches.},
  archive      = {J_TMC},
  author       = {Huixiang Zhu and Yong Xiao and Yingyu Li and Guangming Shi and Marwan Krunz},
  doi          = {10.1109/TMC.2024.3483272},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1636-1653},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SANSee: A physical-layer semantic-aware networking framework for distributed wireless sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and secure geometric range search over encrypted spatial data in mobile cloud. <em>TMC</em>, <em>24</em>(3), 1621-1635. (<a href='https://doi.org/10.1109/TMC.2024.3482321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile computing and the popularity of mobile devices equipped with GPS technology, massive spatial data have become available. Enterprises upload encrypted spatial data to the mobile cloud to save local storage and computation costs. However, the existing secure Geometric Range Search (GRS) solutions are inefficient in terms of building, updating index structure and querying processes. Moreover, the index structures of existing GRS schemes based on Order Preserving Encryption (OPE) leak location order, which may lead to reconstruction attacks. To solve these issues, we first propose an efficient and secure GRS scheme using Radix-Tree, namely GRSRT-I. Specifically, we construct an index structure based on Radix-tree to achieve efficient search and update, then use homomorphic encryption NTRU to resist chosen-plaintext attack, finally design a dual-server architecture to alleviate the burdens on mobile users caused by multiple rounds of interactions. Furthermore, we propose an enhanced scheme, GRSRT-II, by combining Order-Revealing Encryption and OPE, which greatly improves the search efficiency while slightly reducing the security. We formally prove the security of our proposed schemes, and conduct extensive experiments to demonstrate that GRSRT-I can improve the query efficiency by up to at least 1.5 times when compared with previous solutions and GRSRT-II can achieve a higher level of search efficiency.},
  archive      = {J_TMC},
  author       = {Yinbin Miao and Guijuan Wang and Xinghua Li and Hongwei Li and Kim-Kwang Raymond Choo and Rebert H. Deng},
  doi          = {10.1109/TMC.2024.3482321},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1621-1635},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient and secure geometric range search over encrypted spatial data in mobile cloud},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint association, beamforming, and resource allocation for multi-IRS enabled MU-MISO systems with RSMA. <em>TMC</em>, <em>24</em>(3), 1602-1620. (<a href='https://doi.org/10.1109/TMC.2024.3483193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent reflecting surface (IRS) and rate-splitting multiple access (RSMA) technologies are at the forefront of enhancing spectrum and energy efficiency in the next generation multi-antenna communication systems. This paper explores a RSMA system with multiple IRSs, and proposes two purpose-driven scheduling schemes, i.e., the exhaustive IRS-aided (EIA) and opportunistic IRS-aided (OIA) schemes. The aim is to optimize the system weighted energy efficiency (EE) under the above two schemes, respectively. Specifically, the Dinkelbach, branch and bound, successive convex approximation, and the semidefinite relaxation methods are exploited within the alternating optimization framework to obtain effective solutions to the considered problems. The numerical findings indicate that the EIA scheme exhibits better performance compared to the OIA scheme in diverse scenarios when considering the weighted EE, and the proposed algorithm demonstrates superior performance in comparison to the baseline algorithms.},
  archive      = {J_TMC},
  author       = {Chunjie Wang and Xuhui Zhang and Huijun Xing and Liang Xue and Shuqiang Wang and Yanyan Shen and Bo Yang and Xinping Guan},
  doi          = {10.1109/TMC.2024.3483193},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1602-1620},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint association, beamforming, and resource allocation for multi-IRS enabled MU-MISO systems with RSMA},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time video forgery detection via vision-WiFi silhouette correspondence. <em>TMC</em>, <em>24</em>(3), 1585-1601. (<a href='https://doi.org/10.1109/TMC.2024.3483550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For safety guard and crime prevention, video surveillance systems have been pervasively deployed in many security-critical scenarios, such as the residence, retail stores, and banks. However, these systems could be infiltrated by the adversary and the video streams would be modified or replaced, i.e., under the video forgery attack. The prevalence of Internet of Things (IoT) devices and the emergence of Deepfake-like techniques severely emphasize the vulnerability of video surveillance systems under such attacks. To secure existing surveillance systems, in this paper we propose a vision-WiFi cross-modal video forgery detection system, namely WiSil. Leveraging a theoretical model based on the principle of signal propagation, WiSil constructs wave front information of the object in the monitoring area from WiFi signals. With a well-designed deep learning network, WiSil further recovers silhouettes from the wave front information. Based on a Siamese network-based semantic feature extractor, WiSil can eventually determine whether a frame is manipulated by comparing the semantic feature vectors extracted from the video’s silhouette with those extracted from the WiFi’s silhouette. We enhance the basic version of WiSil Fang et al. 2023 by developing a model compression method and a forgery trace localization method. Extensive experiments show that WiSil achieves 95%$+$ accuracy in detecting tampered frames.},
  archive      = {J_TMC},
  author       = {Jianwei Liu and Xinyue Fang and Yike Chen and Jiantao Yuan and Guanding Yu and Jinsong Han},
  doi          = {10.1109/TMC.2024.3483550},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1585-1601},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time video forgery detection via vision-WiFi silhouette correspondence},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SITOff: Enabling size-insensitive task offloading in D2D-assisted mobile edge computing. <em>TMC</em>, <em>24</em>(3), 1567-1584. (<a href='https://doi.org/10.1109/TMC.2024.3483951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC), along with device-to-device (D2D) assisted MEC (D-MEC), are promising technologies that could improve the quality-of-experience for mobile devices (MDs) by offloading their tasks to edge servers or nearby idle MDs. There is a popular trend to develop distributed task offloading algorithms using multi-agent reinforcement learning (MARL), whose adoption of central critics during training makes the offloading still size-sensitive. Therefore, this paper proposes a Size-Insensitive Task Offloading (SITOff) algorithm for D-MEC based on fully-distributed offloading without maintaining any central venue. Specifically, taking advantage of the inherent graph-like structure of D-MEC, SITOff adopts graphs to represent MDs’ states and relationships and form each MD's local knowledge about D-MEC through graph computation. Furthermore, considering the limitation of local knowledge in performing whole performance-oriented offloading, each MD utilizes D2D-transmitting to exchange knowledge with its neighbors and form a comprehensive knowledge about D-MEC to enhance the coordination of distributed offloading. Additionally, regarding the different impacts of neighbors’ knowledge, each MD leverages attention mechanisms to selectively learn its neighbors’ knowledge during knowledge-exchange. Extensive experimental results show the superiority of SITOff over state-of-the-art MARL-based offloading algorithms in D-MEC with various MDs, and the easy collaboration of SITOff with curriculum-learning for large-scale D-MEC offloading.},
  archive      = {J_TMC},
  author       = {Zheyuan Hu and Jianwei Niu and Tao Ren and Xuefeng Liu and Mohsen Guizani},
  doi          = {10.1109/TMC.2024.3483951},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1567-1584},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SITOff: Enabling size-insensitive task offloading in D2D-assisted mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical layer cross-technology communication via explainable neural networks. <em>TMC</em>, <em>24</em>(3), 1550-1566. (<a href='https://doi.org/10.1109/TMC.2024.3480109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-technology communication (CTC) facilitates seamless interaction between different wireless technologies. Most existing methods use reverse engineering to derive the required transmission payload, generating a waveform that the target device can successfully demodulate. However, traditional approaches have certain limitations, including reliance on specific reverse engineering algorithms or the need for manual parameter tuning to reduce emulation distortion. In this work, we present NNCTC, a framework for achieving physical layer cross-technology communication through explainable neural networks, incorporating relevant knowledge from the wireless communication physical layer into the neural network models. We first convert the various signal processing components within the CTC process into neural network models, then build a training framework for the CTC encoder-decoder structure to achieve CTC. NNCTC significantly reduces the complexity of CTC by automatically deriving CTC payloads through training. We demonstrate how NNCTC implements CTC in WiFi systems using OFDM and CCK modulation. On WiFi systems using OFDM modulation, NNCTC outperforms the WEBee and WIDE designs in terms of error performance, achieving an average packet reception ratio (PRR) of 92.3% and an average symbol error rate (SER) as low as 1.3%. In WiFi systems using OFDM modulation, the highest PRR can reach up to 99%.},
  archive      = {J_TMC},
  author       = {Haoyu Wang and Jiazhao Wang and Wenchao Jiang and Shuai Wang and Demin Gao},
  doi          = {10.1109/TMC.2024.3480109},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1550-1566},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Physical layer cross-technology communication via explainable neural networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobility and cost aware inference accelerating algorithm for edge intelligence. <em>TMC</em>, <em>24</em>(3), 1530-1549. (<a href='https://doi.org/10.1109/TMC.2024.3484158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The edge intelligence (EI) has been widely applied recently. Splitting the model between device, edge server, and cloud can significantly improve the performance of EI. The model segmentation without user mobility has been investigated in detail in previous studies. However, in most EI use cases, the end devices are mobile. Few studies have been conducted on this topic. These works still have many issues, such as ignoring the energy consumption of mobile device, inappropriate network assumption, and low effectiveness on adapting user mobility, etc. Therefore, to address the disadvantages of model segmentation and resource allocation in previous studies, we propose mobility and cost aware model segmentation and resource allocation algorithm for accelerating the inference at edge (MCSA). Specifically, in the scenario without user mobility, the loop iteration gradient descent (Li-GD) algorithm is provided. When the mobile user has a large model inference task that needs to be calculated, it will take the energy consumption of mobile user, the communication and computing resource renting cost, and the inference delay into account to find the optimal model segmentation and resource allocation strategy. In the scenario with user mobility, the mobility aware Li-GD (MLi-GD) algorithm is proposed to calculate the optimal strategy. Then, the properties of the proposed algorithms are investigated, including convergence, complexity, and approximation ratio. The experimental results demonstrate the effectiveness of the proposed algorithms.},
  archive      = {J_TMC},
  author       = {Xin Yuan and Ning Li and Kang Wei and Wenchao Xu and Quan Chen and Hao Chen and Song Guo},
  doi          = {10.1109/TMC.2024.3484158},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1530-1549},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobility and cost aware inference accelerating algorithm for edge intelligence},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive obsolete packet management based analysis of age of information for LCFS heterogeneous queueing system. <em>TMC</em>, <em>24</em>(3), 1513-1529. (<a href='https://doi.org/10.1109/TMC.2024.3481062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes the Age of Information (AoI), focusing on transmitting status updates from source to destination. We analyze the Age of Information in a system comprised of two heterogeneous servers with exponential distribution parameters $\mu _{1}$ and $\mu _{2}$, respectively. Our study adopts the stochastic hybrid systems (SHS) methodology to thoroughly assess the system’s performance. We explore various queueing disciplines, including Last-Come-First-Serve (LCFS) with work-conservative and LCFS with probabilistic routing, to accurately quantify AoI and Peak AoI (PAoI) metrics. We have used the Proactive Obsolete Packet Management (POPMAN) approach to identify and discard obsolete packets proactively, thus enhancing server processing efficiency and ensuring orderly packet reception. We also investigate the following parameters, such as the probability of preemption of packets, the probability of packets getting obsolete, the probability of informative packets, and optimal splitting probabilities. Results show an improvement in both AoI and PAoI within the LCFS with work-conservative queueing system with the integration of the POPMAN method. Furthermore, LCFS with probabilistic routing using the POPMAN approach performs similarly to conventional methods. In all the queueing systems studied, as the arrival rate $\lambda \to \infty$, the average AoI and PAoI approach $1/(\mu _{1}+\mu _{2})$. For c servers, they approach $1/(\mu _{1}+\mu _{2}+\cdots +\mu _{c})$.},
  archive      = {J_TMC},
  author       = {Y. Arun Kumar Reddy and T. G Venkatesh},
  doi          = {10.1109/TMC.2024.3481062},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1513-1529},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Proactive obsolete packet management based analysis of age of information for LCFS heterogeneous queueing system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ${\sf Img2Acoustic}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi mathvariant="sans-serif">Img</mml:mi><mml:mn mathvariant="sans-serif">2</mml:mn><mml:mi mathvariant="sans-serif">Acoustic</mml:mi></mml:mrow></mml:math>: A cross-modal gesture recognition method based on few-shot learning. <em>TMC</em>, <em>24</em>(3), 1496-1512. (<a href='https://doi.org/10.1109/TMC.2024.3481443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acoustic-based human gesture recognition (HGR) offers diverse applications due to the ubiquity of sensors and touch-free interaction. However, existing machine learning approaches require substantial training data, making the process time-consuming, costly, and labor-intensive. Recent studies have explored cross-modal methods to reduce the need for large training datasets in behavior recognition, but they typically rely on open-source datasets that closely align with the target domain, limiting flexibility and complicating data collection. In this paper, we propose ${\sf Img2Acoustic}$, a novel cross-modal acoustic-based HGR approach that leverages models trained on open-source image datasets (i.e., EMNIST, Omniglot) to effectively recognize custom gestures detected via acoustic signals. Our model incorporates a task-aware attention layer (TAAL) and a task-aware local matching layer (TALML), enabling seamless transfer of knowledge from image datasets to acoustic gesture recognition. We implement ${\sf Img2Acoustic}$ on commercial devices and conduct comprehensive evaluations, demonstrating that our method not only delivers superior accuracy and robustness compared to existing approaches but also eliminates the need for extensive training data collection.},
  archive      = {J_TMC},
  author       = {Yongpan Zou and Jianhao Weng and Wenting Kuang and Yang Jiao and Victor C. M. Leung and Kaishun Wu},
  doi          = {10.1109/TMC.2024.3481443},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1496-1512},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {${\sf Img2Acoustic}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi mathvariant="sans-serif">Img</mml:mi><mml:mn mathvariant="sans-serif">2</mml:mn><mml:mi mathvariant="sans-serif">Acoustic</mml:mi></mml:mrow></mml:math>: A cross-modal gesture recognition method based on few-shot learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ui-ear: On-face gesture recognition through on-ear vibration sensing. <em>TMC</em>, <em>24</em>(3), 1482-1495. (<a href='https://doi.org/10.1109/TMC.2024.3480216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the convenient design and prolific functionalities, wireless earbuds are fast penetrating in our daily life and taking over the place of traditional wired earphones. The sensing capabilities of wireless earbuds have attracted great interests of researchers on exploring them as a new interface for human-computer interactions. However, due to its extremely compact size, the interaction on the body of the earbuds is limited and not convenient. In this paper, we propose Ui-Ear, a new on-face gesture recognition system to enrich interaction maneuvers for wireless earbuds. Ui-Ear exploits the sensing capability of Inertial Measurement Units (IMUs) to extend the interaction to the skin of the face near ears. The accelerometer and gyroscope in IMUs perceive dynamic vibration signals induced by on-face touching and moving, which brings rich maneuverability. Since IMUs are provided on most of the budget and high-end wireless earbuds, we believe that Ui-Ear has great potential to be adopted pervasively. To demonstrate the feasibility of the system, we define seven different on-face gestures and design an end-to-end learning approach based on Convolutional Neural Networks (CNNs) for classifying different gestures. To further improve the generalization capability of the system, adversarial learning mechanism is incorporated in the offline training process to suppress the user-specific features while enhancing gesture-related features. We recruit 20 participants and collect a realworld datasets in a common office environment to evaluate the recognition accuracy. The extensive evaluations show that the average recognition accuracy of Ui-Ear is over 95% and 82.3% in the user-dependent and user-independent tasks, respectively. Moreover, we also show that the pre-trained model (learned from user-independent task) can be fine-tuned with only few training samples of the target user to achieve relatively high recognition accuracy (up to 95%). At last, we implement the personalization and recognition components of Ui-Ear on an off-the-shelf Android smartphone to evaluate its system overhead. The results demonstrate Ui-Ear can achieve real-time response while only brings trivial energy consumption on smartphones.},
  archive      = {J_TMC},
  author       = {Guangrong Zhao and Yiran Shen and Feng Li and Lei Liu and Lizhen Cui and Hongkai Wen},
  doi          = {10.1109/TMC.2024.3480216},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1482-1495},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Ui-ear: On-face gesture recognition through on-ear vibration sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate prediction of multi-dimensional required resources in 5G via federated deep reinforcement learning. <em>TMC</em>, <em>24</em>(3), 1469-1481. (<a href='https://doi.org/10.1109/TMC.2024.3480136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate prediction of required resources in terms of storage, computing and bandwidth is essential for 5G to host diverse services. The existing efforts illustrate that it is more promising to efficiently predict the unknown required resources with a third-order tensor compared to the 2D-matrix-based solutions. However, most of them fail to leverage the inherent features hidden in network traffic like temporal stability and service correlation to build a third-order tensor for the multi-dimensional required resource prediction in an intelligent manner, incurring coarse-grained prediction accuracy. Furthermore, it is difficult to build a third-order tensor with rate-varied measurements in 5G due to different lengths of measurement time slots. To address these issues, we propose an Accurate Prediction of Multi-Dimensional Required Resources (APMR) approach in 5G via Federated Deep Reinforcement Learning (FDRL). We first confirm the resource requests originated from different Base Stations (BSs) at varied measurement rates have similar features in service and time domains, but cannot directly form a series of regular tensors. Built on these observations, we reshape these measurement data to form a series of standard third-order tensors with the same size, which include many elements obtained from measurements and some unknown elements needed to be inferred. In order to obtain accurately predicted results, the FDRL-based tensor factorization approach is introduced to intelligently utilize multiple specific iteration rules for local model learning, and the accuracy-aware and latency-based depreciation strategies are exploited to aggregate local models for resource prediction. Extensive simulation experiments demonstrate that APMR can accurately predict the multi-dimensional required resources compared to the state-of-the-art approaches.},
  archive      = {J_TMC},
  author       = {Haojun Huang and Qifan Wang and Weimin Wu and Miao Wang and Geyong Min},
  doi          = {10.1109/TMC.2024.3480136},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1469-1481},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Accurate prediction of multi-dimensional required resources in 5G via federated deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-intrusive and efficient estimation of antenna 3-D orientation for WiFi APs. <em>TMC</em>, <em>24</em>(3), 1453-1468. (<a href='https://doi.org/10.1109/TMC.2024.3485228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effectiveness of WiFi-based localization systems heavily relies on the spatial accuracy of WiFi AP. In real-world scenarios, factors such as AP rotation and irregular antenna tilt contribute significantly to inaccuracies, surpassing the impact of imprecise AP location and antenna separation. In this paper, we propose Anteumbler, a non-invasive, accurate, and efficient system for measuring the orientation of each antenna in physical space. By leveraging the fact that maximum received power occurs when a Tx-Rx antenna pair is perfectly aligned, we build a spatial angle model capable of determining antennas’ orientations without prior knowledge. However, achieving comprehensive coverage across the spatial angle necessitates extensive sampling points. To enhance efficiency, we exploit the orthogonality of antenna directivity and polarization, and adopt an iterative algorithm, thereby reducing the number of sampling points by several orders of magnitude. Additionally, to attain the required antenna orientation accuracy, we mitigate the influence of propagation distance using a dual plane intersection model while filtering out ambient noise. Our real-world experiments, covering six antenna types, two antenna layouts, two antenna separations ($\lambda /2$ and $\lambda$ ), and three AP heights, demonstrate that Anteumbler achieves median errors below $\text{6}^\circ$ for both elevation and azimuth angles, and exhibits robustness in NLoS and dynamic environments. Moreover, when integrated into the reverse localization system, Anteumbler deployed over LocAP reduces antenna separation error by $10 \,\mathrm{mm}$, while for user localization system, its integration over SpotFi reduces user localization error by more than $1 \,\mathrm{m}$.},
  archive      = {J_TMC},
  author       = {Dawei Yan and Panlong Yang and Fei Shang and Nikolaos M. Freris and Yubo Yan},
  doi          = {10.1109/TMC.2024.3485228},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1453-1468},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Non-intrusive and efficient estimation of antenna 3-D orientation for WiFi APs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enable practical long-range multi-target backscatter sensing. <em>TMC</em>, <em>24</em>(3), 1437-1452. (<a href='https://doi.org/10.1109/TMC.2024.3480137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backscatter sensing has emerged as a significant technology within the Internet of Things (IoT), prompting extensive research interest. This paper presents LoMu, the first long-range multi-target backscatter sensing system designed for low-cost tags operating under ambient LoRa. LoMuintroduces an orthogonal sensing model that processes backscatter signals from multiple tags to extract motion information. The design addresses several practical challenges, including near-far interference among multiple tags, phase offsets from unsynchronized transceivers, and phase errors due to frequency drift in low-cost tags. To overcome these issues, we propose a conjugate-based energy concentration method to extract high-quality signals and a Hamming-window-based method to mitigate the near-far problem. Additionally, we exploit the relationship between excitation and backscatter signals to synchronize the transmitter (TX) and receiver (RX) and combine double sidebands of backscatter signals to eliminate tag frequency drift. Furthermore, a novel joint estimation algorithm is introduced to exploit both amplitude and phase information in target signals, enhancing frequency sensing results and robustness. Our implementation and extensive experiments demonstrate that LoMucan accurately sense up to 35 tags simultaneously and achieve an average frequency sensing error of 0.5% at a range of 400 meters, which is $4\times$ the range of the state-of-the-art.},
  archive      = {J_TMC},
  author       = {Yihao Liu and Jinyan Jiang and Jumin Zhao and Jiliang Wang},
  doi          = {10.1109/TMC.2024.3480137},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1437-1452},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enable practical long-range multi-target backscatter sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optical sensing-based intelligent toothbrushing monitoring system. <em>TMC</em>, <em>24</em>(3), 1417-1436. (<a href='https://doi.org/10.1109/TMC.2024.3479455'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorrect brushing methods normally lead to poor oral hygiene, and result in severe oral diseases and complications. While effective brushing can address this issue, individuals often struggle with incorrect brushing, like aggressive brushing, insufficient brushing, and missing brushing. To break this stalemate, in this paper, we proposed LiT, a toothbrushing monitoring system to assess the brushing status on 16 surfaces using the Bass technique. LiT utilizes commercial LED toothbrushes’ blue LEDs as transmitters, and incorporates only two low-cost photodetectors as receivers on the toothbrush head. It is challenging to determine optimal deployment positions and minimize photodetectors number to establish the light transmission channel in oral cavity. To address these challenges, we established mathematical models within the oral cavity based on the two photodetectors’ deployment to theoretically validate the feasibility and prove robustness. Furthermore, we designed a comprehensive framework to fight against the implementation challenges including brushing action separation, light interference on the outer surfaces of front teeth, toothpaste diversity, user variations, brushing hand variability, and incorrect brushings. Experimental results demonstrate that LiT achieves a highly accurate surface recognition rate of 95.3%, an estimated error for brushing duration of 6.1%, and incorrect brushing detection accuracy of 96.9%. Furthermore, LiT retains stable capability under a variety of circumstances, such as various lighting conditions, user movement, toothpaste diversity, and left and right-handed users.},
  archive      = {J_TMC},
  author       = {Kaixin Chen and Lei Wang and Yongzhi Huang and Kaishun Wu and Lu Wang},
  doi          = {10.1109/TMC.2024.3479455},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1417-1436},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optical sensing-based intelligent toothbrushing monitoring system},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trajectory optimization and pick-up and delivery sequence design for cellular-connected cargo AAVs. <em>TMC</em>, <em>24</em>(3), 1402-1416. (<a href='https://doi.org/10.1109/TMC.2024.3480910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a cargo autonomous aerial vehicle (AAV)-aided multi-parcel pick-up and delivery network, where the communication ability of the AAV is provided by the ground base stations (GBSs). For such a system setup, our goal is to optimize the trajectory of the cargo AAV while minimizing the combined impact of total energy consumption and total outage time. Simultaneously, we aim to maximize overall user satisfaction throughout the entire flight duration. More specifically, we propose a pick-up and delivery of AAV (PDU) framework to address this problem and this framework consists of two parts. First, a simulated annealing (SA) algorithm is used to obtain the pick-up and delivery (P&D) order of parcels. On the basis of obtaining the P&D order through SA, we further use deep reinforcement learning (DRL) to optimize the flight trajectory of the AAV to ensure the expected communication quality between the AAV and GBSs. To verify the effectiveness of our proposed algorithms, we design three baseline strategies for comparison, and also investigate the effect of using the PDU framework with different weights. Finally, numerical results show that the performance of PDU strategy is improved by about 5%-30% compared with other strategies in solving the performance tradeoff of AAV energy consumption, communication quality, and user satisfaction.},
  archive      = {J_TMC},
  author       = {Jiangling Cao and Liang Yang and Dingcheng Yang and Tiankui Zhang and Lin Xiao and Hongbo Jiang and Dusit Niyato},
  doi          = {10.1109/TMC.2024.3480910},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1402-1416},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Trajectory optimization and pick-up and delivery sequence design for cellular-connected cargo AAVs},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel cycle time: A new measure of short-term fairness. <em>TMC</em>, <em>24</em>(3), 1386-1401. (<a href='https://doi.org/10.1109/TMC.2024.3484177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper puts forth a new metric, dubbed channel cycle time (CCT), to measure the short-term fairness of communication networks. CCT characterizes the average duration between two consecutive successful transmissions of a user, during which all other users successfully accessed the channel at least once. In contrast to existing short-term fairness measures, CCT provides more comprehensive insight into the transient dynamics of communication networks, with a particular focus on users’ delays and jitter. To validate the efficacy of our approach, we analytically characterize the CCTs for two classical communication protocols: slotted Aloha and CSMA/CA. The analysis demonstrates that CSMA/CA exhibits superior short-term fairness over slotted Aloha. Beyond its role as a measurement metric, CCT has broader implications as a guiding principle for the design of future communication networks by emphasizing factors like fairness, delay, and jitter in short-term behaviors.},
  archive      = {J_TMC},
  author       = {Pengfei Shen and Yulin Shao and Haoyuan Pan and Lu Lu and Yonina C. Eldar},
  doi          = {10.1109/TMC.2024.3484177},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1386-1401},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Channel cycle time: A new measure of short-term fairness},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computation offloading and resource allocation in LEO satellite-terrestrial integrated networks with system state delay. <em>TMC</em>, <em>24</em>(3), 1372-1385. (<a href='https://doi.org/10.1109/TMC.2024.3479243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing offloading optimization for energy saving is becoming increasingly important in low-Earth orbit (LEO) satellite-terrestrial integrated networks (STINs) since battery techniques have not kept up with the demand of ground terminal devices. In this paper, we design a delay-based deep reinforcement learning (DRL) framework specifically for computation offloading decisions, which can effectively reduce the energy consumption. Additionally, we develop a multi-level feedback queue for computing allocation (RAMLFQ), which can effectively enhance the CPU’s efficiency in task scheduling. We initially formulate the computation offloading problem with the system delay as Delay Markov Decision Processes (DMDPs), and then transform them into the equivalent standard Markov Decision Processes (MDPs). To solve the optimization problem effectively, we employ a double deep Q-network (DDQN) method, enhancing it with an augmented state space to better handle the unique challenges posed by system delays. Simulation results demonstrate that the proposed learning-based computing offloading algorithm achieves high levels of performance efficiency and attains a lower total cost compared to other existing offloading methods.},
  archive      = {J_TMC},
  author       = {Bo Xie and Haixia Cui and Ivan Wang-Hei Ho and Yejun He and Mohsen Guizani},
  doi          = {10.1109/TMC.2024.3479243},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1372-1385},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Computation offloading and resource allocation in LEO satellite-terrestrial integrated networks with system state delay},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new data completion perspective on sparse CrowdSensing: Spatiotemporal evolutionary inference approach. <em>TMC</em>, <em>24</em>(3), 1357-1371. (<a href='https://doi.org/10.1109/TMC.2024.3480983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile CrowdSensing (MCS) has emerged as a popular paradigm to engage mobile users in collaborative sensing tasks. However, its performance is hindered by its limited spatiotemporal range and the cost of data collection. An effective strategy is to integrate Sparse MCS with data completion, allowing for unsensed data inference. However, when confronted with situations where sensed data is excessively sparse, data inference results may be unsatisfactory due to several challenges including: 1) uneven data distribution, 2) complex spatiotemporal correlation, and 3) the presence of inference noise. To address these challenges, we propose a model named Spatiotemporal Evolutionary Inference (STEI) that achieves accurate inference of unsensed data in Sparse MCS. Specifically, we complete the unsensed data by uncovering strong local correlations in the data and gradually evolving those correlations to the global situation. In each evolution step, we thoroughly consider the impact of spatiotemporal consistency and difference. To minimize the interference of noise during the evolution process, we design an adaptive coefficient to enhance the dependence on sensed data. Finally, to validate the effectiveness of STEI, we conduct extensive qualitative and quantitative experiments using three popular datasets. The experimental results demonstrate that our approach excels in accurately inferring data, particularly in situations where the distribution of data is notably uneven.},
  archive      = {J_TMC},
  author       = {En Wang and Zixuan Song and Mengni Wu and Wenbin Liu and Bo Yang and Yongjian Yang and Jie Wu},
  doi          = {10.1109/TMC.2024.3480983},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1357-1371},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A new data completion perspective on sparse CrowdSensing: Spatiotemporal evolutionary inference approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixed numerology-based intelligent resource management in a sliced 6G Space–Terrestrial integrated radio access network. <em>TMC</em>, <em>24</em>(3), 1338-1356. (<a href='https://doi.org/10.1109/TMC.2024.3494842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although resource sharing and mixed numerology among slices are promising for improving wireless resource utilization, these techniques can compromise isolation performance and cause serious inter numerology interference (INI). Therefore, this paper studies wireless resource management in a mixed numerology-based sliced 6G space–terrestrial integrated radio access network (STI-RAN) with the aim of reducing INI and guaranteeing isolation performance while decreasing interference from Doppler frequency shifts caused by the high-speed movement of low-orbit satellites. First, an isolation performance indicator is defined to evaluate different isolation performances, and a universal spectral distance model is formulated to rewrite the INI power model. Next, the dynamic wireless resource management problem is formulated in a discrete form, yielding a scheme called Flex-$\mu$, which is designed to reduce the INI and Doppler frequency shifts, guarantee isolation performance, and enhance the SINR. Finally, an intelligent multi-characteristic matrix coding-based social group optimization (MultiMatrix-SGO) algorithm is designed to solve the proposed NP-hard discrete optimization problem. Compared with existing schemes, the system utility is efficiently increased by up to 58.32%, the SINR can converge to 38.44 dB, and the isolation performance is guaranteed while the INI and Doppler frequency shifts are reduced.},
  archive      = {J_TMC},
  author       = {Ning Hui and Qian Sun and Jie Zeng and Lin Tian and Yuanyuan Wang and Yiqing Zhou},
  doi          = {10.1109/TMC.2024.3494842},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1338-1356},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mixed numerology-based intelligent resource management in a sliced 6G Space–Terrestrial integrated radio access network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning-based joint caching and routing in AI-driven networks. <em>TMC</em>, <em>24</em>(3), 1322-1337. (<a href='https://doi.org/10.1109/TMC.2024.3481276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To reduce redundant traffic transmission in both wired and wireless networks, optimal content placement problem naturally occurring in many applications is studied. In this paper, considering the limited cache capacity, unknown popularity distribution and non-stationary user demands, we address this problem by jointly optimizing content caching and routing with the objective of minimizing transmission cost. By optimizing the routing with the route-to-least cost-cache policy, the content caching process is modeled as a Markov decision process (MDP), aiming to maximize caching reward. However, the optimization problem consists of multiple nodes selecting caching contents, which leads to the combinatorial increase of the number of action dimensions with the number of possible actions. To handle this curse of dimensionality, we propose an intelligent caching algorithm by embedding action branching architecture into a dueling double deep Q-network (D3QN) to optimize caching decisions, and thus the agent at the controller can adaptively learn and track the underlying dynamics. Considering the independence of each branch, a marginal gain-based replacement rule is proposed to satisfy cache capacity constraint. Our simulation results show that compared with the prior art, the caching reward and hit rate of the proposed algorithm are increased by 35.3% and 33.6% respectively on average.},
  archive      = {J_TMC},
  author       = {Meiyi Yang and Deyun Gao and Weiting Zhang and Dong Yang and Dusit Niyato and Hongke Zhang and Victor C. M. Leung},
  doi          = {10.1109/TMC.2024.3481276},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1322-1337},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Deep reinforcement learning-based joint caching and routing in AI-driven networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sequential privacy budget recycling for federated vector mean estimation: A game-theoretic approach. <em>TMC</em>, <em>24</em>(3), 1308-1321. (<a href='https://doi.org/10.1109/TMC.2024.3484010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving vector mean estimation is a crucial primitive in federated analytics. Existing practices usually resort to Local Differentiated Privacy (LDP) mechanisms that inject random noise into users’ vectors when communicating with users and the central server. Due to the privacy-utility trade-off, the privacy budget has been widely recognized as the bottleneck resource that requires well-provisioning. In this paper, we explore the possibility of privacy budget recycling and propose a novel ChainDP framework enabling users to carry out data aggregation sequentially to recycle the privacy budget. We establish a sequential game to model the user interactions in our framework. We theoretically show the mathematical nature of the sequential game, solve its Nash Equilibrium, and design an incentive mechanism with provable economic properties. To alleviate potential privacy collusion attacks, we further derive a differentially privacy-guaranteed protocol to avoid holistic exposure. Our numerical simulation validates the effectiveness of ChainDP, showing that it can significantly save privacy budget as well as lower estimation error compared to the traditional LDP mechanism.},
  archive      = {J_TMC},
  author       = {Jingyi Li and Guangjing Huang and Liekang Zeng and Lin Chen and Xu Chen},
  doi          = {10.1109/TMC.2024.3484010},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1308-1321},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Sequential privacy budget recycling for federated vector mean estimation: A game-theoretic approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedSN: A federated learning framework over heterogeneous LEO satellite networks. <em>TMC</em>, <em>24</em>(3), 1293-1307. (<a href='https://doi.org/10.1109/TMC.2024.3481275'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, a large number of Low Earth Orbit (LEO) satellites have been launched and deployed successfully in space. Due to multimodal sensors equipped by the LEO satellites, they serve not only for communications but also for various machine learning applications. However, a ground station (GS) may be incapable of downloading such a large volume of raw sensing data for centralized model training due to the limited contact time with LEO satellites (e.g. 5 minutes). Therefore, federated learning (FL) has emerged as the promising solution to address this problem via on-device training. Unfortunately, enabling FL on LEO satellites still face three critical challenges: i) heterogeneous computing and memory capabilities, ii) limited downlink/uplink rate, and iii) model staleness. To this end, we propose FedSN as a general FL framework to tackle the above challenges. Specifically, we first present a novel sub-structure scheme to enable heterogeneous local model training considering different computing, memory, and communication constraints on LEO satellites. Additionally, we propose a pseudo-synchronous model aggregation strategy to dynamically schedule model aggregation for compensating model staleness. Extensive experiments with real-world satellite data demonstrate that FedSN framework achieves higher accuracy, lower computing, and communication overhead than the state-of-the-art benchmarks.},
  archive      = {J_TMC},
  author       = {Zheng Lin and Zhe Chen and Zihan Fang and Xianhao Chen and Xiong Wang and Yue Gao},
  doi          = {10.1109/TMC.2024.3481275},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1293-1307},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedSN: A federated learning framework over heterogeneous LEO satellite networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale mechanism design for networks: Superimposability and dynamic implementation. <em>TMC</em>, <em>24</em>(3), 1278-1292. (<a href='https://doi.org/10.1109/TMC.2024.3499958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network utility maximization (NUM) is a fundamental framework for optimizing next-generation networks. However, self-interested agents with private information pose challenges due to potential system manipulation. To address these challenges, the literature on economic mechanism design has emerged. Existing mechanisms are not suited for large-scale networks due to their complexity, high implementation costs, and difficulty to adapt to dynamic settings. This paper proposes a large-scale mechanism design framework that mitigates these limitations. As the number of agents $I$ approaches infinity, their incentive to misreport decreases rapidly at a rate of $\mathcal {O}(1/I^{2})$. We introduce a superimposable framework applicable to any NUM algorithm without modifications, reducing implementation costs. In the dynamic setting, the large-scale mechanism design framework introduces the decomposability of the problem, enabling agents to align their own interests with the objectives of the dynamic NUM problem. This alignment helps overcome the additional, more stringent incentive constraints encountered in dynamic settings. Extending our results to dynamic settings, we present the design of a Dynamic Large-Scale mechanism with desirable properties and the corresponding Dynamic Superimposable Large-Scale mechanism. Our numerical experiments validate the fact that our proposed schemes are approximately $I$ times faster than the seminal VCG mechanism.},
  archive      = {J_TMC},
  author       = {Meng Zhang and Deepanshu Vasal},
  doi          = {10.1109/TMC.2024.3499958},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1278-1292},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Large-scale mechanism design for networks: Superimposability and dynamic implementation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Travel demand modeling and estimation for high-dimensional mobility. <em>TMC</em>, <em>24</em>(3), 1264-1277. (<a href='https://doi.org/10.1109/TMC.2024.3435436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive amount of data related to spatiotemporal mobility offers new opportunities to understand human mobility with applications in various sectors, including transportation, logistics, and safety. However, the increase in the volume and in the dimension of mobility data makes it challenging to retrieve important information and critical features of spatiotemporal mobility. This paper develops a method to estimate probabilistic occurrences of travel demands considering interactions between origin, destination, and departure time. First, we reveal the important features in the complex structure of mobility data and identify mobility patterns. Then, we derive a data-driven model, accounting for mobility patterns, to estimate and predict travel demands. We quantify the accuracy of the proposed method for a case study using both New York city yellow taxi trip data and for-hire vehicles trip data over the entire city. Results show the accuracy of the proposed method compared to existing approaches.},
  archive      = {J_TMC},
  author       = {Jeongyun Kim and Andrea Conti and Moe Z. Win},
  doi          = {10.1109/TMC.2024.3435436},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1264-1277},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Travel demand modeling and estimation for high-dimensional mobility},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient resource allocation scheme with uncertain network status in edge computing-enabled networks. <em>TMC</em>, <em>24</em>(3), 1249-1263. (<a href='https://doi.org/10.1109/TMC.2024.3412810'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative resource allocation is crucial for reducing overhead and enhancing resource utilization in edge computing-enabled networks. To ensure a satisfactory user experience, we recognize the importance of considering information uncertainty in resource allocation. Therefore, we explore information uncertainty in edge computing-enabled networks, especially within the complex environment of resource coupling. However, existing methods lack a comprehensive and robust solution for coordinating wireless, transport, and computing resource under this information uncertainty. This paper addresses this gap by proposing a joint optimization of access point (AP) selection, computing node association, and traffic engineering, aiming to maximize network utility under the uncertain conditions of wireless status and application QoS requirements. The constraints under these uncertainties are modeled as chance constraints, complicating the problem's solvability. We adopt the Bernstein approximation to establish convex conservative approximations of the chance constraints. Given the problem's substantial size and computational complexity, the alternating direction method of multipliers is employed to solve the approximated problem in a distributed manner. We further derive the closed solutions of the corresponding sub-problems. Extensive simulations validate the superiority of our proposed scheme, demonstrating its ability to achieve a good trade-off between meeting user requirements and optimizing resource utilization.},
  archive      = {J_TMC},
  author       = {Yuxia Cheng and Chengchao Liang and Qianbin Chen and F. Richard Yu},
  doi          = {10.1109/TMC.2024.3412810},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {3},
  number       = {3},
  pages        = {1249-1263},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An efficient resource allocation scheme with uncertain network status in edge computing-enabled networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biometric encoding for replay-resistant smartphone user authentication using handgrips. <em>TMC</em>, <em>24</em>(2), 1230-1248. (<a href='https://doi.org/10.1109/TMC.2024.3474673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biometrics have been widely applied for user authentication. However, existing biometric authentications are vulnerable to biometric spoofing, because they can be observed and forged. In addition, they rely on verifying biometric features that rarely change. To address this issue, we propose to verify the handgrip biometric that can be unobtrusively extracted by acoustic signals when the user holds the phone. This biometric is uniquely associated with the user’s hand geometry, body-fat ratio, and gripping strength, which are hard to reproduce. Furthermore, we propose two biometric encoding techniques (i.e., temporal-frequential and spatial) to convert static biometrics into dynamic biometric features to prevent data reuse. In particular, we develop a biometric authentication system to work with the challenge-response protocol. We encode the ultrasonic signal according to a random challenge sequence and extract a distinct biometric code as the response. We further develop two decoding algorithms to decode the biometric code for user authentication. Additionally, we investigate multiple new attacks and explore using a latent diffusion model to solve the acoustic noise discrepancies between the training and testing data to improve system performance. Extensive experiments show our system achieves 97% accuracy in distinguishing users and rejects 100% replay attacks with $ 0.6 \, s$ challenge sequence.},
  archive      = {J_TMC},
  author       = {Long Huang and Chen Wang},
  doi          = {10.1109/TMC.2024.3474673},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1230-1248},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Biometric encoding for replay-resistant smartphone user authentication using handgrips},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BIT-FL: Blockchain-enabled incentivized and secure federated learning framework. <em>TMC</em>, <em>24</em>(2), 1212-1229. (<a href='https://doi.org/10.1109/TMC.2024.3477616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harnessing the benefits of blockchain, such as decentralization, immutability, and transparency, to bolster the credibility and security attributes of federated learning (FL) has garnered increasing attention. However, blockchain-enabled FL (BFL) still faces several challenges. The primary and most significant issue arises from its essential but slow validation procedure, which selects high-quality local models by recruiting distributed validators. The second issue stems from its incentive mechanism under the transparent nature of blockchain, increasing the risk of privacy breaches regarding workers’ cost information. The final challenge involves data eavesdropping from shared local models. To address these significant obstacles, this paper proposes a Blockchain-enabled Incentivized and Secure Federated Learning (BIT-FL) framework. BIT-FL leverages a novel loop-based sharded consensus algorithm to accelerate the validation procedure, ensuring the same security as non-sharded consensus protocols. It consistently outputs the correct local model selection when the fraction of adversaries among validators is less than $1/2$ with synchronous communication. Furthermore, BIT-FL integrates a randomized incentive procedure, attracting more participants while guaranteeing the privacy of their cost information through meticulous worker selection probability design. Finally, by adding artificial Gaussian noise to local models, it ensures the privacy of trainers’ local models. With the careful design of Gaussian noise, the excess empirical risk of BIT-FL is upper-bounded by $\mathcal {O}(\frac{\ln n_{\min}}{ n_{\min}^{3/2}}+\frac{\ln n}{n})$, where $n$ represents the size of the union dataset, and $n_{{\min}}$ represents the size of the smallest dataset. Our extensive experiments demonstrate that BIT-FL exhibits efficiency, robustness, and high accuracy for both classification and regression tasks.},
  archive      = {J_TMC},
  author       = {Chenhao Ying and Fuyuan Xia and David S. L. Wei and Xinchun Yu and Yibin Xu and Weiting Zhang and Xikun Jiang and Haiming Jin and Yuan Luo and Tao Zhang and Dacheng Tao},
  doi          = {10.1109/TMC.2024.3477616},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1212-1229},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {BIT-FL: Blockchain-enabled incentivized and secure federated learning framework},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AS-MAC: An adaptive scheduling MAC protocol for reducing the end-to-end delay in AUV-assisted underwater acoustic networks. <em>TMC</em>, <em>24</em>(2), 1197-1211. (<a href='https://doi.org/10.1109/TMC.2024.3475428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous Underwater Vehicle (AUV)-assisted Underwater Acoustic Networks (UANs) are promising for complex ocean applications. In essence, an AUV-assisted UAN is still dominated by fixed nodes, and Time Division Multiple Access (TDMA)-based Medium Access Control (MAC) protocols have undisputed practicability in such fixed nodes-dominated UANs since they are simple and easy to deploy. However, AUV-assisted UANs may exist dynamic bidirectional data streams, while most existing protocols assume UANs have a unidirectional data stream, and their fixed scheduling sequence results in the long end-to-end delay in AUV-assisted UANs. In this paper, we first reveal a phenomenon between the data stream and the scheduling sequence, derived from real-world experiments: their consistent direction decreases the packet waiting delay but increases the slot length, and vice versa. To optimize the end-to-end delay, UANs with dynamic bidirectional data streams expect the MAC protocol to provide a flexible scheduling sequence. To this end, we propose a low-delay Adaptive Scheduling MAC protocol (AS-MAC) based on TDMA for AUV-assisted UANs. In AS-MAC, we analyze the relationship between scheduling sequence and data stream, extracting two significant factors: slot length and packet delay. Afterwards, we design Slot Length Model (SLM) and Packet Delay Model (PDM) to analyze the end-to-end delay of different data streams. Based on these two models, we present a Scheduling Sequence and Slot Length allocation Algorithm (SSSLA) to adaptively provide the minimum end-to-end delay for current bidirectional data streams. Extensive simulation results show that AS-MAC efficiently addresses severe queue congestion of the state-of-the-art protocols and reduces the end-to-end delay of different dynamic streams in various scenarios.},
  archive      = {J_TMC},
  author       = {Jiani Guo and Shanshan Song and Jun Liu and Miao Pan and Jun-Hong Cui and GuangJie Han},
  doi          = {10.1109/TMC.2024.3475428},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1197-1211},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AS-MAC: An adaptive scheduling MAC protocol for reducing the end-to-end delay in AUV-assisted underwater acoustic networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-cloud collaborated object detection via bandwidth adaptive difficult-case discriminator. <em>TMC</em>, <em>24</em>(2), 1181-1196. (<a href='https://doi.org/10.1109/TMC.2024.3474743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection, a fundamental task in computer vision, is crucial for various intelligent edge computing applications. However, object detection algorithms are usually heavy in computation, hindering their deployments on resource-constrained edge devices. Traditional edge-cloud collaboration schemes, like deep neural network (DNN) partitioning across edge and cloud, are unfit for object detection due to the significant communication costs incurred by the large size of intermediate results. To this end, we propose a Difficult-Case based Small-Big model (DCSB) framework. It employs a difficult-case discriminator on the edge device to control data transfer between the small model on the edge and the large model in the cloud. We also adopt regional sampling to further reduce the bandwidth consumption and create a discriminator zoo to accommodate the varying networking conditions. Additionally, we extend DCSB to video tasks by developing an adaptive sampling rate update algorithm, aiming to minimize computational demands without sacrificing detection accuracy. Extensive experiments show that DCSB can detect 97.26%-97.96% objects while saving 74.37%-82.23% network bandwidth, compared to cloud-only methods. Furthermore, DCSB significantly outperforms the latest DNN partitioning methods, reducing inference time by 92.60%-95.10% given an 8Mbps transmission bandwidth. In video tasks, DCSB matches the detection accuracy of leading video analysis methods while cutting the computational overhead by 40%.},
  archive      = {J_TMC},
  author       = {Zhiqiang Cao and Yun Cheng and Zimu Zhou and Yongrui Chen and Youbing Hu and Anqi Lu and Jie Liu and Zhijun Li},
  doi          = {10.1109/TMC.2024.3474743},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1181-1196},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Edge-cloud collaborated object detection via bandwidth adaptive difficult-case discriminator},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-target device-free positioning based on spatial-temporal mmWave point cloud. <em>TMC</em>, <em>24</em>(2), 1163-1180. (<a href='https://doi.org/10.1109/TMC.2024.3474671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device-free positioning (DFP) using mmWave signals is an emerging technique that could track a target without attaching any devices. It conducts position estimation by analyzing the influence of targets on their surrounding mmWave signals. With the widespread utilization of mmWave signals, DFP will have many potential applications in tracking pedestrians and robots in intelligent monitoring systems. State-of-the-art DFP work has already achieved excellent positioning performance when there is one target only, but when there are multiple targets, the time-varying target state, such as entering or leaving of the wireless coverage area and close interactions, makes it challenging to track every target. To solve these problems, in this paper, we propose a spatial-temporal analysis method to robustly track multiple targets based on the high precision mmWave point cloud information. Specifically, we propose a high precision spatial imaging strategy to construct fine-grained mmWave point cloud of the targets, design a spatial-temporal point cloud clustering method to determine the target state, and then leverage a gait based identity and trajectory association scheme and a particle filter to achieve robust identity-aware tracking. Extensive evaluations on a 77 GHz mmWave testbed have been conducted to demonstrate the effectiveness and robustness of our proposed schemes.},
  archive      = {J_TMC},
  author       = {Jie Wang and Jingmiao Wu and Yingwei Qu and Qi Xiao and Qinghua Gao and Yuguang Fang},
  doi          = {10.1109/TMC.2024.3474671},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1163-1180},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-target device-free positioning based on spatial-temporal mmWave point cloud},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CV-cast: Computer Vision–Oriented linear coding and transmission. <em>TMC</em>, <em>24</em>(2), 1149-1162. (<a href='https://doi.org/10.1109/TMC.2024.3478048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote inference allows lightweight edge devices, such as autonomous drones, to perform vision tasks exceeding their computational, energy, or processing delay budget. In such applications, reliable transmission of information is challenging due to high variations of channel quality. Traditional approaches involving spatio-temporal transforms, quantization, and entropy coding followed by digital transmission may be affected by a sudden decrease in quality (the digital cliff) when the channel quality is less than expected during design. This problem can be addressed by using Linear Coding and Transmission (LCT), a joint source and channel coding scheme relying on linear operators only, allowing to achieve reconstructed per-pixel error commensurate with the wireless channel quality. In this paper, we propose CV-Cast: the first LCT scheme optimized for computer vision task accuracy instead of per-pixel distortion. Using this approach, for instance at 10 dB channel signal-to-noise ratio, CV-Cast requires transmitting 28% less symbols than a baseline LCT scheme in semantic segmentation and 15% in object detection tasks. Simulations involving a realistic 5G channel model confirm the smooth decrease in accuracy achieved with CV-Cast, while images encoded by JPEG or learned image coding (LIC) and transmitted using classical schemes at low Eb/N0 are subject to digital cliff.},
  archive      = {J_TMC},
  author       = {Jakub Žádník and Michel Kieffer and Anthony Trioux and Markku Mäkitalo and Pekka Jääskeläinen},
  doi          = {10.1109/TMC.2024.3478048},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1149-1162},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CV-cast: Computer Vision–Oriented linear coding and transmission},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TRIMP: Three-sided stable matching for distributed vehicle sharing system using stackelberg game. <em>TMC</em>, <em>24</em>(2), 1132-1148. (<a href='https://doi.org/10.1109/TMC.2024.3475481'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed Vehicle Sharing System (DVSS) leverages emerging technologies such as blockchain to create a secure, transparent, and efficient platform for sharing vehicles. In such a system, both efficient matching of users with available vehicles and optimal pricing mechanisms play crucial roles in maximizing system revenue. However, most existing schemes utilize user-to-vehicle (two-sided) matching and pricing, which are unrealistic for DVSS due to the lack of participation of service providers. To address this issue, we propose in this paper a novel Three-sided stable Matching with an optimal Pricing (TRIMP) scheme. First, to achieve maximum utilities for all three parties simultaneously, we formulate the optimal policy and pricing problem as a three-stage Stackelberg game and derive its equilibrium points accordingly. Second, relying on these solutions from the Stackelberg game, we construct a three-sided cyclic matching for DVSS. Third, as the existence of such a matching is NP-complete, we design a specific vehicle sharing algorithm to realize stable matching. Extensive experiments demonstrate the effectiveness of our TRIMP scheme, which optimizes the matching process and ensures efficient resource allocation, leading to a more stable and well-functioning decentralized vehicle sharing ecosystem.},
  archive      = {J_TMC},
  author       = {Yang Xu and Shanshan Zhang and Chen Lyu and Jia Liu and Tarik Taleb and Shiratori Norio},
  doi          = {10.1109/TMC.2024.3475481},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1132-1148},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TRIMP: Three-sided stable matching for distributed vehicle sharing system using stackelberg game},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harmonizing global and local class imbalance for federated learning. <em>TMC</em>, <em>24</em>(2), 1120-1131. (<a href='https://doi.org/10.1109/TMC.2024.3476340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is to collaboratively train a global model among distributed clients by iteratively aggregating their local updates without sharing their raw data, whereby the global modal can approximately converge to the centralized training way over a global dataset that composed of all local datasets (i.e., union of all users’ local data). However, in real-world scenarios, the distributions of the data classes are often imbalanced not only locally, but also in the global dataset, which severely deteriorate the FL performance due to the conflicting knowledge aggregation. Existing solutions for FL class imbalance either focus on the local data to regulate the training process or purely aim at the global datasets, which often fail to alleviate the class imbalance problem if there is mismatch between the local and global imbalance. Considering these limitations, this paper proposes a Global-Local Joint Learning method, namely GLJL, which simultaneously harmonizes the global and local class imbalance issue by jointly embedding the local and the global factors into each client’s loss function. Through extensive experiments over popular datasets with various class imbalance settings, we show that the proposed method can significantly improve the model accuracy over minority classes without sacrificing the accuracy of other classes.},
  archive      = {J_TMC},
  author       = {Jialiang Zhu and Hao Zheng and Wenchao Xu and Haozhao Wang and Zhiming He and Yuxuan Liu and Shuang Wang and Qi Sun},
  doi          = {10.1109/TMC.2024.3476340},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1120-1131},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Harmonizing global and local class imbalance for federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application adaptive light-weight deep learning (AppAdapt-LWDL) framework for enabling edge intelligence in dairy processing. <em>TMC</em>, <em>24</em>(2), 1105-1119. (<a href='https://doi.org/10.1109/TMC.2024.3475634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dairy industry is experiencing a surge in data from Edge devices, using spectroscopic techniques for milk quality assessment. Milk spectral data can help understand the species of milk producer and detect inter-species adulteration. Transmitting raw milk spectral data to the cloud for processing faces challenges due to limited network resources such as bandwidth, computational memory, and energy availability. Edge processing offers a solution by training data closer to the source, enhancing efficiency and real-time analysis by providing reduced latency, improved accuracy, resource-aware computation, and real-time customization. However, traditional Deep Learning (DL) methods such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) struggle on resource-constrained Edge devices due to complexity. To address this, we propose an Edge-Centric Application-Adaptive Light-Weight DL approach (AppAdapt-LWDL) for milk species identification and adulteration detection. Our method optimizes DL models via double model optimization, involving low-magnitude pruning and post-training quantization. Our novel application-adaptive algorithm balances speed and accuracy by determining the pruning ratio automatically for the specific application. The chosen model is then quantized for smaller databases, ideal for embedded devices. The AppAdapt-LWDL framework significantly accelerates training, speeds up inferencing, enhances energy efficiency, and maintains accuracy based on application needs.},
  archive      = {J_TMC},
  author       = {Rahul Umesh Mhapsekar and Lizy Abraham and Steven Davy and Indrakshi Dey},
  doi          = {10.1109/TMC.2024.3475634},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1105-1119},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Application adaptive light-weight deep learning (AppAdapt-LWDL) framework for enabling edge intelligence in dairy processing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). End-to-end steady-state adaptive slicing method for dynamic network state and load. <em>TMC</em>, <em>24</em>(2), 1090-1104. (<a href='https://doi.org/10.1109/TMC.2024.3473908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing has become a primary function of 5G/6G network resource management. However, the existing slicing schemes have not sufficiently discussed the reconfiguration optimization schemes brought by user behavior changes and mobile network environment fluctuations, leading to excessive service interruption rates and slice reconfiguration costs in dynamic environments. To address this problem, this paper proposes an End-to-end Steady-state Adaptive slicing method for Dynamic network state and load (ESAD). To realize the steady-state slicing decisions, ESAD takes the steady-state degree of network slicing and reconfiguration cost as the objective and constructs the slicing reconfiguration probability evaluation function based on the service load dynamics function and the time-varying function of the network channel conditions. To improve the predictability and steady-state degree of the slicing decision, ESAD introduces an ensemble deep learning method to predict the load service fluctuation based on the user behavior model and employs reinforcement learning to compute the channel dynamics boundary, which guides the slicing decision to balance the network dynamics factors. Experiments on quality of service assurance for 5G cloud game rendering class prove that ESAD can reduce reconfiguration probability and long-term reconfiguration cost by 49.45%–58.50% while improving system QoS assurance and capacity.},
  archive      = {J_TMC},
  author       = {Boyi Tang and Yijun Mo and Chen Yu and Huiyu Liu},
  doi          = {10.1109/TMC.2024.3473908},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1090-1104},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {End-to-end steady-state adaptive slicing method for dynamic network state and load},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual class-aware contrastive federated semi-supervised learning. <em>TMC</em>, <em>24</em>(2), 1073-1089. (<a href='https://doi.org/10.1109/TMC.2024.3474732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated semi-supervised learning (FSSL), facilitates labeled clients and unlabeled clients jointly training a global model without sharing private data. Existing FSSL methods predominantly employ pseudo-labeling and consistency regularization to exploit the knowledge of unlabeled data, achieving notable success in raw data utilization. However, the effectiveness of these methods is challenged by large deviations between uploaded local models of labeled and unlabeled clients, as well as confirmation bias introduced by noisy pseudo-labels, both of which negatively affect the global model's performance. In this paper, we present a novel FSSL method called Dual Class-aware Contrastive Federated Semi-Supervised Learning (DCCFSSL). This method considers both the local class-aware distribution of each client's data and the global class-aware distribution of all clients’ data within the feature space. By implementing a dual class-aware contrastive module, DCCFSSL establishes a unified training objective for different clients to tackle large deviations and incorporates contrastive information in the feature space to mitigate confirmation bias. Additionally, DCCFSSL introduces an authentication-reweighted aggregation technique to improve the server's aggregation robustness. Our comprehensive experiments show that DCCFSSL outperforms current state-of-the-art methods on three benchmark datasets and surpasses the FedAvg with relabeled unlabeled clients on CIFAR-10, CIFAR-100, and STL-10 datasets.},
  archive      = {J_TMC},
  author       = {Qi Guo and Di Wu and Yong Qi and Saiyu Qi},
  doi          = {10.1109/TMC.2024.3474732},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1073-1089},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dual class-aware contrastive federated semi-supervised learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Long-term or temporary? hybrid worker recruitment for mobile crowd sensing and computing. <em>TMC</em>, <em>24</em>(2), 1055-1072. (<a href='https://doi.org/10.1109/TMC.2024.3470993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores an interesting worker recruitment challenge where the mobile crowd sensing and computing (MCSC) platform hires workers to complete tasks with varying quality requirements and budget limitations, amidst uncertainties in worker participation and local workloads. We propose an innovative hybrid worker recruitment framework that combines offline and online trading modes. The offline mode enables the platform to overbook long-term workers by pre-signing contracts, thereby managing dynamic service supply. This is modeled as a 0-1 integer linear programming (ILP) problem with probabilistic constraints on service quality and budget. To address the uncertainties that may prevent long-term workers from consistently meeting service quality standards, we also introduce an online temporary worker recruitment scheme as a contingency plan. This scheme ensures seamless service provisioning and is likewise formulated as a 0-1 ILP problem. To tackle these problems with NP-hardness, we develop three algorithms, namely, i) exhaustive searching, ii) unique index-based stochastic searching with risk-aware filter constraint, iii) geometric programming-based successive convex algorithm. These algorithms are implemented in a stagewise manner to achieve optimal or near-optimal solutions. Extensive experiments demonstrate our effectiveness in terms of service quality, time efficiency, etc.},
  archive      = {J_TMC},
  author       = {Minghui Liwang and Zhibin Gao and Seyyedali Hosseinalipour and Zhipeng Cheng and Xianbin Wang and Zhenzhen Jiao},
  doi          = {10.1109/TMC.2024.3470993},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1055-1072},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Long-term or temporary? hybrid worker recruitment for mobile crowd sensing and computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monitoring correlated sources: AoI-based scheduling is nearly optimal. <em>TMC</em>, <em>24</em>(2), 1043-1054. (<a href='https://doi.org/10.1109/TMC.2024.3471391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the design of scheduling policies to minimize the monitoring error of a collection of correlated sources, where only one source can be observed at any given time. We model correlated sources as a discrete-time Wiener process, where the increments are multivariate normal random variables, with a general covariance matrix that captures the correlation structure between the sources. Under a Kalman filter-based optimal estimation framework, we show that the performance of all scheduling policies oblivious to instantaneous error can be lower and upper bounded by the weighted sum of Age of Information (AoI) across the sources for appropriately chosen weights. We use this insight to design scheduling policies that are only a constant factor away from optimality, and make the rather surprising observation that AoI-based scheduling that ignores correlation is sufficient to obtain performance guarantees. We also derive scaling results showing that the optimal error scales roughly as the square of the system's dimensionality, even with correlation. Finally, we provide simulation results to verify our claims.},
  archive      = {J_TMC},
  author       = {Rudrapatna Vallabh Ramakanth and Vishrant Tripathi and Eytan Modiano},
  doi          = {10.1109/TMC.2024.3471391},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1043-1054},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Monitoring correlated sources: AoI-based scheduling is nearly optimal},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast quantum convolutional neural networks for low-complexity object detection in autonomous driving applications. <em>TMC</em>, <em>24</em>(2), 1031-1042. (<a href='https://doi.org/10.1109/TMC.2024.3470328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection applications, especially in autonomous driving, have drawn attention due to the advancements in deep learning. Additionally, with continuous improvements in classical convolutional neural networks (CNNs), there has been a notable enhancement in both the efficiency and speed of these applications, making autonomous driving more reliable and effective. However, due to the exponentially rapid growth in the complexity and scale of visual signals used in object detection, there are limitations regarding computation speeds while conducting object detection solely with classical computing. Motivated by this, this paper proposes the quantum object detection engine (QODE), which implements a quantum version of CNN, named QCNN, in object detection. Furthermore, this paper proposes a novel fast quantum convolution algorithm that processes the multi-channel of visual signals based on a small number of qubits and constructs the output channel data, thereby achieving relieved computational complexity. Our QODE, equipped with fast quantum convolution, demonstrates feasibility in object detection with multi-channel data, addressing a limitation of current QCNNs due to the scarcity of qubits in the current era of quantum computing. Moreover, this paper introduces a heterogeneous knowledge distillation training algorithm that enhances the performance of our QODE.},
  archive      = {J_TMC},
  author       = {Emily Jimin Roh and Hankyul Baek and Donghyeon Kim and Joongheon Kim},
  doi          = {10.1109/TMC.2024.3470328},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1031-1042},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Fast quantum convolutional neural networks for low-complexity object detection in autonomous driving applications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of data acquisition and trajectory planning for UAV-assisted wireless powered internet of things. <em>TMC</em>, <em>24</em>(2), 1016-1030. (<a href='https://doi.org/10.1109/TMC.2024.3470831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of Internet of Things (IoT) technology has led to the emergence of a large number of Intelligent Sensing Devices (ISDs). Since their limited physical sizes constrain the battery capacity, wireless powered IoT networks assisted by Unmanned Aerial Vehicles (UAVs) for energy transfer and data acquisition have attracted great interest. In this paper, we formulate an optimization problem to maximize system energy efficiency while satisfying the constraints of UAV mobility and safety, ISD quality of service and task completion time. The formulated problem is constructed as a Constrained Markov Decision Process (CMDP) model, and a Multi-agent Constrained Deep Reinforcement Learning (MCDRL) algorithm is proposed to learn the optimal UAV movement policy. In addition, an ISD-UAV connection assignment algorithm is designed to manage the connection in the UAV sensing range. Finally, performance evaluations and analysis based on real-world data demonstrate the superiority of our solution.},
  archive      = {J_TMC},
  author       = {Zhaolong Ning and Hongjing Ji and Xiaojie Wang and Edith C. H. Ngai and Lei Guo and Jiangchuan Liu},
  doi          = {10.1109/TMC.2024.3470831},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {1016-1030},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of data acquisition and trajectory planning for UAV-assisted wireless powered internet of things},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unknown worker recruitment with long-term incentive in mobile crowdsensing. <em>TMC</em>, <em>24</em>(2), 999-1015. (<a href='https://doi.org/10.1109/TMC.2024.3471569'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many mobile crowdsensing applications require efficient recruitment of workers whose qualities are often unknown a priori. While prior research has explored multi-armed bandit-based mechanisms with short-term incentives to address this unknown worker recruitment challenge, these mechanisms mostly neglect the enduring participation issues stemming from privacy concern and selection starvation in the long-term task. Therefore, in this paper, we focus on incentivizing long-term participation of unknown workers, thereby providing crucial assurance for crowdsensing applications. We first establish an auction framework based on shuffle differential privacy (SDP), where we leverage SDP’s privacy amplification effect to mitigate privacy-related utility loss when dealing with the privacy-sensitive worker and the utility-sensitive platform. Following this, we model the selection requirements of workers as fairness constraints and propose two novel fairness-aware incentive mechanisms, GFA and IFA, to ensure group and individual fairness for unknown workers, respectively. Theoretical analyses highlight the desirable properties of GFA and IFA, complemented by an in-depth exploration of fairness violation and regret. Finally, numerical simulations are conducted on two real-world datasets, validating the superior performance of the proposed mechanisms.},
  archive      = {J_TMC},
  author       = {Qihang Zhou and Xinglin Zhang and Zheng Yang},
  doi          = {10.1109/TMC.2024.3471569},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {999-1015},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Unknown worker recruitment with long-term incentive in mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedSiam-DA: Dual-aggregated federated learning via siamese network for non-IID data. <em>TMC</em>, <em>24</em>(2), 985-998. (<a href='https://doi.org/10.1109/TMC.2024.3472898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an effective mobile edge computing framework that enables multiple participants to collaboratively train intelligent models, without requiring large amounts of data transmission while protecting privacy. However, FL encounters challenges due to non-independent and identically distributed (non-IID) data from different participants. The existing methods, whether focusing on local training or global aggregation, often suffer from insufficient unilateral optimization. Achieving effective local-global collaborative optimization, particularly in the absence of additional reference models or datasets, is both crucial and challenging. To address this, we propose a novel approach: Dual-Aggregated Federated learning based on a triple Siamese network (FedSiam-DA). This method enhances the FL algorithm on both client and server sides. On the client side, we establish a triple Siamese network incorporating a stop-gradient scheme, which leverages a contrastive learning strategy to control the update directions of local models. On the server side, we introduce a dual aggregation mechanism with dynamic weights for local updates, improving the global model’s ability to assimilate personalized knowledge from local models. Extensive experiments on multiple benchmark datasets demonstrate that FedSiam-DA significantly improves model performance under non-IID data conditions compared to existing methods.},
  archive      = {J_TMC},
  author       = {Xin Wang and Yanhan Wang and Ming Yang and Feng Li and Xiaoming Wu and Lisheng Fan and Shibo He},
  doi          = {10.1109/TMC.2024.3472898},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {985-998},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedSiam-DA: Dual-aggregated federated learning via siamese network for non-IID data},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScooterID: Posture-based continuous user identification from mobility scooter rides. <em>TMC</em>, <em>24</em>(2), 970-984. (<a href='https://doi.org/10.1109/TMC.2024.3473609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobility scooters serve as a powerful last-mile transportation tool for people with mobility challenges. Given the unique riding behavior and posture of mobility scooter riders, such user-specific mobility scooter ride data has tremendous potential towards the design of continuous user identification and authentication mechanisms. However, there have been no prior research efforts in the literature exploring this unique modality for the design of continuous user identification techniques. To address this gap, this paper proposes ScooterID, the first framework which employs rider posture data collected from cameras on mobility scooters to continuously identify (and authenticate) users/riders. As part of this framework, a machine learning based model comprising of a spatio-temporal Graph Convolutional Network and a body-part-informed encoder is designed to effectively capture a user’s subtle upper-body movements during mobility scooter rides into discriminating embedding vectors. These embeddings can then be used to reliably and continuously identify and authenticate users/riders. Experiments with real-world mobility scooter ride data show that ScooterID achieves high levels of authentication accuracy with few enrollment video samples. ScooterID also performs efficiently on resource-constrained devices (e.g., Raspberry Pis) and is robust against adversarial perturbations to authentication inputs.},
  archive      = {J_TMC},
  author       = {Devan Shah and Ruoqi Huang and Nisha Vinayaga-Sureshkanth and Tingting Chen and Murtuza Jadliwala},
  doi          = {10.1109/TMC.2024.3473609},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {970-984},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ScooterID: Posture-based continuous user identification from mobility scooter rides},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handling failures in secondary radio access failure handling in operational 5G networks. <em>TMC</em>, <em>24</em>(2), 956-969. (<a href='https://doi.org/10.1109/TMC.2024.3477462'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we conduct a measurement study with three US operators to reveal three types of problematic failure handling on secondary radio access which have not been reported before. Compared to primary radio access failures, secondary radio access failures do not hurt radio access availability but significantly impact data performance, particularly when 5G is used as secondary radio access to boost throughput. Improper failure handling results in significant throughput loss, which is unnecessary in most instances. We then pinpoint the root causes behind these three types of problematic failure handling. When 5G provides higher throughput, failures are more likely to be falsely triggered by a specific event, causing the User Equipment (UE) to unnecessarily lose well-performing 5G connections. Moreover, after failures, the recovery of secondary radio access may fail due to inconsistent parameter settings or be delayed due to missing specific signaling fields. To address these issues, we propose SCGFailure Manager (SFM), a solution to optimize the detection and recovery of secondary radio access failures. Our evaluation results demonstrate that SFM can effectively avoid 60%-80% of problematic failure handling and double throughput in more than half of failure instances.},
  archive      = {J_TMC},
  author       = {Yanbing Liu and Chunyi Peng},
  doi          = {10.1109/TMC.2024.3477462},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {956-969},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Handling failures in secondary radio access failure handling in operational 5G networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based SIC ordering and power allocation for non-orthogonal multiple access networks. <em>TMC</em>, <em>24</em>(2), 939-955. (<a href='https://doi.org/10.1109/TMC.2024.3470828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-orthogonal multiple access (NOMA) emerges as a superior technology for enhancing spectral efficiency, reducing latency, and improving connectivity compared to orthogonal multiple access. In NOMA networks, successive interference cancellation (SIC) plays a crucial role in decoding user signals sequentially. The challenge lies in the joint optimization of SIC ordering and power allocation, a task complicated by the factorial nature of ordering combinations. This study introduces an innovative solution, the Attention-based SIC Ordering and Power Allocation (ASOPA) framework, targeting an uplink NOMA network with dynamic SIC ordering. ASOPA aims to maximize weighted proportional fairness by employing deep reinforcement learning, strategically decomposing the problem into two manageable subproblems: SIC ordering optimization and optimal power allocation. We use an attention-based neural network to process real-time channel gains and user weights, determining the SIC decoding order for each user. A baseline network, serving as a mimic model, aids in the reinforcement learning process. Once the SIC ordering is established, the power allocation subproblem transforms into a convex optimization problem, enabling efficient calculation of optimal transmit power for all users. Extensive simulations validate ASOPA’s efficacy, demonstrating a performance closely paralleling the exhaustive method, with over 97% confidence in normalized network utility. Compared to the current state-of-the-art implementation, i.e., Tabu search, ASOPA achieves over 97.5% network utility of Tabu search. Furthermore, ASOPA has two orders of magnitude less execution latency than Tabu search when $N=10$ and even three orders magnitude less execution latency less than Tabu search when $N=20$ . Notably, ASOPA maintains a low execution latency of approximately 50 milliseconds in a ten-user NOMA network, aligning with static SIC ordering algorithms. Furthermore, ASOPA demonstrates superior performance over baseline algorithms besides Tabu search in various NOMA network configurations, including scenarios with imperfect channel state information, multiple base stations, and multiple-antenna setups. These results underscore the robustness and effectiveness of ASOPA, demonstrating its ability to ability to achieve good performance across various NOMA network environments.},
  archive      = {J_TMC},
  author       = {Liang Huang and Bincheng Zhu and Runkai Nan and Kaikai Chi and Yuan Wu},
  doi          = {10.1109/TMC.2024.3470828},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {939-955},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Attention-based SIC ordering and power allocation for non-orthogonal multiple access networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure localization for underwater wireless sensor networks via AUV cooperative beamforming with reinforcement learning. <em>TMC</em>, <em>24</em>(2), 924-938. (<a href='https://doi.org/10.1109/TMC.2024.3472643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In harsh underwater environments, the localization of network nodes faces severe challenges due to open deployment environments. Most existing underwater localization methods suffer from privacy leaks. However, privacy protection schemes applied in terrestrial networks are not viable for underwater acoustic networks due to stratification effects and multipath complexities. In this paper, we introduce a secure localization scheme for underwater wireless sensor networks (UWSNs) utilizing cooperative beamforming among mobile underwater anchor nodes. With this scheme, the underwater sensor communicates and ranges with mobile anchor nodes to perform self-localization via time difference of arrival (TDOA) algorithm. However, the presence of eavesdroppers poses a threat by intercepting information emitted by the anchors. To avoid localization information leakage, then we model the secure localization requirement as a multi-anchors multi-objective dual joint optimization problem to enhance both security and energy performance. The deep reinforcement learning (DRL)-based multi-agent deep deterministic policy gradient (MADDPG) algorithm is applied to solve the optimization problem. Both simulation and field experimental results robustly validate the efficiency and accuracy of the proposed secure localization scheme.},
  archive      = {J_TMC},
  author       = {Rong Fan and Azzedine Boukerche and Pan Pan and Zhigang Jin and Yishan Su and Fei Dou},
  doi          = {10.1109/TMC.2024.3472643},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {924-938},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Secure localization for underwater wireless sensor networks via AUV cooperative beamforming with reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive blind beamforming for intelligent surface. <em>TMC</em>, <em>24</em>(2), 907-923. (<a href='https://doi.org/10.1109/TMC.2024.3468618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Configuring intelligent surface (IS) or passive antenna array without any channel knowledge, namely blind beamforming, is a frontier research topic in the wireless communication field. Existing methods in the previous literature for blind beamforming include the RFocus and the CSM, the effectiveness of which has been demonstrated on hardware prototypes. However, this paper points out a subtle issue with these blind beamforming algorithms: the RFocus and the CSM may fail to work in the non-line-of-sight (NLoS) channel case. To address this issue, we suggest a grouping strategy that enables adaptive blind beamforming. Specifically, the reflective elements (REs) of the IS are divided into three groups; each group is configured randomly to obtain a dataset of random samples. We then extract the statistical feature of the wireless environment from the random samples, thereby coordinating phase shifts of the IS without channel acquisition. The RE grouping plays a critical role in guaranteeing performance gain in the NLoS case. In particular, if we place all the REs in the same group, the proposed algorithm would reduce to the RFocus and the CSM. We validate the advantage of the proposed blind beamforming algorithm in the real-world networks at 3.5 GHz aside from simulations.},
  archive      = {J_TMC},
  author       = {Wenhai Lai and Wenyu Wang and Fan Xu and Xin Li and Shaobo Niu and Kaiming Shen},
  doi          = {10.1109/TMC.2024.3468618},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {907-923},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive blind beamforming for intelligent surface},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). O-RAN-enabled intelligent network slicing to meet service-level agreement (SLA). <em>TMC</em>, <em>24</em>(2), 890-906. (<a href='https://doi.org/10.1109/TMC.2024.3476338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing plays a critical role in enabling multiple virtualized and independent network services to be created on top of a common physical network infrastructure. In this paper, we introduce a deep reinforcement learning (DRL)-based radio resource management (RRM) solution for radio access network (RAN) slicing under service-level agreement (SLA) guarantees. The objective of this solution is to minimize the SLA violation. Our method is designed with a two-level scheduling structure that works seamlessly under Open Radio Access Network (O-RAN) architecture. Specifically, at an upper level, a DRL-based inter-slice scheduler is working on a coarse time granularity to allocate resources to network slices. And at a lower level, an existing intra-slice scheduler such as proportional fair (PF) is working on a fine time granularity to allocate slice dedicated resources to slice users. This setting makes our solution O-RAN compliant and ready to be deployed as an ‘xApp’ on the RAN Intelligent Controller (RIC). For performance evaluation and proof of concept purposes, we develop two platforms, one industry-level simulator and one O-RAN compliant testbed; evaluation on both platforms demonstrates our solution’s superior performance over conventional methods.},
  archive      = {J_TMC},
  author       = {Jiongyu Dai and Lianjun Li and Ramin Safavinejad and Shadab Mahboob and Hao Chen and Vishnu V Ratnam and Haining Wang and Jianzhong Zhang and Lingjia Liu},
  doi          = {10.1109/TMC.2024.3476338},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {890-906},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {O-RAN-enabled intelligent network slicing to meet service-level agreement (SLA)},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resource collaboration between satellite and wide-area mobile base stations in integrated satellite-terrestrial network. <em>TMC</em>, <em>24</em>(2), 875-889. (<a href='https://doi.org/10.1109/TMC.2024.3472081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integrated satellite-terrestrial network with cascaded downlinks from satellites to wide-area mobile base stations and subsequently to terrestrial users enables global communication for terrestrial 4G/5G cellular users and is widely used in emergency rescue scenarios. However, in this network, satellites and wide-area mobile base stations are controlled by distinct resource scheduling systems with disparate packet queues, which means resources allocated by the satellite to the wide-area mobile base stations may not match the resources allocated by the wide-area mobile base stations to the terrestrial users, leading to coordination inefficiencies and resource wastage. To tackle this challenge, a resource collaborative scheduling mechanism based on cooperative game theory for cascaded downlinks is established, which effectively adapts to distinct resource scheduling systems with various QoS constraints. Then, the utility function of the Nash product is converted into a max-min problem, and a convex transformation method is proposed for the non-convex optimization problem. Simulation results demonstrate that the proposed collaborative scheduling mechanism effectively improves resource utilization and the transmission rate of cascaded downlinks.},
  archive      = {J_TMC},
  author       = {Zhen Li and Chunxiao Jiang and Jiachen Sun and Jianhua Lu},
  doi          = {10.1109/TMC.2024.3472081},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {875-889},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Resource collaboration between satellite and wide-area mobile base stations in integrated satellite-terrestrial network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interference recycling: Effective utilization of interference for enhancing data transmission. <em>TMC</em>, <em>24</em>(2), 859-874. (<a href='https://doi.org/10.1109/TMC.2024.3467339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of wireless communication technologies, Internet of Things (IoT) has emerged as one of the most important application scenarios. Due to the high density of IoT devices and the limited spectrum resources, along with the miniaturization and sustainability requirements of these devices, the development of low-cost interference management (IM) methods has become crucial for widespread use of IoT. Interference has long been known to harm network performance. Since a desired signal can be distorted by interference, and thus be incorrectly decoded at the destination, we argue that interference can also be transformed intentionally to extract the desired data from interfering signal(s). Based on this observation, we propose Interference ReCycling (IRC) for the IoT. Under IRC, a recycling signal is generated using the interference a victim IoT device is subjected to, and then sent by the device’s associated gateway. Under the influence of the recycling signal, the desired data of the interfered/victim IoT transmission-pair can be recovered from the interference at the IoT device. We also show that the interfered user’s spectral efficiency (SE) with IRC can be optimized further by properly distributing the transmit power used for the desired signal’s transmission and the recycling signal. We validate the feasibility of IRC by implementing the method on the Universal Software Radio Peripheral (USRP) platform. Our theoretical analysis, experimental and numerical evaluation have shown that the proposed IRC can fully exploit interference, and hence can significantly improve the SE of the victim IoT device compared to other existing IM methods.},
  archive      = {J_TMC},
  author       = {Zhao Li and Lijuan Zhang and Chengyu Liu and Siwei Le and Jie Chen and Kang G. Shin and Zheng Yan and Jia Liu},
  doi          = {10.1109/TMC.2024.3467339},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {859-874},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Interference recycling: Effective utilization of interference for enhancing data transmission},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaWiFi, collaborative WiFi sensing for cross-environment adaptation. <em>TMC</em>, <em>24</em>(2), 845-858. (<a href='https://doi.org/10.1109/TMC.2024.3474853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) based Wi-Fi sensing has witnessed great development in recent years. Although decent results have been achieved in certain scenarios, Wi-Fi based activity recognition is still difficult to deploy in real smart homes due to the limited cross-environment adaptability, i.e. a well-trained Wi-Fi sensing neural network in one environment is hard to adapt to other environments. To address this challenge, we propose AdaWiFi, a DL-based Wi-Fi sensing framework that allows multiple Internet-of-Things (IoT) devices to collaborate and adapt to various environments effectively. The key innovation of AdaWiFi includes a collective sensing model architecture that utilizes complementary information between distinct devices and avoids the biased perception of individual sensors and an accompanying model adaptation technique that can transfer the sensing model to new environments with limited data. We evaluate our system on a public dataset and a custom dataset collected from three complex sensing environments. The results demonstrate that AdaWiFi is able to achieve significantly better sensing adaptation effectiveness (e.g. 30% higher accuracy with one-shot adaptation) as compared with state-of-the-art baselines.},
  archive      = {J_TMC},
  author       = {Naiyu Zheng and Yuanchun Li and Shiqi Jiang and Yuanzhe Li and Rongchun Yao and Chuchu Dong and Ting Chen and Yubo Yang and Zhimeng Yin and Yunxin Liu},
  doi          = {10.1109/TMC.2024.3474853},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {845-858},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AdaWiFi, collaborative WiFi sensing for cross-environment adaptation},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum-assisted joint virtual network function deployment and maximum flow routing for space information networks. <em>TMC</em>, <em>24</em>(2), 830-844. (<a href='https://doi.org/10.1109/TMC.2024.3466857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network function virtualization (NFV)-enabled space information network (SIN) has emerged as a promising method to facilitate global coverage and seamless service. This paper proposes a novel NFV-enabled SIN to provide end-to-end communication and computation services for ground users. Based on the multi-functional time expanded graph (MF-TEG), we jointly optimize the user association, virtual network function (VNF) deployment, and flow routing strategy (U-VNF-R) to maximize the total processed data received by users. The original problem is a mixed-integer linear program (MILP) that is intractable for classical computers. Inspired by quantum computing techniques, we propose a hybrid quantum-classical Benders’ decomposition (HQCBD) algorithm. Specifically, we convert the master problem of the Benders’ decomposition into the quadratic unconstrained binary optimization (QUBO) model and solve it with quantum computers. To further accelerate the optimization, we also design a multi-cut strategy based on the quantum advantages in parallel computing. Numerical results demonstrate the effectiveness and efficiency of the proposed algorithm and U-VNF-R scheme.},
  archive      = {J_TMC},
  author       = {Yu Zhang and Yanmin Gong and Lei Fan and Yu Wang and Zhu Han and Yuanxiong Guo},
  doi          = {10.1109/TMC.2024.3466857},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {830-844},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Quantum-assisted joint virtual network function deployment and maximum flow routing for space information networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From cells to freedom: 6G's evolutionary shift with cell-free massive MIMO. <em>TMC</em>, <em>24</em>(2), 812-829. (<a href='https://doi.org/10.1109/TMC.2024.3468003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cell-free massive MIMO (CF-mMIMO) is emerging as a technological pillar for future sixth generation (6G) mobile networks, promising consistently high spectral and energy efficiencies across the coverage area. Despite the reported advantages of CF-mMIMO over traditional cellular-based massive MIMO (mMIMO), the extensive deployment of access points (APs) and the associated fronthaul links present significant economic and logistical challenges. This paper proposes a transitional framework to facilitate the gradual integration of CF-mMIMO into existing cellular infrastructures, allowing mobile network operators to progressively realize the benefits of a distributed network topology. A comprehensive analysis of the spectral, energy, and computational efficiencies in heterogeneous network scenarios, incorporating both macrocellular and cell-free components, is presented. Our contributions include a unified assessment framework encompassing spectral, energy and computational aspects, a novel channel virtualization mechanism for effective downlink precoding, and a realistic industry-backed power consumption model for joint network operation. The potential performance gains are demonstrated and guidelines for the incremental deployment of CF-mMIMO are provided through detailed numerical results, ensuring a balanced trade-off between integration costs and operational benefits. This approach aims to leverage the capabilities of emerging network architectures to achieve a seamless evolution towards fully distributed 6G networks.},
  archive      = {J_TMC},
  author       = {Guillem Femenias and Felip Riera-Palou},
  doi          = {10.1109/TMC.2024.3468003},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {812-829},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {From cells to freedom: 6G's evolutionary shift with cell-free massive MIMO},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint configuration optimization and GPU allocation for multi-tenant real-time video analytics on resource-constrained edge. <em>TMC</em>, <em>24</em>(2), 794-811. (<a href='https://doi.org/10.1109/TMC.2024.3465434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying deep neural network (DNN) models on resource-constrained edge devices for real-time video analytics poses significant challenges due to the high resource demands of these models. Current edge-based video analytics approaches often overlook optimizing deep learning models and GPU resource allocations in multi-tenant scenarios. In this paper, we present JSAS-MTMGS, a collaborative video analytics system employing three innovative design strategies. First, we propose a novel video configuration optimization space based on a joint DNN model sharing and splitting scheme to balance computational loads for collaborative processing. This approach reduces network transmission data volume and alleviates resource contention. Second, we design a GPU resource allocation scheme that combines GPU batching with spatial sharing to optimize GPU utilization and increase system throughput, all without relying on costly offline latency collection. Finally, we define the configuration optimization problem alongside GPU allocation as a convex problem and apply convex optimization to make scheduling decisions dynamically. Our experiments demonstrate that JSAS-MTMGS has the best service quality among all compared algorithms.},
  archive      = {J_TMC},
  author       = {Hanling Wang and Tianyu Li and Mei Zhang and Qing Li and Huan Cui and Yong Jiang and Zhenhui Yuan},
  doi          = {10.1109/TMC.2024.3465434},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {794-811},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint configuration optimization and GPU allocation for multi-tenant real-time video analytics on resource-constrained edge},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and error-free secret key generation leveraging sorted indices matching. <em>TMC</em>, <em>24</em>(2), 779-793. (<a href='https://doi.org/10.1109/TMC.2024.3465042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secret key generation exploiting inherent channel randomness stands as an important paradigm for physical-layer security in wireless networks. However, existing work relying on quantization has some difficulties in eliminating inconsistent key bits due to the impact of ambient noise. Recent studies propose to match the segmented channel samples (i.e., channel episodes) of similar variation patterns between legitimate peers to achieve error-free key generation, but they also suffer from high computational overhead and reduced accuracy for large key lengths. This work proposes a secret key generation method based on sorted indices matching (SIM-SKG), aiming at efficient and error-free key generation. Specifically, we sort the channel samples to ensure each channel episode with a unique variation pattern for accurate matching. To avoid the impact of half-duplex communication mode and ambient noise, we propose to match the indices instead of the channel samples as in existing studies. We also develop a noise perturbation scheme that further mitigates the ambiguity during indices matching. Extensive experimental studies demonstrate the high efficiency and accuracy of SIM-SKG under various scenarios for both RSS and CSI channel measurements. Specifically, SIM-SKG achieves error-free key generation with a length of 2048 bits within as little as 1.7 $msec$. Moreover, theoretical analyses and experiments also confirm the security of the SIM-SKG method against various attacks.},
  archive      = {J_TMC},
  author       = {Yicong Du and Huan Dai and Hongbo Liu and Guyue Li and Yanzhi Ren and Ke Zhang},
  doi          = {10.1109/TMC.2024.3465042},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {779-793},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient and error-free secret key generation leveraging sorted indices matching},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Catch me if you can: Deep meta-RL for search-and-rescue using LoRa UAV networks. <em>TMC</em>, <em>24</em>(2), 763-778. (<a href='https://doi.org/10.1109/TMC.2024.3468382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-range (LoRa) wireless networks have been widely proposed as efficient wireless access networks for battery-constrained Internet of Things (IoT) devices. However, applying the LoRa-based IoT network in search-and-rescue (SAR) operations will have limited coverage caused by high signal attenuation due to terrestrial blockages, especially in highly remote areas. To overcome this challenge, using unmanned aerial vehicles (UAVs) as a flying LoRa gateway to transfer messages from ground LoRa nodes to the ground rescue station can be a promising solution. In this paper, an artificial intelligence-empowered SAR operation framework using a UAV-assisted LoRa network in different unknown search environments is designed and implemented. The problem of the flying LoRa (FL) gateway control policy is modeled as a partially observable Markov decision process to move the UAV towards the LoRa transmitter carried by a lost person in the known remote search area. A deep reinforcement learning (RL)-based policy is designed to determine the adaptive FL gateway trajectory in a given search environment. Then, as a general solution, a deep meta-RL framework is used for SAR in any new and unknown environments. The proposed deep meta-RL framework integrates the information of the prior FL gateway experience in the previous SAR environments to the new environment and then rapidly adapts the UAV control policy model for SAR operation in a new and unknown environment. To analyze the performance of the proposed framework in real-world scenarios, the proposed SAR system is experimentally tested in three environments: a university campus, a wide plain, and a slotted canyon at Mongasht mountain ranges, Iran. Experimental results show that if the deep meta-RL-based control policy is applied instead of the deep RL-based one, the number of SAR time slots decreases from 141 to 50. Moreover, in the slotted canyon environment, the UAV energy consumption under the deep meta-RL policy is respectively 57% and 23% less than the deep RL and Actor-Critic RL policies.},
  archive      = {J_TMC},
  author       = {Mehdi Naderi Soorki and Hossein Aghajari and Sajad Ahmadinabi and Hamed Bakhtiari Babadegani and Christina Chaccour and Walid Saad},
  doi          = {10.1109/TMC.2024.3468382},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {763-778},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Catch me if you can: Deep meta-RL for search-and-rescue using LoRa UAV networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D facial tracking and user authentication through lightweight single-ear biosensors. <em>TMC</em>, <em>24</em>(2), 749-762. (<a href='https://doi.org/10.1109/TMC.2024.3470339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial landmark tracking and 3D reconstruction have gained considerable attention due to their numerous applications such as human-computer interactions, facial expression analysis, and emotion recognition, etc. Traditional approaches require users to be confined to a particular location and face a camera under constrained recording conditions, which prevents them from being deployed in many application scenarios involving human motions. In this paper, we propose the first single-earpiece lightweight biosensing system, BioFace-3D, that can unobtrusively, continuously, and reliably sense the entire facial movements, track 2D facial landmarks, and further render 3D facial animations. Our single-earpiece biosensing system takes advantage of the cross-modal transfer learning model to transfer the knowledge embodied in a high-grade visual facial landmark detection model to the low-grade biosignal domain. After training, our BioFace-3D can directly perform continuous 3D facial reconstruction from the biosignals, without any visual input. Additionally, by utilizing biosensors, we also showcase the potential for capturing both behavioral aspects, such as facial gestures, and distinctive individual physiological traits, establishing a comprehensive two-factor authentication/identification framework. Extensive experiments involving 16 participants demonstrate that BioFace-3D can accurately track 53 major facial landmarks with only 1.85 mm average error and 3.38% normalized mean error, which is comparable with most state-of-the-art camera-based solutions. Experiments also show that the system can authenticate users with high accuracy (e.g., over 99.8% within two trials for three gestures in series), low false positive rate (e.g., less 0.24%), and is robust to various types of attacks.},
  archive      = {J_TMC},
  author       = {Yi Wu and Xiande Zhang and Tianhao Wu and Bing Zhou and Phuc Nguyen and Jian Liu},
  doi          = {10.1109/TMC.2024.3470339},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {749-762},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {3D facial tracking and user authentication through lightweight single-ear biosensors},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaKnife: Flexible DNN offloading for inference acceleration on heterogeneous mobile devices. <em>TMC</em>, <em>24</em>(2), 736-748. (<a href='https://doi.org/10.1109/TMC.2024.3466931'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of deep neural network (DNN) intelligence into embedded mobile devices is expanding rapidly, supporting a wide range of applications. DNN compression techniques, which adapt models to resource-constrained mobile environments, often force a trade-off between efficiency and accuracy. Distributed DNN inference, leveraging multiple mobile devices, emerges as a promising alternative to enhance inference efficiency without compromising accuracy. However, effectively decoupling DNN models into fine-grained components for optimal parallel acceleration presents significant challenges. Current partitioning methods, including layer-level and operator or channel-level partitioning, provide only partial solutions and struggle with the heterogeneous nature of DNN compilation frameworks, complicating direct model offloading. In response, we introduce AdaKnife, an adaptive framework for accelerated inference across heterogeneous mobile devices. AdaKnife enables on-demand mixed-granularity DNN partitioning via computational graph analysis, facilitates efficient cross-framework model transitions with operator optimization for offloading, and improves the feasibility of parallel partitioning using a greedy operator parallelism algorithm. Our empirical studies show that AdaKnife achieves a 66.5% reduction in latency compared to baselines.},
  archive      = {J_TMC},
  author       = {Sicong Liu and Hao Luo and XiaoChen Li and Yao Li and Bin Guo and Zhiwen Yu and YuZhan Wang and Ke Ma and YaSan Ding and Yuan Yao},
  doi          = {10.1109/TMC.2024.3466931},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {736-748},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {AdaKnife: Flexible DNN offloading for inference acceleration on heterogeneous mobile devices},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TidyBlock: A novel consensus mechanism for DAG-based blockchain in IoT. <em>TMC</em>, <em>24</em>(2), 722-735. (<a href='https://doi.org/10.1109/TMC.2024.3464494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of directed acyclic graph (DAG)-based blockchain and Internet of Things (IoT) aims at improving the efficiency of data storage. However, if massive IoT data are not placed in an organized way, the search and usage of the data for upper-level applications can be burdensome, since they have to examine the data block by block, which also increases the difficulty of data verification, affecting consensus efficiency. To maintain the high throughput advantage of DAG-based blockchain applied in IoT and improve the data analysis efficiency, we propose a novel consensus mechanism named TidyBlock, including the transaction collation mechanism for block generation and the block selection mechanism for verification. The first mechanism can tidy up scattered transactions before they are packaged into blocks, while the second one can collate blocks to facilitate verification, realizing a two-layer collation of IoT data so as to increase analysis efficiency of upper-level IoT applications. Additionally, the second mechanism can provide a self-driven incentive for rational participants to follow the first one in case they are reluctant to do extra collation work. Theoretical analysis is provided to demonstrate the validity of our proposed algorithms by formal methods. Extensive simulations based on synthetic data verify the rationality and effectiveness of the proposed mechanisms.},
  archive      = {J_TMC},
  author       = {Xidi Qu and Shengling Wang and Kun Li and Jianhui Huang and Xiuzhen Cheng},
  doi          = {10.1109/TMC.2024.3464494},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {722-735},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {TidyBlock: A novel consensus mechanism for DAG-based blockchain in IoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Debiased device sampling for federated edge learning in wireless networks. <em>TMC</em>, <em>24</em>(2), 709-721. (<a href='https://doi.org/10.1109/TMC.2024.3464740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a privacy-preserved distributed machine learning paradigm, federated edge learning (FEL) was designed to absorb knowledge from user devices to construct intelligent services without transmitting raw data. However, this paradigm depends on the local training and model parameter transmission of user devices, therefore the computing power, storage capacity and network resources of the devices become the key factors to achieve energy well-budgeted and timely message transmission FEL. While in the wireless networks, those resources for devices are normally heterogeneous or limited. This paper aims to offer tangible solutions for optimal convergence and Quality of Service (QoS) assurance of FEL in wireless networks. First, we define a mathematical model for energy-efficient message transmission of FEL and formulate an optimization problem involving device sampling and resource allocation to attain optimal training convergence within energy and time constraints. Second, we theoretically analyze the impact of limited resources on sampling strategies and training convergence, thus simplifying the optimization problem for solvability. Third, we introduce an iterative heuristic algorithm that utilizes available resources to reduce client sampling bias. Extensive experiments show that our method can effectively obtain the debiased sampling strategy, and outperforms similar methods by minimizing device disconnection due to energy use and enhancing model convergence and performance.},
  archive      = {J_TMC},
  author       = {Siguang Chen and Qun Li and Yanhang Shi and Xue Li},
  doi          = {10.1109/TMC.2024.3464740},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {709-721},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Debiased device sampling for federated edge learning in wireless networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal image and radio frequency fusion for optimizing vehicle positioning. <em>TMC</em>, <em>24</em>(2), 696-708. (<a href='https://doi.org/10.1109/TMC.2024.3469252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a multi-modal vehicle positioning framework that jointly localizes vehicles with channel state information (CSI) and images is designed. In particular, we consider an outdoor scenario where each vehicle can communicate with only one BS, and hence, it can upload its estimated CSI to only its associated BS. Each BS is equipped with a set of cameras, such that it can collect a small number of labeled CSI, a large number of unlabeled CSI, and the images taken by cameras. To exploit the unlabeled CSI data and position labels obtained from images, we design an meta-learning based hard expectation-maximization (EM) algorithm. Specifically, since we do not know the corresponding relationship between unlabeled CSI and the multiple vehicle locations in images, we formulate the calculation of the training objective as a minimum matching problem. To reduce the impact of label noises caused by incorrect matching between unlabeled CSI and vehicle locations obtained from images and achieve better convergence, we introduce a weighted loss function on the unlabeled datasets, and study the use of a meta-learning algorithm for computing the weighted loss. Subsequently, the model parameters are updated according to the weighted loss function of unlabeled CSI samples and their matched position labels obtained from images. Simulation results show that the proposed method can reduce the positioning error by up to 61% compared to a baseline that does not use images and uses only CSI fingerprint for vehicle positioning.},
  archive      = {J_TMC},
  author       = {Ouwen Huan and Tao Luo and Mingzhe Chen},
  doi          = {10.1109/TMC.2024.3469252},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {696-708},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-modal image and radio frequency fusion for optimizing vehicle positioning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bison: A binary sparse network coding based contents sharing scheme for D2D-enabled mobile edge caching network. <em>TMC</em>, <em>24</em>(2), 677-695. (<a href='https://doi.org/10.1109/TMC.2024.3463735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge caching network (MEN), which enables popular or reusable content caching and sharing among adjacent mobile edge devices, has become a promising solution to reduce the traffic and burden over backhaul links. Network coding (NC), represented by classical random linear network coding (RLNC), is utilized to facilitate content delivery and increase throughput in MEN. However, as the harsh decoding condition results in unacceptable time and storage overhead, classical RLNC schemes struggle to be widely deployed in practice. In this work, we propose a cost-effective NC-based content-sharing scheme based on binary sparse network coding (BSNC), called Bison, for D2D-enabled MEN. Based on the shared relationship between the binary sparse coded block (BSCB), Bison first designs a caching maintenance module to characterize the sharing progress and maintain the caching state of each edge node. Then, Bison defines a matching metric named neighbor utility to evaluate neighbors’ matching values by considering nodes’ demand and content decodability. Guiding by the metric, Bison achieves the most beneficial matching relationship among edge nodes through a proposed online matching policy. Finally, Bison devises a coded block delivery strategy to enable the sharing of valuable content between two matched edge nodes. Extensive experiments in simulations and real-world Android testbeds demonstrate its effectiveness and efficiency, wherein Bison is at least 30% less than the RLNC-based scheme on time consumption and at least 10% less than the classical BSNC-based scheme on storage overhead. The results also show that our matching policy and coded block delivery strategy can perform with a low response latency on edge and mobile devices.},
  archive      = {J_TMC},
  author       = {Cheng Peng and Jun Yin and Lei Wang and Fu Xiao},
  doi          = {10.1109/TMC.2024.3463735},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {677-695},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Bison: A binary sparse network coding based contents sharing scheme for D2D-enabled mobile edge caching network},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Echoes of fingertip: Unveiling POS terminal passwords through wi-fi beamforming feedback. <em>TMC</em>, <em>24</em>(2), 662-676. (<a href='https://doi.org/10.1109/TMC.2024.3465564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years, point-of-sale (POS) terminals are no longer limited to wired connections, with many relying on Wi-Fi for data transmission. Although Wi-Fi offers the convenience of wireless connectivity, it introduces significant security vulnerabilities. This work presents a non-intrusive method for eavesdropping POS passwords via Wi-Fi sensing, named ${\mathsf {BeamThief}}$. Instead of conventional Wi-Fi Channel State Information (CSI) readings, our approach employs Wi-Fi Beamforming Feedback Information (BFI) for an eavesdropping attack. Compared to CSI, which can only be extracted through intruding into the Access Point (AP) or from a limited selection of commercial Wi-Fi cards (e.g., Intel-5300), BFI readings can be more readily obtained from a broad array of commercial Wi-Fi devices. A key technological contribution of ${\mathsf {BeamThief}}$ is the development of an analysis model for predicting finger motion trajectories. This model is based on the physical relationship between BFI readings and finger motion, thus eliminating the need for extensive labeled training data. Furthermore, we employ Maximum Ratio Combining (MRC) to enhance the BFI series, ensuring performance across various scenarios. We implement ${\mathsf {BeamThief}}$ using everyday commercial Wi-Fi devices and conduct a series of experiments to assess the impact of this attack. Experimental results demonstrate that ${\mathsf {BeamThief}}$ achieves an accuracy rate 79$\%$ in inferring 6-digit POS passwords within the top-100 attempts.},
  archive      = {J_TMC},
  author       = {Siyu Chen and Hongbo Jiang and Jingyang Hu and Tianyue Zheng and Mengyuan Wang and Zhu Xiao and Daibo Liu and Jun Luo},
  doi          = {10.1109/TMC.2024.3465564},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {662-676},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Echoes of fingertip: Unveiling POS terminal passwords through wi-fi beamforming feedback},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpaceRTC: Unleashing the low-latency potential of mega-constellations for wide-area real-time communications. <em>TMC</em>, <em>24</em>(2), 642-661. (<a href='https://doi.org/10.1109/TMC.2024.3470330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-perceived latency is important for the quality of experience (QoE) of wide-area real-time communications (RTC). With the rapid development of low Earth orbit (LEO) mega-constellations, this paper explores a futuristic yet important problem facing the RTC community: can we exploit emerging mega-constellations to facilitate low-latency RTC globally? We carry out our quest in three steps. First, through a measurement study associated with a large number of geo-distributed RTC users, we quantitatively expose that the meandering routes in the client-to-cloud and inter-cloud-site segment of existing cloud-based RTC architecture are critical culprits for the high latency issue suffered by wide-area RTC sessions. Second, we propose SpaceRTC, a satellite-cloud cooperative framework that dynamically selects relay servers upon satellites and cloud sites to build an overlay network which enables diverse close-to-optimal paths. SpaceRTC judiciously allocates RTC flows of different sessions upon the network to facilitate low-latency interactions and adaptively selects bitrates to offer high user-perceived QoE in energy-limited space circumstance. Finally, we implement a testbed based on public constellation information and real-world RTC traces. Extensive experiments demonstrate that SpaceRTC can deliver near-optimal interactive latency, with up to 53.3% average latency reduction and 103.6% average bitrate improvement as compared to other state-of-the-art cloud-based solutions.},
  archive      = {J_TMC},
  author       = {Zeqi Lai and Weisen Liu and Qian Wu and Hewu Li and Jingxi Xu and Yibo Wang and Yuanjie Li and Jun Liu},
  doi          = {10.1109/TMC.2024.3470330},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {642-661},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {SpaceRTC: Unleashing the low-latency potential of mega-constellations for wide-area real-time communications},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmTAA: A contact-less thoracoabdominal asynchrony measurement system based on mmWave sensing. <em>TMC</em>, <em>24</em>(2), 627-641. (<a href='https://doi.org/10.1109/TMC.2024.3461784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thoracoabdominal Asynchrony (TAA) is a key metric in respiration monitoring, which characterizes the non-parallel periodical motion of human's rib cage (RC) and abdomen (AB) during each breath. Long-term measurement of TAA plays a significant role in respiration health tracking. Existing TAA measurement methods including Respiratory Inductive Plethysmography (RIP) and Optoelectronic Plethysmography (OEP) all intrusive to subjects and have certain requirements on operation conditions, which limit their usage to hospital scenario. To address this gap, we propose mmTAA, the first mmWave-based, non-intrusive TAA measurement system ready for ubiquitous usage in daily-life. In mmTAA, we design a Two-stage RC-AB centroid finding module, aiming to identify the most probable location of RC-AB centroid, which can best represent RC and AB in mmWave sensing scenario. Subsequently, we design TAANet, a novel Convolutional Neural Network (CNN)-based architecture with residual modules, tailored for TAA measurement. Meanwhile, in order to address the imbalance of continuous data, we add imbalance information equalizer including feature and label equalizer during network training. We implement mmTAA on a commonly used multi-antenna mmWave radar. We prototype, deploy and evaluate mmTAA on 25 subjects and 25.7h data in total. mmTAA achieves 4.01$^{\circ }$ MAE and 1.56$^{\circ }$ average error, close to OEP method.},
  archive      = {J_TMC},
  author       = {Fenglin Zhang and Zhebin Zhang and Le Kang and Anfu Zhou and Huadong Ma},
  doi          = {10.1109/TMC.2024.3461784},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {627-641},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmTAA: A contact-less thoracoabdominal asynchrony measurement system based on mmWave sensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploitation and confrontation: Sustainability analysis of crowdsourcing. <em>TMC</em>, <em>24</em>(2), 614-626. (<a href='https://doi.org/10.1109/TMC.2024.3463417'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Game theory is an effective analytical tool for crowdsourcing. Existing studies based on it share a commonality: the influence of players’ decisions is bilateral. However, the status is broken by the zero-determinant (ZD) strategy, where the ZD player can unilaterally control the opponent's expected payoff. Thereby, crowdsourcing games trigger conclusions that differ from traditional ones. By addressing three questions, this paper is the first work to analyze the turbulence in crowdsourcing caused by the inequality between the requestor and the worker in the ZD game. The first question reveals the potential for the requestor to exploit the worker; the second question quantifies the worker's tolerance towards exploitation, providing a basis for confrontation; the third question serves as the cornerstone for maintaining the crowdsourcing, regulating the requestor's exploitative behavior. To answer these questions, we extend ZD strategies from binary games to continuous ones, not only revealing the requestor's dominance but also enriching the theoretical system of ZD strategies and broadening their application. Furthermore, we introduce the worker's dissatisfaction degree, identifying the exponential trend and decay rate, revealing optimal timing and speed for the worker's effective confrontation and maximum exploitation for the requestor. Numerical simulations have validated the effectiveness of our analyses.},
  archive      = {J_TMC},
  author       = {Hang Zhao and Shengling Wang and Hongwei Shi and Jianhui Huang and Yu Guo and Xiuzhen Cheng},
  doi          = {10.1109/TMC.2024.3463417},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {614-626},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploitation and confrontation: Sustainability analysis of crowdsourcing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CRS: A cost-aware resource scheduling framework for deep learning task orchestration in mobile clouds. <em>TMC</em>, <em>24</em>(2), 600-613. (<a href='https://doi.org/10.1109/TMC.2024.3464491'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) has found extensive application in supporting various mobile applications. The efficient execution of DL tasks is paramount for ensuring the effectiveness of AI-driven mobile applications. While previous research has predominantly focused on minimizing the completion time of DL tasks, the associated cost of execution has often been overlooked. Nonetheless, cost becomes a critical factor, particularly when utilizing DL infrastructure rented from third-party cloud service providers. In this paper, we propose a cost-aware resource scheduling framework named CRS for orchestrating DL task execution in mobile cloud systems. Our aim is to minimize server rental costs by strategically orchestrating DL jobs with diverse deadlines and workload scales across rented cloud servers. We formally define the problem and prove its NP-hardness by reducing it to a multiple knapsack problem (MKP). To solve this problem, we devise an approximation algorithm with a guaranteed upper bound performance ratio of $1+\frac{1}{e-1}$. We evaluate CRS against state-of-the-art baselines through simulations of various job arrival scenarios in a real elastic mobile cloud system. The results demonstrate that CRS, on average, reduces rental costs by 45.1% compared to other baselines, while simultaneously achieving a shorter average job completion time (JCT) and maximum job completion time (i.e., makespan).},
  archive      = {J_TMC},
  author       = {Linchang Xiao and Zili Xiao and Di Wu and Miao Hu and Yipeng Zhou},
  doi          = {10.1109/TMC.2024.3464491},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {600-613},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CRS: A cost-aware resource scheduling framework for deep learning task orchestration in mobile clouds},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot adaptation to unseen conditions for wireless-based human activity recognition without fine-tuning. <em>TMC</em>, <em>24</em>(2), 585-599. (<a href='https://doi.org/10.1109/TMC.2024.3462466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless-based human activity recognition (WHAR) enables various promising applications. However, since WHAR is sensitive to changes in sensing conditions (e.g., different environments, users, and new activities), trained models often do not work well under new conditions. Recent research uses meta-learning to adapt models. However, they must fine-tune the model, which greatly hinders the widespread adoption of WHAR in practice because model fine-tuning is difficult to automate and requires deep-learning expertise. The fundamental reason for model fine-tuning in existing works is because their goal is to find the mapping relationship between data samples and corresponding activity labels. Since this mapping reflects the intrinsic properties of data in the perceptual scene, it is naturally related to the conditions under which the activity is sensed. To address this problem, we exploit the principle that under the same sensing condition, data of the same activity class are more similar (in a certain latent space) than data of other classes, and this property holds invariant across different conditions. Our main observation is that meta-learning can actually also transform WHAR design into a learning problem that is always under similar conditions, thus decoupling the dependence on sensing conditions. With this capability, general and accurate WHAR can be achieved, avoiding model fine-tuning. In this paper, we implement this idea through two innovative designs in a system called RoMF. Extensive experiments using FMCW, Wi-Fi and acoustic three sensing signals show that it can achieve up to 95.3% accuracy in unseen conditions, including new environments, users and activity classes.},
  archive      = {J_TMC},
  author       = {Xiaotong Zhang and Qingqiao Hu and Zhen Xiao and Tao Sun and Jiaxi Zhang and Jin Zhang and Zhenjiang Li},
  doi          = {10.1109/TMC.2024.3462466},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {585-599},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Few-shot adaptation to unseen conditions for wireless-based human activity recognition without fine-tuning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative video streaming with super-resolution in multi-user MEC networks. <em>TMC</em>, <em>24</em>(2), 571-584. (<a href='https://doi.org/10.1109/TMC.2024.3461685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing quality of experience (QoE) demand for video streaming has prompted the integration of video super-resolution and multi-access edge computing networks (MEC). With super-resolution, the low-resolution frames can be reconstructed into high-resolution ones by edge node and end device collaboratively, which is beneficial in improving QoE. However, the existing works focus on designing video streaming strategies in single-user scenarios, which cannot be applied to multi-user scenarios due to the resource contention among users, as well as the huge solution space of coupled bitrate selection and workload share between edge-end. To fill this gap, we propose a collaborative video streaming strategy with super-resolution in multi-user MEC networks, named Co-Video, to maximize the average QoE by making optimal bitrate selection and workload share. We first formulate the problem as an optimization problem towards maximum average QoE, where the QoE incorporates playback delay, video quality, and smoothness. Then, we transform the optimization problem into a partially observable Markov decision process (POMDP) and exploit the Co-Video strategy based on the multi-agent soft actor-critic (MASAC) algorithm. Specifically, Co-Video utilizes the branching actor network to converge to good policy stably. Finally, trace-driven simulations on real-world bandwidth traces demonstrate that Co-Video outperforms the state-of-the-art baselines.},
  archive      = {J_TMC},
  author       = {Xiaobo Zhou and Jiaxin Zeng and Shuxin Ge and Xilai Liu and Tie Qiu},
  doi          = {10.1109/TMC.2024.3461685},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {571-584},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Collaborative video streaming with super-resolution in multi-user MEC networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards workload-constrained efficient order assignment in last-mile delivery. <em>TMC</em>, <em>24</em>(2), 557-570. (<a href='https://doi.org/10.1109/TMC.2024.3469236'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient order assignment in last-mile delivery benefits customers, couriers, and the platform. State-of-the-practice order assignment is based on the static delivery area partition, which cannot adapt well to the dynamic order quantity and destination distributions on different days. State-of-the-art methods focus on balancing order amounts or the payoff among couriers dynamically, neglecting the courier's workload in delivering orders. This paper explores the courier's heterogeneous behaviors for delivering orders to different destinations to measure the courier's workload and then achieve more efficient order assignments under the fair workload constraint. We design a workload-constrained order assignment system, called WORD, to reduce the cost of the last-mile delivery, i.e., the couriers’ total travel distance and overdue order rate. Specifically, the heterogeneous behaviors for delivering orders are first utilized for workload calculation. Then a two-stage order assignment framework is designed, including a sort-based initialization algorithm for initializing the assignment under the fair workload constraint and a coalition-game-based improvement algorithm for improving the assignment. Extensive evaluation results with real-world logistics data from one of the largest logistics companies in China show that WORD reduces the cost of the order assignment by up to 51.9% under the fair workload constraint compared to the state-of-the-art methods.},
  archive      = {J_TMC},
  author       = {Wenjun Lyu and Xiaolong Jin and Haotian Wang and Yiwei Song and Shuai Wang and Yunhuai Liu and Tian He and Desheng Zhang},
  doi          = {10.1109/TMC.2024.3469236},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {557-570},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Towards workload-constrained efficient order assignment in last-mile delivery},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive incentive and resource allocation for blockchain-supported edge video streaming systems: A cooperative learning approach. <em>TMC</em>, <em>24</em>(2), 539-556. (<a href='https://doi.org/10.1109/TMC.2024.3437745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing significantly enhanced the growth of edge-assistant video streaming applications. However, challenges such as unpredictable wireless conditions, resource constraints, and task redundancy have intertwined impacts on the overall performance of edge video streaming systems (EVS). Therefore, it is essential to have an integrated framework that addresses resource management, computational offloading, and video task preprocessing. Existing optimization strategies often neglect the simultaneous management of computational offloading, resource allocation, and video task preprocessing, leading to a suboptimal system utility. Moreover, they struggle to handle high-dimensional decision variables. On the other hand, learning-based adaptive schemes fall short in integrating distributed decisions and ensuring the scalability of wireless devices. Additionally, current approaches lack adaptive incentives. To bridge these gaps, we propose a novel framework called AIRA, which is based on improved multi-agent reinforcement learning (MARL) and smart contracts. AIRA manages resources, video compression, and adaptive incentives in a distributed manner. It consists of a MARL-driven cooperative learning algorithm (CLA) and a smart contract-guided adaptive incentive mechanism. Leveraging an actor-critic structure, the CLA enables wireless devices to master strategies for resource allocation, video task compression, and offloading, utilizing historical data. Notably, the CLA incorporates an attention mechanism to select pivotal tuples from the observation-action pairings among different agents, ensuring improved scalability and computational prowess. Evaluations based on real-world trajectories demonstrate that AIRA enables adaptive incentives. Compared to state-of-the-art approaches, CLA effectively enhances the long-term system utility and scalability of EVS.},
  archive      = {J_TMC},
  author       = {Shijing Yuan and Qingshi Zhou and Jie Li and Song Guo and Hongyang Chen and Chentao Wu and Yang Yang},
  doi          = {10.1109/TMC.2024.3437745},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {539-556},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive incentive and resource allocation for blockchain-supported edge video streaming systems: A cooperative learning approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous authentication via wrist photoplethysmogram: An extensive study. <em>TMC</em>, <em>24</em>(2), 525-538. (<a href='https://doi.org/10.1109/TMC.2024.3443208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous authentication (CA) based on wrist photoplethysmogram (PPG) has been increasingly studied, but still requires further extensive investigation on PPG reliability over time and heart rates for real-world deployments. In this paper, we first analyze the inadequacy of current research, i.e., limited generalization capability for new users and insufficient experiments due to the absence of across-session data under different heart rates (HR). To address these problems, we then propose a unified and scalable feature extraction framework for wrist PPG-based CA. Given a continuous PPG waveform, our framework first encodes the PPG of each period separately, then extracts variability features contained in consecutive multi-period PPG for user authentication. On two datasets with a total of 155 subjects, we evaluate the performances of our system using different across-session levels and HR intervals, respectively. Despite more stringent experimental settings, we achieve even better performances than in previous studies. Using the subject-exclusive cross-validation protocol, our system reaches an average accuracy of 92.1% under the constraint of equal error rates in across-session evaluation, and average accuracy ranges from 86.4% (high HR) to 91.4% (low HR) for different HR intervals.},
  archive      = {J_TMC},
  author       = {Jinxiao Wu and Xuanshu Luo and Siwei Chen and Yongqiang Lyu and Xiangyang Ji and Yan Meng and Dongsheng Wang},
  doi          = {10.1109/TMC.2024.3443208},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {525-538},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Continuous authentication via wrist photoplethysmogram: An extensive study},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Packet scheduling in multi-flow carrier aggregation with QoS provisioning: Cross-layer design and performance analysis. <em>TMC</em>, <em>24</em>(2), 507-524. (<a href='https://doi.org/10.1109/TMC.2024.3425628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-flow carrier aggregation (CA) is a key solution for improving data rates in heterogeneous networks (HetNets). Efficient data splitting is important to enhance the performance of multi-flow CA. In this paper, packet scheduling in multi-flow CA is investigated via a cross-layer design approach with comprehensive quality of service (QoS) provisioning. In particular, an approximation of the average queuing delay (AQD) in terms of the average buffers’ length and the average packet arrivals is presented. Then, using the presented AQD approximation, various packet scheduling schemes are developed in this paper. First, a low-complexity buffer and link aware heuristic packet scheduling scheme is proposed. Next, in order to reduce backhaul signalling overhead, a hybrid packet scheduling scheme that combines heuristic packet scheduling and random packet scheduling is considered. Then, an optimal packet scheduling scheme, albeit of high computational complexity, is proposed as a benchmark for other packet scheduling schemes. Also, the performances of the proposed packet scheduling schemes are analyzed and various data link layer performance parameters such as AQD, packet loss probability (PLP) and statistical queuing delay bound (SDB) are taken into consideration. Moreover, the complexity and the signalling overhead of the proposed packet scheduling schemes are investigated. Presented numerical results demonstrate that the heuristic packet scheduling scheme can achieve a performance that is similar to the performance of the optimal packet scheduling scheme while overcoming complexity and feasibility issues of the optimal packet scheduling scheme. Also, the hybrid packet scheduling scheme reduces signalling overhead while reaping some of the benefits of using buffer and link aware packet scheduling.},
  archive      = {J_TMC},
  author       = {Abdulaziz Alorainy},
  doi          = {10.1109/TMC.2024.3425628},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {507-524},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Packet scheduling in multi-flow carrier aggregation with QoS provisioning: Cross-layer design and performance analysis},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive mobility load balancing through interior-point policy optimization for open radio access networks. <em>TMC</em>, <em>24</em>(2), 500-506. (<a href='https://doi.org/10.1109/TMC.2024.3407979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The future B5G/6G wireless networks are required to intelligently operate and optimize in all ranges of scenarios, however, most of the functions in self-organizing network (SON) architectures are rule-based and reactive, these rigid functions can not keep up with the dynamic nature of the future wireless networks. In this article, we propose an active load balancing framework considering mobility load balancing (MLB) and mobility robust optimization (MRO), which are two essential functions to ensure a seamless user experience for the future B5G/6G. The proposed method is based on random subspace Bayesian additive regression tree (RS-BART) and interior point policy optimization (IPO). Specifically, user trajectory is firstly forecasted by utilizing RS-BART, and then the joint problem of MLB and MRO is further solved by IPO. Simulations based on open radio access network (O-RAN) reveal that the proposed method reduces load deviation and radio link failure ratio by 40% and 50% compared to the previous ones.},
  archive      = {J_TMC},
  author       = {Miaona Huang and Jun Chen},
  doi          = {10.1109/TMC.2024.3407979},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  number       = {2},
  pages        = {500-506},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Proactive mobility load balancing through interior-point policy optimization for open radio access networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedCRAC: Improving federated classification performance on long-tailed data via classifier representation adjustment and calibration. <em>TMC</em>, <em>24</em>(1), 482-499. (<a href='https://doi.org/10.1109/TMC.2024.3466208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning has been a popular distributed training paradigm that enables to train a shared model with data privacy protection. However, non-Independent Identically Distribution and long-tailed data distribution characteristics across mobile devices results in evident performance degradation, especially for classification tasks. Although plenty of research studies devote to alleviating classification performance degradation caused by highly-skewed data distribution, they still cannot improve the distinguishability of model representation on hard-to-learn tail classes, and face obvious divergence of local classifiers in FL setting. To this end, we propose Federated Classifier Representation Adjustment and Calibration to improve the representation distinguishability of tail classes and achieve inter-client representation alignment with acceptable resource consumption on attaching operations. We first design a Class Similarity-Aware Margin matrix to enlarge class representation discrepancy and improve local classifier discriminability on tail classes during client-side local training process. To mitigate the divergence of local classifiers across clients, we further propose the Self Distillation Classifier Calibration to achieve the aggregated global classifier calibration with the assistance of generated pseudo representation samples via self-distillation manner. We conduct various experiments under wide-range long-tailed and heterogeneous data settings. Experimental results show that FedCRAC outperforms state-of-the-art methods in terms of accuracy and resource consumption.},
  archive      = {J_TMC},
  author       = {Xujing Li and Sheng Sun and Min Liu and Ju Ren and Xuefeng Jiang and Tianliu He},
  doi          = {10.1109/TMC.2024.3466208},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {482-499},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FedCRAC: Improving federated classification performance on long-tailed data via classifier representation adjustment and calibration},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-hop task offloading and relay selection for IoT devices in mobile edge computing. <em>TMC</em>, <em>24</em>(1), 466-481. (<a href='https://doi.org/10.1109/TMC.2024.3462731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To bridge the gap of conventional single-hop task offloading schemes in infrastructure-free scenarios, multi-hop task offloading schemes for IoT devices in Mobile Edge Computing (MEC) are desired to jointly optimize task offloading decisions and routing paths. In this paper, we investigate a hierarchical multi-hop edge computing framework and propose a joint Task Offloading and Relay Selection (TORS) scheme. It considers real-time computation at each relay node and employs directional searches to facilitate the task execution and results reporting at the fastest speed. However, finding the optimal TORS solution is a formidable challenge due to the time-varying network environments, the strong interdependence of decision sets across different time slots, and the high computational complexity. To address these challenges, we first leverage Lyapunov optimization to transform the stochastic TORS problem into a deterministic per-slot block problem, avoiding the need for extensive system prior knowledge. Subsequently, we propose a Soft Actor-Critic (SAC)-based algorithm, SAC-TORS, to find a satisfactory TORS solution with minimal computational complexity in a distributed manner. Accordingly, each IoT device can independently make self-determined and directional decisions with observable network information. Through extensive experiments, we demonstrate that the SAC-TORS outperforms state-of-the-art solutions, achieving performance improvements of up to 66%.},
  archive      = {J_TMC},
  author       = {Ting Li and Yinlong Liu and Tao Ouyang and Hangsheng Zhang and Kai Yang and Xu Zhang},
  doi          = {10.1109/TMC.2024.3462731},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {466-481},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-hop task offloading and relay selection for IoT devices in mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage deep energy optimization in IRS-assisted UAV-based edge computing systems. <em>TMC</em>, <em>24</em>(1), 449-465. (<a href='https://doi.org/10.1109/TMC.2024.3461719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating wireless-powered Mobile Edge Computing (MEC) with Unmanned Aerial Vehicles (UAVs) leverages computation offloading services for mobile devices, significantly enhancing the mobility and control of MEC networks. However, current research has not focused on customizing system designs for Terahertz (THz) communication networks. When dealing with THz communication, one must account for blockage vulnerability due to severe THz wave propagation attenuation and insufficient diffraction. The Intelligent Reflecting Surface (IRS) can effectively address these limitations in the model, enhancing spectrum efficiency and coverage capabilities while reducing blockage vulnerability in THz networks. In this paper, we introduce an upgraded MEC system that integrates IRS and UAVs into THz communication networks, focusing on a binary offloading policy for studying the computation offloading problem. Our primary objective is to optimize the energy consumption of both UAVs and User Electronic Devices, alongside refining the phase shift of the IRS reflector. The problem is a Mixed Integer Non-Linear Programming problem known as NP-hard. To tackle this challenge, we propose a two-stage deep learning-based optimization framework named Iterative Order-Preserving Policy Optimization (IOPO). Unlike exhaustive search methods, IOPO continually updates offloading decisions through an order-preserving quantization method, thereby accelerating convergence and reducing computational complexity, especially when handling complex problems with extensive solution spaces. The numerical results demonstrate that the proposed algorithm significantly improves energy efficiency and achieves near-optimal performance compared to benchmark methods.},
  archive      = {J_TMC},
  author       = {Jianqiu Wu and Zhongyi Yu and Jianxiong Guo and Zhiqing Tang and Tian Wang and Weijia Jia},
  doi          = {10.1109/TMC.2024.3461719},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {449-465},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Two-stage deep energy optimization in IRS-assisted UAV-based edge computing systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving linear speedup in asynchronous federated learning with heterogeneous clients. <em>TMC</em>, <em>24</em>(1), 435-448. (<a href='https://doi.org/10.1109/TMC.2024.3461852'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is an emerging distributed training paradigm that aims to learn a common global model without exchanging or transferring the data that are stored locally at different clients. The Federated Averaging (FedAvg)-based algorithms have gained substantial popularity in FL to reduce the communication overhead, where each client conducts multiple localized iterations before communicating with a central server. In this paper, we focus on FL where the clients have diverse computation and/or communication capabilities. Under this circumstance, FedAvg can be less efficient since it requires all clients that participate in the global aggregation in a round to initiate iterations from the latest global model, and thus the synchronization among fast clients and straggler clients can severely slow down the overall training process. To address this issue, we propose an efficient asynchronous federated learning (AFL) framework called Delayed Federated Averaging (DeFedAvg). In DeFedAvg, the clients are allowed to perform local training with different stale global models at their own paces. Theoretical analyses demonstrate that DeFedAvg achieves asymptotic convergence rates that are on par with the results of FedAvg for solving nonconvex problems. More importantly, DeFedAvg is the first AFL algorithm that provably achieves the desirable linear speedup property, which indicates its high scalability. Additionally, we carry out extensive numerical experiments using real datasets to validate the efficiency and scalability of our approach when training deep neural networks.},
  archive      = {J_TMC},
  author       = {Xiaolu Wang and Zijian Li and Shi Jin and Jun Zhang},
  doi          = {10.1109/TMC.2024.3461852},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {435-448},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Achieving linear speedup in asynchronous federated learning with heterogeneous clients},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FD MU-MIMO systems: Performance analysis in the presence of imperfect CSI and non-ideal transceivers. <em>TMC</em>, <em>24</em>(1), 422-434. (<a href='https://doi.org/10.1109/TMC.2024.3462721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work outlines a framework for full-duplex (FD) multiple-input multiple-output (MIMO) communication systems considering practical conditions, such as imperfect channel state information (CSI) and hardware impairments (HWIs). We analyze the performance of FD multi-user (MU) MIMO systems, specifically studying the effects of practical channel estimation errors and HWIs on the spectral efficiency (SE) performance of FD MU-MIMO systems. Maximum ratio combining/maximum ratio transmission (MRC/MRT) and zero-forcing reception/zero-forcing transmission (ZFR/ZFT) linear detectors/precoders are considered at the base station (BS). Moreover, linear minimum mean square error (LMMSE) and least square (LS) error estimation are used to estimate the channel at the BS. Mathematical derivations for the lower bounds of uplink (UL) and downlink (DL) SEs are presented in the context of imperfect CSI and HWIs. Computer simulations validate the analytical derivations. The results demonstrate the tightness of the obtained bounds.},
  archive      = {J_TMC},
  author       = {Emad Saleh and Malek Alsmadi and Salama Ikki},
  doi          = {10.1109/TMC.2024.3462721},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {422-434},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FD MU-MIMO systems: Performance analysis in the presence of imperfect CSI and non-ideal transceivers},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient coordination of federated learning and inference offloading at the edge: A proactive optimization paradigm. <em>TMC</em>, <em>24</em>(1), 407-421. (<a href='https://doi.org/10.1109/TMC.2024.3466844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from hardware upgrades and deep learning techniques, more and more end devices can independently support a variety of intelligent applications. Further powered by edge computing technologies, the end-edge collaboration paradigm becomes one mainstream approach for achieving advanced edge intelligence (EI). To fully exploit the system resources, it is desirable to coordinate diverse EI services efficiently. Thus, we present a novel framework to jointly optimize the cost-performance trade-off for two distinct but typical EI services, where end devices simultaneously perform federated learning (FL) model training and conduct model inference with the assistance of edge offloading. However, balancing the long-term cost-performance trade-off is highly non-trivial, especially in the absence of knowledge of future system dynamics. Moreover, the capacity heterogeneity further increases the difficulty of service coordination among resource-limited end devices. To overcome these challenges, we first analyze the optimality of inference offloading decisions with and without FL model training and quantify their mutual effects due to local resource contention. By incorporating the loss estimation of FL training model, we then propose a novel proactive policy with theoretical guarantees, which proactively controls the stopping of FL training procedure to balance well the trade-offs between FL model performance and resource costs while fulfilling the inference performance requirements. Extensive results show the efficiency and robustness of our proposed algorithm for EI service coordination in dynamic end-edge collaboration scenarios.},
  archive      = {J_TMC},
  author       = {Ke Luo and Kongyange Zhao and Tao Ouyang and Xiaoxi Zhang and Zhi Zhou and Hao Wang and Xu Chen},
  doi          = {10.1109/TMC.2024.3466844},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {407-421},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Efficient coordination of federated learning and inference offloading at the edge: A proactive optimization paradigm},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiPhase: A human activity recognition approach by fusing of reconstructed WiFi CSI phase features. <em>TMC</em>, <em>24</em>(1), 394-406. (<a href='https://doi.org/10.1109/TMC.2024.3461672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) is an important task in the field of human-computer interaction. Given the penetration of WiFi devices in our daily lives, HAR using WiFi channel state information (CSI) is a more cost-efficient and comfortable approach. However, most existing approaches ignore the correlation between CSI sub-carriers, which makes their models inefficient and need to rely on deeper and more complex networks to further improve performance. To solve these problems, we propose a reconstructed WiFi CSI phase based HAR approach (WiPhase), which contains a two-stream model to fuse both temporal features and sub-carrier correlation features of reconstructed CSI phase. Specifically, a gated pseudo-Siamese network (GPSiam) is designed to capture the temporal features of the reconstructed sparse CSI phase integration representation (CSI-PIR), and a dynamic resolution based graph attention network (DRGAT) is designed to capture the nonlinear correlation between CSI sub-carriers by the reconstructed CSI phase graph. Furthermore, dendrite network (DD) makes the final decision by combining the features output from GPSiam and DRGAT. Experimental results show that WiPhase outperforms the existing state-of-the-art approaches.},
  archive      = {J_TMC},
  author       = {Xingcan Chen and Chenglin Li and Chengpeng Jiang and Wei Meng and Wendong Xiao},
  doi          = {10.1109/TMC.2024.3461672},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {394-406},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WiPhase: A human activity recognition approach by fusing of reconstructed WiFi CSI phase features},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model decomposition and reassembly for purified knowledge transfer in personalized federated learning. <em>TMC</em>, <em>24</em>(1), 379-393. (<a href='https://doi.org/10.1109/TMC.2024.3466227'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized federated learning (pFL) is to collaboratively train non-identical machine learning models for different clients to adapt to their heterogeneously distributed datasets. State-of-the-art pFL approaches pay much attention on exploiting clients’ inter-similarities to facilitate the collaborative learning process, meanwhile, can barely escape from the irrelevant knowledge pooling that is inevitable during the aggregation phase, and thus hindering the optimization convergence and degrading the personalization performance. To tackle such conflicts between facilitating collaboration and promoting personalization, we propose a novel pFL framework, dubbed pFedC, to first decompose the global aggregated knowledge into several compositional branches, and then selectively reassemble the relevant branches for supporting conflicts-aware collaboration among contradictory clients. Specifically, by reconstructing each local model into a shared feature extractor and multiple decomposed task-specific classifiers, the training on each client transforms into a mutually reinforced and relatively independent multi-task learning process, which provides a new perspective for pFL. Besides, we conduct a purified knowledge aggregation mechanism via quantifying the combination weights for each client to capture clients’ common prior, as well as mitigate potential conflicts from the divergent knowledge caused by the heterogeneous data. Extensive experiments over various models and datasets demonstrate the effectiveness and superior performance of the proposed algorithm.},
  archive      = {J_TMC},
  author       = {Jie Zhang and Song Guo and Xiaosong Ma and Wenchao Xu and Qihua Zhou and Jingcai Guo and Zicong Hong and Jun Shan},
  doi          = {10.1109/TMC.2024.3466227},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {379-393},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Model decomposition and reassembly for purified knowledge transfer in personalized federated learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-user task offloading in UAV-assisted LEO satellite edge computing: A game-theoretic approach. <em>TMC</em>, <em>24</em>(1), 363-378. (<a href='https://doi.org/10.1109/TMC.2024.3465591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned Aerial Vehicle (UAV)-assisted Low Earth Orbit (LEO) satellite edge computing (ULSE) networks can address the challenge communications issues in areas with harsh terrain and achieve global wireless coverage to provide services for mobile user devices (MUDs). This paper studies the LEO-UAV task offloading problem where MUDs compete for limited resources in the ULSE networks. We formulate the optimization problem with the goal of minimizing the cost of all MUDs while meeting resource constraint and satellite coverage time constraint. We first theoretically prove that this problem is NP-hard. We then reformulate the problem as a LEO-UAV task offloading game (LUTO-Game), and show that there is at least one Nash equilibrium solution for the LUTO-Game. We propose a joint UAV and LEO satellite task offloading (JULTO) algorithm to obtain the Nash equilibrium offloading strategy, and analyze the performance of the worst-case offloading strategy obtained by the JULTO algorithm. Finally, extensive experiments, including convergence analysis and comparison experiments, are carried out to validate the effectiveness of our JULTO algorithm.},
  archive      = {J_TMC},
  author       = {Ying Chen and Jie Zhao and Yuan Wu and Jiwei Huang and Xuemin Sherman Shen},
  doi          = {10.1109/TMC.2024.3465591},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {363-378},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-user task offloading in UAV-assisted LEO satellite edge computing: A game-theoretic approach},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Near-field beam training for extremely large-scale MIMO based on deep learning. <em>TMC</em>, <em>24</em>(1), 352-362. (<a href='https://doi.org/10.1109/TMC.2024.3462960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extremely Large-scale Array (ELAA) is considered a frontier technology for future communication systems, playing a crucial role in enhancing the rate and spectral efficiency of wireless networks. As ELAA employs a multitude of antennas operating at higher frequencies, users are typically situated in the near-field region where the spherical wavefront propagates. Near-field beam training requires information on both angle and distance, which inevitably leads to a significant increase in the beam training overhead. To address this challenge, we propose a near-field beam training method based on deep learning. Specifically, we employ a convolutional neural network (CNN) to efficiently extract channel characteristics from historical data by strategically selecting padding and kernel sizes. The negative value of the user average achievable rate is utilized as the loss function to optimize the beamformer, maximizing the achievable rate in multi-user networks without relying on predefined beam codebooks. Once deployed, the model requires only pre-estimated channel state information (CSI) to compute the optimal beamforming vector. Simulation results demonstrate that the proposed scheme achieves more stable beamforming gains and substantially outperforms traditional beam training approaches. Furthermore, owing to the inherent traits of deep learning methodologies, this approach substantially diminishes the near-field beam training overhead.},
  archive      = {J_TMC},
  author       = {Jiali Nie and Yuanhao Cui and Zhaohui Yang and Weijie Yuan and Xiaojun Jing},
  doi          = {10.1109/TMC.2024.3462960},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {352-362},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Near-field beam training for extremely large-scale MIMO based on deep learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical-layer CTC from BLE to wi-fi with IEEE 802.11ax. <em>TMC</em>, <em>24</em>(1), 338-351. (<a href='https://doi.org/10.1109/TMC.2024.3462941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wi-Fi is the de facto standard for providing wireless access to the Internet in the 2.4 GHz ISM band. Tens of billions of Wi-Fi devices (e.g., smartphones) have been shipped worldwide with limited types of wireless radios operating only when Wi-Fi connectivity is available, making it challenging to access data in heterogeneous IoT devices. However, the direct connection between Wireless Personal Area Network (WPAN) technologies, such as Bluetooth, and Wi-Fi presents challenges due to the inherent distinct physical layer. In our work, a novel communication method called BlueWi has been introduced, which serves as a cross technology communication method that enables BLE devices to establish connections and engage in communication with Wi-Fi based WPAN networks. We let BLE signals hitchhike on ongoing Wi-Fi signals, enabling Wi-Fi to recognize specific BLE signal waveforms in the frequency domain. By analyzing the decoded Wi-Fi payload, BlueWi can retrieve the BLE data, ensuring this method remains fully compatible with existing commodity Wi-Fi hardware. The direct sequence spread spectrum scheme is appended to handle general BLE frames and can be considered as “COPY” operation, which allows for better correlation and detection of the signal at the receiver. Evaluations conducted using both USRP and commodity devices have demonstrated that BlueWi can achieve concurrent wireless communication from BLE commercial chips to Wi-Fi networks with a frame reception rate exceeding 96%.},
  archive      = {J_TMC},
  author       = {Demin Gao and Liyuan Ou and Yongrui Chen and Xiuzhen Guo and Ruofeng Liu and Yunhuai Liu and Tian He},
  doi          = {10.1109/TMC.2024.3462941},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {338-351},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Physical-layer CTC from BLE to wi-fi with IEEE 802.11ax},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge-aware parameter coaching for communication-efficient personalized federated learning in mobile edge computing. <em>TMC</em>, <em>24</em>(1), 321-337. (<a href='https://doi.org/10.1109/TMC.2024.3464512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized Federated Learning (pFL) can improve the accuracy of local models and provide enhanced edge intelligence without exposing the raw data in Mobile Edge Computing (MEC). However, in the MEC environment with constrained communication resources, transmitting the entire model between the server and the clients in traditional pFL methods imposes substantial communication overhead, which can lead to inaccurate personalization and degraded performance of mobile clients. In response, we propose a Communication-Efficient pFL architecture to enhance the performance of personalized models while minimizing communication overhead in MEC. First, a Knowledge-Aware Parameter Coaching method (KAPC) is presented to produce a more accurate personalized model by utilizing the layer-wise parameters of other clients with adaptive aggregation weights. Then, convergence analysis of the proposed KAPC is developed in both the convex and non-convex settings. Second, a Bidirectional Layer Selection algorithm (BLS) based on self-relationship and generalization error is proposed to select the most informative layers for transmission, which reduces communication costs. Extensive experiments are conducted, and the results demonstrate that the proposed KAPC achieves superior accuracy compared to the state-of-the-art baselines, while the proposed BLS substantially improves resource utilization without sacrificing performance.},
  archive      = {J_TMC},
  author       = {Mingjian Zhi and Yuanguo Bi and Lin Cai and Wenchao Xu and Haozhao Wang and Tianao Xiang and Qiang He},
  doi          = {10.1109/TMC.2024.3464512},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {321-337},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Knowledge-aware parameter coaching for communication-efficient personalized federated learning in mobile edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combinatorial data augmentation: A key enabler to bridge geometry- and data-driven WiFi positioning. <em>TMC</em>, <em>24</em>(1), 306-320. (<a href='https://doi.org/10.1109/TMC.2024.3465510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the emergence of various wireless sensing technologies, numerous positioning algorithms have been introduced in the literature, categorized into geometry-driven positioning (GP) and data-driven positioning (DP). These approaches have respective limitations, e.g., a non-line-of-sight issue for GP and the lack of a high-dimensional and labeled dataset for DP, which could be complemented by integrating both methods. To this end, this paper aims to introduce a novel principle called combinatorial data augmentation (CDA), a catalyst for the two approaches’ seamless integration. Specifically, GP-based data samples augmented from different positioning element combinations are called preliminary estimated locations (PELs), which can be used as high-dimensional inputs for DP. We confirm the CDA’s effectiveness from field experiments based on WiFi round-trip times (RTTs) and inertial measurement units (IMUs) by designing several CDA-based positioning algorithms. First, we show that CDA offers various metrics quantifying each PEL’s reliability, thereby extracting important PELs for WiFi RTT positioning. Second, CDA helps compute the observation error covariance matrix of a Kalman filter for fusing two position estimates derived by WiFi RTTs and IMUs. Third, we use the important PELs and the above position estimate as the corresponding input feature and the real-time label for fingerprint-based positioning as a representative DP algorithm. It provides accurate and reliable positioning results, with an average positioning error of 1.58 (m) and a standard deviation of 0.90 (m).},
  archive      = {J_TMC},
  author       = {Seung Min Yu and Kyuwon Han and Jihong Park and Seong-Lyun Kim and Seung-Woo Ko},
  doi          = {10.1109/TMC.2024.3465510},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {306-320},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Combinatorial data augmentation: A key enabler to bridge geometry- and data-driven WiFi positioning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scrava: Super resolution-based bandwidth-efficient cross-camera video analytics. <em>TMC</em>, <em>24</em>(1), 293-305. (<a href='https://doi.org/10.1109/TMC.2024.3461879'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massively deployed cameras form a tightly connected network which generates video streams continuously. Benefiting from advances in computer vision, automated real-time analytics of video streams can be of practical value in various scenarios. As cameras become more dense, cross-camera video analytics has emerged. Combining video contents from multiple cameras for analytics is certainly more promising than single-camera analytics, which can realize cross-camera pedestrian tracking and cross-camera complex behavior recognition. Some works focused on optimization of cross-camera video analytic applications, but most of them ignore specific network situation between cameras and edge servers. Furthermore, most of them ignore the super resolution technique, which is proven to be a source of efficiency. In this paper, we first verify the potential gain of super resolution on cross-camera video analytic tasks. Then, we design and implement a cross-camera real-time video streaming analytic system, ${\mathsf {Scrava}}$, which leverages super resolution to augment low-resolution videos and simultaneously reduce bandwidth consumption. ${\mathsf {Scrava}}$ enables real-time cross-camera video analytics and enhances video segments with the SR module under poor network conditions. We take cross-camera pedestrian tracking as an example, and experimentally verifies the effectiveness of super resolution on real-time cross-camera video analytics. Compared with using low-resolution video segments, ${\mathsf {Scrava}}$ can improve the F1 score by 47.16%, verifying the feasibility of exploiting super resolution to improve the performance of real-time cross-camera video analytic systems.},
  archive      = {J_TMC},
  author       = {Yu Liang and Sheng Zhang and Jie Wu},
  doi          = {10.1109/TMC.2024.3461879},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {293-305},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Scrava: Super resolution-based bandwidth-efficient cross-camera video analytics},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure two-party frequent itemset mining with guaranteeing differential privacy. <em>TMC</em>, <em>24</em>(1), 276-292. (<a href='https://doi.org/10.1109/TMC.2024.3464744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent itemset mining is an essential task in data analysis. Therefore, it is crucial to design practical methods for privacy-preserving frequent itemset mining, enabling private data analysis. For two-party data analysis tasks, each party possesses its portion of the data and is reluctant to share the data with the other. While secure computation can enable two-party frequent itemset mining, the output of exact top-$k$ itemsets may still leave the adversary a chance to infer the sensitive information. Differential privacy has been utilized in various data analysis tasks to safeguard participating individuals. However, addressing how to ensure differential privacy for two-party frequent itemset mining remains unexplored. To prevent each party’s data from being leaked to the other while achieving differential privacy for releasing the output, this paper investigates the problem of differentially private two-party frequent itemset mining. We first propose a practical method that can efficiently select the frequent items of the union of two confidential databases in a differentially private way without the need to combine all elements. Then we extend this technique for general frequent itemset mining. Extensive experiments were conducted on real-world datasets, and the results show that the proposed method can achieve satisfactory utility with affordable overheads.},
  archive      = {J_TMC},
  author       = {Wenjie Chen and Haoyu Chen and Tingxuan Han and Wei Tong and Sheng Zhong},
  doi          = {10.1109/TMC.2024.3464744},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {276-292},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Secure two-party frequent itemset mining with guaranteeing differential privacy},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Charger placement with wave interference. <em>TMC</em>, <em>24</em>(1), 261-275. (<a href='https://doi.org/10.1109/TMC.2024.3460403'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To guarantee the reliability for WRSNs, placing sufficient static chargers effectively ensures charging coverage for the entire network. However, this approach leads to a considerable number of sensors located within charging overlaps. The destructive wave interference caused by concurrent charging in these overlaps may weaken sensors received power, thereby negatively impacting charging performance. This work addresses a CHArging utIlity maximizatioN (CHAIN) problem, which aims to maximize the overall charging utility while considering wave interference among multiple chargers. Specifically, given a set of stationary sensors, we investigate how to determine optimal positions for a fixed number of chargers. To tackle this problem, we first develop a charging model with wave interference, then propose a two-step charger placement scheme to identify the optimal charger positions. In the first step, we maximize the overall additive power of the waves involved in interference by selecting an appropriate initial position for each charger. Then, in the second step, we maximize the overall charging utility by finding the optimal final position for each charger around its initial position. Finally, to evaluate the performance of our scheme, we conduct extensive simulations and field experiments and the results suggest that CHAIN performs better than the existing algorithms.},
  archive      = {J_TMC},
  author       = {Jing Xue and Die Wu and Jian Peng and Wenzheng Xu and Tang Liu},
  doi          = {10.1109/TMC.2024.3460403},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {261-275},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Charger placement with wave interference},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traffic load-aware resource management strategy for underwater wireless sensor networks. <em>TMC</em>, <em>24</em>(1), 243-260. (<a href='https://doi.org/10.1109/TMC.2024.3459896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater Wireless Sensor Networks (UWSNs) represent a promising technology that enables diverse underwater applications through acoustic communication. However, it encounters significant challenges including harsh communication environments, limited energy supply, and restricted signal transmission. This paper aims to provide efficient and reliable communication in underwater networks with limited energy and communication resources by optimizing the scheduling of communication links and adjusting transmission parameters (e.g., transmit power and transmission rate). The efficient and reliable communication multi-objective optimization problem (ERCMOP) is formulated as a decentralized partially observable Markov decision process (Dec-POMDP). A Traffic Load-Aware Resource Management (TARM) strategy based on deep multi-agent reinforcement learning (MARL) is presented to address this problem. Specifically, a traffic load-aware mechanism that leverages the overhear information from neighboring nodes is designed to mitigate the disparity between partial observations and global states. Moreover, by incorporating a solution space optimization algorithm, the number of candidate solutions for the deep MARL-based decision-making model can be effectively reduced, thereby optimizing the computational complexity. Simulation results demonstrate the adaptability of TARM in various scenarios with different transmission demands and collision probabilities, while also validating the effectiveness of the proposed approach in supporting efficient and reliable communication in underwater networks with limited resources.},
  archive      = {J_TMC},
  author       = {Tong Zhang and Yu Gou and Jun Liu and Jun-Hong Cui},
  doi          = {10.1109/TMC.2024.3459896},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {243-260},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Traffic load-aware resource management strategy for underwater wireless sensor networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Romeo: Fault detection of rotating machinery via fine-grained mmWave velocity signature. <em>TMC</em>, <em>24</em>(1), 227-242. (<a href='https://doi.org/10.1109/TMC.2024.3463955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time velocity monitoring is pivotal for fault detection of rotating machinery. However, existing methods rely on either troublesome deployments of optical encoders and IMU sensors or various tachometers delivering coarse-grained velocity measurements insufficient for fault detection. To overcome these limitations, we propose Romeo as the first work to exploit the mmWave radar for rotating machinery fault detection by extracting a fine-grained velocity signature. Though mmWave radars should capture instant rotation information with their claimed high sensitivity and sampling rate, direct adoption entails significant efforts for high-precision velocity measurement per radar to handle; particularly, exhausted system calibration and noise interference. To this end, we first develop a phase-velocity model to characterize the relationship between the mmWave signal phase and the fine-grained angular velocity. We then explore the geometric properties of specific positions in the rotation trajectory to precisely calibrate the rotation sensing model, leading to an iterative algorithm for accurate angular velocity measurement. Finally, we propose a simple yet effective fault detection algorithm by extracting a unique velocity signature. Our extensive experiments show Romeo achieves a median error of 0.4$^\circ$/s for fine-grained angular speed measurement, outperforming SOTA solutions with over ×16 angular speed granularity and ×7 measurement precision.},
  archive      = {J_TMC},
  author       = {Yanni Yang and Pengfei Hu and Jun Luo and Zhenlin An and Jiannong Cao and Dongxiao Yu and Xiuzhen Cheng},
  doi          = {10.1109/TMC.2024.3463955},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {227-242},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Romeo: Fault detection of rotating machinery via fine-grained mmWave velocity signature},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of device placement and model partitioning for cooperative DNN inference in heterogeneous edge computing. <em>TMC</em>, <em>24</em>(1), 210-226. (<a href='https://doi.org/10.1109/TMC.2024.3457793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EdgeAI represents a compelling approach for deploying DNN models at network edge through model partitioning. However, most existing partitioning strategies have primarily concentrated on homogeneous environments, neglecting the effect of device placement and their inapplicability to heterogeneous settings. Moreover, these strategies often rely on either data parallelism or model parallelism, each presenting its own limitations, including data synchronization and communication overhead. This paper aims at enhancing inference performance through a pipeline system of devices through leveraging both parallel and sequential relationships among them. Accordingly, the problem of Multi-Device Cooperative DNN Inference is formulated by optimizing both device placement and model partitioning, taking into account the unique characteristics of heterogeneous edge resources and DNN models, with the goal of maximizing throughput. To this end, we propose an evolutionary device placement technique to determine the pipeline stage of devices by enhancing a variant of particle swarm optimization. Subsequently, an adaptive model partitioning strategy is developed by combining intra-layer and inter-layer model partitioning based on dynamic programming and the input-output mapping of DNN layers, respectively, to accommodate edge resource limitations. Finally, we construct a simulation model and a prototype, and the extensive results demonstrate that our proposed algorithm outperforms current state-of-the-art algorithms.},
  archive      = {J_TMC},
  author       = {Penglin Dai and Biao Han and Ke Li and Xincao Xu and Huanlai Xing and Kai Liu},
  doi          = {10.1109/TMC.2024.3457793},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {210-226},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of device placement and model partitioning for cooperative DNN inference in heterogeneous edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PHCG: PLC honeypoint communication generator for industrial IoT. <em>TMC</em>, <em>24</em>(1), 198-209. (<a href='https://doi.org/10.1109/TMC.2024.3455564'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of mobile and wireless technologies, the industrial manufacturing sector has entered the era of automation. The proliferation of mobile devices, sensor networks, and remote monitoring systems enables factory equipment to be more flexibly connected and controlled. However, the trend towards industrial networks also brings new challenges. Industrial control systems (ICSs) and programmable logic controllers (PLCs) are more susceptible to hacker attacks and interference. Honeypoints have been developed to protect ICSs from addressing these threats, including potential internal attacks. Honeypoints are active deception systems that mitigate the limitations of conventional defense mechanisms, which successfully entice and neutralize internal enemies. This paper presents the PLC Honeypoint Communication Generator (PHCG), enhancing honeypoint protective capabilities in industrial IoT systems. Using an automated construction process, PHCG provides a convenient and efficient deployment method, ensuring quick and effective functioning. The functionality of a PLC relies on a data generation model trained on PLC response data. This model allows PHCG to imitate genuine PLC responses accurately when given authorized commands. The experimental results illustrate the adaptability of information produced by PHCG in different communication processes, with satisfactory timescales for both model training and data generation.},
  archive      = {J_TMC},
  author       = {Hao Liu and Yinghai Zhou and Binxing Fang and Yanbin Sun and Ning Hu and Zhihong Tian},
  doi          = {10.1109/TMC.2024.3455564},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {198-209},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PHCG: PLC honeypoint communication generator for industrial IoT},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-aided digital twin offloading mechanism in space-air-ground networks. <em>TMC</em>, <em>24</em>(1), 183-197. (<a href='https://doi.org/10.1109/TMC.2024.3455417'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-air-ground (SAG) integrated heterogenous networks can provide pervasive intelligence services for various ground users (GUs). The network can help cellular networks release network resources and alleviate congestion pressure. Moreover, one important application of the network is that digital twin (DT) can enable nearly-instant wireless connectivity and highly-reliable data mapping from physical systems to digital world in a real-time fashion. The integration of SAG and DT (SAG-DT) reduces the gap between data analysis and physical status, which can further realize robust edge intelligence services. However, the random computation task arrival, time-varying channel gains, and the lack of mutual trust among ground GUs hinder better quality of service in the promising SAG-DT network. In this paper, we envision a SAG-DT integrated blockchain model to transfer the task data to the aerial network, and then perform the computation offloading, energy harvesting and privacy protection. Moreover, we propose a Lyapunov-aided multi-agent deep federated reinforcement learning (MADFRL) algorithm framework to optimize the CPU cycle frequency, the size of block, the number of DTs, and harvested energy to minimize the execution costs and privacy overhead. Extensive performance analyses indicate that the MADFRL algorithm framework can strengthen the data privacy via blockchain verification mechanism and approaches the optimal performance on the basis of lower computation complexity. Finally, simulation results corroborate that the proposed Lyapunov-aided MADFRL algorithm is superior to advanced benchmarks in terms of execution costs, task processing quantities and privacy overhead.},
  archive      = {J_TMC},
  author       = {Yongkang Gong and Haipeng Yao and Zehui Xiong and C. L. Philip Chen and Dusit Niyato},
  doi          = {10.1109/TMC.2024.3455417},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {183-197},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Blockchain-aided digital twin offloading mechanism in space-air-ground networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Budget-constrained digital twin synchronization and its application on fidelity-aware queries in edge computing. <em>TMC</em>, <em>24</em>(1), 165-182. (<a href='https://doi.org/10.1109/TMC.2024.3455357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advance of mobile edge computing (MEC) and the Internet of Things (IoT), digital twin (DT) has become an emerging technology for provisioning IoT services between the real world and the cyber world. In this paper, we consider the state updating of DTs in an MEC network through synchronizing DTs with their physical objects. We make use of an energy-constrained UAV for data collection in a sensor network, as an illustrative example for the DT state updating of each object (sensor), and then use the DT data of objects (sensors) later for fidelity-aware query services. To this end, we first formulate a novel DT state staleness minimization, under a given update budget per update round. We then propose an optimal algorithm for a special case of the problem where the budget per update round is exactly $K$ objects synchronizing with their DTs. We then devise an algorithm for the DT state staleness minimization problem by reducing to the award collection maximization problem, assuming that the volume of the update data generated by each object per update round is given. Otherwise, we adopt a deep learning method to predict the volume of the update data. To demonstrate the importance of the DT state staleness in practical applications, we consider fidelity-aware query services in the MEC network, and we develop a cost-effective evaluation plan for each query. We finally evaluate the performance of the proposed algorithms through simulations. Simulation results demonstrate that the proposed algorithms are promising.},
  archive      = {J_TMC},
  author       = {Yuchen Li and Weifa Liang and Zichuan Xu and Wenzheng Xu and Xiaohua Jia},
  doi          = {10.1109/TMC.2024.3455357},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {165-182},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Budget-constrained digital twin synchronization and its application on fidelity-aware queries in edge computing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The structure of hypergraphs arising in cellular mobile communication systems. <em>TMC</em>, <em>24</em>(1), 150-164. (<a href='https://doi.org/10.1109/TMC.2024.3460170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An assumption that researchers have often used to model interference in a wireless network is the unit disk graph model. While many theoretical results and performance guarantees have been obtained under this model, an open research direction is to extend these results to hypergraph interference models. Motivated by recent results that the worst-case performance of the distributed maximal scheduling algorithm is characterized by the interference degree of the hypergraph, in the present work we investigate properties of the interference degree of the hypergraph and the structure of hypergraphs arising from physical constraints. We show that the problem of computing the interference degree of a hypergraph is NP-hard and we prove some properties and results concerning this hypergraph invariant. We investigate which hypergraphs are realizable, i.e. which hypergraphs arise in practice, based on physical constraints, as the interference model of a wireless network. In particular, a question that arises naturally is: what is the maximal value of $r$ such that the hypergraph $K_{1,r}$ is realizable? We determine this quantity for various integral and nonintegral values of the path loss exponent of signal propagation. We also investigate hypergraphs generated by line networks.},
  archive      = {J_TMC},
  author       = {Ashwin Ganesan},
  doi          = {10.1109/TMC.2024.3460170},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {150-164},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {The structure of hypergraphs arising in cellular mobile communication systems},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). T-READi: Transformer-powered robust and efficient multimodal inference for autonomous driving. <em>TMC</em>, <em>24</em>(1), 135-149. (<a href='https://doi.org/10.1109/TMC.2024.3462437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by autonomous vehicles (AVs), deep analytics to fuse their outputs for a robust perception become imperative. However, existing fusion methods often make two assumptions rarely holding in practice: i) similar data distributions for all inputs and ii) constant availability for all sensors. Because, for example, lidars have various resolutions and failures of radars may occur, such variability often results in significant performance degradation in fusion. To this end, we present t-READi, an adaptive inference system that accommodates the variability of multimodal sensory data and thus enables robust and efficient perception. t-READi identifies variation-sensitive yet structure-specific model parameters; it then adapts only these parameters while keeping the rest intact. t-READi also leverages a cross-modality contrastive learning method to compensate for the loss from missing modalities. Both functions are implemented to maintain compatibility with existing multimodal deep fusion methods. The extensive experiments evidently demonstrate that compared with the status quo approaches, t-READi not only improves the average inference accuracy by more than 6% but also reduces the inference latency by almost 15× with the cost of only 5% extra memory overhead in the worst case under realistic data and modal variations.},
  archive      = {J_TMC},
  author       = {Pengfei Hu and Yuhang Qian and Tianyue Zheng and Ang Li and Zhe Chen and Yue Gao and Xiuzhen Cheng and Jun Luo},
  doi          = {10.1109/TMC.2024.3462437},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {135-149},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {T-READi: Transformer-powered robust and efficient multimodal inference for autonomous driving},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Argus: Enabling cross-camera collaboration for video analytics on distributed smart cameras. <em>TMC</em>, <em>24</em>(1), 117-134. (<a href='https://doi.org/10.1109/TMC.2024.3459409'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overlapping cameras offer exciting opportunities to view a scene from different angles, allowing for more advanced, comprehensive and robust analysis. However, existing video analytics systems for multi-camera streams are mostly limited to (i) per-camera processing and aggregation and (ii) workload-agnostic centralized processing architectures. In this paper, we present Argus, a distributed video analytics system with cross-camera collaboration on smart cameras. We identify multi-camera, multi-target tracking as the primary task of multi-camera video analytics and develop a novel technique that avoids redundant, processing-heavy identification tasks by leveraging object-wise spatio-temporal association in the overlapping fields of view across multiple cameras. We further develop a set of techniques to perform these operations across distributed cameras without cloud support at low latency by (i) dynamically ordering the camera and object inspection sequence and (ii) flexibly distributing the workload across smart cameras, taking into account network transmission and heterogeneous computational capacities. Evaluation of three real-world overlapping camera datasets with two Nvidia Jetson devices shows that Argus reduces the number of object identifications and end-to-end latency by up to 7.13× and 2.19× (4.86× and 1.60× compared to the state-of-the-art), while achieving comparable tracking quality.},
  archive      = {J_TMC},
  author       = {Juheon Yi and Utku Günay Acer and Fahim Kawsar and Chulhong Min},
  doi          = {10.1109/TMC.2024.3459409},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {117-134},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Argus: Enabling cross-camera collaboration for video analytics on distributed smart cameras},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P3ID: A privacy-preserving person identification framework towards multi-environments based on transfer learning. <em>TMC</em>, <em>24</em>(1), 102-116. (<a href='https://doi.org/10.1109/TMC.2024.3459944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Concerns surrounding privacy leakages caused by prevalent vision-based person identifications are countless. A promising privacy-preserving solution is to identify the wireless signals reflecting persons, which, however, faces a major challenge of losing efficacy in multi-environments. In this paper, we work on person identification based on wireless signals using transfer learning, toward tackling the performance deterioration across environments. We investigate the feature variations induced by environmental shifts based on data measurements. Lay our foundation on the feature alignment concept, we propose a novel wireless-based person identification framework using transfer learning. In the framework, we integrate a series of signal processing methods including signal selection, pre-processing, and augmentation, where the first includes a reference environment to assist the feature extraction while the latter two respectively reduce the data noise and improve the data diversity. We also propose a model generalization method where a neural network is employed to align features from different environments, which facilitates the extraction of environment-independent features while incorporating both person and environment information. On a real wireless testbed consisting of an Impulse Radio Ultra-WideBand (IR-UWB) radar, we build and publicly release a dataset with 22,264 samples of ten individuals from three environments, varying in testing distance and obstruction condition. Extensive experimental evaluations demonstrate that the proposed framework can improve the identification accuracy across environments, and surpasses state-of-the-art methods by up to 18.06%.},
  archive      = {J_TMC},
  author       = {Hanxiang He and Xintao Huan and Jing Wang and Yong Luo and Han Hu and Jianping An},
  doi          = {10.1109/TMC.2024.3459944},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {102-116},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {P3ID: A privacy-preserving person identification framework towards multi-environments based on transfer learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Network-wide data collection based on in-band network telemetry for digital twin networks. <em>TMC</em>, <em>24</em>(1), 86-101. (<a href='https://doi.org/10.1109/TMC.2024.3456584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Digital Twin Network (DTN) establishes a real-time virtual mirror of physical networks. Data collection plays an essential role in DTN, which collects the status data of physical network for building highly consistent digital twins. In this paper, we present a network-wide data collection scheme based on In-band Network Telemetry (INT). To build a lifelike mirror of the physical network, the probing path set is required to cover all links so that network topology, traffic load, and port-level device information is captured. We present a Latency-aware High-degree Replicated First (LHRF) vertex-cut graph partitioning algorithm to partition the network into several balanced subgraphs while trying to replicate the high-degree vertexes among partitions first. LHRF aims to balance the length and accumulated latency of the probing paths. With shorter and stabler probing latencies, the information received by digital twin can reflect the latest and consistent network-wide status. To prevent the packets from being fragmented due to overlong paths, a deep limited search (DLS) based path planning algorithm is employed to generate non-overlapped probing paths covering all edges in the separated subgraphs. Simulation results demonstrate that the proposed scheme generates more balanced INT paths with constrained path length and shorter, stabler probing delay.},
  archive      = {J_TMC},
  author       = {Zhihao Wang and Dingde Jiang and Shahid Mumtaz},
  doi          = {10.1109/TMC.2024.3456584},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {86-101},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Network-wide data collection based on in-band network telemetry for digital twin networks},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ambient light reflection-based eavesdropping enhanced with cGAN. <em>TMC</em>, <em>24</em>(1), 72-85. (<a href='https://doi.org/10.1109/TMC.2024.3460392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sound eavesdropping using light has been an area of considerable interest and concern, as it can be achieved over long distances. However, previous work has often lacked stealth (e.g., active emission of laser beams) or been limited in the range of realistic applications (e.g., using direct light from a device’s indicator LED or a hanging light bulb). In this paper, we present EchoLight, a non-intrusive, passive and long-range sound eavesdropping method that utilizes the extensive reflection of ambient light from vibrating objects to reconstruct sound. We analyze the relationship between reflection light signals and sound signals, particularly in situations where the frequency response of reflective objects and the efficiency of diffuse reflection are suboptimal. Based on this analysis, we have introduced an algorithm based on cGAN to address the issues of nonlinear distortion and spectral absence in the frequency domain of sound. We extensively evaluate EchoLight’s performance in a variety of real-world scenarios. It demonstrates the ability to accurately reconstruct audio from a variety of source distances, attack distances, sound levels, light sources, and reflective materials. Our results reveal that the reconstructed audio exhibits a high degree of similarity to the original audio over 40 meters of attack distance.},
  archive      = {J_TMC},
  author       = {Guoming Zhang and Heqiang Fu and Zhijie Xiang and Xinyan Zhou and Pengfei Hu and Xiuzhen Cheng and Yanni Yang},
  doi          = {10.1109/TMC.2024.3460392},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {72-85},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Ambient light reflection-based eavesdropping enhanced with cGAN},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and performance of resonant beam Communications—Part i: Quasi-static scenario. <em>TMC</em>, <em>24</em>(1), 62-71. (<a href='https://doi.org/10.1109/TMC.2024.3458415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This two-part paper studies a point-to-point resonant beam communication (RBCom) system, where two separately deployed retroreflectors are adopted to generate the resonant beam between the transmitter and the receiver, and analyzes the transmission rate of the considered system under both the quasi-static and mobile scenarios. Part I of this paper focuses on the quasi-static scenario where the locations of the transmitter and the receiver are relatively fixed. Specifically, we propose a new information-bearing scheme which adopts a synchronization-based amplitude modulation method to mitigate the echo interference caused by the reflected resonant beam. With this scheme, we show that the quasi-static RBCom channel is equivalent to a Markov channel and can be further simplified as an amplitude-constrained additive white Gaussian noise channel. Moreover, we develop an algorithm that jointly employs the bisection and exhaustive search to maximize its capacity upper and lower bounds. Finally, numerical results validate our analysis. Part II of this paper discusses the performance of the RBCom system under the mobile scenario.},
  archive      = {J_TMC},
  author       = {Dongxu Li and Yuanming Tian and Chuan Huang and Qingwen Liu and Shengli Zhou},
  doi          = {10.1109/TMC.2024.3458415},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {62-71},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Design and performance of resonant beam Communications—Part i: Quasi-static scenario},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LI2: A new learning-based approach to timely monitoring of points-of-interest with UAV. <em>TMC</em>, <em>24</em>(1), 45-61. (<a href='https://doi.org/10.1109/TMC.2024.3461708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) play a critical role in disaster response, swiftly gathering information from various points-of-interest (PoIs) across extensive areas. The freshness of this information is measured by the age of information (AoI), representing the time since the latest information acquisition of a specific PoI. However, devising AoI-minimizing routes for UAVs in obstructed post-disaster environments poses unique challenges that have yet to be fully overcome. Obstacles, like post-disaster barriers, can impede direct flight paths between PoIs, and limited battery life requires energy-conscious route planning. Additionally, existing solutions fail to universally minimize varying data freshness requirements. This research addresses the AoI-driven UAV travel problem, seeking to establish periodic routes that optimize AoI metrics while considering energy and general graph constraints. We develop a learning-based algorithm to enhance the current route iteratively, utilizing guidance from a deep reinforcement learning (DRL) agent and executing a series of operations to potentially decrease AoI while adhering to topological and energy constraints. The algorithm is validated on real post-disaster datasets, demonstrating significant improvements in various AoI metrics compared to other learning-based approaches. Furthermore, our algorithm outperforms approximation algorithms and can approach the global optimum when tailored to existing AoI-minimizing problems.},
  archive      = {J_TMC},
  author       = {Ziyao Huang and Weiwei Wu and Kui Wu and Hang Yuan and Chenchen Fu and Feng Shan and Jianping Wang and Junzhou Luo},
  doi          = {10.1109/TMC.2024.3461708},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {45-61},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LI2: A new learning-based approach to timely monitoring of points-of-interest with UAV},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed age-of-information scheduling with NOMA via deep reinforcement learning. <em>TMC</em>, <em>24</em>(1), 30-44. (<a href='https://doi.org/10.1109/TMC.2024.3459101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many emerging applications in edge computing require processing of huge volumes of data generated by end devices, using the freshest available information. In this paper, we address the distributed optimization of multi-user long-term average Age-of-Information (AoI) objectives in edge networks that use NOMA transmission. This poses a challenge of non-convex online optimization, which in existing work often requires either decision making in a combinatorial space or a global view of entire network states. To overcome this challenge, we propose a reinforcement learning-based framework that adopts a novel hierarchical decomposition of decision making. Specifically, we propose three different types of distributed agents to learn with respect to efficiency of AoI scheduling, fairness of AoI scheduling, as well as a high-level policy balancing these potentially conflicting design objectives. Not only does the proposed decomposition improve learning performance due to disentanglement of different design objectives/rewards, but it also enables the algorithm to learn the best policy while also learning the explanations – as actions can be directly compared in terms of the design objectives. Our evaluations show that the proposed algorithm improves the long-term average AoI by $200\%{-}300\%$ and 400% compared to prior works with NOMA and the optimal solution without NOMA, respectively.},
  archive      = {J_TMC},
  author       = {Congwei Zhang and Yifei Zou and Zuyuan Zhang and Dongxiao Yu and Jorge Torres Gómez and Tian Lan and Falko Dressler and Xiuzhen Cheng},
  doi          = {10.1109/TMC.2024.3459101},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {30-44},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed age-of-information scheduling with NOMA via deep reinforcement learning},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WordWhisper: Exploiting real-time, hardware-dependent IoT communication against eavesdropping. <em>TMC</em>, <em>24</em>(1), 15-29. (<a href='https://doi.org/10.1109/TMC.2024.3443333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure protocol-independent communication is increasingly demanding to support information exchange among neighbor Internet of Things (IoT) devices. For example, recent works utilize ultrasound at the resonant frequency range of a gyroscope to build communication between a speaker and the gyroscope. However, they are vulnerable to eavesdropping attacks and may have limitations in communication delays. In this work, we present WordWhisper, an efficient, word-level, and speaker-to-gyroscope communication system, with which only the target device can receive the correct information. We theoretically analyze Micro-Electro-Mechanical System (MEMS) gyroscope resonance and propose a hardware-dependent mechanism to defend against eavesdropping, making non-target gyroscopes receive ineffective information. Note that WordWhisper is free of costly data collection from gyroscopes, we train and update our decoding model based on the synthesized data (generated from theoretical MEMS resonance analysis) rather than the costly collected data from gyroscopes. Meanwhile, we address the challenge of eavesdropping when it comes to multiple attackers. We evaluate WordWhisper over 50 MEMS gyroscopes and 100 words. Extensive evaluations demonstrate that WordWhisper can achieve word-level communication with 99.33% accuracy while the recognition accuracy drops to a random guess for the non-target. Our decoding delay is lower than 0.63 seconds.},
  archive      = {J_TMC},
  author       = {Junyang Zhang and Jiahui Hou and Ye Tian and Xiang-Yang Li},
  doi          = {10.1109/TMC.2024.3443333},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {15-29},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {WordWhisper: Exploiting real-time, hardware-dependent IoT communication against eavesdropping},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pattern-sensitive local differential privacy for finite-range time-series data in mobile crowdsensing. <em>TMC</em>, <em>24</em>(1), 1-14. (<a href='https://doi.org/10.1109/TMC.2024.3445973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series data is crucial for the development of mobile crowdsensing (MCS). Participant’s privacy is one of the major concerns because MCS data often contain sensitive individual information. Existing privacy-preserving mechanisms for time-series data do not preserve salient patterns of the time series and take into account that the perturbed data may fall outside the valid data interval, leading to data distortion. To overcome these deficiencies, we first perform dynamic feature extraction and incorporate an adaptive sampling scheme that is sensitive to the distinction of short-term patterns and stable patterns. Then a Bounded Laplace (BLP) mechanism is adopted with a theoretical guarantee on the data perturbation range so as to address the issue of data going beyond the valid range. We establish theoretically that the proposed Adaptive Sampling and Randomized perturbation mechanism based on dynamic Temporal patterns (ASRT) satisfies the metric-based $w$-event $\epsilon$-LDP for privacy protection. Empirical results of extensive experiments on realworld datasets demonstrate that our proposed method is superior to existing protection mechanisms and the efficacy of our ASRT in enhancing data utility without introducing outliers.},
  archive      = {J_TMC},
  author       = {Zhetao Li and Xiyu Zeng and Yong Xiao and Chengxin Li and Wentai Wu and Haolin Liu},
  doi          = {10.1109/TMC.2024.3445973},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Pattern-sensitive local differential privacy for finite-range time-series data in mobile crowdsensing},
  volume       = {24},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
