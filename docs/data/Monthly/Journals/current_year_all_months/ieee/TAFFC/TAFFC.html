<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc">TAFFC - 179</h2>
<ul>
<li><details>
<summary>
(2025). Reading moods by mouse-cursor tracking: Representational similarity analysis. <em>TAFFC</em>, <em>16</em>(3), 2499-2506. (<a href='https://doi.org/10.1109/TAFFC.2025.3550304'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Theories of Constructed Emotion and Grounded Cognition suggest that our sensorimotor experiences underpin the formation of emotions. This study explores this premise by examining how movements of a computer cursor can reflect moods of participants. We conducted an experiment where participants engaged in a simple choice-reaching task, with their mouse-cursor movements tracked pixel by pixel. Mood assessments were conducted using the PANAS-X scale before and after the task. Through Intersubject Representational Similarity Analysis, we investigated the correlation between the patterns of mouse movements and self-reported moods. Our findings reveal a significant association between negative emotions, such as fear and hostility, and certain movement patterns, e.g., randomness and deviations from a direct path. Furthermore, our machine learning-based Representational Similarity Analysis (ML-RSA) underscores the value of second-order similarity measures, revealing meaningful alignments between sensorimotor behaviors and emotional states across distinct measurement domains. These findings highlight the potential of cursor-tracking as a tool for exploring the interplay between emotion and action.},
  archive      = {J_TAFFC},
  author       = {Takashi Yamauchi and Kunxia Wang},
  doi          = {10.1109/TAFFC.2025.3550304},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2499-2506},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Reading moods by mouse-cursor tracking: Representational similarity analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of algorithmic transparency on user experience and physiological responses in affect-aware task adaptation. <em>TAFFC</em>, <em>16</em>(3), 2491-2498. (<a href='https://doi.org/10.1109/TAFFC.2025.3530318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In affect-aware task adaptation, users’ psychological states are recognized with diverse measurements and used to adapt computer-based tasks. User experience with such adaptation improves as the accuracy of psychological state recognition and task adaptation increases. However, it is unclear how user experience is influenced by algorithmic transparency: the degree to which users understand the computer's decision-making process. We thus created an affect-aware task adaptation system with 4 algorithmic transparency levels (none/low/medium/high) and conducted a study where 93 participants first experienced adaptation with no transparency for 16 minutes, then with one of the other 3 levels for 16 minutes. User experience questionnaires and physiological measurements (respiration, skin conductance, heart rate) were analyzed with mixed 2×3 analyses of variance (time × transparency group). Self-reported interest/enjoyment and competence were lower with low transparency than with medium/high transparency, but did not differ between medium and high transparency. The transparency level may also influence participants’ respiratory responses to adaptation errors, but this finding is based on ad-hoc t-tests and should be considered preliminary. Overall, results show that the degree of algorithmic transparency does influence self-reported user experience. Since transparency information is relatively easy to provide, it may represent a worthwhile design element in affective computing.},
  archive      = {J_TAFFC},
  author       = {Mohammad Sohorab Hossain and Joshua D. Clapp and Vesna D. Novak},
  doi          = {10.1109/TAFFC.2025.3530318},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2491-2498},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Effects of algorithmic transparency on user experience and physiological responses in affect-aware task adaptation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anti-stress: A wearable device with real-time breathing feedback for stress relief. <em>TAFFC</em>, <em>16</em>(3), 2479-2490. (<a href='https://doi.org/10.1109/TAFFC.2025.3564868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relieving stress is crucial for the health of modern individuals. Deep breathing exercises have been shown to be effective for stress relief. However, the effectiveness of deep breathing exercises is limited if feedback on breathing patterns is not provided instantly. For this, we propose a wearable device called Anti-Stress, which can provide users with breathing patterns in real-time, enabling them to dynamically adjust their breathing according to Anti-Stress instructions for optimal relaxation. In addition, Anti-Stress provides users with objective stress indices by measuring the user's heart rate variability (HRV). The accuracy of Anti-Stress in detecting breathing patterns is up to 99%, and its response time is less than 200 ms, ensuring users can receive immediate guidance. To evaluate the effectiveness of Anti-Stress, we recruited 60 participants to conduct an empirical experiment. The ANCOVA analysis shows that the experimental group significantly reduces stress by using Anti-Stress compared to the control group without it. Furthermore, the usability questionnaires including SUS and QUIS demonstrate Anti-Stress's higher usability compared to a traditional non-biofeedback app. Our analysis also reveals that people with high openness or neuroticism personalities had better stress relief by using Anti-Stress. The results of this study demonstrate the feasibility and practicality of our respiratory feedback mechanism in helping users relieve stress and enhancing the effectiveness of deep breathing exercises.},
  archive      = {J_TAFFC},
  author       = {Jin-Wei Hou and Edward T.-H. Chu and Chia-Rong Lee},
  doi          = {10.1109/TAFFC.2025.3564868},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2479-2490},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Anti-stress: A wearable device with real-time breathing feedback for stress relief},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting the intrinsic neighborhood semantic structure for domain adaptation in EEG-based emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2466-2478. (<a href='https://doi.org/10.1109/TAFFC.2025.3564272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inherent non-stationarity and individual differences present in electroencephalogram (EEG) signals, developing a generalizable model that performs well on new subjects is challenging in EEG-based emotion recognition. Most existing domain adaptation (DA) methods typically mitigate these discrepancies by aligning the marginal distributions of domain feature representations. However, when there is a significant difference in the class-conditional distribution between domain features and labels, the domain-invariant features learned by aligning marginal distributions may have limited discriminative ability for unlabeled target instances or even prove counterproductive. To address this issue, we propose a Neighborhood Semantic Aware Learning-based Dynamic Graph Attention Convolution (NSAL-DGAT) approach that learns target semantic information by considering the inter-domain semantic topological structure, thereby improving classifier adaptation for target instances. Specifically, the proposed NSAL framework is designed to capitalize on the insight that after domain feature alignment, some target samples and their neighboring source samples exhibit similar semantics. By leveraging the neighborhood topological structure, we extract and incorporate semantic target features to train a more transferable classifier. Besides, we implement an entropy weighting mechanism to emphasize representative target semantic information, encouraging target instances to prioritize high-confidence individuals within the source neighborhood. We have conducted extensive experiments on the public SEED dataset and our collected the Hearing-Impaired EEG Dataset (HIED). The experimental results underscore the efficacy of our proposed NSAL-DGAT approach, showcasing state-of-the-art accuracy in subject-dependent as well as subject-independent scenarios.},
  archive      = {J_TAFFC},
  author       = {Yi Yang and Ze Wang and Yu Song and Ziyu Jia and Boyu Wang and Tzyy-Ping Jung and Feng Wan},
  doi          = {10.1109/TAFFC.2025.3564272},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2466-2478},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Exploiting the intrinsic neighborhood semantic structure for domain adaptation in EEG-based emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of multimodal self-supervised architectures for daily life affect recognition. <em>TAFFC</em>, <em>16</em>(3), 2454-2465. (<a href='https://doi.org/10.1109/TAFFC.2025.3562552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition of affects (an umbrella term including but not limited to emotions, mood, and stress) in daily life is crucial for maintaining mental well-being and preventing long-term health issues. Wearable devices, such as smart bands, can collect physiological data including heart rate variability, electrodermal activity, skin temperature, and acceleration facilitating daily life affect monitoring via machine learning models. However, accurately labeling this data for model evaluation is challenging in affective computing research, as individuals often provide subjective, inaccurate, or incomplete labels in their daily lives. This study introduces the adaptation of self-supervised learning architectures for multimodal daily life stress and emotion recognition tasks, focusing on self-representation and contrastive learning methods. By leveraging unlabeled multimodal physiological signals, we aim to alleviate the need for extensive labeled data and enhance model generalizability. Our research demonstrates that self-supervised learning can effectively learn meaningful representations from physiological data without explicit labels, offering a promising approach for developing robust affect recognition systems that can operate in dynamic and uncontrolled environments. This work represents a significant improvement in recognizing affects in the wild, with potential implications for personalized mental health support and timely interventions.},
  archive      = {J_TAFFC},
  author       = {Yekta Said Can and Mohamed Benouis and Bhargavi Mahesh and Elisabeth André},
  doi          = {10.1109/TAFFC.2025.3562552},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2454-2465},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Application of multimodal self-supervised architectures for daily life affect recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-kernel embedding fusion framework for physiological signal based emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2440-2453. (<a href='https://doi.org/10.1109/TAFFC.2025.3562905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physiological signal-based emotion recognition requires effective fusion of multi-modal physiological signals to improve recognition accuracy. In this paper, a multi-kernel embedding fusion framework (MKEFF) is proposed for multi-modal physiological signal emotion recognition. Specifically, multi-kernel learning and kernel approximation techniques are used to compute the multi-kernel embeddings of the original feature vectors of each modality independently. The embeddings are then fed in parallel to their respective representation learning layer, where the proposed sparse relation learning method is applied to all the modalities to explore the correlation and diversity among them. Finally, a distribution alignment based fusion method is proposed to align each modality in the subspace, and a weighted summation fusion is performed to obtain the fused representations. Extensive cross-subject emotion recognition experiments are conducted on three public datasets, DEAP, DECAF, and SEED-IV, to evaluate the proposed method. The experimental results demonstrate that the proposed method achieves better classification performance and interpretability than the state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Xinrun He and Jian Huang and Zhongzheng Fu and Yixuan Li and Dongrui Wu},
  doi          = {10.1109/TAFFC.2025.3562905},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2440-2453},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A multi-kernel embedding fusion framework for physiological signal based emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wearable sensor-based multimodal physiological responses of socially anxious individuals in social contexts on zoom. <em>TAFFC</em>, <em>16</em>(3), 2428-2439. (<a href='https://doi.org/10.1109/TAFFC.2025.3562787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Correctly identifying an individual’s social context from passively worn sensors holds promise for delivering just-in-time adaptive interventions (JITAIs) to treat social anxiety. In this study, we present results using passively collected data from a within-subjects experiment that assessed physiological responses across different social contexts (i.e., alone versus with others), social phases (i.e., pre- and post-interaction versus during an interaction), social interaction sizes (i.e., dyadic versus group interactions), and levels of social threat (i.e., implicit versus explicit social evaluation). Participants in the study ($N=46$) reported moderate to severe social anxiety symptoms as assessed by the Social Interaction Anxiety Scale ($\geq$34 out of 80). Univariate paired difference tests, multivariate random forest models, and cluster analyses were used to explore physiological response patterns across different social and non-social contexts. Our results suggest that social context is more reliably distinguishable than social phase, group size, or level of social threat, and that there is considerable variability in physiological response patterns even among distinguishable contexts. Implications for real-world context detection and future deployment of JITAIs are discussed.},
  archive      = {J_TAFFC},
  author       = {Emma R. Toner and Mark Rucker and Zhiyuan Wang and Maria A. Larrazabal and Lihua Cai and Debajyoti Datta and Haroon Lone and Mehdi Boukhechba and Bethany A. Teachman and Laura E. Barnes},
  doi          = {10.1109/TAFFC.2025.3562787},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2428-2439},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Wearable sensor-based multimodal physiological responses of socially anxious individuals in social contexts on zoom},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoAffinity: A multimodal dataset for cognitive load and affect assessment in remote collaboration. <em>TAFFC</em>, <em>16</em>(3), 2410-2427. (<a href='https://doi.org/10.1109/TAFFC.2025.3562559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the relationship between cognitive load and affective state in remote work is vital for designing intuitive collaboration. We present CoAffinity, a multimodal dataset encompassing eight structured remote-work tasks, during which 39 participants provided self-reported measures (arousal, valence, positive/negative affect, and cognitive-load) while being recorded via audio, video, and physiological signals (PPG and GSR). Spanning over 38 hours of annotated data, our approach involved precise timestamp alignment, short and long-session labelling, and subsequent machine-learning and deep-learning benchmarks. Key findings show that integrating multiple modalities, especially physiological data, significantly improves the detection of cognitive load and emotion, while group synchrony metrics highlight how physiological coherence shifts under varied task demands. By capturing complex cognitive-emotional dynamics in realistic remote settings, CoAffinity aims to advance affective computing, inform human-computer interaction research, and foster more empathetic remote collaboration tools.},
  archive      = {J_TAFFC},
  author       = {Tamil Selvan Gunasekaran and Kunal Gupta and Yun Suen Pai and Huidong Bai and Mark Billinghurst},
  doi          = {10.1109/TAFFC.2025.3562559},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2410-2427},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {CoAffinity: A multimodal dataset for cognitive load and affect assessment in remote collaboration},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stream dynamic heterogeneous graph recurrent neural network for multi-label multi-modal emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2396-2409. (<a href='https://doi.org/10.1109/TAFFC.2025.3561439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of the relationship between emotions and physiological signals of subjects under multimedia stimulation is an emerging field, and many important advances are made. However, there are still some challenges: 1) How to effectively utilize the complementarity among spatial-spectral-temporal domain information. 2) How to employ the heterogeneity and the correlation among multi-modal physiological signals simultaneously. 3) How to improve the robustness of the model dealing with missing channels. 4) How to model the dependency among different emotions. In this paper, we propose a novel two-stream Dynamic Heterogeneous Graph Recurrent Neural Network called DHGRNN. Specifically, DHGRNN consists of a spatial-temporal stream, a spatial-spectral stream, a fusion layer, and a multi-label classifier. Each stream is composed of a graph transformer network, evolved graph convolutional neural network, and gated recurrent units. We propose a graph-based two-stream structure to fuse the information of the spatial-spectral-temporal domain simultaneously. Graph transformer network and evolved graph convolutional neural network are used to model the heterogeneity and correlation of multi-modal physiological signals, respectively. To deal with the problem of robustness in the face of missing channel data, we transform it into the problem of dynamic graphs and use a dynamic graph neural network to improve the robustness. In addition, we propose a multi-label classifier to model the dependency among different emotion dimensions. Experiments on three public datasets demonstrate that our proposed model outperforms existing state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Jing Wang and Zhiyang Feng and Xiaojun Ning and Youfang Lin and Badong Chen and Ziyu Jia},
  doi          = {10.1109/TAFFC.2025.3561439},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2396-2409},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Two-stream dynamic heterogeneous graph recurrent neural network for multi-label multi-modal emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial label learning for emotion recognition from EEG. <em>TAFFC</em>, <em>16</em>(3), 2381-2395. (<a href='https://doi.org/10.1109/TAFFC.2025.3562027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully supervised learning has recently achieved promising performance in various electroencephalography (EEG) learning tasks by training on large datasets with ground truth labels. However, labeling EEG data for affective experiments is challenging, as it can be difficult for participants to accurately distinguish between similar emotions, resulting in ambiguous labeling (reporting multiple emotions for one EEG instance). This notion could cause model performance degradation, as the ground truth is hidden within multiple candidate labels. To address this issue, Partial Label Learning (PLL) has been proposed to identify the ground truth from candidate labels during the training phase, and has shown good performance in the computer vision domain. However, PLL methods have not yet been adopted for EEG representation learning or implemented for emotion recognition tasks. In this paper, we adapt and re-implement six state-of-the-art PLL approaches for emotion recognition from EEG on two large emotion datasets (SEED-IV and SEED-V). These datasets contain four and five categories of emotions, respectively. We evaluate the performance of all methods in classical, circumplex-based and real-world experiments. The results show that PLL methods can achieve strong results in affective computing from EEG and achieve comparable performance to fully supervised learning. We also investigate the effect of label disambiguation, a key step in many PLL methods. The results show that in most cases, label disambiguation would benefit the model when the candidate labels are generated based on their similarities to the ground truth rather than obeying a uniform distribution. This finding suggests the potential of using label disambiguation-based PLL methods for circumplex-based and real-world affective tasks.},
  archive      = {J_TAFFC},
  author       = {Guangyi Zhang and Ali Etemad},
  doi          = {10.1109/TAFFC.2025.3562027},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2381-2395},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Partial label learning for emotion recognition from EEG},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EmoSphere++: Emotion-controllable zero-shot text-to-speech via emotion-adaptive spherical vector. <em>TAFFC</em>, <em>16</em>(3), 2365-2380. (<a href='https://doi.org/10.1109/TAFFC.2025.3561267'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional text-to-speech (TTS) has advanced significantly, but challenges persist due to the complexity of emotions and limitations in emotional speech datasets and models. A key issue with previous studies is the reliance on limited emotional speech datasets or extensive manual annotations, which restrict generalization across different speakers and emotional styles. To address this, we propose EmoSphere++, an emotion-controllable zero-shot TTS model capable of generating expressive speech with fine-grained control over emotional style and intensity—without requiring manual annotations. We introduce a novel emotion-adaptive spherical vector that effectively captures emotional style and intensity, along with a joint attribute style encoder that enhances generalization to both seen and unseen speakers. To further improve emotion transfer in zero-shot scenarios, we introduce an additional disentanglement method to enhance the style transfer performance for zero-shot scenarios. Through both objective and subjective evaluations, we demonstrate the benefits of the proposed model in emotion style and intensity modeling, as well as its effectiveness in enhancing emotional expressiveness across both seen and unseen speakers.},
  archive      = {J_TAFFC},
  author       = {Deok-Hyeon Cho and Hyung-Seok Oh and Seung-Bin Kim and Seong-Whan Lee},
  doi          = {10.1109/TAFFC.2025.3561267},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2365-2380},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {EmoSphere++: Emotion-controllable zero-shot text-to-speech via emotion-adaptive spherical vector},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual facial features transfer for facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 2352-2364. (<a href='https://doi.org/10.1109/TAFFC.2025.3561139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) models based on deep learning mostly rely on a supervised train-once-test-all approach. These approaches assume that a model trained on an in-the-wild facial expression dataset with one type of domain distribution will perform well on a test dataset with a domain distribution shift. However, facial images in real-world can be from different domain distributions from which the model has been trained. However, re-training models on only new domain distributions will severely affect the performance of the previous domain. Re-training on all previous and new data can improve overall performance but is computationally expansive. In this study, we oppose the train-once-test-all approach and propose a buffer-based continual learning approach to enhance the performance of multiple in-the-wild datasets. We propose a model that continually leverages attention to important facial features from the pre-trained model to improve performance in multiple datasets. We validated our model using split-in-the-wild datasets where the dataset is provided to the model in an incremental setting instead of all at once. Furthermore, to evaluate the model performance, we continually used three in-the-wild datasets representing different domains (Domain-FER). Extensive experiments on these datasets reveal that the proposed model achieves better results than other Continual FER models.},
  archive      = {J_TAFFC},
  author       = {Rahul Singh Maharjan and Lorenzo Bonicelli and Marta Romeo and Simone Calderara and Angelo Cangelosi and Rita Cucchiara},
  doi          = {10.1109/TAFFC.2025.3561139},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2352-2364},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Continual facial features transfer for facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affect and personality aided modeling of transcribed speech for depression severity estimation. <em>TAFFC</em>, <em>16</em>(3), 2334-2351. (<a href='https://doi.org/10.1109/TAFFC.2025.3560476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic detection of depression has gained significant attention due to its potential for early diagnosis and intervention. We propose a novel method that seamlessly integrates emotion, sentiment, and personality features as distinct yet interconnected components within a unified transformer-based architecture for depression severity estimation. Our key contribution lies in a joint cross-attention technique, which adeptly fuses the information gleaned from these different text representations, allowing for the nuanced interplay between them to be effectively captured. This technique not only enables the model to comprehend intricate interdependencies but also enhances the model’s ability to discern subtle contextual cues within the textual data. We undertake a comprehensive experimental environment to meticulously evaluate the discrete components comprising the architecture. The resultant findings gleaned from these experiments substantiate the self-contained efficacy of the envisioned architecture. Finally, we compare our method with state-of-the-art studies utilizing different combinations of audial, visual, and textual modalities. The final results demonstrate that our method achieves promising results in automatic depression severity estimation. This study underscores the potential of text-driven analysis in mental health assessment, opening avenues for more effective, accessible, and non-intrusive depression severity estimation tools.},
  archive      = {J_TAFFC},
  author       = {Kaan Gönç and Hamdi Dibeklioğlu},
  doi          = {10.1109/TAFFC.2025.3560476},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2334-2351},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Affect and personality aided modeling of transcribed speech for depression severity estimation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CorMulT: A semi-supervised modality correlation-aware multimodal transformer for sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 2321-2333. (<a href='https://doi.org/10.1109/TAFFC.2025.3559866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis is an active research area that combines multiple data modalities, e.g., text, image and audio, to analyze human emotions, and benefits a variety of applications. Existing multimodal sentiment analysis methods can be roughly classified as modality interaction-based methods, modality transformation-based methods and modality similarity-based methods. However, most of these methods highly rely on the strong correlations between modalities, and cannot fully uncover and utilize the correlations between modalities to enhance sentiment analysis. Therefore, these methods usually achieve unsatisfactory performance for identifying the sentiment of multimodal data with weak correlations. To address this issue, we proposed a two-stage semi-supervised model termed Correlation-aware Multimodal Transformer (CorMulT) which consists of pre-training stage and prediction stage. At the pre-training stage, a modality correlation contrastive learning module is designed to efficiently learn modality correlation coefficients between different modalities. At the prediction stage, the learned correlation coefficients are fused with modality representations to make the sentiment prediction. According to the experiments on the popular multimodal dataset CMU-MOSEI, CorMulT obviously surpasses the state-of-the-art multimodal sentiment analysis methods.},
  archive      = {J_TAFFC},
  author       = {Yangmin Li and Ruiqi Zhu and Wengen Li},
  doi          = {10.1109/TAFFC.2025.3559866},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2321-2333},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {CorMulT: A semi-supervised modality correlation-aware multimodal transformer for sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring emotion expression recognition in older adults interacting with a virtual coach. <em>TAFFC</em>, <em>16</em>(3), 2303-2320. (<a href='https://doi.org/10.1109/TAFFC.2025.3558141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The EMPATHIC project aimed to design an emotionally expressive virtual coach capable of engaging healthy seniors to improve well-being and promote independent aging. In particular, the system’s human sensing capabilities allow for the perception of emotional states to provide a personalized experience. This paper outlines the development of the emotion expression recognition module of the virtual coach, encompassing data collection, annotation design, and a first methodological approach, all tailored to the project requirements. With the latter, we investigate the role of various modalities, individually and combined, for discrete emotion expression recognition in this context: speech from audio, and facial expressions, gaze, and head dynamics from video. The collected corpus includes users from Spain, France, and Norway, and was annotated separately for the audio and video channels with distinct emotional labels, allowing for a performance comparison across cultures and label types. Results confirm the informative power of the modalities studied for the emotional categories considered, with multimodal methods generally outperforming others (around 68% accuracy with audio labels and 72-74% with video labels). The findings are expected to contribute to the limited literature on emotion recognition applied to older adults in conversational human-machine interaction, and guide the development of future systems.},
  archive      = {J_TAFFC},
  author       = {Cristina Palmero and Mikel deVelasco and Mohamed Amine Hmani and Aymen Mtibaa and Leila Ben Letaifa and Pau Buch-Cardona and Raquel Justo and Terry Amorese and Eduardo González-Fraile and Begoña Fernández-Ruanova and Jofre Tenorio-Laranga and Anna Torp Johansen and Micaela Rodrigues da Silva and Liva Jenny Martinussen and Maria Stylianou Korsnes and Gennaro Cordasco and Anna Esposito and Mounim A. El-Yacoubi and Dijana Petrovska-Delacrétaz and M. Inés Torres and Sergio Escalera},
  doi          = {10.1109/TAFFC.2025.3558141},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2303-2320},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Exploring emotion expression recognition in older adults interacting with a virtual coach},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hugging rain man: A novel facial action units dataset for analyzing atypical facial expressions in children with autism spectrum disorder. <em>TAFFC</em>, <em>16</em>(3), 2287-2302. (<a href='https://doi.org/10.1109/TAFFC.2025.3558914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Children with Autism Spectrum Disorder (ASD) often exhibit atypical facial expressions. However, the specific objective facial features that underlie this subjective perception remain unclear. In this paper, we introduce a novel dataset, Hugging Rain Man (HRM), which includes facial action units (AUs) manually annotated by FACS experts for both children with ASD and typically developing (TD) children. The dataset comprises a rich collection of posed and spontaneous facial expressions, totaling approximately 130,000 frames, along with 22 AUs, 10 Action Descriptors (ADs), and atypicality ratings. A statistical analysis of static images from the HRM reveals significant differences between the ASD and TD groups across multiple AUs and ADs when displaying the same emotional expressions, confirming that participants with ASD tend to demonstrate more irregular and diverse expression patterns. Subsequently, a temporal regression method was employed to analyze atypicality of dynamic sequences, thereby bridging the gap between subjective perception and objective facial characteristics. Furthermore, baseline results for AU detection are provided for future research reference. This work not only contributes to our understanding of the unique facial expression characteristics associated with ASD but also provides potential tools for ASD early screening.},
  archive      = {J_TAFFC},
  author       = {Yanfeng Ji and Shutong Wang and Ruyi Xu and Jingying Chen and Yuxuan Quan and Xinzhou Jiang and Zhengyu Deng and Junpeng Liu},
  doi          = {10.1109/TAFFC.2025.3558914},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2287-2302},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Hugging rain man: A novel facial action units dataset for analyzing atypical facial expressions in children with autism spectrum disorder},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDL: Dynamic direction learning for semi-supervised facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 2274-2286. (<a href='https://doi.org/10.1109/TAFFC.2025.3558884'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most semi-supervised facial expression recognition (FER) algorithms leverage pseudo-labeling to mine additional information from unlabeled samples. Despite its good performance, two critical issues persist: class imbalance and domain shift. The former is a typical challenge due to the significant variation in sample numbers across different FER classes, resulting in highly imbalanced pseudo labels in existing semi-supervised methods. For the latter, given that labeled and unlabeled data usually come from different sources, a considerable domain gap might exist, leading the model to generate low-quality pseudo labels. To tackle these issues, we introduce a novel semi-supervised FER algorithm called Dynamic Direction Learning (DDL), which consists of adaptive balance learning (ABL) and adaptive alignment learning (AAL). ABL allows a balanced training process by dynamically adjusting the constraints of self-training based on the performance of a balanced validation dataset. Moreover, AAL adaptively aligns the feature distribution of labeled and unlabeled data by minimizing their distance in feature space. Additionally, a role rotation mechanism (RRM) is proposed to avoid confirmation bias, which further improves self-training. Extensive experiments demonstrate that DDL achieves state-of-the-art performance on different FER datasets.},
  archive      = {J_TAFFC},
  author       = {Yaqi Li and Jing Jiang and Yuhang Zhang and Han Fang and Jiani Hu and Weihong Deng},
  doi          = {10.1109/TAFFC.2025.3558884},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2274-2286},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {DDL: Dynamic direction learning for semi-supervised facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards speaker-unknown emotion recognition in conversation via progressive contrastive deep supervision. <em>TAFFC</em>, <em>16</em>(3), 2261-2273. (<a href='https://doi.org/10.1109/TAFFC.2025.3558222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in conversation has attained increasing attention for perceiving user emotion in practical conversational applications. Conversational utterances spoken alternately by different speakers inspire most studies to leverage speaker information based on golden speaker labels. In this work, we challenge the existing paradigm of utilizing available speaker labels with a more realistic scenario, where the speaker identity of each utterance is unknown during inference. We propose Progressive Contrastive Deep Supervision for multimodal emotion recognition in conversation (PCDS), incorporating speaker diarization and emotion recognition into one unified framework. To facilitate joint task learning, we inject speaker and emotion bias into the network progressively via contrastive deep supervision, with the task-irrelevant contrast being the intermediate transition. To obtain explicit speaker dependency, we propose a speaker contrast and clustering module (SCC) to endow the capability of partitioning speakers into groups even when neither speaker label nor number of speakers is known as a priori. Experiments on two ERC benchmarks, including IEMOCAP and MELD demonstrate the effectiveness of the proposed method. We also show that progressive contrastive deep supervision helps reconcile the underlying tension between speaker diarization and emotion recognition.},
  archive      = {J_TAFFC},
  author       = {Siyuan Shen and Feng Liu and Hanyang Wang and Aimin Zhou},
  doi          = {10.1109/TAFFC.2025.3558222},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2261-2273},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Towards speaker-unknown emotion recognition in conversation via progressive contrastive deep supervision},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing emotion regulation in mental disorder treatment: An AIGC-based closed-loop music intervention system. <em>TAFFC</em>, <em>16</em>(3), 2245-2260. (<a href='https://doi.org/10.1109/TAFFC.2025.3557873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental disorders have increased rapidly and have emerged as a serious social health issue in the recent decade. Undoubtedly, the timely treatment of mental disorders is crucial. Emotion regulation has been proven to be an effective method for treating mental disorders. Music therapy as one of the methods that can achieve emotional regulation has gained increasing attention in the field of mental disorder treatment. However, traditional music therapy methods still face some unresolved issues, such as the lack of real-time capability and the inability to form closed-loop systems. With the advancement of artificial intelligence (AI), especially AI-generated content (AIGC), AI-based music therapy holds promise in addressing these issues. In this paper, an AIGC-based closed-loop music intervention system demonstration is proposed to regulate emotions for mental disorder treatment. This system demonstration consists of an emotion recognition model and a music generation model. The emotion recognition model can assess mental states, while the music generation model generates the corresponding emotional music for regulation. The system continuously performs recognition and regulation, thus forming a closed-loop process. In the experiment, we first conduct experiments on both the emotion recognition model and the music generation model to validate the accuracy of the recognition model and the music quality generated by the music generation models. In conclusion, we conducted comprehensive tests on the entire system to verify its feasibility and effectiveness.},
  archive      = {J_TAFFC},
  author       = {Lin Shen and Haojie Zhang and Cuiping Zhu and Ruobing Li and Kun Qian and Fuze Tian and Bin Hu and Björn W. Schuller and Yoshiharu Yamamoto},
  doi          = {10.1109/TAFFC.2025.3557873},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2245-2260},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Enhancing emotion regulation in mental disorder treatment: An AIGC-based closed-loop music intervention system},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). THERADIA WoZ: An ecological corpus for appraisal-based affect research in healthcare. <em>TAFFC</em>, <em>16</em>(3), 2233-2244. (<a href='https://doi.org/10.1109/TAFFC.2025.3557465'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present THERADIA WoZ, an ecological corpus designed for audiovisual research on affect in healthcare. Two groups of senior individuals, consisting of 52 healthy participants and 9 individuals with Mild Cognitive Impairment (MCI), performed Computerised Cognitive Training (CCT) exercises while receiving support from a virtual assistant, tele-operated by a human in the role of a Wizard-of-Oz (WoZ). The audiovisual expressions produced by the participants were fully transcribed, and partially annotated based on dimensions derived from recent appraisal theory models, including novelty, intrinsic pleasantness, goal conduciveness, and coping. Additionally, the annotations included 23 affective labels from the literature of achievement affects. We present the data collection, transcription, and annotation protocols, alongside a detailed analysis of the annotated dimensions and labels. Baseline methods and results for their automatic prediction are also presented. Results reveal that the dimensions of appraisal theory can be predicted, with the performance varying across different modalities. The corpus aims to serve as a valuable resource for researchers in affective computing, and is made available to both industry and academia.},
  archive      = {J_TAFFC},
  author       = {Hippolyte Fournier and Sina Alisamir and Safaa Azzakhnini and Isabella Zsoldos and Eléonore Trân and Gérard Bailly and Frédéric Elisei and Béatrice Bouchot and Brice Varini and Patrick Constant and Joan Fruitet and Franck Tarpin-Bernard and Solange Rossato and François Portet and Olivier Koenig and Hanna Chainay and Fabien Ringeval},
  doi          = {10.1109/TAFFC.2025.3557465},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2233-2244},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {THERADIA WoZ: An ecological corpus for appraisal-based affect research in healthcare},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical study of super-resolution on low-resolution micro-expression recognition. <em>TAFFC</em>, <em>16</em>(3), 2215-2232. (<a href='https://doi.org/10.1109/TAFFC.2025.3556442'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition (MER) in low-resolution (LR) scenarios presents an important and complex challenge, particularly for practical applications such as group MER in crowded environments. Despite considerable advancements in super-resolution (SR) techniques for enhancing the quality of LR images and videos, few study has focused on investigate SR for improving LR MER. The scarcity of investigation can be attributed to the inherent difficulty in capturing the subtle motions of micro-expressions, even in original-resolution MER samples, which becomes even more challenging in LR samples due to the loss of distinctive features. Furthermore, a lack of systematic benchmarking and thorough analysis of SR-assisted MER methods has been noted. This paper tackles these issues by conducting a series of benchmark experiments that integrate both SR and MER methods, guided by an in-depth literature survey. Specifically, we employ seven cutting-edge state-of-the-art (SOTA) MER techniques and evaluate their performance on samples generated from 22 SOTA SR techniques, thereby addressing the problem of SR in MER. Through our empirical study, we uncover the primary challenges associated with SR-assisted MER and identify avenues to tackle these challenges by leveraging recent advancements in both SR and MER methodologies. Our analysis provides insights for progressing toward more efficient SR-assisted MER.},
  archive      = {J_TAFFC},
  author       = {Ling Zhou and Mingpei Wang and Xiaohua Huang and Wenming Zheng and Qirong Mao and Guoying Zhao},
  doi          = {10.1109/TAFFC.2025.3556442},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2215-2232},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {An empirical study of super-resolution on low-resolution micro-expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view facial expressions analysis of autistic children in social play. <em>TAFFC</em>, <em>16</em>(3), 2200-2214. (<a href='https://doi.org/10.1109/TAFFC.2025.3557458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Atypical facial expressions during interaction are among the early symptoms of autism spectrum disorder (ASD) and are included in standard diagnostic assessments. However, current methods rely on subjective human judgments, introducing bias and limiting objectivity. This paper proposes an automated framework for objective and quantitative assessment of autistic children’s facial expressions during social play. Initially, we utilize four synchronized cameras to record interactions between ASD children and teachers during structured activities dominated by the teacher. To address challenges posed by head movements and occluded faces, we introduce a multi-view facial expression recognition strategy. Its effectiveness is demonstrated by experiments in real-world applications. To quantify the patterns of affect status and the dynamic complexity of facial expressions, we use the temporally accumulated distribution of the basic facial expressions and the multi-dimensional multi-scale entropy of the facial expression sequence. Analysis of these features revealed significant differences between ASD and TD groups. Experimental results, derived from our quantified features, confirm conclusions drawn from previous research and experiential observations. With these facial expression features, ASD and typically developing (TD) children are accurately classified (accuracy 92.1%, precision 94.4%, 89.5% sensitivity, 94.7% specificity) in empirical experiments, suggesting the potential of our framework for improved ASD assessment.},
  archive      = {J_TAFFC},
  author       = {Jiabei Zeng and Yujian Yuan and Lu Qu and Fei Chang and Xuran Sun and Jinqiuyu Gong and Xuling Han and Min Liu and Hang Zhao and Qiaoyun Liu and Shiguang Shan and Xilin Chen},
  doi          = {10.1109/TAFFC.2025.3557458},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2200-2214},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multi-view facial expressions analysis of autistic children in social play},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCRVT: Multi-hierarchical cross-reconstruction networks with versatile transformer for speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2189-2199. (<a href='https://doi.org/10.1109/TAFFC.2025.3557153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two challenges for Speech Emotion Recognition (SER) are efficiently capturing features to explore speech-emotion correlations and reducing the SER model redundancy while increasing performance. To address these challenges, we propose Multi-Hierarchical Cross-Reconstruction Networks with Versatile Transformer (MCRVT), including a Spectrogram-Based SER Model with Versatile Attention (SSER-VA), a pre-trained model (WavLM), and Multi-Hierarchical Feature Fusion with Cross Attention (MHFF-CA). Specifically, SSER-VA takes the log-Mel spectrogram as the input, and we propose a versatile attention module to reduce redundancy and increase efficiency. MHFF-CA is employed to refine intermediate features from WavLM and SSER-VA to improve competitiveness in various tasks, where Contrastive Reconstruction Networks (CRN) and Cross Attention-based Feature Fusion (CAF) are utilized to reconstruct and fuse features, respectively. Effectiveness experiments on IEMOCAP and MELD datasets illustrate MCRVT's superior performance compared to SOTA methods. Exploratory experiments on D-vlog, BioCAS2024, and IEMOCAP (speaker-independent setting) datasets demonstrate the competitiveness of our method in various speech classification tasks.},
  archive      = {J_TAFFC},
  author       = {Xin-Heng Li and Zhen-Tao Liu and Yu-Jie Zou and Jinhua She and Kaoru Hirota},
  doi          = {10.1109/TAFFC.2025.3557153},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2189-2199},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MCRVT: Multi-hierarchical cross-reconstruction networks with versatile transformer for speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotion recognition by learning the manifold of fused multiscale information of EEG signals. <em>TAFFC</em>, <em>16</em>(3), 2172-2188. (<a href='https://doi.org/10.1109/TAFFC.2025.3555226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has consistently indicated that the fusion of electroencephalography (EEG) features from multiple modalities can integrate cognitive state expressions across diverse dimensions, resulting in a substantial increase in emotion recognition accuracy. However, redundant information within the fused multimodal features could lead to the curse of dimensionality and overfitting of the learning model. In this work, we propose a multiscale EEG feature fusion and representation strategy for EEG emotion recognition named manifold of multiscale information fusion (MMIF), in which the optimal manifold of the multiscale fusion of local and global brain activation patterns can be automatically learned to realize an efficient representation of emotional EEG signals. To evaluate the performance, in this work, both off- and online EEG emotion recognition experiments were conducted, and the experimental results consistently verified the effectiveness and feasibility of the MMIF applied in real-time emotion decoding systems. Furthermore, the analytical experiments confirmed the discriminative capabilities and cognitive interpretability of the MMIF. In summary, the proposed MMIF model may provide an efficient avenue for exploring representations and enhancing the discrimination of multimodal fusion features, which may also provide a promising solution for designing online affective braincomputer interaction systems.},
  archive      = {J_TAFFC},
  author       = {Cunbo Li and Shuhan Zhang and Yufeng Mu and Lei Yang and Yueheng Peng and Fali Li and Yangsong Zhang and Zhen Liang and Zehong Cao and Feng Wan and Dezhong Yao and Peiyang Li and Peng Xu},
  doi          = {10.1109/TAFFC.2025.3555226},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2172-2188},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Emotion recognition by learning the manifold of fused multiscale information of EEG signals},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing cross-dataset EEG emotion recognition: A novel approach with emotional EEG style transfer network. <em>TAFFC</em>, <em>16</em>(3), 2157-2171. (<a href='https://doi.org/10.1109/TAFFC.2025.3555439'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG)-based emotion recognition has achieved remarkable success in both subject-dependent and subject-independent scenarios. However, overcoming the challenges associated with reduced performance in EEG emotion recognition across devices, time, space, and subjects (i.e., cross-dataset) remains a significant obstacle for affective brain-computer interfaces (aBCIs). The key issue lies in the distributional mismatch between source and target domain EEG signals. To tackle the significant inter-domain differences in cross-dataset EEG emotion recognition, this paper introduces an innovative framework termed the Emotional EEG Style Transfer Network (E$^{2}$STN), which aims to effectively capture the emotional content information from the source domain and the style features from the target domain, facilitating the reconstruction of stylized emotion EEG representations. These stylized EEG representations significantly enhance the discriminative prediction performance in cross-dataset EEG emotion recognition. Specifically, E$^{2}$STN consists of three key modules: a Transfer Module for domain style transfer, a Transfer Evaluation Module for evaluating transfer quality, and a Discriminative Module for making discriminative predictions. Extensive experiments demonstrate that E$^{2}$STN achieves the state-of-the-art performance in cross-dataset emotion EEG recognition. To the best of our knowledge, this is the first work to explicitly address cross-dataset emotion EEG recognition. The experimental results provide a valuable benchmark for future research in this area.},
  archive      = {J_TAFFC},
  author       = {Yijin Zhou and Fu Li and Yang Li and Youshuo Ji and Lijian Zhang and Yuanfang Chen and Huaning Wang},
  doi          = {10.1109/TAFFC.2025.3555439},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2157-2171},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Enhancing cross-dataset EEG emotion recognition: A novel approach with emotional EEG style transfer network},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SeeNet: A soft emotion expert and data augmentation method to enhance speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2142-2156. (<a href='https://doi.org/10.1109/TAFFC.2025.3555406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech emotion recognition (SER) systems are designed to enable machines to recognize emotional states in human speech during human-computer interactions, enhancing the interactive experience. While considerable progress has been achieved in this field recently, SER systems still encounter challenges related to performance and robustness, primarily stemming from the limited labeled data. To this end, we propose a novel multitask learning framework to learn a distinctive and robust emotional representation by our “Soft Emotion Expert Network (SeeNet)”. SeeNet consists of three components: a pretrained model, an auxiliary task soft emotion expert (SEE) module and an energy-based mixup (EBM) data augmentation module. The pretrained model and EBM module are employed to mitigate the challenges arising from limited labeled data, thereby enhancing the model performance and bolstering robustness. The SEE module as an auxiliary task is designed to assist the main task of SER by enhancing the distinction between samples exhibiting high similarity across categories. This aims to further improve the performance and robustness of the system. Comprehensive experiments on three different settings and multiple datasets are conducted to evaluate the performance and robustness of our proposed method. The experimental results demonstrate that SeeNet surpasses the state-of-the-art (SOTA) methods in both performance and robustness.},
  archive      = {J_TAFFC},
  author       = {Qifei Li and Yingming Gao and Yuhua Wen and Ziping Zhao and Ya Li and Björn W. Schuller},
  doi          = {10.1109/TAFFC.2025.3555406},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2142-2156},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SeeNet: A soft emotion expert and data augmentation method to enhance speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UA-DAAN: An uncertainty-aware dynamic adversarial adaptation network for EEG-based depression recognition. <em>TAFFC</em>, <em>16</em>(3), 2130-2141. (<a href='https://doi.org/10.1109/TAFFC.2025.3555433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is a common mental disorder characterized by symptoms such as a depressed mood, loss of interest, low self-esteem, and anxiety. Clinical diagnosis of depression often faces challenges due to the lack of objective indicators and the subjectivity of psychiatrists and patients. In recent years, with the rapid advancement of artificial intelligence technology, automatic depression diagnosis methods based on physiological signals have emerged. These methods have helped enhance the accuracy and objectivity of diagnosis. One such physiological signal used is the electroencephalogram (EEG), which is an easily obtainable, noninvasive, and cost-effective electrical signal recording the activity of neurons in the cerebral cortex. EEG is commonly used to observe brain states and diagnose mental illnesses. However, due to the high individual variability of EEG signals, existing methods often do not adequately address the issue of removing individual variability. Additionally, achieving high model reliability in disease recognition is crucial, but existing methods typically lack uncertainty estimation of recognition results. To tackle these challenges, this study introduces an uncertainty-aware dynamic adversarial adaptation network (UA-DAAN). This network incorporates adversarial learning concepts to address the significant individual variability in EEG data. It utilizes uncertainty-aware optimization of the dynamic domain adversarial process in a Bayesian neural network (BNN) to enhance the transferability of class-related features between source and target domains, improving the overall model's robustness, accuracy, and reliability. The experimental results strongly prove the effectiveness of this model in depression recognition.},
  archive      = {J_TAFFC},
  author       = {Jian Shen and Lechun You and Yu Ma and Zeguang Zhao and Huajian Liang and Yanan Zhang and Bin Hu},
  doi          = {10.1109/TAFFC.2025.3555433},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2130-2141},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {UA-DAAN: An uncertainty-aware dynamic adversarial adaptation network for EEG-based depression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An EEG method to identify image preference with an Explicit/Implicit task brain-computer interface. <em>TAFFC</em>, <em>16</em>(3), 2116-2129. (<a href='https://doi.org/10.1109/TAFFC.2025.3554534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately determining an individual's preference for images remains a major challenge in the field of emotional research. This study proposes a novel paradigm for identifying individual image preferences using electroencephalography (EEG) signals and brain-computer interface (BCI). The paradigm involves both explicit and implicit tasks, where participants perform a typical event-related potential-based brain-computer interface(ERP-BCI) operation and their subjective image preferences are identified, respectively. Two experiments with a total of 27 participants demonstrate that event-related potential (ERP) signals during explicit BCI tasks are significantly influenced by target image preferences, enabling high-accuracy image preference recognition. Online experiments selecting positive and negative preference images from a candidate pool show top-1 accuracy approaching 100% and top-3 accuracy exceeding 90%. These results indicate the effectiveness of the proposed EEG-based image preference recognition paradigm, laying the groundwork for preference analysis applications.},
  archive      = {J_TAFFC},
  author       = {Yulei Li and Shuyi Li and Hongzhi Qi},
  doi          = {10.1109/TAFFC.2025.3554534},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2116-2129},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {An EEG method to identify image preference with an Explicit/Implicit task brain-computer interface},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal cross-subject emotion feature alignment and recognition with EEG and eye movements. <em>TAFFC</em>, <em>16</em>(3), 2102-2115. (<a href='https://doi.org/10.1109/TAFFC.2025.3554399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal emotion recognition has attracted much attention in human-computer interaction, because it provides complementary information for the recognition model. However, the distribution drift among subjects and the heterogeneity of different modalities pose challenges to multi-modal emotion recognition, thereby limiting its practical application. Most of the current multi-modal emotion recognition methods are difficult to suppress above uncertainties in fusion. In this paper, we propose a cross-subject multi-modal emotion recognition framework, which jointly learns subject-independent representation and common feature between EEG and eye movements. First, we design the dynamic adversarial domain adaptation for cross-subject distribution alignment, dynamically selecting source domains in training. Second, we simultaneously capture intra-modal and inter-modal emotion-related features by both self-attention and cross-attention mechanisms, thus obtaining the robust and complementary representation of emotional information. Then, two contrastive loss functions are imposed on above network to further reduce inter-modal heterogeneity, and mine higher-order semantic similarity between synchronously collected multi-modal data. Finally, we used the output of the softmax layer as the predicted value. The experimental results on several multi-modal emotion datasets with EEG and eye movements demonstrate that our method is significantly superior to the state-of-the-art emotion recognition approaches.},
  archive      = {J_TAFFC},
  author       = {Qi Zhu and Ting Zhu and Lunke Fei and Chuhang Zheng and Wei Shao and David Zhang and Daoqiang Zhang},
  doi          = {10.1109/TAFFC.2025.3554399},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2102-2115},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multi-modal cross-subject emotion feature alignment and recognition with EEG and eye movements},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From translation to generative LLMs: Classification of code-mixed affective tasks. <em>TAFFC</em>, <em>16</em>(3), 2090-2101. (<a href='https://doi.org/10.1109/TAFFC.2025.3553399'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Code-mixed (CM) discourse combines multiple languages in a single text. It is commonly used in informal discourse in countries with several official languages, but also in many other countries in combination with English or neighboring languages. With the recent rise of large transformer language models dominating NLP tasks, we explored their effectiveness in CM contexts. We developed four new bilingual pre-trained masked language models for Hinglish and English-Slovene languages, tailored to handle informal language. We then evaluated monolingual, bilingual, few-lingual, massively multilingual, and larger generative models across multiple languages using two affective tasks involving CM texts: sentiment analysis and offensive speech prediction in social media posts. We compared these models with two translation baselines, one obtained with a neural machine translation tool and the other produced by large generative models. The experiments conducted in five languages: French, Hindi, Russian, Slovene, and Tamil, reveal that fine-tuned bilingual models and multilingual models designed for social media texts outperform others, with massively multilingual and monolingual models following, while larger generative models lag. For the affective tasks studied, models generally performed better on CM data than on non-CM data. The monolingual models with translated datasets rarely compete with multilingual models trained on CM datasets.},
  archive      = {J_TAFFC},
  author       = {Anjali Yadav and Tanya Garg and Matej Klemen and Matej Ulčar and Basant Agarwal and M. Robnik-Šikonja},
  doi          = {10.1109/TAFFC.2025.3553399},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2090-2101},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {From translation to generative LLMs: Classification of code-mixed affective tasks},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Injecting multimodal information into pre-trained language model for multimodal sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 2074-2089. (<a href='https://doi.org/10.1109/TAFFC.2025.3553149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing availability of computational and data resources, numerous powerful pre-trained language models (PLMs) have emerged for natural language processing tasks. However, how to inject nonverbal modalities into PLMs to handle multimodal information remains a practical problem. In this paper, we explore the application of PLM on multimodal sentiment analysis from a different perspective. Unlike many recent methods that develop multimodal fusion layers that are sequential to attention layers, we investigate the effectiveness of cross-modal additive attention that is parallel to attention layers, which takes the language modality as dominant modality. Moreover, we devise a gating mechanism to control the flow of nonverbal information by estimating its discriminative level. In this way, we can prevent noisy multimodal information from damaging the performance of pre-trained language model. In our framework, nonverbal modalities serve as auxiliary roles to provide the model with additional information and improve the understanding of multimodal human language. Additionally, cross-modal margin and matching losses are proposed to align the distributions of various modalities and simultaneously retain modality-specific information, which to some extent address the shortcoming of contrastive learning loss. Comprehensive experiments show that our approach surpasses existing state-of-the-art methods on multimodal sentiment analysis and emotion recognition tasks.},
  archive      = {J_TAFFC},
  author       = {Sijie Mai and Ying Zeng and Aolin Xiong and Haifeng Hu},
  doi          = {10.1109/TAFFC.2025.3553149},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2074-2089},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Injecting multimodal information into pre-trained language model for multimodal sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MemRank: Memory-augmented similarity ranking for video-based depression severity estimation. <em>TAFFC</em>, <em>16</em>(3), 2062-2073. (<a href='https://doi.org/10.1109/TAFFC.2025.3553090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have shown substantial promise in visual depression severity estimation. Nonetheless, their effectiveness is limited by the scarce availability of labeled depression data, potentially leading to overfitting during representation learning. One feasible approach to address this issue is to incorporate, in the training objective, regularization that considers the unique characteristics of depression data. Typical regularization includes the similarity ranking through ordered consistency between visual features and their target scores. However, previous ranking methods are limited to using only samples within a mini-batch, resulting in a decreased regularization effect in depression representation learning. To address this limitation, we propose MemRank, a global similarity ranking method that operates not only on mini-batch samples but also on a well-designed feature memory, which stores smoothed and dynamically updated feature prototypes at diverse levels of depression during training. Furthermore, we show that incorporating the feature memory in the regression loss enhances the stability of training a deep regressor, leading to improved depression predictions. Empirically and analytically, we show that our MemRank outperforms alternative ranking methods and achieves state-of-the-art results on two benchmark datasets.},
  archive      = {J_TAFFC},
  author       = {Yonghong Li and Zeqiang Wei and Guodong Guo and Xiuzhuang Zhou},
  doi          = {10.1109/TAFFC.2025.3553090},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2062-2073},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MemRank: Memory-augmented similarity ranking for video-based depression severity estimation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal depression assessment framework integrating personality and gait for older adults with medical conditions. <em>TAFFC</em>, <em>16</em>(3), 2048-2061. (<a href='https://doi.org/10.1109/TAFFC.2025.3552835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elderly individuals often suffer from underlying medical conditions, resulting in a significant decline in quality of life and a heightened susceptibility to depression. Presently, AI screening tools based on behavioral indicators offer an objective and effective approach to diagnosing depression. However, current AI depression screening tools are primarily tailored to adolescents and adults, exhibiting shortcomings in their applicability and accuracy for elderly individuals with underlying medical conditions. To address the above issues, first, this paper constructs a depression dataset for elderly people with underlying diseases by using semi-structured interviews. Second, based on cognitive science insights, it is recognized that personality factors significantly influence behavioral expressions and also determine the attitudes of elderly individuals toward current life circumstances/health issues. Therefore, besides annotating depression severity, the Big Five-10 personality scale was utilized to annotate participant personalities. Finally, a late fusion-based multi-task learning framework was proposed, and the effects of introducing gait information and personality annotation on the performance of depression assessment were investigated. The experimental findings affirm the importance of integrating gait information and personality assessment in improving depression detection effectiveness. This study provides valuable foundational resources, as well as beneficial references and insights, for the research on depression in the elderly.},
  archive      = {J_TAFFC},
  author       = {Yuliang Zhao and Huawei Zhang and Jian Li and Siyang Song and Chao Lian and Yinghao Liu and Yulin Wang and Changzeng Fu},
  doi          = {10.1109/TAFFC.2025.3552835},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2048-2061},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multimodal depression assessment framework integrating personality and gait for older adults with medical conditions},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying stable EEG patterns in manipulation task for negative emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 2033-2047. (<a href='https://doi.org/10.1109/TAFFC.2025.3551330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Negative emotion recognition during manipulation task plays crucial role in human-machine interaction, where diverse cognitive variables coexist and influence each other. However, traditional emotion experiments often overemphasize emotion induction while overlooking other practical cognitive tasks, which leads participants to suffer from simplistic emotional experiences and ultimately compromises the real-world applicability of the emotional data collected. To incorporate critical cognitive variables into emotion elicitation, we utilize joystick-based real-time emotion annotation to encourage subjects to continuously feel emotional intensity, to advisedly decide when to manipulate the joystick, and to physically operate it. Consequently, at least two essential cognitive variables—decision-making and action—are integrated into emotion perception. Following this, we develop a novel negative emotion dataset called CRED, which includes a variety of physiological data, particularly Electroencephalograph (EEG). To assess the stability of emotional EEG patterns, we employ strict statistical analysis and a dual-branch transformer (DBT) with the gradient-based attribution method on the proposed CRED. Additionally, two well-known public datasets (SEED and SEED-V) are used to verify the DBT. Compared to traditional methods, DBT improves classification accuracy by approximately 5% on CRED and by around 2% on the public datasets. The experimental results indicate that the occipital lobe plays a crucial role in the discrimination of negative emotions; the critical frequency bands vary between the five emotions in the CRED. Specifically, the low-delta rhythm is associated with anger, while fear is influenced by both theta and alpha rhythms; disgust is found to be significant in the theta rhythm; and for neutral emotions, both low-delta and alpha rhythms are identified as crucial. In summary, our findings demonstrate the existence of stable emotional EEG patterns when additional cognitive variables are involved.},
  archive      = {J_TAFFC},
  author       = {Yu Pei and Shaokai Zhao and Liang Xie and Zhiguo Luo and Dongdong Zhou and Chuang Ma and Ye Yan and Erwei Yin},
  doi          = {10.1109/TAFFC.2025.3551330},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2033-2047},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Identifying stable EEG patterns in manipulation task for negative emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PeTracker: Poincaré-based dual-strategy emotion tracker for emotion recognition in conversation. <em>TAFFC</em>, <em>16</em>(3), 2020-2032. (<a href='https://doi.org/10.1109/TAFFC.2025.3549926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing use of interactive applications, the importance of Emotion recognition in conversation (ERC) is growing. Current research in the ERC domain mainly emphasizes the extraction of contextual information. However, challenges arise due to multi-turn conversation scenarios and the natural transformation of emotions, particularly in identifying subtle emotion transfers. Moreover, emotions exhibit nonlinear characteristics in semantic spaces, leading to potential confusion when discerning similar semantic emotions in the Euclidean semantic space. To address these issues, this study proposes a Poincaré-based dual-strategy emotion tracker for emotion recognition in conversation (PeTracker), which introduces the hyperbolic space representation in the ERC domain. Based on the spatial properties of the hyperbolic space representation to capture the nonlinear relationships among features, PeTracker encompasses two learning strategies. Poincaré emotional geometry curriculum learning (PGCL) and Poincaré emotional stratification contrastive learning (PSCL). In PGCL, the similarity of emotion labels is effectively discerned using the Poincaré distance, quantifying emotion transfer distances and facilitating the identification of subtle emotion transfers in utterance. In PSCL, PeTracker extracts and adapts multi-level features, mapping them to the Poincaré ball space to build emotion prototype-based contrastive learning. This process enhances the model’s ability to distinguish between similar emotion labels. while alleviating potential label confusion issues. Experimental results on three general datasets demonstrate that PeTracker achieves optimal or near-optimal performance. Furthermore, the study investigates the role and impact of the Poincaré ball in differentiating similar emotions.},
  archive      = {J_TAFFC},
  author       = {YuKun Cao and Luobin Huang and Yijia Tang},
  doi          = {10.1109/TAFFC.2025.3549926},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2020-2032},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {PeTracker: Poincaré-based dual-strategy emotion tracker for emotion recognition in conversation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReSup: Reliable label noise suppression for facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 2006-2019. (<a href='https://doi.org/10.1109/TAFFC.2025.3549017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the ambiguous and subjective property of the facial expression, the label noise is widely existing in the FER dataset. For this problem, in the training phase, current methods often directly predict whether the label is noised or not, aiming to reduce the contribution of the noised data. However, we argue that this kind of method suffers from the low reliability of such noise data decision operation. It makes that some mistakenly abounded clean data are not utilized sufficiently and some mistakenly kept noised data disturbing the model learning. In this paper, we propose a more reliable noise-label suppression method called ReSup. First, instead of directly predicting noised or not, ReSup makes the noise data decision by modeling the distribution of noise and clean labels simultaneously according to the disagreement between the prediction and the target. Specifically, to achieve optimal distribution modeling, ReSup models the similarity distribution of all samples. To further enhance the reliability of our noise decision results, ReSup uses two networks to jointly achieve noise suppression. Specifically, ReSup utilize the property that two networks are less likely to make the same mistakes, making two networks swap decisions and tending to trust decisions with high agreement. Extensive experiments on popular datasets shows the effectiveness of ReSup.},
  archive      = {J_TAFFC},
  author       = {Xiang Zhang and Yan Lu and Huan Yan and Jinyang Huang and Yu Gu and Yusheng Ji and Zhi Liu and Bin Liu},
  doi          = {10.1109/TAFFC.2025.3549017},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {2006-2019},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {ReSup: Reliable label noise suppression for facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomic modulations to cardiac dynamics in response to affective touch: Differences between social touch and self-touch. <em>TAFFC</em>, <em>16</em>(3), 1996-2005. (<a href='https://doi.org/10.1109/TAFFC.2025.3548778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The autonomic nervous system plays a vital role in self-regulation and responding to environmental demands. Autonomic dynamics have been hypothesized to be involved in perceptual awareness and establishment and maintenance of the sense of a bodily self at a neural level. We hypothesized that the autonomic activity measured from cardiac dynamics could differentiate between social touch and self-touch. In our study, we used a newly developed method to analyze the temporal dynamics of cardiac sympathetic and parasympathetic activities during an ecologically valid affective touch experiment. We revealed that different types of touch conditions—social-touch, self-touch, and a control object-touch—resulted in a decrease in sympathetic activity. This decrease was more pronounced during social touch, as compared to the other conditions. Moreover, we quantified an increase in parasympathetic activity specifically during social touch, further distinguishing it from self-touch. Importantly, by combining the sympathetic and parasympathetic indices, we successfully differentiated social touch from the other experimental conditions, indicating that social touch exhibited the most substantial changes in cardiac autonomic indices. These findings may have important clinical implications as they provide insights into the neurophysiology of touch, relevant for aberrant affective touch processing in specific psychiatric disorders and for the comprehension of nociceptive touch.},
  archive      = {J_TAFFC},
  author       = {Diego Candia-Rivera and Rebecca Boehme and Paula C. Salamone},
  doi          = {10.1109/TAFFC.2025.3548778},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1996-2005},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Autonomic modulations to cardiac dynamics in response to affective touch: Differences between social touch and self-touch},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiview attention fusion for explainable body language behavior recognition. <em>TAFFC</em>, <em>16</em>(3), 1984-1995. (<a href='https://doi.org/10.1109/TAFFC.2025.3548781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Body language behavior , including gestures and fine-grained movements not only reflects human emotions, but also serves as a versatile cue for enhancing emotional intelligence and creating responsive technologies. In this work, we explore the efficacy of multiview-multimodal cues for explainable prediction of bodily behavior. This paper proposes an attention fusion method that combines features extracted from (1) multiview videos termed “RGB”, (2) their multiview Discrete Cosine Transform representations termed “DCT” and (3) three stream skeleton features termed “Skeleton”, via a transformer-based approach. We evaluate our approach on the diverse BBSI (Balazia et al., 2022) and Drive&Act (Martin et al., 2019) datasets. Empirical results confirm that the RGB, DCT and Skeleton features enable discovery of multiple class-specific behaviors resulting in explainable predictions. Our key findings are: (a) Multimodal approaches outperform unimodal counterparts in categorizing bodily behavioral classes; (b) Efficient class predictions and plausible explanations are achieved with both unimodal and multimodal approaches; and (c) Empirical results confirm the superiority of our approach compared to state-of-the-art methods on both datasets.},
  archive      = {J_TAFFC},
  author       = {Surbhi Madan and Rishabh Jain and Ramanathan Subramanian and Abhinav Dhall},
  doi          = {10.1109/TAFFC.2025.3548781},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1984-1995},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multiview attention fusion for explainable body language behavior recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a robust group-level emotion recognition via uncertainty-aware learning. <em>TAFFC</em>, <em>16</em>(3), 1970-1983. (<a href='https://doi.org/10.1109/TAFFC.2025.3547994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group-level emotion recognition (GER) is an inseparable part of human behavior analysis, aiming to recognize an overall emotion in a multi-person scene. However, the existing methods are devoted to combing diverse emotion cues while ignoring the inherent uncertainties under unconstrained environments, such as congestion and occlusion occurring within a group. Additionally, since only group-level labels are available, inconsistent emotion predictions among individuals in one group can confuse the network. In this paper, we propose an uncertainty-aware learning (UAL) method to extract more robust representations for GER. By explicitly modeling the uncertainty, we adopt stochastic embedding sourced from a Gaussian distribution instead of deterministic point embedding. It helps capture the probabilities of emotions and facilitates diverse inferences. Additionally, we adaptively assign uncertainty-sensitive scores as the fusion weights for individuals’ faces within a group. Moreover, we developed an image enhancement module to evaluate and filter samples, strengthening the model’s data-level robustness against uncertainties. The overall three-branch model, encompassing face, object, and scene components, is guided by a proportional-weighted fusion strategy and integrates the proposed uncertainty-aware method to produce the final group-level output. Experimental results demonstrate the effectiveness and generalization ability of our method across three widely used databases.},
  archive      = {J_TAFFC},
  author       = {Qing Zhu and Qirong Mao and Jialin Zhang and Xiaohua Huang and Wenming Zheng},
  doi          = {10.1109/TAFFC.2025.3547994},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1970-1983},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Towards a robust group-level emotion recognition via uncertainty-aware learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Micro-expression key frame inference. <em>TAFFC</em>, <em>16</em>(3), 1955-1969. (<a href='https://doi.org/10.1109/TAFFC.2025.3548284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions (MEs) are brief, involuntary facial movements critical for detecting lies, drawing growing interest in psychology and computer science. However, annotating ME can burden human coders with excessive time commitment and overwhelming information that compromises coding reliability and efficiency. Such difficulties in data annotation also led to the small sample size problem and hindered the development of ME analysis. Specifically, our psychological research highlights the complexities involved in human annotation of key frames. To facilitate the annotating process of ME, we proposed the Micro-Expression Key Frame Inference (ME-KFI) problem, aiming to identify MEs’ temporal locations from a single frame, reducing manual annotation effort. We propose a Micro-Expression Contrastive Identification Annotation (MECIA) method as a solution to ME-KFI, including three modules: a contrastive module, an identification module, and an annotation module, corresponding to the three steps of manual annotation. The network’s outputs infer the key frame of ME clips. MECIA demonstrates superior performance over random baselines on SAMM and CAS(ME)$^{2}$ databases and maintains comparable recognition accuracy with ground-truth clips.},
  archive      = {J_TAFFC},
  author       = {Su-Jing Wang and Yu-Han Miao and Jingting Li and Ling Zhou and Zizhao Dong and Mengyi Sun and Xiaolan Fu},
  doi          = {10.1109/TAFFC.2025.3548284},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1955-1969},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Micro-expression key frame inference},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stress severity detection in college students using emotional pulse signals and deep learning. <em>TAFFC</em>, <em>16</em>(3), 1942-1954. (<a href='https://doi.org/10.1109/TAFFC.2025.3547753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {College students face increasing stress from difficulties with studies, employment, and social interactions, which, if left unaddressed, may lead to depression and physical illnesses. Currently, the detection of stress severity relies on self-assessment scales, while machine learning or deep learning-based approaches primarily focus on classification. This study proposes an approach using pulse signals containing emotional cues and deep learning to automatically detect the severity of stress in college students. First, pulse signals of 177 college students were collected using photoplethysmography (PPG) during they watched five virtual reality (VR) emotional scenes, including calm, sadness, happiness, fear, and tension. Pulse rate variability (PRV) and discrete PPG (dPPG) were extracted from these signals as input for detecting stress severity. Then, the proposed stress detection framework, 1DCNN-BiLSTM + Cross-Attention + XGBoost, was employed to detect stress severity, incorporating an emotional Cross-Attention mechanism. The impact of induced emotions on stress severity detection performance was examined. The results indicated that stress severity detection in emotional scenes outperformed in calm. Furthermore, the detection performance that integrates multiple emotions surpassed single emotions. The fusion of PRV and dPPG signals yielded the best detection performance. This study provides an end-to-end automated approach for detecting stress severity in college students.},
  archive      = {J_TAFFC},
  author       = {Mi Li and Junzhe Li and Yanbo Chen and Bin Hu},
  doi          = {10.1109/TAFFC.2025.3547753},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1942-1954},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Stress severity detection in college students using emotional pulse signals and deep learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing correctness, fairness, and robustness of speech emotion recognition models. <em>TAFFC</em>, <em>16</em>(3), 1929-1941. (<a href='https://doi.org/10.1109/TAFFC.2025.3547218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning models for speech emotion recognition (SER) can be trained for different tasks and are usually evaluated based on a few available datasets per task. Tasks could include arousal, valence, dominance, emotional categories, or tone of voice. Those models are mainly evaluated in terms of correlation or recall, and always show some errors in their predictions. The errors manifest themselves in model behaviour, which can be very different along different dimensions even if the same recall or correlation is achieved by the model. This paper introduces a testing framework to investigate behaviour of speech emotion recognition models, by requiring different metrics to reach a certain threshold in order to pass a test. The test metrics can be grouped in terms of correctness, fairness, and robustness. It also provides a method for automatically specifying test thresholds for fairness tests, based on the datasets used, and recommendations on how to select the remaining test thresholds. We evaluated a xLSTM-based and nine transformer-based acoustic foundation models against a convolutional baseline model, testing their performance on arousal, valence, dominance, and emotional category classification. The test results highlight, that models with high correlation or recall might rely on shortcuts – such as text sentiment –, and differ in terms of fairness.},
  archive      = {J_TAFFC},
  author       = {Anna Derington and Hagen Wierstorf and Ali Özkil and Florian Eyben and Felix Burkhardt and Björn W. Schuller},
  doi          = {10.1109/TAFFC.2025.3547218},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1929-1941},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Testing correctness, fairness, and robustness of speech emotion recognition models},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond overfitting: Doubly adaptive dropout for generalizable AU detection. <em>TAFFC</em>, <em>16</em>(3), 1916-1928. (<a href='https://doi.org/10.1109/TAFFC.2025.3545915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Action Units (AUs) are essential for conveying psychological states and emotional expressions. While automatic AU detection systems leveraging deep learning have progressed, they often overfit to specific datasets and individual features, limiting their cross-domain applicability. To overcome these limitations, we propose a doubly adaptive dropout approach for cross-domain AU detection, which enhances the robustness of convolutional feature maps and spatial tokens against domain shifts. This approach includes a Channel Drop Unit (CD-Unit) and a Token Drop Unit (TD-Unit), which work together to reduce domain-specific noise at both the channel and token levels. The CD-Unit preserves domain-agnostic local patterns in feature maps, while the TD-Unit helps the model identify AU relationships generalizable across domains. An auxiliary domain classifier, integrated at each layer, guides the selective omission of domain-sensitive features. To prevent excessive feature dropout, a progressive training strategy is used, allowing for selective exclusion of sensitive features at any model layer. Our method consistently outperforms existing techniques in cross-domain AU detection, as demonstrated by extensive experimental evaluations. Visualizations of attention maps also highlight clear and meaningful patterns related to both individual and combined AUs, further validating the approach's effectiveness.},
  archive      = {J_TAFFC},
  author       = {Yong Li and Yi Ren and Xuesong Niu and Yi Ding and Xiu-Shen Wei and Cuntai Guan},
  doi          = {10.1109/TAFFC.2025.3545915},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1916-1928},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Beyond overfitting: Doubly adaptive dropout for generalizable AU detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the relationship between stress-physiology and pain in the daily life of patients with chronic widespread pain. <em>TAFFC</em>, <em>16</em>(3), 1903-1915. (<a href='https://doi.org/10.1109/TAFFC.2025.3545477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic widespread pain remains a complex and incompletely understood condition. To complement existing pain assessment strategies, this study explored the ecological validity of unobtrusively captured daily life physiological signals as indicators of pain. Therefore, we collected physiological data using a wearable wristband from 46 patients with chronic widespread pain for seven days. Linear mixed-effect models revealed several significant associations between physiological signals, such as mean heart rate and momentary pain intensity. However, making individual pain predictions with multivariate machine learning models did not add value. While this study underscores the potential of ambulatory physiology for pain assessment, future research should validate and expand upon these initial findings to further enhance pain management strategies.},
  archive      = {J_TAFFC},
  author       = {Emilie Pattyn and Nattapong Thammasan and Hannah Davidoff and Walter De Raedt and Gudrun Vera Eisele and Ruud van Stiphout and Maarten De Vos and Olivia J. Kirtley and Peter Van Wambeke and Bart Morlion and Elfi Vergaelen and Chris Van Hoof},
  doi          = {10.1109/TAFFC.2025.3545477},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1903-1915},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Exploring the relationship between stress-physiology and pain in the daily life of patients with chronic widespread pain},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic and emotional dual channel for emotion recognition in conversation. <em>TAFFC</em>, <em>16</em>(3), 1885-1902. (<a href='https://doi.org/10.1109/TAFFC.2025.3544608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition in conversation (ERC) aims at accurately identifying emotional states expressed in conversational content. Existing ERC methods, although relying on semantic understanding, often encounter challenges when confronted with incomplete or misleading semantic information. In addition, when dealing with the interaction between emotional and semantic information, existing methods are often difficult to effectively distinguish the complex relationship between the two, which affects the accuracy of emotion recognition. To address the problems of semantic misdirection and emotional cross-talk encountered by traditional models when confronted with complex conversational data, we propose a semantic and emotional dual channel (SEDC) strategy for emotion recognition in conversations to process emotional and semantic information independently. Under this strategy, emotion information provides an auxiliary recognition function when the semantics are unclear or lacking, enhancing the accuracy of the model. Our model consists of two modules: the emotion processing module accurately captures the emotional features of each utterance through contrastive learning, and then constructs a dialogue emotion propagation map to simulate the emotional information conveyed in the dialogue; the semantic processing module combines an external knowledge base to enhance the semantic expression of the dialogue through knowledge enhancement strategies. This divide-and-conquer approach allows us to more deeply analyze the emotional and semantic dimensions of complex dialogues. Experimental results on the IEMOCAP, EmoryNLP, MELD, and DailyDialog datasets show that our approach significantly outperforms existing techniques and effectively improves the accuracy of dialogue emotion recognition.},
  archive      = {J_TAFFC},
  author       = {Zhenyu Yang and Zhibo Zhang and Yuhu Cheng and Tong Zhang and Xuesong Wang},
  doi          = {10.1109/TAFFC.2025.3544608},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1885-1902},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Semantic and emotional dual channel for emotion recognition in conversation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empathy level alignment via reinforcement learning for empathetic response generation. <em>TAFFC</em>, <em>16</em>(3), 1873-1884. (<a href='https://doi.org/10.1109/TAFFC.2025.3544594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empathetic response generation, aiming to understand the user’s situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Traditional approaches typically employ maximum likelihood estimation as the optimization objective during training, yet fail to align the empathy levels between generated and target responses. To this end, we propose an empathetic response generation framework using reinforcement learning (EmpRL). The framework develops an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. EmpRL utilizes the pre-trained T5 model as the generator and further fine-tunes it to initialize the policy. To align the empathy levels between generated and target responses within a given context, an empathy reward function containing three empathy communication mechanisms—emotional reaction, interpretation, and exploration—is constructed using pre-designed and pre-trained empathy identifiers. During reinforcement learning training, the proximal policy optimization algorithm is used to fine-tune the policy, enabling the generation of empathetic responses. Both automatic and human evaluations demonstrate that the proposed EmpRL framework significantly improves the quality of generated responses, enhances the similarity in empathy levels between generated and target responses, and produces empathetic responses covering both affective and cognitive aspects.},
  archive      = {J_TAFFC},
  author       = {Hui Ma and Bo Zhang and Bo Xu and Jian Wang and Hongfei Lin and Xiao Sun},
  doi          = {10.1109/TAFFC.2025.3544594},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1873-1884},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Empathy level alignment via reinforcement learning for empathetic response generation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic depression recognition with an ensemble of multimodal spatio-temporal routing features. <em>TAFFC</em>, <em>16</em>(3), 1855-1872. (<a href='https://doi.org/10.1109/TAFFC.2025.3543226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression, driven by growing societal pressures, significantly disrupts individuals’ physical and mental health. Automatic Depression Recognition (ADR) via facial videos has gained attention to enhance diagnostic accuracy and efficiency. However, extant methods often segment videos, losing long-term behavioral cues and introducing noise, while also exhibiting performance drops across diverse cultural and racial datasets. This study proposes a multimodal ADR approach encompassing three key components: (1) Long-term Depression Behavior Module (LDBM) employing a Transformer to capture extended depression cues, (2) Noisy Information Elimination (NIE) strategy leveraging LDBM attention scores to reduce noise and boost diagnostic precision, and (3) Multimodal Spatio-temporal Routing Feature Ensemble (MSRE) that fuses texture, Facial Action Primitives (FAPs), and Remote Photoplethysmography (rPPG) data for improved cross-dataset generalizability. Experiments on AVEC 2013, AVEC 2014, and a newly constructed CMDep dataset of 123 clinically diagnosed participants validate our method, achieving MAE/RMSE scores of 5.38/6.74, 5.09/6.83, and 5.59/8.03, respectively. The CMDep dataset includes facial expression and voice signals, with labels derived from BDI-II scores. Additionally, our method has been integrated into a user-friendly mobile application, providing a tool for real-time self-assessment of depression. This integration broadens the scope of depression detection, making it accessible to diverse populations worldwide.},
  archive      = {J_TAFFC},
  author       = {Yaowei Wang and Zulong Lin and Chengrong Yang and Yujue Zhou and Yun Yang},
  doi          = {10.1109/TAFFC.2025.3543226},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1855-1872},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Automatic depression recognition with an ensemble of multimodal spatio-temporal routing features},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SigWavNet: Learning multiresolution signal wavelet network for speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1839-1854. (<a href='https://doi.org/10.1109/TAFFC.2025.3537991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of human-computer interaction and psychological assessment, speech emotion recognition (SER) plays an important role in deciphering emotional states from speech signals. Despite advancements, challenges persist due to system complexity, feature distinctiveness issues, and noise interference. This paper introduces a new end-to-end (E2E) deep learning multi-resolution framework for SER, addressing these limitations by extracting meaningful representations directly from raw waveform speech signals. By leveraging the properties of the fast discrete wavelet transform (FDWT), including the cascade algorithm, conjugate quadrature filter, and coefficient denoising, our approach introduces a learnable model for both wavelet bases and denoising through deep learning techniques. The framework incorporates an activation function for learnable asymmetric hard thresholding of wavelet coefficients. Our approach exploits the capabilities of wavelets for effective localization in both time and frequency domains. We then combine one-dimensional dilated convolutional neural networks (1D dilated CNN) with a spatial attention layer and bidirectional gated recurrent units (Bi-GRU) with a temporal attention layer to efficiently capture the nuanced spatial and temporal characteristics of emotional features. By handling variable-length speech without segmentation and eliminating the need for pre or post-processing, the proposed model outperformed state-of-the-art methods on IEMOCAP and EMO-DB datasets.},
  archive      = {J_TAFFC},
  author       = {Alaa Nfissi and Wassim Bouachir and Nizar Bouguila and Brian Mishara},
  doi          = {10.1109/TAFFC.2025.3537991},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1839-1854},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SigWavNet: Learning multiresolution signal wavelet network for speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-guided reconstruction network for sentiment analysis with uncertain missing modalities. <em>TAFFC</em>, <em>16</em>(3), 1825-1838. (<a href='https://doi.org/10.1109/TAFFC.2025.3541743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Sentiment Analysis (MSA) is an attractive research that aims to integrate sentiment expressed in textual, visual, and acoustic signals. There are two main problems in the existing methods: 1) the dominant role of the text is underutilization in unaligned multimodal data, and 2) the modality under uncertain missing feature is not sufficiently explored. This paper proposes a Text-guided Reconstruction Network (TgRN) for MSA with uncertain missing modalities in non-aligned sequences. The TgRN network includes three primary modules: Text-guided Extraction Module (TEM), Reconstruction Module (RM) and Text-guided Fusion Module (TFM). First, the TEM consists of the text-guided cross attention units and self-attention units to capture inter-modal features and intra-modal features, respectively. Second, leveraging enhanced attention units and a three-way squeeze-and-excitation block, the RM is designed to learn semantic information from incomplete data and reconstruct missing modality features. Third, the TFM utilizes a progressive modality-mixing adaptation gate to explore the dynamic correlations between nonverbal and verbal modalities, effectively addressing the modality gap issue. Finally, under the supervision of sentiment prediction loss and reconstruction loss, the TgRN effectively processes both uncertain missing-modality conditions and ideal complete modality conditions. Extensive experiments on CMU-MOSI and CH-SIMS demonstrate that our proposed method outperforms state-of-the-art approaches.},
  archive      = {J_TAFFC},
  author       = {Piao Shi and Min Hu and Satoshi Nakagawa and Xiangming Zheng and Xuefeng Shi and Fuji Ren},
  doi          = {10.1109/TAFFC.2025.3541743},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1825-1838},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Text-guided reconstruction network for sentiment analysis with uncertain missing modalities},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal depression prediction. <em>TAFFC</em>, <em>16</em>(3), 1814-1824. (<a href='https://doi.org/10.1109/TAFFC.2025.3542023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While existing depression prediction methods based on deep learning show promise, their practical application is hindered by the lack of trustworthiness, as these deep models are often deployed as black box models, leaving us uncertain on the confidence of their predictions. For high-risk clinical applications like depression prediction, uncertainty quantification is essential in decision-making. In this paper, we introduce conformal depression prediction (CDP), a depression prediction method with uncertainty quantification based on conformal prediction (CP), giving valid confidence intervals with theoretical coverage guarantees for the model predictions. CDP is a plug-and-play module that requires neither model retraining nor an assumption about the depression data distribution. As CDP provides only an average coverage guarantee across all inputs rather than per-input performance guarantee, we further propose CDP-ACC, an improved conformal prediction with approximate conditional coverage. CDP-ACC firstly estimates the prediction distribution through neighborhood relaxation, and then introduces a conformal score function by constructing nested sequences, so as to provide a tighter prediction interval adaptive to specific input. We empirically demonstrate the application of CDP in uncertainty-aware facial depression prediction, as well as the effectiveness and superiority of CDP-ACC on the AVEC 2013 and AVEC 2014 datasets.},
  archive      = {J_TAFFC},
  author       = {Yonghong Li and Shan Qu and Xiuzhuang Zhou},
  doi          = {10.1109/TAFFC.2025.3542023},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1814-1824},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Conformal depression prediction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDRS: Sentiment-aware disentangled representation shifting for multimodal sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 1802-1813. (<a href='https://doi.org/10.1109/TAFFC.2025.3539225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA) aims to leverage the complementary information from multiple modalities for affective understanding of user-generated videos. Existing methods mainly focused on designing sophisticated feature fusion strategies to integrate the separately extracted multimodal representations, ignoring the interference of the information irrelevant to sentiment. In this paper, we propose to disentangle the unimodal representations into sentiment-specific and sentiment-independent features, the former of which are fused for the MSA task. Specifically, we design a novel Sentiment-aware Disentangled Representation Shifting framework, termed SDRS, with two components. Interactive sentiment-aware representation disentanglement aims to extract sentiment-specific feature representations for each nonverbal modality by considering the contextual influence of other modalities with the newly developed cross-attention autoencoder. Attentive cross-modal representation shifting tries to shift the textual representation in a latent token space using the nonverbal sentiment-specific representations after projection. The shifted representation is finally employed to fine-tune a pre-trained language model for multimodal sentiment analysis. Extensive experiments are conducted on three public benchmark datasets, i.e., CMU-MOSI, CMU-MOSEI, and CH-SIMS. The results demonstrate that the proposed SDRS framework not only obtains state-of-the-art results based solely on multimodal labels but also outperforms the methods that additionally require the labels of each modality.},
  archive      = {J_TAFFC},
  author       = {Sicheng Zhao and Zhenhua Yang and Henglin Shi and Xiaocheng Feng and Lingpengkun Meng and Bing Qin and Chenggang Yan and Jianhua Tao and Guiguang Ding},
  doi          = {10.1109/TAFFC.2025.3539225},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1802-1813},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SDRS: Sentiment-aware disentangled representation shifting for multimodal sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing the visual road scene for driver stress estimation. <em>TAFFC</em>, <em>16</em>(3), 1787-1801. (<a href='https://doi.org/10.1109/TAFFC.2025.3539003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the contribution of the visual road scene to estimate the driver-reported stress levels. Our research leverages on previous work showing that environmental factors, such as traffic congestion, weather conditions, and driving context, impact driver’s stress. Each of the models we evaluated is trained and tested with the publicly available AffectiveROAD dataset to estimate three categories of driver-reported stress level. We test three types of modelling approaches: (i) single-frame baselines (Random Forest, SVM, and Convolutional Neural Networks); (ii) Temporal Segment Networks (TSN) and two variants of it, which use learned weights (TSN-w) and LSTM (TSN-LSTM) as consensus functions; and (iii) video classification Transformers. Our experiments reveal that the TSN-w, TSN-LSTM, and Transformer models achieve statistically equivalent performances, all significantly outperforming the other models. Particularly noteworthy is TSN-w, which attains the highest performance observed with an average accuracy of 0.77. We further provide an explainability analysis using Class Activation Mapping and image semantic segmentation to identify the elements of the road scene that contribute the most to high levels of stress. Our results demonstrate that the visible road scene offers significant contextual information for estimating driver-reported stress levels, with potential implications for the design of safer urban road environments.},
  archive      = {J_TAFFC},
  author       = {Cristina Bustos and Albert Sole-Ribalta and Neska Elhaouij and Javier Borge-Holthoefer and Agata Lapedriza and Rosalind Picard},
  doi          = {10.1109/TAFFC.2025.3539003},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1787-1801},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Analyzing the visual road scene for driver stress estimation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupled multi-perspective fusion for speech depression detection. <em>TAFFC</em>, <em>16</em>(3), 1772-1786. (<a href='https://doi.org/10.1109/TAFFC.2025.3538519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech Depression Detection (SDD) has garnered attention from researchers due to its low cost and convenience. However, current algorithms lack methods for extracting interpretable acoustic features based on clinical manifestations. In addition, effectively fusing these features to overcome individual heterogeneity remains a challenge. This study proposes a decoupled multi-perspective fusion (DMPF) model. The model extracts five key features of voiceprint, emotion, pause, energy, and tremor based on the multi-perspective clinical manifestations. These features are then decoupled into common and private features, which fused through graph attention network to obtain the comprehensive depression representation. Notably, this study has collected a depression speech dataset, which includes standardized and comprehensive tasks along with diagnostic labels provided by psychologists. Extensive subject-independent experiments were conducted on the DAIC-WOZ, MODMA and MPSC datasets. The voiceprint features can automatically cluster the depressed and non-depressed populations. Furthermore, DMPF can effectively fuse common and private features from different perspectives, achieving AUC of 84.20%, 85.34%, 86.13% on three datasets. The results illustrate the interpretability of multi-perspective features and demonstrate that the combination of speech manifestations can enhance the detection ability, which can provide a multi-perspective observational tool for physicians and clinical practice.},
  archive      = {J_TAFFC},
  author       = {Minghui Zhao and Hongxiang Gao and Lulu Zhao and Zhongyu Wang and Fei Wang and Wenming Zheng and Jianqing Li and Chengyu Liu},
  doi          = {10.1109/TAFFC.2025.3538519},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1772-1786},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Decoupled multi-perspective fusion for speech depression detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RVISA: Reasoning and verification for implicit sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 1760-1771. (<a href='https://doi.org/10.1109/TAFFC.2025.3537799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the context of the increasing social demand for fine-grained sentiment analysis (SA), implicit sentiment analysis (ISA) poses a significant challenge owing to the absence of salient cue words in expressions. Thus, reliable reasoning is required to understand how sentiment is evoked, enabling the identification of implicit sentiments. In the era of large language models (LLMs), encoder-decoder (ED) LLMs have emerged as popular backbone models for SA applications, given their impressive text comprehension and reasoning capabilities across diverse tasks. In comparison, decoder-only (DO) LLMs exhibit superior natural language generation and in-context learning capabilities. However, their responses may contain misleading or inaccurate information. To accurately identify implicit sentiments with reliable reasoning, this study introduces a two-stage reasoning framework named Reasoning and Verification for Implicit Sentiment Analysis (RVISA), which leverages the generation ability of DO LLMs and reasoning ability of ED LLMs to train an enhanced reasoner. The framework involves three-hop reasoning prompting to explicitly furnish sentiment elements as cues. The generated rationales are then used to fine-tune an ED LLM into a skilled reasoner. Additionally, we develop a straightforward yet effective answer-based verification mechanism to ensure the reliability of reasoning learning. Evaluation of the proposed method on two benchmark datasets demonstrates that it achieves state-of-the-art performance in ISA.},
  archive      = {J_TAFFC},
  author       = {Wenna Lai and Haoran Xie and Guandong Xu and Qing Li},
  doi          = {10.1109/TAFFC.2025.3537799},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1760-1771},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {RVISA: Reasoning and verification for implicit sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LineConGraphs: Line conversation graphs for effective emotion recognition using graph neural networks. <em>TAFFC</em>, <em>16</em>(3), 1747-1759. (<a href='https://doi.org/10.1109/TAFFC.2025.3537538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion Recognition in Conversations (ERC) is an important aspect of affective computing with practical applications in healthcare, education, chatbots, and social media platforms. Previous approaches to $\text {ERC}$ analysis involved using graph neural network architectures to model both speaker and long-term contextual information. In this paper, we introduce new models for $\text {ERC}$ analysis: the LineConGCN and LineConGAT models, which are constructed using a graph construction strategy for conversations called line conversational graphs (LineConGraphs). LineConGraph is designed to capture short-term conversational context using one previous and future utterance, while also capturing long-term context using GCN or GAT layers without explicitly integrating into the graph construction strategy. We evaluate the performance of our proposed models on two benchmark datasets, $\text {IEMOCAP}$ and $\text {MELD}$, and show that our LineConGAT model outperforms the state-of-the-art methods with an F1-score of $\text {64.58}\%$ and $\text {76.50}\%$. Furthermore, we demonstrate that incorporating sentiment shift information into line conversation graphs further enhances $\text {ERC}$ performance in the case of LineConGCN models. We also evaluate the performance of our proposed model by embedding speaker information into LineConGCN and LineConGAT models and show that LineConGAT and LineConGAT with speaker embeddings performed equally for ERC analysis.},
  archive      = {J_TAFFC},
  author       = {Gokul S Krishnan and Sarala Padi and Craig S. Greenberg and Balaraman Ravindran and Dinesh Manocha and Ram D. Sriram},
  doi          = {10.1109/TAFFC.2025.3537538},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1747-1759},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {LineConGraphs: Line conversation graphs for effective emotion recognition using graph neural networks},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSE-GResNet: A simple and highly efficient network for facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 1732-1746. (<a href='https://doi.org/10.1109/TAFFC.2025.3535811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) has recently attracted extensive attention in computer vision. However, existing methods mostly focus on the explicit performance and overlook their computational resources. Hence, achieving competitive performance while maintaining the model efficiency is still a huge challenge. To tackle these issues, we propose a highly lightweight yet effective Channel Shift-Enhancement Gabor-ResNet (CSE-GResNet) to capture the crucial visual properties in facial images. Concretely, we incorporate the Gabor Convolution (GConv) into ResNet to produce the robust GResNet as our backbone with limited memory cost. Furthermore, we propose extremely efficient Channel-Shift Module and Channel-Enhancement Module to insert in the GResNet in cascade. They are adopted to obtain and aggregate the facial informative representation from adjacent channels for extracting the subtle facial expression representation. We conduct extensive experiments on three wild datasets: RAF-DB, FER2013 and SFEW. The results show that the proposed CSE-GResNet achieves superior performance against the state-of-the-art methods with less computational and memory cost.},
  archive      = {J_TAFFC},
  author       = {Shaoping Jiang and Xiaofen Xing and Fang Liu and Xiangmin Xu and Lin Wang and Kailing Guo},
  doi          = {10.1109/TAFFC.2025.3535811},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1732-1746},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {CSE-GResNet: A simple and highly efficient network for facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale hyperbolic contrastive learning for cross-subject EEG emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1716-1731. (<a href='https://doi.org/10.1109/TAFFC.2025.3535542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) serves as a reliable and objective signal for affective computing applications. However, individual differences in EEG signals pose a significant challenge for emotion recognition tasks across subjects. To address this, we proposed a novel method called Multi-Scale Hyperbolic Contrastive Learning (MSHCL), which leverages event-relatedness to learn subject-invariant representations. MSHCL employs contrastive losses at two different scales—emotion and stimulus—to effectively capture complex EEG patterns within a hyperbolic space hierarchy. Our method is evaluated on three datasets: SEED, MPED, and FACED. It achieves 89.3% accuracy on the three-class task for SEED, 38.8% on the seven-class task for MPED, and 77.0% and 45.7% on the binary and nine-class tasks for FACED in cross-subject emotion recognition. These results demonstrate that the proposed MSHCL method superior performance over other baselines and its effectiveness in learning subject-invariant representations.},
  archive      = {J_TAFFC},
  author       = {Jiang Chang and Zhixin Zhang and Yuhua Qian and Pan Lin},
  doi          = {10.1109/TAFFC.2025.3535542},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1716-1731},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multi-scale hyperbolic contrastive learning for cross-subject EEG emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SVD-guided multimodal feature fusion for emotion recognition from facial videos. <em>TAFFC</em>, <em>16</em>(3), 1705-1715. (<a href='https://doi.org/10.1109/TAFFC.2025.3528636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal emotion recognition based on facial videos aims to extract features from different modalities to identify human emotions. The previous work focus on designing various fusion schemes to combine heterogeneous modal data. However, most studies have overlooked the role of different modalities in emotion recognition and have not fully utilized the intrinsic connections between modalities. Furthermore, the multimodal data from facial videos also contain various distractions bad for emotion analysis. How to reduce the impact of distractions and enable a model to mine effective information for emotion recognition from different modalities is still a challenge problem. To address above issue, we propose a SVD-guided multimodal feature fusion method based on facial video for emotion recognition, which uses a hierarchical fusion mechanism and adopts different loss strategies at each level to learn multimodal feature representation. Specifically, we fuse the facial expression and rPPG signal (or Point-of-Gaze) by using the weak supervision strategy and contrastive learning. Subsequently, the fused feature of facial expression and rPPG signal and the fused feature of facial expression and Point-of-Gaze are combined together to construct the unified multimodal feature matrix. Based on this, Singular Value Decomposition (SVD) is used to refine the redundancy information caused by the multimodal fusion and guide the neural network to learn discriminative emotion feature. At the same time, a consistent loss is developed to enhance the multimodal representation. Experiments on three public datasets show that the proposed method achieves better results over the compared methods.},
  archive      = {J_TAFFC},
  author       = {Jindi Bao and Jianjun Qian and Jian Yang},
  doi          = {10.1109/TAFFC.2025.3528636},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1705-1715},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SVD-guided multimodal feature fusion for emotion recognition from facial videos},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectro-temporal modulations incorporated two-stream robust speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1693-1704. (<a href='https://doi.org/10.1109/TAFFC.2025.3531638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based speech emotion recognition (SER) models have shown impressive results in controlled environments, but their performance significantly degrades in noisy conditions. This paper proposes a robust two-stream SER model by combining spectro-temporal modulation features with conventional acoustic features. Experiments were conducted on German (EMODB) and English (RAVDESS) datasets using the clean-train-noisy-test paradigm. The results demonstrate that spectro-temporal modulation features offer superior robustness in noisy conditions compared with conventional acoustic features such as MFCCs and time-frequency features from Mel-spectrograms. Additionally, we analyze weights of modulation features and demonstrate the model emphasizes contours of formants and harmonics, which are crucial features for speech perception in noise, for robust SER. Incorporating the stream of spectro-temporal modulations not only enhances the robustness of the model but also provides deeper insights into the task of SER in noise.},
  archive      = {J_TAFFC},
  author       = {Yih-Liang Shen and Pei-Chin Hsieh and Tai-Shih Chi},
  doi          = {10.1109/TAFFC.2025.3531638},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1693-1704},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Spectro-temporal modulations incorporated two-stream robust speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling fine-grained relations in dynamic space-time graphs for video-based facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 1675-1692. (<a href='https://doi.org/10.1109/TAFFC.2025.3530973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions in videos inherently mirror the dynamic nature of real-world facial events. Consequently, facial expression recognition (FER) should employ a dynamic graph-based representation to effectively capture the relational structure of facial expressions rather than relying on conventional grid or sequence methods. However, existing graph-based approaches have their limitations. Frame-level graph methods provide a coarse representation of the facial graph across time and space, while landmark-based graph methods need to introduce additional facial landmarks, resulting in a static graph structure. To address these challenges, we propose spatial-temporal relation-aware dynamic graph convolutional networks (ST-RDGCN). This fine-grained relation modeling approach enables the dynamic modeling of evolving facial expressions in videos through dynamic space-time graphs, eliminating the need for facial landmarks. ST-RDGCN encompasses three graph construction paradigms: dynamic independent space graph, dynamic joint space-time graph, and dynamic cross space-time graph. Furthermore, we propose a relation-aware space-time graph convolution (RSTG-Conv) operator to learn informative spatiotemporal correlations in dynamic space-time graphs. In extensive experimental evaluations, our ST-RDGCN demonstrates state-of-the-art performance on the five popular video-based FER datasets, achieving overall accuracy scores of 99.69%, 91.67%, 56.51%, 69.37%, and 49.03% on the CK+, Oulu-CASIA, AFEW, DFEW, and FERV39k datasets, respectively. In particular, our ST-RDGCN outperforms the current best method by 3.6% in UAR on the most challenging FERV39k dataset. Furthermore, our analysis reveals that the dynamic cross space-time graph scheme is the most effective among the three dynamic graph construction schemes.},
  archive      = {J_TAFFC},
  author       = {Changqin Huang and Fan Jiang and Zhongmei Han and Xiaodi Huang and Shijin Wang and Yanlai Zhu and Yunliang Jiang and Bin Hu},
  doi          = {10.1109/TAFFC.2025.3530973},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1675-1692},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Modeling fine-grained relations in dynamic space-time graphs for video-based facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DurFlex-EVC: Duration-flexible emotional voice conversion leveraging discrete representations without text alignment. <em>TAFFC</em>, <em>16</em>(3), 1660-1674. (<a href='https://doi.org/10.1109/TAFFC.2025.3530920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional voice conversion (EVC) involves modifying various acoustic characteristics, such as pitch and spectral envelope, to match a desired emotional state while preserving the speaker’s identity. Existing EVC methods often rely on text transcriptions or time-alignment information and struggle to handle varying speech durations effectively. In this paper, we propose DurFlex-EVC, a duration-flexible EVC framework that operates without the need for text or alignment information. We introduce a unit aligner that models contextual information by aligning speech with discrete units representing content, eliminating the need for text or speech-text alignment. Additionally, we design a style autoencoder that effectively disentangles content and emotional style, allowing precise manipulation of the emotional characteristics of the speech. We further enhance emotional expressiveness through a hierarchical stylize encoder that applies the target emotional style at multiple hierarchical levels, refining the stylization process to improve the naturalness and expressiveness of the converted speech. Experimental results from subjective and objective evaluations demonstrate that our approach outperforms baseline models, effectively handling duration variability and enhancing emotional expressiveness in the converted speech.},
  archive      = {J_TAFFC},
  author       = {Hyung-Seok Oh and Sang-Hoon Lee and Deok-Hyeon Cho and Seong-Whan Lee},
  doi          = {10.1109/TAFFC.2025.3530920},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1660-1674},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {DurFlex-EVC: Duration-flexible emotional voice conversion leveraging discrete representations without text alignment},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional connectivity analysis of children with autism under emotional clips. <em>TAFFC</em>, <em>16</em>(3), 1646-1659. (<a href='https://doi.org/10.1109/TAFFC.2025.3528920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder with marked impairments in neural system functioning. Electroencephalography (EEG) offers a promising approach to investigate the neurophysiological basis of ASD, however, most EEG studies in ASD focus on spontaneous brain activity. Emotional processing deficits are a core feature of ASD, but related connectivity patterns remain underexplored due to challenges in data collection and analysis. This study investigates functional brain connectivity differences between children with ASD (n = 32) and typically developing (TD) children (n = 32) across five frequency bands and four connectivity indices. We designed an SVM-MRMR pipeline to classify ASD and TD children using these features. Our findings reveal that ASD children exhibit more coordinated intra-brain networks and oscillatory patterns in the high-frequency range. Additionally, they show an increased number of long-range connections in the Theta band, particularly between the left and right hemispheres. ASD children also demonstrate increased frontal lobe connectivity during positive emotions and heightened temporal lobe activity during negative emotions. Functional connectivity under positive and negative emotional clips achieved a classification accuracy exceeding 85%. These findings suggest that functional connectivity derived from portable EEG devices may serve as a potential biomarker for diagnosing and classifying ASD in real-world applications.},
  archive      = {J_TAFFC},
  author       = {Chang Cai and Jiahui Wang and Jun Lin and Kang Yang and Huicong Kang and Jingying Chen and Wei Wu},
  doi          = {10.1109/TAFFC.2025.3528920},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1646-1659},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Functional connectivity analysis of children with autism under emotional clips},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phonetically-anchored domain adaptation for cross-lingual speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1631-1645. (<a href='https://doi.org/10.1109/TAFFC.2025.3530105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of cross-lingual speech emotion recognition (SER) modeling has significantly increased due to its wide range of applications. Previous studies have primarily focused on technical strategies to adapt features, domains, and labels across languages, often overlooking the underlying commonalities between the languages. In this study, we address the language adaptation challenge in cross-lingual scenarios by incorporating vowel-phonetic constraints. Our approach is structured in two main parts. First, we investigate the vowel-phonetic commonalities associated with specific emotions across languages, particularly focusing on common vowels that prove to be valuable for SER modeling. Second, we utilize these identified common vowels as anchors to facilitate cross-lingual SER. To demonstrate the effectiveness of our approach, we conduct case studies using American English and Taiwanese Mandarin with two naturalistic emotional speech corpora: the MSP-Podcast and BIIC-Podcast corpora. The approach leverages evidence that certain vowels, including monophthongs and diphthongs, exhibit emotion-specific commonality across languages, serving as phonetic anchors to enhance unsupervised cross-lingual SER learning. The proposed model surpasses baseline performance, highlighting the importance of phonetic similarities for effective language adaptation in cross-lingual SER scenarios.},
  archive      = {J_TAFFC},
  author       = {Shreya G. Upadhyay and Luz Martinez-Lucas and William Katz and Carlos Busso and Chi-Chun Lee},
  doi          = {10.1109/TAFFC.2025.3530105},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1631-1645},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Phonetically-anchored domain adaptation for cross-lingual speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised cross-domain facial expression recognition via class adaptive self-training. <em>TAFFC</em>, <em>16</em>(3), 1618-1630. (<a href='https://doi.org/10.1109/TAFFC.2025.3529947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised Cross-Domain Facial Expression Recognition (CD-FER) aims to transfer the recognition ability from annotated source domains to unlabeled target domains. Despite the advancements in CD-FER techniques based on marginal distribution matching, certain inherent properties of facial expressions, such as implicit class margins and imbalanced class distributions, still leave room for improvement in existing models. In this paper, we propose a Class-Adaptive Self-Training (CAST) model for unsupervised CD-FER. In addition to domain alignment, the CAST model leverages self-training to learn pseudo labels and dually enhance aligned representations for explicit class distinction, considering implicit class margins. Furthermore, the CAST model conducts a comprehensive analysis of the negative effects of class distributions on pseudo-label learning from perspectives of class-level representation distributions and predicted probabilities, and subsequently proposes specific solutions. By jointly matching class-level representation distributions and class distributions, the CAST model successfully alleviates conditional distribution discrepancies between domains, which is particularly pertinent for facial expression properties. Experimental results, including assessments on multiple target domains and evaluations of multiple FER models, demonstrate the effectiveness, superiority, and universality of the CAST model.},
  archive      = {J_TAFFC},
  author       = {Shanmin Wang and Qingshan Liu},
  doi          = {10.1109/TAFFC.2025.3529947},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1618-1630},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Unsupervised cross-domain facial expression recognition via class adaptive self-training},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal sentiment analysis with mutual information-based disentangled representation learning. <em>TAFFC</em>, <em>16</em>(3), 1606-1617. (<a href='https://doi.org/10.1109/TAFFC.2025.3529732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis seeks to utilize various types of signals to identify underlying emotions and sentiments. A key challenge in this field lies in multimodal representation learning, which aims to develop effective methods for integrating multimodal features into cohesive representations. Recent advancements include two notable approaches: one focuses on decomposing multimodal features into modality-invariant and -specific components, while the other emphasizes the use of mutual information to enhance the fusion of modalities. Both strategies have demonstrated effectiveness and yielded remarkable results. In this paper, we propose a novel learning framework that combines the strengths of these two approaches, termed mutual information-based disentangled multimodal representation learning. Our approach involves estimating different types of information during feature extraction and fusion stages. Specifically, we quantitatively assess and adjust the proportions of modality-invariant, -specific, and -complementary information during feature extraction. Subsequently, during fusion, we evaluate the amount of information retained by each modality in the fused representation. We employ mutual information or conditional mutual information to estimate each type of information content. By reconciling the proportions of these different types of information, our approach achieves state-of-the-art performance on popular sentiment analysis benchmarks, including CMU-MOSI and CMU-MOSEI.},
  archive      = {J_TAFFC},
  author       = {Hao Sun and Ziwei Niu and Hongyi Wang and Xinyao Yu and Jiaqing Liu and Yen-Wei Chen and Lanfen Lin},
  doi          = {10.1109/TAFFC.2025.3529732},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1606-1617},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multimodal sentiment analysis with mutual information-based disentangled representation learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TFAGL: A novel agent graph learning method using time-frequency EEG for major depressive disorder detection. <em>TAFFC</em>, <em>16</em>(3), 1592-1605. (<a href='https://doi.org/10.1109/TAFFC.2025.3527459'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The abnormality in depression exhibits reciprocal imbalanced connectivity between brain regions rather than increased or decreased activity of one particular area. Current works primarily align the distributions of EEG electrodes with insufficient simulation of neurophysiological structures. Moreover, they neglect significant collaborative relationships among diverse brain regions, which limits the performance of MDD detection. Considering the comprehensive information across brain regions and domains, we propose a novel EEG-based MDD detection model named Time-Frequency Agent Graph Learning (TFAGL), to capture the specific whole-brain level collaborative mechanism of MDD. Specifically, we generate agent nodes adaptively to perform global interactions among regions to sufficiently simulate the function of principal neurons, thereby forming a dynamic local-global connectivity graph to capture connectivity patterns for intra- and inter-regions. Furthermore, interactive learning across different receptive fields through multi-scale graph convolution is applied for each domain and connectivity. Besides, we construct feature extractors for both time and frequency domains and apply intra- and inter-domain constraints to remove redundancy and enhance the discriminability, thus obtaining comprehensive information representations. Extensive experiments on the public EEG MDD detection datasets demonstrate the superiority of TFAGL compared with the state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Zihua Xu and C. L. Philip Chen and Tong Zhang},
  doi          = {10.1109/TAFFC.2025.3527459},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1592-1605},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {TFAGL: A novel agent graph learning method using time-frequency EEG for major depressive disorder detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multitask transformer for cross-corpus speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1581-1591. (<a href='https://doi.org/10.1109/TAFFC.2025.3526592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has significantly advanced the field of Speech Emotion Recognition (SER), yet its efficacy in cross-corpus scenarios remains a challenge. To overcome this limitation, recent studies demonstrate the success of multitask learning, which uses auxiliary tasks to reduce difference between source and target dataset (or transfer knowledge from source to target datasets). Despite the efforts, the overall accuracy for cross-corpus SER is still relatively low and needs attention. To improve performance, we propose a multitask framework with SER as the primary task and contrastive learning and information maximization as auxiliary tasks. We design the auxiliary tasks innovatively to use the target data without emotional labels to develop a better understanding of the target data. The core of our multitask framework is a pre-trained transformer. While transformers have gained attention in SER, their application to cross-corpus scenarios is still limited. Multimodal approaches for cross-corpus scenario is substantially limited as well. We use text as the second modality, developing separate multitask transformers for audio and text and conduct decision-level fusion during inference. We use publicly available and widely used speech corpora, including the IEMOCAP, MSP-IMPROV and EMO-DB databases. The results demonstrate the benefits of the proposed approach, achieving improved performance on the benchmark databases in cross-corpus settings.},
  archive      = {J_TAFFC},
  author       = {Chung-Soo Ahn and Rajib Rana and Carlos Busso and Jagath C. Rajapakse},
  doi          = {10.1109/TAFFC.2025.3526592},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1581-1591},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multitask transformer for cross-corpus speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECDaily: A large-scale benchmark for emotion cause extraction in conversations. <em>TAFFC</em>, <em>16</em>(3), 1570-1580. (<a href='https://doi.org/10.1109/TAFFC.2024.3524124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite growing interest in emotion cause analysis in conversations, existing research is limited by the small scale of available datasets, with the largest benchmarks containing only around 1,000 conversations. This inadequacy poses significant challenges for training effective models and conducting reliable evaluations. Moreover, traditional benchmarks' definition of emotion causes as singular, continuous spans fails to capture the complex nature of conversational emotions, where causes are often scattered across multiple utterances. To address these limitations, we construct the Emotion Cause of DailyDialog (ECDaily), a large-scale dataset containing 13,118 conversations and 102,970 utterances - ten times larger than existing ones. ECDaily uniquely incorporates both individual and aggregated cause span annotations. In addition to the Individual-ECE and Individual-ECPE tasks, we introduce two new tasks - Aggregated-ECE and Aggregated-ECPE - along with a two-stage approach for handling multiple-span causes. We establish five baseline systems using several pre-trained language models for both individual and aggregated tasks. Extensive experiments demonstrate the effectiveness of the baselines trained on ECDaily across multiple tasks, and indicate that ECDaily serves as a robust and comprehensive benchmark for advancing emotion cause analysis in conversations.},
  archive      = {J_TAFFC},
  author       = {Xiangqing Shen and Ke Li and Jiaming An and Zixiang Ding and Rui Xia},
  doi          = {10.1109/TAFFC.2024.3524124},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1570-1580},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {ECDaily: A large-scale benchmark for emotion cause extraction in conversations},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TriagedMSA: Triaging sentimental disagreement in multimodal sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 1557-1569. (<a href='https://doi.org/10.1109/TAFFC.2024.3524789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing multimodal sentiment analysis models are effective at capturing sentiment commonalities across different modalities and discerning emotions. However, these models still face significant challenges when analyzing samples with sentiment polarity differences across modalities. Neural networks struggle to process such divergent sentiment samples, particularly when they are scarce within datasets. While larger datasets could help address this limitation, collecting and annotating them is resource-intensive. To overcome this challenge, we propose TriagedMSA, a multimodal sentiment analysis model with triage capability. Our model introduces the Sentiment Disagreement Triage Network, which identifies sentiment disagreement between modalities within a sample. This triage mechanism reduces mutual influence by learning to distinguish between samples of sentiment agreement and disagreement. To process these two sample types, we develop the Sentiment Selection Attention Network and the Sentiment Commonality Attention Network, both of which enhance modality interaction learning. Furthermore, we propose the Adaptive Polarity Detection (APD) algorithm, which ensures the generalizability of our model across different datasets, regardless of whether unimodal labels are available. The APD algorithm adaptively determines sentiment polarity disagreement or agreement between modalities. We conduct experiments on three multimodal sentiment analysis datasets: CMU-MOSI, CMU-MOSEI and CH-SIMS.v2. The results demonstrate that our proposed methodology outperforms existing state-of-the-art approaches.},
  archive      = {J_TAFFC},
  author       = {Yuanyi Luo and Wei Liu and Qiang Sun and Sirui Li and Jichunyang Li and Rui Wu and Xianglong Tang},
  doi          = {10.1109/TAFFC.2024.3524789},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1557-1569},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {TriagedMSA: Triaging sentimental disagreement in multimodal sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From EEG to eye movements: Cross-modal emotion recognition using constrained adversarial network with dual attention. <em>TAFFC</em>, <em>16</em>(3), 1543-1556. (<a href='https://doi.org/10.1109/TAFFC.2024.3524418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition is a fundamental part of affective computing, obtaining performance gain from multimodal methods. Electroencephalography (EEG) and eye movements are extensively used as they contain complementary information. However, the inconvenient acquisition of EEG is hindering the extensive adoption of multimodal emotion recognition in daily applications while eye movements are more convenient to collect but with lower performance. To tackle this issue, we propose a Constrained Adversarial Network with Dual Attention (CANDA), exploiting the complementary information from multiple modalities during training to improve the test-time performance of single easily acquired modality, i.e., transferring knowledge from a stronger modality to a weaker modality. During training, a common joint space is learned to diminish the distribution discrepancy among different modalities and incorporate the multimodal representations. During test, single modality is converted to the common space achieving comparable performance to multiple modalities. Extensive experiments demonstrate that our model achieves the state-of-the-art performance for cross-modal emotion recognition. Specifically, the mean accuracy increases around 15% on SEED, 15% on SEED-IV, and 2% on SEED-V compared to the latest baseline for emotion recognition. Visualization of features in the joint space illustrates that the distribution of different modalities aligns together with the discriminative ability regarding various emotions.},
  archive      = {J_TAFFC},
  author       = {Yiting Wang and Jia-Wen Liu and Bao-Liang Lu and Wei-Long Zheng},
  doi          = {10.1109/TAFFC.2024.3524418},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1543-1556},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {From EEG to eye movements: Cross-modal emotion recognition using constrained adversarial network with dual attention},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentiment triplet extraction with multi-view contrastive learning. <em>TAFFC</em>, <em>16</em>(3), 1526-1542. (<a href='https://doi.org/10.1109/TAFFC.2024.3521608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment Triplet Extraction (STE) is a challenging Aspect-based Sentiment Analysis task that involves identifying aspect terms, aspect categories, opinion terms, and their corresponding sentiment polarities in sentences. However, the complex relationships and implicit elements constituting sentiment triplets (aspect, opinion, polarity) or (aspect, category, polarity) pose a significant challenge. This paper proposes a novel model called Multi-view Contrastive Learning (MCL) for STE. We treat STE as a text generation task and employ Contrastive Learning at both the triplet and sentiment views. At the triplet view, the source text is used as an anchor, and the target text is regarded as positive samples, while negative samples are obtained by destroying triplet elements in the target text. At the sentiment view, aspect terms are concatenated with their corresponding opinion terms or categories, and the same sentiment polarity in the dataset is used as positive samples, while different polarities are considered negative samples. Our experimental results show that the proposed model outperforms the baseline GAS-EXTRACTION by a significant margin, with every improvement on F1 of 5.21 for Aspect Sentiment Triplet Extraction and 2.98 for Aspect Category Sentiment Detection. These results highlight the effectiveness of incorporating Contrastive Learning in the STE task.},
  archive      = {J_TAFFC},
  author       = {Wenfang Wu and Daling Wang and Ming Wang and Shi Feng and Yifei Zhang},
  doi          = {10.1109/TAFFC.2024.3521608},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1526-1542},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Sentiment triplet extraction with multi-view contrastive learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial expression recognition with label-noisy under dual-branch noise extraction and suppression. <em>TAFFC</em>, <em>16</em>(3), 1514-1525. (<a href='https://doi.org/10.1109/TAFFC.2024.3519359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Noisy labels in Facial Expression Recognition (FER) datasets severely affect the performance of FER models. We propose a novel dual-branch noise extraction and suppression method to address this issue. This algorithm reduces the model’s impact from noisy labels by decreasing the dataset noise ratio and suppressing label-noisy samples. The method comprises three primary stages: sample extraction, pseudo-label generation, and re-training. The approach initially extracts label-noisy samples from the dataset by computing an exponential moving average of the model predictions and the joint probability distribution matrix of noisy and actual labels. The remaining samples form a clean dataset. Next, the training weights of the clean dataset are utilized to assign appropriate pseudo-labels to the label-noisy samples. Subsequently, the noisy labels are replaced with pseudo-labels to create a corrected dataset. The corrected and clean datasets are combined to create the reconstructed dataset, reducing noisy labels within the dataset. Finally, the model is retrained using the reconstructed dataset. Furthermore, this study introduces a novel gradient suppression smoothing function specifically designed to mitigate the impact of label-noisy samples in the dataset during the re-training process. The proposed algorithm is robust, with accuracies of 91.17%, 91.56%, and 91.58% on the RAF-DB dataset with 10%, 20%, and 30% noisy labels, and accuracies of 89.91%, 90.17%, and 89.69% on the corresponding FERPlus.},
  archive      = {J_TAFFC},
  author       = {Yunfei Li and Hao Liu and Daihong Jiang and Jiuzhen Liang},
  doi          = {10.1109/TAFFC.2024.3519359},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1514-1525},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Facial expression recognition with label-noisy under dual-branch noise extraction and suppression},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Major depressive disorder detection using graph domain adaptation with global message-passing based on EEG signals. <em>TAFFC</em>, <em>16</em>(3), 1500-1513. (<a href='https://doi.org/10.1109/TAFFC.2024.3515457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) has been widely used for the detection of major depressive disorder (MDD). Currently, several methods have been proposed to process EEG signals for MDD detection using deep learning algorithms, and some advantages have been achieved. However, the extraction of EEG features for MDD detection remains challenging and most methods have difficulty in extracting common features of EEG signals across subjects. We propose a graph domain adaptation with global message-passing (GMP-GDA) for MDD detection. In addition, an adjacency matrix weighting algorithm is designed in the global message-passing module to learn the weight combinations of different adjacency matrices instead of feature transformation matrices to achieve cross-domain global message-passing between multi-source and target domains. Experimental results demonstrate that the proposed method achieves the superior detection result in each frequency band compared to the baseline systems. Meanwhile, ablation experiments demonstrate the effectiveness of our proposed method.},
  archive      = {J_TAFFC},
  author       = {Hui Wang and Jinghui Yin and Siyuan Gao and Ju Liu and Qiang Wu},
  doi          = {10.1109/TAFFC.2024.3515457},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1500-1513},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Major depressive disorder detection using graph domain adaptation with global message-passing based on EEG signals},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mouse-cursor tracking: Simple scoring algorithms that make it work. <em>TAFFC</em>, <em>16</em>(3), 1488-1499. (<a href='https://doi.org/10.1109/TAFFC.2024.3519257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mouse-cursor tracking, a new action-based measure of behavior, has emerged as one of the promising applications of affective computing. As facial expressions, gaits, electroencephalogram (EEG), and electrodermal activity (EDA) inform the emotions of computer users, the movement of the computer mouse-cursor reveals when people feel anxious, relaxed, attentive, joyful, and sad. However, the mouse tracking analysis has not previously been subject to systematic investigations of psychometric properties. The choice of motor features, experimental manipulations, and data transformation methods is ad hoc. In this study, we evaluate the impact of psychological factors on mouse-based affective computing and propose simple scoring algorithms that incorporate psychometric features such as the frame of reference, habituation, and measurement error. Our results demonstrate that our new dimensionality reduction method, merged PCA, outperforms conventional procedures, improving prediction performance by about $15-30\%$.},
  archive      = {J_TAFFC},
  author       = {Takashi Yamauchi and Shanle Longmire-Monford and Anton Leontyev and Kunxia Wang},
  doi          = {10.1109/TAFFC.2024.3519257},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1488-1499},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Mouse-cursor tracking: Simple scoring algorithms that make it work},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards cyberbullying detection: Building, benchmarking and longitudinal analysis of aggressiveness and Conflicts/Attacks datasets from twitter. <em>TAFFC</em>, <em>16</em>(3), 1473-1487. (<a href='https://doi.org/10.1109/TAFFC.2024.3518587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offense and hate speech are a source of online conflicts which have become common in social media and, as such, their study is a growing topic of research in machine learning and natural language processing. This article presents two Portuguese language offense-related datasets that deepen the study of the subject: an Aggressiveness dataset and a Conflicts/Attacks dataset. While the former is similar to other offense detection related datasets, the latter constitutes a novelty due to the use of the history of the interaction between users. Several studies were carried out to construct and analyze the data in the datasets. The first study included gathering expressions of verbal aggression witnessed by adolescents to guide data extraction for the datasets. The second study included extracting data from Twitter (in Portuguese) that matched the most frequent expressions/words/sentences that were identified in the previous study. The third study consisted in the development of the Aggressiveness dataset, the Conflicts/Attacks dataset, and classification models. In our fourth study, we proposed to examine whether online aggression and conflicts/attacks revealed any trend changes over time with a sample of 86 adolescents. With this study, we also proposed to investigate whether the amount of tweets sent over a period of 273 days was related to online aggression and conflicts/attacks. Finally, we analyzed the percentage of participants who participated in the aggressions and/or attacks/conflicts.},
  archive      = {J_TAFFC},
  author       = {Paula Ferreira and Nádia Pereira and Hugo Rosa and Sofia Oliveira and Luísa Coheur and Sofia Francisco and Sidclay Souza and Ricardo Ribeiro and João P. Carvalho and Paula Paulino and Isabel Trancoso and Ana Margarida Veiga-Simão},
  doi          = {10.1109/TAFFC.2024.3518587},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1473-1487},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Towards cyberbullying detection: Building, benchmarking and longitudinal analysis of aggressiveness and Conflicts/Attacks datasets from twitter},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedAR: Federated artificial resampling for imbalanced facial emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1461-1472. (<a href='https://doi.org/10.1109/TAFFC.2024.3516822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as an essential tool for computing devices to participate in collaborative training of deep learning models. However, due to the decentralized distribution of data over clients/local computing devices, the class imbalance problem has become evident, causing severe degradation in the performance of the global model. Motivated by the emergence of FL models in emotion recognition, the current study proposes an FL-based facial emotion recognition system by addressing local imbalance data problems encountered in client devices. First, the local imbalance problem is mitigated by utilizing the data-level artificial resampling method on the client side. To address the possibility of an adversarial attack using imbalanced data, the local training is equipped with a pre-training check to verify if the data being used is imbalanced above a predefined threshold of imbalance ratio. In case of high imbalance, a pre-training step will balance the data locally without sharing any information with other participants thereby ensuring privacy in the FL framework. Experiments have been conducted by using benchmark facial emotion recognition data with a balanced testing strategy. It indicated that considerable improvement can be achieved by the proposed FL-based facial emotion recognition model.},
  archive      = {J_TAFFC},
  author       = {Sankhadeep Chatterjee and Kushankur Ghosh and Saranya Bhattacharjee and Asit Kumar Das and Soumen Banerjee},
  doi          = {10.1109/TAFFC.2024.3516822},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1461-1472},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {FedAR: Federated artificial resampling for imbalanced facial emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EmoDialect: Leveraging fuzzy matching and dialect-emotion mapping for sentiment analysis. <em>TAFFC</em>, <em>16</em>(3), 1444-1460. (<a href='https://doi.org/10.1109/TAFFC.2024.3514862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment Analysis is a well-explored field in natural language processing, that relies on intricate textual features. However, recent models tend to overlook the influence of dialects, emotions, and their associations, leading to inaccurate classifications. This work presents EmoDialect, a novel fuzzy framework designed to enhance sentiment analysis by mapping dialect with emotions and hence, their coalition coined as EmoDialect. The introduced EmoDialect incorporates dialect-emotion associations in feature extraction and utilizes fuzzy matching for dialect identification. Further, it leverages tweaked term frequency-inverse document frequency and parts-of-speech tagged $\mathcal {N}-$grams to capture dialect-specific sentiment cues. This enhanced EmoDialect feature set enhances sentiment analysis by attuning to the unique linguistic and emotional characteristics of diverse English dialects. Tests conducted on diverse corpora spanning various domains demonstrate the remarkable superiority and consistency of EmoDialect in terms of weighted average F1-scores of 92%, 86.7%, and 93% in dialect, sentiment, and text classification respectively, overtaking its predecessors by a wide margin. Also, EmoDialect was extended to dialect translation, and the related examinations revealed the F1-score of 86.15% warranting its ability to aid cross-cultural communication.},
  archive      = {J_TAFFC},
  author       = {Cherukula Madhu and Sudhakar M.S.},
  doi          = {10.1109/TAFFC.2024.3514862},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1444-1460},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {EmoDialect: Leveraging fuzzy matching and dialect-emotion mapping for sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nonverbal leadership in joint full-body improvisation. <em>TAFFC</em>, <em>16</em>(3), 1431-1443. (<a href='https://doi.org/10.1109/TAFFC.2024.3514933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate nonverbal leadership and address two research questions: 1) is it possible to perceive leadership from nonverbal cues in an unstructured joint full-body activity with no designated leader? 2) what are its nonverbal indicators? To address these questions, we propose eight cues of nonverbal leadership and conduct a two-step validation study on a novel dataset (video, MoCap) of dance improvisation. To explore various leadership strategies, we introduce constraints on how dancers communicate by manipulating their shared sensory channels. In the first stage, 27 persons carried out continuous annotation of leadership in the recorded videos; in the second stage, 92 persons watched 25 short segments indicating who the leader was and reported perceived leadership cues. The results indicate 1) a high consensus among observers regarding nonverbal leadership, but only for certain video segments, and 2) that five leadership cues were frequently observed in our dataset. In the final part, we explore the feasibility of automatically detecting nonverbal leadership using hand-crafted cues and standard machine learning techniques.},
  archive      = {J_TAFFC},
  author       = {Radoslaw Niewiadomski and Léa Chauvigné and Maurizio Mancini and Gualtiero Volpe and Antonio Camurri},
  doi          = {10.1109/TAFFC.2024.3514933},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1431-1443},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Nonverbal leadership in joint full-body improvisation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing EEG-based cross-subject emotion recognition via adaptive source joint domain adaptation. <em>TAFFC</em>, <em>16</em>(3), 1419-1430. (<a href='https://doi.org/10.1109/TAFFC.2024.3514635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG emotion recognition is crucial in both human-machine interaction and healthcare. However, recognizing emotions across different subjects remains challenging due to individual variability. While existing multi-source domain adaptation methods have been utilized for cross-subject EEG emotion decoding, they often struggle with irrelevant or weakly relevant source domains, leading to negative transfer. Additionally, variations within subdomains are often neglected in these studies. We propose a joint domain adaptation method, Adaptive Source Joint Domain Adaptation (ASJDA) to address these issues. ASJDA utilizes an unsupervised adaptive source selection strategy to select a subset of source domains by evaluating the Jensen-Shannon divergence between the source and target domains, choosing those most relevant to the target. Subsequently, it implements joint domain adaptation with these chosen sources at both the domain and category subdomain levels. Our proposed method outperforms existing state-of-the-art methods, achieving cross-subject accuracies of 96.81% in SEED, 89.69% in SEED-IV, and 69.31% in DEAP. This work significantly advances the state of the art in EEG emotion recognition by effectively addressing the challenges of cross-subject variability.},
  archive      = {J_TAFFC},
  author       = {Ke Liu and Xin Luo and Wenrui Zhu and Zhu Liang Yu and Hong Yu and Bin Xiao and Wei Wu},
  doi          = {10.1109/TAFFC.2024.3514635},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1419-1430},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Enhancing EEG-based cross-subject emotion recognition via adaptive source joint domain adaptation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial expression recognition with vision transformer using fused shifted windows. <em>TAFFC</em>, <em>16</em>(3), 1406-1418. (<a href='https://doi.org/10.1109/TAFFC.2024.3511628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions contain massive affective information. Previous methods have focused on using diverse models based on CNN or Transformer to handle the facial expression recognition(FER) task. However, most of them treat the FER task as a general image classification task and neglect the impact of regions of interest (ROIs) for the performance of FER. To verify the influence of different ROIs, in this paper, we propose a vision Transformer based on Fused Shifted windows (FSwin), called FSwin Transformer. The semantic information of the face, obtained by ROIs, guides the FSwin Transformer to focus more on the key regions. The fused shifted windows enable the model to perform global semantic interactions, allowing it to concentrate on key regions without losing the topological structural information of the entire face. Additionally, learnable parameters are introduced to learn the feature weights expressed by each ROI, helping the model dynamically adjust the attention distribution. We have conducted controlled experiments to quantitatively verify the impact of ROIs. And the experimental results show that with the increase of the number of ROIs, the accuracy of FER is significantly improved, demonstrating that the key ROIs play an important role in feature extraction. The results on Jaffe, CK+, FER2013, AffectNet have reached 99.8%, 99.0%, 74.8%, and 68.9%, respectively, which all set new state-of-the-art. Extensive cross-dataset experiments also show that the FSwin Transformer has good generalization ability, proving that our proposed model has a beneficial effect on FER tasks.},
  archive      = {J_TAFFC},
  author       = {Xiao Sun and Rui Wang and Shaokai Chen and Meng Wang},
  doi          = {10.1109/TAFFC.2024.3511628},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1406-1418},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Facial expression recognition with vision transformer using fused shifted windows},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demographic-guided behavior patterns contrast for personality prediction. <em>TAFFC</em>, <em>16</em>(3), 1392-1405. (<a href='https://doi.org/10.1109/TAFFC.2024.3512206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, personality has been considered as a valuable personal factor being incorporated into the provision of personalized learning. Although some studies have endeavored to obtain learners’ personalities implicitly from their learning behaviors, they failed to achieve satisfactory prediction performance. On the one hand, most existing approaches ignore the imbalanced distribution of personality classes, which causes the personality classifiers to be biased toward the non-extreme personality class. On the other hand, the related methods normally focus on constructing statistical behavior features, while the sequence information of learning behaviors is ignored, but actually it can reflect learners’ behavior patterns more finely. In this paper, inspired by the human learning strategy in the face of small samples, we propose an effective Demographic-Guided Behavior Patterns Contrast (DGBPC) model to classify learners’ personalities through the demographic-guided contrast of learners’ coarse behavior patterns. Besides, we construct and publish the Personality and Learning Behavior Dataset (PLBD), which should be one of the largest public datasets regarding Big-Five personality and learning behavior sequence according to our knowledge. The experimental results on PLBD demonstrate that our DGBPC model could generate learner representations with higher discrimination and outperform the related methods in terms of balanced accuracy.},
  archive      = {J_TAFFC},
  author       = {Yu Ji and Wen Wu and Hui Lin and Wenxin Hu and Yi Hu and Liang Kang and Xi Chen and Liang He},
  doi          = {10.1109/TAFFC.2024.3512206},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1392-1405},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Demographic-guided behavior patterns contrast for personality prediction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A micro-expression recognition network based on attention mechanism and motion magnification. <em>TAFFC</em>, <em>16</em>(3), 1379-1391. (<a href='https://doi.org/10.1109/TAFFC.2024.3510302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expressions (MEs) are spontaneous facial movements that reveal an individual’s genuine emotions and play a crucial role in various domains, including lie detection, criminal analysis, mental health treatment, national security, and others. Micro-expression recognition is a highly complex aspect within the domain of affective computing, aimed at identifying subtle facial motions that are difficult for humans to discern accurately. To model the subtle facial muscle motions and the brief duration of MEs, we propose a robust micro-expression recognition (MER) network, named the attention mechanism-based motion magnification guided micro-expression recognition network (AM-MM-MER). This network consists of two primary components: the ST-MEMM network, which enhances subtle motions in micro-expression videos to reveal imperceptible facial muscle motions, and the AM-MER, which focuses on facial landmarks related to micro-expressions and incorporates novel landmark positions to extract the underlying relationships among these landmarks, thereby reducing interference from video magnification and irrelevant identity features. Extensive analysis on the CASME II and SAMM datasets demonstrates the high accuracy and effectiveness of the proposed network, achieving superior results compared to state-of-the-art methods. Ablation studies further illustrate the robustness of the proposed network.},
  archive      = {J_TAFFC},
  author       = {Falin Wu and Yu Xia and Boyi Ma and Tianyang Hu and Jingyao Yang and Haoxin Li and Di Huang},
  doi          = {10.1109/TAFFC.2024.3510302},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1379-1391},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A micro-expression recognition network based on attention mechanism and motion magnification},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGILE: Attribute-guided identity independent learning for facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 1362-1378. (<a href='https://doi.org/10.1109/TAFFC.2024.3508536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within computer vision, Facial Expression Recognition (FER) is a challenging task involving bottlenecks such as obtaining well-separated and compact expression embeddings that are invariant of identity. This study introduces AGILE (Attribute-Guided Identity-Independent Learning), an innovative approach to enhance FER by distilling identity information and promoting discriminative features. Initially, an adaptive $\beta$ Variational Autoencoder (VAE) is proposed from a fixed $\beta$-VAE architecture leveraging the theory of single-dimensional Kalman filter. This enhances disentangled feature learning without compromising the reconstruction quality. Now, to achieve the desired FER objective, we design a two-stage modular scheme built within the framework of adaptive $\beta$-VAE. In the first stage, an expression-driven identity modeling is proposed where an identity encoder is trained with a novel training loss to embed the most likely state corresponding to every subject in latent representation. In the next stage, keeping the identity encoder fixed, an expression encoder is trained with explicit guidance for the latent variables using an adversarial excitation and inhibition mechanism. This form of supervision enhances the transparency and interpretability of the expression space and helps to capture discriminative expression embeddings required for the downstream classification task. Experimental evaluations demonstrate that AGILE outperforms existing methods in identity and expression separability in the latent space and achieves superior performance over state-of-the-art methods on both lab-controlled and in-the-wild datasets, with recognition accuracies of 99.00% on CK+, 90.00% on Oulu-CASIA, 89.01% on MMI, 67.20% on Aff-Wild2, and 68.97% on AFEW.},
  archive      = {J_TAFFC},
  author       = {Mohd Aquib and Nishchal K. Verma and M. Jaleel Akhtar},
  doi          = {10.1109/TAFFC.2024.3508536},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1362-1378},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {AGILE: Attribute-guided identity independent learning for facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SentireCache: Accelerate sentiment classification with saliency-based caching. <em>TAFFC</em>, <em>16</em>(3), 1349-1361. (<a href='https://doi.org/10.1109/TAFFC.2025.3578574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning methodologies have demonstrated exceptional efficacy in sentiment classification tasks. However, their extended inference times often impede practical deployment, particularly in resource-constrained environments. This paper addresses the challenge of reducing inference time by introducing a novel in-GPU caching approach, termed SentireCache, specifically designed for sentiment classification tasks. While traditional caching methods with the cosine similarity measurement have shown some reduction in inference time, they suffer from low hit rates and accuracy. To overcome this limitation, we incorporate a token filtering mechanism based on saliency into the caching system, along with simplified similarity calculation methods. The effectiveness of our proposed approach is theoretically analyzed. Moreover, extensive experimentation is conducted to compare SentireCache with other state-of-the-art caching methods. The results demonstrate a significant 37.7% reduction in inference time with an average performance degradation of 4.69%.},
  archive      = {J_TAFFC},
  author       = {Yilong Zhu and Juncheng Jia and Mianxiong Dong and Jun Qi},
  doi          = {10.1109/TAFFC.2025.3578574},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1349-1361},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SentireCache: Accelerate sentiment classification with saliency-based caching},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of experimental protocols: Towards a uniform framework in virtual reality affective research. <em>TAFFC</em>, <em>16</em>(3), 1334-1348. (<a href='https://doi.org/10.1109/TAFFC.2025.3554496'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of affective computing with virtual reality (VR) often uses machine learning to analyze users’ emotional responses through physiological and behavioral signals, enabling personalized interactions within VR environments. However, current research in this field is characterized by inconsistent experimental protocols, which hinders comprehensive conclusions and cross-study comparisons. To address this gap, a systematic review was conducted following the PRISMA guidelines, identifying 24 studies that used physiological measures and machine learning to predict emotions in VR settings. The review covers five key areas: experimental protocols, VR environments, implicit measurements, emotion models, and machine learning approaches. In addition, it provides guidelines for standardizing data collection, biosignal processing, and emotion modeling. These proposed guidelines aim to establish consistent reporting practices and experimental protocols, thus improving the comparability and reproducibility of future VR affective computing research.},
  archive      = {J_TAFFC},
  author       = {Allison Bayro and Heejin Jeong},
  doi          = {10.1109/TAFFC.2025.3554496},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1334-1348},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A systematic review of experimental protocols: Towards a uniform framework in virtual reality affective research},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speech-based depression assessment: A comprehensive survey. <em>TAFFC</em>, <em>16</em>(3), 1318-1333. (<a href='https://doi.org/10.1109/TAFFC.2024.3521327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression (major depressive disorder) is one of the most common mental illnesses worldwide, causing feelings of sadness and loss of interest, and is a leading cause of suicidal ideation. Limited access to mental health services, stigma, patient privacy and delay in seeking help are the most significant barriers to assessment and effective treatment. In order to enhance the accuracy of depression prediction, automated strategies employing computational models have been widely explored in literature. To this end, automatic Speech Depression Recognition (SDR) methods stand out, as speech comprises a valuable marker of mental health. Interestingly, recording speech comprises a less intrusive and more portable approach than capturing video, thus more easily accepted, especially by the younger generations, who are at a considerable risk of social isolation due to addiction to social networks and excessive use of mobile devices. In this context, this paper presents an up-to-date survey on SDR. More specifically, we a) detail the major challenges and key issues on SDR, b) summarise the most recent approaches existing in the related literature, and c) highlight the open problems. At the same time, we illustrate a framework encompassing the latest tendencies for SDR, along with a suitable comparison of the achieved performances. Finally, we highlight future trends and present the overall findings, providing researchers with best practices and techniques to address the major challenges of SDR, as well as stimulating discussion and improvement in the field.},
  archive      = {J_TAFFC},
  author       = {Samara Soares Leal and Stavros Ntalampiras and Roberto Sassi},
  doi          = {10.1109/TAFFC.2024.3521327},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1318-1333},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Speech-based depression assessment: A comprehensive survey},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A knowledge distillation-based approach to speech emotion recognition. <em>TAFFC</em>, <em>16</em>(3), 1307-1317. (<a href='https://doi.org/10.1109/TAFFC.2025.3574178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to rapid advancements in deep learning, Transformer-based architectures have proven effective in speech emotion recognition (SER), largely due to their ability to model long-term dependencies more effectively than recurrent networks. The current Transformer architecture is not well-suited for SER because its large parameter number demands significant computational resources, making it less feasible in environments with limited resources. Furthermore, its application to SER is limited because human emotions, which are expressed in long segments of continuous speech, are inherently complex and ambiguous. Therefore, designing specialized Transformer models tailored for SER is essential. To address these challenges, we propose a novel knowledge distillation framework that combines meta-knowledge and curriculum-based distillation. Specifically, we fine-tune the teacher model to optimize it for the SER task. For the student model, we embed individual sequence time points into variable tokens, which are used to aggregate the global speech representation. Additionally, we combine supervised contrastive and cross-entropy loss to increase the inter-class distance between learnable features. Finally, we optimize the student model using both meta-knowledge and the curriculum-based distillation framework. Experimental results on two benchmark datasets, IEMOCAP and MELD, demonstrate that our method performs competitively with state-of-the-art approaches in SER.},
  archive      = {J_TAFFC},
  author       = {Ziping Zhao and Jixin Liu and Haishuai Wang and Danushka Bandara and Jianhua Tao},
  doi          = {10.1109/TAFFC.2025.3574178},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1307-1317},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A knowledge distillation-based approach to speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ParaLBench: A large-scale benchmark for computational paralinguistics over acoustic foundation models. <em>TAFFC</em>, <em>16</em>(3), 1290-1306. (<a href='https://doi.org/10.1109/TAFFC.2024.3506554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational paralinguistics (ComParal) aims to develop algorithms and models to automatically detect, analyze, and interpret non-verbal information from speech communication, e. g., emotion, health state, age, and gender. Despite its rapid progress, it heavily depends on sophisticatedly designed models given specific paralinguistic tasks. Thus, the heterogeneity and diversity of ComParal models largely prevent the realistic implementation of ComParal models. Recently, with the advent of acoustic foundation models because of self-supervised learning, developing more generic models that can efficiently perceive a plethora of paralinguistic information has become an active topic in speech processing. However, it lacks a unified evaluation framework for a fair and consistent performance comparison. To bridge this gap, we conduct a large-scale benchmark, namely ParaLBench, which concentrates on standardizing the evaluation process of diverse paralinguistic tasks, including critical aspects of affective computing such as emotion recognition and emotion dimensions prediction, over different acoustic foundation models. This benchmark contains ten datasets with thirteen distinct paralinguistic tasks, covering short-, medium- and long-term characteristics. Each task is carried out on 14 acoustic foundation models under a unified evaluation framework, which allows for an unbiased methodological comparison and offers a grounded reference for the ComParal community. Based on the insights gained from ParaLBench, we also point out potential research directions, i. e., the cross-corpus generalizability, to propel ComParal research in the future. The code associated with this study will be available to foster the transparency and replicability of this work for succeeding researchers.},
  archive      = {J_TAFFC},
  author       = {Zixing Zhang and Weixiang Xu and Zhongren Dong and Kanglin Wang and Yimeng Wu and Jing Peng and Runming Wang and Dong-Yan Huang},
  doi          = {10.1109/TAFFC.2024.3506554},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1290-1306},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {ParaLBench: A large-scale benchmark for computational paralinguistics over acoustic foundation models},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive masking oriented self-taught learning for occluded facial expression recognition. <em>TAFFC</em>, <em>16</em>(3), 1277-1289. (<a href='https://doi.org/10.1109/TAFFC.2025.3544677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-taught learning (STL) is a promising solution that reduces the performance gap between weakly supervised and fully supervised learning for easily accessible, label-free images. The success of traditional STL solutions relies on the assumption that the target appearance is completely visible and well-defined. In real-world facial expression recognition scenarios, however, saliency regions are often partially occluded, which significantly hampers the generalization capability of STL methods. Nevertheless, few studies have investigated the impact of occlusion on STL. In this paper, we propose an interweaved autoencoder network for weakly supervised facial expression recognition in occlusion scenarios. The key innovation of our network lies in the Residual Connection Union (RCU) blocks that can integrate the Convolutional Neural Network (CNN) and Transformer layers into a multi-scale structure. The RCU enables a progressive masking strategy to accurately identify and focus on contributive yet often overlooked image patches by analyzing the relationships among region-level target representations. In addition, we introduce a self-knowledge distillation module for the effective training of the proposed autoencoder network. Extensive experiments are conducted on four public datasets to demonstrate the superiority of our method over related works.},
  archive      = {J_TAFFC},
  author       = {Bin Kang and Shuangshuang Wang and Zongyu Wang and Xin Li and Haie Dou and Lei Wang and Zhijie Xia},
  doi          = {10.1109/TAFFC.2025.3544677},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1277-1289},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Progressive masking oriented self-taught learning for occluded facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Connecting cross-modal representations for compact and robust multimodal sentiment analysis with sentiment word substitution error. <em>TAFFC</em>, <em>16</em>(3), 1265-1276. (<a href='https://doi.org/10.1109/TAFFC.2024.3490694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Sentiment Analysis (MSA) seeks to fuse textual, acoustic, and visual information to predict a speaker’s sentiment states effectively. However, in real-world scenarios, the text modality received by MSA systems is often obtained through automatic speech recognition (ASR) models. Unfortunately, ASR may erroneously recognize sentiment words as phonetically similar neutral alternatives, leading to sentiment degradation in text and impacting MSA accuracy. Recent attempts aim to first identify the sentiment word substitution (SWS) error in ASR results and then refine the corrupted word embeddings using multimodal information for final multimodal fusion. However, such a method includes a burdensome and ambiguous detection operation and ignores the inherent correlations and heterogeneity among different modalities. To address these issues, we propose a more compact system, termed ARF-MSA consisting of three key components to achieving robust MSA with SWS errors: 1) Alignment: we establish connections between the “text-acoustic’ and “text-visual” representations to effectively map the “text-acoustic-visual” data into a unified sentiment space by leveraging their multimodal correlation knowledge; 2) Refinement: we perform fine-grained comparisons between the text modality and the other two modalities in the unified sentiment space, enabling refinement of the sentiment expression within the text modality more concisely; 3) Fusion: Finally, we hierarchically fuse the dominant and non-dominant representation from three heterogeneity modalities to obtain the multimodal feature for MSA. We conduct extensive experiments on the real-world datasets and the results demonstrate the effectiveness of our model.},
  archive      = {J_TAFFC},
  author       = {Qiyuan Sun and Haolin Zuo and Rui Liu and Haizhou Li},
  doi          = {10.1109/TAFFC.2024.3490694},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1265-1276},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Connecting cross-modal representations for compact and robust multimodal sentiment analysis with sentiment word substitution error},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial extremely low-resource autonomous affective learning. <em>TAFFC</em>, <em>16</em>(3), 1261-1264. (<a href='https://doi.org/10.1109/TAFFC.2025.3589682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAFFC},
  author       = {Xinzhou Xu and Björn W. Schuller and Elisabeth André and Erik Cambria},
  doi          = {10.1109/TAFFC.2025.3589682},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1261-1264},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Guest editorial extremely low-resource autonomous affective learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Corrigendum to “ARAUS: A large-scale dataset and baseline models of affective responses to augmented urban soundscapes”. <em>TAFFC</em>, <em>16</em>(2), 1260. (<a href='https://doi.org/10.1109/TAFFC.2024.3435997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Presents corrections to the article “ARAUS: A Large-Scale Dataset and Baseline Models of Affective Responses to Augmented Urban Soundscapes”.},
  archive      = {J_TAFFC},
  author       = {Kenneth Ooi and Zhen-Ting Ong and Karn N. Watcharasupat and Bhan Lam and Joo Young Hong and Woon-Seng Gan},
  doi          = {10.1109/TAFFC.2024.3435997},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1260},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Corrigendum to “ARAUS: A large-scale dataset and baseline models of affective responses to augmented urban soundscapes”},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding musical valence and arousal: Exploring the neural correlates of music-evoked emotions and the role of expressivity features. <em>TAFFC</em>, <em>16</em>(2), 1247-1259. (<a href='https://doi.org/10.1109/TAFFC.2024.3507192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music conveys both basic emotions, like joy and sadness, and complex ones, such as tenderness and nostalgia. Its effects on emotion regulation and reward have attracted much research attention, as the neural correlates of music-evoked emotions may inform neurorehabilitation interventions. Here, we used fMRI to decode and examine the neural correlates of perceived valence and arousal in music excerpts. Twenty participants were scanned while listening to 96 music excerpts, classified beforehand into four categories varying in valence and arousal. Music modulated activity in cortical regions, most noticeably in music-specific subregions of the auditory cortex, thalamus, and regions of the reward network such as the amygdala. Using multivoxel pattern analysis, we created a computational model to decode the perceived valence and arousal of the music excerpts with above-chance accuracy. We further explored associations between musical features and brain activity in valence-, arousal-, reward-, and auditory-related networks. The results emphasize the involvement of distinct musical features, notably expressive features such as vibrato and tonal and spectral dissonance in valence, arousal, and reward brain networks. Using ecologically valid music stimuli, we contribute to delineating the neural correlates of music-evoked emotions with potential implications in the development of novel music-based neurorehabilitation strategies.},
  archive      = {J_TAFFC},
  author       = {Alexandre Sayal and Ana Gabriela Guedes and Inês Almeida and Daniela Jardim Pereira and César F. Lima and Renato Panda and Rui Pedro Paiva and Teresa Sousa and Miguel Castelo-Branco and Inês Bernardino and Bruno Direito},
  doi          = {10.1109/TAFFC.2024.3507192},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1247-1259},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Decoding musical valence and arousal: Exploring the neural correlates of music-evoked emotions and the role of expressivity features},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contribution of EEG signals for students’ stress detection. <em>TAFFC</em>, <em>16</em>(2), 1235-1246. (<a href='https://doi.org/10.1109/TAFFC.2024.3503995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stress is a prevalent global concern impacting individuals across various life aspects. This paper investigates stress detection using electroencephalographic (EEG) signals, which have proven valuable for studying neural correlates of stress. Stress was induced in students, and physiological data was recorded as part of the experimental setup. Different feature sets were extracted and four machine learning models, including LightGBM, Convolutional Neural Network (CNN), K-Nearest Neighbors (KNN), and Support Vector Machine (SVM), were utilized for classification tasks. The findings indicate that the mean and standard deviation of 19 channels consistently outperform other feature sets. LightGBM demonstrates superior performance across all scenarios compared to CNN, KNN, and SVM. Overall, this study presents an effective stress detection approach using EEG signals and demonstrates the potential of integrating simple statistical features for enhanced classification accuracy. The findings contribute to the advancement of stress monitoring technologies, with potential applications in wearables and BCIs for real-time stress management.},
  archive      = {J_TAFFC},
  author       = {Jonah Fernandez and Raquel Martínez and Bianca Innocenti and Beatriz López},
  doi          = {10.1109/TAFFC.2024.3503995},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1235-1246},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Contribution of EEG signals for students’ stress detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The ForDigitStress dataset: A multi-modal dataset for automatic stress recognition. <em>TAFFC</em>, <em>16</em>(2), 1219-1234. (<a href='https://doi.org/10.1109/TAFFC.2024.3501400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a multi-modal stress dataset that uses digital job interviews to induce stress. The dataset provides multi-modal data of 40 participants including audio, video (motion capturing, facial landmarks, eye tracking), as well as physiological information (photoplethysmography, electrodermal activity). In addition to that, the dataset contains time-continuous annotations for stress and occurred emotions (e.g., shame, anger, anxiety, and surprise). In order to establish a baseline, five different machine learning classifiers (Support Vector Machine, K-Nearest Neighbors, Random Forest, Feed-forward Neural Network, and Long-Short-Term Memory Network) have been trained and evaluated on the presented dataset for a binary stress classification task. The best-performing classifier has been a Long-Short-Term Memory Network, which achieved an accuracy of 91.7% and an F1-score of 90.2%. The ForDigitStress dataset is freely available to other researchers.},
  archive      = {J_TAFFC},
  author       = {Alexander Heimerl and Pooja Prajod and Silvan Mertes and Tobias Baur and Matthias Kraus and Ailin Liu and Helen Risack and Nicolas Rohleder and Elisabeth André and Linda Becker},
  doi          = {10.1109/TAFFC.2024.3501400},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1219-1234},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {The ForDigitStress dataset: A multi-modal dataset for automatic stress recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Individual-aware attention modulation for unseen speaker emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 1205-1218. (<a href='https://doi.org/10.1109/TAFFC.2024.3498937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In practical human-computer interaction (HCI) applications, robust speech emotion recognition (SER) for unseen speakers is crucial. Prior research has primarily focused on extracting common representations to enhance the generalization of cross-individual SER. However, most methods ignore the positive effects of individual characteristics. Actually, each speaker can be regarded as an independent individual domain. Personalized SER can be improved if the emotional expressions of individual speech characteristics are effectively utilized. To address the challenges in recognizing emotions for unseen speakers, this paper proposes a novel individual-aware attention modulation (IAM) model. Specifically, the IAM uses meta-learning techniques to extract modulation parameters for obtaining individual-related emotion expressions from individual characteristics. The base model is then modulated to facilitate the transfer of the common emotion representation space to an individual-specific emotion representation space. This transformation is achieved by applying attention modulation within the transformer-based model developed in this paper. In addition, we employ a meta-learning-based method to optimize model parameters, enhancing the adaptability of the model to unseen speakers, and a control factor is introduced to regulate the degree of individual modulation, thus enhancing the robustness of the modulation process. Experimental results demonstrate that the proposed model achieves significantly improved cross-individual SER performance.},
  archive      = {J_TAFFC},
  author       = {Yuanbo Fang and Xiaofen Xing and Zhaojie Chu and Yifeng Du and Xiangmin Xu},
  doi          = {10.1109/TAFFC.2024.3498937},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1205-1218},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Individual-aware attention modulation for unseen speaker emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG-based cross-subject emotion recognition using sparse bayesian learning with enhanced covariance alignment. <em>TAFFC</em>, <em>16</em>(2), 1190-1204. (<a href='https://doi.org/10.1109/TAFFC.2024.3497897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG (Electroencephalography)-based emotion recognition has emerged as a crucial area of research due to its potential applications in mental health, brain-computer interfaces (BCIs), and affective computing. However, the inherent variability in EEG signals across individuals, coupled with limited dataset sizes, significantly hinders the development of robust and generalizable emotion recognition models. To overcome these challenges, we propose the Sparse Bayesian Learning with Enhanced Covariance Alignment (SBLECA) algorithm. SBLECA formulates cross-subject emotion recognition as an end-to-end decoding problem, integrating spatiotemporal filtering and classification within a sparse Bayesian learning (SBL) framework. Crucially, SBLECA incorporates a novel covariance alignment technique to mitigate inter-subject variability in EEG patterns. Rigorous evaluations on two publicly available emotion datasets demonstrate that SBLECA consistently outperforms state-of-the-art methods. Furthermore, SBLECA offers valuable insights into the neural correlates of emotion through interpretable visualizations of learned spatial and temporal filters. SBLECA holds promise as a valuable EEG decoding tool to advance the development and translation of neurotechnologies and biomarkers for brain disorders.},
  archive      = {J_TAFFC},
  author       = {Wenlong Wang and Feifei Qi and Weichen Huang and Yuanqing Li and Zhuliang Yu and Wei Wu},
  doi          = {10.1109/TAFFC.2024.3497897},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1190-1204},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {EEG-based cross-subject emotion recognition using sparse bayesian learning with enhanced covariance alignment},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A low-rank matching attention based cross-modal feature fusion method for conversational emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 1177-1189. (<a href='https://doi.org/10.1109/TAFFC.2024.3498443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational emotion recognition (CER) is an important research topic in human-computer interactions. Although recent advancements in transformer-based cross-modal fusion methods have shown promise in CER tasks, they tend to overlook the crucial intra-modal and inter-modal emotional interaction or suffer from high computational complexity. To address this, we introduce a novel and lightweight cross-modal feature fusion method called Low-Rank Matching Attention Method (LMAM). LMAM effectively captures contextual emotional semantic information in conversations while mitigating the quadratic complexity issue caused by the self-attention mechanism. Specifically, by setting a matching weight and calculating inter-modal features attention scores row by row, LMAM requires only one-third of the parameters of self-attention methods. We also employ the low-rank decomposition method on the weights to further reduce the number of parameters in LMAM. As a result, LMAM offers a lightweight model while avoiding overfitting problems caused by a large number of parameters. Moreover, LMAM is able to fully exploit the intra-modal emotional contextual information within each modality and integrates complementary emotional semantic information across modalities by computing and fusing similarities of intra-modal and inter-modal features simultaneously. Experimental results verify the superiority of LMAM compared with other popular cross-modal fusion methods on the premise of being more lightweight. Also, LMAM can be embedded into any existing state-of-the-art CER methods in a plug-and-play manner, and can be applied to other multi-modal recognition tasks, e.g., session recommendation and humour detection, demonstrating its remarkable generalization ability.},
  archive      = {J_TAFFC},
  author       = {Yuntao Shou and Huan Liu and Xiangyong Cao and Deyu Meng and Bo Dong},
  doi          = {10.1109/TAFFC.2024.3498443},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1177-1189},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A low-rank matching attention based cross-modal feature fusion method for conversational emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse emotion dictionary and CWT spectrogram fusion with multi-head self-attention for depression recognition in parkinson's disease patients. <em>TAFFC</em>, <em>16</em>(2), 1159-1176. (<a href='https://doi.org/10.1109/TAFFC.2024.3498009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is prevalent in patients with Parkinson's disease (PD), due to the dramatic negative impact that behavioral disorders have on daily life. Regrettably, most researchers in the past ignored the study of depression in PD patients, especially when depressive symptoms and PD symptoms are coupled together, it is difficult for researchers to recognize depression from the macro physiological signs of PD patients. Researchers are increasingly turning their attention to the subtle phenomena of emotional expression in conversation, using the textual and spectral features extracted from the audio of interviews as the primary support for understanding emotional states. However, there is still a lack of effective technical means to fuse these two features to recognize depression in PD patients. In this study, we proposed an innovative image fusion approach, fusing a sparse emotion dictionary with textual features and a Continuous Wavelet Transform (CWT) spectrogram with spectral features for the precise recognition of depression in PD patients. The fusion process integrates low-dimensional emotion-related textual cues, contributing to a more comprehensive extraction of emotionally relevant information. Subsequently, we introduce a High and Low Frequency Feature Fusion Multi-headed Self-Attention (HL-MSA) mechanism within a high and low frequency feature fusion network to amalgamate information across different frequency features within the images. The results underscore the efficacy of this novel fusion approach in effectively extracting depressive features in PD patients, attaining advanced recognition performance. Notably, this endeavor represents a pioneering stride in seamlessly fusing a sparse emotion dictionary and CWT spectrogram, exemplifying a promising and effective initiative for recognizing depression in PD patients.},
  archive      = {J_TAFFC},
  author       = {Jian Li and Yuliang Zhao and Yinghao Liu and Huawei Zhang and Peng Shan and Yuanyi Wu and Wanyue Wang and Yulin Wang},
  doi          = {10.1109/TAFFC.2024.3498009},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1159-1176},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Sparse emotion dictionary and CWT spectrogram fusion with multi-head self-attention for depression recognition in parkinson's disease patients},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stimulus-response pattern: The core of robust cross-stimulus facial depression recognition. <em>TAFFC</em>, <em>16</em>(2), 1146-1158. (<a href='https://doi.org/10.1109/TAFFC.2024.3496524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial depression recognition is one of the current hot topics. Mainstream methods mainly focus on how to design deep models to effectively extract the difference in facial movements between depressed patients and healthy people. However, this difference changes when the stimulus source to which the subjects are exposed changes. This leads to the performance degradation in cross-stimulus situation and limits the practical application of this technology. We hold the opinion that why depressed patients show behavioral characteristics different from healthy people is that they have a specific stable pattern of responding to stimulus. Therefore, we incorporate stimuli into the modeling process for the first time and employ deep networks to learn stable representations between stimulus and response to achieve stable and effective modeling. Specifically, we propose a deep modeling framework to learn the stimulus-response pattern of the subject through the interaction relationship between the stimulus videos and the subject’s facial movements. We constructed a balanced depression dataset of 364 individuals with three different stimulus videos to verify the effectiveness of our method. The results show that our method achieves state-of-the-art and the best generalization performance in depression recognition. This stimulus-response pattern modeling provides a new perspective for recognizing depression.},
  archive      = {J_TAFFC},
  author       = {Zhenyu Liu and Shimao Zhang and Bailin Chen and Gang Li and Qiongqiong Chen and Zhijie Ding and Xin Zhang and Bin Hu},
  doi          = {10.1109/TAFFC.2024.3496524},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1146-1158},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Stimulus-response pattern: The core of robust cross-stimulus facial depression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging social media for real-time interpretable and amendable suicide risk prediction with human-in-the-loop. <em>TAFFC</em>, <em>16</em>(2), 1128-1145. (<a href='https://doi.org/10.1109/TAFFC.2024.3494860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suicide presents a global health challenge, prompting the development of diverse prevention strategies. Among them, timely identification of individuals at risk of suicide remains challenging. Although social media offers potential for tracking users’ mental status, harnessing collaboration between AI and human experts for real-time prediction of suicide risk is inadequately explored. This study presents a human-in-the-loop framework for real-time suicide risk prediction based on social media. Once a user made a new post on social media, the AI model assesses user’s suicide risk within the next month with explanation based on the historic and new posts plus domain knowledge. Human experts on the other side look into the explanation to confirm/clarify uncertain information as feedback, enabling consistent evolution of the model. Experiments on the constructed dataset, containing 66 suicidal users and 66 non-suicidal users, show that our method achieved 82.58% prediction accuracy, outperforming competitive baselines by 6.57%. Leveraging human feedback improved prediction accuracy by 4.12%. Consultation with 18 experts (including 6 medical staff and 12 psychologists) was conducted to examine the validity of our method. Ethics considerations, as well as potential and limitations of large language models in mental condition prediction, are also discussed at the end of the paper.},
  archive      = {J_TAFFC},
  author       = {Yi Dai and Jinlei Liu and Lei Cao and Yuanyuan Xue and Xin Wang and Yang Ding and Junrui Tian and Ling Feng},
  doi          = {10.1109/TAFFC.2024.3494860},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1128-1145},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Leveraging social media for real-time interpretable and amendable suicide risk prediction with human-in-the-loop},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affect in spatial navigation: A study of rooms. <em>TAFFC</em>, <em>16</em>(2), 1117-1127. (<a href='https://doi.org/10.1109/TAFFC.2024.3493761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How do spaces make us feel? What is the perceived emotional impact of built form? This study proposes a framework to identify and model the effects that our perceived environment can have by taking into consideration illumination and structural form while acknowledging its temporal dimension. To study this, we recruited 100 participants via a crowd-sourcing platform in order to annotate their perceived arousal or pleasure shifts while watching videos depicting spatial navigation in first person view. Participants’ annotations were recorded as time-continuous unbounded traces, allowing us to extract ordinal labels about how their arousal or pleasure fluctuated as the camera moved between different rooms. Given the subjective nature of the task and the noisy signals from real-time annotation, a number of processing steps are applied in order to convert the data into ordinal relationships between affect metrics in different rooms. Experiments with random forests and other classifiers show that, with the right treatment and data cleanup, simple interior design features can be adequate predictors of human arousal and pleasure changes over time. The dataset is made available in order to prompt exploration of additional modalities as input and ground truth extraction.},
  archive      = {J_TAFFC},
  author       = {Emmanouil Xylakis and Antonios Liapis and Georgios N. Yannakakis},
  doi          = {10.1109/TAFFC.2024.3493761},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1117-1127},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Affect in spatial navigation: A study of rooms},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MECA: Manipulation with emotional intensity-aware contrastive learning and attention-based discriminative learning. <em>TAFFC</em>, <em>16</em>(2), 1104-1116. (<a href='https://doi.org/10.1109/TAFFC.2024.3493416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With recent developments in deep learning, facial expression manipulation (FEM) has become one of the fields receiving great attention. However, many studies focus on learning without considering class distinction in latent space. This paper introduces a representation learning scheme that leverages self-attention and mutual information to effectively account for semantic attributes, such as facial expressions, in the FEM task. Our framework, utilizing attention-based discriminative learning and emotional intensity-aware contrastive learning, is capable of forming a compact embedding space. This compact embedding space can lead to more discerning and richer facial expression synthesis in actual synthesis results. As a result, we have derived facial expression synthesis results that are superior to the previous methods. Also, in terms of the FED metric, which can quantify the degree of facial expression expression in FEM, the proposed method outperforms the other methods. To demonstrate this successful result, we use t-SNE and visualize the actual embedding results for each class. Furthermore, we prove that the latent space formed through the proposed method is also helpful in terms of facial expression recognition.},
  archive      = {J_TAFFC},
  author       = {Seongho Kim and Byung Cheol Song},
  doi          = {10.1109/TAFFC.2024.3493416},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1104-1116},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MECA: Manipulation with emotional intensity-aware contrastive learning and attention-based discriminative learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Create a crowd emotion detection framework with ecological validity. <em>TAFFC</em>, <em>16</em>(2), 1087-1103. (<a href='https://doi.org/10.1109/TAFFC.2024.3492262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ecological validity remains essential for generalizing scientific research into real-world applications. However, current methods for crowd emotion detection lack ecological validity due to limited diversity samples in datasets. This paper proposes a crowd emotion detection framework that improves the ecological validity of models from both dataset and methodological perspectives. Firstly, we develop an Emotional Crowd Generator script within Grand Theft Auto V to generate a large-scale and diverse synthetic emotional crowd dataset, named Emotional-GTA (E-GTA). Secondly, we utilize prior features to enhance the model's generalization ability, especially for rare samples. Building on this, we introduce a dual-driven Graph-based Prior Feature and Image Fusion Network (GPIFN), which further strengthens our model's ecological validity from a methodological perspective. We propose a graphical representation that effectively constructs the Crowd Image Graph (CIG) and the Crowd Prior Features Graph (CPG). The CIG represents crowds from the perspective of the image features, while the CPG represents them from the perspective of prior features. We then design a dual-stream network GPIFN that extracts image features from the CIG and prior features from the CPG. Additionally, we design an Image and Prior Features Fusion Module (IPFM) that efficiently merges image and prior features while maintaining the original stream features. Our experiments demonstrate that both E-GTA and GPIFN greatly enhance ecological validity in real-world scenarios. Our framework achieves state-of-the-art results on real-world datasets: UMN and Violent-Flows.},
  archive      = {J_TAFFC},
  author       = {Xiao Chen and Zhen Liu and Tingting Liu and Jiangjian Xiao},
  doi          = {10.1109/TAFFC.2024.3492262},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1087-1103},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Create a crowd emotion detection framework with ecological validity},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture of hybrid prompts for cross-domain aspect sentiment triplet extraction. <em>TAFFC</em>, <em>16</em>(2), 1074-1086. (<a href='https://doi.org/10.1109/TAFFC.2024.3487870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain Aspect Sentiment Triplet Extraction (ASTE) aims to extract the triplets from the review of a target domain, utilizing knowledge from a source domain. As a newly proposed task, limited work has been devoted to it. Except for solving it in a zero-shot manner with in-domain models, recent work explores a bidirectional generative framework to generate pseudo-labeled target data. However, such a method suffers from low efficiency with two-stage training and unstable pseudo-label quality. In this paper, we propose a Hybrid Prompts Mixture (HiPM) method for cross-domain ASTE to fully utilize domain-independent knowledge. Within this method, given that syntax information is an essential linguistic feature for triplet extraction, we design a syntax-related hard prompt to transfer the structures. Additionally, aspects from different domains exhibit similarities in their respective categories. We take this shared information as the prototypes and enrich them through a warm-up step. The resulting prototypes then act as the source of soft prompts. We further mix the hard and soft prompts with the original sequence into a generative model to extract triplets. Experimental results show that our method outperforms baselines on twelve transfer pairs, and obtains a 1.48% average F1 score improvement over the state-of-the-art cross-domain ASTE model.},
  archive      = {J_TAFFC},
  author       = {Fan Yang and Xiabing Zhou and Min Zhang and Guodong Zhou},
  doi          = {10.1109/TAFFC.2024.3487870},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1074-1086},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Mixture of hybrid prompts for cross-domain aspect sentiment triplet extraction},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic functional spectrum analysis: A framework for exploring the multifunctional interplay among multimodal nonverbal behaviours in conversations. <em>TAFFC</em>, <em>16</em>(2), 1056-1073. (<a href='https://doi.org/10.1109/TAFFC.2024.3491097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel framework named the synergistic functional spectrum analysis (sFSA) is proposed to explore the multifunctional interplay among multimodal nonverbal behaviours in human conversations. This study aims to reveal how multimodal nonverbal behaviours cooperatively perform communicative functions in conversations. To capture the intrinsic nature of nonverbal expressions, functional multiplicity, and interpretational ambiguity, e.g., a single head nod could imply listening, agreeing, or both, a novel concept named the functional spectrum, which is defined as the distribution of perceptual intensities of multiple functions by multiple observers, is introduced in the sFSA. Based on this concept, this paper presents functional spectrum corpora, which target 44 facial expression and 32 head movement functions. Then, spectrum decomposition is conducted to reduce the multimodal functional spectrum to a synergetic functional spectrum in a lower dimension functional space that is spanned by functional basis vectors representing primary and distinctive functionalities across multiple modalities. To that end, we propose a semiorthogonal nonnegative matrix factorization (SO-NMF) method, which assumes the additivity of multiple functions and aims to balance the distinctiveness and expressiveness of the factorization. The results confirm that some primary functional bases can be identified, which can be interpreted as the listener’s backchannel, thinking, and affirmative response functions, and the speaker’s thinking and addressing functions, and their positive emotion functions. In addition, regression models based on convolutional neural networks (CNNs) are presented to estimate the synergistic functional spectrum from the head poses and facial action units measured from conversation data. The results of these analyses and experiments confirm the potential of the sFSA and may lead to future extensions.},
  archive      = {J_TAFFC},
  author       = {Mai Imamura and Ayane Tashiro and Shiro Kumano and Kazuhiro Otsuka},
  doi          = {10.1109/TAFFC.2024.3491097},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1056-1073},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Synergistic functional spectrum analysis: A framework for exploring the multifunctional interplay among multimodal nonverbal behaviours in conversations},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class activation regularization-based facial emotion recognition network and its application in students’ emotional engagement assessment. <em>TAFFC</em>, <em>16</em>(2), 1044-1055. (<a href='https://doi.org/10.1109/TAFFC.2024.3491106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Students’ emotional engagement in the classroom can be evaluated through their facial emotions. However, facial emotion recognition remains challenging due to the uncertainty problem, which is routinely caused by factors such as low resolution, intra-class variability, and inter-class similarity. To deal with this problem, we propose a class activation regularization network, which includes two main modules: class activation dropping module (CADM) and hard sample mining module (HSMM). Our key idea is to perform consistency learning between the original images and class-uncertain regions, thereby enabling models to perceive uncertainty of emotional semantics. In CADM, the class-specific attention is obtained using class activation mapping. Then, the attention is partly discarded by imitating dropout mechanism to obtain class-uncertain regions. In HSMM, the hard samples with high uncertainties are identified based on the class-uncertain regions and confidence thresholds. For the hard samples, regularization loss is applied to align the probability distributions between the original images and the class-uncertain regions, thereby improving the network’s awareness of emotional semantics and its ability to handle uncertainty. Extensive experiments on three datasets demonstrate the competitiveness of the proposed method. In addition, our method can be deployed in the classroom for students’ emotional engagement assessment.},
  archive      = {J_TAFFC},
  author       = {Luhui Xu and Yanling Gan and Yi Jin},
  doi          = {10.1109/TAFFC.2024.3491106},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1044-1055},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Class activation regularization-based facial emotion recognition network and its application in students’ emotional engagement assessment},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpreting stress detection models using SHAP and attention for MuSe-stress 2022. <em>TAFFC</em>, <em>16</em>(2), 1031-1043. (<a href='https://doi.org/10.1109/TAFFC.2024.3488112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding emotional reactions, especially stress, during job interviews holds significant implications for assessing the well-being of candidates and tailoring feedback. However, current techniques, though effective, often lack interpretability. In this study, we investigate emotion recognition by focusing on making sense of machine-learning models. Specifically, our work leverages the power of interpretable methods in detecting stress through multimodal time series. Building upon prior research, our main contribution is a novel method for calculating feature importance scores using Shapley Additive exPlanations (SHAP) and attention. We applied this technique to models from the MuSe 2022 stress detection competition, generating insights into the importance and interplay of various features in Arousal or Valence prediction. Our findings suggest that leveraging SHAP for feature selection can enhance prediction effectiveness while mitigating computational demands. With this, we introduce an advanced, interpretable paradigm for multi-modal emotion recognition in practical stress-detection scenarios.},
  archive      = {J_TAFFC},
  author       = {Ho-Min Park and Ganghyun Kim and Jinsung Oh and Arnout Van Messem and Wesley De Neve},
  doi          = {10.1109/TAFFC.2024.3488112},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1031-1043},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Interpreting stress detection models using SHAP and attention for MuSe-stress 2022},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REMAST: Real-time emotion-based music arrangement with soft transition. <em>TAFFC</em>, <em>16</em>(2), 1016-1030. (<a href='https://doi.org/10.1109/TAFFC.2024.3486224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music as an emotional intervention media has important applications in scenarios such as music therapy, games, and movies. However, music needs real-time arrangement according to changing emotions, bringing challenges to balance emotion real-time fit and soft emotion transition due to the fine-grained and mutable nature of the target emotion. Existing studies mainly focus on achieving emotion real-time fit, while the issue of smooth transition remains understudied, affecting the overall emotional coherence of the music. In this paper, we propose REMAST to address this trade-off. Specifically, we recognize the last timestep's music emotion and fuse it with the current timestep's input emotion. The fused emotion then guides REMAST to generate the music based on the input melody. To adjust music similarity and emotion real-time fit flexibly, we downsample the original melody and feed it into the generation model. Furthermore, we design four music theory features by domain knowledge to enhance emotion information and employ semi-supervised learning to mitigate the subjective bias introduced by manual dataset annotation. According to the evaluation results, REMAST surpasses the state-of-the-art methods in objective and subjective metrics. These results demonstrate that REMAST achieves real-time fit and smooth transition simultaneously, enhancing the coherence of the generated music.},
  archive      = {J_TAFFC},
  author       = {Zihao Wang and Le Ma and Chen Zhang and Bo Han and Yunfei Xu and Yikai Wang and Xinyi Chen and Haorong Hong and Wenbo Liu and Xinda Wu and Kejun Zhang},
  doi          = {10.1109/TAFFC.2024.3486224},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {1016-1030},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {REMAST: Real-time emotion-based music arrangement with soft transition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient bi-modal fusion framework for music emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 999-1015. (<a href='https://doi.org/10.1109/TAFFC.2024.3486340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current methods for Music Emotion Recognition (MER) face challenges in effectively extracting features sensitive to emotions, especially those rich in temporal detail. Moreover, the narrow scope of music-related modalities impedes data integration from multiple sources, while including multiple modalities often leads to redundant information, which can degrade performance. To address these issues, we propose a lightweight framework for music emotion recognition that improves the extraction of features that are both sensitive to emotions and rich in temporal information and that integrates data from both audio and MIDI modalities while minimizing redundancy. Our approach involves developing two innovative unimodal encoders to learn embeddings from audio and MIDI-like features. Additionally, we introduce a Bi-modal Fusion Attention Model (BFAM) that integrates features from low-level to high-level semantic information across different modalities. Experimental evaluations on the EMOPIA and VGMIDI datasets show that our unimodal networks achieve accuracies that are 6.1% and 4.4% higher than baseline algorithms for MIDI and audio on the EMOPIA dataset, respectively. Furthermore, our BFAM achieves a 15.2% improvement in accuracy over the baseline, reaching 82.2%, which underscores its effectiveness for bi-modal MER applications.},
  archive      = {J_TAFFC},
  author       = {Yao Xiao and Haoxin Ruan and Xujian Zhao and Peiquan Jin and Li Tian and Zihan Wei and Xuebo Cai and Yixin Wang and Liang Liu},
  doi          = {10.1109/TAFFC.2024.3486340},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {999-1015},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {An efficient bi-modal fusion framework for music emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring affective peripheral patterns based on body surface potentials with covariance. <em>TAFFC</em>, <em>16</em>(2), 986-998. (<a href='https://doi.org/10.1109/TAFFC.2024.3486165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective patterns based on physiological signals reflect bodily changes linked to specific emotional states. Previous studies on the cardiac electrical signal, a key peripheral physiological signal, were limited by the measurement density of single-lead ECG signal, focusing solely on temporal pattern analysis but ignoring topographic pattern analysis that can reflect the body's emotional response. Our research advances affective peripheral pattern studies by innovatively using body surface potentials to comprehensively monitor cardiac electrical activity with increased measurement density. To tackle the challenge of extracting spatial and temporal features from multi-channel body surface potentials, we establish a dynamic correlation among these diverse channel signals through covariance matrices. Our hypothesis is that the dynamic inter-channel relationship provides a valuable source of insights into emotional clues. Experimental results demonstrate that the extracted spatial and temporal features effectively capture topographic and temporal patterns from cardiac electrical signals, and achieve excellent performance in classification tasks simultaneously. Our finding reveals affective patterns based on body surface potentials for the first time, offering novel insights into affective peripheral patterns analysis.},
  archive      = {J_TAFFC},
  author       = {Wei Wu and Yao Pi and Xianbin Zhang and Lin Xu and Wanqing Wu},
  doi          = {10.1109/TAFFC.2024.3486165},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {986-998},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Exploring affective peripheral patterns based on body surface potentials with covariance},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SEED-VII: A multimodal dataset of six basic emotions with continuous labels for emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 969-985. (<a href='https://doi.org/10.1109/TAFFC.2024.3485057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing emotions from physiological signals is a topic that has garnered widespread interest, and research continues to develop novel techniques for perceiving emotions. However, the emergence of deep learning has highlighted the need for comprehensive and high-quality emotional datasets that enable the accurate decoding of human emotions. To systematically explore human emotions, we develop a multimodal dataset consisting of six basic (happiness, sadness, fear, disgust, surprise, and anger) emotions and the neutral emotion, named SEED-VII. This multimodal dataset includes electroencephalography (EEG) and eye movement signals. The seven emotions in SEED-VII are elicited by 80 different videos and fully investigated with continuous labels that indicate the intensity levels of the corresponding emotions. Additionally, we propose a novel Multimodal Adaptive Emotion Transformer (MAET), that can flexibly process both unimodal and multimodal inputs. Adversarial training is utilized in the MAET to mitigate subject discrepancies, which enhances domain generalization. Our extensive experiments, encompassing both subject-dependent and cross-subject conditions, demonstrate the superior performance of the MAET in terms of handling various inputs. Continuous labels are used to filter the data with high emotional intensity, and this strategy is proven to be effective for attaining improved emotion recognition performance. Furthermore, complementary properties between the EEG signals and eye movements and stable neural patterns of the seven emotions are observed.},
  archive      = {J_TAFFC},
  author       = {Wei-Bang Jiang and Xuan-Hao Liu and Wei-Long Zheng and Bao-Liang Lu},
  doi          = {10.1109/TAFFC.2024.3485057},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {969-985},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SEED-VII: A multimodal dataset of six basic emotions with continuous labels for emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mutual information of crossmodal utterance representation for multimodal sentiment analysis. <em>TAFFC</em>, <em>16</em>(2), 960-968. (<a href='https://doi.org/10.1109/TAFFC.2024.3466968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the continuous progress of Internet technology and social networks, content sharing on social platforms that reflects personal feelings and emotions has proliferated. Consequently, the study of people’s emotions has gained considerable popularity because of the expanding use of social media, which has provided enormous data for data-driven research and more attention on the psychological health of people nowadays. To deepen the analysis of people’s emotions on the internet, Multimodal Sentiment Analysis(MSA) combines multiple modalities such as texts, images, and sounds to comprehensively analyze and assess an individual’s emotional state. However, ignoring the relationship between different modalities, most of the previous multimodal sentiment analysis models were limited to feature extraction of a single modal that cannot precisely predict object’s state of mind. In this article, we propose a framework called Mutual Infomax Utterance Representation (MIUR) which draws on the concept of mutual information in information theory and introduces an information exchange module between modalities, effectively filters out task independent random noise while preserving shared information across modalities as much as possible. We conducted experimental verification on the publicly available popular sentiment datasets MOSI and MOSEI, and the results showed that our model demonstrated significant advancements in contrast to existing advanced models.},
  archive      = {J_TAFFC},
  author       = {Xufei Yin and Dong Yue and Xiangsen Wei},
  doi          = {10.1109/TAFFC.2024.3466968},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {960-968},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Mutual information of crossmodal utterance representation for multimodal sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GHA: A gated hierarchical attention mechanism for the detection of abusive language in social media. <em>TAFFC</em>, <em>16</em>(2), 946-959. (<a href='https://doi.org/10.1109/TAFFC.2024.3483010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of attention mechanisms in deep learning solutions has become popular within natural language processing tasks. The use of these mechanisms allows managing the relevance of the elements of a sequence in accordance with their context, however, this relevance has been observed independently between the pairs of elements of a sequence (self-attention) or between the application domain of a sequence (contextual attention), leading to the loss of relevant information and limiting the representation of the sequences. To tackle these particular issues, we propose a dual attention mechanism, which trades off the previous limitations, by considering the internal and contextual relationships between the elements of the sequence. Additionally, we propose the extension of the dual attention mechanism into a multi-layer perspective, through the weighted fusion of the different encoding layers of deep architectures. As the interpretation of abusive language is highly context-dependent, its identification is an ideal task to evaluate the proposed attention mechanism. Accordingly, we considered six standard collections for the abusive language identification task. The obtained results are encouraging; the proposed hierarchical attention mechanism outperformed the current self-attention and contextual attention mechanisms coupled with recurrent neural networks and Transformers, as well as, state-of-the-art approaches in detecting abusive language.},
  archive      = {J_TAFFC},
  author       = {Horacio Jarquín-Vásquez and Hugo Jair Escalante and Manuel Montes-y-Gómez and Fabio A. González},
  doi          = {10.1109/TAFFC.2024.3483010},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {946-959},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {GHA: A gated hierarchical attention mechanism for the detection of abusive language in social media},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCARE: A novel framework to enhance chinese harmful memes detection. <em>TAFFC</em>, <em>16</em>(2), 933-945. (<a href='https://doi.org/10.1109/TAFFC.2024.3481419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Harmful meme detection presents a significant multimodal challenge that necessitates contextual background knowledge and comprehensive inference. Although some research studies have been related to harmful meme detection in English, detecting harmful memes in Chinese is also an unresolved issue. In this paper, to bridge this gap, we constructed a Chinese harmful meme detection dataset, named CHMEMES. Furthermore, existing multimodal alignment methods have shown poor performance in tasks involving harmful meme detection, where there is a mismatch between the image and text components. To improve the task, we propose a multimodal framework Semantic Contrastive Alignment fRamEwork (SCARE), which enables fully representing both cross-modal and intra-modal information. For cross-modal information, we introduce a cross-modal contrast alignment objective to maximize the mutual information between image and text. For intra-modal information, we design a new intra-modal contrast objective to achieve more robust visual and textual representation learning. Moreover, we present a simple yet efficient vision prompt tuning paradigm for parameter-efficient harmful meme detection. We conduct extensive experiments on the constructed Chinese dataset and the existing English dataset. Experimental results show that our method outperforms state-of-the-art baselines in harmful meme detection.},
  archive      = {J_TAFFC},
  author       = {Tianlong Gu and Mingfeng Feng and Xuan Feng and Xuemin Wang},
  doi          = {10.1109/TAFFC.2024.3481419},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {933-945},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SCARE: A novel framework to enhance chinese harmful memes detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A residual multi-scale convolutional neural network with transformers for speech emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 915-932. (<a href='https://doi.org/10.1109/TAFFC.2024.3481253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The great variety of human emotional expression as well as the differences in the ways they perceive and annotate them make Speech Emotion Recognition (SER) an ambiguous and challenging task. With the development of deep learning, long-term progress has been made in SER systems. However, the existing convolutional neural networks present certain limitations, such as their inability to well capture global features, which contain important emotional information. Moreover, the position encoding in the Transformer structure is relatively fixed and only encodes the time domain dimension, which cannot effectively obtain the position information of discriminative features in the frequency domain dimension. In order to overtake these limitations, we propose an end-to-end Residual Multi-Scale Convolutional Neural Networks (RMSCNN) with Transformer model network. Simultaneously, to further validate the effectivenessof RMSCNN in extracting multi-scale features and delivering pertinent emotion localization data, we developed the RMSC_down network in conjunction with the Wav2Vec 2.0 model. The results of the prediction of Arousal, Valenceand Dominanceon the popular corpora demonstrate the superiority and robustness of our approach for SER, showing an improvement of the recognition accuracy in the public dataset MSP-Podcast 1.9 version.},
  archive      = {J_TAFFC},
  author       = {Tianhao Yan and Hao Meng and Emilia Parada-Cabaleiro and Jianhua Tao and Taihao Li and Björn W. Schuller},
  doi          = {10.1109/TAFFC.2024.3481253},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {915-932},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A residual multi-scale convolutional neural network with transformers for speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive domain alignment neural networks for cross-domain EEG emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 903-914. (<a href='https://doi.org/10.1109/TAFFC.2024.3480355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) - based Emotion recognition is now facing great challenge of the intra- and inter-subject variability of EEG signal. Researchers attempted to handle this challenge by using transfer learning methods which usually share two main limitations: most of these methods align marginal distributions instead of conditional distributions of source and target data, making the alignment process classwise ambiguous; also, they prefer to use Multi-Layer Perceptron (MLP) with redundant parameters as classifiers, which is shown by recent research that could result serious over-fitting towards labeled data and prevent the model to draw a proper representation space. In our work, we propose a novel domain alignment method: Adaptive Domain Alignment Neural Networks (ADANN). Our method directly model conditional distributions of source and target domains by two sets of label-wise prototypes, representing the density maximum of each class, while the normalized correspond similarity naturally represents the conditional probability. The predicted label for a sample is given by the argument maxima of similarities and therefore the MLP classifier is not required. Using context-instance contrastive learning to align two sets of prototypes, their corresponding conditional distributions are being learned simultaneously. Exhaustive cross-domain experiments have been conducted under protocols that are strongly related to practical application scenarios and our proposed method achieves better or similar performance compared with recent state-of-the-art methods.},
  archive      = {J_TAFFC},
  author       = {Xuezhu Hong and Changde Du and Huiguang He},
  doi          = {10.1109/TAFFC.2024.3480355},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {903-914},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Adaptive domain alignment neural networks for cross-domain EEG emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tracking dynamic flow: Decoding flow fluctuations through performance in a fine motor control task. <em>TAFFC</em>, <em>16</em>(2), 891-902. (<a href='https://doi.org/10.1109/TAFFC.2024.3480309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flow, an optimal mental state merging action and awareness, significantly impacts our emotion, performance, and well-being. However, capturing its swift transitions on a fine timescale is challenging due to the sparsity of the existing flow detecting tools. Here we present a fine fingertip force control (F3C) task to induce flow, wherein the task challenge is set at a compatible level with personal skill, and to quantitatively track the flow state variations from synchronous motor control performance. We select eight performance metrics from fingertip force sequence and reveal their significant differences under distinct self-reported flow states. Further, we built a machine learning-based decoder that aims to predict the continuous flow intensity during the user experiment through the performance metrics, taking the self-reported flow as the label. Cross-validation shows that the predicted flow intensity reaches significant correlation with the self-reported flow intensity (r = 0.81). Based on the decoding results, we can capture the flow fluctuations during the intervals between sparse self-reporting probes. This study showcases the feasibility of tracking intrinsic flow variations with high temporal resolution using task performance measures and may serve as foundation for future work aiming to take advantage of flow's dynamics to enhance performance and positive emotions.},
  archive      = {J_TAFFC},
  author       = {Bohao Tian and Shijun Zhang and Sirui Chen and Yuru Zhang and Kaiping Peng and Hongxing Zhang and Dangxiao Wang},
  doi          = {10.1109/TAFFC.2024.3480309},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {891-902},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Tracking dynamic flow: Decoding flow fluctuations through performance in a fine motor control task},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The fNIRS-based emotion recognition by spatial transformer and WGAN data augmentation toward developing a novel affective BCI. <em>TAFFC</em>, <em>16</em>(2), 875-890. (<a href='https://doi.org/10.1109/TAFFC.2024.3477302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The affective brain-computer interface (aBCI) facilitates the objective identification or regulation of human emotions. Current aBCI mainly relies on electroencephalography (EEG). However, research shows that emotions involve a large-scale distributed brain network. Compared to electroencephalography (EEG), functional near-infrared spectroscopy (fNIRS) offers a higher spatial resolution. It holds greater potential in capturing emotional spatial information, which may foster the development of new affective Brain-Computer Interfaces (aBCI). We proposed a novel self-attention-based deep-learning transformer language model for fNIRS cross-subject emotion recognition, which could automatically learn the emotion's spatial attention weight information with strong interpretability. Besides, we performed data augmentation by introducing the wasserstein generative adversarial networks (WGAN). Results showed: (1) We achieved 84% three-category cross-subject emotion decoding accuracy. The spatial transformer module and WGAN improved the accuracy by 12.8% and 4.3%, respectively. (2) Compared with cutting-edge fNIRS research, we led by 10% in three-category decoding accuracy. (3) Compared with cutting-edge EEG research, we lead by 28% in arousal decoding accuracy, 10% in valence decoding accuracy, and 2% in three-category decoding accuracy. (4) Besides, our approach holds the potential to uncover the brain's spatial encoding mechanism of human emotion processing, providing a new direction for building interpretable artificial intelligence models.},
  archive      = {J_TAFFC},
  author       = {Xiaopeng Si and He Huang and Jiayue Yu and Dong Ming},
  doi          = {10.1109/TAFFC.2024.3477302},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {875-890},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {The fNIRS-based emotion recognition by spatial transformer and WGAN data augmentation toward developing a novel affective BCI},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STAA-net: A sparse and transferable adversarial attack for speech emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 861-874. (<a href='https://doi.org/10.1109/TAFFC.2024.3475729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech contains rich information on the emotions of humans, and Speech Emotion Recognition (SER) has been an important topic in the area of human-computer interaction. The robustness of SER models is crucial, particularly in privacy-sensitive and reliability-demanding domains like private healthcare. Recently, the vulnerability of deep neural networks in the audio domain to adversarial attacks has become a popular area of research. However, prior works on adversarial attacks in the audio domain primarily rely on iterative gradient-based techniques, which are time-consuming and prone to overfitting the specific threat model. Furthermore, the exploration of sparse perturbations, which have the potential for better stealthiness, remains limited in the audio domain. To address these challenges, we propose a generator-based attack method to generate sparse and transferable adversarial examples to deceive SER models in an end-to-end and efficient manner. We evaluate our method on two widely-used SER datasets, Database of Elicited Mood in Speech (DEMoS) and Interactive Emotional dyadic MOtion CAPture (IEMOCAP), and demonstrate its ability to generate successful sparse adversarial examples in an efficient manner. Moreover, our generated adversarial examples exhibit model-agnostic transferability, enabling effective adversarial attacks on advanced victim models.},
  archive      = {J_TAFFC},
  author       = {Yi Chang and Zhao Ren and Zixing Zhang and Xin Jing and Kun Qian and Xi Shao and Bin Hu and Tanja Schultz and Björn W. Schuller},
  doi          = {10.1109/TAFFC.2024.3475729},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {861-874},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {STAA-net: A sparse and transferable adversarial attack for speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards multimodal prediction of spontaneous humor: A novel dataset and first results. <em>TAFFC</em>, <em>16</em>(2), 844-860. (<a href='https://doi.org/10.1109/TAFFC.2024.3475736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humor is a substantial element of human social behavior, affect, and cognition. Its automatic understanding can facilitate a more naturalistic human-AI interaction. Current methods of humor detection have been exclusively based on staged data, making them inadequate for ‘real-world’ applications. We contribute to addressing this deficiency by introducing the novel Passau-Spontaneous Football Coach Humor (Passau-SFCH) dataset, comprising about 11 hours of recordings. The Passau-SFCH dataset is annotated for the presence of humor and its dimensions (sentiment and direction) as proposed in Martin's Humor Style Questionnaire. We conduct a series of experiments employing pretrained Transformers, convolutional neural networks, and expert-designed features. The performance of each modality (text, audio, video) for spontaneous humor recognition is analyzed and their complementarity is investigated. Our findings suggest that for the automatic analysis of humor and its sentiment, facial expressions are most promising, while humor direction can be best modeled via text-based features. Further, we experiment with different multimodal approaches to humor recognition, including decision-level fusion and MulT, a multimodal Transformer approach. In this context, we propose a novel multimodal architecture that yields the best overall results.},
  archive      = {J_TAFFC},
  author       = {Lukas Christ and Shahin Amiriparian and Alexander Kathan and Niklas Müller and Andreas König and Björn W. Schuller},
  doi          = {10.1109/TAFFC.2024.3475736},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {844-860},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Towards multimodal prediction of spontaneous humor: A novel dataset and first results},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Measuring affective state: Subject-dependent and -independent prediction based on longitudinal multimodal sensing. <em>TAFFC</em>, <em>16</em>(2), 827-843. (<a href='https://doi.org/10.1109/TAFFC.2024.3474098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current sensors offering passive and continuous monitoring of behavioral patterns potentially enable real-time affective state monitoring. Previous research on affective state prediction with multimodal sensing in daily life has shown only small-to-moderate effects. One reason for this limited success might be the large variability across individuals. Current research is often of short duration, preventing proper within-individual modeling. With an extensive longitudinal data collection of nine months, this research focuses on individual-level predictions of valence and arousal in daily life. Sixteen PhD candidates from The Netherlands provided data about their affective states (self-reported valence and arousal), physiology (Oura rings) and behavioral patterns (AWARE framework for mobile phone data). Supporting our hypothesis, subject-dependent random forest (RF) models significantly outperformed subject-independent leave-one-subject-out (LOSO) models in predicting self-reported valence and arousal. The subject-dependent models achieved an average Spearman's rho correlation of 0.30 [0.14-0.60] for valence and 0.36 [0.16-0.69] for arousal. In many cases, participants’ a priori indicated informative sources matched with the feature importance. Making use of participants’ self-knowledge might thus help to reduce the amount of data to be collected. For future work, longer-term changes in affective state and combinations of features for estimating real behavioral patterns should be explored.},
  archive      = {J_TAFFC},
  author       = {Lea Berkemeier and Wim Kamphuis and Anne-Marie Brouwer and Herman de Vries and Maarten Schadd and Jan Ubbo van Baardewijk and Hilbrand Oldenhuis and Rudolf Verdaasdonk and Lisette van Gemert-Pijnen},
  doi          = {10.1109/TAFFC.2024.3474098},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {827-843},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Measuring affective state: Subject-dependent and -independent prediction based on longitudinal multimodal sensing},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial action unit recognition enhanced by text descriptions of FACS. <em>TAFFC</em>, <em>16</em>(2), 814-826. (<a href='https://doi.org/10.1109/TAFFC.2024.3470524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the descriptions of facial action units (AUs) provide crucial semantic knowledge for representation learning from facial images, they have not been fully explored for facial action unit recognition. In this paper, we propose a method that effectively explores the knowledge existing in AU descriptions to enhance AU recognition. Specifically, the proposed method consists of three components, i.e., AU recognition network, global representation alignment, and AU representation alignment. The AU recognition network extracts global features and AU-specific features for AU prediction from images. To leverage AU textual descriptions fully, we design two-level representation alignment for AU recognition. The global representation alignment component closes the distance between the global facial features and its corresponding positive global embedding extracted from textual descriptions. Then, the AU-specific features are aligned with the positive AU textual embedding by the AU representation alignment component. Negative textual embedding generation strategies are also designed to further boost the two-level representation alignment. Through the two-level alignment, AU textual descriptions guide image representation learning of the AU recognition network. Experiments on two benchmark datasets and one in-the-wild dataset demonstrate the efficacy of the description-enhanced AU recognition method, compared with the state-of-the-art works.},
  archive      = {J_TAFFC},
  author       = {Yanan Chang and Caichao Zhang and Yi Wu and Shangfei Wang},
  doi          = {10.1109/TAFFC.2024.3470524},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {814-826},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Facial action unit recognition enhanced by text descriptions of FACS},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SimFLE: Simple facial landmark encoding for self-supervised facial expression recognition in the wild. <em>TAFFC</em>, <em>16</em>(2), 799-813. (<a href='https://doi.org/10.1109/TAFFC.2024.3470980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition in the wild (FER-W) entails classifying facial emotions in natural environments. The major challenges in FER-W stem from the complexity and ambiguity of facial images, making it difficult to curate a large-scale labeled dataset for training. Additionally, the subtle differences in emotions often reside in the fine-grained details of local facial landmarks, demanding innovative solutions to capture these crucial features efficiently. To address these issues, we employ two distinct self-supervised methods. First, we adopt a contrastive learning method to capture generalized global representations, enabling the model to understand the semantic context of facial expressions without relying on labeled data. Simultaneously, we leverage masked image modeling to focus on embedding fine-grained, local facial landmark information at the patch-level. We introduce a novel module called FaceMAE, which aims to reconstruct the masked facial patches. The semantic masking scheme is designed to preserve highly activated feature activations, allowing the encoding of crucial details of unmasked facial landmarks and their relationships within the broader facial context at the patch-level. It finally guides the backbone network to calibrate the learned global features to be attentive to facial landmarks. Our proposed method, called Simple Facial Landmark Encoding (SimFLE), significantly outperforms supervised baseline and other self-supervised methods in terms of facial landmark localization and overall performance, as demonstrated through extensive experiments across several FER-W benchmarks.},
  archive      = {J_TAFFC},
  author       = {Jiyong Moon and Hyeryung Jang and Seongsik Park},
  doi          = {10.1109/TAFFC.2024.3470980},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {799-813},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SimFLE: Simple facial landmark encoding for self-supervised facial expression recognition in the wild},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A topic-guided self-attention network for daily mental wellbeing prediction using mobile devices. <em>TAFFC</em>, <em>16</em>(2), 783-798. (<a href='https://doi.org/10.1109/TAFFC.2024.3471654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction of daily mental wellbeing holds profound implications for individual healthcare and societal stability. Previous studies have shown the potential of using individual's multimodal behavioral data collected through mobile devices to predict his/her daily mental wellbeing metrics, such as stress, mood, and anxiety. However, effectively capturing long-range dependencies in behavioral time series data while accurately representing the statistical distribution patterns of various behaviors over a certain period is a significant challenge. In this paper, we propose a daily mental wellbeing prediction model based on a Topic-Guided Self-Attention Network (TGSAN). This model utilizes self-attention mechanism to capture long-range dependencies from the behavioral data collected by mobile devices. We utilize a multi-granularity time encoding method to inject time information of different granularities (i.e., day and hour, or week and day) into the behavioral data, thereby enhancing the sensibility of the self-attention network to capture every individual's habitual cyclicality rhythm. Then, we introduce a neural topic model to analyze the statistical distribution characteristics of various behaviors in the monitoring period as behavioral distribution patterns for different individuals, and further propose a topic attention network to enhance the model's classification performance by guiding the weights of long-range dependencies features from the self-attention network with the derived topic information. Compared to state-of-the-art methods, the proposed TGSAN achieved superior performance on datasets that measure different mental health indicators (stress, mood, and anxiety), with F1 scores outperforming by 4.5% and 2.3% on the Crosscheck and StudentLife datasets, respectively, and accuracy outperforming by 3.3% on the GLOBEM dataset. Our study demonstrates the effectiveness and interpretability of combining self-attention mechanisms with neural topic model, for a better understanding of the relationship between different individuals’ behaviors and their mental wellbeing.},
  archive      = {J_TAFFC},
  author       = {Zeju Xu and Guanzheng Liu and Guozhen Zhao and Zhiguo Zhang and Chenzhong Li and Changhong Wang},
  doi          = {10.1109/TAFFC.2024.3471654},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {783-798},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A topic-guided self-attention network for daily mental wellbeing prediction using mobile devices},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling cognitive-affective processes with appraisal and reinforcement learning. <em>TAFFC</em>, <em>16</em>(2), 771-782. (<a href='https://doi.org/10.1109/TAFFC.2024.3470555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational models can advance affective science by shedding light onto the interplay between cognition and emotion from an information processing point of view. We propose a computational model of emotion that integrates reinforcement learning (RL) and appraisal theory, establishing a formal relationship between reward processing, goal-directed task learning, cognitive appraisal, and emotional experiences. The model achieves this by formalizing four evaluative checks from the component process model (CPM) in terms of temporal difference learning updates: suddenness, goal relevance, goal conduciveness, and power. The formalism is task independent and can be applied to any task that is represented as a Markov decision problem (MDP) and solved using RL. We evaluate the model by predicting a range of human emotions based on a series of vignette studies, highlighting its potential to improve our understanding of the role of reward processing in affective experiences.},
  archive      = {J_TAFFC},
  author       = {Jiayi Eurus Zhang and Joost Broekens and Jussi P. P. Jokinen},
  doi          = {10.1109/TAFFC.2024.3470555},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {771-782},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Modeling cognitive-affective processes with appraisal and reinforcement learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leaving none behind: Data-free domain incremental learning for major depressive disorder detection. <em>TAFFC</em>, <em>16</em>(2), 758-770. (<a href='https://doi.org/10.1109/TAFFC.2024.3469189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep learning techniques have shown promising performance in the Major Depressive Disorder (MDD) detection task, they still face limitations in real-world scenarios. Specifically, given the data scarcity, some efforts have resorted to aggregating data from different domains to expand the data volume. However, their effectiveness is currently limited by the domain gap and data privacy. Additionally, the class imbalance issue is particularly severe in our application, leading to biased classifying performance accordingly. To address these challenges, we propose Data-Free Domain Incremental Learning for the MDD detection (DIL-MDD) task, accommodating multiple feature distributions by only accessing well-trained models from previous domains and the data in the current domain. Specifically, DIL-MDD consists of two key modules: Adaptive Class-tailored Threshold Learning (ACTL) and Data-Free Domain Alignment (DFDA). The first module measures the discrepancy between the outputs of two sequential domains, based on which we learn a class-tailored threshold adaptively. Building on this, we differentiate between samples that either exhibit similarities or dissimilarities with the previous domain, where this similar sample set is identified to investigate the feature distribution of the historical data. The second module imposes an alignment constraint to narrow the gap between these two sample sets, thereby exploring the expertise of the previous domain. To validate the effectiveness of the proposed method, we conduct extensive experiments on the public MDD datasets, i.e., DAIC-WOZ, MODMA, and CMDC. We also apply our method to another mental health condition, Autism Spectrum Disorder (ASD), to further demonstrate its applicability. Finally, the ablation studies validate the superiority of the proposed modules.},
  archive      = {J_TAFFC},
  author       = {Tao Chen and Yanrong Guo and Shijie Hao and Richang Hong},
  doi          = {10.1109/TAFFC.2024.3469189},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {758-770},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Leaving none behind: Data-free domain incremental learning for major depressive disorder detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the interpretability through maximizing mutual information for EEG emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 744-757. (<a href='https://doi.org/10.1109/TAFFC.2024.3463469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trustworthy Graph Neural Networks (GNNs) for EEG emotion recognition should identify emotions accurately and elucidate corresponding rationales. Current GNNs have achieved notable performance by dynamically modeling emotional connections between EEG channels. However, these GNNs lack interpretability due to the absence of explicit rationale behind their predictions. This paper conducts a comprehensive identification of important EEG channels to enhance the interpretability of EEG emotion recognition from the perspective of mutual information. Specifically, an Adjacency-Explainable Graph Neural Network (AEG) for ante-hoc interpretability is proposed to capture genuine EEG emotional connections, which gives a theoretical guarantee to remove spurious connections. Moreover, a Channel-wise Adaptive Class Activation Mapping Explainer (CACA) for post-hoc interpretability is developed to locate the EEG channels that contribute most to predictions. Experimental results on three datasets, i.e., SEED, SEED-IV, and DREAMER, prove that imbuing training processes with enhanced interpretability ensures significant performance improvements in emotion recognition. Quantitative comparisons of post-hoc interpretability also demonstrate the superiority of CACA. Furthermore, this paper illustrates two potential applications of the proposed methodologies, showing their broader utility and significance.},
  archive      = {J_TAFFC},
  author       = {Hua Yang and C. L. Philip Chen and Bianna Chen and Tong Zhang},
  doi          = {10.1109/TAFFC.2024.3463469},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {744-757},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Improving the interpretability through maximizing mutual information for EEG emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoding musical neural activity in patients with disorders of consciousness through self-supervised contrastive domain generalization. <em>TAFFC</em>, <em>16</em>(2), 726-743. (<a href='https://doi.org/10.1109/TAFFC.2024.3462603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the brain responses of patients with disorders of consciousness (DOCs), which include comas, vegetative states (VSs, also called unresponsive wakefulness syndrome (UWS)) and minimally conscious states (MCSs), based on electroencephalography (EEG) has important clinical diagnosis implications. However, due to impaired motor and cognitive abilities, patients with DOCs may not be able to express their feelings and their brain responses to different stimuli, making it difficult to correctly label data. EEG classification algorithms trained with these data cannot make reliable classifications and predictions for clinical diagnosis purposes. To identify the brain responses produced for different types of stimuli in patients with DOCs, we proposed a self-supervised contrastive domain generalization framework (SSCDG) for cross-subject EEG classification. The model was first trained with healthy-subject EEG data induced by different stimuli to learn their corresponding unsupervised representations. Then, we used these representations to train a classifier to predict the emotional states of patients with DOCs under the corresponding stimuli. SSCDG was first evaluated on the SEED dataset, and it achieved an accuracy of 87.6%, which was 1.1% higher than that of the state-of-the-art (SOTA) approaches. Moreover, the SSCDG method was utilized to categorize EEG data acquired from seventeen DOC patients, including eleven in a UWS state and six in an MCS state, with seven patients demonstrating notable accuracy in three-class EEG classification tasks. The SSCDG results indicated that the seven patients with DOCs may have shown classifiable EEG responses to the presented stimuli.},
  archive      = {J_TAFFC},
  author       = {Honghua Cai and Jiahui Pan and Qiuyi Xiao and Jiarui Jin and Yuanqing Li and Qiuyou Xie},
  doi          = {10.1109/TAFFC.2024.3462603},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {726-743},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Decoding musical neural activity in patients with disorders of consciousness through self-supervised contrastive domain generalization},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic emotion-dependent network with relational subgraph interaction for multimodal emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 712-725. (<a href='https://doi.org/10.1109/TAFFC.2024.3461148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Emotion Recognition in Conversations (MERC) is an important topic in human-computer interaction. In the MERC task, conversations exhibit dynamic emotional dependency, including inter-speaker and intra-speaker emotional dependency, both are vital in understanding the content. However, current research primarily integrates these two emotional dependencies into one unified module, limiting the accuracy of MERC. In this paper, we propose a dynamic emotion-dependent network with relational subgraph interaction named DEDNet. DEDNet introduces relational subgraphs to separately model two emotional dependencies, enabling structured learning paths for utterances based on distinct emotional dependency types. Specifically, nodes indicate the utterances at different moments in the conversation, while edges define the emotional dependency and temporal relationships between nodes. To explicitly capture the differences between these two emotional dependencies, distinct subgraphs are designed for comprehensive representations. Furthermore, we propose an incremental interactive strategy, sequentially leveraging two emotional dependencies to learn the changes in dependency relationships. We find that modeling inter-speaker emotional dependency can better identify negative emotions and modeling intra-speaker emotional dependency can better recognize positive emotions. Experimental results demonstrate that our model outperforms current state-of-the-art methods on three benchmark datasets, IEMOCAP, MELD and DailyDialog.},
  archive      = {J_TAFFC},
  author       = {Ye Wang and Wei Zhang and Ke Liu and Wei Wu and Feng Hu and Hong Yu and Guoyin Wang},
  doi          = {10.1109/TAFFC.2024.3461148},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {712-725},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Dynamic emotion-dependent network with relational subgraph interaction for multimodal emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale promoted self-adjusting correlation learning for facial action unit detection. <em>TAFFC</em>, <em>16</em>(2), 697-711. (<a href='https://doi.org/10.1109/TAFFC.2024.3460538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Action Unit (AU) detection is a crucial task in affective computing and social robotics as it helps to identify emotions expressed through facial expressions. Anatomically, there are innumerable correlations between AUs, which contain rich information and are vital for AU detection. Previous methods used fixed AU correlations based on expert experience or statistical rules on specific benchmarks, but it is challenging to comprehensively reflect complex correlations between AUs via hand-crafted settings. There are alternative methods that employ a fully connected graph to learn these dependencies exhaustively. However, these approaches can result in a computational explosion and high dependency with a large dataset. To address these challenges, this paper proposes a novel self-adjusting AU-correlation learning (SACL) method with less computation for AU detection. This method adaptively learns and updates AU correlation graphs by efficiently leveraging the characteristics of different levels of AU motion and emotion representation information extracted in different stages of the network. Moreover, this paper explores the role of multi-scale learning in correlation information extraction, and design a simple yet effective multi-scale feature learning (MSFL) method to promote better performance in AU detection. By integrating AU correlation information with multi-scale features, the proposed method obtains a more robust feature representation for the final AU detection. Extensive experiments show that the proposed method outperforms the state-of-the-art methods on widely used AU detection benchmark datasets, with only 28.7% and 12.0% of the parameters and FLOPs of the best method, respectively.},
  archive      = {J_TAFFC},
  author       = {Xin Liu and Kaishen Yuan and Xuesong Niu and Jingang Shi and Zitong Yu and Huanjing Yue and Jingyu Yang},
  doi          = {10.1109/TAFFC.2024.3460538},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {697-711},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multi-scale promoted self-adjusting correlation learning for facial action unit detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale facial expression recognition based on dynamic global and static local attention. <em>TAFFC</em>, <em>16</em>(2), 683-696. (<a href='https://doi.org/10.1109/TAFFC.2024.3458464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To better characterize the differences in category features in Facial Expression Recognition (FER) tasks, and improve inter-class separability and intra-class compactness, we propose a Multiscale Facial Expression Recognition model based on dynamic global and static local attention (MFER) from the perspectives of intra-class and inter-class features. Firstly, we propose Dynamic global and Static local attention (DS Attention) mechanism that fuse contextual information, learn potential regions of global and local features between different expression categories, and represent feature discrepancies between categories to distinguish between different expression categories. Then, we design a Deep Smooth Feature loss function (DSF) to balance the probability difference of encoded intra-class features and promote intra-class features towards corresponding centers. Finally, we construct a Multiscale classifier method (Msc) to learn high-frequency and low-frequency information in the dimensional space, represent deep features of multiscale dimensional space, and alleviate sparse distribution problems in high-dimensional space. Experimental results on public datasets RAF-DB, AffectNet-7, AffectNet-8, and FERPlus show that the proposed model achieves state-of-the-art performance with recognition accuracies of 92.08%, 67.06%, 63.15%, and 91.09%, respectively.},
  archive      = {J_TAFFC},
  author       = {Jie Xu and Yang Li and Guanci Yang and Ling He and Kexin Luo},
  doi          = {10.1109/TAFFC.2024.3458464},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {683-696},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multiscale facial expression recognition based on dynamic global and static local attention},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical knowledge stripping for multimodal sentiment analysis. <em>TAFFC</em>, <em>16</em>(2), 669-682. (<a href='https://doi.org/10.1109/TAFFC.2024.3456117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis (MSA) has emerged as a prominent research area that focuses on leveraging multimodal data to understand intention and sentiment signals. Despite significant progress, two major challenges remain in integrating diverse modalities: modal heterogeneity and interference information. To address these issues, we propose a novel framework called Multimodal Hierarchical Knowledge Stripping (MHKS), which enables the progressive extraction of informative knowledge. First, inspired by the information bottleneck (IB), we design a hierarchical disentanglement strategy to stepwise separate task-relevant and task-irrelevant information at the feature, attribute, and semantic levels. This enables MHKS to extract valuable knowledge in unimodal representations and eliminate interference information. Then, to mitigate the distribution gap across multiple modalities, we further design an adaptive alignment strategy based on contrastive learning. We utilize text modality as a bridge to connect other nonverbal modalities, which encourages adaptive alignment across modalities and facilitates the learning of more harmonized joint representations. Comprehensive experiments on three popular datasets demonstrate our method achieves excellent performance on MSA tasks.},
  archive      = {J_TAFFC},
  author       = {Aolin Xiong and Ying Zeng and Haifeng Hu},
  doi          = {10.1109/TAFFC.2024.3456117},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {669-682},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Hierarchical knowledge stripping for multimodal sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JADFER: Exploring spatial-contextual interaction with joint attention dropping for facial expression recognition. <em>TAFFC</em>, <em>16</em>(2), 655-668. (<a href='https://doi.org/10.1109/TAFFC.2024.3454988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial Expression Recognition (FER) aims to categorize emotional expressions depicted on a human face, and is a challenging task under unconstrained conditions, such as face occlusions and pose variations. Recent methods usually adopt self attention or cross attention to explore global or local relationships among different level features. However, these methods are inclined to focus on the redundant facial regions, causing model overfitting. To address this problem, we propose a new FER model named JADFER, which drops the joint attention in the weight matrix to adaptively enhance facial expression representations. Specifically, our JADFER model consists of three components: Spatial Branch (SB), Contextual Branch (CB), and Spatial-Contextual Interaction (SCI). First, SB runs $N$ paths in parallel, where a Variety loss is designed to guide the paths of SB to focus on different discriminative regions. Meanwhile, CB abstracts the contextual facial representations using self attention with Joint Attention Dropping (JAD). Then, the SCI adopts the spatial features from SB to query the contextual representations from CB through cross attention with JAD, which regulates the attention weights by dropping the similar activations to further enhance the facial embeddings. Experimental results demonstrate that the proposed model outperforms the state-of-the-art methods on several FER benchmarks.},
  archive      = {J_TAFFC},
  author       = {Yu Gao and Weihong Ren and Weibo Jiang and Qian Dong and Wei Nie and Wenhao Wu and Honghai Liu},
  doi          = {10.1109/TAFFC.2024.3454988},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {655-668},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {JADFER: Exploring spatial-contextual interaction with joint attention dropping for facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FERMixNet: An occlusion robust facial expression recognition model with facial mixing augmentation and mid-level representation learning. <em>TAFFC</em>, <em>16</em>(2), 639-654. (<a href='https://doi.org/10.1109/TAFFC.2024.3454102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions can provide a better understanding of people’s mental status and attitudes towards specific things. However, facial occlusion in real world is an unfavorable phenomenon that greatly affects the performance of facial expression recognition models. Recent works addressing the occlusion problem have primarily relied on attention mechanisms or occlusion discarding methods that focus on non-occluded regions of the face. However, these methods have not achieved a good balance between occlusion robustness and model efficiency. In this paper, we propose a simple and efficient model, called FERMixNet, for occluded facial expression recognition. The model incorporates a novel facial mixing augmentation strategy (FERMix) that generates new training samples by simulating real-world facial occlusion and preserving high expression-related semantic information. By co-training the original and newly generated samples, the model’s occlusion robustness is improved without increasing its complexity during inference. Additionally, to further enhance the model’s occlusion robustness, we include mid-level representation learning in the network to learn the discriminative non-occluded local features of the samples with low computational cost. Extensive experiments on four public facial occlusion datasets: Occlusion-RAF-DB, Occlusion-FERPlus and FED-RO show that the proposed model achieves state-of-the-art results which demonstrates the good robustness of our method for occluded facial expression recognition. Meanwhile, the proposed model also achieves state-of-the-art results on the in-the-wild facial expression datasets RAF-DB, AffectNet-8, and AffectNet-7. It proves that the proposed model has good application prospects in real world.},
  archive      = {J_TAFFC},
  author       = {Yansong Huang and Junjie Peng and Wenqiang Zhang and Tong Zhao and Gan Chen and Shuhua Tan and Fen Yi and Lu Wang},
  doi          = {10.1109/TAFFC.2024.3454102},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {639-654},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {FERMixNet: An occlusion robust facial expression recognition model with facial mixing augmentation and mid-level representation learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos. <em>TAFFC</em>, <em>16</em>(2), 624-638. (<a href='https://doi.org/10.1109/TAFFC.2024.3453443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic facial expression recognition (DFER) in the wild is still hindered by data limitations, e.g., insufficient quantity and diversity of pose, occlusion and illumination, as well as the inherent ambiguity of facial expressions. In contrast, static facial expression recognition (SFER) currently shows much higher performance and can benefit from more abundant high-quality training data. Moreover, the appearance features and dynamic dependencies of DFER remain largely unexplored. Recognizing the potential in leveraging SFER knowledge for DFER, we introduce a novel Static-to-Dynamic model (S2D) that leverages existing SFER knowledge and dynamic information implicitly encoded in extracted facial landmark-aware features, thereby significantly improving DFER performance. First, we build and train an image model for SFER, which incorporates a standard Vision Transformer (ViT) and Multi-View Complementary Prompters (MCPs) only. Then, we obtain our video model (i.e., S2D), for DFER, by inserting Temporal-Modeling Adapters (TMAs) into the image model. MCPs enhance facial expression features with landmark-aware features inferred by an off-the-shelf facial landmark detector. And the TMAs capture and model the relationships of dynamic changes in facial expressions, effectively extending the pre-trained image model for videos. Notably, MCPs and TMAs only increase a fraction of trainable parameters (less than +10%) to the original image model. Moreover, we present a novel Emotion-Anchors (i.e., reference samples for each emotion category) based Self-Distillation Loss to reduce the detrimental influence of ambiguous emotion labels, further enhancing our S2D. Experiments conducted on popular SFER and DFER datasets show that we have achieved a new state of the art.},
  archive      = {J_TAFFC},
  author       = {Yin Chen and Jia Li and Shiguang Shan and Meng Wang and Richang Hong},
  doi          = {10.1109/TAFFC.2024.3453443},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {624-638},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobile virtual assistant for multi-modal depression-level stratification. <em>TAFFC</em>, <em>16</em>(2), 611-623. (<a href='https://doi.org/10.1109/TAFFC.2024.3451114'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression not only afflicts hundreds of millions of people but also contributes to a global disability and healthcare burden. The primary method of diagnosing depression relies on the judgment of medical professionals in clinical interviews with patients, which is subjective and time-consuming. Recent studies have demonstrated that text, audio, facial attributes, heart rate, and eye movement could be utilized for depression-level stratification. In this paper, we construct a virtual assistant for automatic depression-level stratification on mobile devices that can actively guide users through voice dialogue and change conversation content using emotion perception. During the conversation, features from text, audio, facial attributes, heart rate, and eye movement are extracted for multi-modal depression-level stratification. We utilize a feature-level fusion framework to integrate five modalities and the deep neural network to classify the varying levels of depression, which include healthy, mild, moderate, or severe depression, as well as bipolar disorder (formerly called manic depression). With outcome data from 168 subjects, experimental results reveal that the total accuracy of feature-level fusion with five modal features achieves the highest accuracy of 90.26 percent.},
  archive      = {J_TAFFC},
  author       = {Eric Hsiao-Kuang Wu and Ting-Yu Gao and Chia-Ru Chung and Chun-Chuan Chen and Chia-Fen Tsai and Shih-Ching Yeh},
  doi          = {10.1109/TAFFC.2024.3451114},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {611-623},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Mobile virtual assistant for multi-modal depression-level stratification},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMGWOFS: A feature selector with trade-off between conflict objectives for EEG-based emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 598-610. (<a href='https://doi.org/10.1109/TAFFC.2024.3450573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is a crucial step in EEG emotion recognition. However, it was often used as a single objective problem to either reduce the number of features or maximize classification accuracy, while neglecting their balance. To address the issue, we proposed Improved Multi-objective Grey Wolf Optimization Feature Selection (IMGWOFS). First, we designed a population initialization operator via discriminability and independence of features to accelerate search speed. Second, we employed a two-stage update strategy to improve the global search capabilities of the EEG feature subsets. Finally, we incorporated an adaptive mutation operator to escape the local optima. We conducted experiments on SEED and DEAP datasets, and the accuracy were 86.87 $\pm$ 1.62 % and 60.65 $\pm$ 1.51 % in the beta band using a smaller number of EEG features. In addition, the frontal lobe was related to emotion processing. In conclusion, IMGWOFS is an effective and feasible feature selection method for EEG-based emotion recognition.},
  archive      = {J_TAFFC},
  author       = {Gang Luo and Shuting Sun and Chang Yan and Shanshan Qu and Dixin Wang and Na Chu and Xuesong Liu and Fuze Tian and Kun Qian and Xiaowei Li and Bin Hu},
  doi          = {10.1109/TAFFC.2024.3450573},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {598-610},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {IMGWOFS: A feature selector with trade-off between conflict objectives for EEG-based emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From extraction to generation: Multimodal emotion-cause pair generation in conversations. <em>TAFFC</em>, <em>16</em>(2), 586-597. (<a href='https://doi.org/10.1109/TAFFC.2024.3446646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an important task in emotion analysis, Multimodal Emotion-Cause Pair Extraction in conversations (MECPE) aims to extract all the emotion-cause utterance pairs from a conversation. However, there are two shortcomings in the MECPE task: 1) it ignores emotion utterances whose causes cannot be located in the conversation but require contextualized inference; 2) it fails to locate the exact causes that occur in vision or audio modalities beyond text. To address these issues, in this paper, we introduce a new task named Multimodal Emotion-Cause Pair Generation in Conversations (MECPG), which aims to identify the emotion utterances with their emotion categories and generate their corresponding causes in a conversation. To tackle the MECPG task, we construct a dataset based on a benchmark corpus for MECPE. We further propose a generative framework named MONICA, which jointly performs emotion recognition and emotion cause generation with a sequence-to-sequence model. Experiments on our annotated dataset show the superiority of MONICA over several competitive systems. Our dataset and source codes will be publicly released.},
  archive      = {J_TAFFC},
  author       = {Heqing Ma and Jianfei Yu and Fanfan Wang and Hanyu Cao and Rui Xia},
  doi          = {10.1109/TAFFC.2024.3446646},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {586-597},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {From extraction to generation: Multimodal emotion-cause pair generation in conversations},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian optimization with tree ensembles to improve depression screening on textual datasets. <em>TAFFC</em>, <em>16</em>(2), 573-585. (<a href='https://doi.org/10.1109/TAFFC.2024.3442557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Improving digital depression screening is important for combating the global mental health crisis. Textual data are promising for depression screening due to their many origins, but the variety presents screening challenges. To improve depression screening with textual data, we propose eXtreme Gradient Boosting (XGBoost) with Bayesian Optimization (BO). We experiment with three different objective functions to optimize our models. We apply our models to screen for depression with three disparate textual datasets containing features extracted from transcripts, SMS text messages, and typed replies. When compared to seven other machine learning methods, our XGBoost with BO models demonstrated impressive generalizability across the datasets, achieving average balanced accuracy scores of 0.60, 0.67, and 0.69 with transcripts, SMS text messages, and typed replies, respectively. Our feature importance assessment revealed that the most important features for these three text types were respectively negative emotion, youth, and love lexical category frequencies. Overall, our research presents a promising depression screening method that offers generalizability across text types, explainability, and computational efficiency.},
  archive      = {J_TAFFC},
  author       = {Tingting Zhao and ML Tlachac},
  doi          = {10.1109/TAFFC.2024.3442557},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {573-585},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Bayesian optimization with tree ensembles to improve depression screening on textual datasets},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EDSA-ensemble: An event detection sentiment analysis ensemble architecture. <em>TAFFC</em>, <em>16</em>(2), 555-572. (<a href='https://doi.org/10.1109/TAFFC.2024.3434355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As global digitization continues to grow, technology becomes more affordable and easier to use, and social media platforms thrive, becoming the new means of spreading information and news. Communities are built around sharing and discussing current events. Within these communities, users are enabled to share their opinions about each event. Using Sentiment Analysis to understand the polarity of each message belonging to an event, as well as the entire event, can help to better understand the general and individual feelings of significant trends and the dynamics on online social networks. In this context, we propose a new ensemble architecture, EDSA-Ensemble (Event Detection Sentiment Analysis Ensemble), that uses Event Detection and Sentiment Analysis to improve the detection of the polarity for current events from Social Media. For Event Detection, we use techniques based on Information Diffusion taking into account both the time span and the topics. To detect the polarity of each event, we preprocess the text and employ several Machine and Deep Learning models to create an ensemble model. The preprocessing step includes several word representation models: raw frequency, $TFIDF$, Word2Vec, and Transformers. The proposed EDSA-Ensemble architecture improves the event sentiment classification over the individual Machine and Deep Learning models.},
  archive      = {J_TAFFC},
  author       = {Alexandru Petrescu and Ciprian-Octavian Truică and Elena-Simona Apostol and Adrian Paschke},
  doi          = {10.1109/TAFFC.2024.3434355},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {555-572},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {EDSA-ensemble: An event detection sentiment analysis ensemble architecture},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Affective computing databases: In-depth analysis of systematic reviews and surveys. <em>TAFFC</em>, <em>16</em>(2), 537-554. (<a href='https://doi.org/10.1109/TAFFC.2024.3507289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of affective computing (AffC) is a hot research topic, where keeping track of the latest state-of-the-art can be cumbersome. Probably, due to this, a huge increase in publications of systematic reviews or surveys (SRoS) is appearing in different journals, covering various aspects such as databases, methods, and overall perspectives. Nevertheless, this increase does not mean more and better information, or at least a clarification of information. The present study analyses 10 SRoS, all published within the last 4 years, focusing only on covering AffC databases, with emphasis on collections where emotion or sentiment can be extracted from the body. It was observed that, depending on the SRoS, different information was presented, sometimes with missing or discrepant data, due to lack of information or by the way it was interpreted. As a result, from those 10 SRoS, a total of 111 different databases were analyzed, which were segmented into three groups (tiers, i.e., citation-based categorization) by their relative importance of appearance in the SRoS. In addition, it is proposed a taxonomy with a minimum set of characterizing information that researchers should address when publishing or reviewing databases.},
  archive      = {J_TAFFC},
  author       = {Pedro J. Vaz and João M. F. Rodrigues and Pedro J. S. Cardoso},
  doi          = {10.1109/TAFFC.2024.3507289},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {537-554},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Affective computing databases: In-depth analysis of systematic reviews and surveys},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explainable AI for audio and visual affective computing: A scoping review. <em>TAFFC</em>, <em>16</em>(2), 518-536. (<a href='https://doi.org/10.1109/TAFFC.2024.3505269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Affective computing often relies on audiovisual data to identify affective states from non-verbal signals, such as facial expressions and vocal cues. Since automatic affect recognition can be used in sensitive applications, such as healthcare and education, it is crucial to understand how models arrive at their decisions. Interpretability of machine learning models is the goal of the emerging research area of Explainable AI (explainable AI (XAI)). This scoping review aims to survey the field of audiovisual affective machine learning to identify how XAI is applied in this domain. We first provide an overview of XAI concepts relevant to affective computing. Next, following the recommended PRISMA guidelines, we perform a literature search in the ACM, IEEE, Web of Science and PubMed databases. After systematically reviewing 1190 articles, a final set of 65 papers is included in our analysis. We quantitatively summarize the scope, methods and evaluation of the XAI techniques used in the identified papers. Our findings show encouraging developments for using XAI to explain models in audiovisual affective computing, yet only a limited set of methods are used in the reviewed works. Following a critical discussion, we provide recommendations for incorporating interpretability in future work for affective machine learning.},
  archive      = {J_TAFFC},
  author       = {David S. Johnson and Olya Hakobyan and Jonas Paletschek and Hanna Drimalla},
  doi          = {10.1109/TAFFC.2024.3505269},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {518-536},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Explainable AI for audio and visual affective computing: A scoping review},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning approaches for stress detection: A survey. <em>TAFFC</em>, <em>16</em>(2), 499-517. (<a href='https://doi.org/10.1109/TAFFC.2024.3455371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stress has a severe impact on individuals irrespective of age, sex, work, or background. The reliable development of stress detection techniques enhances the social, educational, physical, economic, and professional quality of life, preventing chronic stress and proposing alleviation strategies. Research studies examine psychological, cognitive, behavioral, and physiological reactions to identify stress adequately. Deep Learning (DL) has received significant attention in recent years as it deals with high-dimensional, heterogeneous data and automatically learns representative features. This paper presents a survey on stress detection with recent DL approaches, leveraging data from all possible sources (physiological, speech, facial expressions, gestures, and social media content). The methodological outlines, the best results, and the main contributions of each study are discussed. We also describe publicly available datasets used by several of the presented works. Finally, we emphasize various open issues within the field of research and highlight key directions for future work.},
  archive      = {J_TAFFC},
  author       = {Maria Kyrou and Ioannis Kompatsiaris and Panagiotis C. Petrantonakis},
  doi          = {10.1109/TAFFC.2024.3455371},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {499-517},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Deep learning approaches for stress detection: A survey},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential impacts of monologue and conversation on speech emotion recognition. <em>TAFFC</em>, <em>16</em>(2), 485-498. (<a href='https://doi.org/10.1109/TAFFC.2024.3509138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of Speech Emotion Recognition (SER) is significantly dependent on the quality of emotional speech corpora used for model training. Researchers in the field of SER have developed various corpora by adjusting design parameters to enhance the reliability of the training source. For this study, we focus on exploring communication modes of collection, specifically analyzing spontaneous emotional speech patterns gathered during conversation or monologue. While conversations are acknowledged as effective for eliciting authentic emotional expressions, systematic analyses are necessary to confirm their reliability as a better source of emotional speech data. We investigate this research question from perceptual differences and acoustic variability present in both emotional speeches. Our analyses on multi-lingual corpora show that, first, raters exhibit higher consistency for conversation recordings when evaluating categorical emotions, and second, perceptions and acoustic patterns observed in conversational samples align more closely with expected trends discussed in relevant emotion literature. We further examine the impact of these differences on SER modeling, which shows that we can train a more robust and stable SER model by using conversation data. This work provides comprehensive evidence suggesting that conversation may offer a better source compared to monologue for developing an SER model.},
  archive      = {J_TAFFC},
  author       = {Woan-Shiuan Chien and Shreya G. Upadhyay and Wei-Cheng Lin and Carlos Busso and Chi-Chun Lee},
  doi          = {10.1109/TAFFC.2024.3509138},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {485-498},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Differential impacts of monologue and conversation on speech emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The EmoPain@Home dataset: Capturing pain level and activity recognition for people with chronic pain in their homes. <em>TAFFC</em>, <em>16</em>(2), 471-484. (<a href='https://doi.org/10.1109/TAFFC.2024.3390837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chronic pain is a prevalent condition where fear of movement and pain interfere with everyday functioning. Yet, there is no open body movement dataset for people with chronic pain in everyday settings. Our EmoPain@Home dataset addresses this with capture from 18 people with and without chronic pain in their homes, while they performed their routine activities. The data includes labels for pain, worry, and movement confidence continuously recorded for activity instances for the people with chronic pain. We explored baseline two-level pain detection based on this dataset and obtained 0.62 mean F1 score. However, extension of the dataset led to deterioration in performance confirming high variability in pain expressions for real world settings. We investigated baseline activity recognition for this setting as a first step in exploring the use of the activity label as contextual information for improving pain level classification performance. We obtained mean F1 score of 0.43 for 9 activity types, highlighting its feasibility. Further exploration, however, showed that data from healthy people cannot be easily leveraged for improving performance because worry and low confidence alter activity strategies for people with chronic pain. Our dataset and findings lay critical groundwork for automatic assessment of pain experience and behaviour in the wild.},
  archive      = {J_TAFFC},
  author       = {Temitayo Olugbade and Raffaele Andrea Buono and Kyrill Potapov and Alex Bujorianu and Amanda C de C Williams and Santiago de Ossorno Garcia and Nicolas Gold and Catherine Holloway and Nadia Bianchi-Berthouze},
  doi          = {10.1109/TAFFC.2024.3390837},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {471-484},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {The EmoPain@Home dataset: Capturing pain level and activity recognition for people with chronic pain in their homes},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the boundaries of semi-supervised facial expression recognition using in-distribution, out-of-distribution, and unconstrained data. <em>TAFFC</em>, <em>16</em>(2), 458-470. (<a href='https://doi.org/10.1109/TAFFC.2024.3424882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based methods have been the key driving force behind much of the recent success of facial expression recognition (FER) systems. However, the need for large amounts of labelled data remains a challenge. Semi-supervised learning offers a way to overcome this limitation, allowing models to learn from a small amount of labelled data along with a large unlabelled dataset. While semi-supervised learning has shown promise in FER, most current methods from general computer vision literature have not been explored in the context of FER. In this work, we present a comprehensive study on 11 of the most recent semi-supervised methods, in the context of FER, namely Pi-model, Pseudo-label, Mean Teacher, VAT, UDA, MixMatch, ReMixMatch, FlexMatch, CoMatch, and CCSSL. Our investigation covers semi-supervised learning from in-distribution, out-of-distribution, unconstrained, and very small unlabelled data. Our evaluation includes five FER datasets plus one large face dataset for unconstrained learning. Our results demonstrate that FixMatch consistently achieves better performance on in-distribution unlabelled data, while ReMixMatch stands out among all methods for out-of-distribution, unconstrained, and scarce unlabelled data scenarios. Another significant observation is that with an equal number of labelled samples, semi-supervised learning delivers a considerable improvement over supervised learning, regardless of whether the unlabelled data is in-distribution, out-of-distribution, or unconstrained. We also conduct sensitivity analyses on critical hyper-parameters for the two best methods of each setting.},
  archive      = {J_TAFFC},
  author       = {Shuvendu Roy and Ali Etemad},
  doi          = {10.1109/TAFFC.2024.3424882},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {458-470},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Exploring the boundaries of semi-supervised facial expression recognition using in-distribution, out-of-distribution, and unconstrained data},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial. <em>TAFFC</em>, <em>16</em>(2), 456-457. (<a href='https://doi.org/10.1109/TAFFC.2025.3563912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAFFC},
  author       = {Rachael E. Jack and Desmond C. Ong and Khiet Truong and Gale M. Lucas and Shiro Kumano},
  doi          = {10.1109/TAFFC.2025.3563912},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {4-6},
  number       = {2},
  pages        = {456-457},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Editorial},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AVES: An audio-visual emotion stream dataset for temporal emotion detection. <em>TAFFC</em>, <em>16</em>(1), 438-450. (<a href='https://doi.org/10.1109/TAFFC.2024.3440924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human emotions vary over time, which can be vividly described as a stream of emotions. Observing the emotion stream in daily life provides valuable insights into an individual's mental state. However, existing research in emotion understanding has mainly focused on classification tasks, assigning an emotion category to a well-trimmed segment or each frame within a continuous signal. In contrast, the task of temporal emotion detection, which involves locating the boundaries of emotion segments and recognizing their categories in untrimmed signals, has not been fully explored. To advance research in this area, this paper introduces an in-the-wild Audio-Visual Emotion Stream (AVES) dataset, which is reliably annotated with the time boundaries and emotion category for each emotion segment in the videos. Thus, AVES can serve as a solid benchmark for temporal emotion detection tasks. Moreover, considering the flexible boundaries and varying durations of emotion segments, we propose a Boundary Combination Network (BoCoNet) for temporal emotion detection, which leverages short-term temporal context information to first predict the boundaries of emotion segments and then locate the entire emotion segments. Extensive experiments conducted on various representative unimodal and multimodal representations demonstrate that BoCoNet achieves state-of-the-art results. The AVES dataset will be released to the research community. We expect that this paper can advance the research on emotion stream and temporal emotion detection.},
  archive      = {J_TAFFC},
  author       = {Yan Li and Wei Gan and Ke Lu and Dongmei Jiang and Ramesh Jain},
  doi          = {10.1109/TAFFC.2024.3440924},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {438-450},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {AVES: An audio-visual emotion stream dataset for temporal emotion detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distant handshakes: Conveying social intentions through multi-modal soft haptic gloves. <em>TAFFC</em>, <em>16</em>(1), 423-437. (<a href='https://doi.org/10.1109/TAFFC.2024.3438761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the lack of physical touch, social distancing caused by COVID-19 makes it difficult to convey social intentions through handshakes. To mitigate this situation, one promising solution is to simulate distinguishable handshake patterns through haptic feedback devices during distant social communication. This paper reports a distant handshake scheme with multi-modal soft haptic gloves. To address the challenge of duplicating rich haptic stimuli of real handshakes in a compact glove, we extracted four haptic features including grip location, grip strength, skin temperature, and shaking frequency from abundant components of the haptic perception system to mimic handshake behaviors. To guide the interference-free spatial layout of multiple actuators in a limited hand-sized space, we measured the handshake contact area and grip strength of different handshake patterns, which together with the thermosensitivity of the human hand determine the grip location, grip strength, and salient thermal stimulating location. We developed a multi-modal soft haptic glove by rendering four features through pneumatic pressure, thermal, and vibrotactile stimuli, respectively. A user study was conducted to validate the performance of our glove, which set two states for each of the four features, showing over 90% accuracy in distinguishing 16 handshake patterns. Furthermore, a user study on the identification of social intentions yields the finding that distant handshakes with our haptic gloves can convey positive, neutral, and negative social intentions. These results inform the potential of distant handshakes with haptic gloves to convey social intentions in remote interactions including business, politics, and daily life in severe and post-pandemic situations, as well as in future metaverse-based society.},
  archive      = {J_TAFFC},
  author       = {Qianqian Tong and Wenxuan Wei and Yuan Guo and Tianhao Jin and Ziqi Wang and Hongxing Zhang and Yuru Zhang and Dangxiao Wang},
  doi          = {10.1109/TAFFC.2024.3438761},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {423-437},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Distant handshakes: Conveying social intentions through multi-modal soft haptic gloves},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SVFAP: Self-supervised video facial affect perceiver. <em>TAFFC</em>, <em>16</em>(1), 405-422. (<a href='https://doi.org/10.1109/TAFFC.2024.3436913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based facial affect analysis has recently attracted increasing attention owing to its critical role in human-computer interaction. Previous studies mainly focus on developing various deep learning architectures and training them in a fully supervised manner. Although significant progress has been achieved by these supervised methods, the longstanding lack of large-scale high-quality labeled data severely hinders their further improvements. Motivated by the recent success of self-supervised learning in computer vision, this paper introduces a self-supervised approach, termed Self-supervised Video Facial Affect Perceiver (SVFAP), to address the dilemma faced by supervised methods. Specifically, SVFAP leverages masked facial video autoencoding to perform self-supervised pre-training on massive unlabeled facial videos. Considering that large spatiotemporal redundancy exists in facial videos, we propose a novel temporal pyramid and spatial bottleneck Transformer as the encoder of SVFAP, which not only largely reduces computational costs but also achieves excellent performance. To verify the effectiveness of our method, we conduct experiments on nine datasets spanning three downstream tasks, including dynamic facial expression recognition, dimensional emotion recognition, and personality recognition. Comprehensive results demonstrate that SVFAP can learn powerful affect-related representations via large-scale self-supervised pre-training and it significantly outperforms previous state-of-the-art methods on all datasets.},
  archive      = {J_TAFFC},
  author       = {Licai Sun and Zheng Lian and Kexin Wang and Yu He and Mingyu Xu and Haiyang Sun and Bin Liu and Jianhua Tao},
  doi          = {10.1109/TAFFC.2024.3436913},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {405-422},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SVFAP: Self-supervised video facial affect perceiver},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The affective audio dataset (AAD) for non-musical, non-vocalized, audio emotion research. <em>TAFFC</em>, <em>16</em>(1), 394-404. (<a href='https://doi.org/10.1109/TAFFC.2024.3437153'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Affective Audio Dataset (AAD) is a new and novel dataset of non-musical, non-anthropomorphic sounds intended for use in affective research. Sounds are annotated for their affective qualities by sets of human participants. The dataset was created in response to a lack of suitable datasets within the domain of audio emotion recognition. A total of 780 sounds are selected from the BBC Sounds Library. Participants are recruited online and asked to rate a subset of sounds based on how they make them feel. Each sound is rated for arousal and valence. It was found that while evenly distributed, there was bias towards the low-valence, high-arousal quadrant, and displayed a greater range of ratings in comparison to others. The AAD is compared with existing datasets to check its consistency and validity, with differences in data collection methods and intended use-cases highlighted. Using a subset of the data, the online ratings were validated against an in-person data collection experiment with findings strongly correlating. The AAD is used to train a basic affect-prediction model and results are discussed. Uses of this dataset include, human-emotion research, cultural studies, other affect-based research, and industry use such as audio post-production, gaming, and user-interface design.},
  archive      = {J_TAFFC},
  author       = {Harrison Ridley and Stuart Cunningham and John Darby and John Henry and Richard Stocker},
  doi          = {10.1109/TAFFC.2024.3437153},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {394-404},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {The affective audio dataset (AAD) for non-musical, non-vocalized, audio emotion research},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling gold standard moment-to-moment ratings of perception of stress from audio recordings. <em>TAFFC</em>, <em>16</em>(1), 376-393. (<a href='https://doi.org/10.1109/TAFFC.2024.3435502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabling continuous and unobtrusive monitoring of stress is essential for delivering personalized stress interventions at opportune moments. To achieve automatic stress detection on a time-continuous basis, reliable moment-to-moment ratings of stress are required. However, the current research lacks a large-scale multimodal dataset that provides time-continuous ratings of perceived stress. Existing datasets mainly consist of single-valued self-reported ratings obtained after the stress-inducing task or rely on audio-visual recordings to capture moment-to-moment ratings from multiple annotators. The collection of time-continuous ratings of stress based solely on audio recordings has not been extensively explored. In this paper, we introduce an updated version of the publicly available VerBIO dataset that contains moment-to-moment ratings of perceived stress from multiple annotators. These annotators rated their perception of stress by listening to participants who had conducted a public speaking task. Time-continuous ratings of stress are obtained from four annotators using 22 hours of audio recordings from 339 public speaking sessions performed by 53 individuals. These time-continuous ratings of stress perception were obtained from the annotators solely based on speech, without incorporating the visual modality as an expressive marker. We examine the reliability of the annotation scheme employed in this study and investigate the factors contributing to the observed variation in perceived stress among annotators. Next, we introduce an annotation fusion technique based on expectation-maximization to obtain a reliable gold standard rating by aggregating the ratings from multiple annotators. Results indicate that the proposed annotation fusion technique yields aggregated ratings that can be estimated more reliably using acoustic features compared to the ratings yielded from conventional annotation fusion techniques. The newly generated annotations are publicly available within the proposed updated version of the existing VerBIO dataset, facilitating research in the field of continuous stress detection.},
  archive      = {J_TAFFC},
  author       = {Ehsanul Haque Nirjhar and Theodora Chaspari},
  doi          = {10.1109/TAFFC.2024.3435502},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {376-393},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Modeling gold standard moment-to-moment ratings of perception of stress from audio recordings},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The expression of happiness in social media of individuals reporting depression. <em>TAFFC</em>, <em>16</em>(1), 360-375. (<a href='https://doi.org/10.1109/TAFFC.2024.3434482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression has long been studied in the NLP field, with most works focusing on individuals’ negative emotions. People with depression experience happiness, but this has not been extensively studied. Previous works have shown that sentiment or emotion classification approaches are unsuitable for extracting happy moments because they may not be expressed only in positive words. In this work, we conduct a large-scale study of happy moments from social media texts of individuals mentioning a depression diagnosis. We develop an extensive deep learning-based framework to extract happy moments from text, and annotate them with semantic topics, gender labels, and agency and sociality measures. We analyze over 400,000 happy moments and show significant differences in topics, agency, and sociality of users in the depression and control groups, varying by gender. We found that male and female users in the depression group expressed more sociality in their happy moments than control users. Furthermore, male users’ agency was not impaired in depression, while female users in the depression group expressed fewer happy moments with agency than the control group. Our research can inform psychology interventions, which can foster feelings of longer-lasting happiness and represent a promising path of collaboration between computational linguistics and psychology.},
  archive      = {J_TAFFC},
  author       = {Ana-Maria Bucur and Berta Chulvi and Adrian Cosma and Paolo Rosso},
  doi          = {10.1109/TAFFC.2024.3434482},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {360-375},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {The expression of happiness in social media of individuals reporting depression},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mechanoreceptive aβ primary afferents discriminate naturalistic social touch inputs at a functionally relevant time scale. <em>TAFFC</em>, <em>16</em>(1), 346-359. (<a href='https://doi.org/10.1109/TAFFC.2024.3435060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpersonal touch is an important channel of social emotional interaction. How these physical skin-to-skin touch expressions are processed in the peripheral nervous system is not well understood. From microneurography recordings in humans, we evaluated the capacity of six subtypes of cutaneous mechanoreceptive afferents to differentiate human-delivered social touch expressions. Leveraging statistical and classification analyses, we found that single units of multiple mechanoreceptive Aβ subtypes, especially slowly adapting type II (SA-II) and fast adapting hair follicle afferents (HFA), can reliably differentiate social touch expressions at accuracies similar to human recognition. We then identified the most informative firing patterns of SA-II and HFA afferents, which indicate that average durations of 3-4 s of firing provide sufficient discriminative information. Those two subtypes also exhibit robust tolerance to spike-timing shifts of up to 10-20 ms, varying with touch expressions due to their specific firing properties. Greater shifts in spike-timing, however, can change a firing pattern's envelope to resemble that of another expression and drastically compromise an afferent's discrimination capacity. Altogether, the findings indicate that SA-II and HFA afferents differentiate the skin contact of social touch at time scales relevant for such interactions, which are 1-2 orders of magnitude longer than those for non-social touch.},
  archive      = {J_TAFFC},
  author       = {Shan Xu and Steven C. Hauser and Saad S. Nagi and James A. Jablonski and Merat Rezaei and Ewa Jarocka and Andrew G. Marshall and Håkan Olausson and Sarah McIntyre and Gregory J. Gerling},
  doi          = {10.1109/TAFFC.2024.3435060},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {346-359},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Mechanoreceptive aβ primary afferents discriminate naturalistic social touch inputs at a functionally relevant time scale},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring multivariate dynamics of emotions through time-varying self-assessed arousal and valence ratings. <em>TAFFC</em>, <em>16</em>(1), 333-345. (<a href='https://doi.org/10.1109/TAFFC.2024.3434456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotions arise from a complex interplay of various factors, including conscious experience, physiological processes, and contextual elements. Although emotions are inherently dynamic processes, this aspect is oftentimes neglected in experimental protocols. In this study, we employed dynamical systems theory to investigate the time-varying self-assessed emotion ratings. We used the continuous ratings of the publicly available CASE dataset, in which thirty individuals rated their level of arousal and valence while watching videos designed to evoke four different emotions. First, we analyzed the univariate dynamics by reconstructing the phase space from the arousal and valence series separately, and quantified their regularity and spatial complexity by using three metrics: Fuzzy, Sample, and Distribution Entropy. Then, we combined the arousal and valence series and proposed a novel index, the Multichannel Distribution Entropy (MDistEn), to estimate the complexity of the bivariate phase space. By coupling the two dimensions, we found that MDistEn resulted as an effective marker of fear, showing patterns statistically different from all of the other stimuli (p-value $\leq$ 0.001). These findings support the investigation of the time-varying dynamics of annotated emotion ratings as a promising pathway to discriminate the onset of fear-related pathological states.},
  archive      = {J_TAFFC},
  author       = {Andrea Gargano and Mimma Nardelli and Enzo Pasquale Scilingo},
  doi          = {10.1109/TAFFC.2024.3434456},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {333-345},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Exploring multivariate dynamics of emotions through time-varying self-assessed arousal and valence ratings},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grop: Graph orthogonal purification network for EEG emotion recognition. <em>TAFFC</em>, <em>16</em>(1), 319-332. (<a href='https://doi.org/10.1109/TAFFC.2024.3433613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existence of emotion-irrelevant representations and individual variability impedes the extraction of robust emotional representations, limiting the adaptability of EEG emotion recognition. Massive studies focus on the mining of emotion-aware information, overlooking emotion-agnostic information, which is insufficient for the extraction of emotion-relevant features against redundancy and variation. In this paper, Graph Orthogonal Purification Network (Grop) is proposed to enhance individual adaptability through improvements in the orthogonality and transferability between emotion-relevant and emotion-irrelevant features. Specifically, the proposed Grop utilized a graph representation extraction module to capture both emotion-relevant and emotion-irrelevant features by the dual graph. The representation orthogonal purification module is developed to eliminate redundant information through feature projection and feature purification. Moreover, the dual emotional space alignment module is imposed to align distribution discrepancies in different emotion feature spaces. To assess the effectiveness of the proposed Grop, various experiments are conducted on two public EEG emotion datasets, i.e., SEED and SEED-IV. The results achieve state-of-the-art performance, demonstrating the capability of the Grop to capture robust emotion features and alleviate the intra- and inter-subject discrepancies.},
  archive      = {J_TAFFC},
  author       = {Mengqi Wu and C. L. Philip Chen and Bianna Chen and Tong Zhang},
  doi          = {10.1109/TAFFC.2024.3433613},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {319-332},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Grop: Graph orthogonal purification network for EEG emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Versatile audio-visual learning for emotion recognition. <em>TAFFC</em>, <em>16</em>(1), 306-318. (<a href='https://doi.org/10.1109/TAFFC.2024.3433386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most current audio-visual emotion recognition models lack the flexibility needed for deployment in practical applications. We envision a multimodal system that works even when only one modality is available and can be implemented interchangeably for either predicting emotional attributes or recognizing categorical emotions. Achieving such flexibility in a multimodal emotion recognition system is difficult due to the inherent challenges in accurately interpreting and integrating varied data sources. It is also a challenge to robustly handle missing or partial information while allowing direct switch between regression or classification tasks. This study proposes a versatile audio-visual learning (VAVL) framework for handling unimodal and multimodal systems for emotion regression or emotion classification tasks. We implement an audio-visual framework that can be trained even when audio and visual paired data is not available for part of the training set (i.e., audio only or only video is present). We achieve this effective representation learning with audio-visual shared layers, residual connections over shared layers, and a unimodal reconstruction task. Our experimental results reveal that our architecture significantly outperforms strong baselines on the CREMA-D, MSP-IMPROV, and CMU-MOSEI corpora. Notably, VAVL attains a new state-of-the-art performance in the emotional attribute prediction task on the MSP-IMPROV corpus.},
  archive      = {J_TAFFC},
  author       = {Lucas Goncalves and Seong-Gyun Leem and Wei-Cheng Lin and Berrak Sisman and Carlos Busso},
  doi          = {10.1109/TAFFC.2024.3433386},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {306-318},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Versatile audio-visual learning for emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject EEG-based emotion recognition. <em>TAFFC</em>, <em>16</em>(1), 290-305. (<a href='https://doi.org/10.1109/TAFFC.2024.3433470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) is an objective tool for emotion recognition with promising applications. However, the scarcity of labeled data remains a major challenge in this field, limiting the widespread use of EEG-based emotion recognition. In this paper, a semi-supervised Dual-stream Self-attentive Adversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed to tackle the challenge of limited labeled data in cross-subject EEG-based emotion recognition. The DS-AGC framework includes two parallel streams for extracting non-structural and structural EEG features. The non-structural stream incorporates a semi-supervised multi-domain adaptation method to alleviate distribution discrepancy among labeled source domain, unlabeled source domain, and unknown target domain. The structural stream develops a graph contrastive learning method to extract effective graph-based feature representation from multiple EEG channels in a semi-supervised manner. Further, a self-attentive fusion module is developed for feature fusion, sample selection, and emotion recognition, which highlights EEG features more relevant to emotions and data samples in the labeled source domain that are closer to the target domain. Extensive experiments are conducted on four benchmark databases (SEED, SEED-IV, SEED-V, and FACED) using a semi-supervised cross-subject leave-one-subject-out cross-validation evaluation protocol. The results show that the proposed model outperforms existing methods under different incomplete label conditions with an average improvement of 2.17%, which demonstrates its effectiveness in addressing the label scarcity problem in cross-subject EEG-based emotion recognition.},
  archive      = {J_TAFFC},
  author       = {Weishan Ye and Zhiguo Zhang and Fei Teng and Min Zhang and Jianhong Wang and Dong Ni and Fali Li and Peng Xu and Zhen Liang},
  doi          = {10.1109/TAFFC.2024.3433470},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {290-305},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject EEG-based emotion recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vicarious evaluation of a decision model for human-agent social touch interactions. <em>TAFFC</em>, <em>16</em>(1), 277-289. (<a href='https://doi.org/10.1109/TAFFC.2024.3433009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a decision model aimed at determining when a social touch by a socially interactive agent (SIA) would be considered socially ‘correct’ : both coherent (meaningful given the current situation) and acceptable to the human interlocutor. Those decisions are computed by taking the context of interaction and the estimated level of rapport between the human and the agent into account. We then present a study based on a vicarious protocol where participants evaluated recorded interactions between an avatar (a human-controlled 3D character) and our agent. Results indicate that the estimations made by our decision model regarding the state of the situation mirror those made by human observers. Our initial hypothesis that the level of rapport would positively influence the acceptability of a touch receives mixed support. Social touch occurrences did not feel as coherent and acceptable as hoped yet. Avenues of improvement for human-agent interactions including social touch are discussed.},
  archive      = {J_TAFFC},
  author       = {Fabien Boucaud and Catherine Pelachaud and Indira Thouvenin},
  doi          = {10.1109/TAFFC.2024.3433009},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {277-289},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Vicarious evaluation of a decision model for human-agent social touch interactions},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-domain sentiment analysis via disentangled representation and prototypical learning. <em>TAFFC</em>, <em>16</em>(1), 264-276. (<a href='https://doi.org/10.1109/TAFFC.2024.3431946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-domain sentiment analysis (CDSA) aims to predict the sentiment polarities of reviews in the target domain using a sentiment classifier learned from the source labeled domain. Most existing studies are dominant with adversarial learning methods and focus on learning domain-invariant sentiment representations in both the source and target domains. However, since sentiment-specific features are not explicitly decoupled, the model may confuse domain features with sentiment features, thus affecting its generalization ability on target domains. Unlike previous studies, in this paper, we tackle the CDSA task from the view of disentangled representation learning, which explicitly learns the disentangled representations of review, focusing in particular on sentiment and domain semantics. Specifically, we disentangle sentiment-specific and domain-specific features from the text representation of the review by two different linear transformations. Then, we introduce a straightforward disentangled loss to disallow the sentiment-specific feature to capture domain information. Moreover, we leverage target unlabeled data to improve the quality of the learned sentiment-specific features via prototypical learning. It indirectly encourages the sentiment-specific features of target samples having potentially different classes more discriminative. Extensive experiments on widely used CDSA datasets show that our method surpasses competitive baselines and achieves new state-of-the-art results, demonstrating its effectiveness and superiority.},
  archive      = {J_TAFFC},
  author       = {Qianlong Wang and Zhiyuan Wen and Keyang Ding and Bin Liang and Ruifeng Xu},
  doi          = {10.1109/TAFFC.2024.3431946},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {264-276},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Cross-domain sentiment analysis via disentangled representation and prototypical learning},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversity and balance: Multimodal sentiment analysis using multimodal-prefixed and cross-modal attention. <em>TAFFC</em>, <em>16</em>(1), 250-263. (<a href='https://doi.org/10.1109/TAFFC.2024.3430045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Sentiment Analysis (MSA) is the technology of intelligently recognizing and assessing human sentiments using various data forms such as text, image, and audio. Despite current mainstream methods have made significant progress, MSA still faces the following issues: 1) most current methods train models based on pre-extracted features, lacking a sufficient understanding of sentiment diversity in multimodal data and may even lead to the loss of critical information in the raw data; and 2) textual modality, which possesses high-level semantic features, should typically dominate the fusion process, yet current methods fail to fully leverage this characteristic to balance modality information. To address the aforementioned issues, we propose a novel Multimodal Sentiment Analysis framework using Multimodal-Prefixed and Cross-Modal Attention (DB-MPCA). For the first issue, DB-MPCA employs multimodal raw data for pre-training, which not only allows for in-depth exploration of multimodal information but also significantly enhances the model’s learning capabilities and generalization, while reducing the substantial costs associated with manual annotation. Regarding the second issue, DB-MPCA introduces two prefix encoders designed to convert acoustic and visual features into prefix tokens. These tokens are then embedded into a pre-trained language model, where they are encoded together with textual tokens. Through this approach, DB-MPCA effectively learns cross-modal attention while maintaining the dominance of the textual modality, thereby optimizing the fusion of modalities. Comprehensive experiments conducted on the widely utilized dataset (CMU-MOSI) demonstrate the effectiveness of our model, highlighting its superiority over baseline models.},
  archive      = {J_TAFFC},
  author       = {Meng Li and Zhenfang Zhu and Kefeng Li and Hongli Pei},
  doi          = {10.1109/TAFFC.2024.3430045},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {250-263},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Diversity and balance: Multimodal sentiment analysis using multimodal-prefixed and cross-modal attention},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Does gamified breath-biofeedback promote adherence, relaxation, and skill transfer in the wild?. <em>TAFFC</em>, <em>16</em>(1), 237-249. (<a href='https://doi.org/10.1109/TAFFC.2024.3428390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates whether gamification of deep breathing (DB) exercises promotes relaxation, skill transfer, and adherence to treatment in ambulatory settings. We designed a game-biofeedback (GBF) intervention where users perform DB exercises while playing a video game, and the game adapts according to the user's breathing rate using negative reinforcement instrumental conditioning. As a control, we developed an interactive paced-breathing treatment (PACE) where users follow a visual signal with their breathing and touch. In a user study, 30 participants were randomly assigned to GBF or PACE, and were allowed to practice at their leisure over the course of three days. Results show that the GBF group practiced their treatments significantly more often, achieved better skill transfer at post-test, and obtained a higher increase in self-reported positivity and relaxation during treatment. Our findings suggest that the use of negative reinforcement coupled with a fun casual game can be used as an alternative tool to promote relaxation and improve adherence to stress management interventions.},
  archive      = {J_TAFFC},
  author       = {Dennis R. da Cunha Silva and Ricardo Gutierrez-Osuna},
  doi          = {10.1109/TAFFC.2024.3428390},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {237-249},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Does gamified breath-biofeedback promote adherence, relaxation, and skill transfer in the wild?},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating visual context into language models for situated social conversation starters. <em>TAFFC</em>, <em>16</em>(1), 223-236. (<a href='https://doi.org/10.1109/TAFFC.2024.3428704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied conversational agents that interact socially with people in the physical world require multi-modal capabilities, such as appropriately responding to visual features of users. While existing vision-and-language models can generate language based on visual input, this language is not situated in a social interaction in the physical world. We present a novel task called Visual Conversation Starters, where an agent generates a conversation-starting question referring to features visible in an image of the user. We collect a dataset of 4000 images of people with 12000 crowdsourced conversation starters, compare various model architectures: fine-tuning smaller seq2seq or image-to-text models versus zero-shot prompting of GPT-3.5, using image captions versus end-to-end image input, training on human data versus synthetic questions generated by GPT-3.5. Models were used to generate friendly conversation starters which were evaluated on criteria including language fluency, visual grounding, interestingness, politeness. Results show that GPT-3.5 generates more interesting, polite questions than smaller models that are fine-tuned on crowdsourced data, but vision-to-language models are better at referencing visual features, they can mimick GPT-3.5's performance. This demonstrates the feasibility of deep visiolinguistic models for situated social agents, forming an important first stage in creating situated multimodal social interaction.},
  archive      = {J_TAFFC},
  author       = {Ruben Janssens and Pieter Wolfert and Thomas Demeester and Tony Belpaeme},
  doi          = {10.1109/TAFFC.2024.3428704},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {223-236},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Integrating visual context into language models for situated social conversation starters},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-level contrastive learning: Hierarchical alleviation of heterogeneity in multimodal sentiment analysis. <em>TAFFC</em>, <em>16</em>(1), 207-222. (<a href='https://doi.org/10.1109/TAFFC.2024.3423671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, multimodal fusion efforts have achieved remarkable success in Multimodal Sentiment Analysis (MSA). However, most of the existing methods are based on model-level fusion, and the challenge of heterogeneity between modalities is not well resolved. Heterogeneity lies in the different feature distributions and distinct representation spaces among different modalities. To mitigate this problem, we propose that fusion is a progressive process, and we introduce a novel multi-level contrastive learning and multi-layer convolution fusion (MCL-MCF) method for MSA. Due to the relationships among multimodal data, the fusion process that involves single-modal to single-modal, single-modal to bimodal or trimodal, and higher-level fused modality semantic consistency is divided into three levels. The first-level contrast learning alleviates heterogeneity between unimodal modalities at the early level of multimodal feature fusion. The second-level contrast learning mitigates heterogeneity between unimodal and fused modalities. At the third level, we introduce a tensor convolution fusion (TCF) module that extracts high-level semantic features from the fused modalities and mitigates heterogeneity at the higher feature level through contrastive learning. To simulate fusion as a progressive process, MCF is proposed to fuse shallow and deep features to model complex relationships among modalities. Experiments on three public datasets show our approach's state-of-the-art performance.},
  archive      = {J_TAFFC},
  author       = {Cunhang Fan and Kang Zhu and Jianhua Tao and Guofeng Yi and Jun Xue and Zhao Lv},
  doi          = {10.1109/TAFFC.2024.3423671},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {207-222},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multi-level contrastive learning: Hierarchical alleviation of heterogeneity in multimodal sentiment analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eye action units as combinations of discrete eye behaviors for wearable mental state analysis. <em>TAFFC</em>, <em>16</em>(1), 191-206. (<a href='https://doi.org/10.1109/TAFFC.2024.3422448'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental state induced by different task contexts and load levels is of interest for human health and wellness, and eye activity extracted from infrared eye images is well-suited to estimate it. As a useful tool for emotion recognition, facial action units (FAUs) extracted from facial images are well-established, however these are insufficiently detailed for the eye. In this paper, we extract discrete eye behaviors from eyelid, iris and pupil boundaries and propose eye action units (EAUs) based on the eye appearance (behavior), providing a detailed and interpretable representation that shares the advantages of FAUs and describes the wide range of eye states and shapes. Eight volunteers annotated 11 EAUs for 120 eye images, represented by a series of discrete eye behaviors. Analysis shows that the EAUs can be viably characterized by fundamental eye behaviors with moderate to substantial agreement. When evaluating discrete eye behaviors and EAUs for recognition of four mental states and two load levels, the former achieved significantly higher accuracy than conventional features of pupil size change and blink rate, especially using behavior duration features, and EAUs outperformed combinations of discrete eye behaviors in general, implying their utility as an action unit.},
  archive      = {J_TAFFC},
  author       = {Siyuan Chen and Julien Epps},
  doi          = {10.1109/TAFFC.2024.3422448},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {191-206},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Eye action units as combinations of discrete eye behaviors for wearable mental state analysis},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting when and what to explain from multimodal eye tracking and task signals. <em>TAFFC</em>, <em>16</em>(1), 179-190. (<a href='https://doi.org/10.1109/TAFFC.2024.3419696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While interest in the field of explainable agents increases, it is still an open problem to incorporate a proactive explanation component into a real-time human–agent collaboration. Thus, when collaborating with a human, we want to enable an agent to identify critical moments requiring timely explanations. We differentiate between situations requiring explanations about the agent's decision-making and assistive explanations supporting the user. In order to detect these situations, we analyze eye tracking signals of participants engaging in a collaborative virtual cooking scenario. First, we show how users’ gaze patterns differ between moments of user confusion, the agent making errors, and the user successfully collaborating with the agent. Second, we evaluate different state-of-the-art models on the task of predicting whether the user is confused or the agent makes errors using gaze- and task-related data. An ensemble of MiniRocket classifiers performs best, especially when updating its predictions with high frequency based on input samples capturing time windows of 3 to 5 seconds. We find that gaze is a significant predictor of when and what to explain. Gaze features are crucial to our classifier's accuracy, with task-related features benefiting the classifier to a smaller extent.},
  archive      = {J_TAFFC},
  author       = {Lennart Wachowiak and Peter Tisnikar and Gerard Canal and Andrew Coles and Matteo Leonetti and Oya Celiktutan},
  doi          = {10.1109/TAFFC.2024.3419696},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {179-190},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Predicting when and what to explain from multimodal eye tracking and task signals},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage temporal modelling framework for video-based depression recognition using graph representation. <em>TAFFC</em>, <em>16</em>(1), 161-178. (<a href='https://doi.org/10.1109/TAFFC.2024.3415770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-based automatic depression analysis provides a fast, objective and repeatable self-assessment solution, which has been widely developed in recent years. While depression cues may be reflected by human facial behaviours of various temporal scales, most existing approaches either focused on modelling depression from short-term or video-level facial behaviours. In this sense, we propose a two-stage framework that models depression severity from multi-scale short-term and video-level facial behaviours. The short-term depressive behaviour modelling stage first deep learns depression-related facial behavioural features from multiple short temporal scales, where a Depression Feature Enhancement (DFE) module is proposed to enhance the depression-related cues for all temporal scales and remove non-depression related noise. Two novel graph encoding strategies are proposed in the video-level depressive behavior modeling stage, i.e., Sequential Graph Representation (SEG) and Spectral Graph Representation (SPG), to re-encode all short-term features of the target video into a video-level graph representation, summarizing depression-related multi-scale video-level temporal information. As a result, the produced graph representations predict depression severity using both short-term and long-term facial behaviour patterns. The experimental results on AVEC 2013, AVEC 2014 and AVEC 2019 datasets show that the proposed DFE module constantly enhanced the depression severity estimation performance for various CNN models while the SPG is superior than other video-level modelling methods. More importantly, the result achieved for the proposed two-stage framework shows its promising and solid performance compared to widely-used one-stage modelling approaches.},
  archive      = {J_TAFFC},
  author       = {Jiaqi Xu and Hatice Gunes and Keerthy Kusumam and Michel Valstar and Siyang Song},
  doi          = {10.1109/TAFFC.2024.3415770},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {161-178},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Two-stage temporal modelling framework for video-based depression recognition using graph representation},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image encoding and fusion of multi-modal data enhance depression diagnosis in parkinson's disease patients. <em>TAFFC</em>, <em>16</em>(1), 145-160. (<a href='https://doi.org/10.1109/TAFFC.2024.3418415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The diagnosis of depression in individuals with Parkinson's Disease (PD) through the utilization of multimodal fusion techniques represents a significant domain. The primary challenge involves the creation of a robust fusion framework to address the heterogeneity among different modalities effectively. However, previous studies primarily focused on interactions between heterogeneous data, neglecting the structural similarities among isomorphic data, resulting in a substantial loss of feature information when merging heterogeneous data. In this study, we introduced a multi-modal data image encoding and fusion approach for diagnosing depression in PD patients. Additionally, we proposed a multi-modal dataset encompassing motion, facial expression, and audio data. First, we designed an RGB and sparse coding method to encode the multi-modal data, achieving the isomorphic transformation of multi-modal information and extracting feature information from lower-dimensional spaces. Furthermore, we introduced a Spatial-Temporal Network (STN) to fuse the three types of encoded images. We incorporated the Relation Global Attention (RGA) to enhance feature extraction and leverage all encoded image location feature nodes for balanced decision attention. Finally, recognizing the limitations of traditional machine learning algorithms in handling multi-tasks in medical diagnosis, we established a multi-task weighted loss function to achieve depression identification and severity prediction through Multi-Task learning (MTL).},
  archive      = {J_TAFFC},
  author       = {Jian Li and Yuliang Zhao and Huawei Zhang and Wayne Jason Li and Changzeng Fu and Chao Lian and Peng Shan},
  doi          = {10.1109/TAFFC.2024.3418415},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {145-160},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Image encoding and fusion of multi-modal data enhance depression diagnosis in parkinson's disease patients},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal sentimental privileged information embedding for improving facial expression recognition. <em>TAFFC</em>, <em>16</em>(1), 133-144. (<a href='https://doi.org/10.1109/TAFFC.2024.3415625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) has always been one of the key task in affective computing. Over the years, researchers have worked to improve the performance of FER by designing models with more powerful feature extraction, embedding attention mechanism, and reconstructing missing information, etc. Different from the paradigms above, we attempt to improve FER performance by using multimodal sentiment data, such as audio and text, as privileged information (PI) for facial images. To this end, a multimodal privileged information embedded facial expression recognition network (MPI-FER) is proposed in this paper. During the training phase, this model achieves the PI embedding of multimodal data for FER by developing cross-modality translation between multimodal sentiment data. During the test phase, input images alone are sufficient for the model inference to accomplish the FER task input. The MPI-FER is a large-scale, heterogeneous deep neural network. To achieve effective training of this model with limited training samples, we design a multi-stage training strategy of module-wise pre-training followed by end-to-end fine-tuning. In addition, a strategy of filling the multimodal sentiment quaternion is proposed for implementing our method on a facial expression database consisting only of face images. We conducted extensive experiments to evaluate the proposed method on two databases of multimodal sentiment analysis (CH-SIMS and CMU-MOSI) and two databases of FER in the wild (RAF-DB and AffectNet). The results show that embedding multimodal sentiment data as privileged information into the FER task based on face images can significantly improve the accuracy of FER. Furthermore, by only using image in the test phase, the proposed method can achieve better results of multimodal sentiment analysis than those methods achieved by using multimodal sentimental data fusion.},
  archive      = {J_TAFFC},
  author       = {Ning Sun and Changwei You and Wenming Zheng and Jixin Liu and Lei Chai and Haian Sun},
  doi          = {10.1109/TAFFC.2024.3415625},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {133-144},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multimodal sentimental privileged information embedding for improving facial expression recognition},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MOCAS: A multimodal dataset for objective cognitive workload assessment on simultaneous tasks. <em>TAFFC</em>, <em>16</em>(1), 116-132. (<a href='https://doi.org/10.1109/TAFFC.2024.3414330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents MOCAS, a multimodal dataset dedicated for human cognitive workload (CWL) assessment. In contrast to existing datasets based on virtual game stimuli, the data in MOCAS was collected from realistic closed-circuit television (CCTV) monitoring tasks, increasing its applicability for real-world scenarios. To build MOCAS, two off-the-shelf wearable sensors and one webcam were utilized to collect physiological signals and behavioral features from 21 human subjects. After each task, participants reported their CWL by completing the NASA-Task Load Index (NASA-TLX) and Instantaneous Self-Assessment (ISA). Personal background (e.g., personality and prior experience) was surveyed using demographic and Big Five Factor personality questionnaires, and two domains of subjective emotion information (i.e., arousal and valence) were obtained from the Self-Assessment Manikin (SAM), which could serve as potential indicators for improving CWL recognition performance. Technical validation was conducted to demonstrate that target CWL levels were elicited during simultaneous CCTV monitoring tasks; its results support the high quality of the collected multimodal signals.},
  archive      = {J_TAFFC},
  author       = {Wonse Jo and Ruiqi Wang and Go-Eum Cha and Su Sun and Revanth Krishna Senthilkumaran and Daniel Foti and Byung-Cheol Min},
  doi          = {10.1109/TAFFC.2024.3414330},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {116-132},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MOCAS: A multimodal dataset for objective cognitive workload assessment on simultaneous tasks},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new look at breathing for affective studies. <em>TAFFC</em>, <em>16</em>(1), 98-115. (<a href='https://doi.org/10.1109/TAFFC.2024.3413053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In affective computing, breathing has seen lighter use than the heart and EDA channels. Several reasons have contributed to this, including difficulties in disambiguating affective from speech effects and perceived lack of generalizability. Here we report a framework that addresses these issues. The cornerstone of the framework is a comprehensive set of physiologically informed features, comprised of three groups: breathing depth, respiratory time quotient (RTQ), and breathing speed features. The breathing depth features capture either mental arousal or fear effects. The RTQ features capture speech production. The breathing speed features capture arousal effects due to emotional influences. The said framework appears to have broad applicability. In the naturalistic Office Tasks 2019 dataset with speaking sessions, the said features used either in regression or random forest models led to robust classification of arousal ($\overline{\text{AUC}}$ in [0.75, 0.96]) stemming from three different conditions: a) mental-emotional stressor effected through a time-pressured knowledge task; b) pure mental stressor effected through a long knowledge task; c) mental-social stressor effected through a public speech task. In the stylized CASE dataset with silent sessions, the same features and algorithms led to solid classification of arousal ($\overline{\text{AUC}}$ in [0.71, 0.85]) stemming from scary vs. non-scary movie clips.},
  archive      = {J_TAFFC},
  author       = {Nanfei Sun and Ioannis Pavlidis},
  doi          = {10.1109/TAFFC.2024.3413053},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {98-115},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A new look at breathing for affective studies},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying children with autism spectrum disorder via transformer-based representation learning from dynamic facial cues. <em>TAFFC</em>, <em>16</em>(1), 83-97. (<a href='https://doi.org/10.1109/TAFFC.2024.3412032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing autism spectrum disorder (ASD) has faced great challenges due to insufficient professional clinicians and complex procedures. Automated data-driven ASD recognition models can reduce the subjectivity and physician dependency of traditional evaluation methods. Facial data, which can encode important perceptual and social behaviors, have emerged in ASD research to explore novel biomarkers for screening, diagnosing, and treating ASD. However, existing research mainly focuses on extracting low-level hand-crafted facial features for analysis and classification. Determining how to learn discriminative deep representations from dynamic facial data for computational model construction remains an unresolved challenge. In this study, we propose an ASD recognition model based on facial videos to fill the lack of temporal correlation learning of facial features. First, we utilize a vision transformer to extract frame-based global facial features. Then, we use a Longformer to establish the correlation of facial features over time. In the experiment, we recruited 146 subjects between 2 and 8 years of age to record their facial videos under a computer-based eye-tracking experiment and 76 subjects to conduct a smartphone-based experiment. Quantitative comparisons have shown the effectiveness and reliability of the proposed model. Furthermore, we have confirmed the correlation between facial and eye-tracking modalities in visual attention.},
  archive      = {J_TAFFC},
  author       = {Chen Xia and Hexu Chen and Junwei Han and Dingwen Zhang and Kuan Li},
  doi          = {10.1109/TAFFC.2024.3412032},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {83-97},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Identifying children with autism spectrum disorder via transformer-based representation learning from dynamic facial cues},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controllable multi-speaker emotional speech synthesis with an emotion representation of high generalization capability. <em>TAFFC</em>, <em>16</em>(1), 68-82. (<a href='https://doi.org/10.1109/TAFFC.2024.3412152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of multi-speaker emotional speech synthesis is to generate speech for a designated speaker in a desired emotional state. The task is challenging due to the presence of speech variations, such as noise, content, and timbre, which can obstruct emotion extraction and transfer. This paper proposes a new approach to performing multi-speaker emotional speech synthesis. The proposed method, which is based on a seq2seq synthesizer, integrates emotion embedding as a conditioned variable to convey exact emotional information from reference audio to the synthesized speech. To boost emotion representation capability, we utilize a three-dimensional acoustic feature as input. And an emotion generalization module with adaptive instance normalization (AdaIN) is proposed to obtain emotion embedding with high generalization ability, which also results in improved controllability. The derived emotion embedding from the generalization module can be readily conditioned by affine parameters, allowing for control both the emotion category and the emotion intensity of synthesized speech. Various emotional speech synthesis experimental results of the propposed method demonstrate its state-of-the-art performance in multi-speaker emotional speech synthesis, coupled with its advantage of high emotion controllability.},
  archive      = {J_TAFFC},
  author       = {Junjie Zheng and Jian Zhou and Wenming Zheng and Liang Tao and Hon Keung Kwan},
  doi          = {10.1109/TAFFC.2024.3412152},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {68-82},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Controllable multi-speaker emotional speech synthesis with an emotion representation of high generalization capability},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative data-driven study of intensity-based categorical emotion representations for MER. <em>TAFFC</em>, <em>16</em>(1), 56-67. (<a href='https://doi.org/10.1109/TAFFC.2024.3411651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-driven analysis and modeling of music-perceived emotions have widespread applications in MIR, with representations of perceived musical emotions forming a crucial component. Though some emotion representations are popular in the literature, their relative merits and demerits in terms of expressiveness and broad applicability have been sparsely studied. The application-specific emotion representations used in multiple studies lead to incomparability of algorithms and performance metrics and non-reusability of representation-specific emotion data across studies. In this work, we study an intensity ratings-based, categorical emotion representation called Emotion-Word Intensity-Value (EWIV) representation, with emotion classes adapted from the aesthetic concept of Nava Rasa. We also introduce EmoRaga - a novel clip-set annotated with perceived emotions and emotion motifs for emotion analysis of Hindustani classical music. We explore the applicability of EWIV towards diverse MIR applications, e.g., dominant and secondary emotion identification, and temporal emotion pattern study. Last, we report a data-driven comparison of EWIV, categorical and dimensional representations, using statistical out-of-sample goodness of fit tests to measure and compare their representativeness over both benchmark datasets and collected emotion data. We conclude that EWIV is applicable to a range of MIR tasks, with higher representative and generalization potential compared to popular representations in certain cases.},
  archive      = {J_TAFFC},
  author       = {Sanga Chaki and Sourangshu Bhattacharya and Junmoni Borgohain and Priyadarshi Patnaik and Raju Mullick and Gouri Karambelkar},
  doi          = {10.1109/TAFFC.2024.3411651},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {56-67},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A comparative data-driven study of intensity-based categorical emotion representations for MER},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minority views matter: Evaluating speech emotion classifiers with human subjective annotations by an all-inclusive aggregation rule. <em>TAFFC</em>, <em>16</em>(1), 41-55. (<a href='https://doi.org/10.1109/TAFFC.2024.3411290'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When selecting test data for subjective tasks, most studies define ground truth labels using aggregation methods such as the majority or plurality rules. These methods discard data points without consensus, making the test set easier than practical tasks where a prediction is needed for each sample. However, the discarded data points often express ambiguous cues that elicit coexisting traits perceived by annotators. This paper addresses the importance of considering all the annotations and samples in the data, highlighting that only showing the model's performance on an incomplete test set selected by using the majority or plurality rules can lead to bias in the models’ performances. We focus on speech-emotion recognition (SER) tasks. We observe that traditional aggregation rules have a data loss ratio ranging from 5.63% to 89.17%. From this observation, we propose a flexible method named the all-inclusive aggregation rule to evaluate SER systems on the complete test data. We contrast traditional single-label formulations with a multi-label formulation to consider the coexistence of emotions. We show that training an SER model with the data selected by the all-inclusive aggregation rule shows consistently higher macro-F1 scores when tested in the entire test set, including ambiguous samples without agreement.},
  archive      = {J_TAFFC},
  author       = {Huang-Cheng Chou and Lucas Goncalves and Seong-Gyun Leem and Ali N. Salman and Chi-Chun Lee and Carlos Busso},
  doi          = {10.1109/TAFFC.2024.3411290},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {41-55},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Minority views matter: Evaluating speech emotion classifiers with human subjective annotations by an all-inclusive aggregation rule},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotion recognition in the real world: Passively collecting and estimating emotions from natural speech data of individuals with bipolar disorder. <em>TAFFC</em>, <em>16</em>(1), 28-40. (<a href='https://doi.org/10.1109/TAFFC.2024.3407683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotions provide critical information regarding a person's health and well-being. Therefore, the ability to track emotion and patterns in emotion over time could provide new opportunities in measuring health longitudinally. This is of particular importance for individuals with bipolar disorder (BD), where emotion dysregulation is a hallmark symptom of increasing mood severity. However, measuring emotions typically requires self-assessment, a willful action outside of one's daily routine. In this paper, we describe a novel approach for collecting real-world natural speech data from daily life and measuring emotions from these data. The approach combines a novel data collection pipeline and validated robust emotion recognition models. We describe a deployment of this pipeline that included parallel clinical and self-report measures of mood and self-reported measures of emotion. Finally, we present approaches to estimate clinical and self-reported mood measures using a combination of passive and self-reported emotion measures. The results demonstrate that both passive and self-reported measures of emotion contribute to our ability to accurately estimate mood symptom severity for individuals with BD.},
  archive      = {J_TAFFC},
  author       = {Emily Mower Provost and Sarah H Sperry and James Tavernor and Steve Anderau and Anastasia Yocum and Melvin G McInnis},
  doi          = {10.1109/TAFFC.2024.3407683},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {28-40},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Emotion recognition in the real world: Passively collecting and estimating emotions from natural speech data of individuals with bipolar disorder},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FEAD: Introduction to the fNIRS-EEG affective database - Video stimuli. <em>TAFFC</em>, <em>16</em>(1), 15-27. (<a href='https://doi.org/10.1109/TAFFC.2024.3407380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents FEAD, a fNIRS-EEG Affective Database that can be used for training emotion recognition models. The electrical activity and brain hemodynamic responses of 37 participants were recorded, as well as the categorical and dimensional emotion ratings they gave to 24 affective audio-visual stimuli. The relationship between the neurophysiological signals with the subjective ratings was investigated, with a significant correlation found in the prefrontal cortex region. A binary classification of affective states was performed using a subject-dependent approach, taking into account the fusion of both modalities, functional Near-Infrared Spectroscopy and Electroencephalography, and each single modality separately. In addition, we explored the temporal dynamics of the recorded data in shorter trials and found that the fusion of features from both modalities yielded significantly better results than using a single modality. This database will be made publicly available with the aim to encourage researchers to develop more advanced algorithms for affective computing and emotion recognition.},
  archive      = {J_TAFFC},
  author       = {Alireza Farrokhi Nia and Vanessa Tang and Valery Malyshau and Amit Barde and Gonzalo Daniel Maso Talou and Mark Billinghurst},
  doi          = {10.1109/TAFFC.2024.3407380},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {15-27},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {FEAD: Introduction to the fNIRS-EEG affective database - Video stimuli},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic causal disentanglement model for dialogue emotion detection. <em>TAFFC</em>, <em>16</em>(1), 1-14. (<a href='https://doi.org/10.1109/TAFFC.2024.3406710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion detection is a critical technology extensively employed in diverse fields. While the incorporation of commonsense knowledge has proven beneficial for existing emotion detection methods, dialogue-based emotion detection encounters numerous difficulties and challenges due to human agency and the variability of dialogue content. In dialogues, human emotions tend to accumulate in bursts. However, they are often implicitly expressed. This implies that many genuine emotions remain concealed within a plethora of unrelated words and dialogues. In this article, we propose a Dynamic Causal Disentanglement Model founded on the separation of hidden variables, which effectively decomposes the content of dialogues and investigates the temporal accumulation of emotions, thereby enabling more precise emotion recognition. First, we introduce a novel Causal Directed Acyclic Graph (DAG) to establish the correlation between hidden emotional information and other observed elements. Subsequently, our approach utilizes pre-extracted personal states and utterance topics as guiding factors for the distribution of hidden variables, aiming to separate irrelevant ones. Specifically, we propose a Dynamic Causal Disentanglement Model to infer the propagation of utterances and hidden variables, enabling the accumulation of emotion-related information throughout the conversation. To guide this disentanglement process, we leverage the GPT4.0 and LSTM networks to extract utterance topics and personal states as observed information. Finally, we test our approach on popular datasets in dialogue emotion detection and relevant experimental results verified the model's superiority.},
  archive      = {J_TAFFC},
  author       = {Yuting Su and Yichen Wei and Weizhi Nie and Sicheng Zhao and Anan Liu},
  doi          = {10.1109/TAFFC.2024.3406710},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {1-3},
  number       = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Dynamic causal disentanglement model for dialogue emotion detection},
  volume       = {16},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
