<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms">THMS - 84</h2>
<ul>
<li><details>
<summary>
(2025). StoryExplorer: A visualization framework for storyline generation of textual narratives. <em>THMS</em>, <em>55</em>(5), 886--895. (<a href='https://doi.org/10.1109/THMS.2025.3592357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of the exponentially increasing volume of narrative texts such as novels and news, readers struggle to extract and consistently remember storylines from these intricate texts due to the constraints of human working memory and attention span. To tackle this issue, we propose a visualization approach StoryExplorer, which facilitates the process of knowledge externalization of narrative texts and further makes the form of mental models more coherent. Through the formative study and close collaboration with two domain experts, we identified key challenges for the extraction of the storyline. Guided by the distilled requirements, we then propose a set of workflow (i.e., insight finding-scripting-storytelling) to enable users to interactively generate fragments of narrative structures. We then propose a visualization system StoryExplorer that combines stroke annotation and GPT-based visual hints to quickly extract story fragments and interactively construct storylines. To evaluate the effectiveness and usefulness of StoryExplorer, we conducted two case studies and in-depth user interviews with 16 target users. The result shows that users can conveniently and effectively extract the storyline by using StoryExplorer along with the proposed workflow.},
  archive      = {J_THMS},
  author       = {Li Ye and Lei Wang and Shaolun Ruan and Heyu Wang and Yuwei Meng and Yigang Wang and Wei Chen and Zhiguang Zhou},
  doi          = {10.1109/THMS.2025.3592357},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {886--895},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {StoryExplorer: A visualization framework for storyline generation of textual narratives},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Palpation characteristics of an instrumented virtual cricothyroidotomy simulator. <em>THMS</em>, <em>55</em>(5), 876--885. (<a href='https://doi.org/10.1109/THMS.2025.3592791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cricothyroidotomy (CCT) is a critical, life-saving procedure requiring the identification of key neck landmarks through palpation. Interactive virtual simulation offers a promising, cost-effective approach to CCT training with high visual realism. However, developing the palpation skills necessary for CCT requires a haptic interface with tactile sensitivity comparable to human fingers. Such interfaces are often represented by plastic partial mannequins, which require further adaptation to integrate into virtual environments. This study introduces an instrumented physical palpation interface for CCT, integrated into a virtual surgical simulator, and tested on 10 surgeons who practiced the procedure over a training period. Data on haptic interactions collected during the training was analyzed to evaluate participants’ palpation skills and explore their force modulation strategies about landmark identification scores. Our findings suggest that trainees become more precise in their exploration over time, apply greater normal forces around target areas. Initial landmark identification performance influences adjustments in the overall applied pressure.},
  archive      = {J_THMS},
  author       = {Melih Turkseven and Trudi Di Qi and Ganesh Sankaranarayanan and Suvranu De},
  doi          = {10.1109/THMS.2025.3592791},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {876--885},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Palpation characteristics of an instrumented virtual cricothyroidotomy simulator},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing the validity of multiparticipant distributed simulation for understanding and modeling road user interaction. <em>THMS</em>, <em>55</em>(5), 865--875. (<a href='https://doi.org/10.1109/THMS.2025.3591506'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding driver–pedestrian interactions at unsignalized locations has gained additional importance due to recent advancements in vehicle automation. Naturalistic observations can only provide correlational data of limited value for understanding and modeling the mechanisms underlying road user interaction. Therefore, controlled studies in virtual reality (VR) are an important complement, but conventional methods can only accommodate a single human participant. Recently, there has been some interest in studying interactions in VR, by means of distributed simulation, involving multiple human participants. However, there is a lack of validation of this method. Here, we provide a validation study, focusing on a distributed vehicle–pedestrian interaction setup, where pairs of one driver and one pedestrian interacted under various kinematic conditions in a connected virtual environment. To test the validity of the distributed simulation, we used a naturalistic dataset collected in the same U.K. city, at similar locations, and compared the observed behavior between the two settings. Our results indicate a good relative validity of the simulator study, where road users showed similar nonverbal communication behavior in both datasets. As an additional means of validation, we also leveraged a set of game theoretic models that were developed based on the simulator studies, and found that when applied to the naturalistic dataset, we obtained similar (although not identical) model selection results. The findings suggest that distributed simulation can also be useful for development of computational models of interaction. Overall, the findings suggest that distributed simulation can be a highly valuable tool for studying and modeling road user interactions.},
  archive      = {J_THMS},
  author       = {Amir Hossein Kalantari and Yi-Shin Lin and Ali Mohammadi and Natasha Merat and Gustav Markkula},
  doi          = {10.1109/THMS.2025.3591506},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {865--875},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Testing the validity of multiparticipant distributed simulation for understanding and modeling road user interaction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving tactical decision-making through multiobjective contrastive explanations. <em>THMS</em>, <em>55</em>(5), 855--864. (<a href='https://doi.org/10.1109/THMS.2025.3591163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the effectiveness of multiobjective counterfactual explanations (MOCEs) in helping individuals learn tactics, or rules of thumb, to apply when required to select a course of action in a specific context. In this setting, a counterfactual explanation compares one course of action against another. A MOCE presents this comparison by highlighting how the two options differ across a range of objectives or metrics. We conduct a study in which participants are presented with various scenarios alongside courses of action that could be implemented in those scenarios. Counterfactual explanations, including those involving multiple objectives, are used to identify the positive and negative aspects of the provided options. Participants were then required to identify the best course of action in various contexts. Participants trained with MOCE outperformed those given no explanations in seven of eight scenarios and those given single-objective counterfactual explanations (SOCEs) in four. SOCEs gave participants an aggregated outcome (expected rewards) without breaking these into specific objectives. MOCE improved tactic learning, but participants provided with SOCE or no explanation performed better in multitactic scenarios. These findings suggest that MOCE enhances tactical decision-making, but further research is needed for multitactic integration.},
  archive      = {J_THMS},
  author       = {Michelle Blom and Ronal Singh and Tim Miller and Liz Sonenberg and Kerry Trentelman and Adam Saulwick and Steven Wark},
  doi          = {10.1109/THMS.2025.3591163},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {855--864},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Improving tactical decision-making through multiobjective contrastive explanations},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Validation of a formal method for human error rate prediction with negative transfer. <em>THMS</em>, <em>55</em>(5), 844--854. (<a href='https://doi.org/10.1109/THMS.2025.3593085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human error is often associated with system failures. The complexity of human-automation interaction can make it difficult to anticipate what errors can occur and how they contribute to failures. Previous research has shown that task analytic behavior modeling with the enhanced operator function model and the cognitive reliability analysis method (CREAM) can be combined with statistical model checking to make predictions about human error rates, their stochastic impact on system failures, and the effect of negative transfer of design changes on these predictions. These efforts were successful, but the validation studies used artificial examples with limited data. Predictions also slightly overestimated error rates. This article addresses these deficiencies by conducting a validation study based on the prescription order entry interface of the OpenEMR electronic medical record. As part of this, we explored how prediction accuracy for the OpenEMR application changed based on the inclusion/exclusion of planning errors: errors based on people’s ability to formulate task plans, which we hypothesized contributed to error rate overestimation. Results found that our method’s predictions aligned with those observed in the experiment, especially when planning errors were excluded. Negative transfer conditions did not manifest significant differences in error rates experimentally or in model predictions. These results suggest that negative transfer’s impact on human–computer interaction may be overstated in the literature. Finally, higher error rates were observed between the original OpenEMR prescription order entry interface compared to an alternative that we tested. We highly suggest that OpenEMR adopt the alternative.},
  archive      = {J_THMS},
  author       = {Yeonbin Son and Matthew L. Bolton and Emma Crooks and Hannah Palmer and Eunsuk Kang and Christopher Daly},
  doi          = {10.1109/THMS.2025.3593085},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {844--854},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Validation of a formal method for human error rate prediction with negative transfer},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicle routing incorporating implicit preferences: An omnidimensional Human–Algorithm collaboration approach. <em>THMS</em>, <em>55</em>(5), 834--843. (<a href='https://doi.org/10.1109/THMS.2025.3594577'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the complexity and dynamism of real-world needs, traditional vehicle routing solutions usually do not sufficiently consider implicit user preferences, leading to low acceptance and reduced practical performance. To address this issue, we propose an omnidimensional human–algorithm collaboration framework that features a novel “task assignment first, route construction second” algorithm and omnidimensional human–algorithm interaction methods. This framework supports users in expressing complex preference information across four dimensions: decision variables, constraints, objectives, and solutions. The algorithm is designed to learn user preferences and utilize computational search to provide efficient, tailored feedback. A user-friendly graphical interface system was developed to facilitate intuitive human–algorithm interaction. We conducted user experiments with 20 participants based on a real-world multiobjective road cleaning problem. The results demonstrated the system’s effectiveness in incorporating users’ implicit preferences—beyond explicit preferences—and its usability in providing positive user experience. This study represents a significant step toward making vehicle routing algorithms more practical and user-centric.},
  archive      = {J_THMS},
  author       = {Mingwei Chen and Liang Ma and Xiaofang Wang},
  doi          = {10.1109/THMS.2025.3594577},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {834--843},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Vehicle routing incorporating implicit preferences: An omnidimensional Human–Algorithm collaboration approach},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A recent review on subjective and objective assessment of trust in human autonomy teaming. <em>THMS</em>, <em>55</em>(5), 819--833. (<a href='https://doi.org/10.1109/THMS.2025.3589981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing sophistication of autonomous systems and robotics has spurred research into the unique dynamics of human-autonomy teaming (HAT). These advanced technologies aim to enhance decision-making, situational awareness, mutual understanding, and interpersonal relationships, thereby minimizing risks in collaborative endeavors. However, achieving effective HAT requires careful attention to team trust, particularly in scenarios that move beyond simple human–machine dyads to involve complex, multiagent systems and distributed teams. This review undertakes a comprehensive exploration and evaluation of trust measurement techniques within the domain of HAT, focusing on methods that are sensitive to the nuances of these complex team dynamics. Emphasising the significance of trust measurement in optimising team performance, the review categorizes existing empirical works into subjective (e.g., self-report, questionnaires, surveys) and objective (e.g., behavioral, physiological) indices. Drawing insights from recent literature (2019–2024), the article explores the complexities of trust measurement, addressing methodologies employed by researchers and synthesizing their findings. The study suggests directions for further investigation into improving trust assessment techniques and developing practical models to better suit the evolving context of human-autonomy collaboration. The review highlights gaps in current methods and suggests avenues for future research, particularly in refining trust calibration models for dynamic, evolving contexts in human-autonomy collaboration and for understanding how trust is distributed and managed across complex team structures.},
  archive      = {J_THMS},
  author       = {Julakha Jahan Jui and Imali T. Hettiarachchi and Asim Bhatti and Mohamed Ragab Mahmoud Farghaly and Douglas Creighton},
  doi          = {10.1109/THMS.2025.3589981},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {819--833},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A recent review on subjective and objective assessment of trust in human autonomy teaming},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic estimation of mental workload and operator accuracy for time-constrained binary classification tasks. <em>THMS</em>, <em>55</em>(5), 809--818. (<a href='https://doi.org/10.1109/THMS.2025.3591550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human cognitive states, such as mental workload, play a pivotal role in decision making processes within human automation teams. Although subjective measures of mental workload can be obtained using standard questionnaires, such as the NASA-TLX, their administration is often impractical as it interferes with the primary tasks of the human operator. Therefore, it is of interest to estimate these subjective measures from less intrusive observations. Evidence suggests that mental workload is a dynamic process so incorporating historical measurements could reduce its estimation error. In addition, the estimation of operator performance in human automation teams is essential in optimizing task effectiveness and facilitating efficient resource allocation. In this work, we consider a scenario where a human and an automation solve binary classification tasks under time constraints. We present and compare different dynamic schemes to estimate the operator’s performance, i.e., classification accuracy, and its subjective ratings on subscales of the NASA-TLX questionnaire, which measure mental workload across multiple dimensions. These schemes differ in the information available for estimation. We test these schemes on data collected from a scenario, where a human and an automation perform a series of classification tasks for simulated mobile objects. Our analysis of the interaction data and the estimation schemes indicates that employing dynamic estimation for certain NASA-TLX subscale ratings leads to decreased estimation errors.},
  archive      = {J_THMS},
  author       = {Raihan Seraj and Aditya Mahajan and Jerome Le Ny},
  doi          = {10.1109/THMS.2025.3591550},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {809--818},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Dynamic estimation of mental workload and operator accuracy for time-constrained binary classification tasks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Role-based human-machine collaboration task-allocation strategy in multiagent environment. <em>THMS</em>, <em>55</em>(5), 799--808. (<a href='https://doi.org/10.1109/THMS.2025.3591603'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human–machine collaboration task-allocation problem involves three major challenges: role diversity, capability heterogeneity, and task dynamics. Most existing studies treat humans and machines as parallel units through a static “Human + Machine” additive paradigm, which neglects the evolution of capability during collaboration. Some works “deeply couple” relatively low-autonomy machines with humans, thereby limiting the system’s flexibility in resource scheduling and dynamic reconfiguration. This study analyzes the problem from a multiagent system perspective and classifies execution units into three types: human agents, machine agents, and human–machine collaborative agents, and distinguishes between their independent and collaborative capabilities. Next, we propose the dynamic short-board balance synergy assessment method, which integrates the “short-board” concept to quantify collaboration performance and leverages agents that have low independent but high collaborative capabilities. By incorporating multiple constraints, we establish the role-based human–machine collaboration (RBHMC) model, prove its NP-hardness, and design a multi-level solving approach to handle small-scale and medium-to-large-scale data separately. The experimental results indicate that, compared with “Human + Machine” and “Deep Coupling” models, RBHMC outperforms in task completion rate, resource utilization, and system robustness. An industrial case study further validates its applicability and superiority in real-world settings. Finally, RBHMC’s transferability is validated through vertical technology adaptation and horizontal scenario migration, providing a scalable solution for multidomain human–machine collaboration in complex scenarios.},
  archive      = {J_THMS},
  author       = {Xinlei Zhang and Zhaoquan Zhu and Yuanbai Li and Haibin Zhu and Yuxiang Sun and Xianzhong Zhou},
  doi          = {10.1109/THMS.2025.3591603},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {799--808},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Role-based human-machine collaboration task-allocation strategy in multiagent environment},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Natural Human–Robot interaction for vascular interventional surgery: Design and evaluation of a leader device with haptic feedback. <em>THMS</em>, <em>55</em>(5), 788--798. (<a href='https://doi.org/10.1109/THMS.2025.3591542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robot-assisted vascular interventional surgery is a hot topic in the interdisciplinary field of medical science and engineering, it has important research significance. Compared with traditional manual operation, it shows obvious advantages, such as avoiding the radiation exposure to the interventionist, reducing the workload of the interventionist, improving the precision of the operation, and guaranteeing the safety of the operation. However, the human–robot interaction ability has always been a major challenge that cannot be ignored. In this article, we designed and developed a leader device that enables the interventionists to experience the state of surgical instruments naturally and realistically when operating it, as if they were operating actual surgical instruments. The innovations can be summarized as follows: the structure of the leader device is highly consistent with the interventionist’s operating habit, which greatly reduces the interventionist’s operating difficulty. Haptic feedback in both translational direction and circumferential direction is achieved to provide haptic guidance to the interventionists and improve their surgical presence. The leader device is optimized in both structural design and functional realization compared to existing leader devices. This study has great potential in improving human-robot interaction ability, and could provide a reference for the design and evaluation of the leader device.},
  archive      = {J_THMS},
  author       = {Xiaoliang Jin and Aiguo Song and Lifeng Zhu and Cheng Wang},
  doi          = {10.1109/THMS.2025.3591542},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {788--798},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Natural Human–Robot interaction for vascular interventional surgery: Design and evaluation of a leader device with haptic feedback},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating HMIs to foster communications between conventional vehicles and autonomous vehicles at intersections. <em>THMS</em>, <em>55</em>(5), 777--787. (<a href='https://doi.org/10.1109/THMS.2025.3595746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mixed traffic environments that involve conventional vehicles (CVs) and autonomous vehicles (AVs), it is essential for CV drivers to maintain an appropriate level of situation awareness (SA) to ensure safe and efficient interactions with AVs. While previous research has established the benefits of external human–machine interfaces (HMIs) for communicating AV intent, this study extended this knowledge by focusing on the vital but underexplored interaction with CV drivers. Specifically, we investigated how AV communication through HMIs affected CV drivers by systematically comparing internal (iHMI) and external (eHMI) interfaces, and examined their impact on CV driver awareness, cognitive load, and behavior. Initially, we designed eight HMI concepts through a human-centered design process. The two highest-rated concepts were selected for implementation as eHMIs and iHMIs. Subsequently, we designed a within-subjects experiment with three conditions: a control condition without any communication HMI, and two treatment conditions using eHMIs and iHMIs as communication means. We investigated the effects of these conditions on 50 participants in a virtual environment (VR) driving simulator. Self-reported assessments and eye-tracking measures were employed to evaluate participants’ SA, trust, acceptance, and mental workload. Results indicated that the iHMI condition resulted in superior SA among participants and improved trust in the AV compared to the control and eHMI conditions. Additionally, iHMI led to a comparatively lower increase in mental workload compared to the other two conditions. Our study contributes to the development of effective AV-CV communications and has the potential to inform the design of future AV systems.},
  archive      = {J_THMS},
  author       = {Lilit Avetisyan and Aditya Deshmukh and X. Jessie Yang and Feng Zhou},
  doi          = {10.1109/THMS.2025.3595746},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {777--787},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Investigating HMIs to foster communications between conventional vehicles and autonomous vehicles at intersections},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neuroergonomics in digital operating rooms: Applying the two-competitor model of attention to the surgical context. <em>THMS</em>, <em>55</em>(5), 765--776. (<a href='https://doi.org/10.1109/THMS.2025.3601222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to high workload and often excessive working hours, team members in operating rooms perform surgical procedures under difficult conditions and often without sufficient breaks. Despite this, the team must deal with incomplete information and unexpected distractions. This requires a suitable level of attention and the ability to balance the demands of the task with available cognitive resources. Advances in measurement technology and data analysis in neurotechnology open up new possibilities for the assessment of attention processes. Increasingly complex and demanding surgeries, especially, could benefit from the application of neuroergonomic automated assistants to minimize distraction, stress, and fatigue, and to facilitate interactions between team members. Such assistants could improve performance via monitoring of cognitive and affective states as well as the implementation of suitable interventions strategies. Understanding the impact of distractions on performance, enhancing individuals’ resilience to distractions, and potentially employing artificial assistants to mitigate their effects are critical future goals. In order to support such future developments, a standard taxonomy of attention in the operating theater is needed, as is a broader consensus regarding the nature of distraction. Ideally, such a model would serve as a basis for comparison between studies conducted in different laboratories, and in principle could also be used to bridge the gap between the laboratory and the real scenario. Here, we propose the adoption of a model of attention previously shown to be effective for modeling levels of attention in immersion and describe its application in the surgical context.},
  archive      = {J_THMS},
  author       = {David Thinnes and Alexander L. Francis and Volkan Sayman and Daniel Guagnin and Matthias W. Laschke and Michael D. Menger and Jonas Roller and Daniel J. Strauss},
  doi          = {10.1109/THMS.2025.3601222},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {765--776},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Neuroergonomics in digital operating rooms: Applying the two-competitor model of attention to the surgical context},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RingFormer: A neural vocoder with ring attention and convolution-augmented transformer. <em>THMS</em>, <em>55</em>(5), 756--764. (<a href='https://doi.org/10.1109/THMS.2025.3591502'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While transformers demonstrate outstanding performance across various audio tasks, their application to neural vocoders remains challenging. Neural vocoders require the generation of long audio signals at the sample level, which demands high temporal resolution. This results in significant computational costs for attention map generation and limits their ability to efficiently process both global and local information. In addition, the sequential nature of sample generation in neural vocoders poses difficulties for real-time processing, making the direct adoption of transformers impractical. To address these challenges, we propose RingFormer, a neural vocoder that incorporates the ring attention mechanism into a lightweight transformer variant, the convolution-augmented transformer. Ring attention effectively captures local details while integrating global information, making it well suited for processing long sequences and enabling real-time audio generation. RingFormer is trained using adversarial training with two discriminators. It is integrated as a replacement for the decoder module in the variational inference with adversarial learning for end-to-end text-to-speech (VITS) text-to-speech framework, enabling fair comparisons with state-of-the-art vocoders such as HiFi-GAN, iSTFT-Net, and BigVGAN under identical conditions using various objective and subjective metrics. Experimental results show that RingFormer achieves comparable or superior performance to existing models, particularly excelling in real-time audio generation.},
  archive      = {J_THMS},
  author       = {Seongho Hong and Yong-Hoon Choi},
  doi          = {10.1109/THMS.2025.3591502},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {756--764},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {RingFormer: A neural vocoder with ring attention and convolution-augmented transformer},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised face deocclusion via 3-D face reconstruction with outlier segmentation. <em>THMS</em>, <em>55</em>(5), 746--755. (<a href='https://doi.org/10.1109/THMS.2025.3585780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face occlusion poses a challenge for many human–machine systems, such as facial expression perception, social signal analysis, and human identity verification. Accurate face deocclusion is essential for improving the performance of identity recognition, expression recognition, and the robustness of human–machine systems. As a result, this area has attracted significant attention from researchers in recent years. However, most existing methods rely heavily on synthetic occluded face datasets and predefined occlusion masks labels, which limits their applicability in real-world scenarios. To this end, we propose a novel self-supervised generative adversarial networks (GANs)-based framework for face deocclusion in this study, which integrates 3-D facial reconstruction with outlier segmentation guidance. To achieve reliable self-supervised occlusion guidance, we introduce an outlier segmentation module that utilizes statistical priors to generate accurate occlusion masks, facilitating the deocclusion process. Furthermore, we design a GAN-based dual-branch module, which is capable of simultaneously generating the occlusion mask and the deoccluded face. Extensive experiments on the widely used datasets demonstrate the superior performance of our approach on existing methods. Our method achieves 35.71 in peak signal to noise ratio (PSNR) and 0.891 in structural similarity index measure (SSIM) for occluded face restoration, outperforming state-of-the-art techniques.},
  archive      = {J_THMS},
  author       = {Haodong Jin and Muwei Jian and Derui Ding and Hui Yu},
  doi          = {10.1109/THMS.2025.3585780},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {746--755},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Self-supervised face deocclusion via 3-D face reconstruction with outlier segmentation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Why highly reliable decision support systems often lead to suboptimal performance and what we can do about it. <em>THMS</em>, <em>55</em>(5), 736--745. (<a href='https://doi.org/10.1109/THMS.2025.3584662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a growing number of application domains, human decision-making is being supported by automated systems. While previous research has focused extensively on the negative consequences of automation support in terms of an overuse of such systems, we argue that this focus has largely overlooked another crucial issue: Humans often deteriorate the performance of automation. Specifically, human–automation dyads commonly perform worse than the system alone because humans, in an attempt to improve decisions, unfortunately interfere with correct system recommendations. This problem will only grow as systems based on artificial intelligence (AI) become more reliable and the gap between human-only and system-only performance continues to widen. We therefore outline the need for research that addresses this persisting and increasingly relevant issue. One approach to counteract this problem is to make systems more transparent and give humans more information on the system. However, while numerous explainability approaches have been brought forward, only very few show convincing effects. To be truly useful, we argue that systems need to be explainable in terms of effective behavioral guidance. Furthermore, beyond just thinking about how to enable humans to better adapt to the system (as is the case with explainability approaches), systems should be more human-centric, taking into account human strengths and weaknesses, and ultimately adapting to humans to enable synergy between humans and AI.},
  archive      = {J_THMS},
  author       = {Tobias Rieger and Linda Onnasch and Eileen Roesler and Dietrich Manzey},
  doi          = {10.1109/THMS.2025.3584662},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {736--745},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Why highly reliable decision support systems often lead to suboptimal performance and what we can do about it},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cinematographic-aware coherent shot assembly for how-to vlog generation. <em>THMS</em>, <em>55</em>(5), 726--735. (<a href='https://doi.org/10.1109/THMS.2025.3588768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The how-to vlog has gained popularity as a medium for learning practical skills, such as cooking and handcrafting. The sequencing process of these videos should meticulously integrate storytelling and cinematographic elements to ensure audience comprehension and engagement, posing a significant challenge for novice creators. Pioneer efforts have adhered to predefined editing rules or assembled clips based on textual logic, limiting their applicability across diverse educational scenarios and causing visual discontinuities. In contrast, we aim to extract rich cinematographic patterns from well-edited professional how-to vlogs to enhance shot assembly. To this end, we identify two crucial aspects influencing educational value: narrative continuity and scale transition. We model the shot assembly process as a task of selecting the next shot and devise a cinematographic-aware contrastive model to learn representations that distinguish between the good next shots against the bad ones. This method incorporates a two-stream cinematographic-aware encoding module for explicit factor encoding and a situation-adaptive attention-based integration module to accommodate varying assembly scenarios. Quantitative results from our novel professional user-generated vlogs dataset (proVlog-HowTo) clearly demonstrate the proposed method’s effectiveness. User study results further indicate its superiority in generating videos with narrative continuity and smooth transitions.},
  archive      = {J_THMS},
  author       = {Yuqi Zhang and Bin Guo and Ying Zhang and Nuo Li and Qianru Wang and Zhiwen Yu and Qing Li},
  doi          = {10.1109/THMS.2025.3588768},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {726--735},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Cinematographic-aware coherent shot assembly for how-to vlog generation},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design graph guided element importance-aware layout generation with multimodality cascade transformer. <em>THMS</em>, <em>55</em>(5), 716--725. (<a href='https://doi.org/10.1109/THMS.2025.3590785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphic designs are pervasive in our daily lives and widely used to communicate information hierarchically to humans. To achieve this, the layout plays an essential role in guiding readers to understand the importance of different elements and comprehend the content. To deal with the rapidly increasing demands of graphic designs, recent studies attempt to automatically generate layouts based on category information and spatial relations, often resulting in layouts with poor communication quality. In this article, we make the first attempt to explore element importance-aware layout generation under the guidance of a novel design graph, which attracts readers’ attention to a layout by formulating aesthetic relations implicitly involved in graphic designs between element pairs. The core of our approach is a learning-based framework with a new multimodality cascade transformer (MCT) in a coarse-to-fine manner. A hierarchical multimodality fusion (HMF) mechanism and two new losses are introduced to guide the training process progressively. We further collect a new fine-grained advertisement poster layout dataset containing more than 30 K layouts labeled with 91 element labels. Both qualitative and quantitative experiments demonstrate the effectiveness of our approach against existing works. We also conduct user studies and cognitive experiments to evaluate the direct adaptability and attractiveness of generated layouts.},
  archive      = {J_THMS},
  author       = {Qiuyun Zhang and Bin Guo and Lina Yao and Xiaotian Qiao and Hao Wang and Ying Zhang and Zhiwen Yu},
  doi          = {10.1109/THMS.2025.3590785},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {716--725},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Design graph guided element importance-aware layout generation with multimodality cascade transformer},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social whole-body movement aids the objective classification of children with autism and typical development by implementing a machine learning approach. <em>THMS</em>, <em>55</em>(5), 707--715. (<a href='https://doi.org/10.1109/THMS.2025.3595901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorder (ASD) is characterized by high prevalence and a time-consuming diagnostic procedure that greatly depends on clinicians’ experience. An objective and efficient approach to identifying ASD is urgently called upon. In current clinical practice, although the diagnosis of ASD is primarily based on the individual’s behavioral presentation, few studies have examined the feasibility of using behavioral indices to identify ASD, and none has investigated whether behavioral features extracted from social whole-body motor behavior during a face-to-face interaction could aid an accurate detection of ASD. This study aimed to address this question by applying machine learning (ML) algorithms to classify children with ASD and typical development (TD). Twenty children with ASD and twenty-one children with TD were videotaped in a face-to-face conversation. A motion energy analysis technique extracted the whole-body movement time series. Features computed were the power of eighty-six frequencies obtained by fast-Fourier transform. Support vector machine and leave-one-out cross validation were implemented. Results showed that a maximum classification accuracy of 97.56% (specificity = 100%, sensitivity = 95.00% ) could be achieved with three features. The contactless and calibration-free approach proposed in this study may help facilitate an objective, effective and efficient diagnosis of ASD.},
  archive      = {J_THMS},
  author       = {Zhong Zhao and Ziyan Qiu and Xiaobin Zhang and Xingda Qu and Xinyao Hu and Jianping Lu},
  doi          = {10.1109/THMS.2025.3595901},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {707--715},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Social whole-body movement aids the objective classification of children with autism and typical development by implementing a machine learning approach},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human-centered fully adaptive radar for gesture recognition in smart environments. <em>THMS</em>, <em>55</em>(5), 695--706. (<a href='https://doi.org/10.1109/THMS.2025.3591369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, radio frequency (RF) sensing or radar has garnered great interest as an emerging modality to enable human–computer interaction via gesture recognition. Current approaches involve utilization of a radar system that transmits a fixed signal with predetermined frequency, bandwidth, and other waveform parameters. However, gesture recognition accuracy can be greatly impacted by radar transmission parameters, which affect computational load and performance. In this work, we introduce a human-centered, fully adaptive radar (HC-FAR) system for ambient gesture recognition in which a programmable, software-defined radar system dynamically changes its RF transmission in response to human behavior. We design and switch between different transmission modes for different human-computer interaction tasks—human presence detection, trigger detection, and command translation—as well as alter processing so as to minimize computational load. In this way, the proposed HC-FAR paradigm enables dynamic management of the tradeoffs between dimensionality of RF data representations and their resulting computational load with real-time classification accuracy. Our results show that HC-FAR significantly reduces the allocation of computational and spectral resources, while enhancing fine-grain gesture recognition via a joint domain multi-input deep neural network, which takes as input the RF micro-Doppler signature, range, and angle profile.},
  archive      = {J_THMS},
  author       = {Emre Kurtoğlu and Sevgi Z. Gurbuz},
  doi          = {10.1109/THMS.2025.3591369},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {695--706},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human-centered fully adaptive radar for gesture recognition in smart environments},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable viewpoint gesture recognition based on a hybrid graph neural network. <em>THMS</em>, <em>55</em>(5), 686--694. (<a href='https://doi.org/10.1109/THMS.2025.3597964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The variations in camera view and hand spatial pose are the main reasons for the low accuracy and poor robustness of gesture recognition systems. In order to achieve accurate and stable gesture recognition with variable viewpoint, this article carries out research based on 3-D non-Euclidean vector graph features and graph neural networks. First, the 3-D information of hand joints is collected to construct a graph-structured gesture feature dataset, and a joint-based 3-D non-Euclidean vector graph method is proposed to solve the problem that similar gesture features are overly sensitive to spatial position and angle changes. Then, a Multi-Head graph attention network is designed and combined with graph convolutional neural network to explore the optimal hybrid graph neural network gesture recognition model. The experimental results show that, on the dataset processed by the joint-based 3-D non-Euclidean vector graph method, the training, testing, and validation accuracies of the optimal model reach 97.07%, 96.95%, and 87.06%, which are increased by 18.46%, 18.88%, and 44.23% compared to the original dataset, respectively. In conclusion, the method in this article is not only more robust to the variable viewpoint gesture recognition problem, but also has the advantages of low computational resource requirement and high real-time performance.},
  archive      = {J_THMS},
  author       = {Shaoxin Sun and Guanghui Chen and Xiaojie Su and Zhenshan Bing and Alois Knoll},
  doi          = {10.1109/THMS.2025.3597964},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {686--694},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Variable viewpoint gesture recognition based on a hybrid graph neural network},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). View-embedding GCN for skeleton-based cross-view gait recognition. <em>THMS</em>, <em>55</em>(5), 674--685. (<a href='https://doi.org/10.1109/THMS.2025.3595213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait has emerged as a promising biometric modality due to its noninvasive nature and the ability to capture samples from a distance. Model-based gait recognition using skeleton data conveys rich information that remains invariant to carried objects and clothing variations. However, viewing a person from different angles alters their gait posture, resulting in increased intrasubject variability compared to intersubject variability. Therefore, we propose a novel framework, view-embedding modified residual graph convolutional network (VeMResGCN), for cross-view gait recognition (CVGR) by exploiting two modules: modified residual graph convolutional network (MResGCN) and view-embedding feature extraction (VeFE) for view-invariant features. A state-of-the-art pose estimation algorithm extracts skeleton key points from raw video input, from which multiple features (e.g., relative joint positions, motion velocities, and bone structures) are computed. The final feature vector for gait recognition is computed by consolidating the features from the MResGCN and VeFE modules. To the best of authors’ knowledge, this work is the first to extract view-invariant features in a unified graph convolutional network (GCN) for skeleton-based CVGR. We evaluate our proposed framework on two of the largest publicly available skeleton datasets, CASIA-B and OUMVLP-Pose, under challenging covariates of clothing variation and carried objects. Results demonstrate that VeMResGCN significantly outperforms state-of-the-art methods with average rank-1 accuracies of 90.3%, 80.7%, and 73.4% for normal, carried object, and clothing variations on CASIA-B, and 71.0% on OU-MVLP in terms of skeleton-based CVGR. These results demonstrate the ability of our proposed framework to maintain superior CVGR performance despite the presence of carried objects and clothing variations. The proposed framework holds strong implications for real-world biometric applications, including robust person reidentification and surveillance systems, where maintaining consistent recognition across varying views and covariates is crucial.},
  archive      = {J_THMS},
  author       = {Md. Zasim Uddin and Ausrukona Ray and Borsha Das and Md. Atiqur Rahman Ahad},
  doi          = {10.1109/THMS.2025.3595213},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {674--685},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {View-embedding GCN for skeleton-based cross-view gait recognition},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal view-reset deep learning with attentional GRUs for skeleton-based human action recognition. <em>THMS</em>, <em>55</em>(5), 664--673. (<a href='https://doi.org/10.1109/THMS.2025.3601386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based human action recognition has attracted significant attention. However, the skeleton spatial invariance and temporal context modeling are recent challenges for most existing methods. This work proposes a view-reset network, which integrates two important branches: spatial view-reset module (SVRM) and temporal attention module (TAM). In the SVRM, the skeleton model from different perspectives is reset in a unified coordinate system, eliminating the influence of viewpoint changes. In the TAM, the perception of temporal features is jointly enhanced by weighting each frame’s importance in the context. Furthermore, the pretrained residual network (ResNet) is used for prediction. The sample size is increased through data augmentation to improve the robustness of the model. The SVRM, TAM, and ResNet form an end-to-end learning network. The ablation study proved that the model could record the key skeletons and frames in the sequence and then reset the human body to a new position, making it easy for learning. The proposed model is evaluated on four challenging benchmarks based on the performance of the cross-view evaluation metrics. Experiments prove that the proposed model has superior performance and surpasses many state-of-the-art algorithms, with an increase of 1.91% over the top ten on the NTU RGB+D 60 dataset.},
  archive      = {J_THMS},
  author       = {Tianyu Ma and Xuna Wang and Hongwei Gao and Zide Liu and Jiahui Yu and Zhaojie Ju},
  doi          = {10.1109/THMS.2025.3601386},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  number       = {5},
  pages        = {664--673},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Spatiotemporal view-reset deep learning with attentional GRUs for skeleton-based human action recognition},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for improving online active engagement during lower limb rehabilitation training based on EEG signals. <em>THMS</em>, <em>55</em>(4), 650--659. (<a href='https://doi.org/10.1109/THMS.2025.3569194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing online active engagement has been shown to effectively improve rehabilitation outcomes. This article introduces a method for improving online active engagement during lower limb rehabilitation training based on electroencephalogram (EEG) signals using an artificial neural network (ANN) recognition model as well as a virtual reality-based induced active engagement rehabilitation training system (VATS). The VATS consists of a bicycle-style rehabilitation instrument and a virtual reality game. Experiments were performed to train the ANN model and to validate the effectiveness of the VATS. We compared the accuracy of the ANN with that of k-nearest neighbor and support vector machine models, where active engagement results from eye tracker analysis were used as the gold standard. In addition, we used the surface electromyography engagement index (Er) to compare the VATS with and without active engagement feedback. The results showed that the ANN model had the highest average accuracy of 76.95% and a maximum accuracy of 81.48%. A comparison of the ANN, KNN, and SVM models revealed statistically significant differences in accuracy (p < 0.05). Significant differences were observed in the Er values between the VATS with and without active engagement feedback. The results indicated that the ANN has an apparent advantage in active engagement recognition. Moreover, the VATS was effective in improving active engagement. The proposed method utilized EEG signals to characterize active engagement intention and achieved real-time enhancement of active engagement through VATS. This method could be further used to improve the rehabilitation training effect in clinical applications.},
  archive      = {J_THMS},
  author       = {Mingyu Du and Yiyun Yao and Jiayao Xiang and Xin Chen and Yinan Jin and Kewen Zhang and Guanjun Bao and Shibo Cai},
  doi          = {10.1109/THMS.2025.3569194},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {650--659},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A method for improving online active engagement during lower limb rehabilitation training based on EEG signals},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of bundle branch block from 12-lead ECG using fifth-order tensor-domain machine learning. <em>THMS</em>, <em>55</em>(4), 639--649. (<a href='https://doi.org/10.1109/THMS.2025.3563292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bundle branch block (BBB) is a cardiac disease that occurs due to the delay in the heart’s electrical activity during a heartbeat. The early detection of BBB using 12-lead electrocardiogram (ECG) is crucial in clinical studies for monitoring the progress of this disease and initiation of treatment. This article proposes a fifth-order tensor-domain machine learning (FOTDML) approach for the automated detection of BBB using 12-lead ECG recordings. The entire duration of the 12-lead ECG recording of each subject is initially segmented into 12-lead beats using a multilead fusion-based QRS complex detection method. Multivariate fast iterative filtering (MVFIF) decomposes each 12-lead beat into intrinsic mode functions or local components. Then, the continuous wavelet transform is utilized to evaluate the time-frequency representation of the MVFIF mode of the 12-lead beat. A fifth-order tensor containing the information on 12-lead ECG beats, leads, MVFIF-domain local components, frequencies (or scales), and samples is evaluated from the entire duration of the 12-lead ECG recording of each subject. Multilinear singular value decomposition is employed to extract features from the fifth-order tensor. Different machine learning (ML)-based methods are utilized to detect BBB from the fifth-order tensor-domain features of each subject’s entire duration 12-lead ECG recording. The suggested approach is evaluated using 12-lead ECG recordings of subjects from two public databases. The results show that the proposed FOTDML approach has yielded the classification accuracy values of 99.88% and 100%, respectively, for healthy control (HC) versus left BBB versus right BBB and HC versus BBB schemes with the subject-independent hold-out validation strategy. The suggested FOTDML approach has demonstrated higher classification performance than the existing methods to detect BBB using 12-lead ECG.},
  archive      = {J_THMS},
  author       = {Chhaviraj Chauhan and Rajesh Kumar Tripathy and Monika Agrawal},
  doi          = {10.1109/THMS.2025.3563292},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {639--649},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Detection of bundle branch block from 12-lead ECG using fifth-order tensor-domain machine learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effect and sensitivity analysis of VR gaming on human contact force perception. <em>THMS</em>, <em>55</em>(4), 629--638. (<a href='https://doi.org/10.1109/THMS.2025.3566497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging evidence suggests that prolonged virtual reality (VR) exposure may impair human sensory systems. Most research has focused on the visual, proprioceptive, and vestibular systems, but the impact of VR on haptic perception remains unclear. In this study, we investigated alterations in human sensitivity to contact force following VR gaming. A force perception task was designed to assess changes in contact force across six difficulty levels with step sizes ranging from 0.5 to 5 N. A total of 18 participants performed the task before VR, after 10 min, and after an additional 20 min of VR. The perceptual accuracy of correctly perceiving force changes at each difficulty level was measured across three test periods. The results indicated that 66.67% of participants experienced a negative impact from VR at the 1-N change step. Perceptual accuracy significantly decreased in this group, with a 9.17% reduction after 10 min and a 17.50% reduction after an additional 20 min. In contrast, minimal effects were observed in the remaining participants. These findings suggest that even short-term VR exposure can impair force discrimination in certain users, with the effects becoming more pronounced over time.},
  archive      = {J_THMS},
  author       = {Yi-Feng Chen and Han Zi and Jing Zhong and Changqi Zhang and Mingjie Dong and Jin Yuan and Mingming Zhang},
  doi          = {10.1109/THMS.2025.3566497},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {629--638},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Effect and sensitivity analysis of VR gaming on human contact force perception},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formalizing motion plan legibility using empirical manual takeover data in autonomous spacecraft docking. <em>THMS</em>, <em>55</em>(4), 619--628. (<a href='https://doi.org/10.1109/THMS.2025.3573243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spacecraft rendezvous and docking maneuvers are becoming highly automated in an attempt to decrease astronaut workload, but still require a human supervisor to continually monitor the system and manually take over control when systems are not performing as expected. This requirement shifts astronaut workload to a monitoring and failure-mitigation task, which must be characterized to assess the influence of this task shift for automated rendezvous and docking (ARD). The physical performance of manual takeover maneuvers are well studied in other fields, such as automated vehicles, but less is known about the factors that influence the decision leading to takeover. This study operationalizes the concept of automation legibility (i.e., intent-expression) to gain insight into when and where supervisors initiate manual takeover. We hypothesized that fundamental aspects of autonomous agent path planning of initial condition and path curvature influence path legibility and takeover decision-making. The study had $N=33$ participants who performed an ARD monitoring task. Metrics for legibility were defined using the positions, where the human initiated a manual takeover along the ARD path. Results support that initial condition, path curvature, and autonomous agent heading were interacting predictors of path legibility and takeover decision timing and location. Increased legibility was correlated to supervisor perceptions of path-appropriateness. The most legible paths aligned the supervisor’s egocentric viewpoint to the path goal, while putting targets of avoidance in the supervisor’s periphery. Characterizations of cooperative performance in human-automation interaction systems from this study can inform future ARD system design that mitigates workload in ARD task performance.},
  archive      = {J_THMS},
  author       = {Hannah Larson and Leia Stirling},
  doi          = {10.1109/THMS.2025.3573243},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {619--628},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Formalizing motion plan legibility using empirical manual takeover data in autonomous spacecraft docking},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When do drivers maneuver: Experimental investigation and inference of perception-response time for tailored safety systems in intelligent vehicles. <em>THMS</em>, <em>55</em>(4), 609--618. (<a href='https://doi.org/10.1109/THMS.2025.3567611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sudden traffic hazards trigger collision-avoidance behaviors in drivers that can significantly impact vehicle dynamics, potentially conflicting with the existing advanced driver assistance systems (ADAS), such as autonomous emergency braking and steering. This behavior can lead to unexpected vehicle movements, further complicating the situation and elevating the risk of accidents. Understanding and tailoring the inference of drivers’ perception-response time (PRT) is essential for optimizing ADAS activation in intelligent vehicles. This approach allows customization for individual drivers, improving safety and ensuring that interventions are personalized and minimally disruptive to normal driving patterns. To achieve this objective, this study performs high-fidelity simulation experiments to gather a comprehensive multidimensional dataset on drivers’ responses in safety-critical scenarios, primarily focusing on PRT and its influencing factors. Using the collision-avoidance behavior data, a driver evidence accumulation model is created to explain PRT distribution and facilitate real-time personalized inferences. We also analyze the relationship between model parameters and real-world physical significance, demonstrating that driver decisions rely on visual evidence accumulation influenced by dynamic interactions in different scenarios. Our proposed model, by offering a detailed understanding of drivers’ perceptual and decision-making processes, aids in developing personalized driver assistance system activation recommendations. This approach seeks to create personalized and adaptive systems within intelligent vehicles, thereby reducing human-machine conflicts and improving the overall safety of intelligent transportation systems.},
  archive      = {J_THMS},
  author       = {Detong Qin and Qingfan Wang and Quan Li and Tianle Lu and Chen Chen and Hong Wang and Bingbing Nie},
  doi          = {10.1109/THMS.2025.3567611},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {609--618},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {When do drivers maneuver: Experimental investigation and inference of perception-response time for tailored safety systems in intelligent vehicles},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMPD: A dual-modality fusion method for cross-spectral pedestrian detection. <em>THMS</em>, <em>55</em>(4), 599--608. (<a href='https://doi.org/10.1109/THMS.2025.3566102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In urban safety, intelligent transportation, and smart security applications, robust pedestrian detection is paramount. Methods that rely solely on visible light imaging struggle in low-light or adverse weather conditions. To address these challenges, we propose dual-modality pedestrian detection (DMPD)—a novel dual-modality pedestrian detection framework that fuses visible and infrared imaging through innovative fusion strategies. The method integrates a modal alignment module to reduce pixel-level misalignment, a differential modal fusion module to effectively combine complementary features while suppressing noise, and a mix module that enhances multiscale feature extraction via integrated convolution and self-attention mechanisms. Furthermore, the enhanced YOLOv7 is used to further boost feature representation and detection accuracy. Experimental results on the public dataset demonstrate that DMPD achieves a detection $mA{{P}_{50}}$ of 97.1% and a real-time speed of 118 FPS, outperforming state-of-the-art methods under both normal and adverse conditions, including fog, rain, and snow. These results confirm the effectiveness of the proposed fusion strategy in harnessing the complementary strengths of visible and infrared modalities, thereby offering a highly robust and scalable solution for pedestrian detection in complex urban environments.},
  archive      = {J_THMS},
  author       = {Huanyu Yang and Jun Wang and Mengchu Tian and Yuming Bo},
  doi          = {10.1109/THMS.2025.3566102},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {599--608},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {DMPD: A dual-modality fusion method for cross-spectral pedestrian detection},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noninvasive blood glucose monitoring system based on deep learning and multiwavelength near-infrared technology. <em>THMS</em>, <em>55</em>(4), 589--598. (<a href='https://doi.org/10.1109/THMS.2025.3579000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent blood glucose monitoring is crucial for patients with diabetes. However, current blood glucose measurement methods are primarily invasive and, thus, cause discomfort and infection risks. Accordingly, this study developed a noninvasive real-time glucose monitoring system based on a deep learning (DL) model (comprising convolutional neural network and long short-term memory network models) and multiwavelength near-infrared (NIR) light technology. The system is equipped with a portable finger gripper with NIR light-emitting diodes emitting at three distinct wavelengths (810, 860, and 940 nm) to illuminate the finger. A triad spectroscopy sensor is then used to capture finger photoplethysmography (PPG) signals within only 6 s. The captured signals are then transmitted to a server-side DL model for glucose prediction. This DL model analyzes the three-wavelength PPG data to predict a user’s blood glucose value. The predicted glucose value is subsequently displayed on a dedicated smartphone app. In contrast to previously proposed systems relying on machine learning for feature extraction, the proposed system uses a DL model to automatically extract glucose-related features from the three-band PPG signals, ultimately leading to blood glucose predictions with a root-mean-square error of 6.62. Furthermore, the proposed system prioritizes user comfort, portability, and stability, thereby offering a convenient and accessible blood glucose monitoring experience.},
  archive      = {J_THMS},
  author       = {Chih-Wei Peng and Bor-Shyh Lin and Hsin-Yen Lin and Yu-Ching Shau and Bor-Shing Lin},
  doi          = {10.1109/THMS.2025.3579000},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {589--598},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Noninvasive blood glucose monitoring system based on deep learning and multiwavelength near-infrared technology},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BreathRelax: A game-based breathing system combines stress relief and engagement. <em>THMS</em>, <em>55</em>(4), 579--588. (<a href='https://doi.org/10.1109/THMS.2025.3582068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breathing exercises have gained popularity as a stress management technique. They provide benefits such as improved lung function, reduced stress, and better quality of life. However, the repetitive nature of these exercises can become boring over time, decreasing people’s motivation to continue practicing them. In this work, we developed BreathRelax, a breath-based video game that integrates a temperature sensor to detect respiratory signals and provides real-time feedback through computer gaming. BreathRelax can achieve an accuracy rate of 95% in recognizing breathing patterns, with a response time of 0.23 s, demonstrating its reliable and responsive performance. To evaluate the effectiveness of BreathRelax, 60 participants were invited for a comprehensive experiment. ANCOVA analysis demonstrated BreathRelax’s superior stress reduction through significantly improved RMSSD, SDNN, and pNN50 stress indices. In addition, the usability evaluation revealed that participants experienced high enjoyment and engagement in the game while finding it easy to operate. BreathRelax delivered promising results in both relaxation effects and user experience.},
  archive      = {J_THMS},
  author       = {Chang-Yu Lin and Edward T.-H. Chu and Chia-Rong Lee},
  doi          = {10.1109/THMS.2025.3582068},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {579--588},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {BreathRelax: A game-based breathing system combines stress relief and engagement},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MADDPG based distributed multirequest pricing mechanisms for sensing tasks. <em>THMS</em>, <em>55</em>(4), 569--578. (<a href='https://doi.org/10.1109/THMS.2025.3580554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, mobile crowdsensing (MCS) has received widespread attention due to various application demands, such as home chronic rehabilitation via Internet, emergency rescues, and smart cities, where more than one requester (convalescents) need to recruit mobile users simultaneously (doctors) equipped with different sensors to fulfil the tasks, such as labeling, sensing, and diagnosis. However, most of existing MCS works focus on only one requester, and demands of all of mobile requesters (convalescents) are uncertain. Therefore, this article proposed semi-distributed and distributed multirequest pricing mechanisms for multirequest MCS on the basis of MADDPG, and laid theoretical foundations for home chronic rehabilitation via Internet. Extensive simulations demonstrate that our mechanisms outweigh existing benchmarks.},
  archive      = {J_THMS},
  author       = {Jiajun Sun and Dianliang Wu},
  doi          = {10.1109/THMS.2025.3580554},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {569--578},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {MADDPG based distributed multirequest pricing mechanisms for sensing tasks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AVE speech: A comprehensive multimodal dataset for speech recognition integrating audio, visual, and electromyographic signals. <em>THMS</em>, <em>55</em>(4), 559--568. (<a href='https://doi.org/10.1109/THMS.2025.3585165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global aging population faces considerable challenges, particularly in communication, due to the prevalence of hearing and speech impairments. To address these, we introduce the AVE speech, a comprehensive multimodal dataset for speech recognition tasks. The dataset includes a 100-sentence Mandarin corpus with audio signals, lip-region video recordings, and six-channel electromyography data, collected from 100 participants. Each subject read the entire corpus ten times, with each sentence averaging approximately two seconds in duration, resulting in over 55 hours of multimodal speech data per modality. Experiments demonstrate that combining these modalities significantly improves recognition performance, particularly in cross-subject and high-noise environments. To our knowledge, this is the first publicly available sentence-level dataset integrating these three modalities for large-scale Mandarin speech recognition. We expect this dataset to drive advancements in both acoustic and nonacoustic speech recognition research, enhancing cross-modal learning and human–machine interaction.},
  archive      = {J_THMS},
  author       = {Dongliang Zhou and Yakun Zhang and Jinghan Wu and Xingyu Zhang and Liang Xie and Erwei Yin},
  doi          = {10.1109/THMS.2025.3585165},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {559--568},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {AVE speech: A comprehensive multimodal dataset for speech recognition integrating audio, visual, and electromyographic signals},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A smart glove based on inductive sensors for hand gesture recognition. <em>THMS</em>, <em>55</em>(4), 549--558. (<a href='https://doi.org/10.1109/THMS.2025.3566941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an enhanced gesture recognition system based on inductive sensors for accurately recognizing American Sign Language (ASL) hand gestures. In this system, conductive threads are employed to sew coils onto a regular glove covering the fingers, wrist, palm, and ulnar positions. To improve sensitivity, a data acquisition system measures a tank circuit formed by each sewn coil and external components. The system is rigorously tested on ten subjects. A comparative study of three machine learning algorithms (MLAs), including random forest (RF), support vector machine (SVM), and K-nearest neighbors (KNNs), is conducted to detect 26 ASL letters. To improve the diversity and generalizability of the MLA, the generative adversarial network (GAN) data augmentation method is provided, expanding the dataset to 5050 trials for each gesture. The results demonstrate impressive accuracy rates of 99.67% and 97.46% using five-fold cross-validation (5F-CV) and leave-one-subject-out cross-validation (LOSO-CV), respectively, for the RF algorithm. This exhibits higher sensitivity in detecting similar gestures compared to the previous design. The proposed solution addresses the limitations of existing hand gesture recognition designs and offers a practical and effective approach to human-computer interaction.},
  archive      = {J_THMS},
  author       = {Maryam Ravan and Alma Abbasnia and Shokoufeh Davarzani and Reza K. Amineh},
  doi          = {10.1109/THMS.2025.3566941},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {549--558},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A smart glove based on inductive sensors for hand gesture recognition},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gait recognition via motion difference representation learning and salient feature modeling. <em>THMS</em>, <em>55</em>(4), 539--548. (<a href='https://doi.org/10.1109/THMS.2025.3576484'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a periodic movement, gait contains informative biometric traits formed by individual body structures, motion patterns, and behavioral habits. Previous gait recognition methods mainly focus on mining the appearance cues from gait sequences, while neglecting the dynamic motion characteristics. Motion cues are important complementary information for generating high-quality gait representations that can help models accurately recognize individuals. In this article, we propose a novel gait recognition framework named GaitDS to model dynamic motion information and construct salient gait representations. Specifically, we develop a motion information perception module that can directly represent dynamic regions during walking and extract fine-grained motion features based on the appearance of body parts over time. In addition, since some frames in gait sequences share partial similarities, we present saliency identity representation learning to focus on key frames along the temporal dimension, and integrate salient identity features to enhance sequence-level representations. Furthermore, a channel enhanced module is designed to generate more discriminative gait representations, where motion and temporal salient features can be complemented with global representations. Compared with existing state-of-the-art methods, our model achieves superior average rank-1 recognition accuracy on three benchmark datasets, i.e., 93.7% on CASIA-B, 92.4% on OU-MVLP, and 50.7% on Gait3D.},
  archive      = {J_THMS},
  author       = {Wei Huo and Ke Wang and Jun Tang and Nian Wang and Dong Liang},
  doi          = {10.1109/THMS.2025.3576484},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {539--548},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Gait recognition via motion difference representation learning and salient feature modeling},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Voice familiarity in a voice-reminders app for elderly care recipients and their family caregivers. <em>THMS</em>, <em>55</em>(4), 529--538. (<a href='https://doi.org/10.1109/THMS.2025.3582259'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on the effect of voice familiarity in voice reminders between elders and their caregivers. A voice application (for desktops and smartphones) was created to study these effects. Seniors, and their family care providers along with medical providers were consulted for voice application design and improvement opportunities using two user tests with 17 care dyads and 15 medical care providers. Results from qualitative content analysis show that care dyads generally prefer familiar voice reminders for routine tasks (such as taking medication) and generally find caregiver-voiced reminders to be acceptable, while medical care providers have mixed opinions about older adults using recorded reminders.},
  archive      = {J_THMS},
  author       = {Karen P. Valdivia and Jamy Li},
  doi          = {10.1109/THMS.2025.3582259},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {529--538},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Voice familiarity in a voice-reminders app for elderly care recipients and their family caregivers},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An actionability assessment tool for enhancing algorithmic recourse in explainable AI. <em>THMS</em>, <em>55</em>(4), 519--528. (<a href='https://doi.org/10.1109/THMS.2025.3582285'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce and evaluate a tool for researchers and practitioners to assess the actionability of information provided to users to support algorithmic recourse. While there are clear benefits of recourse from the user’s perspective, the notion of actionability in explainable AI research remains vague, and claims of ‘actionable’ explainability techniques are based on researchers’ intuitions. Inspired by definitions and instruments for assessing actionability in other domains, we construct a seven-item tool and investigate its effectiveness through two user studies. We show that the tool discriminates actionability across explanation types and that the distinctions align with human judgments. We illustrate the impact of context on actionability assessments, suggesting that domain-specific tool adaptations may foster more human-centred algorithmic systems. This is a valuable step forward for research and practices into actionable explainability and algorithmic recourse, providing the first clear human-centred tool for assessing actionability in explainable AI.},
  archive      = {J_THMS},
  author       = {Ronal Singh and Tim Miller and Liz Sonenberg and Eduardo Velloso and Frank Vetere and Piers Howe},
  doi          = {10.1109/THMS.2025.3582285},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {519--528},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An actionability assessment tool for enhancing algorithmic recourse in explainable AI},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-grained assessment of upper-limb bradykinesia through multimodal feature enhancement and deep learning. <em>THMS</em>, <em>55</em>(4), 508--518. (<a href='https://doi.org/10.1109/THMS.2025.3570704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bradykinesia is a hallmark symptom of Parkinson’s disease (PD) that significantly affects patients’ functional abilities and quality of life. This study proposed a fine-grained classification method for evaluating the level of bradykinesia in PD patients. Based on inertial signals, surface electromyographic (sEMG) signals, and videos obtained from 40 PD patients and 13 healthy subjects, the proposed data preprocessing method extracts 69-D features from inertial and sEMG (IE) signals, and 7-D skeleton features from videos. A two-stream network, including IE stream, skeleton stream, and decision fusion module, was developed using long short-term memory, full convolutional neural networks, and fully connected neural networks. In addition, the IE stream incorporated a feature shrinking module to process high-dimensional features to reduce redundant features. Furthermore, an LSTM-variational autoencoders method was proposed for data augmentation of categories with fewer samples. The proposed method achieved higher recognition rates (pro/supination movements of hands: 85.51%, finger tapping: 88.06%, hand movements: 90.00%) compared to other methods. With low-cost, compact and lightweight methods, bradykinesia in PD patients can be intelligently assessed, which will enhance patient management and treatment efficiency.},
  archive      = {J_THMS},
  author       = {Fang Lin and Zhelong Wang and Zhenglin Li and Hongyu Zhao and Xin Shi and Ruichen Liu and Jiaxi Li and Daoyong Peng and Bo Ru},
  doi          = {10.1109/THMS.2025.3570704},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {508--518},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Fine-grained assessment of upper-limb bradykinesia through multimodal feature enhancement and deep learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human perception of AI capabilities at classifying perturbed roadway signs. <em>THMS</em>, <em>55</em>(4), 499--507. (<a href='https://doi.org/10.1109/THMS.2025.3573173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence (AI) is crucial to numerous functions required for driving automation systems, including the computer vision techniques used to detect the roadway environment and make real-time decisions. However, the images used as inputs to the AI system may be maliciously perturbed, or manipulated, causing the AI system to make an incorrect classification. In this study, we examined humans’ perception of the AI’s computer vision capability of classifying various road sign images, including the original images, images with two different types of malicious attacks, and images that are scrambled randomly at the pixel level. Our results showed that participants rated the AI agent to be less capable than themselves of classifying the road signs. However, they overestimated the AI’s computer vision capability for correctly classifying images with malicious attacks that should cause the AI system to misclassify the image. These findings suggest that people lack an accurate understanding of the vulnerabilities of AI computer vision technologies and tend to overtrust AI in driving automation systems.},
  archive      = {J_THMS},
  author       = {Katherine R. Garcia and Jing Chen and Yanru Xiao and Scott Mishler and Cong Wang and Bin Hu},
  doi          = {10.1109/THMS.2025.3573173},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {499--507},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human perception of AI capabilities at classifying perturbed roadway signs},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time supervision and guidance for rehabilitation exercises via computer vision. <em>THMS</em>, <em>55</em>(4), 490--498. (<a href='https://doi.org/10.1109/THMS.2025.3566248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rehabilitation exercises are essential for individuals who have lost their ability to function normally. Typically, a functional rehabilitation program consists of two parts: supervised exercises in a rehabilitation center and additional exercises performed independently at home. Home exercises are as important as in-center exercises and can significantly improve recovery time. However, at home, the patient is unsupervised, with a significant risk of performing exercises incorrectly. This article presents a web application designed for hand rehabilitation exercises that uses a pretrained hand landmark model and an artificial neural network (ANN) classifier to ensure correct exercise performance. The application employs transfer learning to extract features from the user's hand movements, with the ANN classifier, designed using the ML5 library, determining if the positions are performed accurately. The ML5 library, a high-level interface to TensorFlow.js, makes the application suitable for web deployment. Physicians can create new exercises and retrain the ANN classifier without programming skills. An evaluation with 12 participants demonstrated an overall classification accuracy of 90.25% and achieved a system usability scale score of 75.30, indicating high usability and potential for real-world rehabilitation scenarios. This lightweight web application is highly accessible and user friendly and runs seamlessly on any smartphone or tablet. This approach shows great promise in enhancing exercise accuracy and safety, thereby improving the rehabilitation process at home and in clinical settings. In addition, it is adaptable for other models for full-body and face tracking, making it versatile for various rehabilitation applications.},
  archive      = {J_THMS},
  author       = {Mohamed Z. Amrani and Christoph W. Borst and Nouara Achour},
  doi          = {10.1109/THMS.2025.3566248},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {490--498},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Real-time supervision and guidance for rehabilitation exercises via computer vision},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Infrastructure-free indoor localization for cultural heritage sites. <em>THMS</em>, <em>55</em>(4), 480--489. (<a href='https://doi.org/10.1109/THMS.2025.3562438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor positioning systems are crucial for preserving and promoting cultural heritage sites by providing visitors with accurate location information and real-time exhibit details. These systems also offer valuable insights into visitor behavior for museums and cultural institutions to optimize their offerings and enhance the overall visitor experience. Their importance lies in driving engagement, fostering deeper connections with visitors, and providing data for future decision-making. This study proposes a magnetic field-based pedestrian localization algorithm as a solution to the challenge of providing accurate location information for visitors in indoor environments during virtual tours. The proposed approach uses magnetic field sequences to identify unique landmarks and a deep neural network (VGG-16) to analyze magnetic signal features and determine the visitor's location based on the nearest landmark. The study's results indicate that this magnetic field-based approach can be reliable for providing location information in indoor environments.},
  archive      = {J_THMS},
  author       = {Mohamed Amin Benatia and Souleyman Sahnoun and Mourad Messaadia},
  doi          = {10.1109/THMS.2025.3562438},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {8},
  number       = {4},
  pages        = {480--489},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Infrastructure-free indoor localization for cultural heritage sites},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Erratum to “Effects of target trajectory bandwidth on manual control behavior in pursuit and preview tracking”. <em>THMS</em>, <em>55</em>(3), 474--475. (<a href='https://doi.org/10.1109/THMS.2025.3561858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This erratum applies to the following published paper [1].},
  archive      = {J_THMS},
  author       = {Kasper van der El and Daan M. Pool and Marinus M. van Paassen and Max Mulder},
  doi          = {10.1109/THMS.2025.3561858},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {474--475},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Erratum to “Effects of target trajectory bandwidth on manual control behavior in pursuit and preview tracking”},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can the human body be a source of energy Harvesting?—Present and future prospective. <em>THMS</em>, <em>55</em>(3), 460--473. (<a href='https://doi.org/10.1109/THMS.2025.3552662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this era, looking for alternative energy harvesting methods to reduce global warming and conserve natural resources is necessary. Every country has been looking for new renewable energy resources. Human beings can act as a good energy source, from a single heartbeat to every tiny movement or activity of the human body. With bioelectronics and wearable device advancements, energy production from the human body has become a promising research area. Recently, several researchers have been working toward harvesting/producing energy from human beings and trying to translate that into electrical form, which wearable and implantable medical devices could use in the long run. This review article discusses different energy sources and harvesting techniques from the human body. The promising sources for energy harvesting are human movements, such as walking, running, etc., upper-limb movement, heartbeat, heat transmission, and breathing, which helps explore diverse types of energies, such as chemical, thermal, kinetic, mechanical, and other energies that can be harvested from the human body. This review would be helpful for the development of self-sustained implants, wearables, and health-tracking medical devices.},
  archive      = {J_THMS},
  author       = {Richa Sharma},
  doi          = {10.1109/THMS.2025.3552662},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {460--473},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Can the human body be a source of energy Harvesting?—Present and future prospective},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Watch out for explanations: Information type and error type affect trust and situational awareness in automated vehicles. <em>THMS</em>, <em>55</em>(3), 450--459. (<a href='https://doi.org/10.1109/THMS.2025.3558437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust and situational awareness (SA) are critical for the acceptance and safety of automated vehicles (AVs). While AV explanations with different information types have been studied to enhance drivers' trust and SA, their effectiveness remains unclear when AVs make errors that do not trigger takeover requests. This study investigated the effects of information type, error type, and their interaction on drivers' trust in AVs, SA, and their relationships. We recruited 300 participants in an online video study with a 3 (information type: why, how, why + how) × 3 (error type: false alarm, miss, correct [no error]) mixed design. How information describes the vehicle's action, while why information refers to the reason for the vehicle's action. Linear mixed models showed that false alarms and misses were associated with lower SA compared with correct scenarios, but possibly due to different reasons. Compared with correct scenarios, both false alarms and misses were associated with lower trust, with misses even lower than false alarms, possibly due to the varying severity of potential consequences. Compared with why and why + how information, how information was generally associated with lower SA and a higher potential of overtrust in false alarms. Trust and SA had a negative linear relationship in misses and false alarms, while no correlations were found in correct scenarios. To mitigate potential overtrust and misinterpretation of situations when AVs make errors, it is crucial to maintain higher SA. We recommend including why information in AV explanations and deploying AV decision systems that are less miss-prone.},
  archive      = {J_THMS},
  author       = {Yaohan Ding and Lesong Jia and Na Du},
  doi          = {10.1109/THMS.2025.3558437},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {450--459},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Watch out for explanations: Information type and error type affect trust and situational awareness in automated vehicles},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of advanced driving assistance warnings under distracted and time constraint conditions on motorized two-wheeler rider performance. <em>THMS</em>, <em>55</em>(3), 440--449. (<a href='https://doi.org/10.1109/THMS.2025.3550253'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study considered the speed hump as a spring, inducing psychological pressure on the rider to regulate their speed. The experiments were conducted on a motorized two-wheeler simulator in a randomized order in four situations: 1) normal, 2) distraction, 3) time pressure, and 4) distraction under time pressure. Naturalistic riding data was also collected to validate the results in normal riding conditions. First, seven latent classes were developed to understand the variation in speed of the rider along the road stretch as the rider approached or departed away from the speed hump. Further, using the spring-mass block theory, the riders' speed was considered to be the combination of constant base speed and varying simple harmonic motion speed. The spring-mass block theory was validated by developing univariate Bayesian regression models. Finally, the impact of inadequate marking and driving assistance systems such as advanced warning, individual vehicle attributes, lifestyle, and road crash history on the speed profile has been quantified using linear mixed effect models. Furthermore, the speed profile equations were validated using field data. The results revealed that unmarked speed humps were less detectable as the riders responded seven meters late to inadequately visible speed humps compared to marked speed humps. The advanced warning helped in smoothening the speed profile of the rider, which is crucial in avoiding crashes due to the speed hump. Overall, this study can further help in improving the adaptability of the advanced driver assistance systems and evidence-based policy making of speed hump placement to minimize road crashes.},
  archive      = {J_THMS},
  author       = {Monik Gupta and Nagendra Rao Velaga},
  doi          = {10.1109/THMS.2025.3550253},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {440--449},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Impact of advanced driving assistance warnings under distracted and time constraint conditions on motorized two-wheeler rider performance},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cybersecurity challenge analysis of work-from-anywhere (WFA) and recommendations guided by a user study. <em>THMS</em>, <em>55</em>(3), 428--439. (<a href='https://doi.org/10.1109/THMS.2025.3552231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many organizations were forced to quickly transition to the work-from-anywhere (WFA) model as a necessity to continue with their operations and remain in business despite the restrictions imposed during the COVID-19 pandemic. Many decisions were made in a rush, and cybersecurity decency tools were not in place to support this transition. In this article, we first attempt to uncover some challenges and implications related to the cybersecurity of the WFA model. Second, we conducted an online user study to investigate the readiness and cybersecurity awareness of employers and their employees who shifted to work remotely from anywhere. The user study questionnaire addressed different resilience perspectives of individuals and organizations. The collected data includes 45 responses from remotely working employees of different organizational types: Universities, government, private, and nonprofit organizations. Despite the importance of security training and guidelines, it was surprising that many participants had not received them. A robust communication strategy is necessary to ensure that employees are informed and updated on security incidents that the organization encounters. In addition, there is an increased need to pay attention to the security-related attributes of employees, such as their behavior, awareness, and compliance. Finally, we outlined best practice recommendations and mitigation tips guided by the study results to help individuals and organizations resist cybercrime and fraud and mitigate WFA-related cybersecurity risks.},
  archive      = {J_THMS},
  author       = {Mohammed Mahyoub and Ashraf Matrawy and Kamal Isleem and Olakunle Ibitoye},
  doi          = {10.1109/THMS.2025.3552231},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {428--439},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Cybersecurity challenge analysis of work-from-anywhere (WFA) and recommendations guided by a user study},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SignEvaluator: A gesture and sentence characteristic-based sign language quality assessment system. <em>THMS</em>, <em>55</em>(3), 418--427. (<a href='https://doi.org/10.1109/THMS.2025.3552476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language is a basic form of communication for hearing-impaired individuals. An evaluation of the quality of sign language gestures helps improve the efficiency of sign language learning. This article proposes SignEvaluator, a sign language quality assessment system with a movement quality feature extractor and assessment generator. In the former, three quality measures are proposed for gestures and sentences. The trajectory of the palm is mapped onto position space with kernel density estimation. For finger movements, the instantaneous energy and curvature of the gesture signals are extracted with Bézier curves. Meanwhile, the performer's familiarity with gestures is indicated by the movement fluency metric of sentences. In the assessment generator, the final assessment results are calculated by combining the weights of different quality metrics and the confidence of different gesture levels. The results indicate that SignEvaluator obtained an F1-score of 0.89 for 702 sentences collected from 20 performers.},
  archive      = {J_THMS},
  author       = {Zhiwen Zheng and Qingshan Wang and Qi Wang and Dazhu Deng},
  doi          = {10.1109/THMS.2025.3552476},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {418--427},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {SignEvaluator: A gesture and sentence characteristic-based sign language quality assessment system},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aided decision processes in dynamic events: Measuring decision support systems' influence and human responsibility. <em>THMS</em>, <em>55</em>(3), 408--417. (<a href='https://doi.org/10.1109/THMS.2025.3553015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision support systems (DSSs) increasingly assist human users who should consider their output and combine it with independently available information when making a decision. Some decision-making events are dynamic, requiring a series of decisions in interdependent stages. In them, users may not use DSS advice optimally, possibly compromising task performance and preventing the DSS from fulfilling its purpose. We report an experiment in which participants made decisions in dynamic events, observing the output from a DSS. They largely followed normative decision-making rules, but decisions were based on the DSS's perceived, and not necessarily the actual, accuracy. Participants mostly made up their minds in the first stage, and using the DSS typically lengthened the time to reach a decision. These findings should be considered when designing DSS and related regulations and operational processes so people can utilize the DSS's full potential.},
  archive      = {J_THMS},
  author       = {Yossef Saad and Joachim Meyer},
  doi          = {10.1109/THMS.2025.3553015},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {408--417},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Aided decision processes in dynamic events: Measuring decision support systems' influence and human responsibility},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on hybrid HumanArtificial intelligence in the metaverse. <em>THMS</em>, <em>55</em>(3), 394--407. (<a href='https://doi.org/10.1109/THMS.2025.3545246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid human–artificial intelligence (H-AI) in the metaverse is facing a growing trend in theoretical research and practical applications. Based on 101 academic papers published from 1996–2024, this article identifies research themes using thematic analysis, technological analysis, comparative analysis, and knowledge integration. The results show that academic interest and attention to H-AI have gradually increased since 2020. Through the results of thematic analysis, this article synthesizes five applications of H-AI domains: industry field, medical field, entertainment field, transportation field, and other fields. By analyzing the impact of H-AI on the integration of the virtual and real world, the role of H-AI in both the virtual and real worlds and the improvements over artificial intelligence will be outlined. This article also identifies challenges and responses that need further attention: disputes over responsibility ownership, bubble issues, lack of trust, and high-cost issues. This article looks ahead to add new skills to the metaverse platform, drive the metaverse as the next wave of the digital economy, promote the new field of metaverse combined with traditional, increase metaverse assistance to people with disabilities, and promote the systematization of the metaverse. This article helps enhance researchers' and practitioners' understanding of H-AI in the metaverse while raising awareness about the current research frontiers and potential future directions.},
  archive      = {J_THMS},
  author       = {Jing Zhang and Sha Li and Hongmei Wang and Wenxi Wang and Liming Chen and Yueliang Wan and Huansheng Ning},
  doi          = {10.1109/THMS.2025.3545246},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {394--407},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A survey on hybrid HumanArtificial intelligence in the metaverse},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain-supervised conditional generative modeling. <em>THMS</em>, <em>55</em>(3), 383--393. (<a href='https://doi.org/10.1109/THMS.2025.3537339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Present machine learning approaches to steer generative models rely on the availability of manual human input. We propose an alternative approach to supervising generative machine learning models by directly detecting task-relevant information from brain responses. That is, requiring humans only to perceive stimulus and react to it naturally. Brain responses of participants (N=30) were recorded via electroencephalography (EEG) while they perceived artificially generated images of faces and were instructed to look for a particular semantic feature, such as “smile” or “young”. A supervised adversarial autoencoder was trained to disentangle semantic image features by using EEG data as a supervision signal. The model was subsequently conditioned to generate images matching users' intentions without additional human input. The approach was evaluated in a validation study comparing brain-conditioned models to manually conditioned and randomly conditioned alternatives. Human assessors scored the saliency of images generated from different models according to the target visual features (e.g., which face image is more “smiling” or more “young”). The results show that brain-supervised models perform comparably to models trained with manually curated labels, without requiring any manual input from humans.},
  archive      = {J_THMS},
  author       = {Jun Ma and Tuukka Ruotsalo},
  doi          = {10.1109/THMS.2025.3537339},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {383--393},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Brain-supervised conditional generative modeling},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG features to quantify the NASA-TLX factors of cognitive workload. <em>THMS</em>, <em>55</em>(3), 372--382. (<a href='https://doi.org/10.1109/THMS.2025.3546515'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring cognitive workload (CWL) is crucial for dynamic task reallocation (i.e., adaptation) between a human and a machine in a human-machine system (HMS). A conventional measurement of the CWL is based on subjectively reported scores about the six factors of the NASA Task Load Index (NASA-TLX) questionnaire. The questionnaire cannot however capture real-time fluctuations of the factors for an objective quantification. Additionally, each of the factors is associated with distinct activities and can be influenced by individual characteristics and/or task contexts. Such HMS adaptation should thus consider the objective quantification of each factor. So far, the quantification remains largely unexplored, while existing studies reveal a potential use of an electroencephalography (EEG) in measuring the CWL levels (e.g., high, medium, and low). Herein, we presented a pioneering study to propose EEG features for quantifying the factors. The pertinence of the features was demonstrated by their strong correlations with the scores of the factors across three distinct cases of visuomotor tasks. The pertinence is the stepping stone toward factor-based interventions in enabling HMS adaptation.},
  archive      = {J_THMS},
  author       = {Nusrat Z. Zenia and Stanley Tarng and Lida Ghaemi Dizaji and Yaoping Hu},
  doi          = {10.1109/THMS.2025.3546515},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {372--382},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {EEG features to quantify the NASA-TLX factors of cognitive workload},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual imagery-based brain-computer interaction paradigms and neural encoding and decoding. <em>THMS</em>, <em>55</em>(3), 358--371. (<a href='https://doi.org/10.1109/THMS.2025.3548943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imagination is a significant mental activity of human beings that can serve as a brain-computer interaction (BCI) paradigm, and visual imagery (VI) has become a relatively new BCI paradigm. VI is the ability to form visual representations in short-term memory without retinal input, offering users advantages like safety and rich content. It is anticipated to address the limitations of traditional BCI paradigms. However, further research is needed in the neural mechanisms, paradigm design, neural encoding, and decoding of VI-based BCI. Despite the existence of experimental studies on VI-BCI, there is a shortage of comprehensive reviews. This article provides a review of the neural mechanisms of VI, and explores paradigm designs for VI-BCI. Additionally, it covers neural encoding and decoding methods. Finally, the existing issues and suggested future research directions for VI-BCI are identified. The aim is to inspire further research in the field of VI-BCI.},
  archive      = {J_THMS},
  author       = {Lei Zhao and Yao Liu and Jing'ao Gao and Peng Ding and Fan Wang and Anmin Gong and Wenya Nan and Yunfa Fu and Tianwen Li},
  doi          = {10.1109/THMS.2025.3548943},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {358--371},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Visual imagery-based brain-computer interaction paradigms and neural encoding and decoding},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imagined Speech–EEG detection using multivariate swarm sparse decomposition-based joint Time–Frequency analysis for intuitive BCI. <em>THMS</em>, <em>55</em>(3), 347--357. (<a href='https://doi.org/10.1109/THMS.2025.3554449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In brain–computer interface (BCI) applications, imagined speech (IMS) decoding based on electroencephalogram (EEG) has established a new neuro-paradigm that offers an intuitive communication tool for physically impaired patients. However, existing IMS–EEG-based BCI systems have introduced difficulties in feasible deployment due to nonstationary EEG signals, suboptimal feature extraction, and limited multiclass scalability. To address these challenges, we have presented a novel approach using the multivariate swarm-sparse decomposition method (MSSDM) for joint time–frequency (JTF) analysis and further developed a feasible end-to-end framework from multichannel IMS–EEG signals for IMS detection. MSSDM employs improved multivariate swarm filtering and sparse spectrum techniques to design optimal filter banks for extracting an ensemble of channel-aligned oscillatory components (CAOCs), significantly enhancing IMS activation-related sub-bands. To enhance channel-aligned information, multivariate JTF images have been constructed using JIF and instantaneous amplitude across channels from the obtained CAOCs. Further, JTF-based deep features (JTFDFs) were computed using different pretrained neural networks and mapped most discriminant features using two well-known feature correlation techniques: Canonical correlation analysis and Hellinger distance-based correlation. The proposed method has been tested on the 5-class BCI competition and 6-class Coretto IMS datasets. The experimental findings on cross-subject and cross-dataset reveal that the novel JTFDF feature-based classification model, MSSDM-SqueezeNet-JTFDF, achieved the highest classification performance against all other existing state-of-the-art methods in IMS recognition. Our introduced EEG–BCI models effectively enhance IMS–EEG patterns across multichannel data and offer great potential for the practical deployment of BCI technologies.},
  archive      = {J_THMS},
  author       = {Shailesh Vitthalrao Bhalerao and Ram Bilas Pachori},
  doi          = {10.1109/THMS.2025.3554449},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {347--357},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Imagined Speech–EEG detection using multivariate swarm sparse decomposition-based joint Time–Frequency analysis for intuitive BCI},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLYKLatent: A learning framework for gaze estimation using deep facial feature learning. <em>THMS</em>, <em>55</em>(3), 333--346. (<a href='https://doi.org/10.1109/THMS.2025.3553404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we present self-learn your key latent (SLYKLatent), a novel approach for enhancing gaze estimation by addressing appearance instability challenges in datasets due to aleatoric uncertainties, covariant shifts, and test domain generalization. SLYKLatent utilizes self-supervised learning for initial training with facial expression datasets, followed by refinement with a patch-based tribranch network and an inverse explained variance weighted training loss function. Our evaluation on benchmark datasets achieves a 10.98% improvement on Gaze360, supersedes the top result with 3.83% improvement on MPIIFaceGaze, and leads on a subset of ETH-XGaze by 11.59%, surpassing existing methods by significant margins. In addition, adaptability tests on RAF-DB and Affectnet show 86.4% and 60.9% accuracies, respectively. Ablation studies confirm the effectiveness of SLYKLatent's novel components. This approach has strong potential in human–robot interaction.},
  archive      = {J_THMS},
  author       = {Samuel Adebayo and Joost C. Dessing and Seán McLoone},
  doi          = {10.1109/THMS.2025.3553404},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {333--346},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {SLYKLatent: A learning framework for gaze estimation using deep facial feature learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinated adaptive impedance control of redundantly actuated parallel manipulators. <em>THMS</em>, <em>55</em>(3), 323--332. (<a href='https://doi.org/10.1109/THMS.2025.3550919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redundantly actuated parallel mechanisms (RAPMs) have been widely adopted in advanced robotic systems and precision machine tools. It has been demonstrated that redundant actuation can improve the performance of mechatronic systems but introduce challenges with respect to control. One main difficulty is in establishing an accurate dynamic model of the RAPM system. With an inaccurate dynamic model, the torque applied by the actuators will be incorrect, leading to increased antagonistic forces in the system. To solve this problem, a novel coordinated adaptive impedance control approach based on a new adaptive impedance control law is presented here, along with proof of the stability of the closed-loop system. The control algorithm has been validated experimentally by a prototype cable-driven parallel manipulator. It can be seen from the experimental results that the proposed control method is an effective way to correct the antagonistic forces of the system, thus facilitating the improvement of its dynamic performance and its efficacy in different applications.},
  archive      = {J_THMS},
  author       = {Nan Ma and David Cheneler and Guangping He and Junjie Yuan and Gui-Bin Bian},
  doi          = {10.1109/THMS.2025.3550919},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {6},
  number       = {3},
  pages        = {323--332},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Coordinated adaptive impedance control of redundantly actuated parallel manipulators},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-based protocol for continuous action iterated dilemma in information lossy networks. <em>THMS</em>, <em>55</em>(2), 315--321. (<a href='https://doi.org/10.1109/THMS.2025.3532598'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel prescribed time-based method for analyzing the convergence of evolutionary game dynamics in an information lossy network. Traditional game theory limits players to two choices, i.e., either cooperation or defection. However, player behavior in real-world scenarios is often multidimensional and complex; therefore, this work employs a continuous action iterated dilemma that allows players to choose a wider range of strategies. Moreover, traditional convergence analysis often relies on Jacobian matrices, which entail complex derivations. In contrast, the proposed strategy employs a time generator-based protocol that achieves agreement between all the players at a prescribed time, explicitly set by the user through a time parameter within the protocol. A comprehensive Lyapunov analysis affirms the prescribed time convergence even when the network is exposed to information loss during data transfer. Numerical simulations illustrate that the proposed scheme leads to a faster agreement at the preassigned time and with a better resilience performance compared to existing methods.},
  archive      = {J_THMS},
  author       = {Syed Muhammad Amrr and Mohamed Zaery and S. M. Suhail Hussain and Mohammad A. Abido},
  doi          = {10.1109/THMS.2025.3532598},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {315--321},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Time-based protocol for continuous action iterated dilemma in information lossy networks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A single-camera method for estimating lift asymmetry angles using deep learning computer vision algorithms. <em>THMS</em>, <em>55</em>(2), 309--314. (<a href='https://doi.org/10.1109/THMS.2025.3539187'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A computer vision (CV) method to automatically measure the revised NIOSH lifting equation asymmetry angle (A) from a single camera is described and tested. A laboratory study involving ten participants performing various lifts was used to estimate A in comparison to ground truth joint coordinates obtained using 3-D motion capture (MoCap). To address challenges, such as obstructed views and limitations in camera placement in real-world scenarios, the CV method utilized video-derived coordinates from a selected set of landmarks. A 2-D pose estimator (HR-Net) detected landmark coordinates in each video frame, and a 3-D algorithm (VideoPose3D) estimated the depth of each 2-D landmark by analyzing its trajectories. The mean absolute precision error for the CV method, compared to MoCap measurements using the same subset of landmarks for estimating A, was 6.25° (SD = 10.19°, N = 360). The mean absolute accuracy error of the CV method, compared against conventional MoCap landmark markers was 9.45° (SD = 14.01°, N = 360).},
  archive      = {J_THMS},
  author       = {Zhengyang Lou and Zitong Zhan and Huan Xu and Yin Li and Yu Hen Hu and Ming-Lun Lu and Dwight M. Werren and Robert G. Radwin},
  doi          = {10.1109/THMS.2025.3539187},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {309--314},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A single-camera method for estimating lift asymmetry angles using deep learning computer vision algorithms},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time series signal analysis with information granulation based on permutation entropy: An application to electroencephalography signals. <em>THMS</em>, <em>55</em>(2), 300--308. (<a href='https://doi.org/10.1109/THMS.2025.3538098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we reported a novel granulation method composed of complexity information based on permutation entropy (PeEn). This method aims to recognize the electroencephalography (EEG) patterns using this proposed granulation method. First, we define the complexity information for granular computing by a technique with fast calculation, i.e., PeEn. Then, the information granule can be constructed based on the time domain information, which completes complexity information. Together with the support vector machine algorithm, the proposed granulation method outperformed the existing classification methods in accuracy. It is utilized by classifying three motor imaginary EEG signals. Two of them are binary-class datasets, i.e., one dataset includes two-hand actions, and another includes hand and foot actions. The third dataset is multiclass, including two hands and two feet actions. In addition, the proposed granulation method overcomes the difficulties in cross-individual cases when classifying the EEG signals with a higher accuracy than the existing methods. Meanwhile, this classification procedure makes it interpretable and has a high performance.},
  archive      = {J_THMS},
  author       = {Youpeng Yang and Sanghyuk Lee and Haolan Zhang and Witold Pedrycz},
  doi          = {10.1109/THMS.2025.3538098},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {300--308},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Time series signal analysis with information granulation based on permutation entropy: An application to electroencephalography signals},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A miner mental state evaluation scheme with decision level fusion based on multidomain EEG information. <em>THMS</em>, <em>55</em>(2), 289--299. (<a href='https://doi.org/10.1109/THMS.2025.3538162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been proven that electroencephalography (EEG) is an effective method for evaluating an individual's mental state. However, when it comes to the evaluation of miners' mental state, there are still some issues with missing EEG dataset and unsatisfactory evaluation accuracy. Therefore, this article proposes a miner mental state evaluation scheme with decision-level fusion based on multidomain EEG information. First, in the comprehensive lab for coal-related programs of Xi'an University of Science and Technology, the coal mine environment is simulated, and a realistic EEG dataset is constructed. Second, the multidomain features are extracted to represent abundant information in time, frequency, time-frequency, and space domain. These features with low dimension are classified adopting support vector machine (SVM), k-nearest neighbor (kNN), and back propagation (BP) network to obtain the optimal evaluation submodel (four domains corresponding to four submodels). Finally, based on the state probabilities provided by the optimal evaluation submodel, we adopt stack fusion and an improved Yager rule to fuse four submodels in order to find the most suitable fusion algorithm. The experimental results demonstrate that the average accuracy can reach 93.19% on the self-built dataset when utilizing the improved Yager rule with weight, and it realizes a better evaluation accuracy.},
  archive      = {J_THMS},
  author       = {Hongguang Pan and Shiyu Tong and Haoqian Song and Xin Chu},
  doi          = {10.1109/THMS.2025.3538162},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {289--299},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A miner mental state evaluation scheme with decision level fusion based on multidomain EEG information},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual interfaces to mitigate eye problems in a virtual environment via triggering eye blinking and movement. <em>THMS</em>, <em>55</em>(2), 278--288. (<a href='https://doi.org/10.1109/THMS.2025.3542452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increase of virtual reality (VR) applications in daily life, protecting the comfort and health of VR users has become increasingly important. The immersive nature of VR often results in decreased eye blinking and movement, putting users at risk of developing conditions such as dry eye syndrome and eye strain. In this article, we propose visual interfaces to induce temporary eye blinks or movements by drawing users' attention temporarily in order to mitigate the negative effects of VR on eye health. Our proposed interfaces can induce eye blinking and movement, which are known to mitigate eye problems in VR. The experimental results confirmed that our interfaces increase the frequency of eye blinking and movement in VR users.},
  archive      = {J_THMS},
  author       = {Jongwook Jeong and Myeongseok Kwak and HyeongYeop Kang},
  doi          = {10.1109/THMS.2025.3542452},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {278--288},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Visual interfaces to mitigate eye problems in a virtual environment via triggering eye blinking and movement},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficacy assessments of virtual reality systems for immersive consumer Testing—Two case studies with tortilla chip evaluations. <em>THMS</em>, <em>55</em>(2), 266--277. (<a href='https://doi.org/10.1109/THMS.2024.3524916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sensory science, the use of immersive technologies has gained popularity for their ability to restore relevant contextual factors during consumer testing and overcome the low ecological validity of controlled laboratory environments. Despite this, there is scant literature evaluating the effectiveness of immersive technologies in facilitating virtual product evaluation experiences; this is especially true with virtual reality (VR) headsets and the unique technical challenges associated with this technology. To fill this gap, we assessed virtual presence, system usability, engagement, and ease of task completion, in subjects using two iterations of a VR application (controllers or hand tracking) designed to address the major limitations of current systems. Results revealed that both systems exceeded the benchmark usability score of 68. System 1 (controllers) performed better for interactions with the virtual tablet interface to answer questions, whereas interactions with the food objects were easier using System 2 (hand tracking). Participants also experienced a high sense of virtual presence using both systems. When measured in System 2, a high level of subject engagement during the immersive product evaluations was observed. These studies indicate that collecting both quantitative and qualitative feedback on VR systems can provide useful insights and directions for application optimization to ensure valid investigation of context effects in future research.},
  archive      = {J_THMS},
  author       = {Kym K. W. Man and Jeremy A. Patterson and Christopher T. Simons},
  doi          = {10.1109/THMS.2024.3524916},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {266--277},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Efficacy assessments of virtual reality systems for immersive consumer Testing—Two case studies with tortilla chip evaluations},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time myoelectric-based neural-drive decoding for concurrent and continuous control of robotic finger forces. <em>THMS</em>, <em>55</em>(2), 256--265. (<a href='https://doi.org/10.1109/THMS.2025.3532209'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural or muscular injuries, such as due to amputation, spinal cord injury, and stroke, can affect hand functions, profoundly impacting independent living. This has motivated the advancement of cutting-edge assistive robotic hands. However, unintuitive myoelectric control of these devices remains challenging, which limits the clinical translation of these devices. Accordingly, we developed a robust motor-intent decoding approach to continuously predict the intended fingertip forces of single and multiple fingers in real time. We used population motor neuron discharge activities (i.e., neural drive from brain to spinal cord) decoded from a high-density surface electromyogram (HD-sEMG) signals as the control signals instead of the conventional global sEMG features. To enable real-time neural-drive prediction, we employed a convolutional neural network model to establish the mapping from global HD-sEMG features to finger-specific neural-drive signals, which were then employed for continuous and real-time control of three prosthetic fingers (index, middle, and ring). As a result, the neural-drive-based approach can decode the motor intent of single-finger and multifinger forces with significantly lower force estimation errors than that obtained using the global HD-sEMG-amplitude approach. Besides, the force prediction accuracy was consistent over time and demonstrated strong robustness to signal interference. Our network-based decoder can also achieve better finger isolation with minimal forces predicted in unintended fingers. Our work demonstrates that the accurate and robust finger force control could be achieved through this new decoding approach. The outcomes offer an efficient intent prediction approach that allows users to have intuitive control of prosthetic fingertip forces in a dexterous way.},
  archive      = {J_THMS},
  author       = {Long Meng and Luis Vargas and Derek G. Kamper and Xiaogang Hu},
  doi          = {10.1109/THMS.2025.3532209},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {256--265},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Real-time myoelectric-based neural-drive decoding for concurrent and continuous control of robotic finger forces},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human comfort index estimation in industrial Human–Robot collaboration task. <em>THMS</em>, <em>55</em>(2), 246--255. (<a href='https://doi.org/10.1109/THMS.2025.3530530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective human–robot collaboration (HRC) requires robots to understand and adapt to humans' psychological states. This research presents a novel approach to quantitatively measure human comfort levels during HRC through the development of two metrics: a comfortability index (CI) and an uncomfortability index (UnCI). We conducted HRC experiments where participants performed assembly tasks while the robot's behavior was systematically varied. Participants' subjective responses (including surprise, anxiety, boredom, calmness, and comfortability ratings) were collected alongside physiological signals, including electrocardiogram, galvanic skin response, and pupillometry data. We propose two novel approaches for estimating CI/UnCI: an adaptation of the emotion circumplex model that maps comfort levels to the arousal–valence space, and a kernel density estimation model trained on physiological data. Time-domain features were extracted from the physiological signals and used to train machine learning models for real-time comfort levels estimation. Our results demonstrate that the proposed approaches can effectively estimate human comfort levels from physiological signals alone, with the circumplex model showing particular promise in detecting high discomfort states. This work enables real-time measurement of human comfort during HRC, providing a foundation for developing more adaptive and human-aware collaborative robots.},
  archive      = {J_THMS},
  author       = {Celal Savur and Jamison Heard and Ferat Sahin},
  doi          = {10.1109/THMS.2025.3530530},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {246--255},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human comfort index estimation in industrial Human–Robot collaboration task},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiOpen: A robust wi-fi-based open-set gesture recognition framework. <em>THMS</em>, <em>55</em>(2), 234--245. (<a href='https://doi.org/10.1109/THMS.2025.3532910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition. However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training. This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known class during testing. To address this issue, we propose WiOpen, a robust Wi-Fi-based open-set gesture recognition (OSGR) framework. Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data. Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise. To tackle these challenges, WiOpen adopts a twofold approach to eliminate uncertainty and define precise decision boundaries. Initially, it addresses uncertainty induced by noise during data preprocessing by utilizing the channel state information (CSI) ratio. Next, it designs the OSGR network based on an uncertainty quantification method. Throughout the learning process, this network effectively mitigates uncertainty stemming from domains. Ultimately, the network leverages relationships among samples' neighbors to dynamically define open-set decision boundaries, successfully realizing OSGR. Comprehensive experiments on publicly accessible datasets confirm WiOpen's effectiveness.},
  archive      = {J_THMS},
  author       = {Xiang Zhang and Jinyang Huang and Huan Yan and Yuanhao Feng and Peng Zhao and Guohang Zhuang and Zhi Liu and Bin Liu},
  doi          = {10.1109/THMS.2025.3532910},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {234--245},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {WiOpen: A robust wi-fi-based open-set gesture recognition framework},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global-local image perceptual score (GLIPS): Evaluating photorealistic quality of AI-generated images. <em>THMS</em>, <em>55</em>(2), 223--233. (<a href='https://doi.org/10.1109/THMS.2025.3527397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the global-local image perceptual score (GLIPS), an image metric designed to assess the photorealistic image quality of AI-generated images with a high degree of alignment to human visual perception. Traditional metrics such as Fréchet inception distance (FID) and kernel inception distance scores do not align closely with human evaluations. The proposed metric incorporates advanced transformer-based attention mechanisms to assess local similarity and maximum mean discrepancy to evaluate global distributional similarity. To evaluate the performance of GLIPS, we conducted a human study on photorealistic image quality. Comprehensive tests across various generative models demonstrate that GLIPS consistently outperforms existing metrics like FID, structural similarity index measure, and multiscale structural similarity index measure in terms of correlation with human scores. In addition, we introduce the interpolative binning scale, a refined scaling method that enhances the interpretability of metric scores by aligning them more closely with human evaluative standards. The proposed metric and scaling approach not only provide more reliable assessments of AI-generated images but also suggest pathways for future enhancements in image generation technologies.},
  archive      = {J_THMS},
  author       = {Memoona Aziz and Umair Rehman and Muhammad Umair Danish and Katarina Grolinger},
  doi          = {10.1109/THMS.2025.3527397},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {223--233},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Global-local image perceptual score (GLIPS): Evaluating photorealistic quality of AI-generated images},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chatbot dialog design for improved human performance in domain knowledge discovery. <em>THMS</em>, <em>55</em>(2), 207--222. (<a href='https://doi.org/10.1109/THMS.2024.3514742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of machine learning (ML) has led to the widespread adoption of developing task-oriented dialog systems for scientific applications (e.g., science gateways) where voluminous information sources are retrieved and curated for domain users. Yet, there still exists a challenge in designing chatbot dialog systems that achieve widespread diffusion among scientific communities. In this article, we propose a novel Vidura advisor design framework (VADF) to develop dialog system designs for information retrieval (IR) and question-answering (QA) tasks, while enabling the quantification of system utility based on human performance in diverse application environments. We adopt a socio-technical approach in our framework for designing dialog systems by utilizing domain expert feedback, which features a sparse retriever for enabling accurate responses in QA settings using linear interpolation smoothing. We apply our VADF for an exemplar science gateway, viz. KnowCOVID-19, to conduct experiments that demonstrate the utility of dialog systems based on IR and QA performance, application utility, and perceived adoption. Experimental results show our VADF approach significantly improves IR performance against retriever baselines (up to 5% increase) and QA performance against large language models (LLMs) such as ChatGPT (up to 43% increase) on scientific literature datasets. In addition, through a usability survey, we observe that measuring application utility and human performance when applying VADF to KnowCOVID-19 translates to an increase in perceived community adoption.},
  archive      = {J_THMS},
  author       = {Roland Oruche and Xiyao Cheng and Zian Zeng and Audrey Vazzana and MD Ashraful Goni and Bruce Wang Shibo and Sai Keerthana Goruganthu and Kerk Kee and Prasad Calyam},
  doi          = {10.1109/THMS.2024.3514742},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {207--222},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Chatbot dialog design for improved human performance in domain knowledge discovery},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy adaptive controller of a wearable assistive upper limb exoskeleton using a disturbance observer. <em>THMS</em>, <em>55</em>(2), 197--206. (<a href='https://doi.org/10.1109/THMS.2025.3529759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The motivation behind the development of a wearable assistive upper limb exoskeleton robot was to provide comprehensive multijoint therapy by assisting physiotherapists in enhancing the recovery of hemiplegic patients. However, the controlling of an upper limb exoskeleton for rehabilitation is a challenging task because of its nonlinear characteristics. This article presents a novel fuzzy adaptive controller that utilizes a high-dimensional integral-type Lyapunov function for a wearable assistive upper limb exoskeleton. A disturbance observer had been used to tackle uncertainties in the exoskeleton's dynamic model, thereby enhancing the tracking performance of the joints. The aim of this control scheme was to overcome unknown parameters in the dynamic model. The performance of the adaptive controller was validated through human interactive experiments and periodically repeated reference trajectory tests. The results demonstrated that the proposed fuzzy adaptive controller, with the inclusion of a disturbance observer, could effectively compensate for uncertain disturbances and could achieve efficient tracking of the reference trajectory. The statistical analysis revealed that the fuzzy adaptive controller performed 45%, 44%, and 31% less in average error compared to adaptive conventional controllers. The findings ascertained the potential of the proposed controller in improving the recovery of motor functions of hemiplegic patients.},
  archive      = {J_THMS},
  author       = {Mohammad Soleimani Amiri and Rizauddin Ramli},
  doi          = {10.1109/THMS.2025.3529759},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {197--206},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Fuzzy adaptive controller of a wearable assistive upper limb exoskeleton using a disturbance observer},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introducing a passive shoulder exoskeleton in a production plant: A longitudinal observation of its effects on workers. <em>THMS</em>, <em>55</em>(2), 185--196. (<a href='https://doi.org/10.1109/THMS.2025.3536199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Occupational exoskeletons have the potential to prevent work-related musculoskeletal disorders. Their widespread adoption should be promoted by investigating their long-term innocuity, sustained effectiveness, and practicability. This article presents a six-months longitudinal study exploring effects of an arm support exoskeleton (ASE) on six male workers, examining potential side effects, ASE's effectiveness, and its integration into daily work practices. Monthly clinical visits were scheduled to monitor workers’ health. Effectiveness, usability and acceptance metrics were collected at the beginning of the study and after six months. No side effects were found in clinical metrics during the study. Significant reductions, consistent overtime, were observed in shoulder muscle activity (up to 30%) and in effort perception-related metrics (up to 2.4 out of 10 points). Usage time settled around 10% of the monthly work-shift and gradually decreased possibly due to external factors (e.g., social, motivational, and seasonal factors) beyond researchers' control. Results encourage the continuation of similar investigations to strengthen these findings and promote the use of occupational exoskeletons.},
  archive      = {J_THMS},
  author       = {Andrea Parri and Ilaria Pacifico and Eleonora Guanziroli and Federica Aprigliano and Silverio Taglione and Francesco Giovacchini and Francesco Saverio Violante and Franco Molteni and Nicola Vitiello and Simona Crea},
  doi          = {10.1109/THMS.2025.3536199},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {185--196},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Introducing a passive shoulder exoskeleton in a production plant: A longitudinal observation of its effects on workers},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Correlations between biomechanical variables and subjective measures of satisfaction while using a passive upper-limb exoskeleton for overhead tasks in the field. <em>THMS</em>, <em>55</em>(2), 176--184. (<a href='https://doi.org/10.1109/THMS.2025.3532358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel evaluation approach on wearing passive upper-limb exoskeletons for overhead tasks in real-world automotive manufacturing lines. We determined that wearing exoskeletons reduced the biomechanical efforts of workers measured by joint kinematics and electromyography as well as the estimated shoulder joint reaction forces and torques derived from simulation. These quantitatively measured variables were statistically associated with subjective measures collected through satisfaction questionnaires. We specifically found that participants increased the shoulder flexion and abduction angles as well as the shoulder range of motion while wearing exoskeletons. Participants also reduced muscle activities, joint torques for shoulder flexion, and reaction forces exerted on the shoulder joints while wearing exoskeletons. Interestingly, our analysis also found that the increased shoulder movement while wearing the device was negatively associated with the satisfaction level. This indicates that although the assistance provided by the device allows users to perform a wider range of arm lifting movements, the deviation from their original movement with the device may lead to decreases in satisfaction levels. This integrative approach using biomechanics and ergonomics suggests that we can potentially predict the subjective scale of satisfaction based on biomechanical variables and preliminarily evaluate the usability and comfort while wearing exoskeletons in real-world settings.},
  archive      = {J_THMS},
  author       = {Sungwoo Park and Moon Ki Jung and Kyujung Kim and HyunSeop Lim and JuYoung Yoon and Dong Jin Hyun},
  doi          = {10.1109/THMS.2025.3532358},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {176--184},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Correlations between biomechanical variables and subjective measures of satisfaction while using a passive upper-limb exoskeleton for overhead tasks in the field},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive virtual fixture based on learning trajectory distribution for comanipulation tasks. <em>THMS</em>, <em>55</em>(2), 165--175. (<a href='https://doi.org/10.1109/THMS.2025.3540123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual fixture is a powerful tool to improve safety and efficiency for co-manipulation tasks. However, traditional virtual fixtures with constant stiffness are inadequate for scenarios where robots need to leave the constraints to perform tasks. To address this, we propose an adaptive virtual fixture based on the motion refinement tube, which dynamically adjusts the guiding force according to the distribution of trajectories. To prevent tube deformation in the Cartesian space due to the neglect of off-diagonal elements of covariance matrices, the refinement tube radii and nonlinear stiffness terms are computed in local coordinate systems based on the decomposed covariance matrix. An energy-tank-based passivity controller is designed to ensure system stability when employing the virtual fixture with state-dependent stiffness terms. In the validation tests with 18 participants, the proposed method showed improvements in task efficiency (18.69% increase) and collision avoidance (97.87% reduction) for a typical pick-and-place task with scattered materials. It also provided better subjective experiences of the users than traditional virtual fixtures. Meanwhile, compared with the method that neglects off-diagonal elements of the covariance matrix, the proposed method exhibited a 4.28% efficiency improvement and a 40.42% decrease in collision occurrences.},
  archive      = {J_THMS},
  author       = {Shaqi Luo and Min Cheng and Ruqi Ding},
  doi          = {10.1109/THMS.2025.3540123},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {165--175},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Adaptive virtual fixture based on learning trajectory distribution for comanipulation tasks},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Receding-horizon reinforcement learning for time-delayed Human–Machine shared control of intelligent vehicles. <em>THMS</em>, <em>55</em>(2), 155--164. (<a href='https://doi.org/10.1109/THMS.2024.3496899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–machine shared control has recently been regarded as a promising paradigm to improve safety and performance in complex driving scenarios. One crucial task in shared control is dynamically optimizing the driving weights between the driver and the intelligent vehicle to adapt to dynamic driving scenarios. However, designing an optimal human–machine shared controller with guaranteed performance and stability is challenging due to nonnegligible time delays caused by communication protocols and uncertainties in driver behavior. This article proposes a novel receding-horizon reinforcement learning approach for time-delayed human–machine shared control of intelligent vehicles. First, we build a multikernel-based data-driven model of vehicle dynamics and driving behavior, considering time delays and uncertainties of drivers' actions. Second, a model-based receding horizon actor–critic learning algorithm is presented to learn an explicit policy for time-delayed human–machine shared control online. Unlike classic reinforcement learning, policy learning of the proposed approach is performed according to a receding-horizon strategy to enhance learning efficiency and adaptability. In theory, the closed-loop stability under time delays is analyzed. Hardware-in-the-loop experiments on the time-delayed human–machine shared control of intelligent vehicles have been conducted in variable curvature road scenarios. The results demonstrate that our approach has significant improvements in driving performance and driver workload compared with pure manual driving and previous shared control methods.},
  archive      = {J_THMS},
  author       = {Xinxin Yao and Jiahang Liu and Xinglong Zhang and Xin Xu},
  doi          = {10.1109/THMS.2024.3496899},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {155--164},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Receding-horizon reinforcement learning for time-delayed Human–Machine shared control of intelligent vehicles},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep radiomics for autism diagnosis and age prediction. <em>THMS</em>, <em>55</em>(2), 144--154. (<a href='https://doi.org/10.1109/THMS.2025.3526957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiomics combined with deep learning is an emerging field within biomedical engineering that aims to extract important characteristics from medical images to develop a predictive model that can support clinical decision-making. This method could be used in the realm of brain disorders, particularly autism spectrum disorder (ASD), to facilitate prompt identification. We propose a novel radiomic features [deep radiomic features (DTF)], involving the use of principal component analysis to encode convolutional neural network (CNN) features, thereby capturing distinctive features related to brain regions in subjects with ASD subjects and their age. Using these features in random forest (RF) models, we explore two scenarios, such as site-specific radiomic analysis and feature extraction from unaffected brain regions to alleviate site-related variations. Our experiments involved comparing the proposed method with standard radiomics (SR) and 2-D/3-D CNNs for the classification of ASD versus healthy control (HC) individuals and different age groups (below median and above median). When using the RF model with DTF, the analysis at individual sites revealed an area under the receiver operating characteristic (ROC) curve (AUC) range of 79%–85% for features, such as the left lateral-ventricle, cerebellum-white-matter, and pallidum, as well as the right choroid-plexus and vessel. In the context of fivefold cross validation with the RF model, the combined features (DTF from 3-D CNN, ResNet50, DarketNet53, and NasNet_large with SR) achieved the highest AUC value of 76.67%. Furthermore, our method also showed notable AUC values for predicting age in subjects with ASD (80.91%) and HC (75.64%). The results indicate that DTFs consistently exhibit predictive value in classifying ASD from HC subjects and in predicting age.},
  archive      = {J_THMS},
  author       = {Ahmad Chaddad},
  doi          = {10.1109/THMS.2025.3526957},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {144--154},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Deep radiomics for autism diagnosis and age prediction},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning mutual excitation for hand-to-hand and human-to-human interaction recognition. <em>THMS</em>, <em>55</em>(2), 134--143. (<a href='https://doi.org/10.1109/THMS.2024.3522974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human–robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairwise entities. Compared with graph convolution, our proposed me-GC gradually learns mutual information in each layer and each stage of graph convolution operations. Extensive experiments on a challenging hand-to-hand interaction dataset, i.e., the Assembely101 dataset, and two large-scale human-to-human interaction datasets, i.e., NTU60-Interaction and NTU120-Interaction consistently verify the superiority of our proposed method, which outperforms the state-of-the-art GCN-based and Transformer-based methods.},
  archive      = {J_THMS},
  author       = {Mengyuan Liu and Chen Chen and Songtao Wu and Fanyang Meng and Hong Liu},
  doi          = {10.1109/THMS.2024.3522974},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {134--143},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Learning mutual excitation for hand-to-hand and human-to-human interaction recognition},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiobjective discrete harmony search optimizer for disassembly line balancing problems considering human factors. <em>THMS</em>, <em>55</em>(2), 124--133. (<a href='https://doi.org/10.1109/THMS.2025.3528629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ecological environment and natural resource issues are becoming more and more prominent, which promotes the recycling of waste products for green economy. Disassembly plays a key role in the remanufacturing and reuse of waste products. However, with the rapid development of production automation, designers tend to ignore the fact that manual operation is more flexible. It is of great importance to consider human factors in a disassembly process. This work considers two human disassembly postures, namely standing and sitting. The multiobjective disassembly line balancing problem considering human posture changes is studied. A mathematical model with the objective functions of maximizing profit, minimizing the number of posture changes at a workstation, and minimizing the difference of maximum posture changes between any two workstations is established. The model is solved through a newly proposed Pareto-based discrete harmony search algorithm. Three neighborhood structures are designed to enlarge the search space for better solutions. Furthermore, an elite reserve strategy is used to improve the global optimization ability of the proposed algorithm. Finally, the proposed model and algorithm are applied to cases of different scales of complexities, and the effectiveness of the proposed model and algorithm is verified in comparison with four competitive algorithms.},
  archive      = {J_THMS},
  author       = {Tingting Wei and Xiwang Guo and Mengchu Zhou and Jiacun Wang and Shixin Liu and Shujin Qin and Ying Tang},
  doi          = {10.1109/THMS.2025.3528629},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {124--133},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A multiobjective discrete harmony search optimizer for disassembly line balancing problems considering human factors},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Supervision of multiple remote tower centers: Evaluating a new air traffic control interface based on mental workload and eye tracking. <em>THMS</em>, <em>55</em>(2), 114--123. (<a href='https://doi.org/10.1109/THMS.2025.3527136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote air traffic control offers inexpensive and efficient service to multiple airports. Recent research shows that one remote air traffic control officer can safely control up to three low-traffic airports simultaneously. In a multiple remote tower center, airports can be flexibly allocated across air traffic control officers based on prospective traffic loads. The main task of the supervisor in such a center is balancing the workload of each air traffic control officer by allocating airports accordingly. This study analyzes the supervisor's visual attention during interaction with a planning tool for their daily tasks. Five use cases were identified as the main tasks of the supervisor representing a mixture of planned and unplanned events. A mixed methods within-subjects design was used to assess the workload and eye-movement patterns associated with each of these use cases. In total, 15 professional air traffic control officers participated in the study. Workload and eye movement were analyzed independently in relation to the use cases but also in combination with each other. Across all use cases, a small correlation between subjective workload ratings and fixation duration was found, supporting previous findings of fixation duration being associated with information processing. Transitions between areas of interest on the supervisor planning tool provided valuable insights into the layout design of future supervisor planning tools.},
  archive      = {J_THMS},
  author       = {Leo Julius Materne and Maik Friedrich},
  doi          = {10.1109/THMS.2025.3527136},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {4},
  number       = {2},
  pages        = {114--123},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Supervision of multiple remote tower centers: Evaluating a new air traffic control interface based on mental workload and eye tracking},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling shared control system between human pilot and autopilot for a carrier-based aircraft landing task. <em>THMS</em>, <em>55</em>(1), 102--111. (<a href='https://doi.org/10.1109/THMS.2024.3502178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a shared control system model between the human pilot and autopilot for the special issue of carrier-based aircraft landing task. A key point of the shared and cooperative control is that the decision-sharing system depends on the longitudinal safety boundaries for manual/automatic landing. Two strategies of the human pilot are adopted, including capture strategy and tracking strategy. A hidden model tracking control method is utilized to model the autopilot. To address the issue of frequent switching between the human pilot and autopilot caused by relying solely on safety boundaries to allocate control authority, fuzzy control theory is introduced to reduce the workload of the human pilot. The time-domain simulation results show that considering the fuzzy control, the frequency of switching and the flight states have been improved compared with the results without fuzzy control. Nonlinear pilot-induced oscillation metric evaluation results show that the human-automation shared and cooperative control considering the fuzzy control can alleviate the workload of the human pilot. The shared and cooperative control system model has certain significance in ensuring the safety of carrier-based aircraft landing.},
  archive      = {J_THMS},
  author       = {Shuting Xu and Wenqian Tan and Liguo Sun},
  doi          = {10.1109/THMS.2024.3502178},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {102--111},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Modeling shared control system between human pilot and autopilot for a carrier-based aircraft landing task},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variable handle-resistance based joystick for post-stroke neurorehabilitation training of hand and wrist in upper extremities. <em>THMS</em>, <em>55</em>(1), 93--101. (<a href='https://doi.org/10.1109/THMS.2024.3486123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The objective was to modulate the resistance of a hand-held device, e.g., joystick, for customizing a rehabilitative therapeutic patient-centric virtual environment protocol. Two similar sets of springs (each set having three springs with graded rigidness) were customized to increase the handle-resistance. The springs were experimentally calibrated to determine individual spring-constant value. The amount of exerted force values during joystick movements were standardized in a cohort of healthy subjects (n = 15). Coefficient of variation (CV) was calculated to determine the variability among healthy subjects. Further, five (n = 5) patients with stroke were enrolled in this pilot study and performed three separate virtual reality sessions using different springs. Task-performance metrics, i.e., time to complete, trajectory smoothness, and relative error, were evaluated for each of the levels. The values of spring-constants as determined experimentally were found to be 1.34 × 103 ± 16.1, 2.23 × 103 ± 29.8, and 6.47 × 103 ± 470.9 N/m for springs with increased rigidity, respectively. The mean force values for different joystick movements were observed to be increasing linearly with increasing spring-rigidity. The calculated CV ≤ 14% indicated the variability in the recorded force values of healthy subjects. Increased task-performance metrics and visual analog scale-fatigue scores for session 2 and 3 as compared to session1, indicated increasing task difficulty at session 2 and 3.},
  archive      = {J_THMS},
  author       = {Debasish Nath and Neha Singh and Onika Banduni and Aprajita Parial and M. V. Padma Srivastava and Venugopalan Y. Vishnu and Amit Mehndiratta},
  doi          = {10.1109/THMS.2024.3486123},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {93--101},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Variable handle-resistance based joystick for post-stroke neurorehabilitation training of hand and wrist in upper extremities},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Individual performance in women's grassroots football: A physical and emotional perspective. <em>THMS</em>, <em>55</em>(1), 83--92. (<a href='https://doi.org/10.1109/THMS.2024.3489795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is essential to monitor and follow up with athletes, both from the point of view of physical and emotional well-being. This allows optimizing the strategy to be followed to achieve full individual and collective development, thus resulting in an improvement in performance, which helps in the prevention of injuries, and better collective work. This is especially important in the early stages of an athlete's career. The present study is based on a follow-up survey consisting of 117 female football players ranging from 10 and 20 years old, making it one of the first studies amongst this age group. A low-cost electronic performance and tracking system was developed to gather data on the players. During the training sessions, objective data (position, distances, etc.) and subjective parameters were collected using forms based on the rate of perceived exertion. This article deals with the evolution of the player's performance from both a physical and mental point of view. An emotional evaluation, based on well-being forms, is carried out and its possible influence on training. Finally, analysis is conducted on the level of health risk. It was found that the performance of female footballers improves with age and in competition-like situations. It has also been concluded that sporting activity leads to healthy lifestyle habits, which translates into a lower risk to their health.},
  archive      = {J_THMS},
  author       = {Luis A. Oliveira Rodríguez and Roberto García Fernández and David Melendi Palacio},
  doi          = {10.1109/THMS.2024.3489795},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {83--92},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Individual performance in women's grassroots football: A physical and emotional perspective},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multidimensional scaling orienting discriminative co-representation learning. <em>THMS</em>, <em>55</em>(1), 71--82. (<a href='https://doi.org/10.1109/THMS.2024.3483848'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Co-representation, which co-represents samples and features, has been widely used in various machine learning tasks, such as document clustering, gene expression analysis, and recommendation systems. It not only reveals the cluster structure of both samples and features, but also reveals the sample–feature correlation. Given a tabular data matrix, co-representation usually exhibits as the co-occurrence structures of rows and columns. However, identifying such structured patterns in complex real-world data can be very challenging. To address this problem, we propose an unsupervised discriminative co-representation learning model based on multidimensional scaling (DCLMDS). The main novelty is that DCLMDS introduces a co-representation learning term to ensure the discriminability between co-occurrence structures. As a result, the co-representation learned by DCLMDS contains richer information of the underlying correlation between samples and features within data. This could subsequently enhance the capacity of machines and systems for processing complex real-world information more proficiently. Furthermore, inspired by the fuzzy set theory, we integrate fuzzy membership degree that can accurately capture the uncertainty within data, thus enabling DCLMDS to learn a more effective co-representation in a soft manner. To evaluate the performance of DCLMDS, we conduct extensive experiments on 18 datasets, and the results demonstrate that DCLMDS can generate both accurate and discriminative co-representation, which well meets our desired outcomes.},
  archive      = {J_THMS},
  author       = {Zhang Qin and Yinghui Zhang and Hongjun Wang and Zhipeng Luo and Chongshou Li and Tianrui Li},
  doi          = {10.1109/THMS.2024.3483848},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {71--82},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Multidimensional scaling orienting discriminative co-representation learning},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved compound gaussian model for bivariate surface EMG signals related to strength training. <em>THMS</em>, <em>55</em>(1), 58--70. (<a href='https://doi.org/10.1109/THMS.2024.3486450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent literature suggests that the surface electromyography (sEMG) signals have nonstationary statistical characteristics, specifically due to the random nature of the covariance. Thus, the suitability of a statistical model for sEMG signals is determined by the choice of an appropriate model for describing the covariance. The purpose of this study is to propose a compound-Gaussian (CG) model for multivariate sEMG signals in which the latent variable of covariance is modeled as a random variable that follows an exponential model. The parameters of the model are estimated using the iterative expectation maximization (EM) algorithm. Further, a new dataset, electromyography analysis of human activities database 2 (EMAHA-DB2), is developed. The proposed model is evaluated through both qualitative and quantitative methods. Based on the model fitting analysis on the sEMG signals from EMAHA-DB2, it is found that the proposed CG model fits more closely to the empirical pdf of sEMG signals than the existing models. In addition, statistical analyses are carried out among the models and estimated parameters under different scenarios. The estimate of the exponential model's rate parameter exhibits a clear relationship with training weights, potentially correlating with underlying motor unit activity. Finally, the average signal power estimates of the channels show distinctive dependency on the training weights, the subject's training experience, and the type of activity.},
  archive      = {J_THMS},
  author       = {Durgesh Kusuru and Anish C. Turlapaty and Mainak Thakur},
  doi          = {10.1109/THMS.2024.3486450},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {58--70},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {An improved compound gaussian model for bivariate surface EMG signals related to strength training},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human-following control method based on adaptive recurrent PID controller with self-tuning filter. <em>THMS</em>, <em>55</em>(1), 48--57. (<a href='https://doi.org/10.1109/THMS.2024.3515045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on human-following robot is important for practical applications. It is a hot field of human–machine technology. This article proposes an adaptive recurrent proportional integral differential (PID) control algorithm with self-tuning filter based on vision to address the issue of insufficient recognition accuracy of specific following targets in the presence of occlusion, multiple people, or deformation. It also aims to further improve the control accuracy and immunity of a human-following robot. First, a depth camera-based red green blue (RGB) picture and a depth image are acquired. The person reidentification algorithm and the YOLOv8 algorithm are used to detect and track the targets. The spatial position information of the targets is calculated by the depth image. Additionally, the orientation proportional differential (PD) controller and the speed proportional integral (PI) controller are built. Its foundation is the discrepancy between the relative posture of the user and the robot. In order to minimize sensor data fluctuations and lessen the negative impacts of relative positional instability, a self-tuning filter is developed. To remember the relative postures between the robot and the user in the history window, an adaptive recurrent mechanism is suggested. The controller has the ability to output the control quantity in an adaptive manner based on the current system state. Finally, experiments are conducted to verify the reliability of the proposed method. The experimental findings demonstrate that the visual pedestrian tracking algorithm proposed in this article is highly adaptable. Compared to the traditional PID, fractional-order PID, and virtual spring model, our method demonstrates significant enhancements, reducing the average distance error by 64.29%, 57.14%, and 60.52% in steering scenarios, and by 42.86%, 40.00%, and 40.00% in straight-ahead scenarios, respectively.},
  archive      = {J_THMS},
  author       = {Wenfeng Li and Jinglong Zhou and Shaoyong Jiang and Chaoqun Wang and Anning Yang},
  doi          = {10.1109/THMS.2024.3515045},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {48--57},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Human-following control method based on adaptive recurrent PID controller with self-tuning filter},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A game-theoretic model of trust in Human–Robot teaming: Guiding human observation strategy for monitoring robot behavior. <em>THMS</em>, <em>55</em>(1), 37--47. (<a href='https://doi.org/10.1109/THMS.2024.3488559'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In scenarios involving robots generating and executing plans, conflicts can arise between cost-effective robot execution and meeting human expectations for safe behavior. When humans supervise robots, their accountability increases, especially when robot behavior deviates from expectations. To address this, robots may choose a highly constrained plan when monitored and a more optimal one when unobserved. While this behavior is not driven by human-like motives, it stems from robots accommodating diverse supervisors. To optimize monitoring costs while ensuring safety, we model this interaction in a trust-based game-theoretic framework. However, pure-strategy Nash equilibrium often fails to exist in this model. To address this, we introduce the concept of a trust boundary within the mixed strategy space, aiding in the discovery of optimal monitoring strategies. Human studies demonstrate the necessity of optimal strategies and the benefits of our suggested approaches.},
  archive      = {J_THMS},
  author       = {Zahra Zahedi and Sailik Sengupta and Subbarao Kambhampati},
  doi          = {10.1109/THMS.2024.3488559},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {37--47},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {A game-theoretic model of trust in Human–Robot teaming: Guiding human observation strategy for monitoring robot behavior},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive load-based affective workload allocation for multihuman multirobot teams. <em>THMS</em>, <em>55</em>(1), 23--36. (<a href='https://doi.org/10.1109/THMS.2024.3509223'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interaction and collaboration between humans and multiple robots represent a novel field of research known as human multirobot systems. Adequately designed systems within this field allow teams composed of both humans and robots to work together effectively on tasks, such as monitoring, exploration, and search and rescue operations. This article presents a deep reinforcement learning-based affective workload allocation controller specifically for multihuman multirobot teams. The proposed controller can dynamically reallocate workloads based on the performance of the operators during collaborative missions with multirobot systems. The operators' performances are evaluated through the scores of a self-reported questionnaire (i.e., subjective measurement) and the results of a deep learning-based cognitive workload prediction algorithm that uses physiological and behavioral data (i.e., objective measurement). To evaluate the effectiveness of the proposed controller, we conduct an exploratory user experiment with various allocation strategies. The user experiment uses a multihuman multirobot CCTV monitoring task as an example and carry out comprehensive real-world experiments with 32 human subjects for both quantitative measurement and qualitative analysis. Our results demonstrate the performance and effectiveness of the proposed controller and highlight the importance of incorporating both subjective and objective measurements of the operators' cognitive workload as well as seeking consent for workload transitions, to enhance the performance of multihuman multirobot teams.},
  archive      = {J_THMS},
  author       = {Wonse Jo and Ruiqi Wang and Baijian Yang and Daniel Foti and Mo Rastgaar and Byung-Cheol Min},
  doi          = {10.1109/THMS.2024.3509223},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {23--36},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Cognitive load-based affective workload allocation for multihuman multirobot teams},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fatigue assessment and control with lower limb exoskeletons. <em>THMS</em>, <em>55</em>(1), 10--22. (<a href='https://doi.org/10.1109/THMS.2024.3503473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acknowledging the vital importance of fatigue management for improving rehabilitation results, customizing treatment, safeguarding patient well-being, and enhancing the quality of life of hemiplegic patients, this study presents the development of a tailored fatigue model and a corresponding human-in-the-loop (HiL) control system for exoskeleton-assisted walking. For this, the selected three-compartment controller fatigue model including a resting recovery parameter was adapted to a dynamic walking task scenario, incorporating a torque–velocity–angle dependency to quantify muscle activity. The model parameters were experimentally verified in a study with six healthy subjects, demonstrating accurate prediction of maximum voluntary contraction (MVC) decline with an average mean absolute error of 4.9%MVC. Subsequently, an HiL control mechanism was developed, utilizing ratings of perceived fatigue and state of fatigue values as reference metrics. The presented control approach effectively regulates fatigue levels within a 0%MVC–6%MVC steady-state error range during simulations. Experimental validation confirmed this performance, however, with partly higher steady-state errors mainly due to the restrictions of the exoskeleton's assistance. This preliminary study provides a promising foundation for future research, demonstrating the potential to manage fatigue effectively in exoskeleton users, offering an improved, personalized experience.},
  archive      = {J_THMS},
  author       = {Lukas Bergmann and Lea Hansmann and Philip von Platen and Steffen Leonhardt and Chuong Ngo},
  doi          = {10.1109/THMS.2024.3503473},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {10--22},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Fatigue assessment and control with lower limb exoskeletons},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effect of an imperfect algorithm on human gait strategies with an active ankle exoskeleton. <em>THMS</em>, <em>55</em>(1), 1--9. (<a href='https://doi.org/10.1109/THMS.2024.3407984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lower-limb active exoskeletons may experience errors in operational settings due to imperfect algorithms, which may impact users' trust in the system and the human-exoskeleton fluency (the coordination of actions between the human and exoskeleton). In this study, we introduced pseudorandom catch trials (errors) in 1.68% of all strides, where an expected exoskeleton torque was not applied for a single stride, to understand the immediate and time-dependent responses to missed actuations. Participants (N = 15) completed a targeted stepping task while walking with a bilateral powered ankle exoskeleton. Human-exoskeleton fluency and trust were inferred from task performance (step accuracy), step characteristics (step length and width), muscle activity, and lower limb joint kinematics. Reductions in ankle plantarflexion during catch trials suggest user adaptation to the exoskeleton. Hip flexion and muscle activity were modulated to mitigate effects of the loss of exoskeleton torque and reduced plantarflexion during catch trials to support task accuracy and maintain step characteristics. Trust was not impacted by this level of error, as there were no significant differences in task performance or gait characteristics over time. Understanding the interactions between human-exoskeleton fluency, task accuracy, and gait strategies will support exoskeleton controller development. Future work will investigate various levels of actuation reliability to understand the transition where performance and trust are affected.},
  archive      = {J_THMS},
  author       = {Man I Wu and Brian S. Baum and Harvey Edwards and Leia Stirling},
  doi          = {10.1109/THMS.2024.3407984},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {2},
  number       = {1},
  pages        = {1--9},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Effect of an imperfect algorithm on human gait strategies with an active ankle exoskeleton},
  volume       = {55},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
