<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tai">TAI - 195</h2>
<ul>
<li><details>
<summary>
(2025). Learning from N-tuple similarities and unlabeled data. <em>TAI</em>, <em>6</em>(9), 2542-2551. (<a href='https://doi.org/10.1109/TAI.2025.3552687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from pairwise similarity and unlabeled data (SU) is a recently emerging weakly-supervised learning method, which learns a classifier from similar data pairs (two instances belonging to the same class) and unlabeled data. However, this framework is insoluble for triplet similarities and unlabeled data. To address this limitation, this article develops a framework for learning from triplet similarities (three instances belonging to the same class) and unlabeled data points, denoted as TSU. This framework not only showcases the feasibility of constructing a TSU classifier but also serves as an inspiration to explore the broader challenge of addressing N-tuple similarities (N ≥ 2) and unlabeled data points. To tackle this more generalized problem, the present article develops an advancing weakly-supervision framework of learning from N-tuple similarities (N instances belong to the same class) and unlabeled data points, named NSU. This framework provides a solid foundation for handling diverse similarity scenarios. Based on these findings, we propose empirical risk minimization estimators for both TSU and NSU classification. The estimation error bounds are also established for the proposed methods. Finally, experiments are performed to verify the effectiveness of the proposed algorithm.},
  archive      = {J_TAI},
  author       = {Junpeng Li and Shuying Huang and Changchun Hua and Yana Yang},
  doi          = {10.1109/TAI.2025.3552687},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2542-2551},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning from N-tuple similarities and unlabeled data},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of unknown-unknowns in human-in-loop human-in-plant safety critical systems. <em>TAI</em>, <em>6</em>(9), 2526-2541. (<a href='https://doi.org/10.1109/TAI.2025.3550913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Errors in artificial intelligence (AI)-enabled autonomous systems (AASs) where both the cause and effect are unknown to the human operator at the time they occur are referred to as “unknown-unknown” errors. This article introduces a methodology for preemptively identifying “unknown-unknown” errors in AAS that arise due to unpredictable human interactions and complex real-world usage scenarios, potentially leading to critical safety incidents through unsafe shifts in operational data distributions. We posit that AAS functioning in human-in-the-loop and human-in-the-plant modes must adhere to established physical laws, even when unknown-unknown errors occur. Our approach employs constructing physics-guided models from operational data, coupled with conformal inference for assessing structural breaks in the underlying model caused by violations of physical laws, thereby facilitating early detection of such errors before unsafe shifts in operational data distribution occur. Validation across diverse contexts—zero-day vulnerabilities in autonomous vehicles, hardware failures in artificial pancreas systems, and design deficiencies in aircraft in maneuvering characteristics augmentation systems (MCASs)—demonstrates our framework's efficacy in preempting unsafe data distribution shifts due to unknown-unknowns. This methodology not only advances unknown-unknown error detection in AAS but also sets a new benchmark for integrating physics-guided models and machine learning to ensure system safety.},
  archive      = {J_TAI},
  author       = {Aranyak Maity and Ayan Banerjee and Sandeep K. S. Gupta},
  doi          = {10.1109/TAI.2025.3550913},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2526-2541},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Detection of unknown-unknowns in human-in-loop human-in-plant safety critical systems},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensuring reliable learning in graph convolutional networks: Convergence analysis and training methodology. <em>TAI</em>, <em>6</em>(9), 2510-2525. (<a href='https://doi.org/10.1109/TAI.2025.3550458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in learning from graph-structured data have highlighted the importance of graph convolutional networks (GCNs). Despite some research efforts on the theoretical aspects of GCNs, a gap remains in understanding their training process, especially concerning convergence analysis. This study introduces a two-stage training methodology for GCNs, incorporating both pretraining and fine-tuning phases. A two-layer GCN model is used for the convergence analysis and case studies. The convergence analysis that employs a Lyapunov-like approach is performed on the proposed learning algorithm, providing conditions to ensure the convergence of the model learning. Additionally, an automated learning rate scheduler is proposed based on the convergence conditions to prevent divergence and eliminate the need for manual tuning of the initial learning rate. The efficacy of the proposed method is demonstrated through case studies on the node classification problem. The results reveal that the proposed method outperforms gradient descent-based optimizers by achieving consistent training accuracies within a variation of 0.1% across various initial learning rates, without requiring manual tuning.},
  archive      = {J_TAI},
  author       = {Xinge Zhao and Chien Chern Cheah},
  doi          = {10.1109/TAI.2025.3550458},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2510-2525},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ensuring reliable learning in graph convolutional networks: Convergence analysis and training methodology},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unimodal distributions for ordinal regression. <em>TAI</em>, <em>6</em>(9), 2498-2509. (<a href='https://doi.org/10.1109/TAI.2025.3549740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many real-world prediction tasks, the class labels contain information about the relative order between the labels that are not captured by commonly used loss functions such as multicategory cross-entropy. In ordinal regression, many works have incorporated ordinality into models and loss functions by promoting unimodality of the probability output. However, current approaches are based on heuristics, particularly nonparametric ones, which are still insufficiently explored in the literature. We analyze the set of unimodal distributions in the probability simplex, establishing fundamental properties and giving new perspectives to understand the ordinal regression problem. Two contributions are then proposed to incorporate the preference for unimodal distributions into the predictive model: 1) UnimodalNet, a new architecture that by construction ensures the output is a unimodal distribution, and 2) Wasserstein regularization, a new loss term that relies on the notion of projection in a set to promote unimodality. Experiments show that the new architecture achieves top performance, while the proposed new loss term is very competitive while maintaining high unimodality.},
  archive      = {J_TAI},
  author       = {Jaime S. Cardoso and Ricardo P. M. Cruz and Tomé Albuquerque},
  doi          = {10.1109/TAI.2025.3549740},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2498-2509},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unimodal distributions for ordinal regression},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MalaNet: A small world inspired neural network for automated malaria diagnosis. <em>TAI</em>, <em>6</em>(9), 2486-2497. (<a href='https://doi.org/10.1109/TAI.2025.3549406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a novel neural network architecture called MalaNet is proposed for the detection and diagnosis of malaria, an infectious disease that poses a major global health challenge. The proposed neural network architecture is inspired by small-world network principles, which generally involve the introduction of new links. A small-world neural network is realized by establishing new connections, thereby reducing the average path length and increasing clustering coefficient. These characteristics are known to enhance interconnectivity and improve feature propagation within the network. In the context of malaria diagnosis, these characteristics of MalaNet can enhance detection accuracy and enable better generalization in scenarios with limited data availability. Broadly, two variants of MalaNet are proposed in this work. First, a small-world-inspired feed-forward neural network (FNN) is developed for symptom and categorical feature-based diagnosis, providing an accessible solution when blood smear images are unavailable. Subsequently, a small-world-inspired convolutional neural network (CNN) is developed for precise and automated diagnosis when blood smear images are available. Both variants of MalaNet are rigorously validated using the National Institute of Health Malaria dataset, a clinical dataset from Federal Polytechnic Ilaro Medical Centre, Nigeria, and the APTOS dataset. Comparative results against several state-of-the-art neural network models in the literature demonstrate MalaNet’s superior performance, generalization capability, and computational efficiency. The small-world neural network architecture proposed in this work enhances feature learning, diagnostic accuracy, and adaptability in limited-data and resource-constrained settings, motivating its application in disease diagnosis where timely and accurate results are critical.},
  archive      = {J_TAI},
  author       = {Shubham Dwivedi and Kartikeya Pandey and Kumar Shubham and Om Jee Pandey and Achyut Mani Tripathi and Tushar Sandhan and Rajesh M. Hegde},
  doi          = {10.1109/TAI.2025.3549406},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2486-2497},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {MalaNet: A small world inspired neural network for automated malaria diagnosis},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-uniform illumination attack for fooling convolutional neural networks. <em>TAI</em>, <em>6</em>(9), 2476-2485. (<a href='https://doi.org/10.1109/TAI.2025.3549396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) have made remarkable strides; however, they remain susceptible to vulnerabilities, particularly to image perturbations that humans can easily recognize. This weakness, often termed as “attacks,” underscores the limited robustness of CNNs and the need for research into fortifying their resistance against such manipulations. This study introduces a novel nonuniform illumination (NUI) attack technique, where images are subtly altered using varying NUI masks. Extensive experiments are conducted on widely accepted datasets including CIFAR10, TinyImageNet, CalTech256, and NWPU-RESISC45 focusing on image classification with 12 different NUI masks. The resilience of VGG, ResNet, MobilenetV3-small, InceptionV3, and EfficientNet_b0 models against NUI attacks are evaluated. Our results show a substantial decline in the CNN models’ classification accuracy when subjected to NUI attacks, due to changes in the image pixel value distribution, indicating their vulnerability under NUI. To mitigate this, a defense strategy is proposed, including NUI-attacked images, generated through the new NUI transformation, into the training set. The results demonstrate a significant enhancement in CNN model performance when confronted with perturbed images affected by NUI attacks. This strategy seeks to bolster CNN models’ resilience against NUI attacks. A comparative study with other attack techniques shows the effectiveness of the NUI attack and defense technique.11The code is available at https://github.com/Akshayjain97/Non-Uniform_Illumination},
  archive      = {J_TAI},
  author       = {Akshay Jain and Shiv Ram Dubey and Satish Kumar Singh and KC Santosh and Bidyut Baran Chaudhuri},
  doi          = {10.1109/TAI.2025.3549396},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2476-2485},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Non-uniform illumination attack for fooling convolutional neural networks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting postgraduate student engagement using artificial intelligence (AI). <em>TAI</em>, <em>6</em>(9), 2464-2475. (<a href='https://doi.org/10.1109/TAI.2025.3548016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing number of international students (IS) enrolled in Australian higher education institutions, combined with the widespread adoption of online and hybrid learning, has significant implications for understanding the factors that influence engagement among this diverse student group. Early identification of students with low engagement facilitates academic success, prevents poor outcomes, optimizes resource allocation, improves teaching strategies, increases motivation, and supports long-term success. This study's main aim is to examine the use of AI to predict student engagement. Development of a theoretically informed survey that aimed to elicit postgraduate students' engagement was developed and validated by expert judgment. In total, 200 copies of the survey were distributed, 121 responses were received, and 96 were considered for this study representing a response rate of 48%. This study promotes a multidimensional approach, utilizing AI and ML methodologies, to determine the influence of social and cultural contexts on student engagement. This approach enables educators and institutions to create effective strategies for enhancing the learning experience of postgraduate students. Multiple AI and ML techniques have been utilized including synthetic data generation methods such GaussianCopula, triplet-based variational autoencoder, generative adversarial networks, CopulaGAN, and conditional tabular generative adversarial network. These techniques are specifically employed to predict various dimensions of engagement, including personal, academic, intellectual, social, and professional engagement. The performance of AI/ML algorithms, including support vector machine, K-nearest neighbors, decision trees, gradient boosting machine, random forest, Naive Bayes, logistic regression, and extra trees, was assessed using several metrics including F1 score, sensitivity, specificity, confusion matrix, and accuracy. The models used in this study achieved up to 85% accuracy, offering a solid foundation for guidelines and support to enhance decision making processes in higher education. These findings provide valuable insights for both academics and policy makers, laying the groundwork for evidence-based strategies to improve student engagement.},
  archive      = {J_TAI},
  author       = {Niusha Shafiabady and Tebbin Koo and Fareed Ud Din and Kabir Sattarshetty and Margaret Yen and Mamoun Alazab and Ethar Alsharaydeh},
  doi          = {10.1109/TAI.2025.3548016},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2464-2475},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Predicting postgraduate student engagement using artificial intelligence (AI)},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep graph convolutional autoencoder with conditional normalizing flow for power distribution systems fault classification and location. <em>TAI</em>, <em>6</em>(9), 2448-2463. (<a href='https://doi.org/10.1109/TAI.2025.3547878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate fault classification and location are critical to ensure the reliability and resilience of large-scale power distribution systems (PDSs). The existing data-driven works in this area struggle to capture essential space-time correlations of PDS measurements and often rely on deterministic and shallow neural architectures. Furthermore, they encounter challenges such as over-smoothing and the inability to capture deep correlations. To overcome these limitations, a novel deep space-time generative graph convolutional autoencoder (SGGCA) is proposed. First, the PDS is modeled as a space-time graph where the nodes and edges show the bus measurements and line impedance values, respectively. The proposed SGGCA's encoder captures deep correlations of the space-time graph using a new graph convolution with early connections and identity transformations to mitigate the over-smoothing. Our encoder encompasses a new recurrent method to adjust graph convolution parameters without relying on node embeddings on the temporal dimension. Additionally, it incorporates generative modeling by capturing the probability distribution function of the latent representation through a conditional normalizing flow model. The extracted generative space-time features are enhanced by a multi-head attention mechanism to better capture task-relevant characteristics of the PDS measurements. The extracted features are fed to sparse decoders to classify and locate the faults in the PDS. The feature sparsity of decoders ensures a high generalization capacity and avoids overfitting. The proposed method is evaluated on the IEEE 69-bus and 123-bus systems. It achieves substantial improvements in fault classification accuracy by 3.33% and 6.26% and enhances fault location accuracy by 6.33% and 5.73% for the respective PDSs compared with state-of-the-art models.},
  archive      = {J_TAI},
  author       = {Mohsen Saffari and Mahdi Khodayar and Mohammad E. Khodayar and Seyed Saeed Fazlhashemi},
  doi          = {10.1109/TAI.2025.3547878},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2448-2463},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep graph convolutional autoencoder with conditional normalizing flow for power distribution systems fault classification and location},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive deep learning based short-term wind speed forecasting model for variable terrain conditions. <em>TAI</em>, <em>6</em>(9), 2437-2447. (<a href='https://doi.org/10.1109/TAI.2025.3547685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wind flow can be highly unpredictable suffer substantial fluctuations in speed and direction due to the shape and height of hills, mountains, and valleys, making accurate wind speed (WS) forecasting essential in complex terrain. Hourly WS data at 50 meters above ground, from MERRA-2, NASA (2015–2021), collected from five Indian wind stations for plain and complex terrain. This article presents a novel and adaptive model for short-term WS forecasting. The article's key contributions are as follows. (a) the partial auto correlation function (PACF) is utilized to minimize the dimension of the set of intrinsic mode functions (IMF), hence reducing training time; (b) The sample entropy (SampEn) was used to calculate the complexity of the reduced set of IMFs. Since a particular deep learning (DL) model-feature-combination was selected based on complexity, the proposed method is adaptive; (c) a novel bidirectional feature-LSTM framework for complicated IMFs has been suggested, resulting in improved forecasting accuracy; (d) the proposed model shows 55.94% superior forecasting performance compared to the persistence, hybrid, ensemble empirical mode decomposition (EEMD), complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN), and variational mode decomposition (VMD)-based DL models. It has achieved the lowest prediction variance between simple and complex terrain at 0.70%, ensuring robust forecasting performance. Dimension reduction of IMF's and complexity-based model-feature selection helps reduce the training time by 68.77%, additionally forecasting quality is improved by 58.58% on average. These benefits highlight the model's adaptability, effectiveness, and resilience in addressing WS forecasting challenges on complex terrain.},
  archive      = {J_TAI},
  author       = {Sourav Malakar and Saptarsi Goswami and Bhaswati Ganguli and Amlan Chakrabarti},
  doi          = {10.1109/TAI.2025.3547685},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2437-2447},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An adaptive deep learning based short-term wind speed forecasting model for variable terrain conditions},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale heliostat field optimization for solar power tower system using matrix-based differential evolution. <em>TAI</em>, <em>6</em>(9), 2422-2436. (<a href='https://doi.org/10.1109/TAI.2025.3545813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent optimization of a solar power tower heliostat field (SPTHF) is critical for harnessing solar energy in various scenarios. However, existing SPTHF optimization methods are typically based on specific geometric layout constraints and assume that each heliostat has the same size and height. As a result, these methods are not flexible or practical in many real-world SPTHF application scenarios. Therefore, this article proposes a novel flexible SPTHF (FSPTHF) model that is more practical and involves fewer assumptions. This model enables the use of different layouts and simultaneous optimization of the parameters of each heliostat. As an FSPTHF can involve hundreds or even thousands of heliostats, optimizing the parameters of all heliostats results in a challenging large-scale optimization problem. To efficiently solve this problem, this article proposes a matrix-based differential evolution algorithm, called HMDE, for large-scale heliostat design. The HMDE uses a matrix-based encoding and representation method to improve optimization accuracy and convergence speed, incorporating two novel designs. First, a dual elite-based mutation method is proposed to enhance the convergence speed of HMDE by learning from multiple elite individuals. Second, a multi-level crossover method is proposed to improve the optimization accuracy and convergence speed by integrating element-level and vector-level crossover based on matrix representation. Extensive experiments were conducted on 30 problem instances based on real-world data with three different layouts and problem dimensions up to 12 000, where state-of-the-art algorithms were used for comparison. The experimental results show that the proposed HMDE can effectively solve large-scale FSPTHF optimization problems.},
  archive      = {J_TAI},
  author       = {Dan-Ting Duan and Jian-Yu Li and Bing Sun and Xiao-Fang Liu and Qiang Yang and Qi-Jia Jiang and Zhi-Hui Zhan and Sam Kwong and Jun Zhang},
  doi          = {10.1109/TAI.2025.3545813},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2422-2436},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Large-scale heliostat field optimization for solar power tower system using matrix-based differential evolution},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GeoDCL: Weak geometrical distortion-based contrastive learning for fine-grained fashion image retrieval. <em>TAI</em>, <em>6</em>(9), 2409-2421. (<a href='https://doi.org/10.1109/TAI.2025.3545791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses fine-grained fashion image retrieval (FIR), which aims at the detailed and precise retrieval of fashion items from extensive databases. Conventional fine-grained FIR methods design complex attention modules to enhance attribute-aware feature discrimination. However, they often ignore the multiview characteristics of real-world fashion data, leading to diminished model accuracy. Furthermore, our empirical analysis revealed that the straightforward application of standard contrastive learning methods to fine-grained FIR often yields suboptimal results. To alleviate this issue, we propose a novel weak geometrical distortion-based contrastive learning (GeoDCL) strategy. Specifically, GeoDCL incorporates both a novel positive pair design and a novel contrastive loss. GeoDCL can be seamlessly integrated into state-of-the-art (SOTA) fine-grained FIR methods during the training stage to enhance performance during inference. When GeoDCL is applied, the model structures of SOTA methods require no modifications. Additionally, GeoDCL is not utilized during inference, ensuring no increase in inference time. Experiments on the FashionAI, DeepFashion, and Zappos50K datasets verified GeoDCL's effectiveness in consistently improving SOTA models. In particular, GeoDCL drastically improved ASENet_V2 from 60.76% to 66.48% in mAP on the FashionAI dataset.},
  archive      = {J_TAI},
  author       = {Ling Xiao and Toshihiko Yamasaki},
  doi          = {10.1109/TAI.2025.3545791},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2409-2421},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {GeoDCL: Weak geometrical distortion-based contrastive learning for fine-grained fashion image retrieval},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling efficient and interpretable cybersecurity reasoning through hyperdimensional computing. <em>TAI</em>, <em>6</em>(9), 2395-2408. (<a href='https://doi.org/10.1109/TAI.2025.3545394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge graphs play a crucial role in addressing the complexities of cybersecurity, as the increasing frequency and sophistication of cyber threats pose significant challenges to traditional defense technologies. In this article, we propose a novel reasoning model, called INCYSER, that is tailored for cybersecurity. By leveraging hyperdimensional computing (HDC) as a symbolic and transparent computational model, INCYSER offers efficient and interpretable reasoning capabilities, ensuring reliable and trustworthy outcomes. Our model combines embedding-based unsupervised learning and HDC-based graph representation learning to construct a general representation for cybersecurity knowledge graphs, enabling diverse tasks including reasoning and general graph operations. Experimental evaluations demonstrate the effectiveness and efficiency of INCYSER, surpassing state-of-the-art models in link prediction and triple classification tasks. Additionally, a comprehensive ablation study examines the impact of various hyperparameters, showcasing the versatility of INCYSER. This work contributes to advancing the field of cybersecurity by introducing an interpretable and representation-based reasoning model for cybersecurity knowledge graphs.},
  archive      = {J_TAI},
  author       = {Ali Zakeri and Hanning Chen and Narayan Srinivasa and Hugo Latapie and Mohsen Imani},
  doi          = {10.1109/TAI.2025.3545394},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2395-2408},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Enabling efficient and interpretable cybersecurity reasoning through hyperdimensional computing},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancement of robot dynamics learning by integrating analytical models into deep neural networks: A data fusion perspective. <em>TAI</em>, <em>6</em>(9), 2384-2394. (<a href='https://doi.org/10.1109/TAI.2025.3544591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precise modeling of dynamical systems can be crucial for engineering applications. Traditional analytical models often struggle when capturing real-world complexities due to challenges in system nonlinearity representation and model parameter determination. Data-driven models, such as deep neural networks (DNNs), offer better accuracy and generalization but require large quantities of high-quality data. The present article introduces a novel method called the synthesized-data neural network (SDNN), which integrates analytical models, which represent physics, with DNNs to enhance the dynamic model. The main steps of the present method are as follows. The first three degrees of freedom (DOF) of a Kinova Gen3 Lite manipulator are formulated using the Euler–Lagrange equations of motion. The experimental data are recorded from the manipulator. Simulated data from the analytical model are combined with experimental data to train the neural network. The model’s performance is evaluated using the mean squared error (MSE) in real-time experiments with the Kinova Gen3 Lite manipulator. Training datasets represent 14 trajectories, with the MSE calculated for four testing trajectories. The obtained results have led to the following conclusions. The SDNN model has shown improved performance in predicting joint torques when compared to the purely analytical model or the purely data-driven model. The SDNN, when trained with synthesized data from 14 trajectories (SDNN-14), achieved the lowest MSE of 2.14, outperforming the analytical model (MSE of 2.81) and the neural network trained solely on experimental data (MSE of 3.05).},
  archive      = {J_TAI},
  author       = {Erfaan Rezvanfar and Jing Wang and Clarence W. de Silva},
  doi          = {10.1109/TAI.2025.3544591},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2384-2394},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Enhancement of robot dynamics learning by integrating analytical models into deep neural networks: A data fusion perspective},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive survey on diagnostic microscopic imaging modalities, challenges, taxonomy, and future directions for cervical abnormality detection and grading. <em>TAI</em>, <em>6</em>(9), 2354-2383. (<a href='https://doi.org/10.1109/TAI.2025.3551669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is one of the most severe diseases, affecting the lives of many people in the modern world. Among the various types of cancer, cervical cancer is one of the most frequently occurring cancers in the female population. In most cases, doctors and practitioners can typically only identify cervical cancer in its latter stages. Planning cancer therapy and increasing patient survival rates become very difficult as the disease progresses. As a result, diagnosing cervical cancer in its initial stages has become imperative to arrange proper therapy and surgery. In this article, we present a survey of automatic computerized methods for diagnosing cervical abnormalities based on microscopic imaging modalities. The present survey was conducted by defining a novel taxonomy of the surveyed techniques based on the approaches they used. We also discuss the challenges and subchallenges associated with an automatic cervical cancer diagnosis based on microscopic imaging modalities. Additionally, surveys on various public and private datasets used by the research community for developing new methods are presented. In this article, the performances of published papers are compared. The article concludes by suggesting possible research directions in these fields.},
  archive      = {J_TAI},
  author       = {Anindita Mohanta and Sourav Dey Roy and Niharika Nath and Abhijit Datta and Mrinal Kanti Bhowmik},
  doi          = {10.1109/TAI.2025.3551669},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {9},
  number       = {9},
  pages        = {2354-2383},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A comprehensive survey on diagnostic microscopic imaging modalities, challenges, taxonomy, and future directions for cervical abnormality detection and grading},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensuring fairness in spectral clustering via disparate impact-based graph construction. <em>TAI</em>, <em>6</em>(8), 2342-2352. (<a href='https://doi.org/10.1109/TAI.2025.3545800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering algorithms rely on graphs where edges are defined based on the similarity between the vertices (data points). The effectiveness and fairness of spectral clustering depend significantly on how the graph is constructed. While automated graph construction methods, which learn graphs from real-valued vector datasets, have demonstrated strong performance in the quality of clustering, fairness concerns still remain. In this work, we introduce a graph construction method that incorporates a new fairness definition—Edge Disparate Impact—into the edge relationships, aiming to produce a fair graph. This approach modifies the optimization process of automated graph construction to account for fairness, resulting in a more equitable graph. Extensive experiments were conducted to compare our method with the latest graph construction techniques and fair spectral clustering algorithms. The results prove that, by using a fair graph for spectral clustering, fairness is improved in the resulting clusters. We also demonstrate that our method outperforms baseline approaches in both fairness and the quality of clustering.},
  archive      = {J_TAI},
  author       = {Adithya K. Moorthy and V. Vijaya Saradhi and Bhanu Prasad},
  doi          = {10.1109/TAI.2025.3545800},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2342-2352},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ensuring fairness in spectral clustering via disparate impact-based graph construction},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedECE: Federated estimation of causal effect based on causal graphical modeling. <em>TAI</em>, <em>6</em>(8), 2327-2341. (<a href='https://doi.org/10.1109/TAI.2025.3545794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal effect estimation as a basic task in causal inference has been widely studied in past decades. In recent years, preserving data privacy has gained significant attention due to increasing incidents of data abuse and data leakage, however, most existing methods do not consider the problem of protecting data privacy when calculating causal effects. Thus in this article, we propose a FedECE (federated estimation of causal effect) framework for causal effect estimation in a federated setting using causal graphical modeling, which comprises two modules: a federated causal structure learning (FedCSL) module and a federated causal effect (FedCE) module. We first instantiate the FedECE framework with a basic FedECE algorithm, called FedECE-B. FedECE-B presents a layer-wise cooperative optimization strategy to learn a global skeleton by the consideration of preserving data privacy. In addition, a distributed optimal consensus strategy for V-structure identification is proposed to orient edges in the learned global skeleton. To tackle the CPDAG problem in the learned causal structure, FedECE-B presents a progressively integrated multiset strategy for federated causal effect computation. To further improve the computational efficiency and accuracy of FedECE-B, we also propose the FedECE-L and FedECE-O algorithms. The extensive experiments validate the effectiveness of the proposed methods.},
  archive      = {J_TAI},
  author       = {Yongsheng Zhao and Kui Yu and Guodu Xiang and Xianjie Guo and Fuyuan Cao},
  doi          = {10.1109/TAI.2025.3545794},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2327-2341},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FedECE: Federated estimation of causal effect based on causal graphical modeling},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ITF-VAE: Variational auto-encoder using interpretable continuous time series features. <em>TAI</em>, <em>6</em>(8), 2314-2326. (<a href='https://doi.org/10.1109/TAI.2025.3545396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning algorithms are driven by data. However, the quantity and quality of data in industries are limited due to multiple process constraints. Generating artificial data and performing a transfer learning task is a common solution to overcome these limitations. Recently, deep generative models have become one of the leading solutions for modeling a given source domain. The main hindrance to using those machine learning approaches is the lack of interpretability. Therefore, we present a novel variational autoencoder approach to generate time series data on a probabilistic latent feature representation and enhance interpretability within the generative model and the output trajectory. We sample selective and parameter values for certain continuous function candidates to assemble the synthetic time series. The sparse design of the generative model enables direct interpretability and matches an estimated posterior distribution of the detected components in the source domain. Through residual stacking, conditionality, and a mixture of prior distributions, we derive a stacked version of the evidence lower bound to learn our network. Tests on synthetic and real industrial datasets underline the performance and interpretability of our generative model. Depending on the model and function candidates, the user can define a trade-off between flexibility and interpretability. Overall, this work presents an innovative interpretable representation of the latent space and further developed evidence lower bound criterion driven by the designed architecture.},
  archive      = {J_TAI},
  author       = {Hendrik Klopries and Andreas Schwung},
  doi          = {10.1109/TAI.2025.3545396},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2314-2326},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ITF-VAE: Variational auto-encoder using interpretable continuous time series features},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance analysis and design of a weighted personalized quantum federated learning. <em>TAI</em>, <em>6</em>(8), 2302-2313. (<a href='https://doi.org/10.1109/TAI.2025.3545393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in federated and quantum computing have improved data privacy and efficiency in distributed systems. Quantum federated learning (QFL), like its classical counterpart, classic federated learning (CFL), struggles with challenges in heterogeneous environments. To address these, we propose wp-QFL, a weighted personalized approach with quantum federated averaging (qFedAvg), tackling non-IID data and local model drift. While CFL personalization has been well explored, its application to QFL remains underdeveloped due to inherent differences. The proposed wp-QFL fills this gap by adapting to data heterogeneity with weighted personalization and drift correction. The code implementation is available at https://github.com/s222416822/wpQFL.},
  archive      = {J_TAI},
  author       = {Dev Gurung and Shiva Raj Pokhrel},
  doi          = {10.1109/TAI.2025.3545393},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2302-2313},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Performance analysis and design of a weighted personalized quantum federated learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing generalization of offline RL in data-limited settings with heuristic rules. <em>TAI</em>, <em>6</em>(8), 2291-2301. (<a href='https://doi.org/10.1109/TAI.2025.3544971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the ability to learn from static datasets, OFFLINE reinforcement learning (RL) emerges as a compelling avenue for real-world applications. However, state-of-the-art offline RL algorithms perform suboptimally when confronted with limited data confined to specific regions within the state space. Performance degradation is attributed to the inability of offline RL algorithms to learn appropriate actions for rare or unseen observations. This article proposes a heuristic rule-based regularization technique and adaptively refines the initial knowledge from heuristics to considerably boost performance in limited data with partially omitted states. The key insight is that the regularization term mitigates erroneous actions for sparse samples and unobserved states covered by domain knowledge. Empirical evaluations on standard offline RL datasets demonstrate a substantial average performance increase compared to ensemble of domain knowledge and existing offline RL algorithms operating on limited data.},
  archive      = {J_TAI},
  author       = {Briti Gangopadhyay and Zhao Wang and Jia-Fong Yeh and Shingo Takamatsu},
  doi          = {10.1109/TAI.2025.3544971},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2291-2301},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Enhancing generalization of offline RL in data-limited settings with heuristic rules},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LaBINet—An approach for seamlessly integrating new advertisement into an existing scene. <em>TAI</em>, <em>6</em>(8), 2281-2290. (<a href='https://doi.org/10.1109/TAI.2025.3544595'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Billboards in multimedia images are critical for capturing wide audiences through advertising. Currently, no open-source platform exists for automated billboard integration, which impacts industries such as filmmaking, advertising, and sports broadcasting. Effective detection and seamless integration of new advertisements into existing frames are essential for this process. This article introduces LaBINet, a technique that leverages advanced deep learning methodologies to localize existing advertisements and utilizes image registration techniques for seamless integration of new ads. The process begins with generating a probabilistic map using AdSegNet to obtain transformed coordinates. Next, seamless integration is performed using the Poisson equation combined with Laplace matrices. To address the challenge of evaluating image quality in the absence of a reference image, we propose an evaluation method that correlates and statistically verifies subjective and objective scores. Experimental results demonstrate that our method outperforms existing techniques in integrating billboards under various lighting conditions, achieving strong subjective preference scores (76–95%) and low distortion scores (median values ranging from 21.817 to 22.529), indicating superior image quality.},
  archive      = {J_TAI},
  author       = {Sukriti Dhang and Mimi Zhang and Soumyabrata Dev},
  doi          = {10.1109/TAI.2025.3544595},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2281-2290},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {LaBINet—An approach for seamlessly integrating new advertisement into an existing scene},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SecureLLAMA: Secure FPGAs using LLAMA large language models. <em>TAI</em>, <em>6</em>(8), 2266-2280. (<a href='https://doi.org/10.1109/TAI.2025.3544590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field-programmable gate arrays (FPGAs) are increasingly utilized in critical applications across sectors such as infrastructure, defense, and autonomous systems. However, the inherent flexibility of FPGAs introduces significant security vulnerabilities, particularly in the hardware description languages (HDLs) used to program them. This article introduces SecureLLAMA, an enhanced version of the LLAMA2 model, specifically designed to detect and mitigate FPGA vulnerabilities. Leveraging a novel dataset “FPGAvul” which includes both real-world examples and synthetically generated vulnerabilities. Our dataset FPGAvul addresses vulnerabilities such as initialization errors, clock domain crossing issues, insecure state machines, resource sharing conflicts, and buffer overflows. SecureLLAMA demonstrates superior accuracy in identifying and addressing security flaws in FPGA configurations. Comprehensive evaluation shows that SecureLLAMA significantly improves the detection of vulnerabilities, providing a robust solution for securing FPGAs in embedded systems. The findings of this research have the potential to advance FPGA security practices, ensuring their safe integration in critical environments where reliability is essential.},
  archive      = {J_TAI},
  author       = {Mansour Alqarni and Akramul Azim},
  doi          = {10.1109/TAI.2025.3544590},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2266-2280},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {SecureLLAMA: Secure FPGAs using LLAMA large language models},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pruning networks only using few-shot pretraining based on gradient similarity frequency. <em>TAI</em>, <em>6</em>(8), 2253-2265. (<a href='https://doi.org/10.1109/TAI.2025.3544582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural network pruning is a popular and promising approach aiming at reducing heavy networks to lightweight ones by removing redundancies. Most existing methods adopt a three-stage pipeline, including pretraining, pruning, and fine-tuning. However, it is time-consuming to train a large and redundant network in the pretraining process. In this work, we propose a new minimal pretraining pruning method, gradient similarity frequency-based pruning (GSFP), which prunes a given network only using few-shot pretraining before training. Instead of pretraining a fully trained over-parameterized model, our method only uses one epoch to obtain the ranked list of convolution filters to be pruned according to their gradient similarity frequency and determines the redundant convolution filters that should be removed. Then, the obtained sparse network is trained in the standard way without the need to fine-tune the inherited weights from the full model. Finally, a series of experiments are conducted to verify the effectiveness of CIFAR10/100 and ImageNet. The results show that our method can achieve remarkable results on some popular networks, such as VGG, ResNet, and DenseNet. Importantly, the proposed pruning approach never requires pretraining the over-parameterized model, thus offering a promising prospect of application and spreading for limited computational resources.},
  archive      = {J_TAI},
  author       = {Haigen Hu and Huihuang Zhang and Qianwei Zhou and Tieming Chen},
  doi          = {10.1109/TAI.2025.3544582},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2253-2265},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Pruning networks only using few-shot pretraining based on gradient similarity frequency},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-generated versus human text: Introducing a new dataset for benchmarking and analysis. <em>TAI</em>, <em>6</em>(8), 2241-2252. (<a href='https://doi.org/10.1109/TAI.2025.3544183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is increasingly embedded in our everyday lives. With the introduction of ChatGPT in November 2022 by OpenAI, people can now ask a bot to generate comprehensive writeups in seconds. This new transformative technology also introduces ethical, safety, and other general concerns. It is important to harness the power of AI to understand whether a body of text is generated by AI or whether it is organically human. In this article, we create and curate a medium-sized dataset of 10 000 records containing both human and machine-generated text and utilize it to train a reliable model to accurately distinguish between the two. First, we use DistilGPT-2 with various inputs to generate machine text. Then, we acquire an equal sample size of human-generated text. All the text is cleaned, explored, and visualized using the uniform manifold approximation and projection (UMAP) dimensionality reduction technique. Finally, the text is transformed into vectors using several techniques, including bag of words, term frequency-inverse document frequency, bidirectional encoder representations from transformer, and neural network-based embeddings. Machine learning experiments are then performed with traditional models such as logistic regression, random forest, and XGBoost, as well as deep learning models such as long short-term memory (LSTM), convolutional neural network (CNN), and CNN-LSTM. Across all vectorization strategies and machine learning algorithms, we measure accuracy, precision, recall, and F1 scores. We also time each exercise. Each model completes its training within an hour, and we observe scores above 90%. We then use the Shapley additive explanations (SHAP) package on machine learning models to explore if and how we can explain the model to further validate results. Lastly, we deploy our TF-IDF Random Forest model to a user-friendly web application using the Streamlit framework, allowing users without coding expertise to interact with the model.},
  archive      = {J_TAI},
  author       = {Ali Al Bataineh and Rachel Sickler and Kerry Kurcz and Kristen Pedersen},
  doi          = {10.1109/TAI.2025.3544183},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2241-2252},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {AI-generated versus human text: Introducing a new dataset for benchmarking and analysis},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiagent actor-critic generative AI for query resolution and analysis. <em>TAI</em>, <em>6</em>(8), 2226-2240. (<a href='https://doi.org/10.1109/TAI.2025.3544173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce multiagent strategic query resolution and diagnostic tool (MASQRAD), a transformative framework for query resolution based on the actor-critic model, which utilizes multiple generative AI agents. MASQRAD is excellent at translating imprecise or ambiguous user inquiries into precise and actionable requests. This framework generates pertinent visualizations and responses to these focused queries, as well as thorough analyses and insightful interpretations for users. MASQRAD addresses the common shortcomings of existing solutions in domains that demand fast and precise data interpretation, such as their incapacity to successfully apply AI for generating actionable insights and their challenges with the inherent ambiguity of user queries. MASQRAD functions as a sophisticated multiagent system but “masquerades” to users as a single AI entity, which lowers errors and enhances data interaction. This approach makes use of three primary AI agents: Actor Generative AI, Critic Generative AI, and Expert Analysis Generative AI. Each is crucial for creating, enhancing, and evaluating data interactions. The Actor AI generates Python scripts to generate data visualizations from large datasets within operational constraints, and the Critic AI rigorously refines these scripts through multiagent debate. Finally, the Expert Analysis AI contextualizes the outcomes to aid in decision-making. With an accuracy rate of 87% when handling tasks related to natural language visualization, MASQRAD establishes new benchmarks for automated data interpretation and showcases a noteworthy advancement that has the potential to revolutionize AI-driven applications.},
  archive      = {J_TAI},
  author       = {Mohammad Wali Ur Rahman and Ric Nevarez and Lamia Tasnim Mim and Salim Hariri},
  doi          = {10.1109/TAI.2025.3544173},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2226-2240},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiagent actor-critic generative AI for query resolution and analysis},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ContentDM: A layout diffusion model for content-aware layout generation. <em>TAI</em>, <em>6</em>(8), 2215-2225. (<a href='https://doi.org/10.1109/TAI.2025.3544172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Content-aware layout generation aims to produce fitting element arrangements based on the background contents, which is used for graphic design applications such as automatic poster layout design. In this article, we propose ContentDM, a layout diffusion model specifically designed for the content-aware layout generation task, overcoming the limitations suffered from existing methods: irrational arrangement among layout elements and lack of refining ability for coarse generated results. ContentDM defines the layout diffusion process through random perturbations applied to both the position and type of layout elements. During the denoising training phase, the content-aware layout generator is trained to reconstruct samples from these perturbed layouts. This process enables the model to learn the correct arrangement patterns within the layout elements, thereby enhancing the rationality of generated layouts. Moreover, we develop an iterative layout inference strategy to enable the layout generator to refine the generated layouts progressively, thereby enhancing the overall quality of the generation results. Extensive experiments demonstrate that ContentDM significantly outperforms existing methods, achieving state-of-the-art performance in content-aware layout generation, both in terms of visual quality and quantitative metrics.},
  archive      = {J_TAI},
  author       = {Honglin Guo and Weizhi Nie and Ruidong Chen and Lanjun Wang and Guoqing Jin and Anan Liu},
  doi          = {10.1109/TAI.2025.3544172},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2215-2225},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ContentDM: A layout diffusion model for content-aware layout generation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multiparticle swarm neural architecture search for high-incidence cancer prediction. <em>TAI</em>, <em>6</em>(8), 2203-2214. (<a href='https://doi.org/10.1109/TAI.2025.3543822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancer is a disease caused by uncontrolled growth and spread of cells, and early diagnosis is essential to improve the cure rate and reduce mortality. Although machine learning and deep learning have shown great potential in early cancer prediction, the accuracy of detection and prediction still needs to be improved due to the different scales of lesion areas. Therefore, we propose an adaptive multiparticle swarm neural architecture search method to automatically explore an efficient deep neural network architecture for high-incidence cancer prediction. First, the multiparticle swarm strategy is used to initialize the high-quality architecture in the scale adaptive search space to enhance multiscale perception. Then, the improved weighted average method is combined with classification accuracy, parameters, and floating-point operations to adaptively update the particle swarm architecture to avoid falling into local optimum. In addition, a method based on weight sharing is used to improve the efficiency of architecture search. The experimental results show that comparing with the manual design network and the existing neural architecture search method, the proposed algorithm achieves average increments of 26.33%, 33.99%, 8.98%, 37.41%, 35.1%, and 51.76% in classification accuracy, F1-Score, Cohen's kappa, AUC, exponential balance accuracy and search efficiency, respectively.},
  archive      = {J_TAI},
  author       = {Liming Xu and Jie Zheng and Chunlin He and Jing Wang and Bochuan Zheng and Jiancheng Lv},
  doi          = {10.1109/TAI.2025.3543822},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2203-2214},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive multiparticle swarm neural architecture search for high-incidence cancer prediction},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Similar or related: Spectral-based item relationship mining with graph convolutional network for complementary recommendation. <em>TAI</em>, <em>6</em>(8), 2193-2202. (<a href='https://doi.org/10.1109/TAI.2025.3543820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complementary recommendation, which aims to recommend frequently copurchased items to users, has gained significant attention. Unlike traditional similarity-based recommendations, complementary recommendation focus on items that are related but not necessarily similar (e.g., computers and keyboards), that aligns with users’ purchasing habits. However, most of current complementary recommendation systems fail to effectively differentiate or measure these two types of relationships. In this article, we propose similar or related: spectral-based item relationship mining with graph convolutional network for complementary recommendation (SR-Rec). First, we design two spectral-based filters to fully mine the similarity and relevance information of items, thereby achieving effective discrimination between the two types of relationships. Then, we compute similarity and relevance scores between items separately, and employ a pairwise self-attention mechanism to measure the impact of these relationships on the final recommendations. Experimental results on three public open-source datasets demonstrate that SR-Rec outperforms state-of-the-art performance in complementary recommendation.},
  archive      = {J_TAI},
  author       = {Gang-Feng Ma and Xu-Hua Yang and Haixia Long and Yujiao Huang},
  doi          = {10.1109/TAI.2025.3543820},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2193-2202},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Similar or related: Spectral-based item relationship mining with graph convolutional network for complementary recommendation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixture density function estimation in shape clustering. <em>TAI</em>, <em>6</em>(8), 2178-2192. (<a href='https://doi.org/10.1109/TAI.2025.3543815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in measurement tools have made it easier to obtain shape data, a collection of point coordinates in vector space that are meaningful when some of them are gathered together. As a result, clustering of shape data becomes increasingly important. However, few studies still perform applicable clustering in various cases because some studies rely on their specific shape representations. Thus, we apply a simple and widely recognized representation and generative model to shape. A configuration matrix of the point coordinates is used for the representation, and it is the simplest and most well-accepted representation in conventional shape analysis. As a generative model, we consider the mixture density function, a well-known model in statistics for expressing a population density function, which is a linear combination of subpopulation density functions. The aim of this article is to present a mixture density-based model that will be useful for clustering shape data. The clustering of shapes involves estimating the parameters of the model, and this estimation is derived using an EM algorithm based on the model. As examples of promising shape-data applications, the computational analyses of ape skulls, American football formations, and baseball pitches were performed. In addition, we evaluated the performance of the EM algorithm by comparing it with other typical clustering methods. The theoretical results not only contribute to statistical estimation for shape data but also extend the clustering of nonvector shape data. The experimental results show that the derived EM algorithm performs well in shape clustering.},
  archive      = {J_TAI},
  author       = {Kazunori Iwata},
  doi          = {10.1109/TAI.2025.3543815},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2178-2192},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Mixture density function estimation in shape clustering},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facilitating continuous facial aging through latent age attribute modulation. <em>TAI</em>, <em>6</em>(8), 2163-2177. (<a href='https://doi.org/10.1109/TAI.2025.3543811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, facial aging has attracted significant research interest due to its broad applications and potential benefits. While generative adversarial networks (GANs) have achieved notable progress in synthesizing realistic facial images, many GAN-based facial aging methods struggle to accurately capture the continuous progression of age-related changes over time. In this article, we propose an innovative framework featuring the latent age attribute module (LAAM), which maps age attributes to a structured latent space that facilitates efficient sampling for precise age attribute modeling. We further introduce the age-AdaIN fusion module (AFM), which seamlessly integrates age features from LAAM with facial content features, enabling the generation of images that exhibit smooth, continuous age transitions. This framework excels in capturing fine-grained aging details, particularly for elderly individuals. Quantitative and qualitative evaluations on benchmark datasets demonstrate the effectiveness of our approach in generating realistic age-progressed facial images, with a notable improvement in elderly aging accuracy and detail.},
  archive      = {J_TAI},
  author       = {Xiyuan Hu and Jinglei Qu and Chen Chen},
  doi          = {10.1109/TAI.2025.3543811},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2163-2177},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Facilitating continuous facial aging through latent age attribute modulation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-dimensional hyperparameter optimization via adjoint differentiation. <em>TAI</em>, <em>6</em>(8), 2148-2162. (<a href='https://doi.org/10.1109/TAI.2025.3540799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an emerging machine learning task, high-dimensional hyperparameter optimization (HO) aims at enhancing traditional deep learning models by simultaneously optimizing the neural networks’ weights and hyperparameters in a joint bilevel configuration. However, such nested objectives can impose nontrivial difficulties for the pursuit of the gradient of the validation risk with respect to the hyperparameters (a.k.a. hypergradient). To tackle this challenge, we revisit its bilevel objective from the novel perspective of continuous dynamics and then solve the whole HO problem with the adjoint state theory. The proposed HO framework, termed Adjoint Diff, is naturally scalable to a very deep neural network with high-dimensional hyperparameters because it only requires constant memory cost in training. Adjoint Diff is in fact, a general framework that some existing gradient-based HO algorithms are well interpreted by it with simple algebra. In addition, we further offer the Adjoint Diff+ framework by incorporating the prevalent momentum learning concept into the basic Adjoint Diff for enhanced convergence. Experimental results show that our Adjoint Diff frameworks outperform several state-of-the-art approaches on three high-dimensional HO instances including, designing a loss function for imbalanced data, selecting samples from noisy labels, and learning auxiliary tasks for fine-grained classification.},
  archive      = {J_TAI},
  author       = {Hongkun Dou and Hongjue Li and Jinyang Du and Leyuan Fang and Qing Gao and Yue Deng and Wen Yao},
  doi          = {10.1109/TAI.2025.3540799},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2148-2162},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {High-dimensional hyperparameter optimization via adjoint differentiation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path-aware few-shot knowledge graph completion. <em>TAI</em>, <em>6</em>(8), 2133-2147. (<a href='https://doi.org/10.1109/TAI.2025.3540796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot knowledge graph completion (FKGC) has emerged as a significant area of interest for addressing the long-tail problem in knowledge graphs. Traditional approaches often focus on the sparse few-shot neighborhood to derive semantic representation, overlooking other critical information forms such as relation paths. In this article, we introduce an innovative method, called PARE, which fully leverages relation paths to enhance the few-shot representation by simultaneously incorporating both neighborhood and relation path information. Inspired by the principles of information transmission, PARE directly models relation paths between entities and parameterizes the information interference within different relation paths. Through parameter learning, PARE effectively captures information propagation along relation paths while mitigating the influence of relation dependency. To preserve neighborhood information, we employ a two-step neighborhood aggregator to resolve few-shot neighbors’ ambiguity and develop a reconstruction module. By integrating the representations of relation paths and contextual neighborhoods, we achieve a comprehensive few-shot representation for two given entities. We utilize a matching processor for knowledge triplet evaluation. Extensive experiments demonstrate that our PARE model outperforms state-of-the-art baselines on widely-used benchmark datasets.},
  archive      = {J_TAI},
  author       = {Shuo Yu and Yingbo Wang and Zhitao Wan and Yanming Shen and Qiang Zhang and Feng Xia},
  doi          = {10.1109/TAI.2025.3540796},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2133-2147},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Path-aware few-shot knowledge graph completion},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond histogram comparison: Distribution-aware simple-path graph kernels. <em>TAI</em>, <em>6</em>(8), 2119-2132. (<a href='https://doi.org/10.1109/TAI.2025.3539642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {R-convolution graph kernels are conventional methods for graph classification. They decompose graphs into substructures and aggregate all the substructure similarity as graph similarity. However, the substructure similarity is based on graph isomorphism, which not only leads to binary similarity values but also cannot be aware of the probability distribution of substructures in each graph. Moreover, the simple sum aggregation is not aware of the probability distribution differences of substructures across graphs. These drawbacks cause inaccurate graph similarity. To resolve these problems, we propose a new method called the distribution-aware simple-path (DASP) graph kernel. The neural language models are employed to capture the probability distribution of substructures (specifically, simple paths) in each graph. A new metric called probabilistic Minkowski distance is developed to capture the probability distribution differences of simple paths across graphs. To further improve the performance, the label alphabet is expanded to enlarge the corpus of simple paths for the neural language models and DASP. Experiments demonstrate that DASP achieves the best classification accuracy on all the selected graph benchmark datasets.},
  archive      = {J_TAI},
  author       = {Wei Ye and Shuhao Tang and Hao Tian and Qijun Chen},
  doi          = {10.1109/TAI.2025.3539642},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2119-2132},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Beyond histogram comparison: Distribution-aware simple-path graph kernels},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-empowered cyber-secure federated learning for trustworthy edge computing. <em>TAI</em>, <em>6</em>(8), 2110-2118. (<a href='https://doi.org/10.1109/TAI.2025.3539258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a privacy-preserving distributed machine learning scheme, where each participant's data remains on the participant's devices and only the local model generated utilizing the local computational power is transmitted throughout the database. However, the distributed computational nature of FL creates the necessity to develop a mechanism that can remotely trigger any network agents, track their activities, and prevent threats to the overall process posed by malicious participants. Particularly, the FL paradigm may become vulnerable due to an active attack from the network participants, called a poisonous attack. In such an attack, the malicious participant acts as a benign agent capable of affecting the global model quality by uploading an obfuscated poisoned local model update to the server. This article presents a cross-device FL model that ensures trustworthiness, fairness, and authenticity in the underlying FL training process. We leverage trustworthiness by constructing a reputation-based trust model based on agents’ contributions toward model convergence. We ensure fairness by identifying and removing malicious agents from the training process through an outlier detection technique. Additionally, we establish authenticity by generating a token for each participating device through a distributed sensing mechanism and storing that unique token in a blockchain smart contract. Further, we insert the trust scores of all agents into a blockchain and validate their reputations using various consensus mechanisms that consider the computational task.},
  archive      = {J_TAI},
  author       = {Ervin Moore and Ahmed Imteaj and Md Zarif Hossain and Shabnam Rezapour and M. Hadi Amini},
  doi          = {10.1109/TAI.2025.3539258},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2110-2118},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Blockchain-empowered cyber-secure federated learning for trustworthy edge computing},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel recursive ensemble feature selection framework for high-dimensional data. <em>TAI</em>, <em>6</em>(8), 2098-2109. (<a href='https://doi.org/10.1109/TAI.2025.3538549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble feature selection combines feature subsets with diversity, potentially providing a better approximation of the optimal feature subset. While extensive research has focused on enhancing diversity among ensemble members, its critical role during the aggregation process remains underexplored. To address this gap, we propose a novel Recursive Ensemble Feature Selection (REFS) framework that explicitly incorporates diversity into the aggregation phase to improve both robustness and accuracy. The framework comprises three key components: 1) a randomization-based feature mapping strategy (RS) to generate diverse base feature selectors optimized for performance; 2) a quantitative diversity metric (DM) to evaluate the complementarity of these selectors; and 3) a fuzzy aggregation (FA) method that leverages order statistics, rank scores, and weight information to effectively integrate multiple ranked feature lists. Experimental evaluations on fifteen real-world datasets demonstrate that REFS consistently outperforms competitive methods in terms of classification accuracy and resilience to parameter variations. By explicitly integrating diversity into the aggregation process, REFS provides a more comprehensive and effective approach to feature selection, paving the way for improved predictive performance across diverse applications.},
  archive      = {J_TAI},
  author       = {Xiaojian Ding and Zihan Xu and Yi Li and Fumin Ma and Shilin Chen},
  doi          = {10.1109/TAI.2025.3538549},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2098-2109},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A novel recursive ensemble feature selection framework for high-dimensional data},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multimodal-driven fusion data augmentation framework for emotion recognition. <em>TAI</em>, <em>6</em>(8), 2083-2097. (<a href='https://doi.org/10.1109/TAI.2025.3537965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pursuit of imbuing computers with emotional intelligence has driven extensive research into physiological signal analysis for emotion recognition. Deep learning techniques offer promising avenues for analyzing physiological signals in this domain. Despite numerous studies on emotion recognition using various physiological signals, challenges persist in classifying multimodal physiological signals due to data scarcity. Current research lacks focus on addressing data insufficiency for multimodal physiological signals. This article proposes an innovative method to address this issue and improve the effect of emotion recognition using multimodal physiological signal data. Our model comprises a physiological signal encoder, a multimodal data generator, and a multimodal emotion recognizer. Specifically, we introduce a customized ConvNeXt-attention fusion model (CNXAF) to fuse diverse physiological signals, generating fused multimodal data. The multimodal data generator employs a conditional self-attention generative adversarial network (c-SAGAN) to synthesize additional data across different categories, augmenting original datasets. Finally, the multimodal emotion recognizer utilizes the ConvNeXt-t classifier for emotion recognition on the extended dataset. Through extensive experimentation, our model achieves accuracies of 96.06$\%$ on the DEAP dataset and 95.70$\%$ on the WESAD dataset, demonstrating the effectiveness of our approach in accurately recognizing emotions. Experimental results underscore the superior performance of our method compared to existing approaches in multimodal emotion recognition research.},
  archive      = {J_TAI},
  author       = {Ao Li and Minchao Wu and Rui Ouyang and Yongming Wang and Fan Li and Zhao Lv},
  doi          = {10.1109/TAI.2025.3537965},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2083-2097},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A multimodal-driven fusion data augmentation framework for emotion recognition},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HDL: Hybrid and dynamic learning for fake face recognition. <em>TAI</em>, <em>6</em>(8), 2073-2082. (<a href='https://doi.org/10.1109/TAI.2025.3537963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face swapping aims to replace a source face with a target face, generating a fake face that is indistinguishable from the real one to the human eye. Existing face recognition methods usually discriminate the fake face as the target face identity, which happens to be misguided. To address this embarrassment, we pioneer a new task called “fake face recognition,” which seeks to discover the identity of the source face based on the fake face. Besides, we design a hybrid and dynamic learning strategy for fake face recognition. Specifically, we hybridize the existing real face recognition dataset with the fake face dataset. Based on the popular margin-based face recognition approach, we achieve dynamic learning by adjusting the margin for the fake face samples. The deep network is guided to first focus on real samples and then explores the identity of implicit commonalities between real and fake samples. To verify the performance of the fake face recognition model, we further organize the existing fake face datasets into face pairs. Extensive experiments on the fake face datasets show that our proposed hybrid and dynamic learning strategy achieves superior average accuracy (98.46%) compared to benchmark studies.},
  archive      = {J_TAI},
  author       = {Baojin Huang and Jiaqi Ma and Guangcheng Wang and Hui Wang},
  doi          = {10.1109/TAI.2025.3537963},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2073-2082},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {HDL: Hybrid and dynamic learning for fake face recognition},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient solution validation of constraint satisfaction problems on neuromorphic hardware: The case of sudoku puzzles. <em>TAI</em>, <em>6</em>(8), 2061-2072. (<a href='https://doi.org/10.1109/TAI.2025.3536428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) offer an effective approach to solving constraint satisfaction problems (CSPs) by leveraging their temporal, event-driven dynamics. Moreover, neuromorphic hardware platforms provide the potential for achieving significant energy efficiency in implementing such models. Building upon these foundations, we present an enhanced, fully spiking pipeline for solving CSPs on the SpiNNaker neuromorphic hardware platform. Focusing on the use case of Sudoku puzzles, we demonstrate that the adoption of a constraint stabilization strategy, coupled with a neuron idling mechanism and a built-in validation process, enables this application to be realized through a series of additional layers of neurons capable of performing control logic operations, verifying solutions, and memorizing the network's state. Simulations conducted in the GPU-enhanced neuronal networks (GeNN) environment validate the contributions of each pipeline component before deployment on SpiNNaker. This approach offers three key advantages: 1) Improved success rates for solving CSPs, particularly for challenging instances from the hard class, surpassing state-of-the-art SNN-based solvers. 2) Reduced data transmission overhead by transmitting only the final activity state from SpiNNaker instead of all generated spikes. 3) Substantially decreased spike extraction time. Compared with previous work focused on the same use case, our approach achieves a significant reduction in the number of extracted spikes (54.63% to 99.98%) and extraction time (88.56% to 96.41%).},
  archive      = {J_TAI},
  author       = {Riccardo Pignari and Vittorio Fra and Enrico Macii and Gianvito Urgese},
  doi          = {10.1109/TAI.2025.3536428},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2061-2072},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Efficient solution validation of constraint satisfaction problems on neuromorphic hardware: The case of sudoku puzzles},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAWL-net: A statistical attentions and wavelet aided lightweight network for classification of cancers in histopathological images. <em>TAI</em>, <em>6</em>(8), 2051-2060. (<a href='https://doi.org/10.1109/TAI.2025.3536424'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Addressing the formidable challenges posed by the diagnosis and management of various types of cancer, including breast, colon, lung, and colorectal cancer, demands innovative solutions to streamline histopathological analysis processes. In this study, we propose a novel lightweight convolutional neural network (CNN) called statistical attentions and wavelet aided lightweight network (SAWL-Net) architecture based on MobileNetV2 equipped with hybrid statistical similarity and wave format-aided attention mechanisms, specifically tailored to the demands of cancer histopathology. By leveraging the capabilities, our model incorporates a lightweight design while ensuring high-performance outcomes. We introduce a unique combination of Pearson correlation coefficient, Spearman rank correlation, and cosine similarity metrics, alongside a specialized wave conversion technique to enhance the detection of similarities across different channels of histopathological data, while providing a holistic approach to the model. In this study, we have considered breast, colorectal, and lung & colon cancer datasets for experimentation. Notably, our model surpasses prevailing state-of-the-art methodologies, showcasing its efficacy in optimizing diagnostic accuracy and expediting treatment strategies for varied cancer types. Our codes are publicly available at the GitHub repository.},
  archive      = {J_TAI},
  author       = {Surya Majumder and Aishik Paul and Friedhelm Schwenker and Ram Sarkar},
  doi          = {10.1109/TAI.2025.3536424},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2051-2060},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {SAWL-net: A statistical attentions and wavelet aided lightweight network for classification of cancers in histopathological images},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication-efficient distributed learning for nash equilibrium of aggregative games over time-varying digraphs. <em>TAI</em>, <em>6</em>(8), 2041-2050. (<a href='https://doi.org/10.1109/TAI.2025.3535458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication efficiency is a major challenge in learning the Nash equilibrium (NE) of aggregative games in a distributed manner. To address this problem, this article focuses on designing a communication-efficient algorithm under unbalanced digraphs, where the cost function of each player is affected by its own actions and the average aggregation function. In particular, the considered games have no central node, and no player has direct access to the aggregation function. To estimate the aggregation function, an auxiliary variable is employed to estimate the right Perron eigenvector of the column-stochastic weight matrix, which extends the dynamic average consensus protocol to time-varying digraphs. Additionally, players exchange information periodically and perform multistep local updates with local information between two consecutive communications. By combining the above two strategies with the gradient descent method, a communication-efficient algorithm is proposed and achieves a linear convergence rate. Then, the communication period selection method is provided to determine the best tradeoff between local updates and information exchange under limited resources. Finally, numerical results demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_TAI},
  author       = {Mingfei Chen and Dong Wang and Xiaopeng Xu and Wenli Yao and Bingyang Zhu},
  doi          = {10.1109/TAI.2025.3535458},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2041-2050},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Communication-efficient distributed learning for nash equilibrium of aggregative games over time-varying digraphs},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAMScore: A content structural similarity metric for image translation evaluation. <em>TAI</em>, <em>6</em>(8), 2027-2040. (<a href='https://doi.org/10.1109/TAI.2025.3535456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image translation has wide applications, such as style transfer and modality conversion, usually aiming to generate images having both high degrees of realism and faithfulness. These problems remain difficult, especially when it is important to preserve content structures. Traditional image-level similarity metrics are of limited use, since the content structures of an image are high-level and not strongly governed by pixelwise faithfulness to an original image. To fill this gap, we introduce SAMScore, a generic content structural similarity metric for evaluating the faithfulness of image translation models. SAMScore is based on the recent high-performance segment anything model (SAM), which allows content similarity comparisons with standout accuracy. We applied SAMScore on 19 image translation tasks and found that it is able to outperform all other competitive metrics on all tasks. We envision that SAMScore will prove to be a valuable tool that will help to drive the vibrant field of image translation, by allowing for more precise evaluations of new and evolving translation models.},
  archive      = {J_TAI},
  author       = {Yunxiang Li and Meixu Chen and Kai Wang and Jun Ma and Alan C. Bovik and You Zhang},
  doi          = {10.1109/TAI.2025.3535456},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2027-2040},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {SAMScore: A content structural similarity metric for image translation evaluation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised multivariate time series anomaly detection by feature decoupling in federated learning scenarios. <em>TAI</em>, <em>6</em>(8), 2013-2026. (<a href='https://doi.org/10.1109/TAI.2025.3533437'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomalies are usually regarded as data errors or novel patterns previously unseen, which are quite different from most observed data. Accurate detection of anomalies is crucial in various application scenarios. This article focuses on unsupervised anomaly detection from multivariate time series (MTS), as real-world data collected from sources such as wearable devices, medical equipment, and industrial machines typically manifest as MTS and are often unlabeled. Anomaly detection in MTS represents a data-driven challenge that traditionally requires substantial centralized data for training models. However, in practice, data are frequently distributed among multiple institutions, with privacy concerns restricting unrestricted access. To address these issues, we introduce feature decoupling federated learning (FDFL), an approach designed to collaboratively train a representation learning network over multiple clients for unsupervised anomaly detection in MTS. Unlike previous methods that simply integrate MTS anomaly detection (MTS-AD) algorithms with federated learning (FL) strategies, FDFL specifically addresses heterogeneity among clients by decoupling the representation network into shared and private branches through a contrastive learning mechanism. This method aggregates shared parameters during each federated round while maintaining client-specific private parameters locally. Additionally, we develop a self-attention block that integrates the representations derived from both shared and private parameters to reconstruct MTS and identify anomalies based on reconstruction errors. Extensive experiments conducted on three publicly available datasets demonstrate that FDFL outperforms existing algorithms in most cases, highlighting the effectiveness and superiority of our proposed method in MTS-AD.},
  archive      = {J_TAI},
  author       = {Yifan He and Xi Ding and Yateng Tang and Jihong Guan and Shuigeng Zhou},
  doi          = {10.1109/TAI.2025.3533437},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {2013-2026},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unsupervised multivariate time series anomaly detection by feature decoupling in federated learning scenarios},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive operator selection for meta-heuristics: A survey. <em>TAI</em>, <em>6</em>(8), 1991-2012. (<a href='https://doi.org/10.1109/TAI.2025.3545792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Appropriate selection of search operators plays a critical role in meta-heuristic algorithm design. Adaptive selection of suitable operators to the characteristics of different optimization stages is an important task that owns promising potential to improve the performance of a meta-heuristic algorithm. A variety of adaptive operator selection methods have been proposed in last decades, from the machine learning and optimization communities. However, the existing studies have not been systematically reviewed so far. To fill the gap, this article provides a comprehensive survey of adaptive operator selection for meta-heuristics. According to the information required for selection, adaptive operator selection methods are classified into two categories: 1) stateless methods; and 2) state-based methods. Each category is further summarized into several key components. The strategies of each component belonging to the two categories are reviewed respectively. The motivation, strengths and weaknesses of the proposed strategies are also discussed. Furthermore, studied meta-heuristics and optimization problems in the literature are summarized. The effects from the difference of meta-heuristics and problems to the specific design of methods are discussed, together with the guidance of selecting the suitable method in different application scenarios. At the end, emerging challenges that could guide further research are discussed.},
  archive      = {J_TAI},
  author       = {Jiyuan Pei and Yi Mei and Jialin Liu and Mengjie Zhang and Xin Yao},
  doi          = {10.1109/TAI.2025.3545792},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {1991-2012},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive operator selection for meta-heuristics: A survey},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Responsible AI: Portraits with intelligent bibliometrics. <em>TAI</em>, <em>6</em>(8), 1977-1990. (<a href='https://doi.org/10.1109/TAI.2024.3510474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shifting the focus from principles to practical implementation, responsible artificial intelligence (AI) has garnered considerable attention across academia, industry, and society at large. Despite being in its nascent stages, this emerging field grapples with nebulous concepts and intricate knowledge frameworks. By analyzing three prevailing concepts–explainable AI, trustworthy AI, and ethical AI, this study defined responsible AI (RAI) and identified its core principles. Methodologically, this study successfully demonstrated the implementation of leveraging AI’s capabilities into bibliometrics for enhanced knowledge discovery and the cross-validation of experimentally examined models with domain insights. Empirically, this study investigated 17 799 research articles contributed by the AI community since 2015. This involves recognizing key technological players and their relationships, unveiling the topical landscape and hierarchy of RAI, charting its evolution, and elucidating the interplay between the responsibility principles and primary AI techniques. An analysis of a core cohort comprising 380 articles from multiple disciplines captures the most recent advancements in RAI. As one of the pioneering bibliometric studies dedicated to exploring RAI, this study will provide comprehensive macro-level insights, enhancing the understanding of RAI while furnishing valuable knowledge support for AI regulation and governance initiatives.},
  archive      = {J_TAI},
  author       = {Yi Zhang and Mengjia Wu and Guangquan Zhang and Jie Lu},
  doi          = {10.1109/TAI.2024.3510474},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {1977-1990},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Responsible AI: Portraits with intelligent bibliometrics},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative adversarial networks for dynamic malware behavior: A comprehensive review, categorization, and analysis. <em>TAI</em>, <em>6</em>(8), 1955-1976. (<a href='https://doi.org/10.1109/TAI.2025.3537966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article highlights the critical role of machine learning (ML) in combating the dynamic nature of cybersecurity threats. Unlike previous studies focusing mainly on static analysis, this work surveys the literature on dynamic analysis-based malware generation and detection. The study addresses the complexities of applying GANs to tabular data with heavy-tailed and multimodal distributions. It also examines the challenges of generating sequential malware behavior data and categorizes GAN-based models and their primary use cases. Furthermore, the article evaluates adversarial losses and their limitations in generating dynamic malware behavior. Finally, it identifies existing metrics to assess GAN generalization in malware research and suggests future research directions based on identified limitations.},
  archive      = {J_TAI},
  author       = {Ghebrebrhan Gebrehans and Naveed Ilyas and Khouloud Eledlebi and Willian Tessaro Lunardi and Martin Andreoni and Chan Yeob Yeun and Ernesto Damiani},
  doi          = {10.1109/TAI.2025.3537966},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {8},
  number       = {8},
  pages        = {1955-1976},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Generative adversarial networks for dynamic malware behavior: A comprehensive review, categorization, and analysis},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLyer: Federated learning-based crop yield prediction for agriculture 5.0. <em>TAI</em>, <em>6</em>(7), 1943-1952. (<a href='https://doi.org/10.1109/TAI.2025.3534149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crop yield prediction is a significant area of precision agriculture. In this article, we propose a crop yield prediction framework named FLyer, based on federated learning and edge computing. In FLyer, the soil and environmental data are locally processed inside the edge servers, and the model parameters are transmitted between the edge servers and the cloud with encrypted gradients. LSTM is used as the local and global models for data analysis. As the LSTM model can capture the temporal dependencies and hold the sequential nature of the data, we use LSTM in FLyer. By encrypting the gradients, the gradient information leakage ratio is reduced, and data privacy is protected. For gradient encryption, we use AES-256, and for data encryption during local storage we use RSA and AES-256. The results demonstrate that FLyer diminishes the latency by $\boldsymbol{\sim}$39% and energy consumption by $\boldsymbol{\sim}$40% than the conventional edge-cloud framework respectively. The experimental results show that the global model in FLyer achieves above 99% accuracy, precision, recall, and F1-score in crop yield prediction. The results also present that the local models also achieve $\boldsymbol{&gt;}$94% accuracy in crop yield prediction.},
  archive      = {J_TAI},
  author       = {Tanushree Dey and Somnath Bera and Anwesha Mukherjee and Debashis De and Rajkumar Buyya},
  doi          = {10.1109/TAI.2025.3534149},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1943-1952},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FLyer: Federated learning-based crop yield prediction for agriculture 5.0},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based selective feature fusion for litchi fruit detection using multimodal UAV sensor measurements. <em>TAI</em>, <em>6</em>(7), 1932-1942. (<a href='https://doi.org/10.1109/TAI.2025.3532205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of precision agriculture, accurate crop detection is crucial for crop yield estimation, and health monitoring using photogrammetric measurements. Achieving high precision requires advance object detection models and multiscale feature fusion. This article addresses key research gaps in litchi crop monitoring, including the lack of a suitable dataset for litchi detection in natural environment and the limitations of conventional deep learning models in handling challenges such as occlusion, overlapping, and background complexities. First, we prepare high-resolution litchi dataset called “UAVLitchi” of 5000 images that include both RGB and multispectral images and next, we propose a selective feature fusion (SFF)-based architecture for litchi detection. By utilizing both RGB and multispectral images, this architecture effectively mitigates the challenges of visual detection arising from the complex cluster growth structure of litchis, offering a robust solution for accurate detection. The integration of SFF within a dual-channel mask-region based convolutional neural network (Mask-RCNN) leading to significant improvements in feature extraction for litchi detection. Experimental results demonstrate impressive performance, achieving an mean average precession (mAP50) of 94.65%, mAP75 of 89.23%, recall of 90.16%, and F1-score of 91.44%.},
  archive      = {J_TAI},
  author       = {Debarun Chakraborty and Bhabesh Deka},
  doi          = {10.1109/TAI.2025.3532205},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1932-1942},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep learning-based selective feature fusion for litchi fruit detection using multimodal UAV sensor measurements},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Positive sample mining: Fuzzy threshold-based contrastive learning for enhanced unsupervised skeleton-based action recognition. <em>TAI</em>, <em>6</em>(7), 1918-1931. (<a href='https://doi.org/10.1109/TAI.2025.3531831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contrastive learning is one of the fundamental paradigms for unsupervised 3-D skeleton-based action recognition. Existing contrastive learning paradigms typically enhance model discrimination by increasing the distance between different action samples in the feature space. However, this approach can inadvertently lead to an increase in the intraclass distance for the same action category, thereby affecting the effectiveness of action recognition. To address this issue, we introduce an innovative unsupervised framework named fuzzy threshold-based contrastive learning (FTCL). This novel approach leverages the concept of fuzzy thresholds to handle sample partitioning within the feature space. In essence, given a dataset of human actions, we distinguish different action samples as “negative samples” and identical action samples as “positive samples.” By analyzing the similarity distribution between these positive and negative samples, we apply the principles of fuzzy thresholds to evaluate the attributes of the negative samples. This refined evaluation facilitates a judicious reassignment of positive and negative sample classifications, thus circumventing the challenges associated with increased intraclass distances. Furthermore, to obtain better action representations from skeleton data, we model and contrast skeleton data from different spatiotemporal perspectives, capturing rich spatiotemporal information in the feature representation of actions. Extensive experiments on the NTU-60, NTU-120, and PKU-MMD datasets were conducted to validate our proposed FTCL. The experimental results demonstrate that our approach achieves significant improvements.},
  archive      = {J_TAI},
  author       = {Hengsheng Xu and Jianqi Zhong and Deliang Lian and Hanxu Hou and Wenming Cao},
  doi          = {10.1109/TAI.2025.3531831},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1918-1931},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Positive sample mining: Fuzzy threshold-based contrastive learning for enhanced unsupervised skeleton-based action recognition},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust control of uncertain quantum systems based on physics-informed neural networks and sampling learning. <em>TAI</em>, <em>6</em>(7), 1906-1917. (<a href='https://doi.org/10.1109/TAI.2025.3531330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-fidelity quantum control is one of the key elements in quantum computing and information processing. In view of possible inaccuracies in quantum system modeling and inevitable errors in control fields, the design of robust control fields is of great importance. In this article, we propose a neural network-based robust control strategy that incorporates physics-informed neural networks (PINNs) and sampling-based learning control techniques for uncertain closed and open quantum systems. We employ the gradient descent algorithm with momentum for the network training, where two methods including direct calculation and automatic differentiation are used to compute the gradient of the loss function with respect to network weights. The direct calculation method demonstrates the internal mechanism of the gradient computation, while the automatic differentiation technology is easier to utilize. We provide some guidelines for the parameter selection of the sampling learning algorithm in the PINN robust control scheme to ensure good control performance. In particular, for open quantum systems with uncertainties, we point out the necessity of fast control. Some simulation experiments are conducted on closed and open systems with uncertainties and the results show the effectiveness of the proposed PINN control scheme in achieving high-fidelity state transfer of uncertain quantum systems.},
  archive      = {J_TAI},
  author       = {Kai Zhang and Qi Yu and Sen Kuang},
  doi          = {10.1109/TAI.2025.3531330},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1906-1917},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Robust control of uncertain quantum systems based on physics-informed neural networks and sampling learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint detection of rhythmic and morphological abnormalities in electrocardiographic images: A multitask learning approach. <em>TAI</em>, <em>6</em>(7), 1894-1905. (<a href='https://doi.org/10.1109/TAI.2025.3530383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electrocardiogram (ECG) is the most widely used diagnostic tool for the characterization of heart function. Although automated methods of ECG interpretation can improve clinical care, but most methods are designed on signal-based data. In this work, we consider images of paper-based representations of multichannel ECG to develop intelligent methods for its analysis. Cardiovascular abnormalities are manifested in ECG through either morphological alterations, rhythmic variations, or a combination of both. To effectively classify these cardiac abnormalities, we formulate a multitask learning framework comprising two primary tasks relating to the classification of morphological and rhythmic abnormalities and an auxiliary task on delineating regions pertaining to the primary tasks. We employ a dynamic task weighting approach based on homoscedastic uncertainty to balance the task-specific losses in the multitask framework. We evaluate our method on two databases: an internal database containing clinical ECG images obtained from multiple medical centres in Assam, India, and the other comprising ECG images extracted from a publicly available 12-lead ECG dataset. Experimental evaluation shows that our proposed deep architecture outperforms single-task learning counterparts and achieves promising performance for both morphological ailments and rhythm classification tasks. Results also demonstrate superior performance compared to other image-based state-of-the-art methods. Moreover, analysis of the post-hoc interpretation in the form of saliency maps verifies the model's performance and provides clinically meaningful inferences to its predictions.},
  archive      = {J_TAI},
  author       = {Pharvesh Salman Choudhary and L.N. Sharma and Samarendra Dandapat},
  doi          = {10.1109/TAI.2025.3530383},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1894-1905},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Joint detection of rhythmic and morphological abnormalities in electrocardiographic images: A multitask learning approach},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Next-generation computer vision in veterinary medicine: A study on canine ophthalmology. <em>TAI</em>, <em>6</em>(7), 1884-1893. (<a href='https://doi.org/10.1109/TAI.2025.3530380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Taking into account the achievements of state-of-the-art computer vision methods in recent years, the aim of this research was to examine the extent to which their application can help in the detection of symptoms of eye diseases in dogs and the diagnosis of ophthalmological conditions in order to provide owners with preliminary information about the disease of their pets and speed up making diagnoses to veterinarians. In the research, clinical data of canine eye diseases including at least one of the 4 symptoms of the disease was collected and a set was formed to train the segmentation model, which was expanded with synthesized data generated using the LoRA Stable Diffusion model verified by an ophthalmologist. An extended segmentation model based on U-Net architecture with ResNet34 backbone was fine-tuned on the prepared set and compared to zero-training GPT-4o and Grounding SAM. The results show that the fine-tuned U-Net model gives the best segmentation results of eye disease symptoms of 97% base of pixel accuracy metric and significantly outperforms other tested methods. The segmentation masks are used as part of the prompts for GPT-4 and GPT-4o to generate diagnoses of diseases having the specified symptoms. The generated diagnostic results were evaluated using text evaluation metrics and that the most accurate diagnosis according to the Bert score of 84% is achieved using GPT-4o in combination with the U-Net segmentation mask. The article proposes a pipeline that gives the best results and solutions to be considered for other diagnostic procedures in ophthalmology and veterinary medicine.},
  archive      = {J_TAI},
  author       = {Matija Burić and Marina Ivašić-Kos},
  doi          = {10.1109/TAI.2025.3530380},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1884-1893},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Next-generation computer vision in veterinary medicine: A study on canine ophthalmology},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). G-mix: A generalized mixup learning framework toward flat minima. <em>TAI</em>, <em>6</em>(7), 1870-1883. (<a href='https://doi.org/10.1109/TAI.2025.3529816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have demonstrated promising results in various complex tasks. However, such DNN models face challenges related to over-parameterization, particularly in scenarios where training data are scarce. In response to these challenges and to improve the generalization capabilities of DNNs, the Mixup technique has emerged, which effectively addresses the limitations posed by over-parameterization. Nevertheless, it still produces suboptimal outcomes. Inspired by the successful sharpness-aware minimization (SAM) method, which establishes a connection between the sharpness of the training loss landscape and model generalization, we propose a new learning framework called Generalized-Mixup, which combines the strengths of Mixup and SAM for training DNN models. The theoretical analysis provided demonstrates how the developed G-Mix framework enhances generalization. Additionally, to further optimize DNN performance with the G-Mix framework, we introduce two novel algorithms: Binary G-Mix (BG-Mix) and Decomposed G-Mix (DG-Mix). These algorithms partition the training data into two subsets based on the sharpness-sensitivity of each example to address the issue of “manifold intrusion” in Mixup. Both theoretical explanations and experimental results reveal that the proposed BG-Mix and DG-Mix algorithms further enhance model generalization across multiple datasets and models, achieving state-of-the-art performance.},
  archive      = {J_TAI},
  author       = {Xingyu Li and Bo Tang},
  doi          = {10.1109/TAI.2025.3529816},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1870-1883},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {G-mix: A generalized mixup learning framework toward flat minima},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-XAI for explaining the explainer: Unveiling image features driving deep learning decisions. <em>TAI</em>, <em>6</em>(7), 1859-1869. (<a href='https://doi.org/10.1109/TAI.2025.3529397'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has revolutionized computer vision by allowing neural networks to automatically learn features from data. However, the highly nonlinear nature of deep neural networks makes them difficult to interpret, leading to concerns about potential biases in critical applications. To address this, researchers have advocated for explainable artificial intelligence (XAI). Many XAI techniques have been proposed but all of them only highlight image regions influencing model decisions, lacking any further explanations. In this article, we propose a posthoc model-agnostic meta-XAI method that explains why specific image regions are used for decisions. The article presents the experimental setup and results, discussing the perturbations used for explanations in color, frequency, shape, shading, and texture. The explanation is given in terms of human-interpretable image features, e.g., color, shape, shading, and texture both as perturbation plots and as visual summary through the use of the newly introduced normalized area under the curve score. The experimental results confirm the previous findings that vision deep learning models are biased toward texture, but also highlight the importance of color, frequency content, and perceptually salient structures in the final decision.},
  archive      = {J_TAI},
  author       = {Simone Bianco},
  doi          = {10.1109/TAI.2025.3529397},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1859-1869},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Meta-XAI for explaining the explainer: Unveiling image features driving deep learning decisions},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based method for irrigation status detection in tomato using plant leaves. <em>TAI</em>, <em>6</em>(7), 1849-1858. (<a href='https://doi.org/10.1109/TAI.2025.3528926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The impact of climate change, arguably global warming and resulting drought, is one of the most escalating agricultural challenges affecting crop productivity. Therefore, effective water management is critical in agricultural practices. The analysis of plant leaves presents an opportunity to gauge irrigation status through automated solutions to encourage broader adoption among farmers. Currently, there is a notable absence of AI methods in the literature for detecting tomato plant irrigation status through leaf analysis. Addressing this gap, we propose a novel end-to-end deep learning (DL)-based method, inspired by the ResNet-50 model. Our model trims unnecessary blocks and reduces larger kernels, significantly streamlining the model to better fit with the leaf image dataset related to the tomato irrigation status. We evaluate our method using a newly developed dataset and find outstanding performance (Precision: 99.05%, Recall: 99.01%, F1-score: 99.01%, mean-average F1: 98.98%, weighted-average F1: 98.95%, Kappa: 98.61%, accuracy: 98.90%) while comparing with the pretrained DL models. Additionally, our model has fewer parameters and lower floating-point operations (FLOPs), enhancing its efficiency and suggesting its potential for more cost-effective and productive irrigation management practices.},
  archive      = {J_TAI},
  author       = {Tej Bahadur Shahi and Chiranjibi Sitaula and Krishna Prasad Bhandari and Shobha Poudel and Rupesh Bhandari and Ravindra Mishra and Bharat Kumar Sharma and Bhogendra Mishra},
  doi          = {10.1109/TAI.2025.3528926},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1849-1858},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep learning-based method for irrigation status detection in tomato using plant leaves},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A trustworthy AIoT-enabled localization system via federated learning and blockchain. <em>TAI</em>, <em>6</em>(7), 1838-1848. (<a href='https://doi.org/10.1109/TAI.2025.3528917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a significant demand for indoor localization technology in smart buildings, and the most promising solution in this field is using radio frequency (RF) sensors and fingerprinting-based methods that employ machine learning models trained on crowd-sourced user data gathered from Internet of Things (IoT) devices. However, this raises security and privacy issues in practice. Some researchers propose to use federated learning (FL) to partially overcome privacy problems, but there still remain security concerns, e.g., single-point failure and malicious attacks. In this article, we propose a framework named DFLoc to achieve precise 3-D localization tasks while considering the following two security concerns. Particularly, we design a specialized blockchain to decentralize the framework by distributing the tasks such as model distribution and aggregation, which are handled by a central server to all clients in most previous works, to tackle the single-point failure issue in ensuring reliable and accurate indoor localization. Moreover, we introduce an updated model verification mechanism within the blockchain to alleviate the concern of malicious node attacks. Experimental results substantiate the framework's capacity to deliver accurate 3-D location predictions and its superior resistance to the impacts of single-point failure and malicious attacks when compared to conventional centralized FL systems.},
  archive      = {J_TAI},
  author       = {Junfei Wang and He Huang and Jingze Feng and Steven Wong and Lihua Xie and Jianfei Yang},
  doi          = {10.1109/TAI.2025.3528917},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1838-1848},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A trustworthy AIoT-enabled localization system via federated learning and blockchain},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Go beyond end-to-end training: Boosting greedy local learning with context supply. <em>TAI</em>, <em>6</em>(7), 1823-1837. (<a href='https://doi.org/10.1109/TAI.2025.3528384'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional end-to-end (E2E) training of deep networks necessitates storing intermediate activations for back-propagation, resulting in a large memory footprint on GPUs and restricted model parallelization. As an alternative, greedy local learning partitions the network into gradient-isolated modules and trains supervisely based on local preliminary losses, thereby providing asynchronous and parallel training methods that substantially reduce memory cost. However, empirical experiments reveal that as the number of segmentations of the gradient-isolated module increases, the performance of the local learning scheme degrades substantially, severely limiting its expansibility. To avoid this issue, we theoretically analyze the greedy local learning from the standpoint of information theory and propose a ContSup scheme, which incorporates context supply between isolated modules to compensate for information loss. Experiments on benchmark datasets (i.e. CIFAR, SVHN, STL-10) achieve SOTA results and indicate that our proposed method can significantly improve the performance of greedy local learning with minimal memory and computational overhead, allowing for the boost of the number of isolated modules.},
  archive      = {J_TAI},
  author       = {Chengting Yu and Fengzhao Zhang and Hanzhi Ma and Aili Wang and Er-Ping Li},
  doi          = {10.1109/TAI.2025.3528384},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1823-1837},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Go beyond end-to-end training: Boosting greedy local learning with context supply},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emergency scheduling of aerial vehicles via graph neural neighborhood search. <em>TAI</em>, <em>6</em>(7), 1808-1822. (<a href='https://doi.org/10.1109/TAI.2025.3528381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The thriving advances in autonomous vehicles and aviation have enabled the efficient implementation of aerial last-mile delivery services to meet the pressing demand for urgent relief supply distribution. Variable neighborhood search (VNS) is a promising technique for aerial emergency scheduling. However, the existing VNS methods usually exhaustively explore all considered neighborhoods with a prefixed order, leading to an inefficient search process and slow convergence speed. To address this issue, this article proposes a novel graph neural neighborhood search (GENIS) algorithm, which includes an online reinforcement learning (RL) agent that guides the search process by selecting the most appropriate low-level local search operators based on the search state. We develop a dual-graph neural representation learning method to extract comprehensive and informative feature representations from the search state. Besides, we propose a reward-shaping policy learning method to address the decaying reward issue along the search process. Extensive experiments conducted across various benchmark instances demonstrate that the proposed algorithm significantly outperforms the state-of-the-art approaches. Further investigations validate the effectiveness of the newly designed knowledge guidance scheme and the learned feature representations.},
  archive      = {J_TAI},
  author       = {Tong Guo and Yi Mei and Wenbo Du and Yisheng Lv and Yumeng Li and Tao Song},
  doi          = {10.1109/TAI.2025.3528381},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1808-1822},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Emergency scheduling of aerial vehicles via graph neural neighborhood search},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Age-aware UAV-aided energy harvesting for the design of wireless rechargeable mobile networks. <em>TAI</em>, <em>6</em>(7), 1797-1807. (<a href='https://doi.org/10.1109/TAI.2025.3528377'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Internet of Things (IoT) technology has enhanced connectivity and automation in industries and daily life. The introduction of mobile IoT devices (IoTDs) has further expanded the productivity of these networks beyond conventional cyber–physical systems, resulting in wireless rechargeable mobile networks (WRMNs). However, the inherent limitations of low-powered IoTDs mandate their repetitive charging in dynamic environments. To address this, we propose radio frequency (RF) energy harvesting from unmanned aerial vehicles (UAVs) to supplement the energy needs of IoTDs. Moreover, the IoTDs’ mobility and nonuniform energy utilization are challenging for UAV scheduling in WRMNs. Additionally, maintaining a balance between efficient utilization of UAV energy and IoTD energy harvesting adds complexity to the problem. In this work, we introduce the age of charging (AoC) metric to quantify IoTDs’ repetitive charging and propose an energy-efficient UAV scheduling scheme to maximize UAV energy usage efficiency (EUE) in WRMNs. Moreover, a Markov decision process (MDP) is formulated to address UAV-EUE maximization. Subsequently, a deep reinforcement learning (DRL) scheme is proposed within the deep deterministic policy gradient (DDPG) framework to optimize UAV charging sequences. The DRL agent (UAV) autonomously learns optimal charging strategies considering IoTD mobility patterns, energy demand fluctuations, and IoTD energy-harvesting capabilities. Simulation results demonstrate the superiority of the proposed DRL algorithm over existing DRL-based UAV scheduling schemes, significantly enhancing the operational lifespan of WRMNs and ensuring network stability and continuous functionality. This motivates the adoption of the proposed DRL scheme for developing autonomous, energy-aware, next-generation IoT applications.},
  archive      = {J_TAI},
  author       = {Aditya Singh and Rajesh M. Hegde},
  doi          = {10.1109/TAI.2025.3528377},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1797-1807},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Age-aware UAV-aided energy harvesting for the design of wireless rechargeable mobile networks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistency-heterogenity balanced fake news detection via cross-modal matching. <em>TAI</em>, <em>6</em>(7), 1787-1796. (<a href='https://doi.org/10.1109/TAI.2025.3527921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating synthetic content through generative AI (GAI) presents considerable hurdles for current fake news detection methodologies. Many existing detection approaches concentrate on feature-based multimodal fusion, neglecting semantic relationships such as correlations and diversities. In this study, we introduce an innovative cross-modal matching-driven approach to reconcile semantic relevance (text–image consistency) and semantic gap (text–image heterogeneity) in multimodal fake news detection. Unlike the conventional paradigm of multimodal fusion followed by detection, our approach integrates textual modality, visual modality (images), and text embedded within images (auxiliary modality) to construct an end-to-end framework. This framework considers the relevance of contents across different modalities while simultaneously addressing the gap in structures, achieving a delicate balance between consistency and heterogeneity. Consistency is fostered by evaluating intermodality correlation via pairwise-similarity scores, while heterogeneity is addressed by employing cross-attention mechanisms to account for intermodality diversity. To achieve equilibrium between consistency and heterogeneity, we employ attention-guided enhanced modality interaction and similarity-based dynamic weight assignment to establish robust frameworks. Comparative experiments conducted on the Chinese Weibo dataset and the English Twitter dataset demonstrate the effectiveness of our approach, surpassing the state-of-the-art by 7% to 13%.},
  archive      = {J_TAI},
  author       = {Ying Guo and Bingxin Li and Kexin Zhen and Jie Liu and Gaolei Li and Qi Wang and Yong-Jin Liu},
  doi          = {10.1109/TAI.2025.3527921},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1787-1796},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Consistency-heterogenity balanced fake news detection via cross-modal matching},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaCoRCE loss for knowledge distillation: A novel approach with network fission and co-teaching technique. <em>TAI</em>, <em>6</em>(7), 1776-1786. (<a href='https://doi.org/10.1109/TAI.2025.3527402'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep models have been successful in almost every research field, and they are capable of handling complex problem statements. But most of the deep neural networks are huge in size with millions/billions of parameters requiring heavy resources and computations to be installed in edge devices. In this article, we present an efficient co-teaching strategy consisting of multiple small networks performing mutually at runtime to consistently improve the efficiency and generalization ability of neural networks. Unlike existing distillation mechanism, that utilizes large capacity pre-train teacher model to transfer knowledge to a smaller network unidirectionally, proposed framework treats all the networks as ‘teacher’ (student-sized) and co-teach them allowing them to compute concurrently and quickly with better generalizations. We have carefully divided the backbone network into small network using depth scaling with regularizations. Multiple small networks are used during the co-teaching process, and the proposed AdaCoRCE loss is used to make the network learn from each other. During training, these networks are provided with the two different views of same data to increase their diversity. Co-teaching scheme allows model to fetch stronger and unique representation of knowledge by using different data views and AdaCoRCE loss. This article provides a generalized framework that could be applied to various network structures (e.g., MobileNets, ResNet, MixNet, etc.) and it demonstrates efficient performance on variety of histology image datasets. In this article, we have used four different publicly available histology dataset on two types of diseases to evaluate the performance of proposed technique. Analysis on colorectal cancer and breast cancer histology images suggests that the proposed model enhances the overall performance of the model in terms of accuracy, GFLOPs and inference time. Further, the proposed framework is also analyzed using benchmark cifar-10 dataset and comparison of our result is done with several state-of-the-art results on mutual/collaborative learning. To the best of our knowledge, we analyzed that the proposed model outperformed these recent models in terms of accuracy, GFLOPs and inference time. Extensive result analysis on different histology benchmark datasets and benchmark cifar-10 dataset suggests that the proposed model is a generally applicable model that could be used for various computer vision-based tasks.},
  archive      = {J_TAI},
  author       = {Shankey Garg and Pradeep Singh},
  doi          = {10.1109/TAI.2025.3527402},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1776-1786},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {AdaCoRCE loss for knowledge distillation: A novel approach with network fission and co-teaching technique},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defense against adversarial faces at the source: Strengthened faces based on hidden disturbances. <em>TAI</em>, <em>6</em>(7), 1761-1775. (<a href='https://doi.org/10.1109/TAI.2025.3527923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition (FR) systems, while widely used across various sectors, are vulnerable to adversarial attacks, particularly those based on deep neural networks. Despite existing efforts to enhance the robustness of FR models, they still face the risk of secondary adversarial attacks. To address this, we propose a novel approach employing “strengthened face” with preemptive defensive perturbations. Strengthened face ensures original recognition accuracy while safeguarding FR systems against secondary attacks. In the white-box scenario, the strengthened face utilizes gradient-based and optimization-based methods to minimize feature representation differences between face pairs. For the black-box scenario, we propose shielded gradient sign descent (SGSD) to optimize the gradient update direction of strengthened faces, ensuring the transferability and effectiveness against unknown adversarial attacks. Experimental results demonstrate the efficacy of strengthened faces in defending against adversarial faces without compromising the performance of FR models or face image visual quality. Moreover, SGSD outperforms conventional methods, achieving an average performance improvement of 4% in transferability across different attack intensities.},
  archive      = {J_TAI},
  author       = {Shuangliang Li and Jinwei Wang and Hao Wu and Jiawei Zhang and Xin Cheng and Xiangyang Luo and Bin Ma},
  doi          = {10.1109/TAI.2025.3527923},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1761-1775},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Defense against adversarial faces at the source: Strengthened faces based on hidden disturbances},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodality consistency for point cloud completion via differentiable rendering. <em>TAI</em>, <em>6</em>(7), 1746-1760. (<a href='https://doi.org/10.1109/TAI.2025.3527922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion aims to acquire complete and high-fidelity point clouds from partial and low-quality point clouds, which are used in remote sensing applications. Existing methods tend to solve this problem solely from the point cloud modality, limiting the completion process to only 3-D structure while overlooking the information from other modalities. Nevertheless, additional modalities possess valuable information that can greatly enhance the effectiveness of point cloud completion. The edge information in depth images can serve as a supervisory signal for ensuring accurate outlines and overall shape. To this end, we propose a brand-new point cloud completion network, dubbed multimodality differentiable rendering (MMDR), which utilizes point-based differentiable rendering (DR) to obtain the depth images to ensure that the model preserves the point cloud structures from the depth image domain. Moreover, the attentional feature extractor (AFE) module is devised to exploit the global features inherent in the partial input, and the extracted global features together with the coordinates and features of the patch center are fed into the point roots predictor (PRP) module to obtain a set of point roots for the upsampling module with point upsampling Transformer (PU-Transformer). Furthermore, the multimodality consistency loss between the depth images from predicted point clouds and corresponding ground truth enables the PU-Transformer to generate a high-fidelity point cloud with predicted point agents. Extensive experiments conducted on various existing datasets give evidence that MMDR surpasses the off-the-shelf methods for point cloud completion after qualitative and quantitative analysis.},
  archive      = {J_TAI},
  author       = {Ben Fei and Yixuan Li and Weidong Yang and Wen-Ming Chen and Zhijun Li},
  doi          = {10.1109/TAI.2025.3527922},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1746-1760},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multimodality consistency for point cloud completion via differentiable rendering},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge intelligence: A deep distilled model for wearables to enable proactive eldercare. <em>TAI</em>, <em>6</em>(7), 1736-1745. (<a href='https://doi.org/10.1109/TAI.2025.3527400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable devices are becoming affordable in our society to provide services from simple fitness tracking to the detection of heartbeat disorders. In the case of elderly populations, these devices have great potential to enable proactive eldercare, which can increase the number of years of independent living. The wearables can capture healthcare data continuously. For meaningful insight, deep learning models are preferable to process this data for robust outcomes. One of the major challenges includes deploying these models on edge devices, such as smartphones and wearables. The bottleneck is a large number of parameters and compute-intensive operations. In this research, we propose a novel knowledge distillation (KD) scheme by introducing a self-revision concept. This scheme effectively reduces model size and transfers knowledge from a deep model to a distilled model by filling learning gaps during the training. To evaluate our distilled model, a publicly available dataset, “growing old together validation (GOTOV)” is utilized, which is based on medical-grade standard wearables to monitor behavioral changes in the elderly. Our proposed model reduces the 0.7 million parameters to 1500, which enables edge intelligence. It achieves a 6% improvement in precision, a 9% increase in recall, and a 9% higher F1-score compared to the shallow model for recognizing elderly behavior.},
  archive      = {J_TAI},
  author       = {Muhammad Fahim and S. M. Ahsan Kazmi and Vishal Sharma and Hyundong Shin and Trung Q. Duong},
  doi          = {10.1109/TAI.2025.3527400},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1736-1745},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Edge intelligence: A deep distilled model for wearables to enable proactive eldercare},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stochastic submodular bandits with delayed composite anonymous bandit feedback. <em>TAI</em>, <em>6</em>(7), 1727-1735. (<a href='https://doi.org/10.1109/TAI.2025.3527375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed feedback is assumed to be composite and anonymous. In other words, the delayed feedback is composed of components of rewards from past actions, with unknown division among the subcomponents. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. Ignoring the problem dependent parameters, we show that regret bound for all the delay models is $\tilde{O}(T^{2/3}+T^{1/3}\nu)$ for time horizon $T$, where $\nu$ is a delay parameter defined differently in the three cases, thus demonstrating an additive term in regret with delay in all the three delay models. The considered algorithm is demonstrated to outperform other full-bandit approaches with delayed composite anonymous feedback. We also demonstrate the generalizability of our analysis of the delayed composite anonymous feedback in combinatorial bandits as long as there exists an algorithm for the offline problem satisfying a certain robustness condition.},
  archive      = {J_TAI},
  author       = {Mohammad Pedramfar and Vaneet Aggarwal},
  doi          = {10.1109/TAI.2025.3527375},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1727-1735},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Stochastic submodular bandits with delayed composite anonymous bandit feedback},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy and fairness in machine learning: A survey. <em>TAI</em>, <em>6</em>(7), 1706-1726. (<a href='https://doi.org/10.1109/TAI.2025.3531326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy and fairness are two crucial pillars of responsible artificial intelligence (AI) and trustworthy machine learning (ML). Each objective has been independently studied in the literature with the aim of reducing utility loss in achieving them. Despite the significant interest attracted from both academia and industry, there remains an immediate demand for more in-depth research to unravel how these two objectives can be simultaneously integrated into ML models. As opposed to well-accepted trade-offs, i.e., privacy-utility and fairness-utility, the interrelation between privacy and fairness is not well-understood. While some works suggest a trade-off between the two objective functions, there are others that demonstrate the alignment of these functions in certain scenarios. To fill this research gap, we provide a thorough review of privacy and fairness in ML, including supervised, unsupervised, semisupervised, and reinforcement learning. After examining and consolidating the literature on both objectives, we present a holistic survey on the impact of privacy on fairness, the impact of fairness on privacy, existing architectures, their interaction in application domains, and algorithms that aim to achieve both objectives while minimizing the utility sacrificed. Finally, we identify research challenges in achieving concurrently privacy and fairness in ML, particularly focusing on large language models.},
  archive      = {J_TAI},
  author       = {Sina Shaham and Arash Hajisafi and Minh K. Quan and Dinh C. Nguyen and Bhaskar Krishnamachari and Charith Peris and Gabriel Ghinita and Cyrus Shahabi and Pubudu N. Pathirana},
  doi          = {10.1109/TAI.2025.3531326},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1706-1726},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Privacy and fairness in machine learning: A survey},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cyber shadows: Neutralizing security threats with AI and targeted policy measures. <em>TAI</em>, <em>6</em>(7), 1697-1705. (<a href='https://doi.org/10.1109/TAI.2025.3527398'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital age, driven by the Artificial Intelligence (AI) revolution, brings significant opportunities but also conceals security threats, which we refer to as cyber shadows. These threats pose risks at individual, organizational, and societal levels. This article examines the systemic impact of these cyber threats and proposes a comprehensive cybersecurity strategy that integrates AI-driven solutions, such as intrusion detection systems (IDS), with targeted policy interventions. By combining technological and regulatory measures, we create a multilevel defense capable of addressing both direct threats and indirect negative externalities. We emphasize that the synergy between AI-driven solutions and policy interventions is essential for neutralizing cyber threats and mitigating their negative impact on the digital economy. Finally, we underscore the need for continuous adaptation of these strategies, especially in response to the rapid advancement of autonomous AI-driven attacks, to ensure the creation of secure and resilient digital ecosystems.},
  archive      = {J_TAI},
  author       = {Marc Schmitt and Pantelis Koutroumpis},
  doi          = {10.1109/TAI.2025.3527398},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {7},
  number       = {7},
  pages        = {1697-1705},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Cyber shadows: Neutralizing security threats with AI and targeted policy measures},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HWEFIS: A hybrid weighted evolving fuzzy inference system for nonstationary data streams. <em>TAI</em>, <em>6</em>(6), 1679-1694. (<a href='https://doi.org/10.1109/TAI.2025.3534755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the problem of concept drift of nonstationary data streams, most evolving fuzzy inference systems (EFISs) still encounter problems. First, a single EFIS has difficulty quickly adjusting its own structure and parameters to adapt itself in an environment with obvious dynamic changes (such as sudden drift). Second, most ensemble EFISs adjust their weights according to errors, which is prone to the risk of model undertraining and repeated training. In this article, a new ensemble EFIS, referred to as a hybrid weighted evolving fuzzy inference system (HWEFIS), is proposed. The HWEFIS uses a detection method based on the edge heterogeneous distance (EHD) to mine similarity information between data distributions after data chunks arrive and uses Dempster–Shafer (DS) evidence theory to combine similarity and error information to generate hybrid weights. In addition, a forgetting factor and penalty mechanism are introduced into each base learner, which increases the ability of the base learner to address nonstationary problems. Experiments are carried out on synthetic datasets and real-world datasets. The experimental results show that the HWEFIS can achieve better performance in nonstationary data streams with complex drift, effectively suppresses the influence of concept drift, and is insensitive to the size of the data chunks.},
  archive      = {J_TAI},
  author       = {Tao Zhao and Haoli Li},
  doi          = {10.1109/TAI.2025.3534755},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1679-1694},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {HWEFIS: A hybrid weighted evolving fuzzy inference system for nonstationary data streams},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COLT: Cyclic overlapping lottery tickets for faster pruning of convolutional neural networks. <em>TAI</em>, <em>6</em>(6), 1664-1678. (<a href='https://doi.org/10.1109/TAI.2025.3534745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pruning refers to the elimination of trivial weights from neural networks. The sub-networks within an overparameterized model produced after pruning are often called lottery tickets. This research aims to generate winning lottery tickets from a set of lottery tickets that can achieve accuracy similar to that of the original unpruned network. We introduce a novel winning ticket called cyclic overlapping lottery ticket (COLT) by data splitting and cyclic retraining of the pruned network from scratch. We apply a cyclic pruning algorithm that keeps only the overlapping weights of different pruned models trained on different data segments. Our results demonstrate that COLT can achieve similar accuracies (obtained by the unpruned model) while maintaining high sparsities. Based on object recognition and detection tasks, we show that the accuracy of COLT is on par with the winning tickets of the lottery ticket hypothesis and, at times, is better. Moreover, COLTs can be generated using fewer iterations than tickets generated by the popular iterative magnitude pruning method. In addition, we also notice that COLTs generated on large datasets can be transferred to small ones without compromising performance, demonstrating its generalizing capability. We conduct all our experiments on Cifar-10, Cifar-100, TinyImageNet, and ImageNet datasets and report superior performance than the state-of-the-art methods. The codes are available at: https://github.com/ismail31416/COLT.},
  archive      = {J_TAI},
  author       = {Md. Ismail Hossain and Mohammed Rakib and M. M. Lutfe Elahi and Nabeel Mohammed and Shafin Rahman},
  doi          = {10.1109/TAI.2025.3534745},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1664-1678},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {COLT: Cyclic overlapping lottery tickets for faster pruning of convolutional neural networks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from mistakes: A multilevel optimization framework. <em>TAI</em>, <em>6</em>(6), 1651-1663. (<a href='https://doi.org/10.1109/TAI.2025.3534151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bi-level optimization methods in machine learning are popularly effective in subdomains of neural architecture search, data reweighting, etc. However, most of these methods do not factor in variations in learning difficulty, which limits their performance in real-world applications. To address the above problems, we propose a framework that imitates the learning process of humans. In human learning, learners usually focus more on the topics where mistakes have been made in the past to deepen their understanding and master the knowledge. Inspired by this effective human learning technique, we propose a multilevel optimization framework, learning from mistakes (LFM), for machine learning. We formulate LFM as a three-stage optimization problem: 1) the learner learns, 2) the learner relearns based on the mistakes made before, and 3) the learner validates his learning. We develop an efficient algorithm to solve the optimization problem. We further apply our method to differentiable neural architecture search and data reweighting. Extensive experiments on CIFAR-10, CIFAR-100, ImageNet, and other related datasets powerfully demonstrate the effectiveness of our approach. The code of LFM is available at: https://github.com/importZL/LFM.},
  archive      = {J_TAI},
  author       = {Li Zhang and Bhanu Garg and Pradyumna Sridhara and Ramtin Hosseini and Pengtao Xie},
  doi          = {10.1109/TAI.2025.3534151},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1651-1663},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning from mistakes: A multilevel optimization framework},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioral decision-making of mobile robots simulating the functions of cerebellum, basal ganglia, and hippocampus. <em>TAI</em>, <em>6</em>(6), 1639-1650. (<a href='https://doi.org/10.1109/TAI.2025.3534150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unknown environments, behavioral decision-making of mobile robots is a crucial research topic in the field of robotics applications. To address the low learning ability and the difficulty of learning from the unknown environments for mobile robots, this work proposes a new learning model that integrates the supervised learning of the cerebellum, reinforcement learning of the basal ganglia, and memory consolidation of the hippocampus. First, to reduce the impact of noise on inputs and enhance the network's efficiency, a multineuron winning strategy and the refinement of the top-$k$ competition mechanism have been adopted. Second, to increase the network's learning speed, a negative learning mechanism has been designed, which allows the robot to avoid obstacles more quickly by weakening the synaptic connections between error neurons. Third, to enhance the decision ability of cerebellar supervised learning, simulating the hippocampal memory consolidation mechanism, memory replay during the agent's offline state enables autonomous learning in the absence of real-time interactions. Finally, to better adjust the roles of cerebellar supervised learning and basal ganglia reinforcement learning in robot behavioral decision-making, a new similarity indicator has been designed. Simulation experiments and real-world experiments validate the effectiveness of the proposed model in this work.},
  archive      = {J_TAI},
  author       = {Dongshu Wang and Qi Liu and Yihai Duan},
  doi          = {10.1109/TAI.2025.3534150},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1639-1650},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Behavioral decision-making of mobile robots simulating the functions of cerebellum, basal ganglia, and hippocampus},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AttDCT: Attention-based deep learning approach for time series classification in the DCT domain. <em>TAI</em>, <em>6</em>(6), 1626-1638. (<a href='https://doi.org/10.1109/TAI.2025.3534141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new deep learning framework for time series classification in the discrete cosine transform (DCT) domain with spectral enhancement and self-attention mechanisms. The time series signal is first partitioned into discrete segments. Each segment is rearranged into a matrix using a sliding window. The signal matrix is then transformed to spectral coefficients using a two-dimensional (2-D) DCT. This is followed by logarithmic contrast enhancement and spectral normalization to enhance the DCT coefficients. The resulting enhanced coefficient matrix serves as input to a deep neural network architecture comprising a self-attention layer, a multilayer convolutional neural network (CNN), and a fully connected multilayer perceptron (MLP) for classification. The AttDCT CNN model is evaluated and benchmarked on 13 different time series classification problems. The experimental results show that the proposed model outperforms state-of-the-art deep learning methods by an average of 2.1% in classification accuracy. It achieves higher classification accuracy on ten of the problems and similar results on the remaining three.},
  archive      = {J_TAI},
  author       = {Amine Haboub and Hamza Baali and Abdesselam Bouzerdoum},
  doi          = {10.1109/TAI.2025.3534141},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1626-1638},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {AttDCT: Attention-based deep learning approach for time series classification in the DCT domain},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpikeNAS-bench: Benchmarking NAS algorithms for spiking neural network architecture. <em>TAI</em>, <em>6</em>(6), 1614-1625. (<a href='https://doi.org/10.1109/TAI.2025.3534136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, neural architecture search (NAS) has marked significant advancements, yet its efficacy is marred by the dependence on substantial computational resources. To mitigate this, the development of NAS benchmarks has emerged, offering datasets that enumerate all potential network architectures and their performances within a predefined search space. Nonetheless, these benchmarks predominantly focus on convolutional architectures, which are criticized for their limited interpretability and suboptimal hardware efficiency. Recognizing the untapped potential of spiking neural networks (SNNs)—often hailed as the third generation of neural networks due to their biological realism and computational thrift—this study introduces SpikeNAS-Bench. As a pioneering benchmark for SNN, SpikeNAS-Bench utilizes a cell-based search space, integrating leaky integrate-and-fire neurons with variable thresholds as candidate operations. It encompasses 15 625 candidate architectures, rigorously evaluated on CIFAR10, CIFAR100, and Tiny-ImageNet datasets. This article delves into the architectural nuances of SpikeNAS-Bench, leveraging various criteria to underscore the benchmark's utility and presenting insights that could steer future NAS algorithm designs. Moreover, we assess the benchmark's consistency through three distinct proxy types: zero-cost-based, early-stop-based, and predictor-based proxies. Additionally, the article benchmarks seven contemporary NAS algorithms to attest to SpikeNAS-Bench's broad applicability. We commit to providing training logs, diagnostic data for all candidate architectures, and we promise to release all code and datasets postacceptance, aiming to catalyze further exploration and innovation within the SNN domain.},
  archive      = {J_TAI},
  author       = {Gengchen Sun and Zhengkun Liu and Lin Gan and Hang Su and Ting Li and Wenfeng Zhao and Biao Sun},
  doi          = {10.1109/TAI.2025.3534136},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1614-1625},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {SpikeNAS-bench: Benchmarking NAS algorithms for spiking neural network architecture},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). T-SNVAE: Deep probabilistic learning with local and global structures for industrial process monitoring. <em>TAI</em>, <em>6</em>(6), 1603-1613. (<a href='https://doi.org/10.1109/TAI.2025.3533438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoder (VAE) is a generative deep learning (DL) model with a probabilistic structure, which makes it tolerant to process uncertainties and more suitable for process monitoring. However, the probabilistic model may disrupt the topological structure of data and lead to the loss of neighborhood information. To address this issue, a process monitoring approach based on t-distributed stochastic neighbor variational autoencoder (t-SNVAE) is proposed to capture probabilistic features that elucidate both local and global structures within the raw data. Specifically, the distances between neighboring data points are transformed into joint probabilities by using t-SN embedding. Through minimizing the Kullback–Leibler divergence of joint probabilities between the original data and the reconstructed data, VAE learns Gaussian features containing both local and global neighborhood information. Finally, monitoring statistics are constructed for monitoring. The efficiency of the proposed approach is verified on a multiphase flow facility and a waste-water treatment process.},
  archive      = {J_TAI},
  author       = {Jian Huang and Zizhuo Liu and Xu Yang and Yupeng Liu and Zhaomin Lv and Kaixiang Peng and Okan K. Ersoy},
  doi          = {10.1109/TAI.2025.3533438},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1603-1613},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {T-SNVAE: Deep probabilistic learning with local and global structures for industrial process monitoring},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network output-feedback distributed formation control for NMASs under communication delays and switching network. <em>TAI</em>, <em>6</em>(6), 1591-1602. (<a href='https://doi.org/10.1109/TAI.2025.3527404'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the neural network (NN) output-feedback distributed formation control problem of nonlinear multiagent systems (NMASs) under communication delays and jointly connected switching network. Since the communication between agents is affected by time-varying delay and some agents cannot access the leader's information under jointly connected switching network, a communication-delay-related distributed formation observer is designed to estimate the leader's information and simultaneously mitigate the effects of communication delays. NNs are adopted to identify unknown functions, and an NN state observer is established to reconstruct unmeasurable states. Then, based on the designed distributed formation observer and NN state observer, an NN output-feedback distributed formation control algorithm is proposed by the backstepping control theory. It is proven that the designed communication-delay-related distributed formation observer errors converge to zero exponentially. Meanwhile, the proposed distributed NN formation control approach ensures the NMAS is stable, and the formation tracking errors converge to a small neighborhood around zero. Finally, we apply the output-feedback distributed formation control scheme to unmanned surface vehicles (USVs), the simulation results verify its effectiveness.},
  archive      = {J_TAI},
  author       = {Haodong Zhou and Shaocheng Tong},
  doi          = {10.1109/TAI.2025.3527404},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1591-1602},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Neural network output-feedback distributed formation control for NMASs under communication delays and switching network},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CVDLLM: Automated cardiovascular disease diagnosis with large-language-model-assisted graph attentive feature interaction. <em>TAI</em>, <em>6</em>(6), 1575-1590. (<a href='https://doi.org/10.1109/TAI.2025.3527401'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiogram (ECG) measurements are essential for detecting and treating cardiovascular disease (CVD). However, manual evaluation of ECGs is prone to errors due to morphological variations. Although machine learning methods have shown promise in diagnosing diseases, automatic CVD diagnosis based on ECGs is still suffering from low diagnosis accuracy due to the limited usage of time-series information and interlead correlations. In this article, we propose a large language model (LLM)-assisted graph attentive feature interaction learning framework (CVDLLM) for automatic ECG diagnosis. It utilizes ECG data from twelve leads to classify eight heart diseases, including rhythm abnormalities and normal conditions. Our framework combines convolutional and recurrent neural networks for independent time-series feature extraction from 12-lead ECG signals. By incorporating features extracted by heart rate variability (HRV) analysis, we employ graph attention neural networks (GAT) and self-attentive feature interaction mechanism (GSAT) for feature interaction and model learning. Leveraging LLMs with pretrained knowledge bases and advanced language comprehension, we extract and learn semantic embeddings from medical case data. This approach equips our framework with a deep semantic layer, significantly enhancing its capacity to understand complex medical texts. Additionally, by representing the twelve leads as a graph, our framework enables highly accurate disease diagnosis based on spatial and temporal interactions with 12-lead ECG signals. We evaluate the performance of our proposed framework and our framework achieves state-of-the-art performance with accuracy, precision, recall, and F1-score.},
  archive      = {J_TAI},
  author       = {Xihe Qiu and Haoyu Wang and Xiaoyu Tan and Yaochu Jin},
  doi          = {10.1109/TAI.2025.3527401},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1575-1590},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CVDLLM: Automated cardiovascular disease diagnosis with large-language-model-assisted graph attentive feature interaction},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging AI to compromise IoT device privacy by exploiting hardware imperfections. <em>TAI</em>, <em>6</em>(6), 1561-1574. (<a href='https://doi.org/10.1109/TAI.2025.3526139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The constrained design, remote deployment, and sensitive data generated by Internet of Things (IoT) devices make them susceptible to various cyberattacks. One such attack is profiling IoT devices by tracking their packet transmissions. While existing methods mitigate these attacks using pseudonymous identities, we propose a novel attack strategy that exploits the physical layer characteristics of IoT devices. Specifically, we demonstrate how an attacker can leverage features extracted from device transmissions to identify packets originating from the same device. Once identified, the attacker can isolate the device's signals and potentially determine its physical location. This attack exploits the fact that microcontroller clock variations exist across devices, even within the same model line. By extracting transmission features and training machine learning (ML) models, we accurately identify the originating device of the packets. This study reveals inherent privacy vulnerabilities in IoT systems due to hardware imperfections that are beyond user control. These limitations have profound implications for the design of security frameworks in emerging ubiquitous sensing environments. Our experiments demonstrate that the proposed attack achieves 99% accuracy in real-world settings and can bypass privacy measures implemented at higher protocol layers. This work highlights the urgent need for privacy protection strategies across multiple layers of the IoT protocol stack.},
  archive      = {J_TAI},
  author       = {Mirza Athar Baig and Asif Iqbal and Muhammad Naveed Aman and Biplab Sikdar},
  doi          = {10.1109/TAI.2025.3526139},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1561-1574},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Leveraging AI to compromise IoT device privacy by exploiting hardware imperfections},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From global to hybrid: A review of supervised deep learning for 2-D image feature representation. <em>TAI</em>, <em>6</em>(6), 1540-1560. (<a href='https://doi.org/10.1109/TAI.2025.3526138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer vision is the science that aims to enable computers to emulate human visual perception, and it encompasses various techniques and methods for extracting and interpreting information from two-dimensional images. Supervised deep 2-D image feature representation is a fundamental problem in computer vision that applies deep learning techniques to extract and process information from a given 2-D image under supervised settings. The goal is to obtain a feature vector that can be utilized for various downstream computer vision applications. The quality of supervised deep 2-D image feature representation algorithms directly affects the performance of downstream applications. However, most of the existing vision research only explores supervised deep 2-D image feature representation for specific subtasks. Therefore, a comprehensive discussion on this topic is needed. In this article, we propose a taxonomy of supervised deep 2-D image feature representation methods based on four categories: global representation, region representation, hash representation, and hybrid representation, and we introduce their typical approaches. Furthermore, we perform a comparative analysis of the representative methods on three fundamental tasks: image classification, object detection, and semantic segmentation, as well as other common tasks. We also discuss the limitations of supervised deep 2-D image feature representation and investigate future directions in image representation to facilitate the advancement of computer vision through image representation.},
  archive      = {J_TAI},
  author       = {Xinyu Dong and Qi Wang and Hongyu Deng and Zhenguo Yang and Weijian Ruan and Wu Liu and Liang Lei and Xue Wu and Youliang Tian},
  doi          = {10.1109/TAI.2025.3526138},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1540-1560},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {From global to hybrid: A review of supervised deep learning for 2-D image feature representation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A spatial-transformation-based causality-enhanced model for glioblastoma progression diagnosis. <em>TAI</em>, <em>6</em>(6), 1529-1539. (<a href='https://doi.org/10.1109/TAI.2025.3526137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differentiation between pseudoprogression and true tumor progression of glioblastoma (GBM) is crucial for choosing appropriate management strategies and increasing the chances of patient survival. Currently, there is a lack of noninvasive and effective methods in clinic for GBM progression diagnosis. Here, we propose an automated early diagnosis method based on diffusion tensor imaging (DTI) with a high potential for this diagnosis. A primary challenge for intelligent diagnostic methods lies in the limited accuracy and stability caused by data insufficiency and the fine-grained nature of diagnostic tasks. To address this challenge, we develop a spatial-transformation-based causality-enhanced model (ST-CEM). This model jointly improves data diversity and the effective utilization of clinically significant discriminative information. Specifically, first, a texture diverse augmentation scheme is designed based on a spatial transformation, which allows for greater texture diversification in the augmented data. Subsequently, an interference information contrastive strategy is developed, where nonlesion features that may introduce interference are actively extracted and decoupled with lesion features. Finally, a causality-enhanced mechanism is introduced to highlight the decoupled lesion features, thereby improving the diagnostic stability of the model. Extensive experiments verified the effectiveness of our model in diagnosis of GBM progression under small-sample conditions. The proposed model achieved an accuracy of 84.1%, precision of 85.8%, and recall of 90.3%, all of which outperform the existing works. Moreover, it demonstrated competitive performance on an additional lung nodule classification dataset.},
  archive      = {J_TAI},
  author       = {Qiang Li and Xinyue Li and Hong Jiang and Xiaohua Qian},
  doi          = {10.1109/TAI.2025.3526137},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1529-1539},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A spatial-transformation-based causality-enhanced model for glioblastoma progression diagnosis},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from heterogeneity: A dynamic learning framework for hypergraphs. <em>TAI</em>, <em>6</em>(6), 1513-1528. (<a href='https://doi.org/10.1109/TAI.2024.3524984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural network (GNN) has gained increasing popularity in recent years owing to its capability and flexibility in modeling complex graph structure data. Among all graph learning methods, hypergraph learning is a technique for exploring the implicit higher-order correlations when training the embedding space of the graph. In this article, we propose a hypergraph learning framework named learning from heterogeneity (LFH) that is capable of dynamic hyperedge construction and attentive embedding update utilizing the heterogeneity attributes of the graph. Specifically, in our framework, the high-quality features are first generated by the pairwise fusion strategy that utilizes explicit graph structure information when generating initial node embedding. Afterward, a hypergraph is constructed through the dynamic grouping of implicit hyperedges, followed by the type-specific hypergraph learning process. To evaluate the effectiveness of our proposed framework, we conduct comprehensive experiments on several popular datasets with twelve state-of-the-art models on both node classification and link prediction tasks, which fall into categories of homogeneous pairwise graph learning, heterogeneous pairwise graph learning, and hypergraph learning. The experimental results demonstrate a significant performance gain (an average of 12.9% in node classification and 12.8% in link prediction) compared with recent state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Tiehua Zhang and Yuze Liu and Zhishu Shen and Xingjun Ma and Peng Qi and Zhijun Ding and Jiong Jin},
  doi          = {10.1109/TAI.2024.3524984},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1513-1528},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning from heterogeneity: A dynamic learning framework for hypergraphs},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamically scaled temperature in self-supervised contrastive learning. <em>TAI</em>, <em>6</em>(6), 1502-1512. (<a href='https://doi.org/10.1109/TAI.2024.3524979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary self-supervised contrastive algorithms such as SimCLR and MoCo, the task of balancing attraction between two semantically similar samples and repulsion between two samples of different classes is primarily affected by the presence of hard negative samples. While the InfoNCE loss has been shown to impose penalties based on hardness, the temperature hyperparameter is the key to regulate the penalties and the tradeoff between uniformity and tolerance. In this work, we focus our attention on improving the performance of InfoNCE loss in self-supervised learning by proposing a novel cosine similarity dependent temperature scaling function to effectively optimize the distribution of the samples in the feature space. We also provide mathematical analyzes to support the construction of such a dynamically scaled temperature function. Experimental evidence shows that the proposed framework outperforms the contrastive loss-based SSL algorithms.},
  archive      = {J_TAI},
  author       = {Siladittya Manna and Soumitri Chattopadhyay and Rakesh Dey and Umapada Pal and Saumik Bhattacharya},
  doi          = {10.1109/TAI.2024.3524979},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1502-1512},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Dynamically scaled temperature in self-supervised contrastive learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated multiarmed bandits under byzantine attacks. <em>TAI</em>, <em>6</em>(6), 1488-1501. (<a href='https://doi.org/10.1109/TAI.2024.3524954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiarmed bandits (MAB) is a sequential decision-making model in which the learner controls the trade-off between exploration and exploitation to maximize its cumulative reward. Federated multiarmed bandits (FMAB) is an emerging framework where a cohort of learners with heterogeneous local models play an MAB game and communicate their aggregated feedback to a server to learn a globally optimal arm. Two key hurdles in FMAB are communication-efficient learning and resilience to adversarial attacks. To address these issues, we study the FMAB problem in the presence of Byzantine clients who can send false model updates threatening the learning process. We analyze the sample complexity and the regret of $\beta$-optimal arm identification. We borrow tools from robust statistics and propose a median-of-means (MoM)-based online algorithm, Fed-MoM-UCB, to cope with Byzantine clients. In particular, we show that if the Byzantine clients constitute less than half of the cohort, the cumulative regret with respect to $\beta$-optimal arms is bounded over time with high probability, showcasing both communication efficiency and Byzantine resilience. We analyze the interplay between the algorithm parameters, a discernibility margin, regret, communication cost, and the arms’ suboptimality gaps. We demonstrate Fed-MoM-UCB's effectiveness against the baselines in the presence of Byzantine attacks via experiments.},
  archive      = {J_TAI},
  author       = {Artun Saday and İlker Demirel and Yiğit Yıldırım and Cem Tekin},
  doi          = {10.1109/TAI.2024.3524954},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1488-1501},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Federated multiarmed bandits under byzantine attacks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MRI joint superresolution and denoising based on conditional stochastic normalizing flow. <em>TAI</em>, <em>6</em>(6), 1472-1487. (<a href='https://doi.org/10.1109/TAI.2024.3515936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) is often limited by noise and low-resolution (LR), which can impact the precision of the diagnosis and treatment of patients. LR images and mixed noise (e.g., Gaussian noise, Rician noise, and Impulse noise) are inherent in MR images, and current approaches typically address image superresolution (SR) reconstruction and denoising separately, resulting in a discrepancy between the actual MRI data distribution and the reconstructed images. This research introduces a new algorithm SRDSNF, the stochastic normalizing flow-based MR image SR and denoising model, which tackles SR and denoising simultaneously through a stochastic normalizing flow. Our method integrates the encoded information of the input image as a conditional variable in each reverse step of the stochastic normalizing flow, ensuring a consistent representation of the spatial distribution between the reconstructed image and the original data. Additionally, we incorporate range-null space decomposition and subsequence sampling techniques to increase the consistency between the original and constructed data and accelerate model generation. To assess the efficacy of our approach, we conducted experiments on the BrainWeb and NFBS datasets, which include simultaneous SR and denoising, standalone denoising, and standalone SR tasks. The results of the experiments illustrate that our method achieves superior SR and denoising performance with fewer sampling steps, closely approximating the ground truths. Furthermore, our results surpass those of existing methods in various tasks, showing improvement of up to 4.64 dB in PSNR and 13.8% in SSIM achieved by our SRDSNF model was contextualized against state-of-the-art approaches such as AMIR, SwinIR, and InstructIR. These methods typically report PSNR improvements ranging from 0.5 to 2 dB and SSIM increases of 3%–6%, underscoring the potential clinical value of our methodology.},
  archive      = {J_TAI},
  author       = {Zhenhong Liu and Xingce Wang and Zhongke Wu and Xiaodong Ju and YiCheng Zhu and Alejandro F. Frangi},
  doi          = {10.1109/TAI.2024.3515936},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1472-1487},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {MRI joint superresolution and denoising based on conditional stochastic normalizing flow},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised action recognition using spatiotemporal, adaptive, and attention-guided refining-network. <em>TAI</em>, <em>6</em>(6), 1460-1471. (<a href='https://doi.org/10.1109/TAI.2024.3430257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous articles on unsupervised skeleton-based action recognition primarily focused on strategies for utilizing features to drive model optimization through methods like contrastive learning and reconstruction. However, designing application-level strategies poses challenges. This article shifts the focus to the generation-level modelings and introduces the spatiotemporal adaptively attentions-guided refining network (AgRNet). AgRNet approaches the reduction of costs and enhancement of efficiency by constructing the adaptive activity-guided attention (AAGA) and adaptive dominant-guided attenuation (ADGA) modules. The AAGA leverages the sparsity of the correlation matrix in the attention mechanism to adaptively filter and retain the active components of the sequence during the modeling process. The ADGA embeds the local dominant features of the sequence, obtained through convolutional distillation, into the globally dominant features under the attention mechanism, guided by the defined attenuation factor. Additionally, the progressive feature modeling (PFM) module is introduced to complement the progressive features in motion sequences that were overlooked by AAGA and ADGA. AgRNet shows efficiency on three public datasets, NTU-RGBD 60, NTU-RGBD 120, and UWA3D.},
  archive      = {J_TAI},
  author       = {Xinpeng Yin and Cheng Zhang and ZiXu Huang and Zhihai He and Wenming Cao},
  doi          = {10.1109/TAI.2024.3430257},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1460-1471},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unsupervised action recognition using spatiotemporal, adaptive, and attention-guided refining-network},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GLAC-GCN: Global and local topology-aware contrastive graph clustering network. <em>TAI</em>, <em>6</em>(6), 1448-1459. (<a href='https://doi.org/10.1109/TAI.2024.3413694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though many deep attributed graph clustering approaches have been developed in recent years, most still suffer from two limitations. First, in the input space, they primarily rely on the original topology structure as the input (to some graph network), lacking the ability to jointly leverage local and global topology information to refine the graph. Second, in the learning process, they usually employ a single graph learning pipeline (with a single input graph), overlooking the opportunities in the joint optimization of multiple graph learning pipelines (with multiple topology structures). In view of this, this article presents a global and local topology-aware contrastive graph clustering network (GLAC-GCN) for attributed graph clustering. Specifically, the local topology structure, and global semantic information are simultaneously utilized to refine the graph. Then a paralleled graph convolutional network (GCN) learning mechanism is designed, where i) both the original graph and the globally and locally refined graph are treated as input graphs, and ii) two pipelines of GCNs are jointly and interactively utilized. Furthermore, a self-adaptive learning mechanism is devised to ensure consistency between multiple learning pipelines via the Kullback–Leibler (KL)-divergence. Meanwhile, the contrastive learning is enforced by minimizing the mismatch of the cluster distributions obtained from different GCN pipelines. Extensive experiments are conducted on seven real-world datasets. Notably, GLAC-GCN achieves the best ACC (or NMI) scores on all (or five) of the seven datasets, demonstrating its superiority over the state-of-the-art approaches. Code available: https://github.com/xuyuankun631/GLAC-GCN.},
  archive      = {J_TAI},
  author       = {Yuan-Kun Xu and Dong Huang and Chang-Dong Wang and Jian-Huang Lai},
  doi          = {10.1109/TAI.2024.3413694},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {6},
  number       = {6},
  pages        = {1448-1459},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {GLAC-GCN: Global and local topology-aware contrastive graph clustering network},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the role of priors in bayesian causal learning. <em>TAI</em>, <em>6</em>(5), 1439-1445. (<a href='https://doi.org/10.1109/TAI.2024.3522867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we investigate causal learning of independent causal mechanisms (ICMs) from a Bayesian perspective. Confirming previous claims from the literature, we show in a didactically accessible manner that unlabeled data (i.e., cause realizations) do not improve the estimation of the parameters defining the mechanism. Furthermore, we observe the importance of choosing an appropriate prior for the cause and mechanism parameters, respectively. Specifically, we show that a factorized prior results in a factorized posterior, which resonates with Janzing and Schölkopf's definition of ICMs via the Kolmogorov complexity of the involved distributions and with the concept of parameter independence of Heckerman et al.},
  archive      = {J_TAI},
  author       = {Bernhard C. Geiger and Roman Kern},
  doi          = {10.1109/TAI.2024.3522867},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1439-1445},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {On the role of priors in bayesian causal learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hierarchical cross-modal spatial fusion network for multimodal emotion recognition. <em>TAI</em>, <em>6</em>(5), 1429-1438. (<a href='https://doi.org/10.1109/TAI.2024.3523250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in emotion recognition research based on physiological data have been notable. However, existing multimodal methods often overlook the interrelations between various modalities, such as video and electroencephalography (EEG) data, in emotion recognition. In this article, a feature fusion-based hierarchical cross-modal spatial fusion network (HCSFNet) is proposed that effectively integrates EEG and video features. By designing an EEG feature extraction network based on 1-D convolution and a video feature extraction network based on 3-D convolution, corresponding modality features are thoroughly extracted. To promote sufficient interaction between the two modalities, a hierarchical cross-modal coordinated attention module is proposed in this article. Additionally, to enhance the network's perceptual ability for emotion-related features, a multiscale spatial pyramid pooling module is also designed. Meanwhile, a self-distillation method is introduced, which enhances the performance while reducing the number of parameters in the network. The HCSFNet achieved an accuracy of 97.78% on the valence–arousal dimension of the Database for Emotion Analysis using Physiological Signals (DEAP) dataset, and it also obtained an accuracy of 60.59% on the MAHNOB-human-computer interaction (HCI) dataset, reaching the state-of-the-art level.},
  archive      = {J_TAI},
  author       = {Ming Xu and Tuo Shi and Hao Zhang and Zeyi Liu and Xiao He},
  doi          = {10.1109/TAI.2024.3523250},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1429-1438},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A hierarchical cross-modal spatial fusion network for multimodal emotion recognition},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep temporally recursive differencing network for anomaly detection in videos. <em>TAI</em>, <em>6</em>(5), 1414-1428. (<a href='https://doi.org/10.1109/TAI.2024.3521877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent video surveillance systems with anomaly detection capabilities are indispensable for outdoor security. Video anomaly detection (VAD) is usually performed by learning patterns representing normal events and declaring an anomaly when an abnormal pattern is encountered. However, the features of normal patterns in a video often vary with time as real-world videos are non-stationary in nature, which makes its handling essential during VAD. To this end, we propose an approach for anomaly detection in videos, where a novel deep temporally recursive differencing network (DDN) diminishes the adverse effects of the non-stationary nature on VAD. The DDN consists of multiple layers of differencing operators of optimized orders, where every two consecutive layers are separated by a suitable nonlinearity. Spatial and temporal features are extracted from nonoverlapping blocks in video frames and fed to the DDN. While the spatial feature is obtained using a pretrained network, our temporal feature computation involves the use of FlowNetS with a new training strategy that does not require ground truth. The features at the output of DDN are used in a predictor based on autoregression and moving average of the regression errors. Then, the predictor's output estimates are compared to the corresponding actual values for anomaly detection, which also involves block-level selection and consistency check. Qualitative evaluation and quantitative comparison with several existing approaches on multiple standard datasets demonstrate the effectiveness of the proposed VAD approach. An ablation study highlighting the significance of the various components of our approach and a hyperparameter analysis are also provided.},
  archive      = {J_TAI},
  author       = {Gargi V. Pillai and Debashis Sen},
  doi          = {10.1109/TAI.2024.3521877},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1414-1428},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep temporally recursive differencing network for anomaly detection in videos},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient hybrid impulsive model for joint classification and segmentation on CT images. <em>TAI</em>, <em>6</em>(5), 1401-1413. (<a href='https://doi.org/10.1109/TAI.2024.3517570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Highly flexible foundation models like artificial neural networks are imperative in medical practice, enabling diverse tasks with little or no task-specific labeled data. The crucial problem remains as how to link latent features and a priori knowledge within multitask decision outputs, particularly in joint classification and segmentation tasks on images. This article develops a hybrid encoder-decoding model substantiating hybrid computations of continuous convolution variables and discrete nerve impulses, where impulsive neurons are adopted to boost nonlinear activations. By presenting a flexible network architecture with regularized multiloss training, this hybrid model can learn shared features of classification and segmentation. The joint decoder does not only provide classification results, but also predicts intelligible task-specific outputs from input images. Applied to the COVID-19 lung CT and the Synapse multiorgan CT datasets, experimental results and ablation studies demonstrate the effectiveness and flexibility of this hybrid model, which outperforms convolution models and human experts. Comparative studies further highlight the high energy-efficient attribute and the decision-output visibility of the hybrid impulsive model, indicating a potential for edge healthcare and biomedical applications.},
  archive      = {J_TAI},
  author       = {Bin Hu and Zhi-Hong Guan and Guanrong Chen and Jürgen Kurths},
  doi          = {10.1109/TAI.2024.3517570},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1401-1413},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Energy-efficient hybrid impulsive model for joint classification and segmentation on CT images},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monocular 3-D reconstruction of blast furnace burden surface based on cross-domain generative self-supervised network. <em>TAI</em>, <em>6</em>(5), 1386-1400. (<a href='https://doi.org/10.1109/TAI.2024.3511515'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate acquisition of the 3-D topography of the blast furnace (BF) burden surface is crucial for optimizing the ironmaking process. However, traditional methods struggle in the high temperature, high pressure, and dusty environment of the BF top, and even advanced industrial endoscopes only capture monocular images, limiting multiview stereoscopic reconstruction. To address these challenges, we propose a novel 3-D reconstruction framework featuring a virtual–real multiview endoscope array for capturing multiview images and a cross-domain point cloud generation self-supervised network (XGSN). The XGSN leverages a progressive multimodal self-attention mechanism and ray-tracing projection to compensate for the lack of 3-D labels, producing a high-fidelity 3-D point cloud. Experimental results show that the proposed method achieves a significant improvement in burden surface reconstruction accuracy, delivering high-quality 3-D mapping with enhanced real-time processing capabilities, demonstrating its potential for challenging industrial environments.},
  archive      = {J_TAI},
  author       = {Zhipeng Chen and Xinyi Wang and Ling Shen and Jinshi Liu and Jianjun He and Jilin Zhu and Weihua Gui},
  doi          = {10.1109/TAI.2024.3511515},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1386-1400},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Monocular 3-D reconstruction of blast furnace burden surface based on cross-domain generative self-supervised network},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt customization for continual learning. <em>TAI</em>, <em>6</em>(5), 1373-1385. (<a href='https://doi.org/10.1109/TAI.2024.3524977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contemporary continual learning approaches typically select prompts from a pool, which function as supplementary inputs to a pretrained model. However, this strategy is hindered by the inherent noise of its selection approach when handling increasing tasks. In response to these challenges, we reformulate the prompting approach for continual learning and propose the prompt customization (PC) method. PC mainly comprises a prompt generation module (PGM) and a prompt modulation module (PMM). In contrast to conventional methods that employ hard prompt selection, PGM assigns different coefficients to prompts from a fixed-sized pool of prompts and generates tailored prompts. Moreover, PMM further modulates the prompts by adaptively assigning weights according to the correlations between input data and corresponding prompts. We evaluate our method on four benchmark datasets for three diverse settings, including the class, domain, and task-agnostic incremental learning tasks. Experimental results demonstrate consistent improvement (by up to 16.2%), yielded by the proposed method, over the state-of-the-art (SOTA) techniques. The code has been released online.},
  archive      = {J_TAI},
  author       = {Yong Dai and Xiaopeng Hong and Yabin Wang and Zhiheng Ma and Dongmei Jiang and Yaowei Wang},
  doi          = {10.1109/TAI.2024.3524977},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1373-1385},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Prompt customization for continual learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly correlated multimodal domain adaptation for pattern classification. <em>TAI</em>, <em>6</em>(5), 1360-1372. (<a href='https://doi.org/10.1109/TAI.2024.3524976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal domain adaptation (MMDA) aims to transfer knowledge across different domains that contain multimodal data. Current methods typically assume that both the source and target domains have paired multimodal data with the same modalities, allowing for direct knowledge transfer between corresponding types of data. However, in certain applications, the source domain benefits from advanced sensors and equipment, capturing more modalities than those available in the target domain. As a result, the information from the source modalities may not strongly align with that of the target modalities. This weak correlation hinders the effective utilization of all source data for the target domain. To address this challenge, we propose a weakly correlated multimodal domain adaptation (WCMMDA) method for pattern classification. WCMMDA is designed to acquire the modality-independent and category-related knowledge from the source domain, enabling the full utilization of available source modalities for effective knowledge transfer. Specifically, modality-invariant features are first extracted from the multimodal data to bridge the heterogeneity gap within each domain. Subsequently, domain-invariant features are further learned from these modality-invariant features to align the feature distributions across the source and target domains. A source-specific classifier is employed here, which predicts pseudo-labels for the target data and enables the feature extractor to explore category-related information in source features. Finally, a target-specific classifier is trained using the pseudolabeled target data, where highly reliable pseudolabels are selected based on confidence to improve classification performance. Extensive experiments are performed on the real-world multimodal datasets to demonstrate the superiority of WCMMDA.},
  archive      = {J_TAI},
  author       = {Shuyue Wang and Zhunga Liu and Zuowei Zhang and Mohammed Bennamoun},
  doi          = {10.1109/TAI.2024.3524976},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1360-1372},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Weakly correlated multimodal domain adaptation for pattern classification},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual–Semantic fuzzy interaction network for zero-shot learning. <em>TAI</em>, <em>6</em>(5), 1345-1359. (<a href='https://doi.org/10.1109/TAI.2024.3524955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize unseen class image objects using manually defined semantic knowledge corresponding to both seen and unseen images. The key of ZSL lies in building the interaction between precise image data and fuzzy semantic knowledge. The fuzziness is attributed to the difficulty in quantifying human knowledge. However, the existing ZSL methods ignore the inherent fuzziness of semantic knowledge and treat it as precise data during building the visual–semantic interaction. This is not good for transferring semantic knowledge from seen classes to unseen classes. In order to solve this problem, we propose a visual–semantic fuzzy interaction network (VSFIN) for ZSL. VSFIN utilize an effective encoder–decoder structure, including a semantic prototype encoder (SPE) and visual feature decoder (VFD). The SPE and VFD enable the visual features to interact with semantic knowledge via cross-attention. To achieve visual–semantic fuzzy interaction in SPE and VFD, we introduce the concept of membership function in fuzzy set theory and design a membership loss function. This loss function allows for a certain degree of imprecision in visual–semantic interaction, thereby enabling VSFIN to becomingly utilize the given semantic knowledge. Moreover, we introduce the concept of rank sum test and propose a distribution alignment loss to alleviate the bias towards seen classes. Extensive experiments on three widely used benchmarks have demonstrated that VSFIN outperforms current state-of-the-art methods under both conventional ZSL (CZSL) and generalized ZSL (GZSL) settings.},
  archive      = {J_TAI},
  author       = {Xuemeng Hui and Zhunga Liu and Jiaxiang Liu and Zuowei Zhang and Longfei Wang},
  doi          = {10.1109/TAI.2024.3524955},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1345-1359},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Visual–Semantic fuzzy interaction network for zero-shot learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A stratified seed selection algorithm for $K$-means clustering on big data. <em>TAI</em>, <em>6</em>(5), 1334-1344. (<a href='https://doi.org/10.1109/TAI.2024.3524370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In $k$-means clustering, the selection of initial seeds significantly influences the quality of the resulting clusters. Moreover, clustering large-sized data introduces an additional challenge for seed selection. We propose a novel and scalable seed selection approach by jointly modeling the quality and diversity of the potential seeds through a principled probabilistic stochastic point process. To this end, we also propose a novel seed quality estimation approach on large data. Our approach quantifies the quality of a seed by measuring the divergence between the distribution of similarity between the closest neighbors and that of the randomly chosen neighbors from exhaustive stratified batches of samples. Unlike many existing scalable approaches, we do not rely on a small sample of the original data; instead, we use the entire data, thereby minimizing the chance of leaving out information about a potentially high-quality seed. The extensive evaluation on a set of benchmark data shows that it outperforms a number of strong, well-known, and recent algorithms measured by three standard metrics.},
  archive      = {J_TAI},
  author       = {Namita Bajpai and Jiaul H. Paik and Sudeshna Sarkar},
  doi          = {10.1109/TAI.2024.3524370},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1334-1344},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A stratified seed selection algorithm for $K$-means clustering on big data},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting LARS for large batch training generalization of neural networks. <em>TAI</em>, <em>6</em>(5), 1321-1333. (<a href='https://doi.org/10.1109/TAI.2024.3523252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates large batch training techniques using layer-wise adaptive scaling ratio (LARS) across diverse settings. In particular, we first show that a state-of-the-art technique, called LARS with the warm-up, tends to be trapped in sharp minimizers early on due to redundant ratio scaling. Additionally, a fixed steep decline in the latter phase restricts deep neural networks from effectively navigating early-phase sharp minimizers. To address these issues, we propose time varying LARS (TVLARS), a novel algorithm that replaces warm-up with a configurable sigmoid-like function for robust training in the initial phase. TVLARS promotes gradient exploration early on, surpassing sharp optimizers and gradually transitioning to LARS for robustness in later stages. Extensive experiments demonstrate that TVLARS consistently outperforms LARS and LAMB in most cases, with up to 2% improvement in classification scenarios. In all self-supervised learning cases, TVLARS achieves up to 10% performance improvement. Our implementation is available at https://github.com/KhoiDOO/tvlars.},
  archive      = {J_TAI},
  author       = {Khoi Do and Minh-Duong Nguyen and Nguyen Tien Hoa and Long Tran-Thanh and Nguyen H. Tran and Quoc-Viet Pham},
  doi          = {10.1109/TAI.2024.3523252},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1321-1333},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Revisiting LARS for large batch training generalization of neural networks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seeking secure synchronous tracking of networked agent systems subject to antagonistic interactions and denial-of-service attacks. <em>TAI</em>, <em>6</em>(5), 1309-1320. (<a href='https://doi.org/10.1109/TAI.2024.3522873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by the group phenomenon of biological populations in nature, swarm intelligence has been derived and has further advanced the research of coordinated control of networked agent systems (NASs). With this in mind, this article delves into the problem of secure synchronous tracking control for high-order NASs subject to antagonistic interactions, particularly under the threat of denial-of-service (DoS) attacks. First, a novel distributed secure control scheme is crafted to address the complex dynamics of NASs that encompass both cooperative and antagonistic interactions among agents. This scheme is pivotal as it enables follower agents to synchronize their tracking with the leader agent, even amidst the disruptive influence of DoS attacks, transcending the conventional bipartite tracking consensus approach. Subsequently, a dynamic, time-varying closed-loop system is generated, which is intrinsically linked to the intermittent nature of DoS attacks, characterized by periods of dormancy and activity. Based on the infinite matrix product convergence analysis method, some essential algebraic conditions are formulated, which hinge on the parameters of DoS attacks, the underlying network structure, and the gain of the controller. These conditions are critical for guaranteeing the attainment of robust synchronous tracking. Finally, some numerical simulation examples are provided to verify the effectiveness of the proposed secure synchronous tracking control scheme for high-order NASs with signed networks. That is, all followers are able to achieve synchronous tracking of the leader when the corresponding topology, as well as control parameter conditions, are satisfied, and the opposite is not possible.},
  archive      = {J_TAI},
  author       = {Weihao Li and Lei Shi and Mengji Shi and Jiangfeng Yue and Boxian Lin and Kaiyu Qin},
  doi          = {10.1109/TAI.2024.3522873},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1309-1320},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Seeking secure synchronous tracking of networked agent systems subject to antagonistic interactions and denial-of-service attacks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VODACBD: Vehicle object detection based on adaptive convolution and bifurcation decoupling. <em>TAI</em>, <em>6</em>(5), 1298-1308. (<a href='https://doi.org/10.1109/TAI.2024.3522871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle object detection is the foundation of autonomous driving system development. The existing state-of-the-art methods mainly focus on the applications and improvement of general-purpose single shot multibox detector (SSD) and you only look once (YOLO) methods. However, these methods overlook the specific characteristics of traffic scenarios, such as frequent changes of camera angles and rapid changes in surrounding environment, thus leading to peculiar deformations and blurring of vehicle objects. To address these issues, we consider making improvements on the targeted deformations, classification, positioning, and other operations for vehicle object images to alleviate the object deformation and blurring, and therefore propose a Vehicle Object Detection method based on adaptive convolution and bifurcation decoupling (VODACBD). Specifically, in VODACBD, to solve the deformation problem of vehicle objects, adaptive convolution, and feature redivision upsampling are proposed to dynamically capture object features; to alleviate the blurring of vehicle objects, a bifurcation decoupling head is proposed to learn vehicle categories, positions, and confidences. Moreover, to further enhance the overall performance, a global optimal transportation algorithm (GlobalOTA) is well designed to improve the quality of training samples. Extensive experiments were conducted on publicly available traffic object detection datasets such as BDD100K, KITTI, and VOC. The experimental results demonstrate that, compared with current state-of-the-art methods, VODACBD not only achieves an average performance improvement of 1.4% but also an average speed improvement of 1.57 times that of the state-of-the-art.},
  archive      = {J_TAI},
  author       = {Yunfei Yin and Zheng Yuan and Yu He and Xianjian Bao},
  doi          = {10.1109/TAI.2024.3522871},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1298-1308},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {VODACBD: Vehicle object detection based on adaptive convolution and bifurcation decoupling},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multilabel black-box adversarial attacks only with predicted labels. <em>TAI</em>, <em>6</em>(5), 1284-1297. (<a href='https://doi.org/10.1109/TAI.2024.3522869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilabel adversarial examples have become a threat to deep neural network models (DNNs). Most of the current work on multilabel adversarial examples are focused on white-box environments. In this article, we focus on a black-box environment where the available information is extremely limited: a label-only black-box environment. Under the label-only black-box environment, the attacker can only obtain the predicted labels, and cannot obtain any other information such as the model's internal structure, parameters, the training dataset, and the output prediction confidence. We propose a label-only black-box attack framework, and through this framework to implement two black-box adversarial attacks: multi-label boundary-based attack (ML-BA) and multilabel label-only black-box attack (ML-LBA). The ML-BA is developed by transplanting the boundary-based attack in the multiclass domain to the multilabel domain, and the ML-LBA is based on differential evolution. Experimental results show that both the proposed algorithms can achieve the hiding single label attack in label-only black-box environments. Besides, ML-LBA requires fewer queries and its perturbations are significantly less. This demonstrates the effectiveness of the proposed label-only black-box attack framework and the advantageous of differential evolution in optimizing high-dimensional problems.},
  archive      = {J_TAI},
  author       = {Linghao Kong and Wenjian Luo and Zipeng Ye and Qi Zhou and Yan Jia},
  doi          = {10.1109/TAI.2024.3522869},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1284-1297},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multilabel black-box adversarial attacks only with predicted labels},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning to communicate among agents for large-scale dynamic path planning with genetic programming hyperheuristic. <em>TAI</em>, <em>6</em>(5), 1269-1283. (<a href='https://doi.org/10.1109/TAI.2024.3522861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Genetic programming hyperheuristic (GPHH) has recently become a promising methodology for large-scale dynamic path planning (LDPP) since it can produce reusable heuristics rather than disposable solutions. However, in this methodology, the extracted local and decentralized heuristic for agents that lack a global systemic view sometimes may be problematic. Therefore, a new challenge is to strike a balance between conciseness to improve generalization ability and incorporation of more global information to obtain better performance. In this work, we target the LDPP problem and propose a communication learning mechanism (ComLGP) for GPHH to address the above difficulties. In ComLGP, a communication function is introduced to serve as a communication protocol and exist in the form of an extra terminal in GPHH. Compared to the classic terminals which are fixed in genetic programing, this communication function undergoes optimization along with the evolutionary process of GPHH. In this way, the communication function can be learned which enables agents to communicate without a predefined communication protocol. Then, a caching and lazy updating mechanism for ComLGP is presented to accelerate the calculation of communication content. Last, we verified our method on 22 scenarios including two real world road networks. The experimental results demonstrate that the proposed ComLGP can successfully learn to communicate. Although in the absence of any manually designed communication features, ComLGP is capable of achieving performance competitive to the state-of-the-art method that employs a predefined communication protocol and outperforms the remaining compared methods in most scenarios.},
  archive      = {J_TAI},
  author       = {Xiao-Cheng Liao and Xiao-Min Hu and Xiang-Ling Chen and Yi Mei and Ya-Hui Jia and Wei-Neng Chen},
  doi          = {10.1109/TAI.2024.3522861},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1269-1283},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning to communicate among agents for large-scale dynamic path planning with genetic programming hyperheuristic},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An intelligent chatbot assistant for comprehensive troubleshooting guidelines and knowledge repository in printed circuit board production. <em>TAI</em>, <em>6</em>(5), 1259-1268. (<a href='https://doi.org/10.1109/TAI.2024.3521873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores an innovative approach to improving printed circuit board (PCB) manufacturing through an intelligent chatbot assistant. Our chatbot leverages the retrieval-augmented generation (RAG) technique within the Langchain framework, integrating the capabilities of large language models (LLMs) ChatGPT and Llama 2. This combined approach empowers the chatbot to deliver not only accurate but also nuanced and detailed responses to user queries, enhancing troubleshooting and knowledge dissemination. We employ a comprehensive evaluation strategy that incorporates both quantitative and qualitative assessments. While quantitative evaluations reveal no significant differences between the models, qualitative feedback overwhelmingly favors the ChatGPT-based model. The positive user feedback, coupled with the ChatGPT chatbot's superior performance in subjective evaluations, highlights its potential to transform PCB manufacturing by minimizing delays and elevating performance standards.},
  archive      = {J_TAI},
  author       = {Supparesk Rittikulsittichai and Thitirat Siriborvornratanakul},
  doi          = {10.1109/TAI.2024.3521873},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1259-1268},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An intelligent chatbot assistant for comprehensive troubleshooting guidelines and knowledge repository in printed circuit board production},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved supervised machine learning for predicting auto insurance purchase patterns. <em>TAI</em>, <em>6</em>(5), 1248-1258. (<a href='https://doi.org/10.1109/TAI.2024.3521870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a predictive model using supervised machine learning, highlighting the importance of advanced optimization algorithms. Our approach focuses on a nonsmooth loss function known for its effectiveness in supervised machine learning. To ensure desirable properties such as second derivatives and convexity, and to handle outliers, we use a smoothing function to approximate the loss function. This enables the development of robust and stable algorithms for accurate predictions. We introduce a new surrogate smoothing function that is twice differentiable and convex, enhancing the effectiveness of our methodology. Using optimization techniques, especially stochastic gradient descent with Nesterov momentum, we optimize the predictive model. We validate our algorithm through a comprehensive convergence analysis and extensive comparisons with two other prediction models. Our experiments on real datasets from insurance companies demonstrate the practical significance of our approach in predicting auto insurance customer interest.},
  archive      = {J_TAI},
  author       = {Mourad Nachaoui and Fatma Manlaikhaf and Soufiane Lyaqini},
  doi          = {10.1109/TAI.2024.3521870},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1248-1258},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Improved supervised machine learning for predicting auto insurance purchase patterns},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NVMS-net: A novel constrained noise-view multiscale network for detecting general image processing based manipulations. <em>TAI</em>, <em>6</em>(5), 1233-1247. (<a href='https://doi.org/10.1109/TAI.2024.3519052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the authenticity and integrity of digital images is a major concern in multimedia forensics, driving research on universal schemes for detecting diverse image manipulations (or processing operations). Although some prior works have addressed general-purpose image manipulation detection, they have been evaluated under constrained environments. Developing an approach that effectively identifies multiple manipulations in real-world scenarios remains a challenge. To address this issue, in this article, we have designed a novel constrained noise-view based multiscale network (NVMS-Net) that jointly exploits constrained noise-view and multiscale feature learning for multiple image processing operation detection. Our NVMS-Net includes four stages: constrained noise-view, generalizable feature learning, multiscale feature learning, and classification. First, a noise extraction layer (NEL) is employed to suppress the image content information for the extraction of noise features. The statistical modeling of these noise features is further improved by statistical modeling layer (SML) to achieve a better noise-view of image tampering artifacts. Then, we use multiple convolution-batch normalization-ReLU (CBR) blocks to learn generalizable noise features that are sensitive to image manipulations. Further, a multiscale aggregation module (MSAM) is proposed that leverages image tampering features at multiple scales through the aggregation of low and high-level features. The extensive experimental results show that the NVMS-Net consistently outperforms the existing approaches on different dataset settings. Importantly, NVMS-Net shows better performance in real-world scenarios, particularly against antiforensic techniques and adversarial attacks. The proposed NVMS-Net provides an overall accuracies of 98.21% and 98.60% on BOSSBase and Dresden datasets, respectively.},
  archive      = {J_TAI},
  author       = {Gurinder Singh and Kapil Rana and Puneet Goyal and Sathish Kumar},
  doi          = {10.1109/TAI.2024.3519052},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1233-1247},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {NVMS-net: A novel constrained noise-view multiscale network for detecting general image processing based manipulations},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning data collection for bayesian inference of hidden markov models. <em>TAI</em>, <em>6</em>(5), 1217-1232. (<a href='https://doi.org/10.1109/TAI.2024.3515939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov models (HMMs) are a powerful class of dynamical models for representing complex systems that are partially observed through sensory data. Existing data collection methods for HMMs, typically based on active learning or heuristic approaches, face challenges in terms of efficiency in stochastic domains with costly data. This article introduces a Bayesian lookahead data collection method for inferring HMMs with finite state and parameter spaces. The method optimizes data collection under uncertainty using a belief state that captures the joint distribution of system states and models. Unlike traditional approaches that prioritize short-term gains, this policy accounts for the long-term impact of data collection decisions to improve inference performance over time. We develop a deep reinforcement learning policy that approximates the optimal Bayesian solution by simulating system trajectories offline. This pretrained policy can be executed in real time, dynamically adapting to new conditions as data is collected. The proposed framework supports a wide range of inference objectives, including point based, distribution based, and causal inference. Experimental results across three distinct systems demonstrate significant improvements in inference accuracy and robustness, showcasing the effectiveness of the approach in uncertain and data-limited environments.},
  archive      = {J_TAI},
  author       = {Mohammad Alali and Mahdi Imani},
  doi          = {10.1109/TAI.2024.3515939},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1217-1232},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep reinforcement learning data collection for bayesian inference of hidden markov models},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsigned road incidents detection using improved RESNET from driver-view images. <em>TAI</em>, <em>6</em>(5), 1203-1216. (<a href='https://doi.org/10.1109/TAI.2024.3515938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Frequent road incidents cause significant physical harm and economic losses globally. The key to ensuring road safety lies in accurately perceiving surrounding road incidents. However, the highly dynamic nature of traffic introduces significant challenges, particularly in detecting sudden and temporary incidents. In this article, we propose a novel detection framework, multihead attention ResNet with dynamic bottleneck (DB_RESNET_MHA), to identify physical unsigned road incidents. Our approach introduces three key innovations. First, we develop a tailored data augmentation strategy to create images that closely mimic the complex variations found in real-world road environments. Second, we enhance model expressiveness by employing attention mechanisms to nonlinearly integrate convolutional kernels within the residual network. Furthermore, we refine the prediction head by applying spatially distinct attention weights, enabling the model to capture intricate correlations between different features more effectively. To demonstrate the effectiveness of our method, we create a dataset for unsigned road incidents (UnsignRI), comprising a total of 16 323 images that capture 12 distinct types of incidents. It stands out as the most comprehensive dataset in the field, encompassing a wide range of geographical features and incident categories. Experimental results show that DB_RESNET_MHA achieves an average accuracy of 96.2% and a f1-score of 0.955 across various categories of unsigned incidents, surpassing other models.},
  archive      = {J_TAI},
  author       = {Changping Li and Bingshu Wang and Jiangbin Zheng and Yongjun Zhang and C.L. Philip Chen},
  doi          = {10.1109/TAI.2024.3515938},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1203-1216},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unsigned road incidents detection using improved RESNET from driver-view images},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Periodic hamiltonian neural networks. <em>TAI</em>, <em>6</em>(5), 1194-1202. (<a href='https://doi.org/10.1109/TAI.2024.3515934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling dynamical systems is a core challenge for science and engineering. Hamiltonian neural networks (HNNs) are state-of-the-art models that regress the vector field of a dynamical system under the learning bias of Hamilton's equations. A recent observation is that embedding biases regarding invariances of the Hamiltonian improve regression performance. One such invariance is the periodicity of the Hamiltonian, which improves extrapolation performance. We propose periodic HNNs that embed periodicity within HNNs using observational, learning, and inductive biases. An observational bias is embedded by training the HNN on data that reflects the periodicity of the Hamiltonian. A learning bias is embedded through the loss function of the HNN. An inductive bias is embedded by a periodic activation function in the HNN. We evaluate the performance of the proposed models on interpolation and extrapolation problems that either assume knowledge of the periods a priori or learn the periods as parameters. We show that the proposed models can interpolate well but are far more effective than the HNN at extrapolating the Hamiltonian and the vector field for both problems and can even extrapolate the vector field of the chaotic double pendulum Hamiltonian system.},
  archive      = {J_TAI},
  author       = {Zi-Yu Khoo and Dawen Wu and Jonathan Sze Choong Low and Stéphane Bressan},
  doi          = {10.1109/TAI.2024.3515934},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1194-1202},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Periodic hamiltonian neural networks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Herb-target interaction prediction by multiinstance learning. <em>TAI</em>, <em>6</em>(5), 1184-1193. (<a href='https://doi.org/10.1109/TAI.2024.3515932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Herb-target interactions (HTIs) are pivotal for unveiling the underlying pharmacological mechanisms between herbs and biological targets (e.g., proteins and nucleic acids). Unlike typical drugs, each of which is made of one pharmacological compound, the herb is composed of different pharmacological ingredients, while contemporary computational approaches mainly focus on HTIs, neglecting the more refined and informative ingredient-target interactions (ITIs). Furthermore, those methods also disregard the complex associations between herbs and ingredients, which have multigranular interactions with targets. We propose a multiinstance learning-based solution (HTI-MIL) to identify HTIs and the finer ITIs. Particularly, HTI-MIL employs autoencoder, convolutional neural networks, and graph convolution networks to learn representations of herbs, targets, and ingredients, respectively. Next, it takes herbs as bags and ingredients as instances to model the interplay between herbs and ingredients by multiinstance learning, and aggregates ingredient features toward their hosting herb. After that, it leverages the representation of ingredients and targets to predict ITIs and fuses the representations of herbs, ingredients, and targets to predict HTIs. Experimental results show that HTI-MIL outperforms competitive methods by at least 5.2%, 4.8%, and 6.4% in AUROC, AUPRC, and F1-score, respectively. HTI-MIL validates on typical herbs, confirms $\boldsymbol{\geq}$90% of the top 20 candidate targets and also finds out novel interactions. In addition, the targets identified by HTI-MIL help to dissect the mechanism of herb-induced liver injury, which further validates its effectiveness.},
  archive      = {J_TAI},
  author       = {Yongzheng Zhu and Liangrui Ren and Rong Sun and Jun Wang and Guoxian Yu},
  doi          = {10.1109/TAI.2024.3515932},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1184-1193},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Herb-target interaction prediction by multiinstance learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised learning of unbiased visual representations. <em>TAI</em>, <em>6</em>(5), 1171-1183. (<a href='https://doi.org/10.1109/TAI.2024.3514554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks often struggle to learn robust representations in the presence of dataset biases, leading to suboptimal generalization on unbiased datasets. This limitation arises because the models heavily depend on peripheral and confounding factors, inadvertently acquired during training. Existing approaches to address this problem typically involve explicit supervision of bias attributes or reliance on prior knowledge about the biases. In this study, we address the challenging scenario where no explicit annotations of bias are available, and there's no prior knowledge about its nature. We present a fully unsupervised debiasing framework with three key steps: first, leveraging the inherent tendency to learn malignant biases to acquire a bias-capturing model; next, employing a pseudo-labeling process to obtain bias labels; and finally, applying cutting-edge supervised debiasing techniques to achieve an unbiased model. Additionally, we introduce a theoretical framework for evaluating model biasedness and conduct a detailed analysis of how biases impact neural network training. Experimental results on both synthetic and real-world datasets demonstrate the effectiveness of our method, showcasing state-of-the-art performance in various settings, occasionally surpassing fully supervised debiasing approaches.},
  archive      = {J_TAI},
  author       = {Carlo Alberto Barbano and Enzo Tartaglione and Marco Grangetto},
  doi          = {10.1109/TAI.2024.3514554},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1171-1183},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unsupervised learning of unbiased visual representations},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CauseTerML: Causal learning via term mining for assessing review discrepancies. <em>TAI</em>, <em>6</em>(5), 1156-1170. (<a href='https://doi.org/10.1109/TAI.2024.3512500'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Innovation is a key driver of modern economic and technological development. Correct and equitable identification of innovation is essential for promoting market competitiveness and ensuring the optimal allocation of resources. Existing research on innovation evaluation mainly focuses on qualitative or quantitative evaluation of the results, while ignoring potential biases in the application process. This work investigates an unexplored issue in the field of innovation evaluation: Whether the technicality of the title of an application affects its degree of attention in the review process? The key lies in two aspects: how to evaluate the technicality of the title and how to quantify this effect. To achieve this goal, we combine the term extraction schemes and causal inference techniques by modelling the fairness detection task in a causal diagram, and propose a novel framework called CauseTerML. The framework can be applied to fairness detection in a variety of application scenarios. Extensive experiments on a real-world patent dataset validate the effectiveness of CauseTerML.},
  archive      = {J_TAI},
  author       = {Wenjie Sun and Chengke Wu and Qinge Xiao and Junjie Jiang and Yuanjun Guo and Ying Bi and Xinyu Wu and Zhile Yang},
  doi          = {10.1109/TAI.2024.3512500},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1156-1170},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CauseTerML: Causal learning via term mining for assessing review discrepancies},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupling dark knowledge via block-wise logit distillation for feature-level alignment. <em>TAI</em>, <em>6</em>(5), 1143-1155. (<a href='https://doi.org/10.1109/TAI.2024.3512498'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD), a learning manner with a larger teacher network guiding a smaller student network, transfers dark knowledge from the teacher to the student via logits or intermediate features, with the aim of producing a well-performed lightweight model. Notably, many subsequent feature-based KD methods outperformed the earliest logit-based KD method and iteratively generated numerous state-of-the-art distillation methods. Nevertheless, recent work has uncovered the potential of the logit-based method, bringing the simple KD form based on logits back into the limelight. Features or logits? They partially implement the KD with entirely distinct perspectives; therefore, choosing between logits and features is not straightforward. This article provides a unified perspective of feature alignment to obtain a better comprehension of their fundamental distinction. Inheriting the design philosophy and insights of feature-based and logit-based methods, we introduce a block-wise logit distillation framework to apply implicit logit-based feature alignment by gradually replacing teacher's blocks as intermediate stepping-stone models to bridge the gap between the student and the teacher. Our method obtains comparable or superior results to state-of-the-art distillation methods. This article demonstrates the great potential of combining logit and features, and we hope it will inspire future research to revisit KD from a higher vantage point.},
  archive      = {J_TAI},
  author       = {Chengting Yu and Fengzhao Zhang and Ruizhe Chen and Aili Wang and Zuozhu Liu and Shurun Tan and Er-Ping Li},
  doi          = {10.1109/TAI.2024.3512498},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1143-1155},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Decoupling dark knowledge via block-wise logit distillation for feature-level alignment},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quantum multimodal neural network model for sentiment analysis on quantum circuits. <em>TAI</em>, <em>6</em>(5), 1128-1142. (<a href='https://doi.org/10.1109/TAI.2024.3511514'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a quantum multimodal neural network (QMNN) model that can be implemented on parameterized quantum circuits (PQCs), providing a novel avenue for processing multimodal data and performing advanced multimodal sentiment analysis tasks. The comprehensive QMNN model is structured into four fundamental blocks: multimodal data preprocessing, unimodal feature extraction, multimodal feature fusion, and optimization. Through these blocks, multimodal data are initially preprocessed and encoded into quantum states. Subsequently, visual and textual features are extracted from the quantum states and are then integrated to learn the interactions between different modalities. Finally, the model parameters are fine-tuned to optimize the sentiment analysis performance. Simulation results confirm that QMNN surpasses state-of-the-art baselines, using significantly lower input dimensions and substantially fewer parameters than classical models. Furthermore, the entanglement, integrity, robustness, and scalability of the model are analyzed in depth. Internally, the strong entanglement within the multimodal fusion block enhances interactions between textual and visual features, and the integrity of the model reflects the indispensable contribution of each component to the overall performance. Externally, robustness ensures the model operates stably under noisy conditions and incomplete inputs, and scalability enables it to efficiently adapt to varying architectural depths and widths. The above simulation results and performance analyses showcase the comprehensive strength of our proposed model.},
  archive      = {J_TAI},
  author       = {Jin Zheng and Qing Gao and Daoyi Dong and Jinhu Lü and Yue Deng},
  doi          = {10.1109/TAI.2024.3511514},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1128-1142},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A quantum multimodal neural network model for sentiment analysis on quantum circuits},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving string stability in cooperative adaptive cruise control through multiagent reinforcement learning with potential-driven motivation. <em>TAI</em>, <em>6</em>(5), 1114-1127. (<a href='https://doi.org/10.1109/TAI.2024.3511513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative adaptive cruise control (CACC) is regarded as a promising technology for achieving efficient and safe collaboration among connected and automated vehicles (CAVs) in a platoon, and multiagent reinforcement learning (MARL) methods are emerging as an effective approach to implementing the CACC technology. However, most MARL methods do not sufficiently tackle the prevalent string stability problem, even when integrating communication mechanisms to improve agents’ understanding of CACC scenarios. This limitation arises because these methods typically learn communication mechanisms based solely on the information directly observable by the agents, neglecting potentially valuable information present in the environment. In this article, we propose a multiagent actor–critic with a potential-driven motivation (MAACPM) approach, which utilizes variational inference theory to infer the potential motivation representation space in the CACC task, providing a more favorable opportunity for adjusting driving behavior within the platoon. Furthermore, we quantify the specific impact of potential motivation on each vehicle by measuring the difference between policies with and without potential motivation. We then utilize this difference as a potential reward signal to incentivize the agent to grasp effective potential motivation. The proposed method was validated in two typical CACC scenarios, where we compared the performance of our MAACPM algorithm with other state-of-the-art MARL methods to demonstrate its effectiveness. Furthermore, we illustrate potential real-world applications of our method by comparing it with actual vehicle driving data.},
  archive      = {J_TAI},
  author       = {Kun Jiang and Min Hua and Xu He and Lu Dong and Quan Zhou and Hongming Xu and Changyin Sun},
  doi          = {10.1109/TAI.2024.3511513},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1114-1127},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Improving string stability in cooperative adaptive cruise control through multiagent reinforcement learning with potential-driven motivation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of an intellectual mechanism of a novel crop recommendation system using improved heuristic algorithm-based attention and cascaded deep learning network. <em>TAI</em>, <em>6</em>(5), 1100-1113. (<a href='https://doi.org/10.1109/TAI.2024.3508654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces an innovative crop recommendation system that leverages an attention-based cascaded deep learning network (AACNet) optimized by an improved migration algorithm (IMA). The system is designed to address the inefficiencies of traditional crop recommendation methods by providing precise, real-time suggestions tailored to specific agricultural factors such as weather, soil type, and time. The AACNet employs recurrent neural networks (RNN) and gated recurrent units (GRU) to analyze time-sensitive agricultural factors, such as weather patterns and soil conditions, while the attention mechanism prioritizes the most significant features for accurate crop recommendations. The IMA optimizes the deep learning network, enhancing the system’s accuracy, precision, recall, and execution time. Experimental results demonstrate that the proposed system outperforms traditional methods, marking a significant advancement in precision agriculture. The system’s potential to revolutionize farming decision-making processes by optimizing resource allocation, reducing costs, and increasing crop yields underscores its importance in global agricultural challenges. This research represents a transformative step towards informed, efficient, and sustainable farming practices.},
  archive      = {J_TAI},
  author       = {Yaganteeswarudu Akkem and Saroj Kumar Biswas},
  doi          = {10.1109/TAI.2024.3508654},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1100-1113},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Analysis of an intellectual mechanism of a novel crop recommendation system using improved heuristic algorithm-based attention and cascaded deep learning network},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain-conditional multimodal synthesis: A survey and taxonomy. <em>TAI</em>, <em>6</em>(5), 1080-1099. (<a href='https://doi.org/10.1109/TAI.2024.3516698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of artificial intelligence generated content (AIGC), conditional multimodal synthesis technologies (e.g., text-to-image) are dynamically reshaping the natural content. Brain signals, serving as potential reflections of how the brain interprets external information, exhibit a distinctive one-to-many correspondence with various external modalities. This correspondence makes brain signals emerge as a promising guiding condition for multimodal synthesis (e.g., image, text, and audio), which is crucial for developing practical brain–computer interface systems and unraveling complex mechanisms underlying human perception. This survey comprehensively examines the emerging field of brain-conditional multimodal synthesis, termed AIGC-brain, to delineate the current landscape and future directions. To begin, related neuroimaging datasets and generative models are introduced as the foundation of AIGC-brain decoding and analysis. Next, we present a comprehensive taxonomy according to AIGC-brain methodologies, followed by task-specific representative work and implementation details to facilitate in-depth comparison and analysis. Quality assessments are then introduced for both qualitative and quantitative evaluation. Finally, this survey explores insights gained, outlining current challenges and prospects of AIGC-brain. As a pioneering survey, this article paves the way for future advances in AIGC-brain research.},
  archive      = {J_TAI},
  author       = {Weijian Mai and Jian Zhang and Pengfei Fang and Zhijun Zhang},
  doi          = {10.1109/TAI.2024.3516698},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1080-1099},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Brain-conditional multimodal synthesis: A survey and taxonomy},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approaching principles of XAI: A systematization. <em>TAI</em>, <em>6</em>(5), 1067-1079. (<a href='https://doi.org/10.1109/TAI.2024.3515937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today's explainable artificial intelligence (XAI) landscape is the product of a long history of ever-changing artificial intelligence (AI) research and attempts to explain it. It can be vast and confusing. Our historical reconstruction of XAI developments relates AI improvements to their inevitable impact on explanation research. The reconstruction provides the basis for an analysis of the state of XAI and for discussing its future developments in our article and in general. We then propose a new taxonomy based on this historical reconstruction and current XAI approaches. It is a balanced mixture of detail and general applicability. It is therefore intended to be useful in a wide variety of contexts. The flowchart inspired nature of our taxonomy relates its dimensions not only to the XAI development process, but also to each other, creating an additional layer of structure. Given the historical reconstruction and our taxonomy, we are able to propose three principles: computing edges, dimensionality reduction, and traceability/blaming. These are capable of structuring the debate in a new way, as they are not intended to be just ideas that current approaches adhere to. We also propose two new principles for the future (embedment and scientific testing) that XAI approaches should adhere to in order to improve their explanations. Our findings provide a structured approach to the analysis and development of XAI methodologies. By integrating historical perspectives with state-of-the-art approaches, our research provides a basis for stimulating discussion about the principles that XAI follows and should follow in the future.},
  archive      = {J_TAI},
  author       = {Raphael Ronge and Bernhard Bauer and Benjamin Rathgeber},
  doi          = {10.1109/TAI.2024.3515937},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1067-1079},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Approaching principles of XAI: A systematization},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative review of deep learning techniques on the classification of irony and sarcasm in text. <em>TAI</em>, <em>6</em>(5), 1052-1066. (<a href='https://doi.org/10.1109/TAI.2024.3515935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides a review of classification methods for irony and sarcasm in textual data. It explores different approaches to detecting irony and sarcasm, their definitions, distinguishing features, and detection methodologies. The study examines a range of datasets used in irony and sarcasm detection research, including short-text datasets from social media platforms and long-text datasets from product reviews and discussion forums. Additionally, the article discusses the various features employed in sarcasm detection experiments, such as lexical, pragmatic, hyperbole, semantic, syntactic, sentiment, and contextual features. It also explores the classification methodologies used. The article concludes by analyzing each classification method and highlighting the latest trends in irony and sarcasm detection.},
  archive      = {J_TAI},
  author       = {Leonidas Boutsikaris and Spyros Polykalas},
  doi          = {10.1109/TAI.2024.3515935},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {5},
  number       = {5},
  pages        = {1052-1066},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A comparative review of deep learning techniques on the classification of irony and sarcasm in text},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ClusVPR: Efficient visual place recognition with clustering-based weighted transformer. <em>TAI</em>, <em>6</em>(4), 1038-1049. (<a href='https://doi.org/10.1109/TAI.2024.3510479'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual place recognition (VPR) is a highly challenging task that has a wide range of applications, including robot navigation and self-driving vehicles. VPR is a difficult task due to duplicate regions and insufficient attention to small objects in complex scenes, resulting in recognition deviations. In this article, we present ClusVPR, a novel approach that tackles the specific issues of redundant information in duplicate regions and representations of small objects. Different from existing methods that rely on convolutional neural networks (CNNs) for feature map generation, ClusVPR introduces a unique paradigm called clustering-based weighted transformer network (CWTNet). CWTNet uses the power of clustering-based weighted feature maps and integrates global dependencies to effectively address visual deviations encountered in large-scale VPR problems. We also introduce the optimized-VLAD (OptLAD) layer, which significantly reduces the number of parameters and enhances model efficiency. This layer is specifically designed to aggregate the information obtained from scale-wise image patches. Additionally, our pyramid self-supervised strategy focuses on extracting representative and diverse features from scale-wise image patches rather than from entire images. This approach is essential for capturing a broader range of information required for robust VPR. Extensive experiments on four VPR datasets show our model's superior performance compared to existing models while being less complex.},
  archive      = {J_TAI},
  author       = {Yifan Xu and Pourya Shamsolmoali and Masoume Zareapoor and Jie Yang},
  doi          = {10.1109/TAI.2024.3510479},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1038-1049},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ClusVPR: Efficient visual place recognition with clustering-based weighted transformer},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unformer: A transformer-based approach for adaptive multiscale feature aggregation in underwater image enhancement. <em>TAI</em>, <em>6</em>(4), 1024-1037. (<a href='https://doi.org/10.1109/TAI.2024.3508667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater imaging is often compromised by light scattering and absorption, resulting in image degradation and distortion. This manifests as blurred details, color shifts, and diminished illumination and contrast, thereby hindering advancements in underwater research. To mitigate these issues, we propose Unformer, an innovative underwater image enhancement (UIE) technique that leverages a transformer-based architecture for multiscale adaptive feature aggregation. Our approach employs a multiscale feature fusion strategy that adaptively restores illumination and detail features. We reevaluate the relationship between convolution and transformer to develop a novel encoder structure. This structure effectively integrates both long-range and short-range dependencies, dynamically combines local and global features, and constructs a comprehensive global context. Furthermore, we propose a unique multibranch decoder architecture that enhances and efficiently extracts spatial context information through the transformer module. Extensive experiments on three datasets demonstrate that our proposed method outperforms other techniques in both subjective and objective evaluations. Compared with the latest methods, Unformer has improved the peak signal-to-noise ratio (PSNR) by 19.5% and 14.8% respectively on the LSUI and EUVP datasets. The code is available at: https://github.com/yhflq/Unformer.},
  archive      = {J_TAI},
  author       = {Yuhao Qing and Yueying Wang and Huaicheng Yan and Xiangpeng Xie and Zhengguang Wu},
  doi          = {10.1109/TAI.2024.3508667},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1024-1037},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Unformer: A transformer-based approach for adaptive multiscale feature aggregation in underwater image enhancement},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aperiodically intermittent control approach to finite-time synchronization of delayed inertial memristive neural networks. <em>TAI</em>, <em>6</em>(4), 1014-1023. (<a href='https://doi.org/10.1109/TAI.2024.3507740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the finite-time synchronization (FTS) for inertial memristive neural networks (IMNNs) with time-delays by the aperiodically intermittent control approach. Compared with the reduced-order method utilized in the existing literature, this article considers the FTS of delayed IMNNs directly without order reduction. First, the error IMNNs with time-delays is designed through the theories of set-valued mappings and differential inclusions, and its finite-time stability problem is discussed by applying the finite-time stability theorem. Furthermore, by constructing nonperiodic intermittent state-feedback controller and nonperiodic intermittent adaptive control strategy, the sufficient criteria to ensure the FTS of the master–slave delayed IMNNs are derived, and the settling times are explicitly estimated. Finally, a simulation to confirm the availability of results is provided.},
  archive      = {J_TAI},
  author       = {Yuxin Jiang and Song Zhu and Mouquan Shen and Shiping Wen and Chaoxu Mu},
  doi          = {10.1109/TAI.2024.3507740},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1014-1023},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Aperiodically intermittent control approach to finite-time synchronization of delayed inertial memristive neural networks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven event-triggered control for discrete-time neural networks subject to actuator saturation. <em>TAI</em>, <em>6</em>(4), 1003-1013. (<a href='https://doi.org/10.1109/TAI.2024.3507736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the data-driven event-triggered control is addressed for unknown discrete-time neural networks (DTNNs) under actuator saturation and external perturbation. The research problem is raised due to the following two reasons: 1) a practical system is often affected by external perturbations and it is costly to acquire an accurate system model; 2) the network bandwidth and the control inputs are always constrained due to physical hardware. To handle the above issues, the methodology is to first establish a model-based stability condition under the designed saturated event-triggered controller and then to transform the model-based stability condition into a data-based stability condition relying only on the perturbation-corrupted data via the extended S-lemma. The key results are: 1) a data-based DTNNs system representation is presented by collecting perturbation-corrupted state-input data. Then, a data-based stability criterion is derived and the saturated event-triggered controller is designed without an explicit system model; 2) an optimization method is presented that can maximize the estimation of attractor (EoA) and minimize the estimated domain of attraction (DoA) simultaneously. Finally, the effectiveness of the proposed approach is illustrated and some quantitative analyses are offered by two numerical examples.},
  archive      = {J_TAI},
  author       = {Yanyan Ni and Zhen Wang and Xia Huang and Hao Shen},
  doi          = {10.1109/TAI.2024.3507736},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {1003-1013},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Data-driven event-triggered control for discrete-time neural networks subject to actuator saturation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A Temporal–Spatial graph network with a learnable adjacency matrix for appliance-level electricity consumption prediction. <em>TAI</em>, <em>6</em>(4), 989-1002. (<a href='https://doi.org/10.1109/TAI.2024.3507734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the electricity consumption of individual appliances, known as appliance-level energy consumption (ALEC) prediction, is essential for effective energy management and conservation. Despite its importance, research in this area is limited and faces several challenges: 1) the correlation between the usage of different appliances has rarely been considered for ALEC prediction; 2) a learnable strategy for obtaining the optimal correlation between different appliance behaviors is lacking; and 3) it is difficult to accurately quantify the usage relationship among different appliances. To address these issues, we propose a graph-based temporal–spatial network that employs a learnable adjacency matrix for appliance-level load prediction in this work. The network comprises a temporal graph convolutional network (TGCN) and a learnable adjacency matrix that enables us to utilize correlations between appliances and quantify their relationships. To validate our approach, we compared our model with six others: a TGCN model with a fixed adjacency matrix where all elements are set to 0; a TGCN model with a fixed adjacency matrix where all elements are set to 0.5, except for the diagonal; a TGCN model with a randomly generated adjacency matrix, except for the diagonal; an Aug-LSTM model; a model with ResNetPlus architecture; and a feed-forward deep neural network. Five houses in four datasets: AMPDs, REFIT, UK-DALE, and SC-EDNRR are utilized. The metrics used in this study include root mean square error, explained variance score, mean absolute error, F-norm and coefficient of determination. Our experiments have validated the accuracy and practicality of our proposed approach across different datasets.},
  archive      = {J_TAI},
  author       = {Dandan Li and Jiaxing Xia and Jiangfeng Li and Changjiang Xiao and Vladimir Stankovic and Lina Stankovic and Qingjiang Shi},
  doi          = {10.1109/TAI.2024.3507734},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {989-1002},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A Temporal–Spatial graph network with a learnable adjacency matrix for appliance-level electricity consumption prediction},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Industrial process monitoring based on deep gaussian and non-gaussian information fusion framework. <em>TAI</em>, <em>6</em>(4), 979-988. (<a href='https://doi.org/10.1109/TAI.2024.3507732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For industrial process monitoring, Gaussian and non-Gaussian data-driven models are two important representatives that have been developed separately in the past years. Although several attempts have been made to combine Gaussian and non-Gaussian data information for integrated process monitoring, this information fusion strategy can be further enhanced under the idea and framework of deep learning. Particularly, through collaborative learning and layer-by-layer information transformation, more patterns of both Gaussian and non-Gaussian components can be effectively extracted in different hidden layers of the deep model. Then, a further Bayesian model fusion strategy is formulated to ensemble monitoring results from both Gaussian and non-Gaussian data-driven models. Therefore, the main contribution of this article is to propose a deep Gaussian and non-Gaussian information fusion framework for data-driven industrial process monitoring. Both feasibility and superiority of the developed model are confirmed through a detailed industrial benchmark case study. Compared to both Gaussian and non-Gaussian deep models, the new deep information fusion model has obtained more satisfactory monitoring results.},
  archive      = {J_TAI},
  author       = {Zhiqiang Ge},
  doi          = {10.1109/TAI.2024.3507732},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {979-988},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Industrial process monitoring based on deep gaussian and non-gaussian information fusion framework},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CheckSelect: Online checkpoint selection for flexible, accurate, robust, and efficient data valuation. <em>TAI</em>, <em>6</em>(4), 968-978. (<a href='https://doi.org/10.1109/TAI.2024.3506494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we argue that data valuation techniques should be flexible, accurate, robust, and efficient (FARE). Here, accuracy and efficiency refer to the notion of identification of most important data points in less time compared to full training. Flexibility refers to the ability of the method to be used with various value functions, while robustness refers to the ability to be used with different data distributions from a related domain. We propose a two-phase approach toward achieving these objectives, where the first phase, checkpoint selection, extracts important model checkpoints while training on a related dataset, and the second data valuation and subset selection (DVSS) phase extracts the high-value subsets. A key challenge in this process is to efficiently determine the most important checkpoints during the training, since the total value function is unknown. We pose this as an online sparse approximation problem and propose a novel online orthogonal matching pursuit algorithm for solving it. Extensive experiments on standard datasets show that CheckSelect provides the best accuracy among the baselines while maintaining efficiency comparable to state of the art. We also demonstrate the flexibility and robustness of CheckSelect on a standard domain adaptation task, where it outperforms existing methods in data selection accuracy without the need to retrain on the full target-domain dataset.},
  archive      = {J_TAI},
  author       = {Soumi Das and Manasvi Sagarkar and Suparna Bhattacharya and Sourangshu Bhattacharya},
  doi          = {10.1109/TAI.2024.3506494},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {968-978},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CheckSelect: Online checkpoint selection for flexible, accurate, robust, and efficient data valuation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A scalable unsupervised and back propagation free learning with SACSOM: A novel approach to SOM-based architectures. <em>TAI</em>, <em>6</em>(4), 955-967. (<a href='https://doi.org/10.1109/TAI.2024.3504479'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of computer vision is predominantly driven by supervised models, which, despite their efficacy, are computationally expensive and often intractable for many applications. Recently, research has expedited alternative avenues such as self-organizing maps (SOM)-based architectures, which offer significant advantages such as tractability, the absence of back-propagation, and feed-forward unsupervised learning. However, these SOM-based approaches frequently suffer from lower accuracy and limited generalization capabilities. To address these shortcomings, we propose a novel model called split and concur SOM (SACSOM). SACSOM overcomes the limitations of closely related SOM-based algorithms by utilizing multiple parallel branches, each equipped with its own SOM modules that process data independently with varying patch sizes. Furthermore, by creating groups of classes and using respective training samples to train independent subbranches in each branch, our approach accommodates datasets with a large number of classes. SACSOM employs a simple yet effective labeling technique requiring minimal labeled samples. The outputs from each branch, filtered by a threshold, contribute to the final prediction. Experimental validation on MNIST-digit, Fashion-MNIST, CIFAR-10, and CIFAR-100 demonstrates that SACSOM achieves competitive accuracy with significantly reduced computation time. Furthermore, it exhibits superior performance and generalization capabilities, even in high-noise scenarios. The weights of the single-layered SACSOM provide meaningful insights into the patch-based learning pattern, enhancing its tractability and making it ideal from the perspective of explainable AI. This study addresses the limitations of current clustering techniques, such as K-means and traditional SOMs, by proposing a lightweight, manageable, and fast architecture that does not require a GPU, making it suitable for low-powered devices.},
  archive      = {J_TAI},
  author       = {Gaurav R. Hirani and Kevin I-Kai Wang and Waleed H. Abdulla},
  doi          = {10.1109/TAI.2024.3504479},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {955-967},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A scalable unsupervised and back propagation free learning with SACSOM: A novel approach to SOM-based architectures},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). APR-net: Defense against adversarial examples based on universal adversarial perturbation removal network. <em>TAI</em>, <em>6</em>(4), 945-954. (<a href='https://doi.org/10.1109/TAI.2024.3504478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attack, a bleeding-edge technique that attempts to fool deep learning classification model by generating adversarial examples with imperceptible perturbations, is becoming a growing threat in artificial intelligence fields. Preprocessing models that remove perturbations are an effective approach for enhancing the robustness of classification models. However, most existing methods overlook a critical issue: although powerful preprocessing operations can remove adversarial perturbations, they may also weaken the representation of key features in the image, leading to decreased defense performance. To address this, we propose a novel universal defense model, APR-Net, which aims to remove adversarial perturbations while effectively preserving high-quality images. The key innovation of APR-Net lies in its dual-module design, which consists of a denoising module and an image restoration module. This design not only effectively eliminates imperceptible adversarial perturbations but also ensures the restoration of high-quality images. Unlike existing methods, APR-Net does not require modifications to the classifier architecture or specialized adversarial training, making it highly versatile. Extensive experiments on the ImageNet dataset demonstrate that APR-Net provides strong defense against various adversarial attack algorithms, significantly improves image quality, and outperforms other state-of-the-art defense methods in terms of overall performance.},
  archive      = {J_TAI},
  author       = {Wenxing Liao and Zhuxian Liu and Minghuang Shen and Riqing Chen and Xiaolong Liu},
  doi          = {10.1109/TAI.2024.3504478},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {945-954},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {APR-net: Defense against adversarial examples based on universal adversarial perturbation removal network},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CH-net: A cross hybrid network for medical image segmentation. <em>TAI</em>, <em>6</em>(4), 934-944. (<a href='https://doi.org/10.1109/TAI.2024.3503541'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and automated segmentation of medical images plays a crucial role in diagnostic evaluation and treatment planning. In recent years, hybrid models have gained considerable popularity in diverse medical image segmentation tasks, as they leverage the benefits of both convolution and self-attention to capture local and global dependencies simultaneously. However, most existing hybrid models treat convolution and self-attention as independent components and integrate them using simple fusion methods, neglecting the potential complementary information between their weight allocation mechanisms. To address this issue, we propose a cross hybrid network (CH-Net) for medical image segmentation, in which convolution and self-attention are hybridized in a cross-collaborative manner. Specifically, we introduce a cross hybrid module (CHM) between the parallel convolution layer and self-attention layer in each building block of CH-Net. This module extracts attention with distinct dimensional information from convolution and self-attention, respectively, and uses this complementary information to enhance the feature representation of both components. In contrast to the traditional approach where each module learned independently, the CHM facilitates the interactive learning of complementary information between convolutional layer and self-attention layer, which significantly enhances the segmentation capabilities of the model. The superiority of our approach over various hybrid models is demonstrated through experimental evaluations conducted on three publicly available benchmarks: ACDC, synapse, and EM.},
  archive      = {J_TAI},
  author       = {Jiale Li and Aiping Liu and Wei Wei and Ruobing Qian and Xun Chen},
  doi          = {10.1109/TAI.2024.3503541},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {934-944},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {CH-net: A cross hybrid network for medical image segmentation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DGeC: Dynamically and globally enhanced convolution. <em>TAI</em>, <em>6</em>(4), 921-933. (<a href='https://doi.org/10.1109/TAI.2024.3502577'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the reasons for the poorer feature extraction ability of vanilla convolution and discover that there mainly exist three key factors that restrict its representation capability, i.e., regular sampling, static aggregation, and limited receptive field. With the cost of extra parameters and computations, existing approaches merely alleviate part of the limitations. It drives us to seek a more lightweight operator to further improve the extracted image features. Through a closer examination of the convolution process, we discover that it is composed of two distinct interactions: spatial-wise interaction and channel-wise interaction. Based on this discovery, we decouple the convolutional blocks into these two interactions which not only reduces the parameters and computations but also enables a richer ensemble of interactions. Then, we propose the dynamically and globally enhanced convolution (DGeC), which includes several components as follows: a dynamic area perceptor block (DAP) that dynamically samples spatial cues, an adaptive global context block (AGC) that introduces the location-aware global image information, and a channel attention perceptor block (CAP) that merges different channel-wise features. The experiments on ImageNet for image classification and on COCO-2017 for object detection validate the effectiveness of DGeC. As a result, our proposed method consistently improves the performance with fewer parameters and computations. In particular, DGeC achieves a 3.1% improvement in top-1 accuracy on ImageNet dataset compared to ResNet50. Moreover, with Faster RCNN and RetinaNet, our DGeC-ResNet50 also consistently outperforms ResNet and ResNeXt.},
  archive      = {J_TAI},
  author       = {Zihang Zhang and Yuling Liu and Zhili Zhou and Gaobo Yang and Xin Liao and Q. M. Jonathan Wu},
  doi          = {10.1109/TAI.2024.3502577},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {921-933},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {DGeC: Dynamically and globally enhanced convolution},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LoRaDIP: Low-rank adaptation with deep image prior for generative low-light image enhancement. <em>TAI</em>, <em>6</em>(4), 909-920. (<a href='https://doi.org/10.1109/TAI.2024.3499950'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents LoRaDIP, a novel low-light image enhancement (LLIE) model based on deep image priors (DIPs). While DIP-based enhancement models are known for their zero-shot learning, their expensive computational cost remains a challenge. In addressing this issue, our proposed LoRaDIP introduces a low-rank adaptation technique, significantly reducing computational expenses without compromising performance. The contributions of this work are threefold. First, we eliminate the need for estimating initial illumination and reflectance, opting instead to directly estimate the illumination map from the observed image in a generative fashion. The illumination is parameterized by a DIP network. Second, considering the overparameterization of DIP networks, we introduce a low-rank adaptation technique to decrease the number of trainable parameters, thereby reducing computational demands. Third, differing from the existing DIP-based models that rely on a preset fixed number of iterations to halt the optimization process of Retinex decomposition, we propose an automatic stopping criterion based on stable rank, preventing unnecessary iterations. LoRaDIP not only inherits the advantage of requiring only the single input image but also exhibits reduced computational costs while maintaining or even surpassing the performance of state-of-the-art models.},
  archive      = {J_TAI},
  author       = {Zunjin Zhao and Daming Shi},
  doi          = {10.1109/TAI.2024.3499950},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {909-920},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {LoRaDIP: Low-rank adaptation with deep image prior for generative low-light image enhancement},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobjective optimization for traveling salesman problem: A deep reinforcement learning algorithm via transfer learning. <em>TAI</em>, <em>6</em>(4), 896-908. (<a href='https://doi.org/10.1109/TAI.2024.3499946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wide range of real applications can be modelled as the multiobjective traveling salesman problem (MOTSP), one of typical combinatorial optimization problems. Meta-heuristics can be used to address MOTSP. However, due to involving iteratively searching large solution space, they often entail significant computation time. Recently, deep reinforcement learning (DRL) algorithms have been employed in generating approximate optimal solutions to the single objective traveling salesman problems, as well as MOTSPs. This study proposes a multiobjective optimization algorithm based on DRL, called multiobjective pointer network (MOPN), where the input structure of the pointer network is redesigned to be applied to MOTSP. Furthermore, a training strategy utilizing a representative model and transfer learning is introduced to enhance the performance of MOPN. The proposed MOPN is insensitive to problem scale, meaning that a trained MOPN can address MOTSPs with different scales. Compared to meta-heuristics, MOPN takes much less time on forward propagation to obtain the pareto front. To verify the performance of our model, extensive experiments are conducted on three different MOTSPs to compare the MOPN with two state-of-the-art DRL models and two multiobjective meta-heuristics. Experimental results demonstrate that the proposed MOPN obtains the best solution with the least training time among all the compared DRL methods.},
  archive      = {J_TAI},
  author       = {Le-yang Gao and Rui Wang and Zhao-hong Jia and Chuang Liu},
  doi          = {10.1109/TAI.2024.3499946},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {896-908},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiobjective optimization for traveling salesman problem: A deep reinforcement learning algorithm via transfer learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FAST: Feature aware similarity thresholding for weak unlearning in black-box generative models. <em>TAI</em>, <em>6</em>(4), 885-895. (<a href='https://doi.org/10.1109/TAI.2024.3499939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heightened emphasis on the regulation of deep generative models, propelled by escalating concerns pertaining to privacy and compliance with regulatory frameworks, underscores the imperative need for precise control mechanisms over these models. This urgency is particularly underscored by instances in which generative models generate outputs that encompass objectionable, offensive, or potentially injurious content. In response, machine unlearning has emerged to selectively forget specific knowledge or remove the influence of undesirable data subsets from pretrained models. However, modern machine unlearning approaches typically assume access to model parameters and architectural details during unlearning, which is not always feasible. In multitude of downstream tasks, these models function as black-box systems, with inaccessible pretrained parameters, architectures, and training data. In such scenarios, the possibility of filtering undesired outputs becomes a practical alternative. Our proposed method feature aware similarity thresholding (FAST) effectively suppresses undesired outputs by systematically encoding the representation of unwanted features in the latent space. We employ user-marked positive and negative samples to guide this process, leveraging the latent space's inherent capacity to capture these undesired representations. During inference, we use this identified representation in the latent space to compute projection similarity metrics with newly sampled latent vectors. Subsequently, we meticulously apply a threshold to exclude undesirable samples from the output.},
  archive      = {J_TAI},
  author       = {Subhodip Panda and A.P. Prathosh},
  doi          = {10.1109/TAI.2024.3499939},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {885-895},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FAST: Feature aware similarity thresholding for weak unlearning in black-box generative models},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Active robust adversarial reinforcement learning under temporally coupled perturbations. <em>TAI</em>, <em>6</em>(4), 874-884. (<a href='https://doi.org/10.1109/TAI.2024.3499938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robust reinforcement learning (RL) aims to improve the generalization of agents under model mismatch. As a major branch of robust RL, adversarial approaches formulate the problem as a zero-sum game in which adversaries seek to apply worst case perturbations to the dynamics. However, the potential constraints of adversarial perturbations are seldom addressed in existing approaches. In this article, we consider temporally coupled settings, where adversarial perturbations change continuously at a bounded rate. This kind of constraint can commonly arise in a variety of real-world situations (e.g., changes in wind speed and ocean currents). We propose a novel robust RL approach, named active robust adversarial RL (ARA-RL), that tackles this problem in an adversarial architecture. First, we introduce a type of RL adversary that generates temporally coupled perturbations on agent actions. Then, we embed a diagnostic module in the RL agent, enabling it to actively detect temporally coupled perturbations in unseen environments. Through adversarial training, the agent seeks to maximize its worst case performance and thus achieve robustness under perturbations. Finally, extensive experiments demonstrate that our proposed approach provides significant robustness against temporally coupled perturbations and outperforms other baselines on several continuous control tasks.},
  archive      = {J_TAI},
  author       = {Jiacheng Yang and Yuanda Wang and Lu Dong and Lei Xue and Changyin Sun},
  doi          = {10.1109/TAI.2024.3499938},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {874-884},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Active robust adversarial reinforcement learning under temporally coupled perturbations},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HGFF: A deep reinforcement learning framework for lifetime maximization in wireless sensor networks. <em>TAI</em>, <em>6</em>(4), 859-873. (<a href='https://doi.org/10.1109/TAI.2024.3497926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning the movement of the sink to maximize the lifetime in wireless sensor networks (WSNs) is an essential problem. Many existing mobile sink techniques based on mathematical programming or heuristics have demonstrated the feasibility of the task. Nevertheless, the huge computational cost or the over-reliance on human knowledge can result in relatively low performance. To balance the need for high-quality solutions to minimize inference time, we propose a new framework to construct the movement path of the sink automatically. We cast the lifetime maximization problem as an optimization task within a heterogeneous graph and learn movement policy for the sink by combining graph neural network (GNN) with deep reinforcement learning. Our approach comprises three key modules: 1) a heterogeneous GNN to learn representations of sites and sensors by aggregating features of neighbor nodes and extracting hierarchical graph features; 2) a multihead attention mechanism that allows the sites to attend to information from sensor nodes, which highly improves the expressive capacity of the learning model; and 3) a greedy policy that learns to append the next best site in the solution incrementally. We design twelve types of static and dynamic maps to simulate different WSNs in the real world, and extensive experiments are conducted to evaluate and analyze our approach. The empirical results show that our approach consistently outperforms the existing methods on all types of maps. Notably, our approach significantly extends the simulation lifetime without sacrificing a large increase in inference time.},
  archive      = {J_TAI},
  author       = {Xiaoxu Han and Xin Mu and Jinghui Zhong},
  doi          = {10.1109/TAI.2024.3497926},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {859-873},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {HGFF: A deep reinforcement learning framework for lifetime maximization in wireless sensor networks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing hierarchical relationships and quality of embedding in latent space. <em>TAI</em>, <em>6</em>(4), 843-858. (<a href='https://doi.org/10.1109/TAI.2024.3497921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing learning models partition the generated representations using hyperplanes which form well defined groups of similar embeddings that is uniquely mapped to a particular class. However, in practical applications, the embedding space does not form distinct boundaries to segregate the class representations. There exists interaction among similar classes which cannot be visually determined in high-dimensional space. Moreover, the structure of the latent space remains obscure. As learned representations are frequently reused to reduce the inference time, it is important to analyse how semantically related classes interact among themselves in the latent space. Therefore, we propose a boundary estimation algorithm that minimises the inclusion of other classes in the embedding space to form groups of similar representations and compare the quality of these class embeddings for various models in an already encoded space. These groups are overlapping to denote ambiguous embeddings that cannot be mapped to a particular class with high confidence. The algorithm determines which representations to be included or discarded to form well defined regions, separating discriminating, ambiguous and rejected embeddings to depict a particular class. Later, we construct relation trees to evaluate the hierarchical relationships formed among the classes, and compare it with the WordNet ontology using phylogenetic tree comparison methods.},
  archive      = {J_TAI},
  author       = {Ankita Chatterjee and Jayanta Mukherjee and Partha Pratim Das},
  doi          = {10.1109/TAI.2024.3497921},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {843-858},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Analyzing hierarchical relationships and quality of embedding in latent space},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Safe multiagent reinforcement learning with bilevel optimization in autonomous driving. <em>TAI</em>, <em>6</em>(4), 829-842. (<a href='https://doi.org/10.1109/TAI.2024.3497919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring safety in multiagent reinforcement learning (MARL), particularly when deploying it in real-world applications such as autonomous driving, emerges as a critical challenge. To address this challenge, traditional safe MARL methods extend MARL approaches to incorporate safety considerations, aiming to minimize safety risk values. However, these safe MARL algorithms often fail to model other agents and lack convergence guarantees, particularly in dynamically complex environments. In this study, we propose a safe MARL method grounded in a Stackelberg model with bilevel optimization, for which convergence analysis is provided. Derived from our theoretical analysis, we develop two practical algorithms, namely constrained Stackelberg Q-learning (CSQ) and constrained Stackelberg multiagent deep deterministic policy gradient (CS-MADDPG), designed to facilitate MARL decision-making in some simulated autonomous driving applications such as traffic management. To evaluate the effectiveness of our algorithms, we developed a safe MARL autonomous driving benchmark and conducted experiments on challenging autonomous driving scenarios, such as merges, roundabouts, intersections, and racetracks. The experimental results indicate that our algorithms, CSQ and CS-MADDPG, outperform several strong MARL baselines, such as Bi-AC, MACPO, and MAPPO-L, regarding reward and safety performance.},
  archive      = {J_TAI},
  author       = {Zhi Zheng and Shangding Gu},
  doi          = {10.1109/TAI.2024.3497919},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {829-842},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Safe multiagent reinforcement learning with bilevel optimization in autonomous driving},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generation with nuanced changes: Continuous image-to-image translation with adversarial preferences. <em>TAI</em>, <em>6</em>(4), 816-828. (<a href='https://doi.org/10.1109/TAI.2024.3497915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most previous methods for continuous image-to-image translation resorted to binary attributes with restrictive description ability and thus cannot achieve satisfactory performance. Some works proposed to use fine-grained semantic information, relative attributes (RAs), preferences over pairs of images on the strength of a specified attribute. However, they still failed to reconcile both goals for smooth translation and for high-quality generation simultaneously. In this work, we propose a new model continuous translation via adversarial preferences (CTAP) to coordinate these two goals for high-quality continuous translation based on RAs. In CTAP, we simultaneously train two modules: a generator that translates an input image to the desired image with smooth nuanced changes w.r.t. the interested attributes; and a ranker that executes adversarial preferences consisting of the input image and the desired image. Particularly, adversarial preferences involve an adversarial ranking process: 1) the ranker thinks no difference between the desired image and the input image in terms of the interested attributes; 2) the generator fools the ranker to believe the attributes of its output image changes as expect compared with the input image. RAs over pairs of real images are introduced to guide the ranker to rank image pairs regarding the interested attributes only. With an effective ranker, the generator would “win” the adversarial game by producing high-quality images that present smooth changes. The experiments on two face datasets and one shoe dataset demonstrate that our CTAP achieves state-of-art results in generating high-fidelity images which exhibit smooth changes over the interested attributes.},
  archive      = {J_TAI},
  author       = {Yinghua Yao and Yuangang Pan and Ivor W. Tsang and Xin Yao},
  doi          = {10.1109/TAI.2024.3497915},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {816-828},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Generation with nuanced changes: Continuous image-to-image translation with adversarial preferences},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial masked autoencoders are robust vision learners. <em>TAI</em>, <em>6</em>(4), 805-815. (<a href='https://doi.org/10.1109/TAI.2024.3497912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised learning, specifically masked image modeling, has achieved significant success, surpassing earlier contrastive learning methods. However, the robustness of these methods against adversarial attacks, which subtly manipulate inputs to mislead models, remains largely unexplored. This study investigates the adversarial robustness of self-supervised learning methods, exposing their vulnerabilities to various adversarial attacks. We introduce adversarial masked autoencoders (AMAEs), a novel framework designed to enforce adversarial robustness during the masked image modeling process. Through extensive experiments on four classification benchmarks involving eight different adversarial attacks, we demonstrate that AMAE consistently outperforms seven state-of-the-art baseline self-supervised learning methods in terms of adversarial robustness.},
  archive      = {J_TAI},
  author       = {Yuchong Yao and Nandakishor Desai and Marimuthu Palaniswami},
  doi          = {10.1109/TAI.2024.3497912},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {805-815},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adversarial masked autoencoders are robust vision learners},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistent counterfactual explanations via anomaly control and data coherence. <em>TAI</em>, <em>6</em>(4), 794-804. (<a href='https://doi.org/10.1109/TAI.2024.3496616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic recourses are popular methods to provide individuals impacted by machine learning models with recommendations on feasible actions for a more favorable prediction. Most of the previous algorithmic recourse methods work under the assumption that the predictive model does not change over time. However, in reality, models in deployment may both be periodically retrained and have their architecture changed. Therefore, it is desirable that the recourse should remain valid when such a model update occurs, unless new evidence arises. We call this feature consistency. This article presents anomaly control and data coherence (ACDC), a novel model-agnostic recourse method that generates counterfactual explanations, i.e., instance-level recourses. ACDC is inspired by anomaly detection methods and uses a one-class classifier to aid the search for valid, consistent, and feasible counterfactual explanations. The one-class classifier asserts that the generated counterfactual explanations lie on the data manifold and are not outliers of the target class. We compare ACDC against several state-of-the-art recourse methods across four datasets. Our experiments show that ACDC outperforms baselines both in generating consistent counterfactual explanations, and in generating feasible and plausible counterfactual explanations, while still having proximity measures similar to the baseline methods targeting the data manifold.},
  archive      = {J_TAI},
  author       = {Maria Movin and Federico Siciliano and Rui Ferreira and Fabrizio Silvestri and Gabriele Tolomei},
  doi          = {10.1109/TAI.2024.3496616},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {4},
  number       = {4},
  pages        = {794-804},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Consistent counterfactual explanations via anomaly control and data coherence},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDM-lag: A diffusion-based decision-making model for autonomous vehicles with lagrangian safety enhancement. <em>TAI</em>, <em>6</em>(3), 780-791. (<a href='https://doi.org/10.1109/TAI.2024.3497918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision-making stands as a pivotal component in the realm of autonomous vehicles (AVs), playing a crucial role in navigating the intricacies of autonomous driving. Amid the evolving landscape of data-driven methodologies, enhancing decision-making performance in complex scenarios has emerged as a prominent research focus. Despite considerable advancements, current learning-based decision-making approaches exhibit potential for refinement, particularly in aspects of policy articulation and safety assurance. To address these challenges, we introduce DDM-Lag, a diffusion decision model, augmented with Lagrangian-based safety enhancements. This work conceptualizes the sequential decision-making challenge inherent in autonomous driving as a problem of generative modeling, adopting diffusion models as the medium for assimilating patterns of decision-making. We introduce a hybrid policy update strategy for diffusion models, amalgamating the principles of behavior cloning and Q-learning, alongside the formulation of an actor–critic architecture for the facilitation of updates. To augment the model's exploration process with a layer of safety, we incorporate additional safety constraints, employing a sophisticated policy optimization technique predicated on Lagrangian relaxation to refine the policy learning endeavor comprehensively. Empirical evaluation of our proposed decision-making methodology was conducted across a spectrum of driving tasks, distinguished by their varying degrees of complexity and environmental contexts. The comparative analysis with established baseline methodologies elucidates our model's superior performance, particularly in dimensions of safety and holistic efficacy.},
  archive      = {J_TAI},
  author       = {Jiaqi Liu and Peng Hang and Xiaocong Zhao and Jianqiang Wang and Jian Sun},
  doi          = {10.1109/TAI.2024.3497918},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {780-791},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {DDM-lag: A diffusion-based decision-making model for autonomous vehicles with lagrangian safety enhancement},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive neural network finite-time event triggered intelligent control for stochastic nonlinear systems with time-varying constraints. <em>TAI</em>, <em>6</em>(3), 773-779. (<a href='https://doi.org/10.1109/TAI.2024.3497913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite-time command-filter event-trigger control based on adaptive neural network is presented in this article for a class of output-feedback stochastic nonlinear system (SNS) with output time-varying constraints and unmeasured states. The adaptive neural network combined with backstepping is utilized to approximate the unknown nonlinear functions of the system. The finite-time command-filter is employed to reduce the difficulty of complex calculation caused by backstepping technique. An adaptive observer is developed to estimate unmeasured states, and a controller is designed to be triggered only when the event-triggered condition is met. The time-varying barrier Lyapunov function is utilized to ensure the output time-varying constraint. The control method proposed in this article not only guarantees the finite-time stability of the system but also meets the output constraint. The effectiveness of the method is demonstrated on the ship maneuvering system with three degrees of freedom.},
  archive      = {J_TAI},
  author       = {Jia Liu and Jiapeng Liu and Qing-Guo Wang and Jinpeng Yu},
  doi          = {10.1109/TAI.2024.3497913},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {773-779},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive neural network finite-time event triggered intelligent control for stochastic nonlinear systems with time-varying constraints},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-level neural-RL-based approach for hierarchical multiplayer systems under mismatched uncertainties. <em>TAI</em>, <em>6</em>(3), 759-772. (<a href='https://doi.org/10.1109/TAI.2024.3493833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI and reinforcement learning (RL) have attracted great attention in the study of multiplayer systems over the past decade. Despite the advances, most of the studies are focused on synchronized decision-making to attain Nash equilibrium, where all the players take actions simultaneously. On the other hand, however, in complex applications, certain players may have an advantage in making sequential decisions and this situation introduces a hierarchical structure and influences how other players respond. The control design for such system is challenging since it relies on solving the coupled Hamilton–Jacobi equation. The situation becomes more difficult when the learning process is exposed to complex uncertainties with unreliable data being exchanged. Therefore, in this article, we develop a new learning-based control approach for a class of nonlinear hierarchical multiplayer systems subject to mismatched uncertainties. Specifically, we first formulate this new problem as a multiplayer Stackelberg–Nash game in conjunction with the hierarchical robust–optimal transformation. Theoretical analysis confirms the equivalence of this transformation and ensures that the designed control policies can achieve stable equilibrium. Then, a two-level neural-RL-based approach is developed to automatically and adaptively learn the solutions. The stability of this online learning process is also provided. Finally, two numerical examples are presented to demonstrate the effectiveness of the developed learning-based robust control design.},
  archive      = {J_TAI},
  author       = {Xiangnan Zhong and Zhen Ni},
  doi          = {10.1109/TAI.2024.3493833},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {759-772},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A two-level neural-RL-based approach for hierarchical multiplayer systems under mismatched uncertainties},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Swin-MGNet: Swin transformer based multiview grouping network for 3-D object recognition. <em>TAI</em>, <em>6</em>(3), 747-758. (<a href='https://doi.org/10.1109/TAI.2024.3492163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent developments in Swin Transformer have shown its great potential in various computer vision tasks, including image classification, semantic segmentation, and object detection. However, it is challenging to achieve desired performance by directly employing the Swin Transformer in multiview 3-D object recognition since the Swin Transformer independently extracts the characteristics of each view and relies heavily on a subsequent fusion strategy to unify the multiview information. This leads to the problem of the insufficient extraction of interdependencies between the multiview images. To this end, we propose an aggregation strategy integrated into the Swin Transformer to reinforce the connections between internal features across multiple views, thus leading to a complete interpretation of isolated features extracted by the Swin Transformer. Specifically, we utilize Swin Transformer to learn view-level feature representations from multiview images and then calculate their view discrimination scores. The scores are employed to assign the view-level features to different groups. Finally, a grouping and fusion network is proposed to aggregate the features from view and group levels. The experimental results indicate that our method attains state-of-the-art performance compared with prior approaches in multiview 3-D object recognition tasks. The source code is available at https://github.com/Qishaohua94/DEST.},
  archive      = {J_TAI},
  author       = {Xin Ning and Limin Jiang and Weijun Li and Zaiyang Yu and Jinlong Xie and Lusi Li and Prayag Tiwari and Fernando Alonso-Fernandez},
  doi          = {10.1109/TAI.2024.3492163},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {747-758},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Swin-MGNet: Swin transformer based multiview grouping network for 3-D object recognition},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced LiDAR-based localization via multiresolution iterative closest point algorithms and feature extraction. <em>TAI</em>, <em>6</em>(3), 738-746. (<a href='https://doi.org/10.1109/TAI.2024.3491950'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle localization is a critical component in autonomous driving systems, and light detection and ranging (LiDAR)-based methods have become increasingly popular for this task. In this article, we present a novel vehicle localization approach based on the point cloud map generated from LiDAR data. In particular, our approach first uses semantic segmentation and feature point extraction techniques to create an efficient feature point map and a long-lasting map from LiDAR sequences with corresponding poses. We then introduce a map-based online localization method that achieves precise vehicle localization using both LiDAR scans and the two point cloud maps, along with a multiresolution ICP strategy. Comprehensive evaluations are conducted on the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) odometry dataset and the collected results demonstrate superior performance over the existing literature in both odometry metrics and absolute translation error. Our multiresolution iterative closest point (ICP)-based method holds significant potential for map-based vehicle localization, offering promising prospects for application in autonomous driving and associated domains.},
  archive      = {J_TAI},
  author       = {Yecheng Lyu and Xinkai Zhang and Feng Tao},
  doi          = {10.1109/TAI.2024.3491950},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {738-746},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Enhanced LiDAR-based localization via multiresolution iterative closest point algorithms and feature extraction},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep feature unsupervised domain adaptation for time-series classification. <em>TAI</em>, <em>6</em>(3), 725-737. (<a href='https://doi.org/10.1109/TAI.2024.3491948'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) for time series classification (TSC) is an important but challenging task. In the process of UDA, feature learning is most critical. Most of the existing works in this area are based on learning domain-invariant feature representation of data with help of some restriction such as MMD. However, they ignored that the mutual effects between the pretrained network and the downstream target network was also conducive to the learning of domain-invariant features. In this article, we propose a deep feature unsupervised domain adaptation (DFUDA) for time series classification. First, we pretrain a network based on consistency learning to ensure the invariant feature extraction from the source and target domains. Then, we propose an end-to-end unsupervised domain adaptation, which includes the layer matching and the unsupervised domain adaptation to promote more confident knowledge transfer. Finally, the pretrained network receives feedback of the domain adaptation's performance. To verify the effectiveness of the proposed method, we perform the comprehensive experiments on fault diagnosis datasets and human activity recognition datasets. The results show that DFUDA outperforms the state of the arts methods for both scenarios.},
  archive      = {J_TAI},
  author       = {Nannan Lu and Tong Yan and Song Zhu and Jiansheng Qian and Min Han},
  doi          = {10.1109/TAI.2024.3491948},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {725-737},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep feature unsupervised domain adaptation for time-series classification},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiattribute deep CNN-based approach for detecting medicinal plants and their use for skin diseases. <em>TAI</em>, <em>6</em>(3), 710-724. (<a href='https://doi.org/10.1109/TAI.2024.3491938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skin health is a critical concern for humans, especially in geographical areas where environmental conditions and lifestyle factors adversely affect their condition, leading to a prevalence of skin diseases. This issue is exacerbated in rural regions, like parts of India, where a notable dermatologist shortage exists, leading to overlooked skin diseases. In response, the use of medicinal plants for dermatological purposes has been a longstanding tradition. However, traditional plant identification often relies on a single attribute, such as leaves or flowers, which can be unreliable due to seasonal variations. This article introduces a novel approach for accurately identifying medicinal plants using a multiattribute deep convolutional neural network. This approach aims to bridge the gap in healthcare access by empowering individuals to recognize and utilize these plants effectively. Our objective is to develop a robust deep CNN model trained on a diverse dataset of images encompassing leaves, trunks, and seeds of medicinal plants associated with skin health. Our findings demonstrate that the model achieves high accuracy in plant identification, effectively addressing the limitations of single-attribute methods. This research not only contributes to the field of medicinal plant classification but also empowers individuals to make informed decisions about their skin health while preserving valuable traditional knowledge.},
  archive      = {J_TAI},
  author       = {Prachi Dalvi and Dhananjay R. Kalbande and Surendra Singh Rathod and Harshal Dalvi and Amey Agarwal},
  doi          = {10.1109/TAI.2024.3491938},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {710-724},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiattribute deep CNN-based approach for detecting medicinal plants and their use for skin diseases},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NL-CoWNet: A deep convolutional Encoder–Decoder architecture for OCT speckle elimination using nonlocal and subband modulated DT-CWT blocks. <em>TAI</em>, <em>6</em>(3), 700-709. (<a href='https://doi.org/10.1109/TAI.2024.3491935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical coherence tomography (OCT), a noninvasive diagnostic technology for identifying and treating various ocular diseases, encounters a loss of image quality due to the introduction of speckles during the image creation process, compromising the precision of disease diagnosis. Researchers have proposed numerous deep convolutional networks to address speckle artifacts in OCT images. This article presents a novel deep convolutional encoder–decoder framework called NL-CoWNet for speckle elimination in OCT images. This despeckling architecture consists of an encoder network having the topology of ResNet34, whose certain feature vectors are passed through nonlocal (NL) neural network blocks and a novel subband modulated dual-tree complex wavelet (CoW) transform (DT-CWT) blocks, followed by a decoder unit with upsampling layers and channel-wise squeeze and excitation (CSE) convolutional blocks. Our network architecture has been validated after numerous ablation studies. Qualitative and quantitative assessments with contemporary and established methodologies have proven that NL-CoWNet excels conspicuously in speckle removal while preserving the structural features of the image.},
  archive      = {J_TAI},
  author       = {P. S. Arun and Bibin Francis and Varun P. Gopi},
  doi          = {10.1109/TAI.2024.3491935},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {700-709},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {NL-CoWNet: A deep convolutional Encoder–Decoder architecture for OCT speckle elimination using nonlocal and subband modulated DT-CWT blocks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semisupervised breast MRI density segmentation integrating fine and rough annotations. <em>TAI</em>, <em>6</em>(3), 690-699. (<a href='https://doi.org/10.1109/TAI.2024.3491693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces an enhanced teacher–student model featuring a novel Vnet architecture that integrates high-pass and low-pass filters to improve the segmentation of breast magnetic resonance imaging (MRI) images. The model effectively utilizes finely annotated, roughly annotated, and unannotated data to achieve precise breast tissue density segmentation. The teacher–student framework incorporates three specialized Vnet networks, each tailored to different types of annotations. By integrating cosine contrast loss functions between finely and roughly annotated models, and innovatively applying high-pass and low-pass filters within the Vnet architecture, the segmentation performance is significantly enhanced. This hybrid filtering approach enables the model to capture both fine-grained and coarse structural details, leading to more accurate segmentation across various MRI image datasets. Experimental results demonstrate the superiority of the proposed method, achieving Dice values of 0.833 on the finely annotated Shenzhen dataset and 0.780 on the Duke dataset, using 15 finely annotated, 15 roughly annotated, and 58 unlabeled samples provided by Shenzhen People's Hospital. These findings underscore its potential clinical application in breast density assessment.},
  archive      = {J_TAI},
  author       = {Tianyu Xie and Yue Sun and Hongxu Yang and Shuo Li and Jinhong Song and Qimin Yang and Hao Chen and Mingxiang Wu and Tao Tan},
  doi          = {10.1109/TAI.2024.3491693},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {690-699},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Semisupervised breast MRI density segmentation integrating fine and rough annotations},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-heterogeneous federated graph learning with prototype propagation. <em>TAI</em>, <em>6</em>(3), 676-689. (<a href='https://doi.org/10.1109/TAI.2024.3490557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated graph learning (FGL) enables clients to collaboratively train a robust graph neural network (GNN) while ensuring their private graph data never leaves the local. However, existing FGL frameworks require all clients to train the identical GNN model, which limits their real-world applicability. Although many model-heterogenous frameworks have been proposed for traditional nongraph federated learning settings, directly transferring them to the FGL setting typically results in suboptimal performance. To fill the gap, this article presents federated prototype propagation network (FedPPN), a lightweight FGL framework that supports clients to train fully customized models. FedPPN only transmits prototypes between clients and the server for knowledge sharing. The core idea is propagating global prototypes on each client's local graph, generating prototype-based node representations and predictions. The prototype-based prediction can then be ensembled with the prediction of local GNN, allowing clients to achieve accurate prediction. We evaluate our FedPPN on six benchmark datasets with different heterogeneous model setups. Experimental results show that our FedPPN outperforms advanced baselines in model accuracy without adding any trainable parameters on clients or the server. Besides, FedPPN's communication cost is significantly lower than methods that rely on model parameter exchange.},
  archive      = {J_TAI},
  author       = {Zhi Liu and Hanlin Zhou and Xiaohua He and Haopeng Yuan and Jiaxin Du and Mengmeng Wang and Guojiang Shen and Xiangjie Kong and Feng Xia},
  doi          = {10.1109/TAI.2024.3490557},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {676-689},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Model-heterogeneous federated graph learning with prototype propagation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging the climate gap: Multimodel framework with explainable decision-making for IOD and ENSO forecasting. <em>TAI</em>, <em>6</em>(3), 661-675. (<a href='https://doi.org/10.1109/TAI.2024.3489535'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate forecasting of the Indian Ocean Dipole (IOD) and El-Niño-Southern Oscillation (NINO3.4) is crucial for understanding regional weather patterns in the Indian subcontinent. Addressing the challenges associated with IOD and NINO3.4 prediction, a robust multitask autoregressive deep learning (DL) model is introduced for precise forecasting of these indices and key grid projections sea surface temperature (SST), surface-level pressure gradient (SLG), and horizontal wind velocity (U-Comp) over a short to mid-term window (20 months). Utilizing spatiotemporal (SST, SLG, U-Comp) and temporal (IOD and NINO3.4) modalities, the proposed model predicts future IOD and NINO3.4, as well as SST, SLG, and U-Comp, in an autoregressive scheme. The multitask learning component regularizes the model, effectively capturing the evolving dynamics of global patterns conditioned on IOD and NINO3.4. The comprehensive evaluation explores various task settings, including a duo-setting that predicts IOD or NINO3.4 with spatiotemporal information, showcasing notable proficiency. In a multitask environment, where both temporal IOD, NINO3.4, and spatiotemporal SST, SLG, U-Comp are predicted, the model successfully forecasts IOD and NINO3.4 indices alongside grid projections with modest accuracy in root mean square error (RMSE). To enhance the model's interpretability regarding spatiotemporal dynamics, a tailored version of Grad-CAM is employed, providing critical insights for climate prediction. This research advances climate prediction models, offering a comprehensive framework with significant implications for informed decision-making in the Indian subcontinent's climatic context.},
  archive      = {J_TAI},
  author       = {Harshit Tiwari and Prashant Kumar and Ramakant Prasad and Kamlesh Kumar Saha and Anurag Singh and Hocine Cherifi and Rajni},
  doi          = {10.1109/TAI.2024.3489535},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {661-675},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Bridging the climate gap: Multimodel framework with explainable decision-making for IOD and ENSO forecasting},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network-based ensemble learning model to identify antigenic fragments of SARS-CoV-2. <em>TAI</em>, <em>6</em>(3), 651-660. (<a href='https://doi.org/10.1109/TAI.2024.3487149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of epitope-based vaccines (EBVs) necessitates the identification of antigenic fragments (AFs) of the target pathogen known as T-cell epitopes (TCEs). TCEs are recognized by immune system, specifically by T cells, B cells, and antibodies. Traditional wet lab methods for identifying TCEs are often costly, challenging, and time-consuming compared to computational approaches. In this study, we propose a neural network-based ensemble machine learning (ML) model trained on physicochemical properties of SARS-CoV-2 peptides sequences to predict TCE sequences. The performance of the model assessed using test dataset demonstrated an accuracy of >95%, surpassing the results of other ML classifiers that were employed for comparative analysis. Through fivefold cross-validation technique, a mean accuracy of approximately 95% was reported. Additionally, when compared to other existing TCE prediction methods using a blind dataset, the proposed model was found to be more accurate and effective. The predicted epitopes may have a strong probability to act as potential vaccine candidates. Nonetheless, it is imperative to subject these epitopes to further scientific examination both in vivo and in vitro, to confirm their suitability as vaccine candidates.},
  archive      = {J_TAI},
  author       = {Syed Nisar Hussain Bukhari and Kingsley A. Ogudo},
  doi          = {10.1109/TAI.2024.3487149},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {651-660},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Neural network-based ensemble learning model to identify antigenic fragments of SARS-CoV-2},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Att2CPC: Attention-guided lossy attribute compression of point clouds. <em>TAI</em>, <em>6</em>(3), 639-650. (<a href='https://doi.org/10.1109/TAI.2024.3486676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the great progress of three-dimensional (3-D) sensing and acquisition technology, the volume of point cloud data has grown dramatically, which urges the development of efficient point cloud compression methods. In this article, we focus on the task of learned lossy point cloud attribute compression (PCAC). We propose an efficient attention-based method for lossy compression of point cloud attributes leveraging on an autoencoder architecture. Specifically, at the encoding side, we conduct multiple downsampling to best exploit the local attribute patterns, in which effective external cross attention (ECA) is devised to hierarchically aggregate features by intergrating attributes and geometry contexts. At the decoding side, the attributes of the point cloud are progressively reconstructed based on the multiscale representation and the zero-padding upsampling tactic. To the best of our knowledge, this is the first approach to introduce attention mechanism to point-based lossy PCAC task. We verify the compression efficiency of our model on various sequences, including human body frames, sparse objects, and large-scale point cloud scenes. Experiments show that our method achieves an average improvement of 1.15 and 2.13 dB in Bjontegaard delta (BD)-peak signal-to-noise ratio (BD-PSNR) of Y channel and YUV channel, respectively, when comparing with the state-of-the-art point-based method deep-PCAC. Codes of this article are available at https://github.com/I2-Multimedia-Lab/Att2CPC.},
  archive      = {J_TAI},
  author       = {Kai Liu and Kang You and Pan Gao and Manoranjan Paul},
  doi          = {10.1109/TAI.2024.3486676},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {639-650},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Att2CPC: Attention-guided lossy attribute compression of point clouds},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NICASU: Neurotransmitter inspired cognitive AI architecture for surveillance underwater. <em>TAI</em>, <em>6</em>(3), 626-638. (<a href='https://doi.org/10.1109/TAI.2024.3486675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain is exceedingly good at learning rich narratives from highly limited experiences. One of the ways this is achieved in our brain is through neuromodulators or neurotransmitters, such as dopamine and nor-epinephrine, in cortical circuits. In terms of symbolic processing, these neuromodulators add “salience” to various emotions and experiences. A salience-based neural network (SANN) architecture was proposed in [1]. We have taken this architecture and have developed a discriminator to enable efficient change detection for underwater applications. In the context of underwater, surveillance can be elucidated as one of the processes of detecting and tracking the moving objects present in underwater videos. Several researchers working on the same tried to develop different techniques for identifying moving objects from outdoor scenes. However, while applying the same for underwater environments, it is found to be unable to preserve the minute details that are important for defining an object's boundary. This is mainly due to the complex scene dynamics of the aquatic environment. Moreover, the intricate natural properties of water and some of its characteristics, such as excessive turbidity, scattering, and low visibility, also make the task of detecting the object present in underwater videos extremely challenging. In this regard, we put forth an adversarial learning-based end-to-end deep learning architecture inspired by the way neurotransmitters work in the human brain to detect underwater moving objects. The proposed architecture uses two modules for underwater object detection. The initial module is a generator composed of a probabilistic learner which is based on multiple down- and up-sampling modules. Further, the discriminator network is composed of a multilevel feature-concatenation component, which can perpetuate specifics at distinct levels. The effectiveness of the proposed method (PM) is confirmed using the underwater change detection and Fish4Knowledge benchmark datasets by contrasting its outcomes with those of different state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Mehvish Nissar and Badri Narayan Subudhi and Amit Kumar Mishra and Vinit Jakhetiya},
  doi          = {10.1109/TAI.2024.3486675},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {626-638},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {NICASU: Neurotransmitter inspired cognitive AI architecture for surveillance underwater},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image tampering detection with frequency-aware attention and multiview fusion. <em>TAI</em>, <em>6</em>(3), 614-625. (<a href='https://doi.org/10.1109/TAI.2024.3486671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Manipulated images are flooding our daily lives, which poses a threat to social security. Recently, many studies have focused on image tampering detection. However, they have poor performance on independent validation due to differences in image scenes and tampering methods. The key question is how to design a network that is able to adaptively enhance the tampering information and suppress the generalization features during training. To this end, we propose a dual-branch network with a frequency adaptation paradigm and a feature fusion module for robust tampering image detection. First, this paradigm is designed to adaptively highlight tampering features through frequency conversion and learnable weight. Second, a feature fusion module is developed to filter redundant features and dynamically fuse two-branch features. Experiments on eight typical datasets demonstrate that our model has advantages over state-of-the-art algorithms, and our paradigm can well empower semantic segmentation networks for tampering detection.},
  archive      = {J_TAI},
  author       = {Xu Xu and Junxin Chen and Wenrui Lv and Wei Wang and Yushu Zhang},
  doi          = {10.1109/TAI.2024.3486671},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {614-625},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Image tampering detection with frequency-aware attention and multiview fusion},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDA-GAN: Multiscale and dual attention generative adversarial network for bone suppression in chest X-rays. <em>TAI</em>, <em>6</em>(3), 604-613. (<a href='https://doi.org/10.1109/TAI.2024.3483731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bone structure in a chest x-ray creates trouble for a radiologist to examine the organs, manifestation of disease, and hidden tiny abnormalities. Bone suppression in chest x-rays allows better examination of lung fields. This has the potential to improve diagnostic accuracy. Dual-energy subtraction imaging is a standard bone suppression technique that delivers a higher dose of radiation and requires specific hardware. This article proposes a novel multiscale and dual attention-guided generative adversarial network (MDA-GAN) to transform chest x-rays into bone-suppressed x-rays in an unsupervised manner. We incorporate a spatial attention module to generate attention maps that were further concatenated with the coarsely generated bone segmentation mask. This dual attention is introduced to the generator at multiple scales in between the skip connection of the encoder and decoder layer. The proposed dual attention multiscale mechanism helps the generator to learn that only bones need to be removed on the chest x-ray without tempering the remaining parts. The proposed MDA-GAN is trained with adversarial loss combined with deep supervised cycle consistency and structure similarity for unpaired training images. We employ supervision heads in all the decoder layers to convert the activation maps into an output comparable to the scaled-down images and minimize the cycle consistency loss in a deep supervised manner. Experiments are conducted on an unpaired dataset including the public and our in-house Indian dataset and results show that incorporating dual attention at multiple scales and deep cycle consistency in translation networks significantly improves the quality of bone-suppressed images. (https://github.com/rB080/ribsup.git.)},
  archive      = {J_TAI},
  author       = {Anushikha Singh and Rukhshanda Hussain and Rajarshi Bhattacharya and Brejesh Lall and B.K. Panigrahi and Anjali Agrawal and Anurag Agrawal and Balamugesh Thangakunam and D.J. Christopher},
  doi          = {10.1109/TAI.2024.3483731},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {604-613},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {MDA-GAN: Multiscale and dual attention generative adversarial network for bone suppression in chest X-rays},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prompt learning for few-shot question answering via self-context data augmentation. <em>TAI</em>, <em>6</em>(3), 589-603. (<a href='https://doi.org/10.1109/TAI.2024.3483201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pretrained language models (PLMs) have shown remarkable performance on question answering (QA) tasks, but they usually require fine-tuning (FT) that depends on a substantial quantity of QA pairs. Therefore, improving the performance of PLMs in scenarios with only a small number of training examples, also known as a few-shot setting, is of great practical significance. Current mitigation strategies for the few-shot QA task largely rely on pretraining a QA task-specific language model from scratch, overlooking the potential of foundational PLMs to generate QA pairs, particularly in the few-shot setting. To address this issue, we propose a prompt-based QA data augmentation method aimed at automating the creation of high-quality QA pairs. It employs the PFT method, adapting the question generation process of PLMs to the few-shot setting. Additionally, we introduce a dynamic text filling training strategy. This strategy simulates the progressive human learning process, thereby alleviating overfitting of PLMs in the few-shot setting and enhancing their reasoning capability to tackle complex questions. Extensive experiments demonstrate that the proposed method outperforms existing approaches across various few-shot configurations.},
  archive      = {J_TAI},
  author       = {Jian-Qiang Qiu and Chun-Yang Zhang and C. L. Philip Chen},
  doi          = {10.1109/TAI.2024.3483201},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {589-603},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Prompt learning for few-shot question answering via self-context data augmentation},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized learning path problem variations: Computational complexity and AI approaches. <em>TAI</em>, <em>6</em>(3), 574-588. (<a href='https://doi.org/10.1109/TAI.2024.3483190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {E-learning courses often suffer from high dropout rates and low student satisfaction. One way to address this issue is to use personalized learning paths (PLPs), which are sequences of learning materials that meet the individual needs of students. However, creating PLPs is difficult and often involves combining knowledge graphs (KGs), student profiles, and learning materials. Researchers typically assume that the problem of creating PLPs belong to the nondeterministic polynomial (NP)-hard class of computational problems. However, previous research in this field has neither defined the different variations of the PLP problem nor formally established their computational complexity. Without clear definitions of the PLP variations, researchers risk making invalid comparisons and conclusions when they use different metaheuristics for different PLP problems. To unify this conversation, this article formally proves the NP-completeness of two common PLP variations and their generalizations and uses them to categorize recent research in the PLP field. It then presents an instance of the PLP problem using real-world data and shows how this instance can be cast into two different NP-complete variations. This article then presents three artificial intelligence (AI) strategies, solving one of the PLP variations with back-tracking and branch and bound heuristics and also converting the PLP variation instance to XCSP${}^{3}$, an intermediate constraint satisfaction language to be resolved with a general constraint optimization solver. This article solves the other PLP variation instance using a greedy search heuristic. The article finishes by comparing the results of the two different PLP variations.},
  archive      = {J_TAI},
  author       = {Sean A. Mochocki and Mark G. Reith and Brett J. Borghetti and Gilbert L. Peterson and John D. Jasper and Laurence D. Merkle},
  doi          = {10.1109/TAI.2024.3483190},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {574-588},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Personalized learning path problem variations: Computational complexity and AI approaches},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Traffexplainer: A framework toward GNN-based interpretable traffic prediction. <em>TAI</em>, <em>6</em>(3), 559-573. (<a href='https://doi.org/10.1109/TAI.2024.3459857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing traffic congestion problems in metropolises, traffic prediction plays an essential role in intelligent traffic systems. Notably, various deep learning models, especially graph neural networks (GNNs), achieve state-of-the-art performance in traffic prediction tasks but still lack interpretability. To interpret the critical information abstracted by traffic prediction models, we proposed a flexible framework termed Traffexplainer toward GNN-based interpretable traffic prediction. Traffexplainer is applicable to a wide range of GNNs without making any modifications to the original model structure. The framework consists of the GNN-based traffic prediction model and the perturbation-based hierarchical interpretation generator. Specifically, the hierarchical spatial mask and temporal mask are introduced to perturb the prediction model by modulating the values of input data. Then the prediction losses are backward propagated to the masks, which can identify the most critical features for traffic prediction, and further improve the prediction performance. We deploy the framework with five representative GNN-based traffic prediction models and analyze their prediction and interpretation performance on three real-world traffic flow datasets. The experiment results demonstrate that our framework can generate effective and faithful interpretations for GNN-based traffic prediction models, and also improve the prediction performance. The code will be publicly available at https://github.com/lingbai-kong/Traffexplainer.},
  archive      = {J_TAI},
  author       = {Lingbai Kong and Hanchen Yang and Wengen Li and Yichao Zhang and Jihong Guan and Shuigeng Zhou},
  doi          = {10.1109/TAI.2024.3459857},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {559-573},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Traffexplainer: A framework toward GNN-based interpretable traffic prediction},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReLAQA: Reinforcement learning-based autonomous quantum agent for quantum applications. <em>TAI</em>, <em>6</em>(3), 549-558. (<a href='https://doi.org/10.1109/TAI.2024.3437335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a quantum reinforcement learning (QRL) approach for robotic applications, which incorporates a Grover-based autonomous quantum agent (GAQA) and a quantum environment represented as a quantum TicTacToe (QTTT) game. The QTTT environment is a quantum circuit of qubits in their superposition states, manipulated by the agent through quantum gates to establish a goal state. By utilizing amplitude estimation and Grover search techniques, the proposed reinforcement learning-based autonomous quantum agent (ReLAQA) enhances the probability amplitudes of the actions taken, which results in reducing the number of observed states required to reach a solution. Empirical results substantiate the quantum advantages of the proposed GAQA in reinforcement learning (RL) tasks by observing fewer states of 6300, outperforming classical agents. Therefore, signifying its potential to enhance complex problem-solving in robotics.},
  archive      = {J_TAI},
  author       = {Ahmad Alomari and Sathish A. P. Kumar},
  doi          = {10.1109/TAI.2024.3437335},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {549-558},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {ReLAQA: Reinforcement learning-based autonomous quantum agent for quantum applications},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep-learning-based uncertainty-estimation approach for unknown traffic identification. <em>TAI</em>, <em>6</em>(3), 533-548. (<a href='https://doi.org/10.1109/TAI.2024.3437332'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real open network environments include the traffic generated by known applications or protocols, which have been previously identified and labeled, and unknown network traffic that cannot be identified based on existing knowledge. Accurately identifying unknown traffic is critical to network management and security, not only to help managers allocate bandwidth appropriately for all types of applications and ensure quality of service but also to prevent security breaches that may result from unknown applications or protocols. Notably, the unknown network traffic has been increasing with the emergence of new applications or protocols, which further increases the difficulty in identifying them. Existing unknown traffic classification methods based on Softmax output confidence values cause bias in the prediction probability due to overconfidence of the model during the training process, thus decreasing the identification accuracy. Thus, for unknown traffic identification, this study proposes a deep-learning-based uncertainty-estimation (EUE) approach. EUE introduces the theory of evidence to the task of identifying unknown traffic by inferring traffic uncertainty directly from traffic evidence without the need for a Softmax layer, thus avoiding overconfidence in the model. Thus, the EUE can accurately identify unknown traffic while classifying known traffic at the application level. We construct two experimental scenarios simulating the real network environments with different proportions of unknown traffic to evaluate EUE. The experimental results show that the proposed approach EUE exhibits excellent classification accuracy.},
  archive      = {J_TAI},
  author       = {Siqi Le and Yingxu Lai and Yipeng Wang and Huijie He},
  doi          = {10.1109/TAI.2024.3437332},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {533-548},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Deep-learning-based uncertainty-estimation approach for unknown traffic identification},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evolutionary multitasking algorithm for efficient multiobjective recommendations. <em>TAI</em>, <em>6</em>(3), 518-532. (<a href='https://doi.org/10.1109/TAI.2024.3414289'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Represented by evolutionary algorithms and swarm intelligence algorithms, nature-inspired metaheuristics have been successfully applied to recommender systems and amply demonstrated effectiveness, in particular, for multiobjective recommendation. Owing to the population-based search paradigm, these algorithms can produce a number of recommendation lists, making diverse tradeoffs between multiple metrics and meeting the requirements of accuracy, novelty, diversity, and other user preferences. However, these algorithms are criticized for the low efficiency of the optimization process, especially when the number of users is large. To address this issue, this article proposes an evolutionary multitasking-based recommendation method, where each task corresponds to a user and all the tasks are optimized simultaneously, thus highly improving the efficiency of recommendation. To enhance the convergence speed, all the users are divided into multiple populations according to the similarity between their preferences, where each population evolves with internal knowledge transfer between users, and all the populations evolve with external knowledge transfer between populations. Experimental results on various datasets verify that the proposed method can better balance between multiple metrics than classical and deep neural network-based recommendation methods and exhibits significantly higher efficiency than evolutionary multiobjective optimization-based recommendation methods.},
  archive      = {J_TAI},
  author       = {Ye Tian and Luke Ji and Yiwei Hu and Haiping Ma and Le Wu and Xingyi Zhang},
  doi          = {10.1109/TAI.2024.3414289},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {518-532},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An evolutionary multitasking algorithm for efficient multiobjective recommendations},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regret and belief complexity tradeoff in gaussian process bandits via information thresholding. <em>TAI</em>, <em>6</em>(3), 508-517. (<a href='https://doi.org/10.1109/TAI.2023.3332023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian optimization is a powerful framework for global search, using maximum a posteriori updates instead of simulated annealing. We cast it as a multiarmed bandit problem with a Gaussian process (GP) for the payoff function. Action selections rely on upper confidence bound (UCB) or expected improvement (EI). Prior works with GPs faced challenges for large iteration horizons ($T$) due to cubic scaling in posterior computation. To address this, we propose a simple thresholding: incorporating an action into the GP posterior only when its conditional entropy surpasses $\epsilon$. Doing so permits us to precisely characterize the tradeoff between regret bounds of GP bandit algorithms and complexity of the posterior distributions depending on the compression parameter $\epsilon$ for both discrete and continuous action sets. To best of our knowledge, this is the first result which allows us to obtain sublinear regret bounds while still maintaining sublinear growth rate of the complexity of the posterior which is linear in the existing literature. Moreover, a provably finite bound on the complexity could be achieved but the algorithm would result in $\epsilon$-regret which means $\textbf{Reg}_{T}/T\rightarrow\mathcal{O}(\epsilon)$ as $T\rightarrow\infty$. Experiments demonstrate state-of-the-art accuracy and complexity tradeoffs for GP bandit algorithms in global optimization, highlighting the benefits of compressed GPs in bandit settings.},
  archive      = {J_TAI},
  author       = {Amrit Singh Bedi and Dheeraj Peddireddy and Vaneet Aggarwal and Brian M. Sadler and Alec Koppel},
  doi          = {10.1109/TAI.2023.3332023},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {508-517},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Regret and belief complexity tradeoff in gaussian process bandits via information thresholding},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair machine learning in healthcare: A survey. <em>TAI</em>, <em>6</em>(3), 493-507. (<a href='https://doi.org/10.1109/TAI.2024.3361836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digitization of healthcare data coupled with advances in computational capabilities has propelled the adoption of machine learning (ML) in healthcare. However, these methods can perpetuate or even exacerbate existing disparities, leading to fairness concerns such as the unequal distribution of resources and diagnostic inaccuracies among different demographic groups. Addressing these fairness problems is paramount to prevent further entrenchment of social injustices. In this survey, we analyze the intersection of fairness in ML and healthcare disparities. We adopt a framework based on the principles of distributive justice to categorize fairness concerns into two distinct classes: equal allocation and equal performance. We provide a critical review of the associated fairness metrics from a ML standpoint and examine biases and mitigation strategies across the stages of the ML lifecycle, discussing the relationship between biases and their countermeasures. The article concludes with a discussion on the pressing challenges that remain unaddressed in ensuring fairness in healthcare ML and proposes several new research directions that hold promise for developing ethical and equitable ML applications in healthcare.},
  archive      = {J_TAI},
  author       = {Qizhang Feng and Mengnan Du and Na Zou and Xia Hu},
  doi          = {10.1109/TAI.2024.3361836},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {3},
  number       = {3},
  pages        = {493-507},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fair machine learning in healthcare: A survey},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence across europe: A study on awareness, attitude and trust. <em>TAI</em>, <em>6</em>(2), 477-490. (<a href='https://doi.org/10.1109/TAI.2024.3461633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the results of an extensive study investigating the opinions on artificial intelligence (AI) of a sample of 4006 European citizens from eight distinct countries (France, Germany, Italy, Netherlands, Poland, Romania, Spain, and Sweden). The aim of the study is to gain a better understanding of people's views and perceptions within the European context, which is already marked by important policy actions and regulatory processes. To survey the perceptions of the citizens of Europe, we design and validate a new questionnaire (PAICE) structured around three dimensions: people's awareness, attitude, and trust. We observe that while awareness is characterized by a low level of self-assessed competency, the attitude toward AI is very positive for more than half of the population. Reflecting on the collected results, we highlight implicit contradictions and identify trends that may interfere with the creation of an ecosystem of trust and the development of inclusive AI policies. The introduction of rules that ensure legal and ethical standards, along with the activity of high-level educational entities, and the promotion of AI literacy are identified as key factors in supporting a trustworthy AI ecosystem. We make some recommendations for AI governance focused on the European context and conclude with suggestions for future work.},
  archive      = {J_TAI},
  author       = {Teresa Scantamburlo and Atia Cortés and Francesca Foffano and Cristian Barrué and Veronica Distefano and Long Pham and Alessandro Fabris},
  doi          = {10.1109/TAI.2024.3461633},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {477-490},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Artificial intelligence across europe: A study on awareness, attitude and trust},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Under the influence: A survey of large language models in fake news detection. <em>TAI</em>, <em>6</em>(2), 458-476. (<a href='https://doi.org/10.1109/TAI.2024.3471735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research into fake news detection has a long history, although it gained significant attention following the 2016 U.S. election. During this time, the widespread use of social media and the resulting increase in interpersonal communication led to the extensive spread of ambiguous and potentially misleading news. Traditional approaches, relying solely on pre-large language model (LLM) techniques and addressing the issue as a simple classification problem, have shown to be insufficient for improving detection accuracy. In this context, LLMs have become crucial, as their advanced architectures overcome the limitations of pre-LLM methods, which often fail to capture the subtleties of fake news. This literature review aims to shed light on the field of fake news detection by providing a brief historical overview, defining fake news, reviewing detection methods used before the advent of LLMs, and discussing the strengths and weaknesses of these models in an increasingly complex landscape. Furthermore, it will emphasize the importance of using multimodal datasets in the effort to detect fake news.},
  archive      = {J_TAI},
  author       = {Soveatin Kuntur and Anna Wróblewska and Marcin Paprzycki and Maria Ganzha},
  doi          = {10.1109/TAI.2024.3471735},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {458-476},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Under the influence: A survey of large language models in fake news detection},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal knowledge extrapolation based on fine-grained tensor graph attention network for responsible AI. <em>TAI</em>, <em>6</em>(2), 448-457. (<a href='https://doi.org/10.1109/TAI.2024.3410932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge guidance is crucial for bridging the gap between high-level artificial intelligence (AI) ethics principles and the practical implementation of responsible AI systems. Diverging from static knowledge inference and temporal interpolation, the task of temporal knowledge extrapolation, which entails predicting future facts based on the evolution of historical facts, poses formidable challenges that remain largely unsolved. In existing temporal extrapolation methods, the structural learning of concurrent facts is primarily addressed by utilizing relation-aware graph neural networks. However, the temporal validity of facts leads to the sparsity of temporal subgraphs, where inherent coarse-level learning mechanisms hinder the capture of nuanced and scarce local semantics. Thus, this article proposes a fine-grained tensor graph attention network (F-GAT) that promotes representation learning of concurrent facts on sparse subgraphs by effectively distinguishing the significance of entities and relations within triplets and acquiring self-attention across diverse triplet contexts. Based on F-GAT, this article proposes a recurrent evolution network [recurrent evolution graph attention network (RE-GAT)] for temporal knowledge extrapolation. RE-GAT employed gated recurrent units to iteratively capture the sequential patterns between adjacent temporal facts, while simultaneously learning enriched embedding through the dual influence of historical factors and structural factors. The competitive results achieved by comparing the entity and relationship prediction performance of the proposed RE-GAT model with advanced methods on six public benchmarks demonstrate the effectiveness of RE-GAT for temporal extrapolation.},
  archive      = {J_TAI},
  author       = {Jing Yang and Chengxuan Huang and Xiangli Yang and Laurence T. Yang and Yuan Gao and Cong Liu},
  doi          = {10.1109/TAI.2024.3410932},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {448-457},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Temporal knowledge extrapolation based on fine-grained tensor graph attention network for responsible AI},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empowering machine learning with scalable feature engineering and interpretable AutoML. <em>TAI</em>, <em>6</em>(2), 432-447. (<a href='https://doi.org/10.1109/TAI.2024.3400752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated feature engineering (FE) has gained considerable attention in academia and industry. Nevertheless, existing systems often lack practical scalability and efficiency. This article introduces BigFeat, a scalable and interpretable framework that streamlines critical phases of the machine learning pipeline: FE, model selection, and hyperparameter tuning. BigFeat presents two execution options: as a standalone FE framework, denoted as BigFeat-FE, and as an AutoML framework, referred to as BigFeat-AutoML. BigFeat-FE optimizes input feature quality with the ultimate aim of maximizing predictive performance, based on a user-defined metric. BigFeat-FE employs a dynamic feature generation and selection mechanism that systematically creates a set of expressive features. These features not only enhance prediction performance but also prioritize interpretability. BigFeat-FE employs a meta-learning technique to warm-start the optimization process, resulting in significant overall performance gains. BigFeat-AutoML, tailored for algorithm selection and hyperparameter tuning, harnesses a random search method over the space of interpretable models. We conducted extensive experiments, and the results demonstrate that BigFeat-FE consistently outperforms state-of-the-art automated FE frameworks, such as AutoFeat and scalable automatic feature engineering (SAFE), across a wide range of datasets, achieving an average performance improvement of 8.65% compared to AutoFeat and 4.71% compared to SAFE, respectively. Additionally, BigFeat-AutoML demonstrates substantial performance improvement when compared to a tree-based pipeline optimization tool (TPOT) for automating machine learning and Autosklearn, with average improvements of 0.74% over TPOT and 2.25% over Autosklearn, respectively. Furthermore, BigFeat's scalability is affirmed through its linear complexity, and execution times, averaging 20 times faster than AutoFeat and 14 times faster than SAFE.},
  archive      = {J_TAI},
  author       = {Hassan Eldeeb and Radwa Elshawi},
  doi          = {10.1109/TAI.2024.3400752},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {432-447},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Empowering machine learning with scalable feature engineering and interpretable AutoML},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fostering human rights in responsible AI: A systematic review for best practices in industry. <em>TAI</em>, <em>6</em>(2), 416-431. (<a href='https://doi.org/10.1109/TAI.2024.3394389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recent rapid development of generative artificial intelligence (AI), and the resulting market growth, has introduced new challenges for social responsibility, an area where companies may need more guidance. In this regard, the literature covers a broad spectrum, from the impact of bias to the potential use of this technology to implement undemocratic surveillance. Another focus area discusses the AI industry's commitment to human rights and social responsibility, examining the diverse actors involved in this commitment and the context-dependent nature of their impact on human rights. This work performs a systematic review and a comparative analysis of the strategies and actions taken by four leading companies—OpenAI, Meta AI Research, Google AI, and Microsoft AI—with respect to five critical dimensions: bias, privacy, cybersecurity, hate speech, and misinformation. Our study analyzes 192 publicly available documents and reveals that depending on the diversity of products and their nature, some companies excel in the research and development of technologies and methodologies for privacy preservation and bias reduction, offering user-friendly tools for managing personal data, establishing expert groups to research the social impact of their technologies, and possessing significant expertise in tackling hate speech and misinformation. Nonetheless, there is an urgent need for greater linguistic, cultural, and geographic diversity in research lines, tools, and collaborative efforts. From this analysis, we draw a set of actionable best practices aimed at supporting the responsible development of AI models, and foundation models, in particular, that are aligned with human rights principles.},
  archive      = {J_TAI},
  author       = {Maria Teresa Baldassarre and Danilo Caivano and Berenice Fernández Nieto and Domenico Gigante and Azzurra Ragone},
  doi          = {10.1109/TAI.2024.3394389},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {416-431},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Fostering human rights in responsible AI: A systematic review for best practices in industry},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LLMaaS: Serving large-language models on trusted serverless computing platforms. <em>TAI</em>, <em>6</em>(2), 405-415. (<a href='https://doi.org/10.1109/TAI.2024.3429480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the emergence of large-language models (LLMs) has profoundly transformed our production and lifestyle. These models have shown tremendous potential in fields, such as natural language processing, speech recognition, and recommendation systems, and are increasingly playing crucial roles in applications such as human–computer interaction and intelligent customer service. Efficient inference solutions for LLMs in data centers have been extensively researched, with a focus on meeting users’ quality of service requirements. In this article, we focus on two additional requirements that responsible LLM inference should meet under QoS conditions: security throughout the model execution process and low maintenance requirements for the inference system. Therefore, we propose LLMaaS, a trusted model inference platform based on a serverless computing platform aimed at providing inference as a service for LLMs. First, we design a trusted serverless computing platform based on software guard extension (SGX), which includes distributed identity verification and SGX device plugins to ensure the security and trustworthiness of the inference process. Additionally, to reduce the maintenance requirements of the system, we enhance the SGX-based deep learning computing framework, including replacing PyTorch and using a greedy algorithm for graph partitioning. We conduct tests on four typical large models, and the experimental results demonstrate that, with minimal overhead and user code modifications, we can ensure the security of model execution.},
  archive      = {J_TAI},
  author       = {Zinuo Cai and Rongbo Ma and Yicheng Fu and Weishan Zhang and Ruhui Ma and Haibing Guan},
  doi          = {10.1109/TAI.2024.3429480},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {405-415},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {LLMaaS: Serving large-language models on trusted serverless computing platforms},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A locality-sensitive-hashing-based collaborative recommendation method for responsible AI-driven recommender systems. <em>TAI</em>, <em>6</em>(2), 393-404. (<a href='https://doi.org/10.1109/TAI.2024.3381571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most representative recommendation solutions, traditional collaborative filtering (CF) models typically have limitations in dealing with large-scale, sparse data to capture complex relationships between users and items. The rise of artificial intelligence (AI) provides powerful tools such as deep neural networks to overcome the data sparsity issue with typical CF models. Existing works on AI-driven recommender systems are focusing on improving recommendation accuracy by extracting rich user/item features from multisource data with deep learning tools to build more accurate user preference model. How to ensure the responsibility of AI-driven recommender systems is still a big challenge. On one hand, AI-driven recommender systems need to access raw data such as user profiles to train recommendation model, which face the risk of leaking user privacy information. On the other hand, the ever-increasing volume of user/item interaction records raises the efficiency challenge to provide recommendation results in a real-time manner. In view of those observations, we propose a collaborative recommendation method by adopting the locality-sensitive-hashing (LSH) technique (i.e., ${\text{DisRec}_{\text{LSH}}}$), which aims to achieve the goal of privacy protection and calculation efficiency for AI-driven recommender systems. More specifically, we split ${\text{DisRec}_{\text{LSH}}}$ into three phases: 1) offline feature extraction: deep neural networks are applied to extract user/item features from multisource user/item data on each local dataset; 2) offline user index building: LSH technique is adopted to map user features to hash codes to quickly recall correlated similar users for a given target user; and 3) online top-N items calculation: with similar users selected by phase 2, top-N items are calculated based on the ranking of predicted user–item rating score. Finally, extensive experiments are conducted on three public datasets to evaluate the efficiency of our proposal.},
  archive      = {J_TAI},
  author       = {Wenmin Lin and Xinyi Zhou and Lu Sun and Lianyong Qi and Sang-Bing Tsai and Yihong Yang and Hanwen Liu and Huaizhen Kou and Lingzhen Kong},
  doi          = {10.1109/TAI.2024.3381571},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {393-404},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A locality-sensitive-hashing-based collaborative recommendation method for responsible AI-driven recommender systems},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nudging toward responsible recommendations: A graph-based approach to mitigate belief filter bubbles. <em>TAI</em>, <em>6</em>(2), 378-392. (<a href='https://doi.org/10.1109/TAI.2024.3373392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized recommendation systems homogenize user preferences, causing an extreme belief imbalance and aggravating user bias. This phenomenon is known as the filter bubble. This article presents the responsible graph-based recommendation (RGRec) system, designed to alleviate the filter bubble effect in personalized recommendation systems. Acting as an intermediate agency between users and existing preference-based recommendation systems, RGRec is composed of three collaborative modules: the multifaceted reasoning-based filter bubbles detection (FBDetect) module, the belief nudging module, and the generative artificial intelligence (GAI)-based recommendation strategy generation module (RecomGen). The FBDetect module identifies users with extreme belief imbalances based on their belief networks, which are represented as heterogeneous graphs. These graphs are then utilized in the Belief Nudging module, where a nudging strategy is employed to adapt prompts for the RecomGen module. Ultimately, the RecomGen module generates contextually rich items for recommendations. Experimental results demonstrate that RGRec can promote diverse content exploration based on user feedback and progressively stimulate interest in topics users initially showed less interest in, encouraging individual exploration.},
  archive      = {J_TAI},
  author       = {Mengyan Wang and Yuxuan Hu and Shiqing Wu and Weihua Li and Quan Bai and Zihan Yuan and Chenting Jiang},
  doi          = {10.1109/TAI.2024.3373392},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {378-392},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Nudging toward responsible recommendations: A graph-based approach to mitigate belief filter bubbles},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latency- and privacy-aware convolutional neural network distributed inference for reliable artificial intelligence systems. <em>TAI</em>, <em>6</em>(2), 365-377. (<a href='https://doi.org/10.1109/TAI.2024.3366880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable artificial intelligence (AI) systems not only propose a challenge on providing intelligent services with high quality for customers but also require customers’ privacy to be protected as much as possible during process of the services. Given the ultrahigh computing load brought by deep-learning-based intelligent services to edge devices and the ultralong distance between edge and cloud, low-latency requirement of intelligent services is hard to meet in single edge computing or cloud computing. Edge–cloud collaborative inference of deep neural networks (DNNs) is considered a feasible solution to the problem. However, former work has not reduced the inference latency to the greatest extent and has not considered privacy protection in distributed systems. To solve the problem, we first establish a novel queue mechanism. Then, the convolution layer split decisions are made based on deep reinforcement learning (DRL) to realize the parallel inference of convolutional neural networks (CNNs) for inference latency reduction. Next, for each CNN, the partition decision is made based on brute force algorithm to further reduce inference latency and protect customers’ privacy. Finally, simulation results show that our method performs better than the existing other methods.},
  archive      = {J_TAI},
  author       = {Yuhao Hu and Xiaolong Xu and Lianyong Qi and Xiaokang Zhou and Xiaoyu Xia},
  doi          = {10.1109/TAI.2024.3366880},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {365-377},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Latency- and privacy-aware convolutional neural network distributed inference for reliable artificial intelligence systems},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking transferable adversarial attacks with double adversarial neuron attribution. <em>TAI</em>, <em>6</em>(2), 354-364. (<a href='https://doi.org/10.1109/TAI.2024.3368361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferable adversarial attacks are a threat to deep neural networks, in particular, for black-box scenarios where access to model information is limited. One can, for example, exploit the intermediate layer neurons to generate transferable adversarial samples. However, current works show limitations in providing feature-level attack mechanisms across multiple victim models. In light of the attribution methods, in this article, we investigate the attribution similarity across different models. We leverage the similarity to incorporate different attribution properties to enhance the sample transferability, for the first time, formulating a novel neuron attribution-based transferable attack termed DANAA++. Specifically, we utilize a range of adversarial attack methods to generate different baseline points through adversarial training. The attribution results are thus obtained along both linear and nonlinear integration paths. In our experiments, the baseline points and integration paths significantly help improve the transferability of adversarial samples. Our approach provides novel insights for building effective attribution-based feature-level adversarial attacks. https://github.com/LMBTough/DANAAPP},
  archive      = {J_TAI},
  author       = {Zhiyu Zhu and Zhibo Jin and Xinyi Wang and Jiayu Zhang and Huaming Chen and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TAI.2024.3368361},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {354-364},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Rethinking transferable adversarial attacks with double adversarial neuron attribution},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy inference attack and defense in centralized and federated learning: A comprehensive survey. <em>TAI</em>, <em>6</em>(2), 333-353. (<a href='https://doi.org/10.1109/TAI.2024.3363670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of new machine learning methods has led to their widespread application across various domains, significantly advancing the field of artificial intelligence. However, the process of training and inferring machine learning models relies on vast amounts of data, which often include sensitive private information. Consequently, the privacy and security of machine learning have encountered significant challenges. Several studies have demonstrated the vulnerability of machine learning to privacy inference attacks, but they often focus on specific scenarios, leaving a gap in understanding the broader picture. We provide a comprehensive review of privacy attacks in machine learning, focusing on two scenarios: centralized learning and federated learning. This article begins by presenting the architectures of both centralized learning and federated learning, along with their respective application scenarios. It then conducts a comprehensive review and categorization of related inference attacks, providing a detailed analysis of the different stages involved in these attacks. Moreover, the article thoroughly describes and compares the existing defense methods. Finally, the article concludes by highlighting open questions and potential future research directions, aiming to contribute to the ongoing competition between privacy attackers and defenders.},
  archive      = {J_TAI},
  author       = {Bosen Rao and Jiale Zhang and Di Wu and Chengcheng Zhu and Xiaobing Sun and Bing Chen},
  doi          = {10.1109/TAI.2024.3363670},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {333-353},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Privacy inference attack and defense in centralized and federated learning: A comprehensive survey},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A membership inference and adversarial attack defense framework for network traffic classifiers. <em>TAI</em>, <em>6</em>(2), 317-332. (<a href='https://doi.org/10.1109/TAI.2024.3357791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malicious traffic identification methods in intrusion detection systems have evolved from rule-based matching to machine learning. However, security risks such as membership inference and adversarial attacks hinder the practical deployment of machine learning-based network intrusion detection systems (ML-NIDSs). In this work, we design a defense framework called hierarchical differential privacy (HierarchicalDP) to safeguard ML-NIDS against membership inference and adversarial attacks. First, we analyze the principles of membership inference and adversarial attacks to find their correlation. Based on this, we propose the feature distribution security metric (FDSM) to measure the risk of membership inference and adversarial attacks on ML-NIDS. Then, we design the HierarchicalDP framework, which partitions network traffic sample features according to security levels and introduces distinct noise on each security level feature to satisfy FDSM, thus defensing against membership inference and adversarial attacks. Finally, we evaluate the defensive performance of the HierarchicalDP framework on two network traffic datasets and four machine-learning models. The HierarchicalDP defense framework, based on Laplace noise, reduces the success rate of membership inference from 64.9% to 54.4% (ineffective binary classification), the evasion rate of adversarial samples from 86.1% to 23.2%, and maintains model accuracy (ACC) fluctuations within 4.2%. Furthermore, the HierarchicalDP framework adjusts sample features without modifying the model, thereby not affecting the inference speed. HierarchicalDP offers efficient and convenient defenses for ML-NIDS deployed in a network.},
  archive      = {J_TAI},
  author       = {Guangrui Liu and Weizhe Zhang and Xurun Wang and Stephen King and Shui Yu},
  doi          = {10.1109/TAI.2024.3357791},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {317-332},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A membership inference and adversarial attack defense framework for network traffic classifiers},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A credible and fair federated learning framework based on blockchain. <em>TAI</em>, <em>6</em>(2), 301-316. (<a href='https://doi.org/10.1109/TAI.2024.3355362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) enables cooperative computation between multiple participants while protecting user privacy. Currently, FL algorithms assume that all participants are trustworthy and their systems are secure. However, the following problems arise in real-world scenarios: 1) Malicious clients disrupt FL through model poisoning and data poisoning attacks. Although some research has proposed secure aggregation methods to solve this problem, most methods have limitations. 2) The current method cannot fairly evaluate client contribution in some scenarios. Some clients exhibit free-rider behavior, seeking to cheat the reward system and manipulate global models. Evaluating client contribution and distributing rewards also present challenges. To address these challenges, we design a trustworthy federated framework to ensure secure computing throughout the federated task process. First, we propose a method of detecting malicious models to guarantee secure model aggregation. Then, we propose a fair method of assessing contribution to identify client-side free-riding behavior. Finally, we implement a computation process based on blockchain and smart contracts to guarantee the trustworthiness and fairness of federated tasks. To validate the performance of our framework, we simulate different types of client attacks and contribution evaluation scenarios on several open-source datasets. The experiments show that our framework ensures the credibility of federated tasks and achieves a fair evaluation of client contributions.},
  archive      = {J_TAI},
  author       = {Leiming Chen and Dehai Zhao and Liping Tao and Kai Wang and Sibo Qiao and Xingjie Zeng and Chee Wei Tan},
  doi          = {10.1109/TAI.2024.3355362},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {301-316},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {A credible and fair federated learning framework based on blockchain},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frequency attacks based on invertible neural networks. <em>TAI</em>, <em>6</em>(2), 292-300. (<a href='https://doi.org/10.1109/TAI.2023.3349238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial attacks reveal the vulnerability of classifiers based on deep neural networks to well-designed perturbations. Most existing attack methods focus on adding perturbations directly to the pixel space. However, the perturbations generated by these methods may be easily perceived by humans. To alleviate the aforementioned problem, we propose a novel high-frequency attack based on invertible neural networks (HA-INN) that relies on INNs to adds perturbations to the high-frequency space of the image instead of the pixel space. In this way, we can fool the classifier while the perturbations are not easily detected by humans. Specifically, we introduce INNs to separate the high-frequency and low-frequency components of the image. And the low-frequency components are guaranteed to be reconstructed as the original image, while the high-frequency components are replaced with resampled high-frequency latent variables with additional adversarial information. Then, the low-frequency components and the high-frequency components with adversarial information are inversely fed into the INN to generate effective adversarial examples. Extensive experiments on two datasets (CIFAR-10 and CIFAR-100) show that our method can generate misleading and transferable cross-architectural adversarial examples with greatly reduced computational resource requirements. Under the white-box setting, the attack success rate of CIFAR10 and CIFAR100 is 99.8% and 99.74%, respectively. Further, under the black-box setting, the adversarial examples generated by our method are more effective than the other methods.},
  archive      = {J_TAI},
  author       = {Ming-Wen Shao and Jian-Xin Yang and Ling-Zhuang Meng and Zhi-Yong Hu},
  doi          = {10.1109/TAI.2023.3349238},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {292-300},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Frequency attacks based on invertible neural networks},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FairCompass: Operationalizing fairness in machine learning. <em>TAI</em>, <em>6</em>(2), 281-291. (<a href='https://doi.org/10.1109/TAI.2023.3348429'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As artificial intelligence (AI) increasingly becomes an integral part of our societal and individual activities, there is a growing imperative to develop responsible AI solutions. Despite a diverse assortment of machine learning fairness solutions is proposed in the literature, there is reportedly a lack of practical implementation of these tools in real-world applications. Industry experts have participated in thorough discussions on the challenges associated with operationalizing fairness in the development of machine learning-empowered solutions, in which a shift toward human-centred approaches is promptly advocated to mitigate the limitations of existing techniques. In this work, we propose a human-in-the-loop approach for fairness auditing, presenting a mixed visual analytical system (hereafter referred to as “FairCompass”), which integrates both subgroup discovery technique and the decision tree-based schema for end users. Moreover, we innovatively integrate an exploration, guidance, and informed analysis loop, to facilitate the use of the knowledge generation model for visual analytics in FairCompass. We evaluate the effectiveness of FairCompass for fairness auditing in a real-world scenario, and the findings demonstrate the system's potential for real-world deployability. We anticipate this work will address the current gaps in research for fairness and facilitate the operationalization of fairness in machine learning systems.},
  archive      = {J_TAI},
  author       = {Jessica Liu and Huaming Chen and Jun Shen and Kim-Kwang Raymond Choo},
  doi          = {10.1109/TAI.2023.3348429},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {281-291},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FairCompass: Operationalizing fairness in machine learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedDGA: Federated multitask learning based on dynamic guided attention. <em>TAI</em>, <em>6</em>(2), 268-280. (<a href='https://doi.org/10.1109/TAI.2024.3350538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of privacy-sensitive data has spurred the development of federated learning (FL), which is an important technology for state-of-the-art machine learning and responsible AI. However, most existing FL methods are constrained in their applicability and generalizability due to their narrow focus on specific tasks. This article presents a novel federated multitask learning (FMTL) framework that is capable of acquiring knowledge across multiple tasks. To address the challenges posed by non-IID data and task imbalance in FMTL, this study proposes a federated fusion strategy based on dynamic guided attention (FedDGA), which adaptively fine-tunes local models for multiple tasks with personalized attention. In addition, this article designed dynamic batch weight (DBW) to balance the task losses and improve the convergence speed. Extensive experiments were conducted on various datasets, tasks, and settings, and the proposed method was compared with state-of-the-art methods such as FedAvg, FedProx, and SCAFFOLD. The results show that our method achieves significant performance gains, with up to 11.1% increase in accuracy over the baselines.},
  archive      = {J_TAI},
  author       = {Haoyun Sun and Hongwei Zhao and Liang Xu and Weishan Zhang and Hongqing Guan and Su Yang},
  doi          = {10.1109/TAI.2024.3350538},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {268-280},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {FedDGA: Federated multitask learning based on dynamic guided attention},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ethical aspects of ChatGPT in software engineering research. <em>TAI</em>, <em>6</em>(2), 254-267. (<a href='https://doi.org/10.1109/TAI.2023.3318183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {ChatGPT can improve software engineering (SE) research practices by offering efficient, accessible information analysis, and synthesis based on natural language interactions. However, ChatGPT could bring ethical challenges, encompassing plagiarism, privacy, data security, and the risk of generating biased or potentially detrimental data. This research aims to fill the given gap by elaborating on the key elements: motivators, demotivators, and ethical principles of using ChatGPT in SE research. To achieve this objective, we conducted a literature survey, identified the mentioned elements, and presented their relationships by developing a taxonomy. Furthermore, the identified literature-based elements (motivators, demotivators, and ethical principles) were empirically evaluated by conducting a comprehensive questionnaire-based survey involving SE researchers. In addition, we employed an interpretive structure modeling approach to analyze the relationships between the ethical principles of using ChatGPT in SE research and develop a level-based decision model. We further conducted a cross-impact matrix multiplication applied to classification analysis to create a cluster-based decision model. These models aim to help SE researchers devise effective strategies for ethically integrating ChatGPT into SE research by following the identified principles by adopting the motivators and addressing the demotivators. The findings of this study will establish a benchmark for incorporating ChatGPT services in SE research with an emphasis on ethical considerations.},
  archive      = {J_TAI},
  author       = {Muhammad Azeem Akbar and Arif Ali Khan and Peng Liang},
  doi          = {10.1109/TAI.2023.3318183},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {254-267},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ethical aspects of ChatGPT in software engineering research},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Operationalizing responsible AI. <em>TAI</em>, <em>6</em>(2), 252-253. (<a href='https://doi.org/10.1109/TAI.2025.3527806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TAI},
  author       = {Qinghua Lu and Apostol Vassilev and Jun Zhu and Foutse Khomh},
  doi          = {10.1109/TAI.2025.3527806},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {2},
  number       = {2},
  pages        = {252-253},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Guest editorial: Operationalizing responsible AI},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning neural network classifiers by distributing nearest neighbors on adaptive hypersphere. <em>TAI</em>, <em>6</em>(1), 234-249. (<a href='https://doi.org/10.1109/TAI.2024.3477436'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the adaptive hypersphere nearest neighbors (ASNN) method is proposed as an optimization framework to enhance the generalization performance of neural network classifiers. In terms of the classification task, the neural network draws decision boundaries by constructing the discriminative features of samples. To learn those features, attributed to the flexibility and separability, the pair-wise constraint-based methods that consist of the pair-wise loss and an embedding space (e.g., hypersphere space) have gained considerable attention over the last decade. Despite their success, pair-wise constraint-based methods still suffer from premature convergence or divergence problems, driven by two main challenges. 1) The poor scalability of the embedding space constrains the variety of the distribution of embedded samples, thereby increasing the optimization difficulty. 2) It is hard to select suitable positive/negative pairs during the training. In order to address the aforementioned problems, we propose an adaptive hypersphere nearest neighbors method. On the one hand, we improve the scalability of features via a scale-adaptive hypersphere embedding space. On the other hand, we introduce a neighborhood-based probability loss, which magnifies the difference between pairs and enhances the discriminative power of features generated by the neural networks based on the nearest neighbor-based pairing strategy. Experiments on UCI datasets and image recognition tasks demonstrate that the proposed ASNN not only achieves improved intraclass consistency and interclass separability of samples, but also outperforms its competitive counterparts.},
  archive      = {J_TAI},
  author       = {Xiaojing Zhang and Shuangrong Liu and Lin Wang and Bo Yang and Jiawei Fan},
  doi          = {10.1109/TAI.2024.3477436},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {234-249},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Learning neural network classifiers by distributing nearest neighbors on adaptive hypersphere},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge probabilization in ensemble distillation: Improving accuracy and uncertainty quantification for object detectors. <em>TAI</em>, <em>6</em>(1), 221-233. (<a href='https://doi.org/10.1109/TAI.2024.3474654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble object detectors have demonstrated remarkable effectiveness in enhancing prediction accuracy and uncertainty quantification. However, their widespread adoption is hindered by significant computational and storage demands, limiting their feasibility in resource-constrained settings. To overcome this, researchers have focused on distilling the knowledge from ensemble object detectors into a single model. In this article, we introduce probabilization based ensemble distillation (ProbED), an innovative ensemble distillation framework that consolidates knowledge from multiple object detectors into a single, resource-efficient model. Unlike traditional ensemble distillation methods that average the outputs of subteachers, ProbED captures comprehensive outcome distributions from all subteachers, providing a more nuanced approach to knowledge transfer. ProbED employs knowledge probabilization to achieve a sophisticated and refined aggregation of teacher knowledge, including feature knowledge, semantic knowledge, and localization knowledge, resulting in dual improvements in prediction accuracy and uncertainty quantification for the student model. In particular, ProED's novel knowledge probabilization-based approach to aggregating teacher knowledge is inspired by our empirical observations, which demonstrate that knowledge probabilization excels in effectively representing uncertainty, improving prediction, and facilitating robust knowledge transfer. Furthermore, we introduce a random smoothing perturbation technique to modify inputs within ProbED, further enhancing the distillation process. Extensive experiments highlight ProbED's ability to significantly enhance the prediction accuracy and uncertainty quantification of various object detectors, demonstrating its superior performance compared to other state-of-the-art techniques.},
  archive      = {J_TAI},
  author       = {Yang Yang and Chao Wang and Lei Gong and Min Wu and Zhenghua Chen and Xiang Li and Xianglan Chen and Xuehai Zhou},
  doi          = {10.1109/TAI.2024.3474654},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {221-233},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Knowledge probabilization in ensemble distillation: Improving accuracy and uncertainty quantification for object detectors},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting few-shot semantic segmentation with prior-driven edge feature enhancement network. <em>TAI</em>, <em>6</em>(1), 211-220. (<a href='https://doi.org/10.1109/TAI.2024.3474650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot semantic segmentation (FSS) focuses on segmenting objects of novel classes with only a small number of annotated samples and has achieved great development. However, compared with general semantic segmentation, inaccurate boundary predictions remain a serious problem in FSS. This is because, in scenarios with few samples, the extracted query features by the model struggle to contain sufficient detailed information to focus on the boundary of the target. To address this issue, we propose a prior-driven edge feature enhancement network (PDEFE) that utilizes the prior information of the object edges to enhance the query feature, thereby promoting the accurate segmentation of the target. Specifically, we first design an edge feature enhancement module (EFEM) that can utilize object edges to enhance the feature of the query object's boundaries. Furthermore, we also propose an edge prior mask generator (EPMG) to generate prior masks for edges based on the gradient information of the image, which can guide the model to pay more attention to the boundaries of the target in the query image. Extensive experiments on PASCAL-$5^{i}$ and COCO-$20^{i}$ demonstrate that PDEFE significantly improves upon two baseline detectors (up to 2.7$\sim$4.2% mIoU in average), achieving state-of-the-art performance.},
  archive      = {J_TAI},
  author       = {Jingkai Ma and Shuang Bai and Wenchao Pan},
  doi          = {10.1109/TAI.2024.3474650},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {211-220},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Boosting few-shot semantic segmentation with prior-driven edge feature enhancement network},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient CORDIC-based activation functions for RNN acceleration on FPGAs. <em>TAI</em>, <em>6</em>(1), 199-210. (<a href='https://doi.org/10.1109/TAI.2024.3474648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs), particularly long short-term memory (LSTM) networks, have emerged as standard tools for tackling a wide range of time series applications, such as natural language processing. However, deploying these models on edge devices presents great challenges due to limited computational resources. Additionally, the implementation of RNN activation functions on low-end hardware devices significantly impacts the overall network performance, as activations constitute the dominant part of execution time. In this work, we propose an efficient approach for implementing commonly used RNN activations, leveraging an optimized coordinate rotation digital computer algorithm (CORDIC). Moreover, we propose a unified hardware architecture for mapping the CORDIC-based method onto field-programmable gate arrays (FPGAs), which can be configured to implement multiple nonlinear activation functions. Our architecture reduces the computational time with fewer iterations in CORDIC compared with existing methods, rendering it particularly suitable for resource-constrained edge devices. Our design is implemented on a Xilinx Zynq-7000 device and evaluated across three RNNs and benchmark datasets. Experimental results demonstrate that our design achieves up to a 2$\boldsymbol{\times}$ speedup while maintaining model accuracy compared with the state-of-the-art designs.},
  archive      = {J_TAI},
  author       = {Wan Shen and Junye Jiang and Minghan Li and Shuanglong Liu},
  doi          = {10.1109/TAI.2024.3474648},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {199-210},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Efficient CORDIC-based activation functions for RNN acceleration on FPGAs},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NPE-DRL: Enhancing perception constrained obstacle avoidance with nonexpert policy guided reinforcement learning. <em>TAI</em>, <em>6</em>(1), 184-198. (<a href='https://doi.org/10.1109/TAI.2024.3464510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obstacle avoidance under constrained visual perception presents a significant challenge, requiring rapid detection and decision-making within partially observable environments, particularly for unmanned aerial vehicles (UAVs) maneuvering agilely in 3-D space. Compared with traditional methods, obstacle avoidance algorithms based on deep reinforcement learning (DRL) offer a better comprehension of the uncertain operational environment in an end-to-end manner, reducing computational complexity, and enhancing flexibility and scalability. However, the inherent trial-and-error learning mechanism of DRL necessitates numerous iterations for policy convergence, leading to sample inefficiency issues. Meanwhile, existing sample-efficient obstacle avoidance approaches that leverage imitation learning often heavily rely on offline expert demonstrations, which are not always feasible in hazardous environments. To address these challenges, we propose a novel obstacle avoidance approach based on nonexpert policy enhanced DRL (NPE-DRL). This approach integrates a fundamental DRL framework with prior knowledge derived from a nonexpert policy-guided imitation learning. During the training phase, the agent starts by online imitating the actions generated by the nonexpert policy during interactions and progressively shifts toward autonomously exploring the environment to generate the optimal policy. Both simulation and physical experiments validate that our approach improves sample efficiency and achieves a better exploration–exploitation balance in both virtual and real-world flights. Additionally, our NPE-DRL-based obstacle avoidance approach shows better adaptability in complex environments characterized by larger scales and denser obstacle configurations, demonstrating a significant improvement in UAVs’ obstacle avoidance capability. Code available at https://github.com/zzzzzyh111/NonExpert-Guided-Visual-UAV-Navigation-Gazebo.},
  archive      = {J_TAI},
  author       = {Yuhang Zhang and Chao Yan and Jiaping Xiao and Mir Feroskhan},
  doi          = {10.1109/TAI.2024.3464510},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {184-198},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {NPE-DRL: Enhancing perception constrained obstacle avoidance with nonexpert policy guided reinforcement learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobjective dynamic flexible job shop scheduling with biased objectives via multitask genetic programming. <em>TAI</em>, <em>6</em>(1), 169-183. (<a href='https://doi.org/10.1109/TAI.2024.3456086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic flexible job shop scheduling is an important combinatorial optimization problem that has rich real-world applications such as product processing in manufacturing. Genetic programming has been successfully used to learn scheduling heuristics for dynamic flexible job shop scheduling. Intuitively, users prefer small and effective scheduling heuristics that can not only generate promising schedules but also are computationally efficient and easy to be understood. However, a scheduling heuristic with better effectiveness tends to have a larger size, and the effectiveness of rules and rule size are potentially conflicting objectives. With the traditional dominance relation-based multiobjective algorithms, there is a search bias toward rule size, since rule size is much easier to optimized than effectiveness, and larger rules are easily abandoned, resulting in the loss of effectiveness. To address this issue, this article develops a novel multiobjective genetic programming algorithm that takes size and effectiveness of scheduling heuristics for optimization via multitask learning mechanism. Specifically, we construct two tasks for the multiobjective optimization with biased objectives using different search mechanisms for each task. The focus of the proposed algorithm is to improve the effectiveness of learned small rules by knowledge sharing between constructed tasks which is implemented with the crossover operator. The results show that our proposed algorithm performs significantly better, i.e., with smaller and more effective scheduling heuristics, than the state-of-the-art algorithms in the examined scenarios. By analyzing the population diversity, we find that the proposed algorithm has a good balance between exploration and exploitation during the evolutionary process.},
  archive      = {J_TAI},
  author       = {Fangfang Zhang and Gaofeng Shi and Yi Mei and Mengjie Zhang},
  doi          = {10.1109/TAI.2024.3456086},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {169-183},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Multiobjective dynamic flexible job shop scheduling with biased objectives via multitask genetic programming},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved and explainable electricity price forecasting model via SHAP-based error compensation approach. <em>TAI</em>, <em>6</em>(1), 159-168. (<a href='https://doi.org/10.1109/TAI.2024.3455313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forecasting errors in power markets, even as small as 1%, can have significant financial implications. However, even high-performance artificial intelligence (AI) based electricity price forecasting (EPF) models have instances when their prediction error is much higher than those shown by mean performance metrics. To date, explainable AI has been used to enhance the model transparency and trustworthiness of AI-based EPF models. However, this article demonstrates that insights from explainable AI (XAI) techniques can be expanded beyond its primary task of explanatory visualizations. This work presents a XAI-based error compensation approach to improve model performance and identify irregular predictions. The first phase of the proposed approach involves error quantification through a Shapley additive explanations (SHAP) based corrector model that fine-tunes the base predictor's forecasts. Using this corrector model's SHAP explanations, the proposed approach distinguishes high-accuracy predictions from lower ones in the second stage. Additionally, these explanations are more simplified than the base model, making them easier for nonexpert users such as bidding agents. Performance enhancement and insightful user-centric explanations are crucial for real-world scenarios such as price spikes during network congestion, high renewable penetration, and fluctuating fuel costs. Case studies discussed here show the efficacy of the proposed approach independent of model architecture, feature combination, or behavioral patterns of electricity prices in different markets.},
  archive      = {J_TAI},
  author       = {Leena Heistrene and Juri Belikov and Dmitry Baimel and Liran Katzir and Ram Machlev and Kfir Levy and Shie Mannor and Yoash Levron},
  doi          = {10.1109/TAI.2024.3455313},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {159-168},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {An improved and explainable electricity price forecasting model via SHAP-based error compensation approach},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Face forgery detection based on fine-grained clues and noise inconsistency. <em>TAI</em>, <em>6</em>(1), 144-158. (<a href='https://doi.org/10.1109/TAI.2024.3455311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deepfake detection has gained increasing research attention in media forensics, and a variety of works have been produced. However, subtle artifacts might be eliminated by compression, and the convolutional neural networks (CNNs)-based detectors are invalidated for fake face images with compression. In this work, we propose a two-stream network for deepfake detection. We observed that high-frequency noise features and spatial features are inherently complementary to each other. Thus, both spatial features and high-frequency noise features are exploited for face forgery detection. Specifically, we design a double-frequency transformer module (DFTM) to guide the learning of spatial features from local artifact regions. To effectively fuse spatial features and high-frequency noise features, a dual-domain attention fusion module (DDAFM) is designed. We also introduce a local relationship constraint loss, which requires only image-level labels, for model training. We evaluate the proposed approach on five large-scale benchmark datasets, and extensive experimental results demonstrate the proposed approach outperforms most SOTA works.},
  archive      = {J_TAI},
  author       = {Dengyong Zhang and Ruiyi He and Xin Liao and Feng Li and Jiaxin Chen and Gaobo Yang},
  doi          = {10.1109/TAI.2024.3455311},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {144-158},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Face forgery detection based on fine-grained clues and noise inconsistency},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spiking diffusion models. <em>TAI</em>, <em>6</em>(1), 132-143. (<a href='https://doi.org/10.1109/TAI.2024.3453229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed spiking neural networks (SNNs) gaining attention for their ultra-low energy consumption and high biological plausibility compared with traditional artificial neural networks (ANNs). Despite their distinguished properties, the application of SNNs in the computationally intensive field of image generation is still under exploration. In this article, we propose the spiking diffusion models (SDMs), an innovative family of SNN-based generative models that excel in producing high-quality samples with significantly reduced energy consumption. In particular, we propose a temporal-wise spiking mechanism (TSM) that allows SNNs to capture more temporal features from a bio-plasticity perspective. In addition, we propose a threshold-guided strategy that can further improve the performances by up to 16.7% without any additional training. We also make the first attempt to use the ANN-SNN approach for SNN-based generation tasks. Extensive experimental results reveal that our approach not only exhibits comparable performance to its ANN counterpart with few spiking time steps, but also outperforms previous SNN-based generative models by a large margin. Moreover, we also demonstrate the high-quality generation ability of SDM on large-scale datasets, e.g., LSUN bedroom. This development marks a pivotal advancement in the capabilities of SNN-based generation, paving the way for future research avenues to realize low-energy and low-latency generative applications.},
  archive      = {J_TAI},
  author       = {Jiahang Cao and Hanzhong Guo and Ziqing Wang and Deming Zhou and Hao Cheng and Qiang Zhang and Renjing Xu},
  doi          = {10.1109/TAI.2024.3453229},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {132-143},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Spiking diffusion models},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Policy consensus-based distributed deterministic multi-agent reinforcement learning over directed graphs. <em>TAI</em>, <em>6</em>(1), 118-131. (<a href='https://doi.org/10.1109/TAI.2024.3452678'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning efficient coordination policies over continuous state and action spaces remains a huge challenge for existing distributed multi-agent reinforcement learning (MARL) algorithms. In this article, the classic deterministic policy gradient (DPG) method is extended to the distributed MARL domain to handle the continuous control policy learning issue for a team of homogeneous agents connected through a directed graph. A theoretical on-policy distributed actor–critic algorithm is first proposed based on a local DPG theorem, which considers observation-based policies, and incorporates consensus updates for the critic and actor parameters. Stochastic approximation theory is then used to obtain asymptotic convergence results of the algorithm under standard assumptions. Thereafter, a practical distributed deterministic actor–critic algorithm is proposed by integrating the theoretical algorithm with the deep reinforcement learning training architecture, which achieves better scalability, exploration ability, and data efficiency. Simulations are carried out in standard MARL environments with continuous action spaces, where the results demonstrate that the proposed distributed algorithm achieves comparable learning performance to solid centralized trained baselines while demanding much less communication resources.},
  archive      = {J_TAI},
  author       = {Yifan Hu and Junjie Fu and Guanghui Wen and Changyin Sun},
  doi          = {10.1109/TAI.2024.3452678},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {118-131},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Policy consensus-based distributed deterministic multi-agent reinforcement learning over directed graphs},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RD-net: Residual-dense network for glaucoma prediction using structural features of optic nerve head. <em>TAI</em>, <em>6</em>(1), 107-117. (<a href='https://doi.org/10.1109/TAI.2024.3447578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glaucoma is called as the silent thief of eyesight. It is related to the internal damage of optical nerve head (ONH). For early screening, the simplest way is to analyze the subtle variations in structural features such as cup to disc ratio (CDR), disc damage likelihood scale (DDLS), rim width of the inferior, superior, nasal, and temporal (ISNT) regions of ONH. This can be done by accurate segmentation of optic disc (OD) and optic cup (OC). In this work, we have introduced a deep learning framework, called residual dense network (RD-NET), for disc and cup segmentation. Based on the segmentation results, the structural features are calculated. The proposed design differs from the traditional U-Net in that it utilizes filters with variable sizes and an alternative optimization method throughout the up- and down-sampling processes. The introduced method is a hybrid deep learning model that incorporates dense residual blocks and squeeze excitation block introduced within the conventional U-Net architecture. Unlike the classical approaches that are primarily based on CDR calculation, in this work, we first segment OD and OC using RD-Net and then analyze ISNT and DDLS. Once a suspicious case is detected, we then go for CDR calculation. In addition to developing an efficient segmentation model, six distinct kinds of data augmentation techniques have been also used in this study to increase the amount of training data. This, in turn, leads to a better estimation of model parameters. The model is rigorously trained and tested on four benchmark datasets namely DRISHTI, RIMONE, ORIGA, and REFUGE. Subsequently, the structural parameters are calculated for glaucoma prediction. The average accuracies are observed to be 0.9940 and 0.9894 for OD and cup segmentation, respectively. The extensive experiments presented in this article show that our method outperforms other existing state-of-the art algorithms.},
  archive      = {J_TAI},
  author       = {Preity and Ashish Kumar Bhandari and Akanksha Jha and Syed Shahnawazuddin},
  doi          = {10.1109/TAI.2024.3447578},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {107-117},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {RD-net: Residual-dense network for glaucoma prediction using structural features of optic nerve head},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MeMIA: Multilevel ensemble membership inference attack. <em>TAI</em>, <em>6</em>(1), 93-106. (<a href='https://doi.org/10.1109/TAI.2024.3445326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leakage of private information in machine learning models can lead to breaches of confidentiality, identity theft, and unauthorized access to personal data. Ensuring the safe and trustworthy deployment of AI systems necessitates addressing privacy concerns to prevent unintentional disclosure and discrimination. One significant threat, membership inference (MI) attacks, exploit vulnerabilities in target learning models to determine if a given sample was part of the training set. However, the effectiveness of existing MI attacks is often limited by the number of classes in the dataset or the need for diverse multilevel adversarial features to exploit overfitted models. To enhance MI attack performance, we propose meMIA, a novel framework based on stacked ensemble learning. meMIA integrates embeddings from a neural network (NN) and a long short-term memory (LSTM) model, training a subsequent NN, termed the meta-model, on the concatenated embeddings. This method leverages the complementary strengths of NN and LSTM models; the LSTM captures order differences in confidence scores, while the NN discerns probability distribution differences between member and nonmember samples. We extensively evaluate meMIA on seven benchmark datasets, demonstrating that it surpasses current state-of-the-art MI attacks, achieving accuracy up to 94.6% and near-perfect recall. meMIA's superior performance, especially on datasets with fewer classes, underscores the urgent need for robust defenses against privacy attacks in machine learning, contributing to the safer and more ethical use of AI technologies.},
  archive      = {J_TAI},
  author       = {Najeeb Ullah and Muhammad Naveed Aman and Biplab Sikdar},
  doi          = {10.1109/TAI.2024.3445326},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {93-106},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {MeMIA: Multilevel ensemble membership inference attack},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preference prediction-based evolutionary multiobjective optimization for gasoline blending scheduling. <em>TAI</em>, <em>6</em>(1), 79-92. (<a href='https://doi.org/10.1109/TAI.2024.3444736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gasoline blending scheduling is challenging, involving multiple conflicting objectives and a large decision space with many mixed integers. Due to these difficulties, one promising solution is to use preference-based multiobjective evolutionary algorithms (PBMOEAs). However, in practical applications, suitable preferences of decision makers are often difficult to generalize and summarize from their operational experience. This article proposes a novel framework called preference prediction-based evolutionary multiobjective optimization (PP-EMO). In PP-EMO, suitable preferences for a new environment can be automatically obtained from historical operational experience by a machine learning-based preference prediction model when we feed the model with the input of the optimization environment. We have found that the predicted preference is able to guide the optimization to efficiently obtain a set of promising scheduling scenarios. Finally, we conducted comparative tests across various environments, and the experimental results demonstrate that the proposed PP-EMO framework outperforms existing methods. Compared with no preference, PP-EMO reduces operating costs by about 25% and decreases blending errors by 50% under demanding operational conditions.},
  archive      = {J_TAI},
  author       = {Wenxuan Fang and Wei Du and Guo Yu and Renchu He and Yang Tang and Yaochu Jin},
  doi          = {10.1109/TAI.2024.3444736},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {79-92},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Preference prediction-based evolutionary multiobjective optimization for gasoline blending scheduling},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive composite fixed-time RL-optimized control for nonlinear systems and its application to intelligent ship autopilot. <em>TAI</em>, <em>6</em>(1), 66-78. (<a href='https://doi.org/10.1109/TAI.2024.3444731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the article, an adaptive fixed-time reinforcement learning (RL) optimized control policy is given for nonlinear systems. Radial basis function neural networks (RBFNNs) are exploited to fit uncertain nonlinearities appeared in the considered systems and RL is applied under the critic-actor architecture by using RBFNNs. Specifically, a novel fixed-time smooth estimation system is proposed to improve the estimating performance of RBFNNs. The introduction of the hyperbolic tangent function effectively avoids the singularity problem of the derivative of the virtual controller. The stability analysis shows that the tracking error inclines to an adjustable region near the origin in a fixed-time interval and the boundedness of all signals is obtained. Finally, the intelligent ship autopilot is simulated to prove the utilizability of the obtained control way.},
  archive      = {J_TAI},
  author       = {Siwen Liu and Yi Zuo and Tieshan Li and Huanqing Wang and Xiaoyang Gao and Yang Xiao},
  doi          = {10.1109/TAI.2024.3444731},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {66-78},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Adaptive composite fixed-time RL-optimized control for nonlinear systems and its application to intelligent ship autopilot},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Migrant resettlement by evolutionary multiobjective optimization. <em>TAI</em>, <em>6</em>(1), 51-65. (<a href='https://doi.org/10.1109/TAI.2024.3443790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Migration has been a universal phenomenon, which brings opportunities as well as challenges for global development. As the number of migrants (e.g., refugees) increases rapidly, a key challenge faced by each country is the problem of migrant resettlement. This problem has attracted scientific research attention, from the perspective of maximizing the employment rate. Previous works mainly formulated migrant resettlement as an approximately submodular optimization problem subject to multiple matroid constraints and employed the greedy algorithm, whose performance, however, may be limited due to its greedy nature. In this article, we propose a new framework called migrant resettlement by evolutionary multiobjective optimization (MR-EMO), which reformulates migrant resettlement as a biobjective optimization problem that maximizes the expected number of employed migrants and minimizes the number of dispatched migrants simultaneously, and employs a multiobjective evolutionary algorithm (MOEA) to solve the biobjective problem. We implement MR-EMO using three MOEAs: the popular nondominated sorting genetic algorithm II (NSGA-II), MOEA based on decomposition (MOEA/D) as well as the theoretically grounded global simple evolutionary multiobjective optimizer (GSEMO). To further improve the performance of MR-EMO, we propose a specific MOEA, called GSEMO using matrix-swap mutation and repair mechanism (GSEMO-SR), which has a better ability to search for feasible solutions. We prove that MR-EMO using either GSEMO or GSEMO-SR can achieve better theoretical guarantees than the previous greedy algorithm. Experimental results under the interview and coordination migration models clearly show the superiority of MR-EMO (with either NSGA-II, MOEA/D, GSEMO or GSEMO-SR) over previous algorithms, and that using GSEMO-SR leads to the best performance of MR-EMO.},
  archive      = {J_TAI},
  author       = {Dan-Xuan Liu and Yu-Ran Gu and Chao Qian and Xin Mu and Ke Tang},
  doi          = {10.1109/TAI.2024.3443790},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {51-65},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Migrant resettlement by evolutionary multiobjective optimization},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intrusion detection approach for industrial internet of things traffic using deep recurrent reinforcement learning assisted federated learning. <em>TAI</em>, <em>6</em>(1), 37-50. (<a href='https://doi.org/10.1109/TAI.2024.3443787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of industrial Internet of Things (IIoT) applications generates massive amount of heterogeneous data that are prone to cyberattacks. The imperative is to secure industrial data from adversarial attacks and develop a robust and secure framework capable of withstanding sophisticated attacks. Toward this machine learning (ML) algorithms are used for intrusion detection by analyzing the devices’ network traffic. However, classical ML models work on entire datasets that are located on a central server and are not a suitable choice for a secure intrusion detection framework. We propose the federated learning (FL)-based network intrusion detection model for IIoT scenarios which only share learned parameters with the central server and keep the data intact to local servers only. The proposed model is assisted with gated recurrent units (GRUs) for FL training to extract temporal dependencies of network traffic attacks in order to improve intrusion detection accuracy. Additionally, to increase the model aggregation rate of FL, we integrate deep reinforcement learning (DRL) to select of IIoT devices with high quality while keeping data privacy and energy-efficiency as main concerns. In contrast to earlier approaches, we consider nonindependent and identically distributed (non-IID) data over recent IIoT datasets. Experimental findings indicate that the proposed framework outperforms state-of-the-art FL and non-FL intrusion detection models in terms of accuracy, precision, recall, F1-score, and receiver operating characterstics (ROC).},
  archive      = {J_TAI},
  author       = {Amandeep Kaur},
  doi          = {10.1109/TAI.2024.3443787},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {37-50},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Intrusion detection approach for industrial internet of things traffic using deep recurrent reinforcement learning assisted federated learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement learned multiagent cooperative navigation in hybrid environment with relational graph learning. <em>TAI</em>, <em>6</em>(1), 25-36. (<a href='https://doi.org/10.1109/TAI.2024.3443783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multirobot cooperative navigation problem (MCNP) is an essential topic in multiagent control. This article proposes a distributed approach named GAR-CoNav to solve the navigation problem of multiagent to multiple destinations in the face of static and dynamic obstacles. Agents are expected to travel to different destinations without conflicting with each other to achieve maximum efficiency. That is, cooperative navigation in hybrid environment. The velocity obstacle encoding is combined with a graph to build a global representation, which helps the agent capture complex interactions in hybrid environment. GAR-CoNav processes and aggregates environmental features through the graph attention network and has scalability for the changing number of entities in the graph. A novel reward function is developed to train agents to achieve an actual cooperative navigation policy. Extensive simulation experiments demonstrate that GAR-CoNav achieves better performance than state-of-the-art methods.},
  archive      = {J_TAI},
  author       = {Wen Ou and Biao Luo and Xiaodong Xu and Yu Feng and Yuqian Zhao},
  doi          = {10.1109/TAI.2024.3443783},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {25-36},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Reinforcement learned multiagent cooperative navigation in hybrid environment with relational graph learning},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Silver lining in the fake news cloud: Can large language models help detect misinformation?. <em>TAI</em>, <em>6</em>(1), 14-24. (<a href='https://doi.org/10.1109/TAI.2024.3440248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the times of advanced generative artificial intelligence, distinguishing truth from fallacy and deception has become a critical societal challenge. This research attempts to analyze the capabilities of large language models (LLMs) for detecting misinformation. Our study employs a versatile approach, covering multiple LLMs with few- and zero-shot prompting. These models are rigorously evaluated across various fake news and rumor detection datasets. Introducing a novel dimension, we additionally incorporate sentiment and emotion annotations to understand the emotional influence on misinformation detection using LLMs. Moreover, to extend our inquiry, we employ ChatGPT to intentionally distort authentic news as well as human-written fake news, utilizing zero-shot and iterative prompts. This deliberate corruption allows for a detailed examination of various parameters such as abstractness, concreteness, and named entity density, providing insights into differentiating between unaltered news, human-written fake news, and its LLM-corrupted counterpart. Our findings aspire to furnish a refined framework for discerning authentic news, human-generated misinformation, and LLM-induced distortions. This multifaceted approach, utilizing various prompt techniques, contributes to a comprehensive understanding of the subtle variations shaping misinformation sources.},
  archive      = {J_TAI},
  author       = {Raghvendra Kumar and Bhargav Goddu and Sriparna Saha and Adam Jatowt},
  doi          = {10.1109/TAI.2024.3440248},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {14-24},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Silver lining in the fake news cloud: Can large language models help detect misinformation?},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sector-based pairs trading strategy with novel pair selection technique. <em>TAI</em>, <em>6</em>(1), 3-13. (<a href='https://doi.org/10.1109/TAI.2024.3433469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A pair trading strategy (PTS) is a balanced approach that involves simultaneous trading of two highly correlated stocks. This article introduces the PTS-return-based pair selection (PTS-R) strategy which is the modification of the traditional PTS. The PTS-R follows a similar framework to the traditional PTS, differing only in the criteria it employs for selecting stock pairs. Moreover, this article proposes a novel trading strategy called sector-based pairs trading strategy (SBPTS) along with its two variants, namely SBPTS-correlation-based pair selection (SBPTS-C) and SBPTS-return-based pair selection (SBPTS-R). The SBPTS focuses on the pairs of stocks within the same sector. It consists of three innovative phases: the classification of input stocks into the respective sectors, the identification of the best-performing sector, and the selection of stock pairs based on their returns. The goal is to identify the pairs with a strong historical correlation and the highest returns within the best-performing sector. These chosen pairs are then used for trading. The strategies are designed to enhance the efficacy of the pairs trading and are validated through experimentation on real-world stock data over a ten-year historical period from 2013 to 2023. The results demonstrate their effectiveness compared to the existing techniques for pair selection and trading strategy.},
  archive      = {J_TAI},
  author       = {Pranjala G. Kolapwar and Uday V. Kulkarni and Jaishri M. Waghmare},
  doi          = {10.1109/TAI.2024.3433469},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {1},
  number       = {1},
  pages        = {3-13},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Sector-based pairs trading strategy with novel pair selection technique},
  volume       = {6},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
