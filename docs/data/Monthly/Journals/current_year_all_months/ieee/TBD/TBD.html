<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TBD</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tbd">TBD - 211</h2>
<ul>
<li><details>
<summary>
(2025). Topology-based node-level membership inference attacks on graph neural networks. <em>TBD</em>, <em>11</em>(5), 2809-2826. (<a href='https://doi.org/10.1109/TBDATA.2025.3558855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph neural networks (GNNs) have obtained considerable attention due to their ability to leverage the inherent topological and node information present in graph data. While extensive research has been conducted on privacy attacks targeting machine learning models, the exploration of privacy risks associated with node-level membership inference attacks on GNNs remains relatively limited. GNNs learn representations that encapsulate valuable information about the nodes. These learned representations can be exploited by attackers to infer whether a specific node belongs to the training dataset, leading to the disclosure of sensitive information. The insidious nature of such privacy breaches often leads to an underestimation of the associated risks. Furthermore, the inherent challenges posed by node membership inference attacks make it difficult to develop effective attack models for GNNs that can successfully infer node membership. We propose a more efficient approach that specifically targets node-level membership inference attacks on GNNs. Initially, we combine nodes and their respective neighbors to carry out node membership inference attacks. To address the challenge of variable-length features arising from the differing number of neighboring nodes, we introduce an effective feature processing strategy. Furthermore, we propose two strategies: multiple training of shadow models and random selection of non-membership data, to enhance the performance of the attack model. We empirically evaluate the efficacy of our proposed method using three benchmark datasets. Additionally, we explore two potential defense mechanisms against node-level membership inference attacks.},
  archive  = {J},
  author   = {Faqian Guan and Tianqing Zhu and Wanlei Zhou and Philip S. Yu},
  doi      = {10.1109/TBDATA.2025.3558855},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2809-2826},
  title    = {Topology-based node-level membership inference attacks on graph neural networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Utility-driven data analytics algorithm for transaction modifications using pre-large concept with single database scan. <em>TBD</em>, <em>11</em>(5), 2792-2808. (<a href='https://doi.org/10.1109/TBDATA.2025.3556615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Utility-driven pattern analysis is a fundamental method for analyzing noteworthy patterns with high utility for diverse quantitative transactional databases. Recently, various approaches have emerged to handle large, dynamic database environments more efficiently by reducing the number of data scans and pattern expansion operations with the pre-large concept. However, existing pre-large-based high utility pattern mining methods either fail to handle real-time transaction modifications or require additional data scans to validate candidate patterns. In this paper, we propose a novel efficient utility-driven pattern mining algorithm using the pre-large concept for transaction modifications. Our method incorporates a single-scan-based framework through the management of actual utility values and discovers high utility patterns without candidate generation for efficient utility-driven dynamic data analysis in the modification environment. We compared the performance of the proposed method with state-of-the-art methods through extensive performance evaluation utilizing real and synthetic datasets. According to the evaluation results and a case study, the suggested method performs a minimum of 1.5 times faster than state-of-the-art methods alongside minimal compromise in memory, and it scaled well with increases in database size. Further statistical analyses indicate that the proposed method reduces the pattern search space compared to the previous method while delivering a complete set of accurate results without loss.},
  archive  = {J},
  author   = {Unil Yun and Hanju Kim and Myungha Cho and Taewoong Ryu and Seungwan Park and Doyoon Kim and Doyoung Kim and Chanhee Lee and Witold Pedrycz},
  doi      = {10.1109/TBDATA.2025.3556615},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2792-2808},
  title    = {Utility-driven data analytics algorithm for transaction modifications using pre-large concept with single database scan},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel concept-cognitive learning model oriented to three-way concept for knowledge acquisition. <em>TBD</em>, <em>11</em>(5), 2779-2791. (<a href='https://doi.org/10.1109/TBDATA.2025.3556637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Concept-cognitive learning (CCL) is the process of enabling machines to simulate the concept learning of the human brain. Existing CCL models focus on formal context while neglecting the importance of skill context. Furthermore, CCL models, which solely focus on positive information, restrict the learning capacity by neglecting negative information, and greatly impeding the acquisition of knowledge. To overcome these issues, we proposes a novel concept-cognitive learning model oriented to three-way concept for knowledge acquisition. First, this paper explains and investigates the relationship between skills and knowledge based on the three-way concept and its properties. Then, in order to simultaneously consider positive and negative information, describe more detailed information, learn more skills, and acquire accurate knowledge, a three-way information granule is described from the perspective of cognitive learning. Then, a transformation method is proposed to transform between different three-way information granules, allowing for the transformation of arbitrary three-way information granule into necessary, sufficient, sufficient and necessary three-way information granules. Finally, algorithm corresponding to the transformation method is designed, and subsequently tested across diverse UCI datasets. The experimental outcomes affirm the effectiveness and excellence of the suggested model and algorithm.},
  archive  = {J},
  author   = {Weihua Xu and Di Jiang},
  doi      = {10.1109/TBDATA.2025.3556637},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2779-2791},
  title    = {A novel concept-cognitive learning model oriented to three-way concept for knowledge acquisition},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revocable DSSE in healthcare systems with range query support. <em>TBD</em>, <em>11</em>(5), 2764-2778. (<a href='https://doi.org/10.1109/TBDATA.2025.3556636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the rapid development of cloud computing, online health monitoring systems are becoming increasingly prevalent. To protect medical data privacy while supporting search operations, Dynamic Searchable Symmetric Encryption (DSSE) technology has been widely used in health monitoring systems. For better monitoring of patient status, keyword range query is also a necessary requirement for the DSSE scheme. Furthermore, in the multi-user setting, user revocation usually leads the owner to download and re-encrypt all indexes, resulting in significant computational overhead. In this paper, we propose a lightweight revocable DSSE scheme with range query support. First, we propose a novel and privacy-preserving range query algorithm that defends plaintext inference attacks. Second, we design a singly linked list structure based on delegatable pseudorandom functions and key-updatable pseudorandom functions, which support lightweight user revocation. Rigorous security analysis proves the security of our proposed range query scheme and demonstrates that our scheme can achieve forward and backward privacy. Experimental evaluations show that our scheme is highly efficient.},
  archive  = {J},
  author   = {Hanqi Zhang and Yandong Zheng and Chang Xu and Liehuang Zhu and Jiayin Wang},
  doi      = {10.1109/TBDATA.2025.3556636},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2764-2778},
  title    = {Revocable DSSE in healthcare systems with range query support},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Split learning on segmented healthcare data. <em>TBD</em>, <em>11</em>(5), 2749-2763. (<a href='https://doi.org/10.1109/TBDATA.2025.3556639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sequential data learning is vital to harnessing the encompassed rich knowledge for diverse downstream tasks, particularly in healthcare (e.g., disease prediction). Considering data sensitiveness, privacy-preserving learning methods, based on federated learning (FL) and split learning (SL), have been widely investigated. Yet, this work identifies, for the first time, existing methods overlook that sequential data are generated by different patients at different times and stored in different hospitals, failing to learn the sequential correlations between different temporal segments. To fill this void, a novel distributed learning framework STSL is proposed by training a model on the segments in order. Considering that patients have different visit sequences, STSL first implements privacy-preserving visit ordering based on a secure multi-party computation mechanism. Then batch scheduling participates patients with similar visit (sub-)sequences into the same training batch, facilitating subsequent split learning on batches. The scheduling process is formulated as an NP-hard optimization problem on balancing learning loss and efficiency and a greedy-based solution is presented. Theoretical analysis proves the privacy preservation property of STSL. Experimental results on real-world eICU data show its superior performance compared with FL and SL ($5\% \sim 28\%$ better accuracy) and effectiveness (a remarkable 75% reduction in communication costs).},
  archive  = {J},
  author   = {Ling Hu and Tongqing Zhou and Zhihuang Liu and Fang Liu and Zhiping Cai},
  doi      = {10.1109/TBDATA.2025.3556639},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2749-2763},
  title    = {Split learning on segmented healthcare data},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PViTGAtt-IP: Severity quantification of lung infections in chest X-rays and CT scans via parallel and cross-attended encoders. <em>TBD</em>, <em>11</em>(5), 2736-2748. (<a href='https://doi.org/10.1109/TBDATA.2025.3556612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The development of a robust and adaptive deep learning technique for the diagnosis of pneumonia and the assessment of its severity was a major challenge. Indeed, both chest X-rays (CXR) and CT scans have been widely studied for the diagnosis, detection and quantification of pneumonia. In this paper, a novel approach (PViTGAtt-IP) based on a parallel array of vision transformers is presented, in which the input image is divided into regions of interest. Each region is fed into an individual model and the collective output gives the severity score. Three parallel architectures were also derived and tested. The proposed models were subjected to rigorous tests on two different datasets: RALO CXRs and Per COVID-19 CT scans. The experimental results showed that the proposed models exhibited high performance in accurately predicting scores for both datasets. In particular, the parallel transformers with multi-gate attention proved to be the best performing model. Furthermore, a comparative analysis using state-of-the-art methods showed that our proposed approach consistently achieved competitive or even better performance in terms of the Mean Absolute Error (MAE) and the Pearson Correlation Coefficient (PC). This emphasizes the effectiveness and superiority of our models in the context of diagnosing and assessing the severity of pneumonia.},
  archive  = {J},
  author   = {Bouthaina Slika and Fadi Dornaika and Fares Bougourzi and Karim Hammoudi},
  doi      = {10.1109/TBDATA.2025.3556612},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2736-2748},
  title    = {PViTGAtt-IP: Severity quantification of lung infections in chest X-rays and CT scans via parallel and cross-attended encoders},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incorporating confused phraseological knowledge based on pinyin input method for chinese spelling correction. <em>TBD</em>, <em>11</em>(5), 2724-2735. (<a href='https://doi.org/10.1109/TBDATA.2025.3552344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Chinese Spelling Correction (CSC) is designed to detect and correct spelling errors that occur in Chinese text. In real life, most keyboard input scenarios use the pinyin input method. Researching spelling errors in this scenario is practical and valuable. However, there is currently no research that has truly proposed a model suitable for this scenario. Considering this concern, this paper proposes a model IPCK-IME, which incorporates confused phraseological knowledge based on the pinyin input method. The model integrates its own phonetic features with external similarity knowledge to guide the model to output more correct characters. Furthermore, to mitigate the influence of spelling errors on the semantics of sentences, a Gaussian bias is introduced into the self-attention network of the model. This approach aims to reduces the focus on typos and improve attention to local context. Empirical evidence indicates that our method surpasses existing models in correcting spelling errors generated by the pinyin input method. And, it is more appropriate for correcting Chinese spelling errors in real input scenarios.},
  archive  = {J},
  author   = {Weidong Zhao and Xiaoyu Wang and Liqing Qiu},
  doi      = {10.1109/TBDATA.2025.3552344},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2724-2735},
  title    = {Incorporating confused phraseological knowledge based on pinyin input method for chinese spelling correction},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive graph structure learning neural rough differential equations for multivariate time series forecasting. <em>TBD</em>, <em>11</em>(5), 2710-2723. (<a href='https://doi.org/10.1109/TBDATA.2025.3552334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multivariate time series forecasting has extensive applications in urban computing, such as financial analysis, weather prediction, and traffic forecasting. Using graph structures to model the complex correlations among variables in time series, and leveraging graph neural networks and recurrent neural networks for temporal aggregation and spatial propagation stage, has shown promise. However, traditional methods’ graph structure node learning and discrete neural architecture are not sensitive to issues such as sudden changes, time variance, and irregular sampling often found in real-world data. To address these challenges, we propose a method called Adaptive Graph structure Learning neural Rough Differential Equations (AGLRDE). Specifically, we combine dynamic and static graph structure learning to adaptively generate a more robust graph representation. Then we employ a spatio-temporal encoder-decoder based on Neural Rough Differential Equations (Neural RDE) to model spatio-temporal dependencies. Additionally, we introduce a path reconstruction loss to constrain the path generation stage. We conduct experiments on six benchmark datasets, demonstrating that our proposed method outperforms existing state-of-the-art methods. The results show that AGLRDE effectively handles aforementioned challenges, significantly improving the accuracy of multivariate time series forecasting.},
  archive  = {J},
  author   = {Yuming Su and Tinghuai Ma and Huan Rong and Mohamed Magdy Abdel Wahab},
  doi      = {10.1109/TBDATA.2025.3552334},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2710-2723},
  title    = {Adaptive graph structure learning neural rough differential equations for multivariate time series forecasting},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective graph contrastive learning for recommendation. <em>TBD</em>, <em>11</em>(5), 2696-2709. (<a href='https://doi.org/10.1109/TBDATA.2025.3552341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, numerous studies have integrated self-supervised contrastive learning with Graph Convolutional Networks (GCNs) to address the data sparsity and popularity bias to enhance recommendation performance. While such studies have made breakthroughs in accuracy metric, they often neglect non-accuracy objectives such as diversity, novelty and percentage of long-tail items, which greatly reduces the user experience in real-world applications. To this end, we propose a novel graph collaborative filtering model named Multi-Objective Graph Contrastive Learning for recommendation (MOGCL), designed to provide more comprehensive recommendations by considering multiple objectives. Specifically, MOGCL comprises three modules: a multi-objective embedding generation module, an embedding fusion module and a transfer learning module. In the multi-objective embedding generation module, we employ two GCN encoders with different goal orientations to generate node embeddings targeting accuracy and non-accuracy objectives, respectively. These embeddings are then effectively fused with complementary weights in the embedding fusion module. In the transfer learning module, we suggest an auxiliary self-supervised task to promote the maximization of the mutual information of the two sets of embeddings, so that the obtained final embeddings are more stable and comprehensive. The experimental results on three real-world datasets show that MOGCL achieves optimal trade-offs between multiple objectives comparing to the state-of-the-arts.},
  archive  = {J},
  author   = {Lei Zhang and Mingren Ke and Likang Wu and Wuji Zhang and Zihao Chen and Hongke Zhao},
  doi      = {10.1109/TBDATA.2025.3552341},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2696-2709},
  title    = {Multi-objective graph contrastive learning for recommendation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MHT-net: A matching-based hierarchical transfer network for glaucoma detection from fundus images. <em>TBD</em>, <em>11</em>(5), 2681-2695. (<a href='https://doi.org/10.1109/TBDATA.2025.3552342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Glaucoma is a chronic and irreversible eye disease. Early detection and treatment can effectively prevent severe consequences. Deep transfer learning is widely used in fundus imaging analysis to remedy the shortage of training data of glaucoma. The model trained on the source domain may struggle to predict glaucoma in the target domain due to distribution differences. Several limitations cannot be ignored: (1) Image matching: enhancing global and local image consistency through bidirectional matching; (2) Hierarchical transfer: developing a strategy for transferring different hierarchical features. To this end, we propose a novel Matched Hierarchical Transfer Network (MHT-Net) to achieve automatic glaucoma detection. We initially create a fundus structure detector to match global and local images using intermediate layers of a pre-trained diagnostic model with source domain data. Next, a hierarchical transfer network is implemented, sharing parameters for general features and using a domain discriminator for specific features. By integrating adversarial and classification losses, the model acquires domain-invariant features, facilitating precise and seamless transfer of fundus information from source to target domains. Extensive experiments demonstrate the effectiveness of our proposed method, outperforming existing glaucoma detection methods. These advantages endow our algorithm as a promising efficient assisted tool in the glaucoma screening.},
  archive  = {J},
  author   = {Linna Zhao and Jianqiang Li and Li Li and Xi Xu},
  doi      = {10.1109/TBDATA.2025.3552342},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2681-2695},
  title    = {MHT-net: A matching-based hierarchical transfer network for glaucoma detection from fundus images},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-guided graph refinement with progressive fusion for multiplex graph contrastive representation learning. <em>TBD</em>, <em>11</em>(5), 2669-2680. (<a href='https://doi.org/10.1109/TBDATA.2025.3552331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multiplex Graph Contrastive Learning (MGCL) has attracted significant attention. However, existing MGCL methods often struggle with suboptimal graph structures and fail to fully capture intricate interdependencies across multiplex views. To address these issues, we propose a novel self-supervised framework, Multiplex Graph Refinement with progressive fusion (MGRefine), for multiplex graph contrastive representation learning. Specifically, MGRefine introduces a multi-view learning module to extract a structural guidance matrix by exploring the underlying relationships between nodes. Then, a progressive fusion module is employed to progressively enhance and fuse representations from different views, capturing and leveraging nuanced interdependencies and comprehensive information across the multiplex graphs. The fused representation is then used to construct a consensus guidance matrix. A self-enhanced refinement module continuously refines the multiplex graphs using these guidance matrices while providing effective supervision signals. MGRefine achieves mutual reinforcement between graph structures and representations, ensuring continuous optimization of the model throughout the learning process in a self-enhanced manner. Extensive experiments demonstrate that MGRefine outperforms state-of-the-art methods and also verify the effectiveness of MGRefine across various downstream tasks on several benchmark datasets.},
  archive  = {J},
  author   = {Qi Dai and Yu Gu and Xiaofeng Zhu and Xiaohua Li and Fangfang Li and Ge Yu},
  doi      = {10.1109/TBDATA.2025.3552331},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2669-2680},
  title    = {Self-guided graph refinement with progressive fusion for multiplex graph contrastive representation learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSST: Multi-scale spatial-temporal representation learning for trajectory similarity computation. <em>TBD</em>, <em>11</em>(5), 2657-2668. (<a href='https://doi.org/10.1109/TBDATA.2025.3552340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Computing trajectory similarity is a fundamental task in trajectory analysis. Traditional heuristic methods suffer from quadratic computational complexity, which limits their scalability to large datasets. Recently, Trajectory Representation Learning (TRL) has been extensively studied to address this limitation. However, most existing TRL algorithms face two key challenges. First, they prioritize spatial similarity while neglecting the intricate spatio-temporal dynamics of trajectories, particularly temporal regularities. Second, these methods are often constrained by predefined single spatial or temporal scales, which can significantly impact performance, since the measurement of trajectory similarity depends on spatial and temporal resolution. To address these issues, we propose MSST, a Multi-Scale Self-supervised Trajectory Representation Learning framework. MSST simultaneously processes spatial and temporal information by generating 3D spatial-temporal tokens, thereby capturing spatio-temporal characteristics of trajectories more effectively. Further, MSST explore the multi-scale characteristics of trajectories. Finally, self-supervised contrastive learning is employed to enhance the consistency between the trajectory representations from different views. Experimental results on three real-world datasets for similarity trajectory computation provide insight into the design properties of our approach and demonstrate the superiority of our approach over existing TRL methods. MSST significantly surpasses all state-of-the-art competitors in terms of effectiveness, efficiency, and robustness. We explore the multi-scale characteristics of trajectories. To the best of our knowledge, this is the first effort in the TRL literature. Compared to previous TRL research, the proposed method can balance the noise and the details of trajectories, enabling a more comprehensive analysis by accounting for the variability inherent in trajectory data across different scales.},
  archive  = {J},
  author   = {Li Li and Junjun Si and Jinna Lv and Junting Lu and Jianyu Zhang and Shuaifu Dai},
  doi      = {10.1109/TBDATA.2025.3552340},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2657-2668},
  title    = {MSST: Multi-scale spatial-temporal representation learning for trajectory similarity computation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing weak supervision for concept prerequisite relation learning. <em>TBD</em>, <em>11</em>(5), 2643-2656. (<a href='https://doi.org/10.1109/TBDATA.2025.3552330'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Concept prerequisite relation learning is used to identify dependency relations between knowledge concepts, which helps learners choose effective learning paths. Currently, most of the mainstream methods utilise deep learning algorithms to capture the prerequisite relations between concepts through supervised or semi-supervised learning. However, these methods are highly dependent on labelled data, which is scarce and costly to annotate in reality. To address this problem, we propose a framework called Weakly Supervised Enhanced Concept Prerequisite Relation Learning (WSECPRL). Specifically, we first generate an enhanced concept pseudo-relation graph without labeled data using the pre-trained language model and the large knowledge base as auxiliary information. Second, we propose an improved variational graph auto-encoder model to correctly determine the concept prerequisite relations. We incorporate a multi-head attention mechanism to enhance the representation learning capability of weakly supervised learning. The model reconstructs a directed graph into multiple undirected graphs by splitting the adjacency matrix and determines the direction of the concept prerequisite relation based on the strength of the dependency relation between concepts. Finally, experimental results on several publicly available datasets demonstrate the effectiveness of our proposed framework, with WSECPRL outperforming existing baseline models in terms of F1 scores and AUC.},
  archive  = {J},
  author   = {Miao Zhang and Jiawei Wang and Kui Xiao and Zhifang Huang and Zhifei Li and Yan Zhang},
  doi      = {10.1109/TBDATA.2025.3552330},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2643-2656},
  title    = {Enhancing weak supervision for concept prerequisite relation learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NetPrompt: Neural network prompting enhances event extraction in large language models. <em>TBD</em>, <em>11</em>(5), 2628-2642. (<a href='https://doi.org/10.1109/TBDATA.2025.3552333'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Event Extraction involves extracting event-related information such as event types and event arguments from context, which has long been tackled through well-designed neural networks or fine-tuned pre-trained language models. These approaches require substantial annotated data for tuning parameters and are resource-intensive. Recently, Prompting strategies with frozen parameters, such as Chain-of-Thought and Self-Consistency, have delivered success in NLP using LLMs by generating intermediate thought steps. However, they suffer from the challenge of error propagation and lack of interaction between different thoughts. In this paper, we propose Neural Network-based Prompting (NetPrompt), a novel network-structured prompting strategy for event extraction. The core idea behind NetPrompt is to imitate the excellent information integration capabilities of neural network structures. Specifically, we first decompose the event extraction problem into diverse intermediate subtasks, and each subtask is represented as a node in different layers of the network, the output of the nodes in the preceding layer is fed into the subsequent layer. Secondly, we propose pruning strategies to adapt the reasoning overhead to different problems. Finally, we have conducted extensive experiments on two widely used event extraction benchmarks to evaluate NetPrompt. The results demonstrated that NetPrompt significantly improved the event extraction performance compared to previous methods.},
  archive  = {J},
  author   = {Lin Mu and Yide Cheng and Jun Shen and Yiwen Zhang and Hong Zhong},
  doi      = {10.1109/TBDATA.2025.3552333},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2628-2642},
  title    = {NetPrompt: Neural network prompting enhances event extraction in large language models},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Residual learning for self-knowledge distillation: Enhancing neural networks through consistency across layers. <em>TBD</em>, <em>11</em>(5), 2615-2627. (<a href='https://doi.org/10.1109/TBDATA.2025.3552326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Knowledge distillation is widely used technique to transfer knowledge from a large pretrained teacher network to a small student network. However, training complex teacher models requires significant computational resources and storage. To address this, a growing area of research, known as self-knowledge distillation (Self-KD), aims to enhance the performance of a neural network by leveraging its own latent knowledge. Despite its potential, existing Self-KD methods often struggle to effectively extract and utilize the model's dark knowledge. In this work, we identify a consistency problem between feature layer and output layer, and propose a novel Self-KD approach called Residual Learning for Self-Knowledge Distillation (RSKD). Our method addresses this issue by enabling the last feature layer of the student model learn the residual gap between the outputs of the pseudo-teacher and the student. Additionally, we extend RSKD by allowing each intermediate feature layer of the student model to learn the residual gap between the corresponding deeper features of the pseudo-teacher and the student. Extensive experiments on various visual datasets demonstrate the effectiveness of the proposed method, which outperforms the state-of-the-art baselines.},
  archive  = {J},
  author   = {Hanpeng Liu and Shuoxi Zhang and Kun He},
  doi      = {10.1109/TBDATA.2025.3552326},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2615-2627},
  title    = {Residual learning for self-knowledge distillation: Enhancing neural networks through consistency across layers},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal subdata selection for prediction based on the distribution of the covariates. <em>TBD</em>, <em>11</em>(5), 2601-2614. (<a href='https://doi.org/10.1109/TBDATA.2025.3552343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Huge data sets are widely available now and there is growing interest in selecting an optimal subsample from the full data set to improve inference efficiency and reduce labeling costs. We propose a new criterion called J–optimality, that builds upon a popular optimal selection criterion that minimizes the Random–X prediction error by additionally incorporating the joint distribution of the covariates. A key advantage of our approach is that we can relate the subsampling selection problem to that of finding an optimal approximate design under a convex criterion, where analytical tools for finding and studying them are already available. Consequently, the J–optimal subsampling method comes with theoretical results and theory-based algorithms for finding them. Simulation results and real data analysis show our proposed methods outperform current subsampling methods and the proposed algorithms can also adapt efficiently to select an optimal subsample from streaming data.},
  archive  = {J},
  author   = {Alvaro Cia-Mina and Jesus Lopez-Fidalgo and Weng Kee Wong},
  doi      = {10.1109/TBDATA.2025.3552343},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2601-2614},
  title    = {Optimal subdata selection for prediction based on the distribution of the covariates},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient antagonistic $k$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math>-plex enumeration in signed graphs. <em>TBD</em>, <em>11</em>(5), 2587-2600. (<a href='https://doi.org/10.1109/TBDATA.2025.3552335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A signed graph is a graph where each edge receives a sign, positive or negative. The signed graph model has been used in many real applications, such as protein complex discovery and social network analysis. Finding cohesive subgraphs in signed graphs is a fundamental problem. A $k$-plex is a common model for cohesive subgraphs in which every vertex is adjacent to all but at most $k$ vertices within the subgraph. In this paper, we propose the model of size-constrained antagonistic $k$-plex in a signed graph. The proposed model guarantees that the resulting subgraph is a $k$-plex and can be divided into two sub-$k$-plexes, both of which have positive inner edges and negative outer edges. This paper aims to identify all maximal antagonistic $k$-plexes in a signed graph. Through rigorous analysis, we show that the problem is NP-Hardness. We propose a novel framework for maximal antagonistic $k$-plexes utilizing set enumeration. Efficiency is improved through pivot pruning and early termination based on the color bound. Preprocessing techniques based on degree and dichromatic graphs effectively narrow the search space before enumeration. Extensive experiments on real-world datasets demonstrate our algorithm’s efficiency, effectiveness, and scalability.},
  archive  = {J},
  author   = {Lantian Xu and Rong-Hua Li and Dong Wen and Qiangqiang Dai and Guoren Wang},
  doi      = {10.1109/TBDATA.2025.3552335},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2587-2600},
  title    = {Efficient antagonistic $k$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math>-plex enumeration in signed graphs},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive architecture search for deep graph neural networks. <em>TBD</em>, <em>11</em>(5), 2572-2586. (<a href='https://doi.org/10.1109/TBDATA.2025.3552336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, Neural Architecture Search (NAS) has emerged as a promising approach for automatically discovering superior model architectures for deep Graph Neural Networks (GNNs). Different methods have paid attention to different types of search spaces. However, due to the time-consuming nature of training deep GNNs, existing NAS methods often fail to explore diverse search spaces sufficiently, which constrains their effectiveness. To crack this hard nut, we propose CAS-DGNN, a novel comprehensive architecture search method for deep GNNs. It encompasses four kinds of search spaces that are the composition of aggregate and update operators, different types of aggregate operators, residual connections, and hyper-parameters. To meet the needs of such a complex situation, a phased and hybrid search strategy is proposed to accommodate the diverse characteristics of different search spaces. Specifically, we divide the search process into four phases, utilizing evolutionary algorithms and Bayesian optimization. Meanwhile, we design two distinct search methods for residual connections (All-connected search and Initial Residual search) to streamline the search space, which enhances the scalability of CAS-DGNN. The experimental results show that CAS-DGNN achieves higher accuracy with competitive search costs across ten public datasets compared to existing methods.},
  archive  = {J},
  author   = {Yukang Dong and Fanxing Pan and Yi Gui and Wenbin Jiang and Yao Wan and Ran Zheng and Hai Jin},
  doi      = {10.1109/TBDATA.2025.3552336},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2572-2586},
  title    = {Comprehensive architecture search for deep graph neural networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust semi-supervised deep nonnegative matrix factorization with constraint propagation for data representation. <em>TBD</em>, <em>11</em>(5), 2557-2571. (<a href='https://doi.org/10.1109/TBDATA.2025.3547174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep nonnegative matrix factorization (DNMF) technique has attached much great attention in recent year, since it can effectively discover the underlying hierarchical structure of complex data. However, most existing unsupervised and semi-supervised DNMF approaches not only suffer from the noisy data seriously, but also fail to enhance the decomposition quality of DNMF obviously by using the obtained supervisory information. To overcome these drawbacks, a robust semi-supervised DNMF method, called the correntropy based semi-supervised DNMF with constraint propagation (CSDCP), is proposed in this paper for learning a compact and meaningful data representation from the original data. Particularly, instead of adopting the traditional Frobenius norm, CSDCP employs the nonlinear and local similarity measure (e.g., correntropy) as the loss function in DNMF to enhance the robustness of DNMF for the noisy data. In addition, the hypergraph based constraint propagation (HCP) algorithm is adopted in CSDCP to exploit the limited supervisory information fully for capturing good data representation. Moreover, algorithm analysis of CSDCP is presented in this paper, including convergence analysis, robustness analysis supervised information analysis, and computational complexity. Extensive experimental results have illustrated that, in comparison to the most related DNMF approaches, CSDCP usually has better clustering results on six nonnegative datasets in clustering tasks.},
  archive  = {J},
  author   = {Siyuan Peng and Jingxing Yin and Zhijing Yang and Feiping Nie and Badong Chen},
  doi      = {10.1109/TBDATA.2025.3547174},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2557-2571},
  title    = {Robust semi-supervised deep nonnegative matrix factorization with constraint propagation for data representation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How to decide like human? a commonsense-aware hierarchical framework for knowledge graph reasoning. <em>TBD</em>, <em>11</em>(5), 2545-2556. (<a href='https://doi.org/10.1109/TBDATA.2025.3544126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Reasoning over knowledge graphs has attracted considerable attention from researchers and is being widely applied to contribute question answering systems, recommender systems, and other information retrieval systems. However, existing reasoning methods tend to suffer from poor interpretability which is not consistent with human commonsense. The trustworthiness and reliability of the knowledge discover outcomes thus decreased as a result. Inspired by the process of human decision-making, we propose a commonsense-aware hierarchical framework called HDLH, which incorporates commonsense knowledge into hierarchical knowledge graph reasoning process with deep reinforcement learning. HDLH implements hierarchical reasoning process through exploration and exploitation sequentially by applying multi-agent reinforcement learning. Multiple agents in HDLH simulate the multi-level decision-making ability of humans, and reason hierarchically and reasonably to maintain its efficiency and interpretability. Moreover, commonsense knowledge is incorporated by means of the reward-shaping function, ultimately guiding the agent to reason more consistently with human perceptions and reduce the huge search space. We evaluated HDLH with various tasks on five real-world datasets. The experimental results reveal that HDLH achieves better performance compared with state-of-the-art baseline models.},
  archive  = {J},
  author   = {Yi Xia and Gang Zhou and Junyong Luo and Mingjing Lan and Ningbo Huang},
  doi      = {10.1109/TBDATA.2025.3544126},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2545-2556},
  title    = {How to decide like human? a commonsense-aware hierarchical framework for knowledge graph reasoning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Higher-order community detection by motif-based modularity optimization. <em>TBD</em>, <em>11</em>(5), 2529-2544. (<a href='https://doi.org/10.1109/TBDATA.2025.3544129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently higher-order community detection based on network motifs has received increasing attention, because motif-based communities reflect not only mesoscale structures but also functional characteristics of real-life networks. In this study, we propose a Modularity Optimization method for Motif-based Community Detection (MOMCD). In order to approximate the global optimum in modularity optimization, an improved nature-inspired metaheuristic algorithm is proposed as optimization strategy. In addition, by comprehensively utilizing motif-based (higher-order) and edge-based (lower-order) structural information, a neighbor community modification operation and a local search operation are also designed to improve the quality of individuals and promote the convergence of MOMCD. Experimental results show that MOMCD is promising and competitive in identifying motif-based communities from synthetic and real-life networks, which outperforms state-of-the-art approaches in terms of quality and accuracy, and deepens our understanding of network structural and functional characteristics.},
  archive  = {J},
  author   = {Jing Xiao and Yu-Cheng Zou and Xiao-Ke Xu},
  doi      = {10.1109/TBDATA.2025.3544129},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2529-2544},
  title    = {Higher-order community detection by motif-based modularity optimization},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LightST: A simplifying spatio-temporal graph neural network for traffic flow forecasting. <em>TBD</em>, <em>11</em>(5), 2517-2528. (<a href='https://doi.org/10.1109/TBDATA.2025.3544131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Traffic flow forecasting task plays an essential role in intelligent transportation systems. Accurately capturing the intricate spatio-temporal dependencies in traffic network signals is the core of precise prediction. Recently, a paradigm that models spatio-temporal dependencies through graph neural networks and time series models has become one of the most promising methods to solve this problem. However, existing methods still have limitations due to ineffectively modeling dynamic spatial dependencies and high time and space complexity. To address these issues, we propose a simplifying and powerful general spatio-temporal traffic flow forecasting model called LightST. Specifically, LightST first embeds temporal covariates and spatial position information to enhance the spatio-temporal modeling capabilities. Then, stacked temporal linear layers are introduced to capture temporal dependencies efficiently. Finally,we propose a concise adaptive spatio-temporal embedding graph convolution method to extract implicit spatial dependencies over time via dynamic graph convolution with adaptive spatio-temporal embedding graph generation. Extensive experiment results on four public traffic flow datasets demonstrate the superiority of our LightST concerning computational efficiency and prediction performance.},
  archive  = {J},
  author   = {Jie Hu and Taichuan Zheng and Lilan Peng and Fei Teng and Shengdong Du and Tianrui Li},
  doi      = {10.1109/TBDATA.2025.3544131},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2517-2528},
  title    = {LightST: A simplifying spatio-temporal graph neural network for traffic flow forecasting},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A theoretical analysis of efficiency constrained utility-privacy bi-objective optimization in federated learning. <em>TBD</em>, <em>11</em>(5), 2503-2516. (<a href='https://doi.org/10.1109/TBDATA.2025.3534622'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated learning (FL) enables multiple clients to collaboratively learn a shared model without sharing their individual data. Concerns about utility, privacy, and training efficiency in FL have garnered significant research attention. Differential privacy has emerged as a prevalent technique in FL, safeguarding the privacy of individual user data while impacting utility and training efficiency. Within Differential Privacy Federated Learning (DPFL), previous studies have primarily focused on the utility-privacy trade-off, neglecting training efficiency, which is crucial for timely completion. Moreover, differential privacy achieves privacy by introducing controlled randomness (noise) on selected clients in each communication round. Previous work has mainly examined the impact of noise level ($\sigma$) and communication rounds ($T$) on the privacy-utility dynamic, overlooking other influential factors like the sample ratio ($q$, the proportion of selected clients). This paper systematically formulates an efficiency-constrained utility-privacy bi-objective optimization problem in DPFL, focusing on $\sigma$, $T$, and $q$. We provide a comprehensive theoretical analysis, yielding analytical solutions for the Pareto front. Extensive empirical experiments verify the validity and efficacy of our analysis, offering valuable guidance for low-cost parameter design in DPFL.},
  archive  = {J},
  author   = {Hanlin Gu and Xinyuan Zhao and Gongxi Zhu and Yuxing Han and Yan Kang and Lixin Fan and Qiang Yang},
  doi      = {10.1109/TBDATA.2025.3534622},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2503-2516},
  title    = {A theoretical analysis of efficiency constrained utility-privacy bi-objective optimization in federated learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk-constrained reinforcement learning with augmented lagrangian multiplier for portfolio optimization. <em>TBD</em>, <em>11</em>(5), 2489-2502. (<a href='https://doi.org/10.1109/TBDATA.2025.3533905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We explored the application of Risk-averse Reinforcement Learning (Risk-averse RL) in Constrained Markov Decision Process (CMDP) in optimizing investment portfolios, incorporating constraints assessment. The investment portfolio must be always constrained with risk characteristics by investors and regulators. Therefore, the hard constraint is necessary for the practical Portfolio optimization. Moreover, traditional portfolio optimization techniques lack flexibility to model complex dynamic financial market. To address this issue, Augmented Lagrangian Multiplier (ALM) was employed to enforce constraints on the agent, mitigating the impact of risk in the decision process. Our proposal of the risk-constrained RL algorithm demonstrated no constraint violations during the testing phase, and outperformance compared to other Risk-averse RL algorithms, fulfilling our primary goal. This suggests that incorporating a risk-constrained RL technique holds promise for portfolio optimization, particularly for risk-averse investors.},
  archive  = {J},
  author   = {Bayaraa Enkhsaikhan and Ohyun Jo},
  doi      = {10.1109/TBDATA.2025.3533905},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2489-2502},
  title    = {Risk-constrained reinforcement learning with augmented lagrangian multiplier for portfolio optimization},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MultiTec: A data-driven multimodal short video detection framework for healthcare misinformation on TikTok. <em>TBD</em>, <em>11</em>(5), 2471-2488. (<a href='https://doi.org/10.1109/TBDATA.2025.3533919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the prevalence of social media and short video sharing platforms (e.g., TikTok, YouTube Shorts), the proliferation of healthcare misinformation has become a widespread and concerning issue that threatens public health and undermines trust in mass media. This paper focuses on an important problem of detecting multimodal healthcare misinformation in short videos on TikTok. Our objective is to accurately identify misleading healthcare information that is jointly conveyed by the visual, audio, and textual content within the TikTok short videos. Three critical challenges exist in solving our problem: i) how to effectively extract information from distractive and manipulated visual content in short videos? ii) How to efficiently identify the interrelation of the heterogeneous visual and speech content in short videos? iii) How to accurately capture the complex dependency of the densely connected sequential content in short videos? To address the above challenges, we develop MultiTec, a multimodal detector that explicitly explores the audio and visual content in short videos to investigate both the sequential relation of video elements and their inter-modality dependencies to jointly detect misinformation in healthcare videos on TikTok. To the best of our knowledge, MultiTec is the first modality-aware dual-attentive short video detection model for multimodal healthcare misinformation on TikTok. We evaluate MultiTec on two real-world healthcare video datasets collected from TikTok. Evaluation results show that MultiTec achieves substantial performance gains compared to state-of-the-art baselines in accurately detecting misleading healthcare short videos.},
  archive  = {J},
  author   = {Lanyu Shang and Yang Zhang and Yawen Deng and Dong Wang},
  doi      = {10.1109/TBDATA.2025.3533919},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2471-2488},
  title    = {MultiTec: A data-driven multimodal short video detection framework for healthcare misinformation on TikTok},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable learning-based community-preserving graph generation. <em>TBD</em>, <em>11</em>(5), 2457-2470. (<a href='https://doi.org/10.1109/TBDATA.2025.3533898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph generation plays an essential role in understanding the formation of complex network structures across various fields, such as biological and social networks. Recent studies have shifted towards employing deep learning methods to grasp the topology of graphs. Yet, most current graph generators fail to adequately capture the community structure, which stands out as a critical and distinctive aspect of graphs. Additionally, these generators are generally limited to smaller graphs because of their inefficiencies and scaling challenges. This paper introduces the Community-Preserving Graph Adversarial Network (CPGAN), designed to effectively simulate graphs. CPGAN leverages graph convolution networks within its encoder and maintains shared parameters during generation to encapsulate community structure data and ensure permutation invariance. We also present the Scalable Community-Preserving Graph Attention Network (SCPGAN), aimed at enhancing the scalability of our model. SCPGAN considerably cuts down on inference and training durations, as well as GPU memory usage, through the use of an ego-graph sampling approach and a short-pipeline autoencoder framework. Tests conducted on six real-world graph datasets reveal that CPGAN manages a beneficial balance between efficiency and simulation quality when compared to leading-edge baselines. Moreover, SCPGAN marks substantial strides in model efficiency and scalability, successfully increasing the size of generated graphs to the 10 million node level while maintaining competitive quality, on par with other advanced learning models.},
  archive  = {J},
  author   = {Sheng Xiang and Chenhao Xu and Dawei Cheng and Ying Zhang},
  doi      = {10.1109/TBDATA.2025.3533898},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2457-2470},
  title    = {Scalable learning-based community-preserving graph generation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CTDI: CNN-transformer-based spatial-temporal missing air pollution data imputation. <em>TBD</em>, <em>11</em>(5), 2443-2456. (<a href='https://doi.org/10.1109/TBDATA.2025.3533882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate and comprehensive air pollution data is essential for understanding and addressing environmental challenges. Missing data can impair accurate analysis and decision-making. This study presents a novel approach, named CNN-Transformer-based Spatial-Temporal Data Imputation (CTDI), for imputing missing air pollution data. Data pre-processing incorporates observed air pollution data and related urban data to produce 24-hour period tensors as input samples. 1-by-1 CNN layers capture the interaction between different types of input data. Deep learning transformer architecture is employed in a spatial-temporal (S-T) transformer module to capture long-range dependencies and extract complex relationships in both spatial and temporal dimensions. Hong Kong air pollution data is statistically analyzed and used to evaluate CTDI in its recovery of generated and actual patterns of missing data. Experimental results show that CTDI consistently outperforms existing imputation methods across all evaluated scenarios, including cases with higher rates of missing data, thereby demonstrating its robustness and effectiveness in enhancing air quality monitoring. Additionally, ablation experiments reveal that each component significantly contributes to the model's performance, with the temporal transformer proving particularly crucial under varying rates of missing data.},
  archive  = {J},
  author   = {Yangwen Yu and Victor O. K. Li and Jacqueline C. K. Lam and Kelvin Chan and Qi Zhang},
  doi      = {10.1109/TBDATA.2025.3533882},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2443-2456},
  title    = {CTDI: CNN-transformer-based spatial-temporal missing air pollution data imputation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data exchange for the metaverse with accountable decentralized TTPs and incentive mechanisms. <em>TBD</em>, <em>11</em>(5), 2431-2442. (<a href='https://doi.org/10.1109/TBDATA.2025.3533924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a global virtual environment, the metaverse poses various challenges regarding data storage, sharing, interoperability, and privacy preservation. Typically, a trusted third party (TTP) is considered necessary in these scenarios. However, relying on a single TTP may introduce biases, compromise privacy, or lead to single-point-of-failure problem. To address these challenges and enable secure data exchange in the metaverse, we propose a system based on decentralized TTPs and the Ethereum blockchain. First, we use the threshold ElGamal cryptosystem to create the decentralized TTPs, employing verifiable secret sharing (VSS) to force owners to share data honestly. Second, we leverage the Ethereum blockchain to serve as the public communication channel, automatic verification machine, and smart contract engine. Third, we apply discrete logarithm equality (DLEQ) algorithms to generate non-interactive zero knowledge (NIZK) proofs when encrypted data is uploaded to the blockchain. Fourth, we present an incentive mechanism to benefit data owners and TTPs from data-sharing activities, as well as a penalty policy if malicious behavior is detected. Consequently, we construct a data exchange framework for the metaverse, in which all involved entities are accountable. Finally, we perform comprehensive experiments to demonstrate the feasibility and analyze the properties of the proposed system.},
  archive  = {J},
  author   = {Liang Zhang and Xingyu Wu and Yuhang Ma and Haibin Kan},
  doi      = {10.1109/TBDATA.2025.3533924},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2431-2442},
  title    = {Data exchange for the metaverse with accountable decentralized TTPs and incentive mechanisms},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the transferability of adversarial examples with random diversity ensemble and variance reduction augmentation. <em>TBD</em>, <em>11</em>(5), 2417-2430. (<a href='https://doi.org/10.1109/TBDATA.2025.3533892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Currently, deep neural networks (DNNs) are susceptible to adversarial attacks, particularly when the network's structure and parameters are known, while most of the existing attacks do not perform satisfactorily in the presence of black-box settings. In this context, model augmentation is considered to be effective to improve the success rates of black-box attacks on adversarial examples. However, the existing model augmentation methods tend to rely on a single transformation, which limits the diversity of augmented model collections and thus affects the transferability of adversarial examples. In this paper, we first propose the random diversity ensemble method (RDE-MI-FGSM) to effectively enhance the diversity of the augmented model collection, thereby improving the transferability of the generated adversarial examples. Afterwards, we put forward the random diversity variance ensemble method (RDE-VRA-MI-FGSM), which adopts variance reduction augmentation (VRA) to improve the gradient variance of the enhanced model set and avoid falling into a poor local optimum, so as to further improve the transferability of adversarial examples. Furthermore, experimental results demonstrate that our approaches are compatible with many existing transfer-based attacks and can effectively improve the transferability of gradient-based adversarial attacks on the ImageNet dataset. Also, our proposals have achieved higher attack success rates even if the target model adopts advanced defenses. Specifically, we have achieved an average attack success rate of 91.4% on the defense model, which is higher than other baseline approaches.},
  archive  = {J},
  author   = {Sensen Zhang and Haibo Hong and Mande Xie},
  doi      = {10.1109/TBDATA.2025.3533892},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2417-2430},
  title    = {Enhancing the transferability of adversarial examples with random diversity ensemble and variance reduction augmentation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multiplex hypergraph attribute-based graph collaborative filtering for cold-start POI recommendation. <em>TBD</em>, <em>11</em>(5), 2401-2416. (<a href='https://doi.org/10.1109/TBDATA.2025.3533908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Within the scope of location-based services and personalized recommendations, the challenges of recommending new and unvisited points of interest (POIs) to mobile users are compounded by the sparsity of check-in data. Traditional recommendation models often overlook user and POI attributes, which exacerbates data sparsity and cold-start problems. To address this issue, a novel multiplex hypergraph attribute-based graph collaborative filtering is proposed for POI recommendation to create a robust recommendation system capable of handling sparse data and cold-start scenarios. Specifically, a multiplex network hypergraph is first constructed to capture complex relationships between users, POIs, and attributes based on the similarities of attributes, visit frequencies, and preferences. Then, an adaptive variational graph auto-encoder adversarial network is developed to accurately infer the users’/POIs’ preference embeddings from their attribute distributions, which reflect complex attribute dependencies and latent structures within the data. Moreover, a dual graph neural network variant based on both Graphsage K-nearest neighbor networks and gated recurrent units are created to effectively capture various attributes of different modalities in a neighborhood, including temporal dependencies in user preferences and spatial attributes of POIs. Finally, experiments conducted on Foursquare and Yelp datasets reveal the superiority and robustness of the developed model compared to some typical state-of-the-art approaches and adequately illustrate the effectiveness of the issues with cold-start users and POIs.},
  archive  = {J},
  author   = {Simon Nandwa Anjiri and Derui Ding and Yan Song and Ying Sun},
  doi      = {10.1109/TBDATA.2025.3533908},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2401-2416},
  title    = {A multiplex hypergraph attribute-based graph collaborative filtering for cold-start POI recommendation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRGTNet: Subregion-guided transformer hash network for fine-grained image retrieval. <em>TBD</em>, <em>11</em>(5), 2388-2400. (<a href='https://doi.org/10.1109/TBDATA.2025.3533916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fine-grained image retrieval (FGIR) is a crucial task in computer vision, with broad applications in areas such as biodiversity monitoring, e-commerce, and medical diagnostics. However, capturing discriminative feature information to generate binary codes is difficult because of high intraclass variance and low interclass variance. To address this challenge, we (i) build a novel and highly reliable fine-grained deep hash learning framework for more accurate retrieval of fine-grained images. (ii) We propose a part significant region erasure method that forces the network to generate compact binary codes. (iii) We introduce a CNN-guided Transformer structure for use in fine-grained retrieval tasks to capture fine-grained images effectively in contextual feature relationships to mine more discriminative regional features. (iv) A multistage mixture loss is designed to optimize network training and enhance feature representation. Experiments were conducted on three publicly available fine-grained datasets. The results show that our method effectively improves the performance of fine-grained image retrieval.},
  archive  = {J},
  author   = {Hongchun Lu and Songlin He and Xue Li and Min Han and Chase Wu},
  doi      = {10.1109/TBDATA.2025.3533916},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2388-2400},
  title    = {SRGTNet: Subregion-guided transformer hash network for fine-grained image retrieval},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly detection in multi-level model space. <em>TBD</em>, <em>11</em>(5), 2376-2387. (<a href='https://doi.org/10.1109/TBDATA.2025.3534625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Anomaly detection (AD) is gaining prominence, especially in situations with limited labeled data or unknown anomalies, demanding an efficient approach with minimal reliance on labeled data or prior knowledge. Building upon the framework of Learning in the Model Space (LMS), this paper proposes conducting AD through Learning in the Multi-Level Model Spaces (MLMS). LMS transforms the data from the data space to the model space by representing each data instance with a fitted model. In MLMS, to fully capture the dynamic characteristics within the data, multi-level details of the original data instance are decomposed. These details are individually fitted, resulting in a set of fitted models that capture the multi-level dynamic characteristics of the original instance. Representing each data instance with a set of fitted models, rather than a single one, transforms it from the data space into the multi-level model spaces. The pairwise difference measurement between model sets is introduced, fully considering the distance between fitted models and the intra-class aggregation of similar models at each level of detail. Subsequently, effective AD can be implemented in the multi-level model spaces, with or without sufficient multi-class labeled data. Experiments on multiple AD datasets demonstrate the effectiveness of the proposed method.},
  archive  = {J},
  author   = {Ao Chen and Xiren Zhou and Yizhan Fan and Huanhuan Chen},
  doi      = {10.1109/TBDATA.2025.3534625},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2376-2387},
  title    = {Anomaly detection in multi-level model space},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCLNet: Generalized contrastive learning for weakly supervised temporal action localization. <em>TBD</em>, <em>11</em>(5), 2365-2375. (<a href='https://doi.org/10.1109/TBDATA.2025.3528727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Weakly supervised temporal action localization (WTAL) aims to precisely locate action instances in given videos by video-level classification supervision, which is partly related to action classification. Most existing localization works directly utilize feature encoders pre-trained for video classification tasks to extract video features, resulting in non-targeted features that lead to incomplete or over-complete action localization. Therefore, we propose Generalized Contrast Learning Network (GCLNet), in which two novel strategies are proposed to improve the pre-trained features. First, to address the issue of over-completeness, GCLNet introduces text information with good context independence and category separability to enrich the expression of video features, as well as proposes a novel generalized contrastive learning approach for similarity metrics, which facilitates pulling closer the features belonging to the same category while pushing farther apart those from different categories. Consequently, it enables more compact intra-class feature learning and ensures accurate action localization. Second, to tackle the problem of incomplete, we exploit the respective advantages of RGB and Flow features in scene appearance and temporal motion expression, designing a hybrid attention strategy in GCLNet to enhance each channel features mutually. This process greatly improves the features through establishing cross-channel consensus. Finally, we conduct extensive experiments on THUMOS14 and ActivityNet1.2, respectively, and the results show that our proposed GCLNet can produce more representative action localization features.},
  archive  = {J},
  author   = {Jing Wang and Dehui Kong and Baocai Yin},
  doi      = {10.1109/TBDATA.2025.3528727},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2365-2375},
  title    = {GCLNet: Generalized contrastive learning for weakly supervised temporal action localization},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emulating reader behaviors for fake news detection. <em>TBD</em>, <em>11</em>(5), 2353-2364. (<a href='https://doi.org/10.1109/TBDATA.2025.3527230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The wide dissemination of fake news has affected our lives in many aspects, making fake news detection important and attracting increasing attention. Existing approaches make substantial contributions in this field by modeling news from a single-modal or multi-modal perspective. However, these modal-based methods can result in sub-optimal outcomes as they ignore reader behaviors in news consumption and authenticity verification. For instance, they haven't taken into consideration the component-by-component reading process: from the headline, images, comments, to the body, which is essential for modeling news with more granularity. To this end, we propose an approach of Emulating the behaviors of readers (Ember) for fake news detection on social media, incorporating readers’ reading and verificating process to model news from the component perspective thoroughly. Specifically, we first construct intra-component feature extractors to emulate the behaviors of semantic analyzing on each component. Then, we design a module that comprises inter-component feature extractors and a sequence-based aggregator. This module mimics the process of verifying the correlation between components and the overall reading and verification sequence. Thus, Ember can handle the news with various components by emulating corresponding sequences. We conduct extensive experiments on nine real-world datasets, and the results demonstrate the superiority of Ember.},
  archive  = {J},
  author   = {Junwei Yin and Min Gao and Kai Shu and Zehua Zhao and Yinqiu Huang and Jia Wang},
  doi      = {10.1109/TBDATA.2025.3527230},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2353-2364},
  title    = {Emulating reader behaviors for fake news detection},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized time series classification via component decomposition and alignment. <em>TBD</em>, <em>11</em>(5), 2338-2352. (<a href='https://doi.org/10.1109/TBDATA.2025.3527215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The objective of domain generalization is to develop a model that can handle the domain shift problem without access to the target domain. In this paper, we propose a new domain generalization approach called Decomposition Framework with Dynamic Component Alignment (DFDCA), which employs signal decomposition on input data and conducts domain alignment on each component, providing another perspective on domain generalization for time series classification. Specifically, we first utilize a neural decomposition module to decompose the original time series data into several components, and design loss functions to guide the network to effectively perform signal decomposition for class-wise domain alignment on the decomposed components. The denoising attention mechanism is then introduced to enhance informative components while suppressing task-irrelevant components. Our proposed approach is evaluated on four publicly available datasets based on the cross-domain setting where the training and test samples are drawn from different distributions. The results demonstrate that it outperforms other baseline methods, achieving state-of-the-art performance.},
  archive  = {J},
  author   = {Yichuan Cheng and Darrick Lee and Harald Oberhauser and Haoliang Li},
  doi      = {10.1109/TBDATA.2025.3527215},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2338-2352},
  title    = {Generalized time series classification via component decomposition and alignment},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view heterogeneous HyperGNN for heterophilic knowledge combination prediction. <em>TBD</em>, <em>11</em>(5), 2321-2337. (<a href='https://doi.org/10.1109/TBDATA.2025.3527216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Knowledge combination prediction involves analyzing current knowledge elements and their relationships, then forecasting how these elements, drawn from various fields, can be creatively combined to form new, innovative solutions. This process is critical for countries and businesses to understand future technology trends and promote innovation in an era of rapid scientific and technological advancement. Existing methods often overlook the integration of knowledge combinations from multiple views, along with their inherent heterophily and the dual “many-to-one” property, where a single knowledge combination can include multiple elements, and a single element may belong to various combinations. To this end, we propose a novel framework named Multi-view Heterogeneous HyperGNN for Heterophilic Knowledge Combination Prediction (H3KCP). Specifically, H3KCP first constructs a hypergraph reflecting the dual “many-to-one” property of knowledge combinations, where each hyperedge may contain several nodes and each node can also belong to multiple hyperedges. Next, the framework employs a multi-view fusion approach to model knowledge combinations, considering heterophily and integrating insights from co-occurrence, co-citation, and hierarchical structure-based views. Furthermore, our analysis of H3KCP from a spectral graph perspective offers insights into its rationality. Finally, extensive experiments on real-world patent datasets and the Open Academic Graph dataset validate the effectiveness and efficiency of our approach, yielding significant insights into knowledge combinations.},
  archive  = {J},
  author   = {Huijie Liu and Shulan Ruan and Han Wu and Zhenya Huang and Defu Lian and Qi Liu and Enhong Chen},
  doi      = {10.1109/TBDATA.2025.3527216},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2321-2337},
  title    = {Multi-view heterogeneous HyperGNN for heterophilic knowledge combination prediction},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Relational clustering-based parallel spaces construction and embedding for dynamic knowledge graph. <em>TBD</em>, <em>11</em>(5), 2308-2320. (<a href='https://doi.org/10.1109/TBDATA.2025.3527238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the increasing amount of data in various domains, knowledge graphs (KGs) have become powerful tools for representing complex and heterogeneous information in a structured way, and for extracting valuable information from knowledge graphs through embedding techniques to support downstream tasks such as recommendation and Q&A systems. Knowledge graphs consist of triples that are continuously added as knowledge is updated. However, most existing embedding models are designed for static graphs, requiring the entire model to be retrained for each update, which is time-consuming. Existing global dynamic embedding models focus on exploiting the structural and relational information of the whole graph to achieve embedding quality, resulting in reduced dynamic efficiency. To address this problem, we propose a relational clustering-based parallel space model in which knowledge from different domains is embedded in different subspaces, allowing each subspace to focus on the data characteristics of a specific domain, thereby improving the quality of knowledge. Second, the new data only affects some subspaces but not the performance of other spaces, improving the model's adaptability to dynamics. Furthermore, we employ two incremental approaches based on the type of added data to improve the efficiency of dynamic embedding while ensuring that the added data preserves the characteristics of the parallel space. The experimental results show that the dynamic embedding efficiency of our model is improved by an average of 50.3% compared to the SOTA dynamic embedding model for the link prediction task. Particularly on FB15K, our model not only improves the efficiency by 41% but also increases the accuracy by 7.5%, demonstrating the accuracy and efficiency of our model.},
  archive  = {J},
  author   = {Yao Liu and Yongfei Zhang},
  doi      = {10.1109/TBDATA.2025.3527238},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2308-2320},
  title    = {Relational clustering-based parallel spaces construction and embedding for dynamic knowledge graph},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Portraying fine-grained tenant portrait for churn prediction using semi-supervised graph convolution and attention network. <em>TBD</em>, <em>11</em>(5), 2296-2307. (<a href='https://doi.org/10.1109/TBDATA.2025.3527200'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the widespread application of Big Data and intelligent information systems, the tenant has become the main form of most scenarios. As a data mining technique, the portrait has been widely used to provide targeted services. Therefore, we transfer the traditional user-driven portrait into tenant driven for churn prediction. To achieve it, this paper first proposes a three-layer architecture and defines the fine-grained features for creating portraits from the perspective of tenants. In a large-scale telecommunication industry dataset of 100,000 tenants, we construct the tenant portrait through the proposed framework, and analyze the influences of the defined features on churn possibility. Then, considering the information missing caused by privacy concerns, we come up with the CrossMatch, a portrait completion model based on semi-supervised and graph convolution, which combines the relation characteristics among tenants for recovering missing information. On this basis, we design the tenant churn prediction method based on a directed attention network. Moreover, we recover missing information on three public node datasets with CrossMatch, achieving around 1-2$\%$ improvement. We then apply the directed attention network for churn prediction and achieve an Accuracy of 75.06$\%$, Precision of 77.78$\%$, and F1-score of 71.43$\%$, which outperforms all the baselines.},
  archive  = {J},
  author   = {Zuodong Jin and Peng Qi and Muyan Yao and Dan Tao},
  doi      = {10.1109/TBDATA.2025.3527200},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2296-2307},
  title    = {Portraying fine-grained tenant portrait for churn prediction using semi-supervised graph convolution and attention network},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data-centric $\ell$-diversity model for securely publishing personal data with enhanced utility. <em>TBD</em>, <em>11</em>(5), 2278-2295. (<a href='https://doi.org/10.1109/TBDATA.2024.3524832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose and implement a novel anonymization model, called data-centric $\ell$-diversity, to effectively safeguard the privacy of individuals with considerably enhanced utility in data publishing scenarios. Through experimental analysis of real-life datasets, we found that when the data quality is poor (e.g., distributions are uneven), most of the existing methods only anonymize some parts of the data (where distributions are balanced) and leave other parts unprocessed, which can lead to explicit privacy disclosures. Furthermore, they do not identify and repair problematic parts of the data before anonymization, and therefore, they are not secure from the threat of privacy breaches. To address these technical problems, in this paper, we implement an automated method that identifies vulnerabilities in the underlying data to be anonymized w.r.t. distribution, and that repairs them by injecting virtual samples of good quality. Later, we implement a data partitioning strategy that creates compact and diverse classes of size $k$, where $k$ is the privacy parameter. Finally, only shallow generalization (or no generalization) is applied to each class to minimally generalize the data, whereas existing methods overly distort data by not improving the quality beforehand, which can lead to poor utility in data-driven services. We conducted detailed experiments on four datasets to justify the performance of our model in realistic scenarios, and achieved promising results from the perspectives of boosted accuracy, privacy preservation, data utility enrichment, and reduced computing overheads. Compared with baseline methods, our model enhanced privacy preservation by 36.56% on three different metrics, and data utility was augmented with 18.65% less information loss and 14.37% greater accuracy. Lastly, our model, on average, has shown a 26.13% reduction in time overheads compared to the SOTA baseline methods.},
  archive  = {J},
  author   = {Abdul Majeed and Seong Oun Hwang},
  doi      = {10.1109/TBDATA.2024.3524832},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2278-2295},
  title    = {A data-centric $\ell$-diversity model for securely publishing personal data with enhanced utility},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FinLLMs: A framework for financial reasoning dataset generation with large language models. <em>TBD</em>, <em>11</em>(5), 2264-2277. (<a href='https://doi.org/10.1109/TBDATA.2024.3524083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Large Language models (LLMs) usually rely on extensive training datasets. In the financial domain, creating numerical reasoning datasets that include a mix of tables and long text often involves substantial manual annotation expenses. To address the limited data resources and reduce the annotation cost, we introduce FinLLMs, a method for generating financial question-answering (QA) data based on common financial formulas using LLMs. First, we compile a list of common financial formulas and construct a graph based on the variables these formulas employ. We then augment the formula set by combining those that share identical variables as new elements. Specifically, we explore formulas obtained by manual annotation and merge those formulas with shared variables by traversing the constructed graph. Finally, utilizing LLMs, we generate financial QA data that encompasses both tabular information and long textual content, building on the collected formula set. Our experiments demonstrate that the synthetic data generated by FinLLMs effectively enhances the performance of various numerical reasoning models in the financial domain, including both pre-trained language models (PLMs) and fine-tuned LLMs. This performance surpasses that of two established benchmark financial QA datasets.},
  archive  = {J},
  author   = {Ziqiang Yuan and Kaiyuan Wang and Shoutai Zhu and Ye Yuan and Jingya Zhou and Yanlin Zhu and Wenqi Wei},
  doi      = {10.1109/TBDATA.2024.3524083},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2264-2277},
  title    = {FinLLMs: A framework for financial reasoning dataset generation with large language models},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-empowered federated learning: Benefits, challenges, and solutions. <em>TBD</em>, <em>11</em>(5), 2244-2263. (<a href='https://doi.org/10.1109/TBDATA.2025.3541560'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated learning (FL) is a distributed machine learning approach that protects user data privacy by training models locally on clients and aggregating them on a parameter server. While effective at preserving privacy, FL systems face limitations such as single points of failure, lack of incentives, and inadequate security. To address these challenges, blockchain technology is integrated into FL systems to provide stronger security, fairness, and scalability. However, blockchain-empowered FL (BC-FL) systems introduce additional demands on network, computing, and storage resources. This survey provides a comprehensive review of recent research on BC-FL systems, analyzing the benefits and challenges associated with blockchain integration. We explore why blockchain is applicable to FL, how it can be implemented, and the challenges and existing solutions for its integration. Additionally, we offer insights on future research directions for the BC-FL system.},
  archive  = {J},
  author   = {Zeju Cai and Jianguo Chen and Yuting Fan and Zibin Zheng and Keqin Li},
  doi      = {10.1109/TBDATA.2025.3541560},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2244-2263},
  title    = {Blockchain-empowered federated learning: Benefits, challenges, and solutions},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digital twin data management: A comprehensive review. <em>TBD</em>, <em>11</em>(5), 2224-2243. (<a href='https://doi.org/10.1109/TBDATA.2025.3533891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Digital Twins are virtual representations of physical assets and systems that rely on effective Data Management to integrate, process, and analyze diverse data sources. This article comprehensively examines Data Management challenges, architectures, techniques, and applications in the context of Digital Twins. It explores key issues such as data heterogeneity, quality assurance, scalability, security, and interoperability. The paper outlines architectural approaches like centralized, distributed, cloud-based, and blockchain solutions and Data Management techniques for modeling, integration, fusion, quality management, and visualization. Domain-specific considerations across manufacturing, smart cities, healthcare, and other sectors are discussed. Finally, open research challenges related to standards, real-time data processing, intelligent Data Management, and ethical aspects are highlighted. By synthesizing the state-of-the-art, this review serves as a valuable reference for developing robust Data Management strategies that enable Digital Twin deployments.},
  archive  = {J},
  author   = {Ezekiel B. Ouedraogo and Ammar Hawbani and Xingfu Wang and Zhi Liu and Liang Zhao and Mohammed A. A. Al-qaness and Saeed Hamood Alsamhi},
  doi      = {10.1109/TBDATA.2025.3533891},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2224-2243},
  title    = {Digital twin data management: A comprehensive review},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid self-supervised learning framework for vertical federated learning. <em>TBD</em>, <em>11</em>(5), 2210-2223. (<a href='https://doi.org/10.1109/TBDATA.2024.3403386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Vertical federated learning (VFL), a variant of Federated Learning (FL), has recently drawn increasing attention as the VFL matches the enterprises’ demands of leveraging more valuable features to achieve better model performance. However, conventional VFL methods may run into data deficiency as they exploit only aligned and labeled samples (belonging to different parties), leaving often the majority of unaligned and unlabeled samples unused. The data deficiency hampers the effort of the federation. In this work, we propose a Federated Hybrid Self-Supervised Learning framework, named FedHSSL, that utilizes cross-party views (i.e., dispersed features) of samples aligned among parties and local views (i.e., data augmentation) of unaligned samples within each party to improve the representation learning capability of the VFL joint model. FedHSSL further exploits invariant features across parties to boost the performance of the joint model through partial model aggregation. FedHSSL, as a framework, can work with various representative SSL methods. We empirically demonstrate that FedHSSL methods outperform baselines by large margins. We provide an in-depth analysis of FedHSSL regarding label leakage, which is rarely investigated in existing self-supervised VFL works. The experimental results show that, with proper protection, FedHSSL achieves the best privacy-utility trade-off against the state-of-the-art label inference attack compared with baselines.},
  archive  = {J},
  author   = {Yuanqin He and Yan Kang and Xinyuan Zhao and Jiahuan Luo and Lixin Fan and Yuxing Han and Qiang Yang},
  doi      = {10.1109/TBDATA.2024.3403386},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2210-2223},
  title    = {A hybrid self-supervised learning framework for vertical federated learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards fair and scalable trial assignment in federated bandits: A shapley value approach. <em>TBD</em>, <em>11</em>(5), 2195-2209. (<a href='https://doi.org/10.1109/TBDATA.2024.3403369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated multi-armed bandits extend the multi-armed bandits framework to the federated learning setting where multiple clients in the same exploration space collaboratively identify the optimal arm. While previous studies demonstrated its efficiency like classical federated learning, in this paper, we present the first work that reveals the serious fairness problem in federated multi-armed bandits when clients have heterogeneous and overlapping armsets. The fairness problem happens because clients with different trial requirements should conduct different numbers of trials summing up to a global requirement, but they wish to minimize their exploration effort. To address the novel fairness concern, we formally formulate the fairness-aware trial assignment as a coalitional game. Based on the theoretically derived trial requirements, we devise a Shapley value-based trial assignment mechanism to guarantee fairness. Regardless of the #P-hard complexity when deriving the general Shapley value, we achieve an accurate computation of trial assignment with polynomial complexity by exploiting its unique characteristic. We further carefully control the numbers of trials in each iteration to resolve the communication bottleneck and minimize the wasted trials. Experiment results show that, compared to the naïve federated scheme, our design outperforms with both high fairness metrics and high efficiency in total trials and communication.},
  archive  = {J},
  author   = {Zibo Wang and Yifei Zhu and Dan Wang and Zhu Han},
  doi      = {10.1109/TBDATA.2024.3403369},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2195-2209},
  title    = {Towards fair and scalable trial assignment in federated bandits: A shapley value approach},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized federated learning with contrastive momentum. <em>TBD</em>, <em>11</em>(5), 2184-2194. (<a href='https://doi.org/10.1109/TBDATA.2024.3403387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In this paper, we propose pFedMo, a personalized federated learning algorithm with contrastive momentum. In pFedMo, we design a score function to personalize worker models by distilling knowledge from the aggregator's representation model so as to address the non-i.i.d. issue. To accelerate the convergence, we leverage the momentum acceleration on both the worker side and the aggregator side. However, the typical momentum without personalization does not suit well for the worker models with personalization, influencing convergence performance. To address this, we develop a personalized/contrastive momentum method for efficient momentum acceleration. We provide mathematical proof for the convergence of pFedMo on non-i.i.d. data. Extensive experiments based on real-world datasets and IoT system are conducted, verifying that pFedMo outperforms existing mainstream benchmarks, and achieves up to 35.90% accuracy increase and 3.64x training time speedup under a wide range of settings.},
  archive  = {J},
  author   = {Sen Fu and Zhengjie Yang and Chuang Hu and Wei Bao},
  doi      = {10.1109/TBDATA.2024.3403387},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2184-2194},
  title    = {Personalized federated learning with contrastive momentum},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous device collaboration based federated learning for big data applications. <em>TBD</em>, <em>11</em>(5), 2174-2183. (<a href='https://doi.org/10.1109/TBDATA.2024.3404104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the era of Big Data, artificial intelligence and information science are the key technologies to extract the value of data and enhance the competitiveness of enterprises. The characteristics of distributed, small-scale, and sparse lead to the isolated data island problem. To solve these problems, Federated Learning is proposed. However, a large number of terminal models need to be uploaded to the server in Federated Learning, especially for the actual scenario of Internet of Things. Therefore, huge communication costs are required which dramatically increases the pressure on the backbone network. Furthermore, the low quality of the local model will lead to decreased accuracy and convergence rates of the model. To overcome the above limitations, we propose heterogeneous device collaboration based federated learning (HDCFL), which constructs a three-layer structure for Federated Learning by leveraging edge computing and designs a heterogeneous device collaboration method that groups the terminals based on their computing power, communication time, and data volume to train the model. Then, we conduct a theoretical analysis of the proposed algorithm which verifies its advantage. At last, the experimental result demonstrates that the proposed algorithm consistently achieves superior performance in terms of both convergence speed and accuracy compared with state-of-the-art baselines.},
  archive  = {J},
  author   = {Wenhua Wang and Quan Yang and Yuzhu Liang and Yang Xu and Qin Liu and Tian Wang},
  doi      = {10.1109/TBDATA.2024.3404104},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2174-2183},
  title    = {Heterogeneous device collaboration based federated learning for big data applications},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harnessing the power of local supervision in federated learning. <em>TBD</em>, <em>11</em>(5), 2162-2173. (<a href='https://doi.org/10.1109/TBDATA.2024.3403383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated learning is widely accepted as a privacy-preserving paradigm for training a shared global model across multiple client devices in a collaborative fashion. However, in practice, the significantly limited computational power on client devices has been a major barrier when we wish to train large models with potentially hundreds of millions of parameters. In this paper, we propose a new architecture, referred to as Infocomm, that incorporates locally supervised learning in federated learning. With locally supervised learning, the disadvantages of split learning can be avoided by using a more flexible way to offload training from resource constrained clients to a more capable server. Infocomm enables parallel training of different modules of the neural network in both the server and clients in a gradient-isolated fashion. The efficacy in reducing both training time and communication time is supported by our theoretical analysis and empirical results. In the scenario involving larger models and fewer available local data, Infocomm has been observed to reduce the elapsed time per round by over 37% without sacrificing accuracy compared to both conventional federated learning or directly combining federated learning and split learning, which showcases the advantages of Infocomm under power-constrained IoT scenarios.},
  archive  = {J},
  author   = {Fei Wang and Baochun Li},
  doi      = {10.1109/TBDATA.2024.3403383},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2162-2173},
  title    = {Harnessing the power of local supervision in federated learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating blockchain-enabled federated learning with clustered clients. <em>TBD</em>, <em>11</em>(5), 2148-2161. (<a href='https://doi.org/10.1109/TBDATA.2024.3403390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the rapid development of Big Data, Federated learning (FL) has found numerous applications, enabling machine learning (ML) on edge devices while preserving privacy. However, FL still faces crucial challenges, such as single point of failure and poisoning attacks, which motivate the integration of blockchain-enabled FL (BeFL). Beyond that, the efficiency issue still limits the further application of BeFL. To address these issues, we propose a novel decentralized framework: Accelerating Blockchain-Enabled Federated Learning with Clustered Clients (ABFLCC), who utilize actual training time for clustering clients to achieve hierarchical FL and solve the single point of failure problem through blockchain. Additionally, the framework clusters edge devices considering their actual training times, which allows for synchronous FL within clusters and asynchronous FL across clusters simultaneously. This approach guarantees that devices with a similar training time have a consistent global model version, improving the stability of the converging process, while the asynchronous learning between clusters enhances the efficiency of convergence. The proposed framework is evaluated through simulations on three real-world public datasets, demonstrating a training efficiency improvement of 30% to 70% in terms of convergence time compared to existing BeFL systems.},
  archive  = {J},
  author   = {Laizhong Cui and Yinghao Li and Yipeng Zhou and Youyang Qu and Jiangchuan Liu},
  doi      = {10.1109/TBDATA.2024.3403390},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2148-2161},
  title    = {Accelerating blockchain-enabled federated learning with clustered clients},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MemDefense: Defending against membership inference attacks in IoT-based federated learning via pruning perturbations. <em>TBD</em>, <em>11</em>(5), 2135-2147. (<a href='https://doi.org/10.1109/TBDATA.2024.3403388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depending on large-scale devices, the Internet of Things (IoT) provides massive data support for resource sharing and intelligent decision, but privacy risks also increase. As a popular distributed learning framework, Federated Learning (FL) is widely used because it does not need to share raw data while only parameters to collaboratively train models. However, Federated Learning is not spared by some emerging attacks, e.g., membership inference attack. Therefore, for IoT devices with limited resources, it is challenging to design a defense scheme against the membership inference attack ensuring high model utility, strong membership privacy and acceptable time efficiency. In this article, we propose MemDefense, a lightweight defense mechanism to prevent membership inference attack from local models and global models in IoT-based FL, while maintaining high model utility. MemDefense adds crafted pruning perturbations to local models at each round of FL by deploying two key components, i.e., parameter filter and noise generator. Specifically, the parameter filter selects the apposite model parameters which have little impact on the model test accuracy and contribute more to member inference attacks. Then, the noise generator is used to find the pruning noise that can reduce the attack accuracy while keeping high model accuracy, protecting each participant's membership privacy. We comprehensively evaluate MemDefense with different deep learning models and multiple benchmark datasets. The experimental results show that low-cost MemDefense drastically reduces the attack accuracy within limited drop of classification accuracy, meeting the requirements for model utility, membership privacy and time efficiency.},
  archive  = {J},
  author   = {Meng Shen and Jin Meng and Ke Xu and Shui Yu and Liehuang Zhu},
  doi      = {10.1109/TBDATA.2024.3403388},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2135-2147},
  title    = {MemDefense: Defending against membership inference attacks in IoT-based federated learning via pruning perturbations},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cepe-FL: Communication-efficient and privacy-enhanced federated learning via adaptive compressive sensing. <em>TBD</em>, <em>11</em>(5), 2119-2134. (<a href='https://doi.org/10.1109/TBDATA.2024.3403393'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Model updates are exchanged between server(s) and participants in Federated Learning (FL), which can result in excessive delay, especially for large models. Existing communication-efficient FL approaches such as quantization and top-k sampling apply compression to gradients assuming that gradients are sparse and can tolerate small deviations. This can hardly be applied to down-link transmission. In this work, we employ compressive sensing on model parameters instead of gradients and propose a two-way adaptive compression scheme, Cepe-FL, which exploits dictionary learning to project non-sparse model parameters into sparse representations to ensure reconstruction accuracy. Cepe-FL supports joint model reconstruction with drastic reduction in computational complexity from $O(n)$ to $O(1)$. Cepe-FL adjusts the compression ratio adaptively according to the training loss, achieving the best trade-off between communication and model precision. Furthermore, it demonstrates efficacy in defending against membership inference attacks since only compressed models are exchanged. We conduct extensive experiments on three image classification tasks and compare with three communication-efficient approaches including FedPAQ, FedAvg and T-FedAvg. Cepe-FL presents the best performance in all tasks under IID and non-IID scenarios. We also implement white-box membership inference attacks, and the results show Cepe-FL can significantly suppress success ratio of inference in comparison with other approaches.},
  archive  = {J},
  author   = {Ye Liu and Shan Chang and Yiqi Liu},
  doi      = {10.1109/TBDATA.2024.3403393},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2119-2134},
  title    = {Cepe-FL: Communication-efficient and privacy-enhanced federated learning via adaptive compressive sensing},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRACTAL: Data-aware clustering and communication optimization for decentralized federated learning. <em>TBD</em>, <em>11</em>(5), 2102-2118. (<a href='https://doi.org/10.1109/TBDATA.2024.3403381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Decentralized federated learning (DFL) is a promising technique to enable distributed machine learning over edge nodes without relying on a centralized parameter server. However, existing DFL network topologies, such as fully connected, partially connected, or lower-tier hierarchical topology often struggle to effectively address the unique challenges presented by edge networks, including edge heterogeneity, communication resource constraint, and data Non-IID. In order to tackle these challenges, we propose a data-aware clustering algorithm, called FRACTAL, to construct a multi-tier hierarchical topology in a bottom-up manner taking into consideration both data distribution and communication efficiency for DFL. We theoretically explore the quantitative relationship between the convergence bound of multi-tier FL and the data distribution among each-tier servers. To further improve communication efficiency and address edge heterogeneity, we deploy a time-sharing communication scheduling algorithm within each fractal unit (the basic structure in FRACTAL consisting of multiple nodes and an aggregator), called magic mirror method (MMM), to determine the optimal order of model distributing and uploading for nodes. We conduct extensive experiments on the classical models and datasets to evaluate the performance of FRACTAL, and the results show that FRACTAL can significantly accelerate the DFL model training by 48.6%–72.3% compared with the state-of-the-art solutions.},
  archive  = {J},
  author   = {Qianpiao Ma and Jianchun Liu and Hongli Xu and Qingmin Jia and Renchao Xie},
  doi      = {10.1109/TBDATA.2024.3403381},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  number   = {5},
  pages    = {2102-2118},
  title    = {FRACTAL: Data-aware clustering and communication optimization for decentralized federated learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial special issue on federated learning for big data applications. <em>TBD</em>, <em>11</em>(5), 2099-2101. (<a href='https://doi.org/10.1109/TBDATA.2024.3417057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Xiaowen Chu and Wei Wang and Cong Wang and Yang Liu and Rongfei Zeng and Christopher G. Brinton},
  doi     = {10.1109/TBDATA.2024.3417057},
  journal = {IEEE Transactions on Big Data},
  month   = {10},
  number  = {5},
  pages   = {2099-2101},
  title   = {Guest editorial special issue on federated learning for big data applications},
  volume  = {11},
  year    = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NAGphormer+: A tokenized graph transformer with neighborhood augmentation for node classification in large graphs. <em>TBD</em>, <em>11</em>(4), 2085-2098. (<a href='https://doi.org/10.1109/TBDATA.2024.3524081'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph Transformers, emerging as a new architecture for graph representation learning, suffer from the quadratic complexity and can only handle graphs with at most thousands of nodes. To this end, we propose a Neighborhood Aggregation Graph Transformer (NAGphormer) that treats each node as a sequence containing a series of tokens constructed by our proposed Hop2Token module. For each node, Hop2Token aggregates the neighborhood features from different hops into different representations, producing a sequence of token vectors as one input. In this way, NAGphormer could be trained in a mini-batch manner and thus could scale to large graphs with millions of nodes. To further enhance the model's generalization, we propose NAGphormer+, an extended model of NAGphormer with a novel data augmentation method called Neighborhood Augmentation (NrAug). Based on the output of Hop2Token, NrAug simultaneously augments the features of neighborhoods from global as well as local views. In this way, NAGphormer+ can fully utilize the neighborhood information of multiple nodes, thereby undergoing more comprehensive training and improving the model's generalization capability. Extensive experiments on benchmark datasets from small to large demonstrate the superiority of NAGphormer+ against existing graph Transformers and mainstream GNNs, as well as the original NAGphormer.},
  archive  = {J},
  author   = {Jinsong Chen and Chang Liu and Kaiyuan Gao and Gaichao Li and Kun He},
  doi      = {10.1109/TBDATA.2024.3524081},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {2085-2098},
  title    = {NAGphormer+: A tokenized graph transformer with neighborhood augmentation for node classification in large graphs},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated multi-view multi-label classification. <em>TBD</em>, <em>11</em>(4), 2072-2084. (<a href='https://doi.org/10.1109/TBDATA.2024.3522812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-view multi-label classification is a crucial machine learning paradigm aimed at building robust multi-label predictors by integrating heterogeneous features from various sources while addressing multiple correlated labels. However, in real-world applications, concerns over data confidentiality and security often prevent data exchange or fusion across different sources, leading to the challenging issue of data islands. To tackle this problem, we propose a general federated multi-view multi-label classification method, FMVML, which integrates a novel multi-view multi-label classification technique into a federated learning framework. This approach enables cross-view feature fusion and multi-label semantic classification while preserving the data privacy of each independent source. Within this federated framework, we first extract view-specific information from each individual client to capture unique characteristics and then consolidate consensus information from different views on the global server to represent shared features. Unlike previous methods, our approach enhances cross-view fusion and semantic expression by jointly capturing both feature and semantic aspects of specificity and commonality. The final label predictions are generated by combining the view-specific predictions from individual clients and the consensus predictions from the global server. Extensive experiments across various applications demonstrate that FMVML fully leverages multi-view data in a privacy-preserving manner and consistently outperforms state-of-the-art methods.},
  archive  = {J},
  author   = {Hongdao Meng and Yongjian Deng and Qiyu Zhong and Yipeng Wang and Zhen Yang and Gengyu Lyu},
  doi      = {10.1109/TBDATA.2024.3522812},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {2072-2084},
  title    = {Federated multi-view multi-label classification},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tailored definitions with easy reach: Complexity-controllable definition generation. <em>TBD</em>, <em>11</em>(4), 2061-2071. (<a href='https://doi.org/10.1109/TBDATA.2024.3522805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The task of complexity-controllable definition generation refers to providing definitions with different readability for words in specific contexts. This task can be utilized to help language learners eliminate reading barriers and facilitate language acquisition. However, the available training data for this task remains scarce due to the difficulty of obtaining reliable definition data and the high cost of data standardization. To tackle those challenges, we introduce a general solution from both the data-driven and method-driven perspectives. We construct a large-scale standard Chinese dataset, COMPILING, which contains both difficult and simple definitions and can serve as a benchmark for future research. Besides, we propose a multitasking framework SimpDefiner for unsupervised controllable definition generation. By designing a parameter-sharing scheme between two decoders, the framework can extract the complexity information from the non-parallel corpus. Moreover, we propose the SimpDefiner guided prompting (SGP) method, where simple definitions generated by SimpDefiner are utilized to construct prompts for GPT-4, hence obtaining more realistic and contextually appropriate definitions. The results demonstrate SimpDefiner's outstanding ability to achieve controllable generation and better results could be achieved when GPT-4 is incorporated.},
  archive  = {J},
  author   = {Liner Yang and Jiaxin Yuan and Cunliang Kong and Jingsi Yu and Ruining Chong and Zhenghao Liu and Erhong Yang},
  doi      = {10.1109/TBDATA.2024.3522805},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {2061-2071},
  title    = {Tailored definitions with easy reach: Complexity-controllable definition generation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking large language model power in industry: Privacy-preserving collaborative creation of knowledge graph. <em>TBD</em>, <em>11</em>(4), 2046-2060. (<a href='https://doi.org/10.1109/TBDATA.2024.3522814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Semantic expertise remains a reliable foundation for industrial decision-making, while Large Language Models (LLMs) can augment the often limited empirical knowledge by generating domain-specific insights, though the quality of this generative knowledge is uncertain. Integrating LLMs with the collective wisdom of multiple stakeholders could enhance the quality and scale of knowledge, yet this integration might inadvertently raise privacy concerns for stakeholders. In response to this challenge, Federated Learning (FL) is harnessed to improve the knowledge base quality by cryptically leveraging other stakeholders’ knowledge, where knowledge base is represented in Knowledge Graph (KG) form. Initially, a multi-field hyperbolic (MFH) graph embedding method vectorizes entities, furnishing mathematical representations in lieu of solely semantic meanings. The FL framework subsequently encrypted identifies and fuses common entities, whereby the updated entities’ embedding can refine other private entities’ embedding locally, thus enhancing the overall KG quality. Finally, the KG complement method refines and clarifies triplets to improve the overall quality of the KG. An experiment assesses the proposed approach across different industrial KGs, confirming its effectiveness as a viable solution for collaborative KG creation, all while maintaining data security.},
  archive  = {J},
  author   = {Liqiao Xia and Junming Fan and Ajith Parlikad and Xiao Huang and Pai Zheng},
  doi      = {10.1109/TBDATA.2024.3522814},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {2046-2060},
  title    = {Unlocking large language model power in industry: Privacy-preserving collaborative creation of knowledge graph},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MC-GNN: Multi-channel graph neural networks with hilbert-schmidt independence criterion. <em>TBD</em>, <em>11</em>(4), 2036-2045. (<a href='https://doi.org/10.1109/TBDATA.2024.3522817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph Neural Networks (GNNs) have been proven to be useful for learning graph-based knowledge. However, one of the drawbacks of GNN techniques is that they may get stuck in the problem of over-squashing. Recent studies attribute to the message passing paradigm that it may amplify some specific local relations and distort long-range information under a certain GNN. To alleviate such phenomena, we propose a novel and general GNN framework, dubbed MC-GNN, which introduces the multi-channel neural architecture to learn and fuse multi-view graph-based information. The purpose of MC-GNN is to extract distinct channel-based graph features and adaptively adjust the importance of the features. To this end, we use the Hilbert-Schmidt Independence Criterion (HSIC) to enlarge the disparity between the embeddings encoded by each channel and follow an attention mechanism to fuse the embeddings with adaptive weight adjustment. MC-GNN can apply multiple GNN backbones, which provides a solution for learning structural relations from a multi-view perspective. Experimental results demonstrate that the proposed MC-GNN is superior to the compared state-of-the-art GNN methods.},
  archive  = {J},
  author   = {Shicheng Cui and Deqiang Li and Jing Zhang},
  doi      = {10.1109/TBDATA.2024.3522817},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {2036-2045},
  title    = {MC-GNN: Multi-channel graph neural networks with hilbert-schmidt independence criterion},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online non-stationary pricing incentives for budget-limited crowdsensing. <em>TBD</em>, <em>11</em>(4), 2025-2035. (<a href='https://doi.org/10.1109/TBDATA.2024.3522804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The promising applications of mobile crowdsensing (MCS) have attracted much research interest recently, especially for the posted-pricing scenes. However, existing works mainly focus on the stationary MCS, no matter whether in a stochastic or adversarial environment, where each price (or arm) remains identical over time. However, in many realistic MCS applications such as environment monitoring and recommendation systems, stationary bandits do not model the posted-pricing sequential decision problems where the reward distributions of each price (arm) and cost distribution vary over time due to the changes in light intensity and mobile devices’ remnant energy. While in this paper, we study a more general submodular crowdsensing scene to address the non-stationary sequential pricing problems, and construct a monotonic submodular function merging the marginal reward and temporal difference errors (TD-errors) of deep reinforcement learning (DRL). Moreover, we explore a weighted budget-limited non-stationary pricing mechanism by using the deep deterministic policy gradient (DDPG) method for submodular MCS from the perspectives of the hard-drop and soft-drop weights. Our mechanism can readily be extended to non-submodular MCS or other MCS scenes. Extensive simulations demonstrate that our mechanism outweighs existing benchmarks.},
  archive  = {J},
  author   = {Jiajun Sun and Dianliang Wu},
  doi      = {10.1109/TBDATA.2024.3522804},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {2025-2035},
  title    = {Online non-stationary pricing incentives for budget-limited crowdsensing},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-aware triangle counting over geo-distributed datacenters. <em>TBD</em>, <em>11</em>(4), 2008-2024. (<a href='https://doi.org/10.1109/TBDATA.2024.3522816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Counting triangles is an important topic in many practical applications, such as anomaly detection, community search, and recommendation systems. For triangle counting in large and dynamic graphs, recent work has focused on distributed streaming algorithms. These works assume that the graph is processed in the same location, while in reality, the graph stream may be generated and processed in datacenters that are geographically distributed. This raises new challenges to existing triangle counting algorithms, due to the multi-level heterogeneities in network bandwidth and communication prices in geo-distributed datacenters. In this article, we propose a cost-aware framework named ${\sf GeoTri}$ based on the Master-Worker-Aggregator architecture, which takes both the cost and performance objectives into consideration for triangle counting in geo-distributed datacenters. The two core parts of this framework are the cost-aware nodes assignment strategy in master, which is critical to obtain node's position and distribute edges reasonably to reduce the cost (i.e., time cost and monetary cost), and cost-aware neighbor transfer strategy among workers, which further eliminates redundancy in data transfers. Additionally, we conduct extensive experiments on seven real-world graphs, and the results demonstrate that ${\sf GeoTri}$ significantly lowers both runtime and monetary cost while exhibiting nice accuracy and scalability.},
  archive  = {J},
  author   = {Delong Ma and Ye Yuan and Yanfeng Zhang and Chunze Cao and Yuliang Ma},
  doi      = {10.1109/TBDATA.2024.3522816},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {2008-2024},
  title    = {Cost-aware triangle counting over geo-distributed datacenters},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards fraud detection via fine-grained classification of user behavior. <em>TBD</em>, <em>11</em>(4), 1994-2007. (<a href='https://doi.org/10.1109/TBDATA.2024.3517313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The mass volume of data in the modern business world requires fraud detection to be automated. Hence, some researchers constructed the fraud scenario into graph data and proposed graph-based fraud detection methods. These methods treat the problem of fraud detection as a binary node classification task. However, the differences between the nodes of the same class are ignored. In this paper, we try to distinguish differences in behavior among nodes of the same class to improve the model’s ability to detect deviation, i.e., we make a fine-grained classification of user behavior (called prototypes) and propose an adaptive prototype-based graph neural network (APGNN) for fraud detection. APGNN learns node behavior representations by extracting both neighborhood and global information, supplying preliminary knowledge for the adaptive creation of several prototypes, each representing a distinct behavior pattern. Subsequently, a new loss function is employed to enhance the prototypes’ capacity to capture these behavior patterns and to amplify the feature differences between different prototypes. Nodes are then projected onto these prototypes to derive the final behavior patterns. Extensive experiments on four real-world datasets show that this method can provide better fraud detection as well as a more understandable result.},
  archive  = {J},
  author   = {Xinzhi Wang and Hang Yu and Jiayu Guo and Pengbo Li and Xiangfeng Luo},
  doi      = {10.1109/TBDATA.2024.3517313},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1994-2007},
  title    = {Towards fraud detection via fine-grained classification of user behavior},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph prompt learning method for the demand-responsive transport routing problem. <em>TBD</em>, <em>11</em>(4), 1983-1993. (<a href='https://doi.org/10.1109/TBDATA.2024.3512951'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Demand Responsive Transport (DRT) plays a crucial role in mitigating the inefficiencies of current public transit systems. Efficient routing is paramount for enhancing the flexibility and applicability of this transportation mode. Machine learning techniques, such as the attention-based encoder-decoder methodology, have the capability to produce solutions within seconds after offline training. However, these algorithms encounter convergence issues during training process, and demonstrate limited generalization ability, particularly across different scales. Thus, this paper proposes a graph prompt learning-based method comprising an information encoder, token generation, and token mapping to effectively train models that can adapt to diverse vehicles and demand variations. Particularly, token generation considers the characteristics of the problem by integrating vehicle and customer urgency information each time step. Token mapping obtains vehicle decoding sequences through attention mechanisms and mask function. The proposed model's performance is comprehensively evaluated against commonly baselines across various request contexts. Results show that our method can significantly reduce the computational time, and improve the quality of routing solution compared with baselines. Overall, the proposed model can enhance the routing efficiency of DRT systems through token mapping and prompts design.},
  archive  = {J},
  author   = {Ke Zhang and Meng Li},
  doi      = {10.1109/TBDATA.2024.3512951},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1983-1993},
  title    = {Graph prompt learning method for the demand-responsive transport routing problem},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A privacy-preserving large-scale image retrieval framework with vision GNN hashing. <em>TBD</em>, <em>11</em>(4), 1970-1982. (<a href='https://doi.org/10.1109/TBDATA.2024.3505052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the growing popularity of cloud services, companies and individuals outsource images to cloud servers to reduce storage and computing burdens. The images are encrypted before outsourcing for privacy protection. It has become urgent to solve the privacy-preserving image retrieval problem on the cloud. There are three main challenges in this area. First, how can we achieve high retrieval accuracy on the encryption domain? Second, how can we improve efficiency in large-scale encrypted image retrieval? Third, how can we ensure the reliability of the retrieval results? The existing schemes only consider some of these characteristics and the retrieval accuracy is insufficient. In this paper, we propose a privacy-preserving large-scale image retrieval framework with vision graph convolutional neural network hashing (ViGH). To the best of our knowledge, this is the first framework that is able to address all the above challenges with more advanced accuracy performance. To be specific, cycle-consistent adversarial networks and vision graph convolutional networks (ViG) are utilized to increase retrieval accuracy. By embedding encrypted images into hash codes, we can obtain high retrieval efficiency by Hamming distances. Cloud servers store the hash codes on the blockchain (Ethereum). The retrieval algorithm on the smart contracts and the consensus mechanism of blockchain ensure reliability of the retrieval results. The experimental results on three common datasets verify the effectiveness and efficiency of the proposed privacy-preserving image retrieval framework. The reliability of the retrieval results is ensured by the consensus mechanism of blockchain with no need for verification.},
  archive  = {J},
  author   = {Yuan Cao and Fanlei Meng and Xinzheng Shang and Jie Gui and Yuan Yan Tang},
  doi      = {10.1109/TBDATA.2024.3505052},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1970-1982},
  title    = {A privacy-preserving large-scale image retrieval framework with vision GNN hashing},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential modal multistage adaptive fusion networks via knowledge distillation for RGB-D mirror segmentation. <em>TBD</em>, <em>11</em>(4), 1959-1969. (<a href='https://doi.org/10.1109/TBDATA.2024.3505057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mirrors play a significant role in our daily lives and are ubiquitous. However, deep learning computer vision models find them challenging owing to the negative impact of reflected information on scene understanding. This study addresses two key challenges faced by multimodal models. First, the cross-modal variability of features at different stages is generally overlooked by contemporary backbone networks. Second, good performance has only been achieved at an unacceptable computational expense, owing to the numerous parameters used. To address the first challenge, we propose a differential-mode multistage adaptive fusion network (differential mode refers to images generated by different sensors that are differentiated to complement each other) that incorporates two-step fusion in the coding stage to account for the degrees of difference among the cross-modal features. In the first stage, wherein considerable differences in modal features exist, multi-angle fusion is performed. In the second stage, wherein the differences are smaller, a hierarchical adaptive fusion strategy is employed. Regarding the second challenge, we introduce a companion training framework for mirror segmentation that combines knowledge distillation and contrastive learning. Our proposed scheme achieves state-of-the-art performance on an available mirror segmentation dataset without requiring numerous parameters.},
  archive  = {J},
  author   = {Wujie Zhou and Han Zhang and Weiwei Qiu},
  doi      = {10.1109/TBDATA.2024.3505057},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1959-1969},
  title    = {Differential modal multistage adaptive fusion networks via knowledge distillation for RGB-D mirror segmentation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel graph learning with temporal stamp encoding for fraudulent transactions detections. <em>TBD</em>, <em>11</em>(4), 1945-1958. (<a href='https://doi.org/10.1109/TBDATA.2024.3499338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Financial transaction systems have become the critical backbone of modern society, and the sharp increase in fraudulent transactions has become an unavoidable significant topic. Their presence poses a severe threat to financial markets, impacting the health of the economic and social welfare systems of various countries. However, most existing fraud detection methods are limited to detecting individual fraudulent entities within static transaction networks, which are neither suitable for continuously changing dynamic transaction networks nor capable of detecting the increasingly prevalent organized fraud crimes. This paper introduces a novel approach, Parallel Graph Learning with Temporal Stamp Encoding (PGLTSE). On the one hand, it designs a history information module to perform temporal dimension feature learning to adapt to the continuous changes in transaction information in Continuous-Time Dynamic Graphs (CTDG). On the other hand, it designs a gang-aware risk propagation algorithm to infer the risk of organized fraudulent activities in the global transaction relation graph. By simultaneously conducting parallel graph representation learning in both homogeneous global transaction relation graphs and heterogeneous local entity interaction graphs, it aggregates local interaction and global association information for end-to-end training. Extensive experiments on diverse real-world datasets substantiate the superior performance of PGLTSE over existing methods, demonstrating its practical efficacy in detecting complex and evolving fraudulent behaviors in financial networks.},
  archive  = {J},
  author   = {Jiacheng Ma and Sheng Xiang and Qiang Li and Liangyu Yuan and Dawei Cheng and Changjun Jiang},
  doi      = {10.1109/TBDATA.2024.3499338},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1945-1958},
  title    = {Parallel graph learning with temporal stamp encoding for fraudulent transactions detections},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced and robust data publishing scheme for private and useful 1:M microdata. <em>TBD</em>, <em>11</em>(4), 1932-1944. (<a href='https://doi.org/10.1109/TBDATA.2024.3495497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A data publishing deal conducted with anonymous microdata can preserve the privacy of people. However, anonymizing data with multiple records of an individual (1:M dataset) is still a challenging problem. After anonymizing the 1:M microdata, the vertical correlation can be exploited to launch privacy attacks. In this paper, a novel privacy preserving model $l_{c}, l_{s}$-ANGEL is proposed. To validate the new model, two privacy attacks are presented, namely, a Vertical correlation attack ($V_{c0}$) and a Vulnerable sensitive attribute attack ($V_{sa}$) on 1:M datasets, which breach the privacy of individuals. Furthermore, the proposed model is examined through High-Level Petri Nets (HLPNs). Our experiments on three real-world datasets;“INFORMS”,“YOUTUBE”, and “IMDb” demonstrate that the proposed model outperforms the state-of-the-art models. Our practices and lessons learned in this work can direct future concrete steps towards Multiple Sensitive Attributes, where we can expand the proposed model to dynamic datasets.},
  archive  = {J},
  author   = {Muhammad Rizwan and Ammar Hawbani and Xingfu Wang and Adeel Anjum and Pelin Angin and Yigit Sever and Sanchuan Chen and Liang Zhao and Ahmed Al-Dubai},
  doi      = {10.1109/TBDATA.2024.3495497},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1932-1944},
  title    = {An enhanced and robust data publishing scheme for private and useful 1:M microdata},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GeoVoronoi: A voronoi diagram generation system for large-scale geographical point data via spatial attribute association. <em>TBD</em>, <em>11</em>(4), 1918-1931. (<a href='https://doi.org/10.1109/TBDATA.2024.3495499'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Voronoi diagram is commonly used to visualize geographical point dataset with a collection of plane-partitioned facets. As the size of the geographical point dataset increases, facets are densely distributed, and present different sizes and irregular shapes, leading to overdrawing and confusion problems, and hampering the visual perception of Voronoi diagram and insightful exploration of geographical point data. In this paper, we propose a novel Voronoi diagram generation framework to visualize and explore large-scale geographical point datasets. Firstly, an attribute-based blue noise sampling model is designed to select a subset of points to generate the simplified Voronoi diagram, retaining both the spatial distribution and attribute relationship of the original large-scale geographical points. Then a couple of optimization schemes are integrated into the sampling model to replace the representative points, aiming to enhance the visual perception of Voronoi diagram, such as shape balance and color characterization. Furthermore, we implement an interactive online Voronoi diagram generation tool, GeoVoronoi, enabling users to generate meaningful facets according to their requirements. Quantitative comparisons, case studies and user studies based on real-world datasets have demonstrated the effectiveness of our proposed method in the generation of credible Voronoi diagram and in-depth exploration of geographical point datasets.},
  archive  = {J},
  author   = {Zhiguang Zhou and Haoxuan Wang and Zhendong Yang and Yuanyuan Chen and Xiaohui Chen and Ying Lai and Wei Chen and Yuwei Meng},
  doi      = {10.1109/TBDATA.2024.3495499},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1918-1931},
  title    = {GeoVoronoi: A voronoi diagram generation system for large-scale geographical point data via spatial attribute association},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ranking neighborhood and class prototype contrastive learning for time series. <em>TBD</em>, <em>11</em>(4), 1907-1917. (<a href='https://doi.org/10.1109/TBDATA.2024.3495509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Time series are often complex and rich in information but sparsely labeled and therefore challenging to model. Existing contrastive learning methods conduct augmentations and maximize their similarity. However, they ignore the similarity of adjacent timestamps and suffer from the problem of sampling bias. In this paper, we propose a self-supervised framework for learning generalizable representations of time series, called $\mathbf {R}$anking n$\mathbf {E}$ ighborhood and cla$\mathbf {S}$s prototyp$\mathbf {E}$ contr$\mathbf {A}$stive $\mathbf {L}$earning (RESEAL). It exploits information about similarity ranking to learn an embedding space, ensuring that positive samples are ranked according to their temporal order. Additionally, RESEAL introduces a class prototype contrastive learning module. It contrasts time series representations and their corresponding centroids as positives against truly negative pairs from different clusters, mitigating the sampling bias issue. Extensive experiments conducted on several multivariate and univariate time series tasks (i.e., classification, anomaly detection, and forecasting) demonstrate that our representation framework achieves significant improvement over existing baselines of self-supervised time series representation.},
  archive  = {J},
  author   = {Chixuan Wei and Jidong Yuan and Yi Zhang and Zhongyang Yu and Yanze Liu and Haiyang Liu},
  doi      = {10.1109/TBDATA.2024.3495509},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1907-1917},
  title    = {Ranking neighborhood and class prototype contrastive learning for time series},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale feature-guided adversarial examples quality assessment via hierarchical perception of human visual system. <em>TBD</em>, <em>11</em>(4), 1894-1906. (<a href='https://doi.org/10.1109/TBDATA.2024.3495515'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep neural networks (DNNs) reveal significant robustness deficiencies due to their susceptibility to being misled by small and imperceptible adversarial examples, thus it is crucial to improve the robustness of DNNs against such harmful perturbations. The current $L_{p}$ specification ignores differences in human visual perception when measuring similarity, and most existing image quality assessment (IQA) methods and adversarial example datasets lack subjective scores for evaluation. In this paper, we construct a new database of adversarial examples, called the AED, which contains 35 original images, 1050 adversarial examples, and the corresponding subjective scores of adversarial examples. Then, a novel full-reference IQA model for the quality evaluation of the adversarial examples is proposed by taking into full consideration the hierarchical perception of human visual system (HVS) and the outstanding capabilities of the multi-scale feature extraction network in feature extraction. Specifically, a feature encoding network that uses continuous convolution layers to pre-extract features and expand the receptive field of the image is employed. To simulate the HVS hierarchical perception, the features of different scales are further obtained by designing a multi-scale feature extraction network. The structural similarity scores of the feature maps at different scales are calculated for jointly arriving at the final IQA score of the adversarial examples. Experimental results have demonstrated that our proposed model is closer to the perception of HVS in small imperceptible distortions evaluation of adversarial examples compared with other classical and state-of-the-art models.},
  archive  = {J},
  author   = {Wenying Wen and Minghui Huang and Li Dong and Yushu Zhang and Yuming Fang},
  doi      = {10.1109/TBDATA.2024.3495515},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1894-1906},
  title    = {Multiscale feature-guided adversarial examples quality assessment via hierarchical perception of human visual system},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language models for link stealing attacks against graph neural networks. <em>TBD</em>, <em>11</em>(4), 1879-1893. (<a href='https://doi.org/10.1109/TBDATA.2024.3489427'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph data contains rich node features and unique edge information, which have been applied across various domains, such as citation networks or recommendation systems. Graph Neural Networks (GNNs) are specialized for handling such data and have shown impressive performance in many applications. However, GNNs may contain of sensitive information and susceptible to privacy attacks. For example, link stealing is a type of attack in which attackers infer whether two nodes are linked or not. Previous link stealing attacks primarily relied on posterior probabilities from the target GNN model, neglecting the significance of node features. Additionally, variations in node classes across different datasets lead to different dimensions of posterior probabilities. The handling of these varying data dimensions posed a challenge in using a single model to effectively conduct link stealing attacks on different datasets. To address these challenges, we introduce Large Language Models (LLMs) to perform link stealing attacks on GNNs. LLMs can effectively integrate textual features and exhibit strong generalizability, enabling attacks to handle diverse data dimensions across various datasets. We design two distinct LLM prompts to effectively combine textual features and posterior probabilities of graph nodes. Through these designed prompts, we fine-tune the LLM to adapt to the link stealing attack task. Furthermore, we fine-tune the LLM using multiple datasets and enable the LLM to learn features from different datasets simultaneously. Experimental results show that our approach significantly enhances the performance of existing link stealing attack tasks in both white-box and black-box scenarios. Our method can execute link stealing attacks across different datasets using only a single model, making link stealing attacks more applicable to real-world scenarios.},
  archive  = {J},
  author   = {Faqian Guan and Tianqing Zhu and Hui Sun and Wanlei Zhou and Philip S. Yu},
  doi      = {10.1109/TBDATA.2024.3489427},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1879-1893},
  title    = {Large language models for link stealing attacks against graph neural networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M-graphormer: Multi-channel graph transformer for node representation learning. <em>TBD</em>, <em>11</em>(4), 1867-1878. (<a href='https://doi.org/10.1109/TBDATA.2024.3489418'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, the Graph Transformer has demonstrated superiority on various graph-level tasks by facilitating global interactions among nodes. However, as for node-level tasks, the existing Graph Transformer cannot perform as well as expected. Actually, a node in a real-world graph does not necessarily have relationships with every other node, and this global interaction weakens node features. This raises a fundamental question: should we partition out an appropriate interaction channel based on graph structure so that noisy and irrelevant information will be filtered and every node can aggregate information in the optimal channel? We first perform a series of experiments on manually created graphs with varying homophily ratios. Surprisingly, we observe that different graph structures indeed require distinct optimal interaction channels. This leads us to ask whether we can develop a partitioning rule that ensures each node interacts with relevant and valuable targets. To overcome this challenge, we propose a novel Graph Transformer named Multi-channel Graphormer. The model is evaluated on six network datasets with different homophily ratios for the node classification task. Moreover, comprehensive experiments are conducted on two real datasets for the recommendation task. Experimental results show that the Multi-channel Graphormer surpasses state-of-the-art baselines, demonstrating superior performance.},
  archive  = {J},
  author   = {Xinglong Chang and Jianrong Wang and Mingxiang Wen and Yingkui Wang and Yuxiao Huang},
  doi      = {10.1109/TBDATA.2024.3489418},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1867-1878},
  title    = {M-graphormer: Multi-channel graph transformer for node representation learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stable learning via dual feature learning. <em>TBD</em>, <em>11</em>(4), 1852-1866. (<a href='https://doi.org/10.1109/TBDATA.2024.3489413'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Stable learning aims to leverage the knowledge in a relevant source domain to learn a prediction model that can generalize well to target domains. Recent advances in stable learning mainly proceed by eliminating spurious correlations between irrelevant features and labels through sample reweighting or causal feature selection. However, most existing stable learning methods either only weaken partial spurious correlations or discard part of true causal relationships, resulting in generalization performance degradation. To tackle these issues, we propose the Dual Feature Learning (DFL) algorithm for stable learning, which consists of two phases. Phase 1 first learns a set of sample weights to balance the distribution of treated and control groups corresponding to each feature, and then uses the learned sample weights to assist feature selection to identify part of irrelevant features for completely isolating spurious correlations between these irrelevant features and labels. Phase 2 first learns two groups of sample weights again using the subdataset after feature selection, and then obtains high-quality feature representations by integrating a weighted cross-entropy model and an autoencoder model to further get rid of spurious correlations. Using synthetic and four real-world datasets, the experiments have verified the effectiveness of DFL, in comparison with eleven state-of-the-art methods.},
  archive  = {J},
  author   = {Shuai Yang and Xin Li and Minzhi Wu and Qianlong Dang and Lichuan Gu},
  doi      = {10.1109/TBDATA.2024.3489413},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1852-1866},
  title    = {Stable learning via dual feature learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Big data analysis for industrial activity recognition using attention-inspired sequential temporal convolution network. <em>TBD</em>, <em>11</em>(4), 1840-1851. (<a href='https://doi.org/10.1109/TBDATA.2024.3489414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep-learning-based human activity recognition (HAR) methods have significantly transformed a wide range of domains over recent years. However, the adoption of Big Data techniques in industrial applications remains challenging due to issues such as generalized weight optimization, diverse viewpoints, and the complex spatiotemporal features of videos. To address these challenges, this work presents an industrial HAR framework consisting of two main phases. First, a squeeze bottleneck attention block (SBAB) is introduced to enhance the learning capabilities of the backbone model for contextual learning, which allows for the selection and refinement of an optimal feature vector. In the second phase, we propose an effective sequential temporal convolutional network (STCN), which is designed in parallel fashion to mitigate the issues of exploding and vanishing gradients associated with sequence learning. The high-dimensional spatiotemporal feature vectors from the STCN undergo further refinement through our proposed SBAB in a sequential manner, to optimize the features for HAR and enhance the overall performance. The efficacy of the proposed framework is validated through extensive experiments on six datasets, including data from industrial and general activities.},
  archive  = {J},
  author   = {Altaf Hussain and Tanveer Hussain and Waseem Ullah and Samee Ullah Khan and Min Je Kim and Khan Muhammad and Javier Del Ser and Sung Wook Baik},
  doi      = {10.1109/TBDATA.2024.3489414},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1840-1851},
  title    = {Big data analysis for industrial activity recognition using attention-inspired sequential temporal convolution network},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based complex logical query on temporal knowledge graph via graph neural network. <em>TBD</em>, <em>11</em>(4), 1828-1839. (<a href='https://doi.org/10.1109/TBDATA.2024.3489421'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Answering complex logical queries on large-scale Knowledge Graphs (KGs) efficiently and accurately has always been crucial for question-answering systems. Recent studies have significantly improved the performance of complex logical queries on massive knowledge graphs by leveraging graph neural networks (GNNs). However, the existing GNN-based methods still have limitations in dealing with long-sequence logical queries. They usually decompose complex queries into multiple independent first-order logical queries, which leads to the inability to optimize globally, and the query accuracy will drop sharply with the increase of query length. In addition, the knowlege in the real world is dynamically changing, but most of the existing methods are more suitable for dealing with static knowledge graphs, and there is still much room for improvement when dealing with logical queries in temporal knowledge graphs. In this paper, we propose a novel Temporal Complex Logical Query (TCLQ) model to achieve temporal logical queries on temporal knowledge graphs. We add time series embedding into GNN, and use multi-layer GRUs to aggregate the node features of previous time and current time, which effectively enhances the time series reasoning ability of the model. In order to solve the problem that the accuracy of logical query model decreases significantly with the increase of query sequence length, we establish a multi-level attention coefficients model to learn and optimize the whole logical queries, thus reducing the error accumulation problem when the queries are decomposed into multiple independent first-order logical queries. We conduct experiments on multiple temporal datasets and demonstrate the effectiveness of TCLQ.},
  archive  = {J},
  author   = {Luyi Bai and Linshuo Xu and Lin Zhu},
  doi      = {10.1109/TBDATA.2024.3489421},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1828-1839},
  title    = {Attention-based complex logical query on temporal knowledge graph via graph neural network},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DGNN: Decoupled graph neural networks with structural consistency between attribute and graph embedding representations. <em>TBD</em>, <em>11</em>(4), 1813-1827. (<a href='https://doi.org/10.1109/TBDATA.2024.3489420'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph neural networks (GNNs) exhibit a robust capability for representation learning on graphs with complex structures, demonstrating superior performance across various applications. Most existing GNNs utilize graph convolution operations that integrate both attribute and structural information through coupled way. And these GNNs, from an optimization perspective, seek to learn a consensus and compromised embedding representation that balances attribute and graph information, selectively exploring and retaining valid information in essence. To obtain a more comprehensive embedding representation, a novel GNN framework, dubbed Decoupled Graph Neural Networks (DGNN), is introduced. DGNN separately explores distinctive embedding representations from the attribute and graph spaces by decoupled terms. Considering that the semantic graph, derived from attribute feature space, contains different node connection information and provides enhancement for the topological graph, both topological and semantic graphs are integrated by DGNN for powerful embedding representation learning. Further, structural consistency between the attribute embedding and the graph embedding is promoted to effectively eliminate redundant information and establish soft connection. This process involves facilitating factor sharing for adjacency matrices reconstruction, which aims at exploring consensus and high-level correlations. Finally, a more powerful and comprehensive representation is achieved through the concatenation of these embeddings. Experimental results conducted on several graph benchmark datasets demonstrate its superiority in node classification tasks.},
  archive  = {J},
  author   = {Jinlu Wang and Jipeng Guo and Yanfeng Sun and Junbin Gao and Shaofan Wang and Yachao Yang and Baocai Yin},
  doi      = {10.1109/TBDATA.2024.3489420},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1813-1827},
  title    = {DGNN: Decoupled graph neural networks with structural consistency between attribute and graph embedding representations},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic radio map construction with minimal manual intervention: A state space model-based approach with imitation learning. <em>TBD</em>, <em>11</em>(4), 1799-1812. (<a href='https://doi.org/10.1109/TBDATA.2024.3489425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fingerprint localization methods typically require a substantial amount of manual effort to collect fingerprint data from various scenarios to construct an accurate radio map. While some existing research has attempted to use path planning strategies to save on labor costs, these approaches often suffer from being time-consuming and prone to locally optimal solutions. To address these shortcomings, our paper proposes a novel approach that utilizes imitation learning to construct and update a highly accurate radio map with minimal manual intervention in dynamic environments. Specifically, we employ a multivariate Gaussian process model to fit a rough standby fingerprint database with only a few pilot data points. We then utilize a state space model to calculate the variation range of the pilot data, which forms the CSI error band used to filter the rough radio map. Imitation learning and a confidence coefficient are utilized to predict and calibrate the global CSI data distribution. And we utilize the K-nearest neighbor algorithm to achieve the real-time localization function. Experimental results show that our proposed algorithm outperforms several state-of-the-art approaches in most test cases, exhibiting low computation complexity, lower localization error, and saving 73.3% of the manual workload.},
  archive  = {J},
  author   = {Xiaoqiang Zhu and Tie Qiu and Wenyu Qu and Xiaobo Zhou and Tuo Shi and Tianyi Xu},
  doi      = {10.1109/TBDATA.2024.3489425},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1799-1812},
  title    = {Dynamic radio map construction with minimal manual intervention: A state space model-based approach with imitation learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reward shaping based on optimal-policy-free. <em>TBD</em>, <em>11</em>(4), 1787-1798. (<a href='https://doi.org/10.1109/TBDATA.2024.3489415'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Existing research on potential-based reward shaping (PBRS) relies on optimal policy in Markov decision process (MDP) where optimal policy is regarded as the ground truth. However, in some practical application scenarios, there is an extrapolation error challenge between the computed optimal policy and the real-world optimal policy. At this time, the optimal policy is unreliable. To address this challenge, we design a Reward Shaping based on Optimal-Policy-Free to get rid of the dependence on the optimal policy. We view reinforcement learning as probabilistic inference on a directed graph. Essentially, this inference propagates information from the rewarding states in the MDP and results in a function which is leveraged as a potential function for PBRS. Our approach utilizes a contrastive learning technique on directed graph Laplacian. Here, this technique does not change the structure of the directed graph. Then, the directed graph Laplacian is used to approximate the true state transition matrix in MDP. The potential function in PBRS can be learned through the message passing mechanism which is built on this directed graph Laplacian. The experiments on Atari, MuJoCo and MiniWorld show that our approach outperforms the competitive algorithms.},
  archive  = {J},
  author   = {Jianghui Sang and Yongli Wang and Zaki Ahmad Khan and Xiaoliang Zhou},
  doi      = {10.1109/TBDATA.2024.3489415},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1787-1798},
  title    = {Reward shaping based on optimal-policy-free},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFML: An asynchronous federated meta-learning mechanism for charging station occupancy prediction with biased and isolated data. <em>TBD</em>, <em>11</em>(4), 1772-1786. (<a href='https://doi.org/10.1109/TBDATA.2024.3484651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Electric vehicles (EVs) are driving green and low-carbon transport in modern cities. It makes charging station occupancy prediction (CSOP) critual for intelligent transportation systems (ITS) to achieve a balance between the supply and demand in resolving the dynamics between EVs and changing stations. Even though several Big Data-based solutions have been discussed, they are still struggling to collaboratively utilize heterogeneous data and distributed computing resources located at both physically and logicially isolated charging stations to better support context-driven CSOP. To addres this challenge, we propose an Asynchronous Federated Meta-learning Mechanism (AFML) for CSOP, which can train a meta-model with strong adaptation ability in an asynchronous and collaborative manner. In general, it incorporates an adaptive reptile algorithm (AR) and an weighted aggregation strategy (WA) to jointly ensure the training efficiency and model adaptivity. Evaluations on real-world CSOP datasets demonstrate that compared to the second best method, AFML can significantly improve forecasting accuracy by 14%, accelerate model convergence by 9% and enhance model generalizability by 10%, illustrating its merits in support CSOP to embrace a smart and sustainable city.},
  archive  = {J},
  author   = {Qiyang Chen and Linlin You and Haohao Qu and Ahmed M. Abdelmoniem and Chau Yuen},
  doi      = {10.1109/TBDATA.2024.3484651},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1772-1786},
  title    = {AFML: An asynchronous federated meta-learning mechanism for charging station occupancy prediction with biased and isolated data},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting encrypted traffic classification using feature-enhanced recurrent neural network with angle constraint. <em>TBD</em>, <em>11</em>(4), 1760-1771. (<a href='https://doi.org/10.1109/TBDATA.2024.3484674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the surge in various types of network traffic and the widespread application of encryption technology, the classification of encrypted traffic plays an increasingly important role in ensuring network security, enhancing quality of service, and managing network traffic. However, most existing methods often suffer from issues such as excessive reliance on manual feature extraction and expert knowledge, unstable classification performance, and lack of transfer learning capabilities. To address these challenges, this paper proposes a high-performance hybrid encrypted traffic classification framework, FERNN-AC. It directly extracts features from raw traffic and fully explores and utilizes the spatiotemporal information of traffic data by integrating specially designed feature enhancement module and temporal feature extraction module in a reasonable manner. It introduces angle constraints and can be combined with meta-learning, thereby improving classification performance while possessing certain transfer learning capabilities. The experiments are conducted on three datasets, and the results shows that compare with relevant baseline methods, FERNN-AC has excellent and stable classification performance.},
  archive  = {J},
  author   = {Gongxun Miao and Guohua Wu and Zhen Zhang and Yongjie Tong and Bing Lu},
  doi      = {10.1109/TBDATA.2024.3484674},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1760-1771},
  title    = {Boosting encrypted traffic classification using feature-enhanced recurrent neural network with angle constraint},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LBFL: A lightweight blockchain-based federated learning framework with proof-of-contribution committee consensus. <em>TBD</em>, <em>11</em>(4), 1745-1759. (<a href='https://doi.org/10.1109/TBDATA.2024.3481952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Blockchain technology makes it possible to design robust decentralized federated learning (FL). Minimizing the communication cost and storage consumption incurred is one of the essential challenges. In addition, maintaining the security and privacy of Big Data raises to be a difficult problem. Aiming to tackle these challenges, this paper presents LBFL (a Lightweight Blockchain-based FL framework) that offers three novel features. First, it employs a new committee consensus mechanism called Proof-of-Contribution, which is used to avoid the selection latency from the competition of miners and alleviate the congestion in cross-validation of parameters in an asynchronous fashion. Second, LBFL employs a role-adaptive incentive mechanism to estimate devices’ workloads and identify malicious nodes effectively. Third, to cope with the excessive storage overheads incurred in full-replication, LBFL applies a new storage partition mechanism that distributes triple redundant chunks in Reed-Solomon coding (RSC) evenly to participating devices with high fault tolerance and recovery efficiency. To evaluate LBFL, empirical studies are performed on the famous MNIST dataset and LBFL is compared with the state-of-the-art FL frameworks. The results demonstrate that LBFL can reduce evaluation latency and storage consumption by 69.2% and 72.1%, respectively, and the learning efficiency of LBFL is higher than the state-of-the-art methods. In particular, important findings are obtained: the proposed role-adaptive incentive mechanism can properly identify malicious devices and switch the roles of legitimate devices to achieve good decentralization.},
  archive  = {J},
  author   = {Shaojie Qiao and Yuhe Jiang and Nan Han and Wei Hua and Yufeng Lin and Shengjie Min and Xindong Wu},
  doi      = {10.1109/TBDATA.2024.3481952},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1745-1759},
  title    = {LBFL: A lightweight blockchain-based federated learning framework with proof-of-contribution committee consensus},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information switching patterns of risk communication in social media during disasters. <em>TBD</em>, <em>11</em>(4), 1733-1744. (<a href='https://doi.org/10.1109/TBDATA.2024.3524828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In an era increasingly affected by natural and human-caused disasters, the role of social media in disaster communication has become ever more critical. Despite substantial research on social media use during crises, a significant gap remains in detecting crisis-related misinformation. Detecting deviations in information is fundamental for identifying and curbing the spread of misinformation. This study introduces a novel Information Switching Pattern Model to identify dynamic shifts in perspectives among users who mention each other in crisis-related narratives on social media. These shifts serve as evidence of crisis misinformation affecting user-mention network interactions. The study utilizes advanced natural language processing, network science, and census data to analyze geotagged tweets related to compound disaster events in Oklahoma in 2022. The impact of misinformation is revealed by distinct engagement patterns among various user types, such as bots, private organizations, non-profits, government agencies, and news media throughout different disaster stages. These patterns show how different disasters influence public sentiment, highlight the heightened vulnerability of mobile home communities, and underscore the importance of education and transportation access in crisis response. Understanding these engagement patterns is crucial for detecting misinformation and leveraging social media as an effective tool for risk communication during disasters.},
  archive  = {J},
  author   = {Khondhaker Al Momin and Arif Mohaimin Sadri and Kristin Olofsson and K.K. Muraleetharan and Hugh Gladwin},
  doi      = {10.1109/TBDATA.2024.3524828},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1733-1744},
  title    = {Information switching patterns of risk communication in social media during disasters},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative network-based retrieval model for open source domain experts. <em>TBD</em>, <em>11</em>(4), 1720-1732. (<a href='https://doi.org/10.1109/TBDATA.2024.3524829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Aiming at the problem that the GitHub platform only supports the retrieval of developers through their usernames and it is difficult to directly obtain developers' expertise information, this paper proposes an open source domain expert retrieval model (OSDERM) based on the network representation learning algorithm OSC2vec (Open Source Collaboration to Vector). The model mainly consists of two core parts: Expert Profiling and Expert Finding. Expert Profiling aims to enrich the expertise information in the search results by labeling the expertise of developers; while Expert Finding achieves rapid location of the most suitable domain experts through keyword matching, which greatly saves the time and effort of searching for experts in the open source community. Experiments using the GitHub ecological dataset show that the model outperforms existing comparative algorithms in discovering open source domain experts, and can provide an effective reference for enterprise recruitment},
  archive  = {J},
  author   = {Qingxi Peng and Zhenjie Weng and Wei Wang and Xinyi Wang and Lan You},
  doi      = {10.1109/TBDATA.2024.3524829},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1720-1732},
  title    = {A collaborative network-based retrieval model for open source domain experts},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing re-indexing for top-k personalized PageRank computation on dynamic graphs. <em>TBD</em>, <em>11</em>(4), 1707-1719. (<a href='https://doi.org/10.1109/TBDATA.2024.3524833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Top-k Personalized PageRank (PPR) is a graph analysis method used to determine the $k$ most important nodes with respect to a source node. To realize fast Top-k PPR computation, indexing for each node is effective. When we apply the index-based Top-k PPR methods to dynamic graphs, the index becomes stale with edge updates, and index correction is required. Although the existing methods perform index correction for every update to guarantee Top-k PPR accuracy, they involve heavy re-indexing computation or significant memory overhead. This paper proposes a method that achieves comparable accuracy to guaranteed methods while significantly reducing re-indexing by focusing on the fact that index references are concentrated on the nodes whose index is unlikely to change due to edge updates. In particular, our method omits re-indexing as long as we achieve comparable accuracy. Furthermore, our method involves the minimum memory overhead among the existing index-based methods. The space complexity of the index is $\Theta (n + m)$, where $n$ and $m$ are the number of nodes and edges of the graph, respectively. The evaluation results using real-world datasets show that our method achieves more than 0.999 Normalized Discounted Cumulative Gain until 20% of edges are updated from index generation.},
  archive  = {J},
  author   = {Tsuyoshi Yamashita and Naoki Matsumoto and Kunitake Kaneko},
  doi      = {10.1109/TBDATA.2024.3524833},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1707-1719},
  title    = {Reducing re-indexing for top-k personalized PageRank computation on dynamic graphs},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A marketing topic traceability model based on domain preference and heterogeneous network. <em>TBD</em>, <em>11</em>(4), 1692-1706. (<a href='https://doi.org/10.1109/TBDATA.2024.3524831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The development of social networks has prompted a shift in marketing strategies, with a surging demand for marketing in vertical domains characterized by high user stickiness and specialization. To address this, we propose a traceability model based on domain preference and heterogeneous networks. First, considering the problem of marketing topic vertical domains features metric and the influence of users’ preference degree for domains on topic propagation, the domains are treated as latent semantics, and the user-topic association matrix sparse matrix is densified using a latent factor model to mine the domain preference information efficiently. Second, considering the complexity of the association between multi-type elements in marketing topics, the HLN2vec (Heterogeneous Layer-wise Networks) model is proposed. This model uses heterogeneous network representation learning and incorporates multi-layer attention networks to learn the representations to portray a marketing topic’s key elements and their relationships. Finally, this paper proposes the DP-Rank(Domain Preference-based) algorithm, which uses domain preference features and an adaptive random walking strategy to quantify element influence. Based on experiments, the proposed model robustly applies in social networks and exhibits clear advantages in measuring vertical domain features of marketing topics, constructing multi-type element relationship networks, and discovering core element influence.},
  archive  = {J},
  author   = {Tun Li and Di Lei and Qian Li and Rong Wang and Chaolong Jia and Yunpeng Xiao},
  doi      = {10.1109/TBDATA.2024.3524831},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1692-1706},
  title    = {A marketing topic traceability model based on domain preference and heterogeneous network},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tucker-based high-accuracy multi-modal clustering for social information network. <em>TBD</em>, <em>11</em>(4), 1677-1691. (<a href='https://doi.org/10.1109/TBDATA.2024.3524830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the explosion of social media platforms, a substantial amount of data is generated from social information network. Tensor-based multi-modal clustering methods have been widely applied in various scenarios of social information network by mining potential correlative relationships from large-scale heterogeneous data. Nevertheless, the accuracy and efficiency of tensor-based multi-modal clustering methods are seriously restricted by noise data and the curse of dimensionality. Therefore, this paper presents a Tucker-based multi-modal clustering (TuMC) and an improved TuMC (ITuMC) to enhance the accuracy and efficiency of multi-modal clustering. First, we propose two Tucker-based attribute weight ranking learning approaches to calculate weight tensor efficiently. Then, we present a calculation approach for Tucker-based selective weighted tensor distance (SWTD) and a TuMC method. Meanwhile, an ITuMC method is explored by optimizing the calculation efficiency of the SWTD to further improve clustering speed. Finally, we present a Tucker-based multi-modal clustering and service framework for social information network. Extensive experimental results based on social Geolife GPS trajectory and electricity consumption datasets demonstrate that the TuMC and ITuMC methods can cluster multi-source heterogeneous data with both higher accuracy and efficiency under complex social information network by DVI, AR and execution time measurement.},
  archive  = {J},
  author   = {Ren Li and Huazhong Liu and Xiaotong Zhou and Jiawei Wang and Jihong Ding and Laurence T. Yang and Hua Li and Yunfan Zhang},
  doi      = {10.1109/TBDATA.2024.3524830},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1677-1691},
  title    = {Tucker-based high-accuracy multi-modal clustering for social information network},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GE-GNN: Gated edge-augmented graph neural network for fraud detection. <em>TBD</em>, <em>11</em>(4), 1664-1676. (<a href='https://doi.org/10.1109/TBDATA.2025.3562486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph Neural Networks (GNNs) play a significant role and have been widely applied in fraud detection tasks, exhibiting substantial improvements in detection performance compared to conventional methodologies. However, within the intricate structure of fraud graphs, fraudsters usually camouflage themselves among a large number of benign entities. An effective solution to address the camouflage problem involves the incorporation of complex and abundant edge information. Nevertheless, existing GNN-based methods frequently neglect to integrate this crucial information into the message passing process, thereby limiting their efficacy. To address the above issues, this study proposes a novel Gated Edge-augmented Graph Neural Network(GE-GNN). Our approach begins with an edge-based feature augmentation mechanism that leverages both node and edge features within a single relation. Subsequently, we apply the augmented representation to the message passing process to update the node embeddings. Furthermore, we design a gate logistic to regulate the expression of augmented information. Finally, we integrate node features across different relations to obtain a comprehensive representation. Extensive experimental results on two real-world datasets demonstrate that the proposed method outperforms several state-of-the-art methods.},
  archive  = {J},
  author   = {Wenxin Zhang and Cuicui Luo},
  doi      = {10.1109/TBDATA.2025.3562486},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1664-1676},
  title    = {GE-GNN: Gated edge-augmented graph neural network for fraud detection},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Casformer: Information popularity prediction with adaptive cascade sampling and graph transformer in social networks. <em>TBD</em>, <em>11</em>(4), 1652-1663. (<a href='https://doi.org/10.1109/TBDATA.2024.3524839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Predicting the popularity of information in social networks is crucial for effective social marketing and recommendation systems. However, accurately comprehending the complex dynamics of information diffusion remains a challenging task. Existing methods, including feature-based approaches, point process models, and deep learning techniques, often fail to capture the fine-grained features of information cascades, such as dynamic diffusion patterns, cascade statistics, and the interplay between spatial and temporal information. To address these limitations, we propose Casformer, a novel graph-based Transformer architecture that effectively learns both micro-level time-aware structural information and macro-level long-term influence along the information propagation process. Casformer employs a cascade attention network (CAT) to capture the micro-level features and a Transformer model to learn the macro-level influence. Furthermore, we introduce an adaptive cascade graph sampling strategy based on the temporal diffusion pattern and cascade statistics of information to obtain the most informative cascade graph sequence. By leveraging multi-level fine-grained evolving features of information cascades, Casformer achieves high accuracy in information popularity prediction. Experimental results on real-world social network and scientific citation network datasets demonstrate the effectiveness and superiority of Casformer compared to state-of-the-art methods in information popularity prediction.},
  archive  = {J},
  author   = {Biao Wang and Zhao Li and Zenghui Xu and Ji Zhang},
  doi      = {10.1109/TBDATA.2024.3524839},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1652-1663},
  title    = {Casformer: Information popularity prediction with adaptive cascade sampling and graph transformer in social networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial: Big data analytics in complex social information networks. <em>TBD</em>, <em>11</em>(4), 1650-1651. (<a href='https://doi.org/10.1109/TBDATA.2024.3485316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This special issue deals with research related to applications of and methods to support Big Data analytics in complex social information networks. The digital age and the rise of social media have sped up changes to social systems with unforeseen consequences. However, there are major challenges created.},
  archive  = {J},
  author   = {Desheng Dash Wu and David L. Olson},
  doi      = {10.1109/TBDATA.2024.3485316},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1650-1651},
  title    = {Editorial: Big data analytics in complex social information networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Denoising neural relation extraction for spatio-temporal recommendation system. <em>TBD</em>, <em>11</em>(4), 1640-1649. (<a href='https://doi.org/10.1109/TBDATA.2024.3407594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Point-of-Interest (POI) recommendation system in location-based social networks is pivotal, offering versatile applications. Personalized recommendations hinge on pre-established user relationships, emphasizing geographic proximity and character similarities. To attain spatial and temporal information, natural language text mining, particularly relation extraction tasks, emerges as crucial, drawing from the abundant textual data on the Internet. However, challenges arise with traditional supervised methods relying on human-annotated data and distant supervision introducing noises, which prickly and seriously degrade the effectiveness of the sentence encoding for valid relation mentions. To address this issue, we aim to rectify the sentence encoder by acquiring external knowledge from an existing fully supervised dataset. More specifically, we craft transfer learning, together with adversarial training, to discover the semantic consistency in the valid relation mentions. During training, the sentence encoder receives data from both domains successively and generates a consistent output for them. While the discriminator distinguishes, as far as possible, which domain these sentences come from. After the above training procedure, the encoder can be made insensitive to the noise in distantly supervised data for denoising purposes. The experimental results on the widely used dataset NYT10 demonstrate that our model outperforms current state-of-the-art methods for relation extraction.},
  archive  = {J},
  author   = {Ye Wang and Lihong Guo and Yang Yu and Yuan Gao},
  doi      = {10.1109/TBDATA.2024.3407594},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1640-1649},
  title    = {Denoising neural relation extraction for spatio-temporal recommendation system},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward high-quality spatiotemporal recommendation: Trajectory recovery based on spatial and temporal dependencies. <em>TBD</em>, <em>11</em>(4), 1628-1639. (<a href='https://doi.org/10.1109/TBDATA.2025.3570071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The rapid advancement of location and information technologies has generated a significant volume of human mobility data, which has been extensively utilized in spatiotemporal recommendation systems, including personalized point-of-interest recommendation, route recommendation, and location-aware event recommendation. Achieving high-quality recommendation results necessitates excellent quality of input trajectory data. However, trajectories obtained from GPS-enabled devices often contain missing and erroneous data that is unevenly distributed over time and highly sparse, which significantly hampers the effectiveness spatiotemporal data analytics. Therefore, trajectory recovery plays an important role in spatiotemporal recommendation systems. The objective of trajectory recovery is to utilize historical trajectories to restore missing locations, providing high-quality data for spatiotemporal recommendation systems. The development of an effective trajectory recovery mechanism faces three major challenges: 1) Complex and multi-granularity transition patterns among different locations; 2) Difficulty in discovering spatio-temporal dependencies; and 3) Data sparsity and noise. To address these challenges, we propose an attentional model with spatio-temporal recurrent neural networks, ARMove, to recover human mobility from long and sparse trajectories. In ARMove, we first design a spatio-temporal weighted recurrent neural network to capture users’ long-term preferences. Next, we introduce a multi-granularity trajectory encoder to model complex transition patterns and multi-level periodicity of human mobility. An attention-based history aggregation module is proposed to leverage historical mobility information. Extensive evaluation results reveal that our model outperforms the state-of-the-art models, demonstrating its ability to reconstruct high-quality and fine-grained human mobility trajectories.},
  archive  = {J},
  author   = {Yihao Zhao and Chenhao Wang and Hongyu Wang and Shunzhi Zhu and Lisi Chen},
  doi      = {10.1109/TBDATA.2025.3570071},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1628-1639},
  title    = {Toward high-quality spatiotemporal recommendation: Trajectory recovery based on spatial and temporal dependencies},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Where shall we go: Point-of-interest group recommendation with user preference embedding. <em>TBD</em>, <em>11</em>(4), 1614-1627. (<a href='https://doi.org/10.1109/TBDATA.2024.3499356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the context of geo-social networks, the objective of Point-of-Interest (POI) group recommendation is to propose POIs that align with the preferences of all members within a specific temporal group. POI group recommendation is significant in enhancing user experience, promoting social interaction, and providing convenient access to information. It also aids in community building and business promotion in real-life scenarios. However, existing studies fail to capture user preferences accurately and reach consensus with respect to preferences for POIs, which leads to the recommendation of POIs with low accuracy. To tackle this issue, we propose a Point-of-Interest (POI) group recommendation model, named PGR-PM, leveraging user preference embedding. Specifically, we first propose a strategy for representing user preferences dynamically by means of POI embedding. Subsequently, we propose a hybrid weight fusion strategy that utilizes an attention mechanism to aggregate the preferences of members within a temporal group. Furthermore, we design a three-layer perceptron structure to recommend POIs for the group. Finally, we conduct comprehensive experiments across four extensively employed real-world datasets, with the findings affirming the efficacy of our proposed approach.},
  archive  = {J},
  author   = {Yuliang Ma and Zhong-Zhong Jiang and Mingyang Sun and Ye Yuan and Guoren Wang},
  doi      = {10.1109/TBDATA.2024.3499356},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1614-1627},
  title    = {Where shall we go: Point-of-interest group recommendation with user preference embedding},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Many hands make light work: Group influence maximization in evolving social networks. <em>TBD</em>, <em>11</em>(4), 1600-1613. (<a href='https://doi.org/10.1109/TBDATA.2024.3499345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As the adage “many hands make light work” suggests, collaborative influence often surpasses individual influence. Inspired by this insight, we undertook a study on group influence maximization in evolving social networks, which is applicable to domains such as social media marketing and financial risk management. Our goal is to reveal how collaborative influence propagates in dynamic settings. Existing research concentrates predominantly on static networks and overlooks the dynamics of evolving social structures. Recognizing the limitations of current influence propagation models for our specific issue, we introduce an innovative model rooted in user behaviors. It considers temporal aspects, and we also suggest a methodology for assessing influence propagation probabilities based on both user behaviors and duration. We introduce an algorithm for extracting user groups using community search, improving efficiency through supergraph construction. Additionally, we present an influence maximization algorithm based on group dynamics with a 3-degree propagation framework. Recognizing diminishing influence, a 3-degree truncation strategy effectively enhances the group influence propagation efficiency. This approach efficiently captures the influence spread and accelerates convergence, boosting the search efficiency. Finally, we conducted comprehensive experiments on real-world and synthetic datasets. The results distinctly highlight the superiority of the proposed algorithms.},
  archive  = {J},
  author   = {Yuliang Ma and Yu Chen and Peng Wei and Ye Yuan and Guoren Wang and Zhong-Zhong Jiang},
  doi      = {10.1109/TBDATA.2024.3499345},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1600-1613},
  title    = {Many hands make light work: Group influence maximization in evolving social networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal learning with decoupled causal attention for multivariate time series. <em>TBD</em>, <em>11</em>(4), 1589-1599. (<a href='https://doi.org/10.1109/TBDATA.2024.3499312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In multivariate time series prediction tasks, the inter- and intra-variable relations have significant influence on prediction outcomes. In many engineering and industrial scenarios, the multivariate time series also contain a large number of subjective influencing factors, such as settings and behaviors of users. Existing learning methods neglect the interactions of these subjective factors among variables. This leads to the learning of incorrect inter-variable influences, consequently yielding inaccurate prediction results. To address this challenge, we propose a Decoupled Casal Attention Network (DECA) for multivariate time series prediction from a spatiotemporal learning perspective. multivariate time series prediction. The causality decoupling module, based on the captured causal relations among variables, disentangles the subjective factors from the objective factors. Then the objective learning module utilizes an objective causal attention to capture objective cross-variable dependencies; while the subjective learning module utilizes a subjective causal graph attention to capture subjective influences. Finally, the prediction module fuses the multi-scale features of subjective and objective factors to produce predictions. The performance is evaluated using three benchmark datasets. Results indicate that, compared to state-of-the-art methods, DECA exhibits superior accuracy in multivariate time series prediction and can be effectively used for recommendations.},
  archive  = {J},
  author   = {Xin Bi and Qinghan Jin and Meiling Song and Xin Yao and Xiangguo Zhao and Ye Yuan and Guoren Wang},
  doi      = {10.1109/TBDATA.2024.3499312},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  number   = {4},
  pages    = {1589-1599},
  title    = {Spatiotemporal learning with decoupled causal attention for multivariate time series},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial high-performance recommender systems based on spatiotemporal data. <em>TBD</em>, <em>11</em>(4), 1588. (<a href='https://doi.org/10.1109/TBDATA.2024.3451088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Shuo Shang and Qi Liu and Renhe Jiang and Ryosuke Shibasaki and Panos Kalnis and Christian S. Jensen},
  doi     = {10.1109/TBDATA.2024.3451088},
  journal = {IEEE Transactions on Big Data},
  month   = {8},
  number  = {4},
  pages   = {1588},
  title   = {Editorial high-performance recommender systems based on spatiotemporal data},
  volume  = {11},
  year    = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Big data-driven advancements and future directions in vehicle perception technologies: From autonomous driving to modular buses. <em>TBD</em>, <em>11</em>(3), 1568-1587. (<a href='https://doi.org/10.1109/TBDATA.2025.3527208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The rapid development of Big Data and artificial intelligence (AI) is revolutionizing the automotive and transportation industries, leading to the creation of the Autonomous Modular Bus (AMB). Designed to address the key challenges of modern public transportation systems, the AMB adopts a modular dynamic assembly approach. However, existing research on the AMB predominantly focuses on operational aspects, whereas in-transit docking remains the primary obstacle to its commercial deployment. This challenge stems from the fact that current perception accuracy in autonomous vehicles is limited to the decimeter level, with insufficient capability to manage adverse weather and complex traffic conditions. To enable AMBs to achieve full-scenario autonomous driving capabilities, this paper reviews current perception technologies from three perspectives: single-vehicle single-sensor perception, multi-sensor fusion perception, and cooperative perception. It examines the characteristics of existing perception solutions and evaluates their applicability to AMB-specific requirements. Furthermore, considering the unique challenges of in-transit docking, this paper identifies and proposes four future research directions for advancing AMB perception systems as well as general autonomous driving technologies.},
  archive  = {J},
  author   = {Hongyi Lin and Yang Liu and Liang Wang and Xiaobo Qu},
  doi      = {10.1109/TBDATA.2025.3527208},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1568-1587},
  title    = {Big data-driven advancements and future directions in vehicle perception technologies: From autonomous driving to modular buses},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advances in robust federated learning: A survey with heterogeneity considerations. <em>TBD</em>, <em>11</em>(3), 1548-1567. (<a href='https://doi.org/10.1109/TBDATA.2025.3527202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the field of heterogeneous federated learning (FL), the key challenge is to efficiently and collaboratively train models across multiple clients with different data distributions, model structures, task objectives, computational capabilities, and communication resources. This diversity leads to significant heterogeneity, which increases the complexity of model training. In this paper, we first outline the basic concepts of heterogeneous FL and summarize the research challenges in FL in terms of five aspects: data, model, task, device and communication. In addition, we explore how existing state-of-the-art approaches cope with the heterogeneity of FL, and categorize and review these approaches at three different levels: data-level, model-level, and architecture-level. Subsequently, the paper extensively discusses privacy-preserving strategies in heterogeneous FL environments. Finally, the paper discusses current open issues and directions for future research, aiming to promote the further development of heterogeneous FL.},
  archive  = {J},
  author   = {Chuan Chen and Tianchi Liao and Xiaojun Deng and Zihou Wu and Sheng Huang and Zibin Zheng},
  doi      = {10.1109/TBDATA.2025.3527202},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1548-1567},
  title    = {Advances in robust federated learning: A survey with heterogeneity considerations},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of rumors and their sources in social networks: A comprehensive survey. <em>TBD</em>, <em>11</em>(3), 1528-1547. (<a href='https://doi.org/10.1109/TBDATA.2024.3522801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the recent advancements in social network platform technology, an overwhelming amount of information is spreading rapidly. In this situation, it can become increasingly difficult to discern what information is false or true. If false information proliferates significantly, it can lead to undesirable outcomes. Hence, when we receive some information, we can pose the following two questions: $(i)$ Is the information true? $(ii)$ If not, who initially spread that information? The first problem is the rumor detection issue, while the second is the rumor source detection problem. A rumor-detection problem involves identifying and mitigating false or misleading information spread via various communication channels, particularly online platforms and social media. Rumors can range from harmless ones to deliberately misleading content aimed at deceiving or manipulating audiences. Detecting misinformation is crucial for maintaining the integrity of information ecosystems and preventing harmful effects such as the spread of false beliefs, polarization, and even societal harm. Therefore, it is very important to quickly distinguish such misinformation while simultaneously finding its source to block it from spreading on the network. However, most of the existing surveys have analyzed these two issues separately. In this work, we first survey the existing research on the rumor-detection and rumor source detection problems with joint detection approaches, simultaneously. This survey deals with these two issues together so that their relationship can be observed and it provides how the two problems are similar and different. The limitations arising from the rumor detection, rumor source detection, and their combination problems are also explained, and some challenges to be addressed in future works are presented.},
  archive  = {J},
  author   = {Otabek Sattarov and Jaeyoung Choi},
  doi      = {10.1109/TBDATA.2024.3522801},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1528-1547},
  title    = {Detection of rumors and their sources in social networks: A comprehensive survey},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards enhancing inter-domain routing security with visualization and visual analytics. <em>TBD</em>, <em>11</em>(3), 1508-1527. (<a href='https://doi.org/10.1109/TBDATA.2024.3481899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the complex landscape of the Internet, inter-domain routing systems are essential for ensuring seamless connectivity and reachability across autonomous systems. However, the lack of dependable security validation mechanisms in these systems poses persistent challenges. Vulnerabilities such as prefix hijacking, path forgery, and route leakage not only compromise network operators and users, but also threaten the stability and accessibility of the Internet’s core infrastructure. To address this, visualization and visual analytics techniques are adept at identifying and detecting security threats, offering network administrators effective methods to monitor and maintain network operations. This paper presents a comprehensive survey of the state-of-the-art research in visualization and visual analytics for inter-domain routing security. We delineate four scenarios for tasks analysis in network visualization: monitoring, detection, verification, and discovery. Each category is explored in detail, focusing on the employed data sources and visualization techniques. Several key findings are presented at the end of each category, aimed at providing researchers and practitioners with research inspiration. Furthermore, we examine the trends of academic interest observed in recent decades and propose potential directions for future research in visual analytics pertaining to Internet infrastructure security.},
  archive  = {J},
  author   = {Jingwei Tang and Guodao Sun and Jiahui Chen and Gefei Zhang and Qi Jiang and Yanbiao Li and Guangxing Zhang and Jian Liu and Haixia Wang and Ronghua Liang},
  doi      = {10.1109/TBDATA.2024.3481899},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1508-1527},
  title    = {Towards enhancing inter-domain routing security with visualization and visual analytics},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep residual coupled prompt learning for zero-shot sketch-based image retrieval. <em>TBD</em>, <em>11</em>(3), 1493-1507. (<a href='https://doi.org/10.1109/TBDATA.2024.3481898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Zero-shot sketch-based image retrieval (ZS-SBIR) aims to utilize freehand sketches for retrieving natural images with similar semantics in realistic zero-shot scenarios. Existing works focus on zero-shot semantic transfer using category word embedding and leveraging teacher-student networks to alleviate catastrophic forgetting of pre-trained models. They aim to retain rich discriminative features to achieve zero-shot semantic transfer. However, the category word embedding method is insufficient in flexibility, thereby limiting their retrieval performances in ZS-SBIR scenarios. In addition, the teacher network used for generating guidance signals results in computational redundancy, requiring repeated processing of mini-batch inputs. To address these issues, we propose a deep residual coupled prompt learning (DRCPL) for ZS-SBIR. Specifically, we leverage the text encoder of CLIP to generate category classification weights, thereby improving the flexibility and generality of zero-shot semantic transfer. To tune text and vision representations effectively, we introduce learnable prompts at the input and freeze the parameters of the CLIP encoder. This approach not only effectively prevents catastrophic forgetting, but also significantly reduces the computational complexity of the model. We also introduce the text-vision prompt coupling function to enhance the coordinated consistency between the text and vision representations, ensuring that the two branches can train collaboratively. Finally, we gradually establish stage feature relationships by learning prompts independently at different early stages to facilitate rich contextual learning. Comprehensive experimental results demonstrate that our DRCPL method achieves state-of-the-art performance in ZS-SBIR tasks.},
  archive  = {J},
  author   = {Guangyao Zhuo and Zhenqiu Shu and Zhengtao Yu},
  doi      = {10.1109/TBDATA.2024.3481898},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1493-1507},
  title    = {Deep residual coupled prompt learning for zero-shot sketch-based image retrieval},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A knowledge-guided event-relation graph learning network for patient similarity with chinese electronic medical records. <em>TBD</em>, <em>11</em>(3), 1475-1492. (<a href='https://doi.org/10.1109/TBDATA.2024.3481955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Feature sparse problem is commonly existing in patient similarity calculation task with clinical data, to track which, some approaches have been proposed to use Graph Neural Network (GNN) to model the complex structural information in patient Electronic Medical Records (EMRs). These GNN based approaches usually treat medical concepts (i.e., symptoms, diseases) as nodes to learn spatial features and adopt Recurrent Neural Network (RNN) to learn temporal sequence of these concepts. However, in many cases, several sequential concepts contained in EMR text are considered as occur simultaneously in the clinical diagnosis (i.e., some symptoms are detected simultaneously by once test), learning temporal sequence of these sequential concepts might cause noise for patient similarity calculation. Furthermore, the limited discriminative capability of concepts cannot provide sufficient indicative information for similarity learning. To this end, we propose a Knowledge-guided Event-relation Graph Learning Network (KEGLN) for patient similarity calculation. Specifically, after event extraction, we first construct element-relation graphs and use the first Graph Convolutional Network (GCN) and Graph Attention Network (GAT) layer to aggregate features from each event and its involved elements for reducing the noise produced by temporal sequence of concepts. Meanwhile, the entity description and attribute-value structure are extracted to supplement background knowledge of elements (concepts and trigger words). For the updated event nodes, we then design a event-relation graph and adopt the second GCN and GAT layer to aggregate information from events and their directly neighbors to extract spatial features of events at the current moment. Finally, the Bidirectional Long Short-Term Memory (BiLSTM) model is adopted to learn temporal dependency of event nodes to capture the dynamic change of disease progress. Through diverse datasets and extensive experiments, our KEGLN model outperforms all baselines for Chinese patient similarity calculation.},
  archive  = {J},
  author   = {Zhichao Zhu and Jianqiang Li and Chun Xu and Jingchen Zou and Qing Zhao},
  doi      = {10.1109/TBDATA.2024.3481955},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1475-1492},
  title    = {A knowledge-guided event-relation graph learning network for patient similarity with chinese electronic medical records},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online news-based economic sentiment index. <em>TBD</em>, <em>11</em>(3), 1464-1474. (<a href='https://doi.org/10.1109/TBDATA.2024.3474211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The accurate prediction of industry trends has become increasingly challenging because of unforeseen events. To address this challenge, this study proposes a deep learning approach to generate an economic sentiment index by integrating Natural Language Processing (NLP) models and image-clustering techniques. We first employ sampling techniques to create standardized online news datasets. Feature engineering techniques from the Korean Bidirectional Encoder Representations from Transformers (KoBERT) model are then used to generate relevance and sentiment scores for the textual data. Further, to enhance visualization and clustering, we transform the textual data into joint plot images, which are grouped into distinct clusters based on news categories. Finally, using Multi-criteria Decision Analysis, the various scores and cluster information are synthesized to generate the final economic sentiment index. This approach improves visualization and enhances the interpretability of the generated index. The proposed algorithm is applied to construct a new economic sentiment index for the Information and Communications Technology (ICT) industry in South Korea.},
  archive  = {J},
  author   = {Nathaniel Kang and Dongeun Min and Yonghun Cho and Dong-Whan Ko and Hyun Hak Kim and Joon Yeon Choeh and Jongho Im},
  doi      = {10.1109/TBDATA.2024.3474211},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1464-1474},
  title    = {Online news-based economic sentiment index},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale data quality challenges, framework and evaluation in metro systems. <em>TBD</em>, <em>11</em>(3), 1447-1463. (<a href='https://doi.org/10.1109/TBDATA.2024.3474215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data quality is a fundamental challenge for downstream data mining tasks. While numerous studies have addressed data quality issues in various contexts, there is a notable lack of systematic research on data quality in metro systems. Metro systems generate a vast volume of multisource heterogeneous datasets daily, and many data mining tasks have been developed for operational and management purposes. Therefore, investigating data quality problems in metro systems is crucial. In this paper, we systematically explore data quality issues in metro systems. First, we present a comprehensive analysis method to examine data quality problems such as missing data, noise, and weak semantics. Second, we design five metrics to measure data quality and propose a set of quality improvement approaches. These approaches include a travel pattern-based missing value imputation method, a heuristic trajectory noise filtering method, and a data semantics enhancement method. Additionally, we develop an automated pipeline solution where the data quality enhancement algorithms are seamlessly integrated with the data processing pipeline. Finally, we provide a case study to illustrate the significant benefits of our data quality improvement methods. We conducted extensive experiments to validate our methods on a set of large-scale datasets collected from a metro system, which include, Wi-Fi signal data, and electronic fence data. The results indicate that 1) the proposed imputation method surpasses other baselines by 26.47% to 44.82%; 2) the proposed noise filtering method outperforms other baselines by an average of 12.22%; and 3) the proposed data semantics enrichment method exceeds the baseline method by 37.34% in terms of maximum accuracy.},
  archive  = {J},
  author   = {Tailan Yuan and Wen Xiong and Siyuan Liu},
  doi      = {10.1109/TBDATA.2024.3474215},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1447-1463},
  title    = {Large-scale data quality challenges, framework and evaluation in metro systems},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Verifiable and privacy-preserving $k$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math>-NN query scheme with multiple keys. <em>TBD</em>, <em>11</em>(3), 1434-1446. (<a href='https://doi.org/10.1109/TBDATA.2024.3463543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a basic primitive in spatial and multimedia databases, the $k$-nearest neighbors ($k$-NN) query has been widely used in electronic medicine, location-based services and so on. With the boom in cloud computing, it is currently a trend to upload massive data to the cloud server to enjoy its powerful storage and computing resources. Recently, research communities and commercial applications have proposed many schemes to support $k$-NN query on cloud data. However, most of the existing schemes were designed under the assumption that the query users (QUs) are fully trusted and hold the key of the data owner (DO). In this case, even if the queries were encrypted, the QUs can capture the query content from each other, leading to the query privacy leakage. Unfortunately, to the best of our knowledge, few $k$-NN query schemes can ensure data security and result verification under the key confidentiality condition. In this paper, we propose a verifiable and privacy-preserving $k$-NN query scheme with multiple keys (VP$k$NN), in which each QU's partial private key can only decrypt the encrypted query results belonging to its own, but not the encrypted database, the encrypted query data and query results of other QUs. Moreover, our proposal not only answers the query efficiently, but also ensures the privacy of the data, the query and the result, and the verification of the correctness of the results. Finally, the complexity and security are theoretically analyzed, and the practicality and efficiency of our proposed scheme are compared by simulation experiments.},
  archive  = {J},
  author   = {Yunzhen Zhang and Baocang Wang and Zhen Zhao},
  doi      = {10.1109/TBDATA.2024.3463543},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1434-1446},
  title    = {Verifiable and privacy-preserving $k$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math>-NN query scheme with multiple keys},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distilling fair representations from fair teachers. <em>TBD</em>, <em>11</em>(3), 1419-1433. (<a href='https://doi.org/10.1109/TBDATA.2024.3460532'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As an increasing number of data-driven deep learning models are deployed in our daily lives, the issue of algorithmic fairness has become a major concern. These models are trained on data that inevitably contains various biases, leading them to learn unfair representations that differ across demographic subgroups, resulting in unfair predictions. Previous work on fairness has attempted to remove subgroup information from learned features, aiming to contribute to similar representations across subgroups and lead to fairer predictions. However, identifying and removing this information is extremely challenging due to the “black box” nature of neural networks. Moreover, removing desired features without affecting other features is difficult, as features are often correlated, potentially harming model prediction performance. This paper aims to learn fair representations without degrading model prediction performance. We adopt knowledge distillation, allowing unfair models to learn fair representations directly from a fair teacher. The proposed method provides a novel approach to obtaining fair representations while maintaining valid prediction performance. We evaluate the proposed method, FairDistill, on four datasets (CIFAR-10, UTKFace, CelebA, and Adult) under diverse settings. Extensive experiments demonstrate the effectiveness and robustness of the proposed method.},
  archive  = {J},
  author   = {Huan Tian and Bo Liu and Tianqing Zhu and Wanlei Zhou and Philip S. Yu},
  doi      = {10.1109/TBDATA.2024.3460532},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1419-1433},
  title    = {Distilling fair representations from fair teachers},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developing novel algorithms for generating inexact data through triangle distribution. <em>TBD</em>, <em>11</em>(3), 1411-1418. (<a href='https://doi.org/10.1109/TBDATA.2024.3460529'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The manuscript introduces the mathematical representation of the neutrosophic triangular distribution, encompassing probability density functions and cumulative distribution functions. Two algorithms are introduced for the generation of random variates based on this distribution. Through simulations and a comparative examination with traditional statistical approaches, the research illustrates the versatility and resilience of the neutrosophic triangular distribution. The findings underscore its effectiveness in addressing uncertainty, particularly in scenarios with varying degrees of indeterminacy. The study emphasizes the substantial influence of uncertainty on the generation of random variates, with potential implications for decision-making and data analysis.},
  archive  = {J},
  author   = {Muhammad Aslam},
  doi      = {10.1109/TBDATA.2024.3460529},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1411-1418},
  title    = {Developing novel algorithms for generating inexact data through triangle distribution},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-tuning a biased model for improving fairness. <em>TBD</em>, <em>11</em>(3), 1397-1410. (<a href='https://doi.org/10.1109/TBDATA.2024.3460537'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fairness has emerged as a crucial concern in machine learning since biased models would generate dissimilar predictions for different groups, perpetuating social inequalities. Although numerous techniques have been proposed to address the fairness issue in machine learning, most rely on incorporating fairness constraints during the training phase, rendering them ineffective once the model is deployed. This paper explores the potential of fine-tuning biased models to enhance fairness, particularly suitable for scenarios where retraining the model is not feasible. Our approach is rooted in an empirical analysis of the distribution of bias within a biased model, and we fine-tune the model parameter in a limited scope so that the performance of the original model can be maintained. We first observe that fine-tuning a biased model leads to deviations from its initial state, with deep layers undergoing the most significant changes. We then design and apply a bias-discovery algorithm, revealing that bias predominantly resides in the model’s deep layers. Based on these observations, we propose a straightforward yet highly effective method for debiasing the model: fine-tuning the classification head. We conduct a thorough theoretical analysis to justify the proposed method and provide guidance for fine-tuning. Furthermore, we experimentally validate our method on tabular and image datasets using four networks (CNN, AlexNet, VGG-11, and ResNet-18).},
  archive  = {J},
  author   = {Huiqiang Chen and Tianqing Zhu and Bo Liu and Wanlei Zhou and Philip S. Yu},
  doi      = {10.1109/TBDATA.2024.3460537},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1397-1410},
  title    = {Fine-tuning a biased model for improving fairness},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph data model and graph query language based on the monadic second-order logic. <em>TBD</em>, <em>11</em>(3), 1381-1396. (<a href='https://doi.org/10.1109/TBDATA.2024.3455172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the wide application of graphs in various fields, graph query languages have attracted more and more attention. Existing graph query languages, such as GraphQL and SoQL, mostly have similar expressive power as the first-order logic or its extended versions, and are limited when used to express various queries. In this paper, since the graph data model is the base of the graph query language, we propose a new graph data model with the expressive power of monadic second-order logic (abbr. MSOL), and then present a more expressive SQL-like declarative graph query language named $SOGQL$ to support more common queries efficiently. Specifically, a new graph calculus is first proposed based on MSOL for attributed graphs. Then, the new graph data model is proposed. Its graph algebra, which operates on graph sets, has seven fundamental operators such as union, filter, map, and reduce. Next, the graph query language $SOGQL$ is proposed based on the graph data model. Since the graph algebra has the same expressive power as the graph calculus, $SOGQL$ has the expressive power of MSOL, and can express queries with constraints on subgraphs. Moreover, applied with $SOGQL$, a prototype system named $SOGDB$ is implemented. $SOGDB$ is applied with $SOGQL$, and the experimental results show its efficiency.},
  archive  = {J},
  author   = {Yunkai Lou and Chaokun Wang and Songyao Wang},
  doi      = {10.1109/TBDATA.2024.3455172},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1381-1396},
  title    = {Graph data model and graph query language based on the monadic second-order logic},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weak multi-label data stream classification under distribution changes in labels. <em>TBD</em>, <em>11</em>(3), 1369-1380. (<a href='https://doi.org/10.1109/TBDATA.2024.3453760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-label stream classification aims to address the challenge of dynamically assigning multiple labels to sequentially-arrived instances. In real situations, only partial labels of instances can be observed due to the expensive human annotations, and the problem of label distribution changes arises from multiple labels in a streaming mode, but few existing works jointly consider such challenges. Motivated by this, we propose the problem of weak multi-label stream classification (WMSC) and an online classification algorithm robust to weak labels. Specifically, we incrementally update the margin-based model using information from both the past model and the current incoming instance with partially observed labels. To increase the robustness to weak labels, we first adjust the classification margin of negative labels using the label causality matrix, which is constructed by the conditional probability of label pairs. Second, we introduce the label prototype matrix to regulate the margin by controlling the weighting parameter of the slack term. Additionally, to handle the potential distribution changes in labels, we utilize the instance-specific threshold via online thresholding to perform binary classification, which is formulated as a regression problem. Finally, theoretical analysis and empirical experimental results are presented to demonstrate the effectiveness of WMSC in classifying unobserved streaming instances.},
  archive  = {J},
  author   = {Yizhang Zou and Xuegang Hu and Peipei Li and Jun Hu},
  doi      = {10.1109/TBDATA.2024.3453760},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1369-1380},
  title    = {Weak multi-label data stream classification under distribution changes in labels},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AnesFormer: An end-to-end framework for EEG-based anesthetic state classification. <em>TBD</em>, <em>11</em>(3), 1357-1368. (<a href='https://doi.org/10.1109/TBDATA.2024.3489419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To determine the real-time changes in brain arousal introduced by anesthetics, Electroencephalogram (EEG) is often used as an objective neuroimaging evidence to link the neurobehavioral states of patients. However, EEG signals often suffer from a low signal-to-noise ratio due to environmental noise and artifacts, which limits its application for a reliable estimation of depth of anesthesia (DoA), especially under high cross-subject variability. In this study, we propose an end-to-end deep learning based framework, termed as AnesFormer, which contains a data selection model, a self-attention based classification model, and a baseline update mechanism. These three components are integrated in a dynamic and seamless manner to achieve the goal of improving the effectiveness and robustness of DoA estimation in a leave-one-out setting. In the experiment, we apply the proposed framework to an office-based dataset and a hospital-based dataset, and use seven existing models as benchmarks. In addition, we conduct an ablation experiment to show the significance of each component in AnesFormer. Our main results indicate that 1) the proposed framework generally performs better than the existing methods for DoA estimation in terms of effectiveness and robustness; 2) each designed component in AnesFormer is likely to contribute to the DoA classification improvement.},
  archive  = {J},
  author   = {Qihang Wang and Ying Chen and Qinge Xiao},
  doi      = {10.1109/TBDATA.2024.3489419},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1357-1368},
  title    = {AnesFormer: An end-to-end framework for EEG-based anesthetic state classification},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-granularity feature interaction and multi-region selection based triplet visual question answering. <em>TBD</em>, <em>11</em>(3), 1346-1356. (<a href='https://doi.org/10.1109/TBDATA.2024.3453750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurately locating the question-related regions in one given image is crucial for visual question answering (VQA). The current approaches suffer two limitations: (1) Dividing one image into multiple regions may lose parts of semantic information and original relationships between regions; (2) Choosing only one or all image regions to predict the answer may correspondingly result in the insufficiency or redundancy of information. Therefore, how to effectively mine the relationship between image regions and choose the relevant image regions are vital. In this paper, we propose a novel Multi-granularity feature interaction and Multi-region selection-based triplet VQA model (M2TVQA). To tackle the first limitation, we propose the multi-granularity feature interaction strategy that adaptively supplements the global coarse-granularity features with the regional fine-granularity features. To overcome the second limitation, we design the Top-$K$ learning strategy to adaptively select $K$ most relevant image regions to the question, even if the selected regions are far away in space. Such a strategy can select as many relevant image regions as possible and reduce introducing noise. Finally, we construct the multi-modality triplet to predict the answer of VQA. Extended experiments on two public outside knowledge datasets OK-VQA and KRVQA verify the effectiveness of the proposed model.},
  archive  = {J},
  author   = {Heng Liu and Boyue Wang and Yanfeng Sun and Junbin Gao and Xiaoyan Li and Yongli Hu and Baocai Yin},
  doi      = {10.1109/TBDATA.2024.3453750},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1346-1356},
  title    = {Multi-granularity feature interaction and multi-region selection based triplet visual question answering},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PosParser: A heuristic online log parsing method based on part-of-speech tagging. <em>TBD</em>, <em>11</em>(3), 1334-1345. (<a href='https://doi.org/10.1109/TBDATA.2024.3453756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Log parsing, the process of transforming raw logs into structured data, is a key step in the complex computer system's intelligent operation and maintenance and therefore has received extensive attention. Among all log parsing methods, heuristic log parsing methods are lightweight and can work in a streaming mode to well meet the real-time parsing requirements. However, the existing log representations used in the heuristic log parsing methods are not powerful in distinguishing log messages, which leads to low parsing accuracy and weak generality. Inspired by trigger word extraction of the event detection task in natural language processing (NLP), this paper proposes an online log parser, named PosParser, which employs the part-of-speech (PoS) tagging to extract a function token sequence (FTS) as the log message representation, and then identify event templates of log messages through the FTS. Experimental results on sixteen logs from real systems demonstrate that the FTS is powerful in distinguishing log messages from different event templates, and PosParser not only performs better in terms of parsing accuracy than state-of-the-art methods but is also comparable to them in efficiency.},
  archive  = {J},
  author   = {Jinzhao Jiang and Yuanyuan Fu and Jian Xu},
  doi      = {10.1109/TBDATA.2024.3453756},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1334-1345},
  title    = {PosParser: A heuristic online log parsing method based on part-of-speech tagging},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view few-shot reasoning for emerging entities in knowledge graphs. <em>TBD</em>, <em>11</em>(3), 1321-1333. (<a href='https://doi.org/10.1109/TBDATA.2024.3453749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A knowledge graph (KG) is a form of representing knowledge of the objective world. With the expansion of knowledge, KGs frequently incorporate new entities, which often possess limited associated data, known as few-shot features. Addressing the missing knowledge for these emerging entities is crucial practically, but there are significant challenges due to data scarcity. Previously developed methods based on knowledge graph embedding (KGE) and graph neural networks (GNNs) focusing on instance-level KGs are confronted with challenges of data scarcity and model simplicity, rendering them inapplicable to reasoning tasks in few-shot scenarios. To tackle these issues, we propose a multi-view few-shot KG reasoning method for emerging entities. The primary focus of our method lies in resolving the problem of link prediction for emerging entities with limited associated triples from multiple perspectives. Distinct from previous methods, our approach initially abstracts a concept-view KG from the conventional instance-view KG, enabling the formulation of commonsense rules. Additionally, we employ the aggregation of multi-hop subgraph features to enhance the representation of emerging entities. Furthermore, we introduce a more efficient cross-domain negative sampling strategy and a multi-view triple scoring function based on commonsense rules. Our experimental evaluations highlight the effectiveness of our method in few-shot contexts, demonstrating its robustness and adaptability in both cross-shot and zero-shot scenarios, significantly outperforming existing models in these challenging settings.},
  archive  = {J},
  author   = {Cheng Yan and Feng Zhao and Xiaohui Tao and Xiaofeng Zhu},
  doi      = {10.1109/TBDATA.2024.3453749},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1321-1333},
  title    = {Multi-view few-shot reasoning for emerging entities in knowledge graphs},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mixup virtual adversarial training for robust vision transformers. <em>TBD</em>, <em>11</em>(3), 1309-1320. (<a href='https://doi.org/10.1109/TBDATA.2024.3453754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Inspired by the success of transformers in natural language processing, vision transformers have been proposed to address a wide range of computer vision tasks, such as image classification, object detection and image segmentation, and they have achieved very promising performance. However, the robustness of vision transformers has been relatively under-explored. Recent studies have revealed that pre-trained vision transformers are also vulnerable to white-box adversarial attacks on the downstream image classification tasks. The adversarial attacks (e.g., FGSM and PGD) designed for convolutional neural networks (CNNs) can also cause severe performance drop for vision transformers. In this paper, we evaluate the robustness of vision transformers fine-tuned with the off-the-shelf methods under adversarial attacks on CIFAR-10 and CIFAR-100 and further propose a data-augmented virtual adversarial training approach called MixVAT, which is able to enhance the robustness of pre-trained vision transformers against adversarial attacks on the downstream tasks with the unlabelled data. Extensive results on multiple datasets demonstrate the superiority of our approach over baselines on adversarial robustness, without compromising generalization ability of the model.},
  archive  = {J},
  author   = {Weili Shi and Sheng Li},
  doi      = {10.1109/TBDATA.2024.3453754},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1309-1320},
  title    = {Mixup virtual adversarial training for robust vision transformers},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Debiased cross contrastive quantization for unsupervised image retrieval. <em>TBD</em>, <em>11</em>(3), 1298-1308. (<a href='https://doi.org/10.1109/TBDATA.2024.3453751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Contrastive quantization (applying vector quantization to contrastive learning) has achieved great success in large-scale image retrieval because of its advantage of high computational efficiency and small storage space. This article designs a novel optimization framework to simultaneously optimize the cross quantization and the debiased contrastive learning, termed Debiased Cross Contrastive Quantization (DCCQ). The proposed framework is implemented in an end-to-end network, resulting in both reduced quantization error and deletion of many false negative samples. Specifically, to increase the distinguishability between codewords, DCCQ introduces the codeword similarity loss and soft quantization entropy loss for network training. Furthermore, the memory bank strategy and multi-crop image augmentation strategy are employed to promote the effectiveness and efficiency of contrastive learning. Extensive experiments on three large-scale real image benchmark datasets show that the proposed DCCQ yields state-of-the-art results.},
  archive  = {J},
  author   = {Zipeng Chen and Yuan-Gen Wang and Lin-Cheng Li},
  doi      = {10.1109/TBDATA.2024.3453751},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1298-1308},
  title    = {Debiased cross contrastive quantization for unsupervised image retrieval},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALAD: A new unsupervised time series anomaly detection paradigm based on activation learning. <em>TBD</em>, <em>11</em>(3), 1285-1297. (<a href='https://doi.org/10.1109/TBDATA.2024.3453762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Time series anomaly detection has been received growing interest in industrial and academic communities due to its substantial theoretical value and practical significance in reality. Recent advanced methods for time series anomaly detection are based on deep learning techniques, since they have shown their superiority in some specific situations. However, most existing deep learning-based anomaly detection methods require predefined, specific tasks of reconstruction or prediction, necessitating task-specific loss functions. Designing such anomaly-aware loss functions poses a significant challenge due to the ambiguity in defining ground-truth anomalies. Moreover, these methods often rely on complex network architectures that tend to lead to over-generalization, resulting in even abnormal data being well reconstructed or fitted. To mitigate this situation, grounded in activation learning theory, we propose a novel unsupervised time series anomaly detection paradigm termed ALAD. ALAD utilizes a straightforward fully connected network architecture, measuring the typicality of input patterns through the sum of the squared output. Despite its simplicity, ALAD achieves competitive performance compared to state-of-the-art models trained using backpropagation. By utilizing various real-world and synthetic datasets, experimental results have confirmed the effectiveness and feasibility of the proposed paradigm. This work also demonstrates that biologically-plausible local learning can sometimes outperform backpropagation in real-world scenarios.},
  archive  = {J},
  author   = {Fengqian Ding and Bo Li and Xianye Ben and Jia Zhao and Hongchao Zhou},
  doi      = {10.1109/TBDATA.2024.3453762},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1285-1297},
  title    = {ALAD: A new unsupervised time series anomaly detection paradigm based on activation learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DistillSleepNet: Heterogeneous multi-level knowledge distillation via teacher assistant for sleep staging. <em>TBD</em>, <em>11</em>(3), 1273-1284. (<a href='https://doi.org/10.1109/TBDATA.2024.3453763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate sleep staging is crucial for the diagnosis of diseases such as sleep disorders. Existing sleep staging models with excellent performance are usually large and require a lot of computational resources, limiting their application on wearable devices. Therefore, it is a key issue to distil the knowledge embedded in large models into small heterogeneous models for better deployment. In the process of knowledge distillation of heterogeneous models for sleep electroencephalography (EEG) signals, we mainly deal with three major challenges: 1) There are large structural differences between heterogeneous sleep staging models; 2) What kind of knowledge should be conveyed in sleep EEG signals in the knowledge distillation of heterogeneous models; 3) Significant scale differences exist between heterogeneous models. To address these challenges, we design a generic heterogeneous model knowledge distillation framework for sleep staging. Specifically, we first propose a knowledge distillation strategy for heterogeneous models that addresses the large structural differences between heterogeneous models. Then, a multi-level knowledge distillation module is designed to effectively transfer important multi-level feature knowledge. In addition, the teacher assistant module is introduced to ease the scale difference between the heterogeneous models which further enhances the knowledge distillation performance. Experimental results on both Sleep-EDF and ISRUC datasets show that our distillation framework achieves state-of-the-art performance.},
  archive  = {J},
  author   = {Ziyu Jia and Heng Liang and Yucheng Liu and Haichao Wang and Tianzi Jiang},
  doi      = {10.1109/TBDATA.2024.3453763},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1273-1284},
  title    = {DistillSleepNet: Heterogeneous multi-level knowledge distillation via teacher assistant for sleep staging},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient forward and backward private conjunctive searchable encryption with comprehensive verification. <em>TBD</em>, <em>11</em>(3), 1259-1272. (<a href='https://doi.org/10.1109/TBDATA.2024.3442540'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Dynamic searchable symmetric encryption (DSSE) enables the retrieval and update of massive encrypted data and is thus widely applied in cloud storage. Malicious cloud servers may tamper with outsourced data or return incorrect search results, making verification indispensable. However, current verifiable conjunctive DSSE with forward and backward privacy cannot simultaneously achieve accurate search and empty result verification. Given the above problems, we propose a novel forward and backward private conjunctive DSSE with comprehensive verification called VCFB. VCFB introduces the notion of random blinding factors and secure dynamic cross-tags to achieve accurate conjunctive search with sublinear overhead. We design the search state and construct chain structures to ensure forward security. The new verification algorithm based on bilinear-map dynamic accumulators can guarantee the verifiability of search results even if an empty result is returned. We use a sample check method to verify the new dynamic cross-tags, reducing computation costs. We precisely define the leakage function of VCFB and give detailed security proof, demonstrating that VCFB is forward and backward private under the malicious server model. Experimental results show that VCFB ensures efficient and accurate conjunctive search, outperforming the similar verifiable conjunctive DSSE scheme regarding indexing storage and update performance.},
  archive  = {J},
  author   = {Yue Ge and Ying Gao and Lin Qi and Jiankai Qiu},
  doi      = {10.1109/TBDATA.2024.3442540},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1259-1272},
  title    = {Efficient forward and backward private conjunctive searchable encryption with comprehensive verification},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MMFFHS: Multi-modal feature fusion for hate speech detection on social media. <em>TBD</em>, <em>11</em>(3), 1247-1258. (<a href='https://doi.org/10.1109/TBDATA.2024.3445372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Millions of people use social media platforms such as Facebook, YouTube, and Twitter to stay updated on news, enjoy entertainment, and share personal moments with peers. These platforms have now become medium channels for spreading rumors, posting hate speech, cyberbullying, etc. Hate speech frequently appears on social media platforms nowadays. Sometimes, it impairs readers’ mental and emotional health and societal order. Therefore, timely detection is required to prevent the spread of hate speech posts on social media platforms. The researchers have reported some research works on textual hate speech detection. However, social media posts are not limited to text; images and text with images are also used in the posts, termed multimodal data. The text-based model may not be efficient enough to handle the multimodal data. Therefore, this study introduces a reliable architecture that utilizes deep and transfer learning frameworks to classify multimodal social media posts into hate and non-hate. The proposed model is compatible with text, images, and images with text-based social posts to categorize hate and non-hate. The proposed framework MMFFHS, a feature-fusion-based model, performed better than the existing models by achieving 70.26% accuracy.},
  archive  = {J},
  author   = {Pradeep Kumar Roy},
  doi      = {10.1109/TBDATA.2024.3445372},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1247-1258},
  title    = {MMFFHS: Multi-modal feature fusion for hate speech detection on social media},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring new frontiers in agricultural NLP: Investigating the potential of large language models for food applications. <em>TBD</em>, <em>11</em>(3), 1235-1246. (<a href='https://doi.org/10.1109/TBDATA.2024.3442542'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper explores new frontiers in agricultural natural language processing (NLP) by investigating the effectiveness of food-related text corpora for pretraining transformer-based language models. Specifically, we focus on semantic matching, establishing mappings between food descriptions and nutrition data through fine-tuning AgriBERT with the FoodOn ontology. Our work introduces an expanded comparison with state-of-the-art language models such as GPT-4, Mistral-large, Claude 3 Sonnet, and Gemini 1.0 Ultra. This exploratory investigation, rather than a direct comparison, aims to understand how AgriBERT, a domain-specific, fine-tuned, open-source model, complements the broad knowledge and generative abilities of these advanced LLMs in addressing the unique challenges of the agricultural sector. We also experiment with other applications, such as cuisine prediction from ingredients, expanding our research to include various NLP tasks beyond semantic matching. Overall, this paper underscores the potential of integrating domain-specific models like AgriBERT with advanced LLMs to enhance the performance and applicability of agricultural NLP applications.},
  archive  = {J},
  author   = {Saed Rezayi and Zhengliang Liu and Zihao Wu and Chandra Dhakal and Bao Ge and Haixing Dai and Gengchen Mai and Ninghao Liu and Chen Zhen and Tianming Liu and Sheng Li},
  doi      = {10.1109/TBDATA.2024.3442542},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1235-1246},
  title    = {Exploring new frontiers in agricultural NLP: Investigating the potential of large language models for food applications},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Domain adaptation for label distribution learning. <em>TBD</em>, <em>11</em>(3), 1221-1234. (<a href='https://doi.org/10.1109/TBDATA.2024.3442562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Label distribution learning (LDL) suffers from the dilemma of insufficient target data in real-world applications, while domain adaptation (DA) seems to be able to provide a solution. However, most existing methods of DA, assuming that the instances can correspond to the explicit class information, are devoted only to classification but not to LDL. We argue that indiscriminately applying such DA methods might cause performance degradation in LDL tasks. In this paper, we propose LDL-DA, a novel algorithm dedicated to supervised domain adaptation for label distribution learning, which jointly learns a shared encoding representation from two aspects: 1) contrastive alignment of scarce supervised target data, and 2) minimizing the distance between prototypes of the same label combination. Experiments show that LDL-DA outperforms existing DA methods adapted to LDL, and provides early positive results in DA for LDL. To the best of our knowledge, this paper is the first research on DA for LDL.},
  archive  = {J},
  author   = {Haitao Wu and Weiwei Li and Xiuyi Jia},
  doi      = {10.1109/TBDATA.2024.3442562},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1221-1234},
  title    = {Domain adaptation for label distribution learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geometric algebra multi-order graph neural network for traffic prediction. <em>TBD</em>, <em>11</em>(3), 1206-1220. (<a href='https://doi.org/10.1109/TBDATA.2024.3442533'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate traffic prediction is crucial for urban traffic management. Spatial-temporal graph neural networks, which combine graph neural networks with time series processing, have been extensively employed in traffic prediction. However, traditional graph neural networks only capture pairwise spatial relationships between road network nodes, neglecting high-order interactions among multiple nodes. Meanwhile, most work for extracting temporal dependencies suffers from implicit modeling and overlooks the internal and external dependencies of time series. To address these challenges, we propose a Geometric Algebraic Multi-order Graph Neural Network (GA-MGNN). Specifically, in the temporal dimension, we design a convolution kernel based on the rotation matrix of geometric algebra, which not only learns internal dependencies between different time steps in time series but also external dependencies between time series and convolution kernels. In the spatial dimension, we construct a tokenized hypergraph and integrate dynamic graph convolution with attention hypergraph convolution to comprehensively capture multi-order spatial dependencies. Additionally, we design a segmented loss function based on traffic periodic information to further improve prediction accuracy. Extensive experiments on seven real-world datasets demonstrate that GA-MGNN outperforms state-of-the-art baselines.},
  archive  = {J},
  author   = {Di Zang and Zhe Cui and Zengqiang Wang and Juntao Lei and Yongjie Ding and Chenguang Wei and Junqi Zhang},
  doi      = {10.1109/TBDATA.2024.3442533},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1206-1220},
  title    = {Geometric algebra multi-order graph neural network for traffic prediction},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moving conditional GAN close to data: Synthetic tabular data generation and its experimental evaluation. <em>TBD</em>, <em>11</em>(3), 1188-1205. (<a href='https://doi.org/10.1109/TBDATA.2024.3442534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, data has ousted oil as the most economical resource in the world, but most companies are reluctant to share customer/user data in pure form and on a large scale due to privacy concerns. Many innovative technologies (e.g., federated learning, split learning) are employed to meet the growing demand for privacy preservation. Despite these technologies, acquiring personal data in order to optimize utility, and then sharing it on a large scale, is still very challenging. Thanks to the rapid development of artificial intelligence (AI), a relatively new and promising solution to resolve these challenges is to generate synthetic data (SD) by mirroring the original dataset’s properties. SD is a promising solution to address growing privacy demands as well as the utility/analytics requirements of many industry stakeholders. In this paper, we propose and implement an SD generation method from a real dataset containing both numerical and categorical attributes by using an improved conditional generative adversarial network (CGAN), and we quantify the feasibility of SD on technical and theoretical grounds. We provide a detailed analysis of SD in original and anonymized forms with the help of multiple use cases, whereas prior research simply assumed that privacy issues in SD are small because AI models do not overfit or SD has a poor connection with real data. We provide insights into the characteristics of SD (distributions, value frequencies, correlations, etc.) produced by the CGAN in relation to the real data. To the best of our knowledge, this is the pioneering work that provides an experiment-based analysis of the quality, privacy, and utility of SD in relation to a real benchmark dataset.},
  archive  = {J},
  author   = {Abdul Majeed and Seong Oun Hwang},
  doi      = {10.1109/TBDATA.2024.3442534},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1188-1205},
  title    = {Moving conditional GAN close to data: Synthetic tabular data generation and its experimental evaluation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can rumor detection enhance fact verification? unraveling cross-task synergies between rumor detection and fact verification. <em>TBD</em>, <em>11</em>(3), 1171-1187. (<a href='https://doi.org/10.1109/TBDATA.2024.3442555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, rumor detection (fake news detection) has seen a surge in research interest, and fact verification (fake news checking) has simultaneously become a significant research aspect. Despite the inherent distinction between fact verification and rumor detection – the former being a three-category task and the latter a binary one – there has yet to be in-depth exploration into the synergies between these two tasks. Furthermore, given the severe scarcity and the time-consuming and costly construction nature of fact verification datasets, few-shot/zero-shot fact verification methods are particularly favored. To tackle these challenges, we conduct a series of studies around “How can rumor detection enhance few-shot fact verification, and to what extent?”. Specifically, we systematically investigate the knowledge transferability between the two tasks, proposing a framework, Det2Ver, that is applicable to both rumor detection and fact verification. Through the construction of adaptive prompt templates and prompt-tuned LLMs like T5, Det2Ver structural-level synchronizes the two tasks and utilizes the external knowledge from rumor detection to reinforce fact verification task. We demonstrate the significance and effectiveness of Det2Ver. Through the few-shot/zero-shot experiments on three widely-used datasets, compared to other LLMs prompt-tuning baselines, the Det2Ver for cross-task knowledge augmentation brings a significant improvement in macro-F1 for fact verification.},
  archive  = {J},
  author   = {Weiqiang Jin and Mengying Jiang and Tao Tao and Hao Zhou and Xiaotian Wang and Biao Zhao and Guang Yang},
  doi      = {10.1109/TBDATA.2024.3442555},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1171-1187},
  title    = {Can rumor detection enhance fact verification? unraveling cross-task synergies between rumor detection and fact verification},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive federated learning framework for diabetic retinopathy grading and lesion segmentation. <em>TBD</em>, <em>11</em>(3), 1158-1170. (<a href='https://doi.org/10.1109/TBDATA.2024.3442548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Diabetic retinopathy (DR) is a debilitating ocular complication demanding timely intervention and treatment. The rapid evolution of deep learning (DL) has notably enhanced the efficiency of conventional manual diagnosis. However, the scarcity of existing DR datasets hinders the progress of data-driven DL models, especially for pixel-level lesion annotation datasets, which severely impedes the advancement of DR lesion segmentation tasks required for precise interpretations of DR grading. Furthermore, the escalating concerns surrounding medical data security and privacy induce data collection challenges for traditional centralized learning, exacerbating the issue of data silos. Federated learning (FL) emerges as a privacy-preserving distributed learning paradigm. Nevertheless, the existing literature lacks a comprehensive FL framework for DR diagnosis and fails to exploit multiple diverse DR datasets simultaneously. To address the challenges of data scarcity and privacy, we construct a high-quality pixel-level DR lesion annotation dataset (TJDR) and propose a novel FL-based DR diagnosis framework including both DR grading and multi-lesion segmentation. Moreover, to tackle the scarcity of pixel-level DR lesion datasets, we propose $\bm {\alpha }$-Fed and adaptive-$\bm {\alpha }$-Fed, two efficient cross-dataset FL algorithms. Extensive experiments demonstrate the effectiveness of our proposed framework and the two cross-dataset FL algorithms.},
  archive  = {J},
  author   = {Jingxin Mao and Xiaoyu Ma and Yanlong Bi and Rongqing Zhang},
  doi      = {10.1109/TBDATA.2024.3442548},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1158-1170},
  title    = {A comprehensive federated learning framework for diabetic retinopathy grading and lesion segmentation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient multi-view heterogeneous hypergraph convolutional network for heterogeneous information network representation learning. <em>TBD</em>, <em>11</em>(3), 1144-1157. (<a href='https://doi.org/10.1109/TBDATA.2024.3442549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Heterogeneous hypergraph neural networks are powerful tools to capture complex correlations among various nodes in Heterogeneous Information Networks (HINs). Despite satisfied performances of them, they are still plagued by the following problems: 1) They cannot capture the correlations in structural and semantic view at once, leading to topological information loss. 2) Due to the number of nodes being greater than the number of node types, node-level self-attention they used causes massive parameters and leads to high time consumption. 3) Interactions in meta-paths may be redundant, resulting in the correlations bias. To address the three issues, we propose an efficient Multi-View Heterogeneous Hypergraph Convolutional Network (MVH $^{2}$ GCN). It first constructs relational and semantic hypergraphs based on different types of edges and meta-paths respectively, to represent the complex correlations in structural view and semantic view. Meanwhile, the clean semantic hypergraphs are generated by structure learning network to avoid redundancy. Then, an efficient hypergraph convolutional network is designed to learn node embeddings. By doing so, correlations in the two views are captured. Finally, the learned node embeddings from two views are aggregated via a gated embedding fusion module for downstream tasks. Experiment results demonstrate that MVH $^{2}$ GCN is effective and efficient.},
  archive  = {J},
  author   = {Rui Bing and Guan Yuan and Yanmei Zhang and Senzhang Wang and Bohan Li and Yong Zhou},
  doi      = {10.1109/TBDATA.2024.3442549},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1144-1157},
  title    = {An efficient multi-view heterogeneous hypergraph convolutional network for heterogeneous information network representation learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-free knowledge filtering and distillation in federated learning. <em>TBD</em>, <em>11</em>(3), 1128-1143. (<a href='https://doi.org/10.1109/TBDATA.2024.3442551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In federated learning (FL), multiple parties collaborate to train a global model by aggregating their local models while keeping private training sets isolated. One problem hindering effective model aggregation is data heterogeneity. Federated ensemble distillation tackles this problem by using fused local-model knowledge to train the global model rather than directly averaging model parameters. However, most existing methods fuse all knowledge indiscriminately, which makes the global model inherit some data-heterogeneity-caused flaws from local models. While knowledge filtering is a potential coping method, its implementation in FL is challenging due to the lack of public data for knowledge validation. To address this issue, we propose a novel data-free approach (FedKFD) that synthesizes credible labeled data to support knowledge filtering and distillation. Specifically, we construct a prediction capability description to characterize the samples where a local model makes correct predictions. FedKFD explores the intersection of local-model-input space and prediction capability descriptions with a conditional generator to synthesize consensus-labeled proxy data. With these labeled data, we filter for relevant local-model knowledge and further train a robust global model through distillation. The theoretical analysis and extensive experiments demonstrate that our approach achieves improved generalization, superior performance, and compatibility with other FL efforts.},
  archive  = {J},
  author   = {Zihao Lu and Junli Wang and Changjun Jiang},
  doi      = {10.1109/TBDATA.2024.3442551},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1128-1143},
  title    = {Data-free knowledge filtering and distillation in federated learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Where is the next step? predicting the scientific impact of research career. <em>TBD</em>, <em>11</em>(3), 1116-1127. (<a href='https://doi.org/10.1109/TBDATA.2024.3442550'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Predicting the scientific impact of research scholars is increasingly crucial for career planning, particularly for young scholars considering career transitions. However, predicting a scholar's future development, especially after they move to a different academic group, presents significant challenges. To tackle this issue, we propose a Future Publication Impact Prediction Network (FPIPN) based on graph neural networks. FPIPN leverages rich information from a heterogeneous academic graph for impact prediction. We employ a hierarchical attention mechanism to learn the significance of graph information and utilize a knowledge distillation strategy to assess future impact based on historical records. Extensive experiments on a real-world academic dataset showcase the effectiveness of our approach compared to state-of-the-art methods.},
  archive  = {J},
  author   = {Hefu Zhang and Yong Ge and Yan Zhuang and Enhong Chen},
  doi      = {10.1109/TBDATA.2024.3442550},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1116-1127},
  title    = {Where is the next step? predicting the scientific impact of research career},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Restoration of recaptured screen images with a divide and conquer strategy. <em>TBD</em>, <em>11</em>(3), 1103-1115. (<a href='https://doi.org/10.1109/TBDATA.2024.3442538'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Moiré patterns in recaptured screen images are image defects that can affect image quality to an extreme extent. Different from other image defects, moiré artefacts can vary greatly in scales, colours and shapes. Such moiré patterns mix with image content and disturb image features of different scales in different ways, making moiré pattern removal a challenging task. In this paper, we present a novel divide-and-conquer strategy to solve the problem. In the divide stage, we innovatively decompose images into different layers, as well as into structure components and detail components. Then in the conquer stage, guided by the layers retrieved from the divide stage, we can restore coarse and fine image components independently, which greatly improve the demoiréing performance. Our strategy outperforms state-of-arts in both quantitative and qualitative evaluations.},
  archive  = {J},
  author   = {Yujing Sun and Hao Xiong and Siu Ming Yiu},
  doi      = {10.1109/TBDATA.2024.3442538},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1103-1115},
  title    = {Restoration of recaptured screen images with a divide and conquer strategy},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified representation learning for discrete attribute enhanced completely cold-start recommendation. <em>TBD</em>, <em>11</em>(3), 1091-1102. (<a href='https://doi.org/10.1109/TBDATA.2024.3387276'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recommender systems face a daunting challenge when entities (users or items) without any historical interactions, known as the “Completely Cold-Start Problem”. Due to the absence of collaborative signals, Collaborative Filtering (CF) schema fails to deduce user preferences or item characteristics for such cold entities. A common solution is incorporating auxiliary discrete attributes as the bridge to spread collaborative signals to cold entities. Most previous works involve embedding collaborative signals and discrete attributes into different spaces before aligning them for information propagation. Nevertheless, we argue that the separate embedding approach disregards potential high-order similarities between two signals. Furthermore, existing alignment modules typically narrow the geometric-based distance, lacking in-depth exploration of semantic overlap between collaborative signals and cold entities. In this paper, we propose a novel discrete attribute-enhanced completely cold-start recommendation framework, which aims to improve recommendation performance by modeling heterogeneous signals in a unified space. Specifically, we first construct a heterogeneous user-item-attribute graph and capture high-order similarities between heterogeneous signals in a graph-based message-passing manner. To achieve better information alignment, we propose two self-supervised alignment modules from the semantic mutual information and user-item preference perspective. Extensive experiments on six real-world datasets in two types of discrete attribute scenarios consistently verify the effectiveness of our framework.},
  archive  = {J},
  author   = {Haoyue Bai and Min Hou and Le Wu and Yonghui Yang and Kun Zhang and Richang Hong and Meng Wang},
  doi      = {10.1109/TBDATA.2024.3387276},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1091-1102},
  title    = {Unified representation learning for discrete attribute enhanced completely cold-start recommendation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-directional transfer graph contrastive learning for social recommendation. <em>TBD</em>, <em>11</em>(3), 1078-1090. (<a href='https://doi.org/10.1109/TBDATA.2024.3387340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph Neural Networks (GNNs) have emerged as an effective approach for social recommender systems. GNNs excel at capturing the graph-structured semantic information within the collaborative interaction graph and social networks. Recently, some methods have introduced self-supervised learning to GNNs, aiming to enhance recommendation performance by mitigating the data sparsity issue. However, these methods treat the interaction graph and social network as separate entities, which severely limits the number of samples available for self-supervised learning. Moreover, these separated methods also exacerbate the problem of information islands in collaborative and social domains, resulting in suboptimal performance. To tackle these challenges, we propose an innovative self-supervised social recommendation method called Bi-directional Transfer Graph Contrastive Learning (BTGCL). BTGCL jointly encodes node representations within both collaborative domain and social domain, then generates node views through feature augmentation. To bridge the information gap between domains, we devise a bi-directional migration mechanism that aligns features from the collaborative and social domains of the same positive pair. Through extensive experiments conducted on three publicly available datasets, we demonstrate the effectiveness of our proposed method in enhancing social recommendation performance.},
  archive  = {J},
  author   = {Lei Sang and Mingyuan Liu and Yi Zhang and Yuee Huang and Yiwen Zhang},
  doi      = {10.1109/TBDATA.2024.3387340},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1078-1090},
  title    = {Bi-directional transfer graph contrastive learning for social recommendation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). X-ECON: Explainable event-consumption commonsense reasoning. <em>TBD</em>, <em>11</em>(3), 1066-1077. (<a href='https://doi.org/10.1109/TBDATA.2023.3348999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This article investigates a novel commonsense inference task that comprises reasoning about the subsequent events chains and probable consumption intents, given an event described in brief free-form text. For instance, given the event “fall in love”, the system anticipates ensuing events such as “get married”$\rightarrow$“be pregnant”$\rightarrow$“have a baby”. Concurrently, the system predicts the likely consumption intents of the event participants, such as purchasing “jewelry”, “maternity clothing”, and “baby food”. The event chains generated in this process provide explicit, meaningful explanations, aiding the understanding of inferred consumption intents. To facilitate this study, we construct a new crowdsourced corpus: x-ECON. This corpus comprises 10,144 event chains and 150 event-related product categories, offering a wide range of everyday consumer events and situations. We introduce a baseline reasoning framework that not only infers consumption intent but also generates an event chain to explain its inference. Our experimental results suggest that involving the event chain reasoning in the event-consumption reasoning system can help improve the neural networks’ reason about the probable consumption intents of the event participants. Additionally, our method demonstrates the applicability of our approach in improving the performance of recommendation systems, by highlighting the role of explainable commonsense inference on the consumption intent. We also evaluated the performance of ChatGPT on our x-ECON dataset, showing that explainable event-consumption commonsense reasoning is a challenging task for large language models.},
  archive  = {J},
  author   = {Xiao Ding and Bibo Cai and Xue Li and Ting Liu and Bing Qin},
  doi      = {10.1109/TBDATA.2023.3348999},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1066-1077},
  title    = {X-ECON: Explainable event-consumption commonsense reasoning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HyFit: Hybrid fine-tuning with diverse sampling for abstractive summarization. <em>TBD</em>, <em>11</em>(3), 1054-1065. (<a href='https://doi.org/10.1109/TBDATA.2024.3387311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Abstractive summarization has made significant progress in recent years, which aims to generate a concise and coherent summary that contains the most important facts from the source document. Current fine-tuning approaches based on pre-training models typically rely on autoregressive and maximum likelihood estimation, which may result in inconsistent historical distributions generated during the training and inference stages, i.e., exposure bias problem. To alleviate this problem, we propose a hybrid fine-tuning model(HyFit), which combines contrastive learning and reinforcement learning in a diverse sampling space. Firstly, we introduce reparameterization and probability-based sampling methods to generate a set of summary candidates called candidates bank, which improves the diversity and quality of the decoding sampling space and incorporates the potential for uncertainty. Secondly, hybrid fine-tuning with sampled candidates bank, upweighting confident summaries and downweighting unconfident ones. Experiments demonstrate that HyFit significantly outperforms the state-of-the-art models on SAMSum and DialogSum. HyFit also shows good performance on low-resource summarization, on DialogSum dataset, using only approximate 8% of the examples exceed the performance of the base model trained on all examples.},
  archive  = {J},
  author   = {Shu Zhao and Yuanfang Cheng and Yanping Zhang and Jie Chen and Zhen Duan and Yang Sun and Xinyuan Wang},
  doi      = {10.1109/TBDATA.2024.3387311},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1054-1065},
  title    = {HyFit: Hybrid fine-tuning with diverse sampling for abstractive summarization},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). To write or not to write as a machine? that’s the question. <em>TBD</em>, <em>11</em>(3), 1042-1053. (<a href='https://doi.org/10.1109/TBDATA.2025.3536938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Considering the potential of tools such as ChatGPT or Gemini to generate texts in a similar way to a human would do, having reliable detectors of AI –AI-generated content (AIGC)– is vital to combat the misuse and the surrounding negative consequences of those tools. Most research on AIGC detection has focused on the English language, often overlooking other languages that also have tools capable of generating human-like texts, such is the case of the Spanish language. This paper proposes a novel multilingual and multi-task approach for detecting machine versus human-generated text. The first task classifies whether a text is written by a machine or by a human, which is the research objective of this paper. The second task consists in detect the language of the text. To evaluate the results of our approach, this study has framed the scope of the AuTexTification shared task and also we have collected a different dataset in Spanish. The experiments carried out in Spanish and English show that our approach is very competitive concerning the state of the art, as well as it can generalize better, thus being able to detect an AI-generated text in multiple domains.},
  archive  = {J},
  author   = {Robiert Sepúlveda-Torres and Iván Martínez-Murillo and Estela Saquete and Elena Lloret and Manuel Palomar},
  doi      = {10.1109/TBDATA.2025.3536938},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1042-1053},
  title    = {To write or not to write as a machine? that’s the question},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology NLI task. <em>TBD</em>, <em>11</em>(3), 1027-1041. (<a href='https://doi.org/10.1109/TBDATA.2025.3536928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, ChatGPT and GPT-4 have emerged and gained immense global attention due to their unparalleled performance in language processing. Despite demonstrating impressive capability in various open-domain tasks, their adequacy in highly specific fields like radiology remains untested. Radiology presents unique linguistic phenomena distinct from open-domain data due to its specificity and complexity. Assessing the performance of large language models (LLMs) in such specific domains is crucial not only for a thorough evaluation of their overall performance but also for providing valuable insights into future model design directions: whether model design should be generic or domain-specific. To this end, in this study, we evaluate the performance of ChatGPT/GPT-4 on a radiology natural language inference (NLI) task and compare it to other models fine-tuned specifically on task-related data samples. We also conduct a comprehensive investigation on ChatGPT/GPT-4’s reasoning ability by introducing varying levels of inference difficulty. Our results show that 1) ChatGPT and GPT-4 outperform other LLMs in the radiology NLI task and 2) other specifically fine-tuned Bert-based models require significant amounts of data samples to achieve comparable performance to ChatGPT/GPT-4. These findings not only demonstrate the feasibility and promise of constructing a generic model capable of addressing various tasks across different domains, but also highlight several key factors crucial for developing a unified model, particularly in a medical context, paving the way for future artificial general intelligence (AGI) systems. We release our code and data to the research community.},
  archive  = {J},
  author   = {Zihao Wu and Lu Zhang and Chao Cao and Xiaowei Yu and Zhengliang Liu and Lin Zhao and Yiwei Li and Haixing Dai and Chong Ma and Gang Li and Wei Liu and Quanzheng Li and Dinggang Shen and Xiang Li and Dajiang Zhu and Tianming Liu},
  doi      = {10.1109/TBDATA.2025.3536928},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1027-1041},
  title    = {Exploring the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology NLI task},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adapt anything: Tailor any image classifier across domains and categories using text-to-image diffusion models. <em>TBD</em>, <em>11</em>(3), 1013-1026. (<a href='https://doi.org/10.1109/TBDATA.2025.3536933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We study a novel problem in this paper, that is, if a modern text-to-image diffusion model can tailor any image classifier across domains and categories. Existing domain adaption works exploit both source and target data for domain alignment so as to transfer the knowledge from the labeled source data to the unlabeled target data. However, as the development of text-to-image diffusion models, we wonder if the high-fidelity synthetic data can serve as a surrogate of the source data in real world. In this way, we do not need to collect and annotate the source data for each image classification task in a one-for-one manner. Instead, we utilize only one off-the-shelf text-to-image model to synthesize images with labels derived from text prompts, and then leverage them as a bridge to dig out the knowledge from the task-agnostic text-to-image generator to the task-oriented image classifier via domain adaptation. Such a one-for-all adaptation paradigm allows us to adapt anything in the world using only one text-to-image generator as well as any unlabeled target data. Extensive experiments validate the feasibility of this idea, which even surprisingly surpasses the state-of-the-art domain adaptation works using the source data collected and annotated in real world.},
  archive  = {J},
  author   = {Weijie Chen and Haoyu Wang and Shicai Yang and Lei Zhang and Wei Wei and Yanning Zhang and Luojun Lin and Di Xie and Yueting Zhuang},
  doi      = {10.1109/TBDATA.2025.3536933},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1013-1026},
  title    = {Adapt anything: Tailor any image classifier across domains and categories using text-to-image diffusion models},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-modal assessment framework for comparison of specialized deep learning and general-purpose large language models. <em>TBD</em>, <em>11</em>(3), 1001-1012. (<a href='https://doi.org/10.1109/TBDATA.2025.3536937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recent years have witnessed tremendous advancements in Al tools (e.g., ChatGPT, GPT-4, and Bard), driven by the growing power, reasoning, and efficiency of Large Language Models (LLMs). LLMs have been shown to excel in tasks ranging from poem writing and coding to essay generation and puzzle solving. Despite their proficiency in general queries, specialized tasks such as metaphor understanding and fake news detection often require finely tuned models, posing a comparison challenge with specialized Deep Learning (DL). We propose an assessment framework to compare task-specific intelligence with general-purpose LLMs on suicide and depression tendency identification. For this purpose, we trained two DL models on a suicide and depression detection dataset, followed by testing their performance on a test set. Afterward, the same test dataset is used to evaluate the performance of four LLMs (GPT-3.5, GPT-4, Google Bard, and MS Bing) using four classification metrics. The BERT-based DL model performed the best among all, with a testing accuracy of 94.61%, while GPT-4 was the runner-up with accuracy 92.5%. Results demonstrate that LLMs do not outperform the specialized DL models but are able to achieve comparable performance, making them a decent option for downstream tasks without specialized training. However, LLMs outperformed specialized models on the reduced dataset.},
  archive  = {J},
  author   = {Mohammad Nadeem and Shahab Saquib Sohail and Dag Øivind Madsen and Ahmed Ibrahim Alzahrani and Javier Del Ser and Khan Muhammad},
  doi      = {10.1109/TBDATA.2025.3536937},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {1001-1012},
  title    = {A multi-modal assessment framework for comparison of specialized deep learning and general-purpose large language models},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Terrain scene generation using a lightweight vector quantized generative adversarial network. <em>TBD</em>, <em>11</em>(3), 988-1000. (<a href='https://doi.org/10.1109/TBDATA.2025.3536926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Natural terrain scene images play important roles in the geographical research and application. However, it is challenging to collect a large set of terrain scene images. Recently, great progress has been made in image generation. Although impressive results can be achieved, the efficiency of the state-of-the-art methods, e.g., the Vector Quantized Generative Adversarial Network (VQGAN), is still dissatisfying. The VQGAN confronts two issues, i.e., high space complexity and heavy computational demand. To efficiently fulfill the terrain scene generation task, we first collect a Natural Terrain Scene Data Set (NTSD), which contains 36,672 images divided into 38 classes. Then we propose a Lightweight VQGAN (Lit-VQGAN), which uses the fewer parameters and has the lower computational complexity, compared with the VQGAN. A lightweight super-resolution network is further adopted, to speedily derive a high-resolution image from the image that the Lit-VQGAN generates. The Lit-VQGAN can be trained and tested on the NTSD. To our knowledge, either the NTSD or the Lit-VQGAN has not been exploited before.1 Experimental results show that the Lit-VQGAN is more efficient and effective than the VQGAN for the image generation task. These promising results should be due to the lightweight yet effective networks that we design.},
  archive  = {J},
  author   = {Yan Wang and Huiyu Zhou and Xinghui Dong},
  doi      = {10.1109/TBDATA.2025.3536926},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {988-1000},
  title    = {Terrain scene generation using a lightweight vector quantized generative adversarial network},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Use of transfer learning for affordable in-context fake review generation. <em>TBD</em>, <em>11</em>(3), 976-987. (<a href='https://doi.org/10.1109/TBDATA.2025.3536927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fake content is a noteworthy threat which is managed by assorted means. This is a serious problem for online shopping platforms whose products can be affected by negative or positive reviews. Artificial intelligence is commonly applied for fake review generation, being transfer learning a promising approach to reduce training requirements. However, the feasibility of generating in-context fake reviews using transfer learning has not been explored yet. This paper analyses the suitability of a couple of transformers (T5 and BART) to generate realistic in-context fake reviews. Results show that 1) the diversity of generated reviews is comparable to existing works; 2) human-based detection is close to random; 3) just reviews generated with one of the used transformers can be detected with 38% precision; and 1 h of training and 8 k real reviews are needed to produce realistic fake reviews.},
  archive  = {J},
  author   = {Luis Ibañez-Lissen and Lorena González-Manzano and José M. de Fuentes and Manuel Goyanes},
  doi      = {10.1109/TBDATA.2025.3536927},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {976-987},
  title    = {Use of transfer learning for affordable in-context fake review generation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical multi-relational graph representation learning for large-scale prediction of drug-drug interactions. <em>TBD</em>, <em>11</em>(3), 961-975. (<a href='https://doi.org/10.1109/TBDATA.2025.3536924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Most existing methods for predicting drug-drug interactions (DDI) predominantly concentrate on capturing the explicit relationships among drugs, overlooking the valuable implicit correlations present between drug pairs (DPs), which leads to weak predictions. To address this issue, this paper introduces a hierarchical multi-relational graph representation learning (HMGRL) approach. Within the framework of HMGRL, we leverage a wealth of drug-related heterogeneous data sources to construct heterogeneous graphs, where nodes represent drugs and edges denote clear and various associations. The relational graph convolutional network (RGCN) is employed to capture diverse explicit relationships between drugs from these heterogeneous graphs. Additionally, a multi-view differentiable spectral clustering (MVDSC) module is developed to capture multiple valuable implicit correlations between DPs. Within the MVDSC, we utilize multiple DP features to construct graphs, where nodes represent DPs and edges denote different implicit correlations. Subsequently, multiple DP representations are generated through graph cutting, each emphasizing distinct implicit correlations. The graph-cutting strategy enables our HMGRL to identify strongly connected communities of graphs, thereby reducing the fusion of irrelevant features. By combining every representation view of a DP, we create high-level DP representations for predicting DDIs. Two genuine datasets spanning three distinct tasks are adopted to gauge the efficacy of our HMGRL. Experimental outcomes unequivocally indicate that HMGRL surpasses several leading-edge methods in performance.},
  archive  = {J},
  author   = {Mengying Jiang and Guizhong Liu and Yuanchao Su and Weiqiang Jin and Biao Zhao},
  doi      = {10.1109/TBDATA.2025.3536924},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {961-975},
  title    = {Hierarchical multi-relational graph representation learning for large-scale prediction of drug-drug interactions},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large language model-informed ECG dual attention network for heart failure risk prediction. <em>TBD</em>, <em>11</em>(3), 948-960. (<a href='https://doi.org/10.1109/TBDATA.2025.3536922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Heart failure (HF) poses a significant public health challenge, with a rising global mortality rate. Early detection and prevention of HF could significantly reduce its impact. We introduce a novel methodology for predicting HF risk using 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual attention ECG network designed to capture complex ECG features essential for early HF risk prediction, despite the notable imbalance between low and high-risk groups. This network incorporates a cross-lead attention module and 12 lead-specific temporal attention modules, focusing on cross-lead interactions and each lead's local dynamics. To further alleviate model overfitting, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-Report alignment task. The network is then fine-tuned for HF risk prediction using two specific cohorts from the U.K. Biobank study, focusing on patients with hypertension (UKB-HYP) and those who have had a myocardial infarction (UKB-MI). The results reveal that LLM-informed pre-training substantially enhances HF risk prediction in these cohorts. The dual attention design not only improves interpretability but also predictive accuracy, outperforming existing competitive methods with C-index scores of 0.6349 for UKB-HYP and 0.5805 for UKB-MI. This demonstrates our method's potential in advancing HF risk assessment with clinical complex ECG data.},
  archive  = {J},
  author   = {Chen Chen and Lei Li and Marcel Beetz and Abhirup Banerjee and Ramneek Gupta and Vicente Grau},
  doi      = {10.1109/TBDATA.2025.3536922},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {948-960},
  title    = {Large language model-informed ECG dual attention network for heart failure risk prediction},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TinyLVLM-eHub: Towards comprehensive and efficient evaluation for large vision-language models. <em>TBD</em>, <em>11</em>(3), 933-947. (<a href='https://doi.org/10.1109/TBDATA.2025.3536930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Large Vision-Language Models (LVLMs) have made significant strides in various multimodal tasks. Notably, GPT4V, Claude, Gemini, and others showcase exceptional multimodal capabilities, marked by profound comprehension and reasoning skills. This study introduces a comprehensive and efficient evaluation framework, TinyLVLM-eHub, to assess LVLMs’ performance, including proprietary models. TinyLVLM-eHub covers six key multimodal capabilities, such as visual perception, knowledge acquisition, reasoning, commonsense understanding, object hallucination, and embodied intelligence. The benchmark, utilizing 2.1K image-text pairs, provides a user-friendly and accessible platform for LVLM evaluation. The evaluation employs the ChatGPT Ensemble Evaluation (CEE) method, which improves alignment with human evaluation compared to word-matching approaches. Results reveal that closed-source API models like GPT4V and GeminiPro-V excel in most capabilities compared to previous open-source LVLMs, though they show some vulnerability in object hallucination. This evaluation underscores areas for LVLM improvement in real-world applications and serves as a foundational assessment for future multimodal advancements.},
  archive  = {J},
  author   = {Wenqi Shao and Meng Lei and Yutao Hu and Peng Gao and Peng Xu and Kaipeng Zhang and Fanqing Meng and Siyuan Huang and Hongsheng Li and Yu Qiao and Ping Luo},
  doi      = {10.1109/TBDATA.2025.3536930},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {933-947},
  title    = {TinyLVLM-eHub: Towards comprehensive and efficient evaluation for large vision-language models},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expertise or hallucination? a comprehensive evaluation of ChatGPT's aptitude in clinical genetics. <em>TBD</em>, <em>11</em>(3), 919-932. (<a href='https://doi.org/10.1109/TBDATA.2025.3536939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Whether viewed as an expert or as a source of ‘knowledge hallucination’, the use of ChatGPT in medical practice has stirred ongoing debate. This study sought to evaluate ChatGPT's capabilities in the field of clinical genetics, focusing on tasks such as ‘Clinical genetics exams’, ‘Associations between genetic diseases and pathogenic genes’, and ‘Limitations and trends in clinical genetics’. Results indicated that ChatGPT performed exceptionally well in question-answering tasks, particularly in clinical genetics exams and diagnosing single-gene diseases. It also effectively outlined the current limitations and prospective trends in clinical genetics. However, ChatGPT struggled to provide comprehensive answers regarding multi-gene or epigenetic diseases, particularly with respect to genetic variations or chromosomal abnormalities. In terms of systematic summarization and inference, some randomness was evident in ChatGPT's responses. In summary, while ChatGPT possesses a foundational understanding of general knowledge in clinical genetics due to hyperparameter learning, it encounters significant challenges when delving into specialized knowledge and navigating the complexities of clinical genetics, particularly in mitigating ‘Knowledge Hallucination’. To optimize its performance and depth of expertise in clinical genetics, integration with specialized knowledge databases and knowledge graphs is imperative.},
  archive  = {J},
  author   = {Yingbo Zhang and Shumin Ren and Jiao Wang and Chaoying Zhan and Mengqiao He and Xingyun Liu and Rongrong Wu and Jing Zhao and Cong Wu and Chuanzhu Fan and Bairong Shen},
  doi      = {10.1109/TBDATA.2025.3536939},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {919-932},
  title    = {Expertise or hallucination? a comprehensive evaluation of ChatGPT's aptitude in clinical genetics},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AugGPT: Leveraging ChatGPT for text data augmentation. <em>TBD</em>, <em>11</em>(3), 907-918. (<a href='https://doi.org/10.1109/TBDATA.2025.3536934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Text data augmentation is an effective strategy for overcoming the challenge of limited sample sizes in many natural language processing (NLP) tasks. This challenge is especially prominent in the few-shot learning (FSL) scenario, where the data in the target domain is generally much scarcer and of lowered quality. A natural and widely used strategy to mitigate such challenges is to perform data augmentation to better capture data invariance and increase the sample size. However, current text data augmentation methods either can’t ensure the correct labeling of the generated data (lacking faithfulness), or can’t ensure sufficient diversity in the generated data (lacking compactness), or both. Inspired by the recent success of large language models (LLM), especially the development of ChatGPT, we propose a text data augmentation approach based on ChatGPT (named ”AugGPT”). AugGPT rephrases each sentence in the training samples into multiple conceptually similar but semantically different samples. The augmented samples can then be used in downstream model training. Experiment results on multiple few-shot learning text classification tasks show the superior performance of the proposed AugGPT approach over state-of-the-art text data augmentation methods in terms of testing accuracy and distribution of the augmented samples.},
  archive  = {J},
  author   = {Haixing Dai and Zhengliang Liu and Wenxiong Liao and Xiaoke Huang and Yihan Cao and Zihao Wu and Lin Zhao and Shaochen Xu and Fang Zeng and Wei Liu and Ninghao Liu and Sheng Li and Dajiang Zhu and Hongmin Cai and Lichao Sun and Quanzheng Li and Dinggang Shen and Tianming Liu and Xiang Li},
  doi      = {10.1109/TBDATA.2025.3536934},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {907-918},
  title    = {AugGPT: Leveraging ChatGPT for text data augmentation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CHEAT: A large-scale dataset for detecting CHatGPT-writtEn AbsTracts. <em>TBD</em>, <em>11</em>(3), 898-906. (<a href='https://doi.org/10.1109/TBDATA.2025.3536929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The powerful ability of ChatGPT has caused widespread concern in the academic community. Malicious users could synthesize dummy academic content through ChatGPT, which is extremely harmful to academic rigor and originality. The need to develop ChatGPT-written content detection algorithms calls for large-scale datasets. In this paper, we initially investigate the possible negative impact of ChatGPT on academia, and present a large-scale CHatGPT-writtEn AbsTract dataset (CHEAT) to support the development of detection algorithms. In particular, the ChatGPT-written abstract dataset contains 35,304 synthetic abstracts, with $Generation$, $Polish$, and $Fusion$ as prominent representatives. Based on these data, we perform a thorough analysis of the existing text synthesis detection algorithms. We show that ChatGPT-written abstracts are detectable with well-trained detectors, while the detection difficulty increases with more human guidance involved.},
  archive  = {J},
  author   = {Peipeng Yu and Jiahan Chen and Xuan Feng and Zhihua Xia},
  doi      = {10.1109/TBDATA.2025.3536929},
  journal  = {IEEE Transactions on Big Data},
  month    = {6},
  number   = {3},
  pages    = {898-906},
  title    = {CHEAT: A large-scale dataset for detecting CHatGPT-writtEn AbsTracts},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial emerging horizons: The rise of large language models and cross-modal generative AI. <em>TBD</em>, <em>11</em>(3), 896-897. (<a href='https://doi.org/10.1109/TBDATA.2025.3537217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Guang Yang and Jing Zhang and Giorgos Papanastasiou and Ge Wang and Dacheng Tao},
  doi     = {10.1109/TBDATA.2025.3537217},
  journal = {IEEE Transactions on Big Data},
  month   = {6},
  number  = {3},
  pages   = {896-897},
  title   = {Editorial emerging horizons: The rise of large language models and cross-modal generative AI},
  volume  = {11},
  year    = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combine the growth of cascades and impact of users for diffusion prediction. <em>TBD</em>, <em>11</em>(2), 887-895. (<a href='https://doi.org/10.1109/TBDATA.2024.3460530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Information diffusion and diffusion prediction have attracted a great deal of research attention over the past decades. Existing approaches usually make predictions based on the order of the activated users, while recently, some studies have taken the social network into consideration and begun to analyze the influence of neighbors via some graph neural networks. However, they ignore the fact that the interests of users and their neighbors may dynamically change along with the growth of the cascade, and thus fail to model the potential impact of activated users. To address the above shortcomings, we proposed in this paper a deep learning model that combines the Mode of cascades Growth and potential Impact of users (MGI). It leverages GCNs to represent users from the social network to model their static features. Besides, we designed an attention mechanism on the cascade sequence to compute features of activated users, and added the popularity variable to model features of users in cascades. Finally, we combined the growth of cascades and impact of users in our model for diffusion prediction. We conducted extensive experiments on several real-world datasets, and the experimental results demonstrate that our model significantly outperforms the state-of-the-art methods in diffusion prediction.},
  archive  = {J},
  author   = {Pengfei Jiao and Peng Yan and Jilin Zhang and Biao Wang and Wang Zhang and Nailiang Zhao},
  doi      = {10.1109/TBDATA.2024.3460530},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {887-895},
  title    = {Combine the growth of cascades and impact of users for diffusion prediction},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic NN-descent: An efficient k-NN graph construction method. <em>TBD</em>, <em>11</em>(2), 879-886. (<a href='https://doi.org/10.1109/TBDATA.2024.3460534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a classic k-NN graph construction method, NN-Descent has been adopted in various applications for its simplicity, genericness, and efficiency. However, its memory consumption is high due to the employment of two extra supporting graph structures. In this paper, a novel k-NN graph construction method is proposed. Similar to NN-Descent, the k-NN graph is constructed by doing cross-matching continuously on the sampled neighbors on each neighborhood. Whereas different from NN-Descent, the cross-matching is undertaken directly on the k-NN graph under construction. It makes the extra graph structures adopted to support the cross-matching no longer necessary. Moreover, no synchronization between different threads is needed within one iteration. The high-quality graph is constructed at the high-speed efficiency and considerably better memory efficiency over NN-Descent on both the multi-thread CPU and the GPU.},
  archive  = {J},
  author   = {Jie-Feng Wang and Wan-Lei Zhao and Shihai Xiao and Jiajie Yao and Xuecang Zhang},
  doi      = {10.1109/TBDATA.2024.3460534},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {879-886},
  title    = {Dynamic NN-descent: An efficient k-NN graph construction method},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning-based distributed spatio-temporal $k$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math> nearest neighbors join. <em>TBD</em>, <em>11</em>(2), 861-878. (<a href='https://doi.org/10.1109/TBDATA.2024.3442539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The rapid development of positioning technology produces an extremely large volume of spatio-temporal data with various geometry types such as point, line string, polygon, or a mixed combination of them. As one of the most fundamental but time-consuming operations, $k$ nearest neighbors join ($k$NN join) has attracted much attention. However, most existing works for $k$NN join either ignore temporal information or consider only point data. Besides, most of them do not automatically adapt to the different features of spatio-temporal data. This paper proposes to address a novel and useful problem, i.e., ST-$k$NN join, which considers both spatial closeness and temporal concurrency. To support ST-$k$NN join over a large amount of spatio-temporal data with any geometry types efficiently, we propose a novel distributed solution based on Apache Spark. Specifically, our method adopts a two-round join framework. In the first round join, we propose a new spatio-temporal partitioning method that achieves spatio-temporal locality and load balance at the same time. We also propose a lightweight index structure, i.e., Time Range Count Index (TRC-index), to enable efficient ST-$k$NN join. In the second round join, to reduce the data transmission among different machines, we remove duplicates based on spatio-temporal reference points before shuffling local results. Furthermore, we design a set of models based on Bayesian optimization to automatically determine the values for the introduced parameters. Extensive experiments are conducted using three real big datasets, showing that our method is much more scalable and achieves 9X faster than baselines, and that the proposed models can always predict appropriate parameters for different datasets.},
  archive  = {J},
  author   = {Ruiyuan Li and Jiajun Li and Minxin Zhou and Rubin Wang and Huajun He and Chao Chen and Jie Bao and Yu Zheng},
  doi      = {10.1109/TBDATA.2024.3442539},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {861-878},
  title    = {Learning-based distributed spatio-temporal $k$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math> nearest neighbors join},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A query-aware method for approximate range search in hamming space. <em>TBD</em>, <em>11</em>(2), 848-860. (<a href='https://doi.org/10.1109/TBDATA.2024.3436636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The range search in Hamming space is to explore the binary vectors whose Hamming distances with a query vector are within a given searching threshold. It arises as the core component of many applications, such as image retrieval, pattern recognition, and machine learning. Existing searching methods in Hamming space require much pre-processing overhead, which are not suitable for processing multiple batches of incoming data in a short time. Moreover, significant pre-processing overhead can be a burden when the number of queries is relatively small. In this paper, we propose a query-aware method for the approximate range search in Hamming space with no pre-process. Specifically, to eliminate the impact of data skewness, we introduce JS-divergence to measure the divergence between data's distribution and query's distribution, and specially design a Query-Aware Dimension Partitioning (QADP) strategy to partition the dimensions into several subspaces according to the scales of given searching thresholds. In the subspaces, the candidates can be efficiently obtained by the basic Pigeonhole Principle and our proposed Anti-Pigeonhole Principle. Furthermore, a sampling strategy is designed to estimate the Hamming distance between the query vector and arbitrary binary vector to obtain the final approximate searching results among the candidates. Experimental results on four real-world datasets illustrate that, in comparison with benchmark methods, our method possesses the superior advantages on searching accuracy and efficiency. The proposed method can increase the searching efficiency up to nearly 16 times with high searching accuracy.},
  archive  = {J},
  author   = {Yang Song and Yu Gu and Min Huang and Ge Yu},
  doi      = {10.1109/TBDATA.2024.3436636},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {848-860},
  title    = {A query-aware method for approximate range search in hamming space},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ANF: Crafting transferable adversarial point clouds via adversarial noise factorization. <em>TBD</em>, <em>11</em>(2), 835-847. (<a href='https://doi.org/10.1109/TBDATA.2024.3436593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transfer-based adversarial attacks involve generating adversarial point clouds in surrogate models and transferring them to other models to assess 3D model robustness. However, current methods rely too much on surrogate model parameters, limiting transferability. In this work, we use Shapley value to identify positive and negative features, guiding optimization of adversarial noise in feature space. To effectively mislead the 3D classifier, we factorize the adversarial noise into positive and negative noise, with the former keeping the features of the adversarial point cloud close to the negative features, and the latter and the adversarial noise moving it away from the positive features. Finally, a novel adversarial point cloud attack method with Adversarial Noise Factorization is proposed, which is abbreviated as ANF. ANF simultaneously optimizes the adversarial noise and its positive and negative noise in the feature space, only relying on partial network parameters, which significantly reduces the reliance on the surrogate model and improves the transferability of the adversarial point cloud. Experiments on well-recognized benchmark datasets show that the transferability of adversarial point clouds generated by ANF could be improved by more than 26.7$\%$ on average over state-of-the-art transfer-based adversarial attack methods.},
  archive  = {J},
  author   = {Hai Chen and Shu Zhao and Xiao Yang and Huanqian Yan and Yuan He and Hui Xue and Fulan Qian and Hang Su},
  doi      = {10.1109/TBDATA.2024.3436593},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {835-847},
  title    = {ANF: Crafting transferable adversarial point clouds via adversarial noise factorization},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal deep learning for semisupervised classification of hyperspectral and LiDAR data. <em>TBD</em>, <em>11</em>(2), 821-834. (<a href='https://doi.org/10.1109/TBDATA.2024.3433494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep learning (DL) has emerged as a competitive method in single-modality-dominated remote sensing (RS) data classification tasks, but its classification performance inevitably encounters a bottleneck due to the lack of representation diversity in complicated spatial structures with various land cover types. Therefore, the RS community has been actively researching multimodal feature learning techniques for the same scene. However, expert annotation of multisource data consumes a significant amount of time and cost. This article proposes an end-to-end method called semisupervised multimodal dual-path network (SMDN). This method simultaneously explores spatial-spectral features contained in hyperspectral images (HSI) and elevation information provided by light detection and ranging (LiDAR). SMDN exploits an unsupervised novel encoder-decoder structure as the backbone network to construct a multimodal DL architecture by jointly training with a data-specific branch. To obtain discriminative multimodal representations, SMDN is able to guide the collaborative training of two different unsupervised features mapped in the latent subspace with limited labeled training samples. Furthermore, after a simple modification of the fusion strategy in SMDN, it can be applied to unsupervised classification problems. Experimental results on benchmark RS datasets validate the effectiveness of the developed SMDN compared over many state-of-the-art methods.},
  archive  = {J},
  author   = {Chunyu Pu and Yingxu Liu and Shuai Lin and Xu Shi and Zhengying Li and Hong Huang},
  doi      = {10.1109/TBDATA.2024.3433494},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {821-834},
  title    = {Multimodal deep learning for semisupervised classification of hyperspectral and LiDAR data},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFS-FCM with memory: A model for air quality multi-dimensional prediction with interpretability. <em>TBD</em>, <em>11</em>(2), 810-820. (<a href='https://doi.org/10.1109/TBDATA.2024.3433467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In order to represent the influences of different semantics on targets and improve the prediction with interpretability ability for multi-dimensional time series, we integrate Axiomatic Fuzzy Set (AFS) and Fuzzy Cognitive Map (FCM) with memory for fuzzy knowledge representation and prediction in this paper. The AFS is used to extract semantics of concepts for fuzzy representation using data distribution. The FCM with memory is trained to model the influence relationships between different semantics of concepts and multiple targets based on multi-dimensional time series data. And a multi- dimensional learning algorithm of AFS-FCM with memory based on gradient descent is developed to investigate the influences of different semantics of concepts on multiple targets. Finally, we validate our model by comparing with other FCMs, intrinsic interpretable models and machine learning methods for prediction of air quality multidimensional time series data, and discuss the performance of AFS-FCM with different transformation functions. The model can not only predict air quality accurately, but also explicitly reveal the specific quantitative relationship of different semantics of meteorology on air quality.},
  archive  = {J},
  author   = {Zhen Peng and Wanquan Liu and Sung-Kwun Oh},
  doi      = {10.1109/TBDATA.2024.3433467},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {810-820},
  title    = {AFS-FCM with memory: A model for air quality multi-dimensional prediction with interpretability},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systemic pipeline of identifying lncRNA-disease associations to the prognosis and treatment of hepatocellular carcinoma. <em>TBD</em>, <em>11</em>(2), 800-809. (<a href='https://doi.org/10.1109/TBDATA.2024.3433380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Exploring disease mechanisms at the lncRNA level provides valuable guidance for disease prognosis and treatment. Recently, there has been a surge of interest in exploring disease mechanisms via computational methods to overcome the challenge of tremendous manpower and material resources in biological experiments. However, current computational methods suffer from two main limitations: simple data structures that do not consider the close association between multiple types of data, and the lack of a systematic pathogenesis analysis that identified disease-associated lncRNAs are not applied to the downstream disease prognosis and therapeutic analysis from the perspective of data analysis. In this end, we present a systemic pipeline including disease-associated lncRNAs identification and downstream pathogenesis analysis on how the predicted lncRNAs are involved in the disease prognosis and therapy. Due to the importance of identifying disease-associated lncRNAs and the weak interpretability of existing computational identification methods, we propose a novel approach named iLncDA-PT to identify disease-associated lncRNAs considering the interactions between various bio-entities outperforming the other state-of-the-art methods, and then we conduct a systematically subsequent analysis on prognosis and therapy for a specific disease, hepatocellular carcinoma (HCC), as an example. Finally, we reveal a significant association between immune checkpoint expression, tumor microenvironment, and drug treatment.},
  archive  = {J},
  author   = {Wenxiang Zhang and Ye Yuan and Hang Wei and Wenjing Zhang and Bin Liu},
  doi      = {10.1109/TBDATA.2024.3433380},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {800-809},
  title    = {A systemic pipeline of identifying lncRNA-disease associations to the prognosis and treatment of hepatocellular carcinoma},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALD-GCN: Graph convolutional networks with attribute-level defense. <em>TBD</em>, <em>11</em>(2), 788-799. (<a href='https://doi.org/10.1109/TBDATA.2024.3433553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph Neural Networks(GNNs), such as Graph Convolutional Network, have exhibited impressive performance on various real-world datasets. However, many researches have confirmed that deliberately designed adversarial attacks can easily confuse GNNs on the classification of target nodes (targeted attacks) or all the nodes (global attacks). According to our observations, different attributes tend to be differently treated when the graph is attacked. Unfortunately, most of the existing defense methods can only defend at the graph or node level, which ignores the diversity of different attributes within each node. To address this limitation, we propose to leverage a new property, named Attribute-level Smoothness (ALS), which is defined based on the local differences of graph. We then propose a novel defense method, named GCN with Attribute-level Defense (ALD-GCN), which utilizes the ALS property to provide attribute-level protection to each attributes. Extensive experiments on real-world graphs have demonstrated the superiority of the proposed work and the potentials of our ALS property in the attacks.},
  archive  = {J},
  author   = {Yihui Li and Yuanfang Guo and Junfu Wang and Shihao Nie and Liang Yang and Di Huang and Yunhong Wang},
  doi      = {10.1109/TBDATA.2024.3433553},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {788-799},
  title    = {ALD-GCN: Graph convolutional networks with attribute-level defense},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secret specification based personalized privacy-preserving analysis in big data. <em>TBD</em>, <em>11</em>(2), 774-787. (<a href='https://doi.org/10.1109/TBDATA.2024.3433433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The pursuit of refined data analysis and the preservation of privacy in Big Data pose significant concerns. Among the paramount paradigms for addressing these challenges, differential privacy stands out as a vital area of research. However, traditional differential privacy tends to be excessively restrictive when it comes to individuals’ control over their own data. It often treats all data as inherently sensitive, whereas in reality, not all information related to individuals is sensitive and requires an identical level of protection. In this paper, we define secret specification-based differential privacy (SSDP), where the term “secret specification” implies enabling users to decide what aspects of their information are sensitive and what are not, prior to data generation or processing. By allowing individuals to independently define their secret specifications, the SSDP achieves personalized privacy protection and facilitates effective data analysis. To enable the targeted application of SSDP, we further present task-specific mechanisms designed for database and graph data scenarios. Finally, we assess the trade-offs between privacy and utility inherent in the proposed mechanisms through comparative experiments conducted on real datasets, demonstrating the utility enhancements offered by SSDP mechanisms in practical applications.},
  archive  = {J},
  author   = {Jiajun Chen and Chunqiang Hu and Zewei Liu and Tao Xiang and Pengfei Hu and Jiguo Yu},
  doi      = {10.1109/TBDATA.2024.3433433},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {774-787},
  title    = {Secret specification based personalized privacy-preserving analysis in big data},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local high-order graph learning for multi-view clustering. <em>TBD</em>, <em>11</em>(2), 761-773. (<a href='https://doi.org/10.1109/TBDATA.2024.3433525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As the accumulation of multi-view data continues to grow, multi-view clustering has become increasingly important in research fields like data mining. However, current methods have been criticized for their unsatisfactory performance, such as insufficient exploration of intra-view high-order relationships and poor characterization of inter-view diverse features. To overcome these challenges, we propose a novel approach called Local High-order Graph Learning for Multi-View Clustering (LHGL_MVC). Our method aims to explore high-order relationships within a view while also considering diverse information between views. In LHGL_MVC, we learn the initial graphs of each view through self-representation, which are decomposed into consistent and diverse parts to better capture the diversity of different views. Based on consistent parts, we propose a novel local high-order graph learning approach to more effectively explore high-order relationships between samples within each view. At the same time, we leverage high-order relationships between views using the rotated tensor nuclear norm. Finally, we obtain a unified graph for clustering by fusing all consistent affinity graphs and their high-order graphs with adaptive weights. All procedures are integrated into an overall objective function, which mutually promotes during the optimization process. The comprehensive experiments conducted on eleven real-world datasets demonstrate that LHGL_MVC significantly outperforms existing algorithms in various measurements, highlighting the superiority of the proposed method.},
  archive  = {J},
  author   = {Zhi Wang and Qiang Lin and Yaxiong Ma and Xiaoke Ma},
  doi      = {10.1109/TBDATA.2024.3433525},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {761-773},
  title    = {Local high-order graph learning for multi-view clustering},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient learning for billion-scale heterogeneous information networks. <em>TBD</em>, <em>11</em>(2), 748-760. (<a href='https://doi.org/10.1109/TBDATA.2024.3428331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Heterogeneous graph neural networks (HGNNs) excel at understanding heterogeneous information networks (HINs) and have demonstrated state-of-the-art performance across numerous tasks. However, previous works tend to study small datasets, which deviate significantly from real-world scenarios. More specifically, their heterogeneous message passing results in substantial memory and time overheads, as it requires aggregating heterogeneous neighbor features multiple times. To address this, we propose an Efficient Heterogeneous Graph Neural Network (EHGNN) that leverages heterogeneous personalized PageRank (HPPR) to preserve the influence between all nodes, then approximates message passing and selectively loads neighbor information for one aggregation, significantly reducing memory and time usage. In addition, we employ some lightweight techniques to ensure the performance of EHGNN. Evaluations on various HIN benchmarks in node classification and link prediction tasks unequivocally establish the superiority of EHGNN, surpassing the State-of-the-Art by 11$\%$ in terms of performance. In addition, EHGNN achieves a remarkable 400$\%$ boost in training and inference speed while utilizing less memory. Notably, EHGNN can handle a 200-million-node, 1-billion-link HIN within 18 hours on a single machine, using only 170 GB of memory, which is much lower than the previous minimum requirement of 600 GB.},
  archive  = {J},
  author   = {Ruize Shi and Hong Huang and Xue Lin and Kehan Yin and Wei Zhou and Hai Jin},
  doi      = {10.1109/TBDATA.2024.3428331},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {748-760},
  title    = {Efficient learning for billion-scale heterogeneous information networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph convolutional networks with collaborative feature fusion for sequential recommendation. <em>TBD</em>, <em>11</em>(2), 735-747. (<a href='https://doi.org/10.1109/TBDATA.2024.3426355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sequential recommendation seeks to understand user preferences based on their past actions and predict future interactions with items. Recently, several techniques for sequential recommendation have emerged, primarily leveraging graph convolutional networks (GCNs) for their ability to model relationships effectively. However, real-world scenarios often involve sparse interactions, where early and recent short-term preferences play distinct roles in the recommendation process. Consequently, vanilla GCNs struggle to effectively capture the explicit correlations between these early and recent short-term preferences. To address these challenges, we introduce a novel approach termed Graph Convolutional Networks with Collaborative Feature Fusion (COFF). Specifically, our method addresses the issue by initially dividing each user interaction sequence into two segments. We then construct two separate graphs for these segments, aiming to capture the user's early and recent short-term preferences independently. To obtain robust prediction, we employ multiple GCNs in a collaborative distillation manner, incorporating a feature fusion module to establish connections between the early and recent short-term preferences. This approach enables a more precise representation of user preferences. Experimental evaluations conducted on five popular sequential recommendation datasets demonstrate that our COFF model outperforms recent state-of-the-art methods in terms of recommendation accuracy.},
  archive  = {J},
  author   = {Jianping Gou and Youhui Cheng and Yibing Zhan and Baosheng Yu and Weihua Ou and Yi Zhang},
  doi      = {10.1109/TBDATA.2024.3426355},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {735-747},
  title    = {Graph convolutional networks with collaborative feature fusion for sequential recommendation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust joint graph learning for multi-view clustering. <em>TBD</em>, <em>11</em>(2), 722-734. (<a href='https://doi.org/10.1109/TBDATA.2024.3426277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In real-world applications, multi-view datasets often comprise diverse data sources or views, inevitably accompanied by noise. However, most existing graph-based multi-view clustering methods utilize fixed graph similarity matrices to handle noisy multi-view data, necessitating additional clustering steps for obtaining the final clustering. This paper proposes a Robust Joint Graph learning for Multi-view Clustering (RJGMC) based on $ \ell _{1}$-norm to address these problems. RJGMC integrates the learning processes of the graph similarity matrix and the unified graph matrix to improve mutual reinforcement between these graph matrices. Simultaneously, employing the $ \ell _{1}$-norm to generate the unified graph matrix enhances the algorithm's robustness. A rank constraint is imposed on the graph Laplacian matrix of the unified graph matrix, where clustering can be divided directly without additional processing. In addition, we also introduce a method for automatically assigning optimal weights to each view. The optimization of this objective function employs an alternating optimization approach. Experimental results on synthetic and real-world datasets demonstrate that the proposed method outperforms other state-of-the-art techniques regarding clustering performance and robustness.},
  archive  = {J},
  author   = {Yanfang He and Umi Kalsom Yusof},
  doi      = {10.1109/TBDATA.2024.3426277},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {722-734},
  title    = {Robust joint graph learning for multi-view clustering},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Worker similarity-based label completion for crowdsourcing. <em>TBD</em>, <em>11</em>(2), 710-721. (<a href='https://doi.org/10.1109/TBDATA.2024.3426310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In real-world crowdsourcing scenarios, it is a common phenomenon that each worker only annotates a few instances, resulting in a significantly sparse crowdsourcing label matrix. Consequently, only a small number of workers influence the inferred integrated label of each instance, which may weaken the performance of label integration algorithms. To address this problem, we propose a novel label completion algorithm called Worker Similarity-based Label Completion (WSLC). WSLC is grounded on the assumption that workers with similar cognitive abilities will annotate similar labels on the same instances. Specifically, we first construct a data set for each worker that includes all instances annotated by this worker and learn a feature vector for each worker. Then, we define a metric based on cosine similarity to estimate worker similarity based on the learned feature vectors. Finally, we complete the labels for each worker on unannotated instances based on the worker similarity and the annotations of similar workers. The experimental results on one real-world and 34 simulated crowdsourced data sets consistently show that WSLC effectively addresses the problem of the sparse crowdsourcing label matrix and enhances the integration accuracies of label integration algorithms.},
  archive  = {J},
  author   = {Xue Wu and Liangxiao Jiang and Wenjun Zhang and Chaoqun Li},
  doi      = {10.1109/TBDATA.2024.3426310},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {710-721},
  title    = {Worker similarity-based label completion for crowdsourcing},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Denoising implicit feedback for graph collaborative filtering via causal intervention. <em>TBD</em>, <em>11</em>(2), 696-709. (<a href='https://doi.org/10.1109/TBDATA.2024.3423727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The performance of graph collaborative filtering (GCF) models could be affected by noisy user-item interactions. Existing studies on data denoising either ignore the nature of noise in implicit feedback or seldom consider the long-tail distribution of historical interaction data. For the first challenge, we analyze the role of noise from a causal perspective: noise is an unobservable confounder. Therefore, we use the instrumental variable for causal intervention without requiring confounder observation. For the second challenge, we consider degree distribution of nodes in the course of causal intervention. And then we propose a model named causal graph collaborative filtering (CausalGCF) to denoise implicit feedback for GCF. Specifically, we design a degree augmentation strategy as the instrumental variable. First, we divide nodes into head and tail nodes according to their degree. Then, we purify the interactions of the head nodes and enrich those of the tail nodes based on similarity. We perform degree augmentation strategy from the user and item sides to obtain two different graph structures, which are trained together with self-supervised learning. Empirical studies on four real and four synthetic datasets demonstrate the effectiveness of CausalGCF, which is more robust against noisy interactions in implicit feedback than the baselines.},
  archive  = {J},
  author   = {Huiting Liu and Huaxiu Zhang and Peipei Li and Peng Zhao and Xindong Wu},
  doi      = {10.1109/TBDATA.2024.3423727},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {696-709},
  title    = {Denoising implicit feedback for graph collaborative filtering via causal intervention},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual graph convolutional networks for social network alignment. <em>TBD</em>, <em>11</em>(2), 684-695. (<a href='https://doi.org/10.1109/TBDATA.2024.3423699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social network alignment aims to discover the potential correspondence between users across different social platforms. Recent advances in graph representation learning have brought a new upsurge to network alignment. Most existing representation-based methods extract local structural information of social networks from users’ neighborhoods, but the global structural information has not been fully exploited. Therefore, this manuscript proposes a dual graph convolutional networks-based method (DualNA) for social network alignment, which combines user representation learning and user alignment in a unified framework. Specifically, we design dual graph convolutional networks as feature extractors to capture the local and global structural information of social networks, and apply a two-part constraint mechanism, including reconstruction loss and contrastive loss, to jointly optimize the graph representation learning process. As a result, the learned user representations can not only preserve the local and global features of original networks, but also be distinguishable and suitable for the downstream task of social network alignment. Extensive experiments on three real-world datasets show that our proposed method outperforms all baselines. The ablation studies further illustrate the rationality and effectiveness of our method.},
  archive  = {J},
  author   = {Xiaoyu Guo and Yan Liu and Daofu Gong and Fenlin Liu},
  doi      = {10.1109/TBDATA.2024.3423699},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {684-695},
  title    = {Dual graph convolutional networks for social network alignment},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Minerva: Decentralized collaborative query processing over InterPlanetary file system. <em>TBD</em>, <em>11</em>(2), 669-683. (<a href='https://doi.org/10.1109/TBDATA.2024.3423729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data silos create barriers to accessing and utilizing data dispersed over networks. Directly sharing data easily suffers from the long downloading time, the single point failure and the untraceable data usage. In this paper, we present Minerva, a peer-to-peer cross-cluster data query system based on the InterPlanetary File System (IPFS). Minerva makes use of the distributed Hash table (DHT) lookup to pinpoint the locations that store content chunks. We theoretically model the DHT query delay and introduce a fat Merkle tree structure as well as the DHT caching to reduce it. We design the query plan for read and write operations on top of Apache Drill that enables the collaborative query with decentralized workers. We conduct comprehensive experiments on Minerva, and the results show that Minerva achieves up to $2.08 \times$ query performance acceleration compared to the original IPFS data query, and can complete data analysis queries on the Internet-like environments within an average latency of 0.615 second. With a collaborative query, Minerva could perform up to $1.39 \times$ performance acceleration than the centralized query with raw data shipment.},
  archive  = {J},
  author   = {Zhiyi Yao and Bowen Ding and Qianlan Bai and Yuedong Xu},
  doi      = {10.1109/TBDATA.2024.3423729},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {669-683},
  title    = {Minerva: Decentralized collaborative query processing over InterPlanetary file system},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the convergence of federated learning algorithms without data similarity. <em>TBD</em>, <em>11</em>(2), 659-668. (<a href='https://doi.org/10.1109/TBDATA.2024.3423693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Data similarity assumptions have traditionally been relied upon to understand the convergence behaviors of federated learning methods. Unfortunately, this approach often demands fine-tuning step sizes based on the level of data similarity. When data similarity is low, these small step sizes result in an unacceptably slow convergence speed for federated methods. In this paper, we present a novel and unified framework for analyzing the convergence of federated learning algorithms without the need for data similarity conditions. Our analysis centers on an inequality that captures the influence of step sizes on algorithmic convergence performance. By applying our theorems to well-known federated algorithms, we derive precise expressions for three widely used step size schedules: fixed, diminishing, and step-decay step sizes, which are independent of data similarity conditions. Finally, we conduct comprehensive evaluations of the performance of these federated learning algorithms, employing the proposed step size strategies to train deep neural network models on benchmark datasets under varying data similarity conditions. Our findings demonstrate significant improvements in convergence speed and overall performance, marking a substantial advancement in federated learning research.},
  archive  = {J},
  author   = {Ali Beikmohammadi and Sarit Khirirat and Sindri Magnússon},
  doi      = {10.1109/TBDATA.2024.3423693},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {659-668},
  title    = {On the convergence of federated learning algorithms without data similarity},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable federated disentangling network for non-IID domain feature. <em>TBD</em>, <em>11</em>(2), 648-658. (<a href='https://doi.org/10.1109/TBDATA.2024.3423694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated Learning (FL), as an efficient decentralized distributed learning approach, enables multiple institutions to collaboratively train a model without sharing their local data. Despite its advantages, the performance of FL models is substantially impacted by the domain feature shift arising from different acquisition devices/clients. Moreover, existing FL methods often prioritize accuracy without considering reliability factors such as confidence or uncertainty, leading to unreliable predictions in safety-critical applications. Thus, our goal is to enhance FL performance by addressing non-domain feature issues and ensuring model reliability. In this study, we introduce a novel approach named RFedDis (Reliable Federated Disentangling Network). RFedDis leverages feature disentangling to capture a global domain-invariant cross-client representation while preserving local client-specific feature learning. Additionally, we incorporate an uncertainty-aware decision fusion mechanism to effectively integrate the decoupled features. This ensures dynamic integration at the evidence level, producing reliable predictions accompanied by estimated uncertainties. Therefore, RFedDis is the FL approach to combine evidential uncertainty with feature disentangling, enhancing both performance and reliability in handling non-IID domain features. Extensive experimental results demonstrate that RFedDis outperforms other state-of-the-art FL approaches, providing outstanding performance coupled with a high degree of reliability.},
  archive  = {J},
  author   = {Meng Wang and Kai Yu and Chun-Mei Feng and Yiming Qian and Ke Zou and Lianyu Wang and Rick Siow Mong Goh and Xinxing Xu and Yong Liu and Huazhu Fu},
  doi      = {10.1109/TBDATA.2024.3423694},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {648-658},
  title    = {Reliable federated disentangling network for non-IID domain feature},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCLCSE: Dynamic curriculum learning based contrastive learning of sentence embeddings. <em>TBD</em>, <em>11</em>(2), 635-647. (<a href='https://doi.org/10.1109/TBDATA.2024.3423650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Recently, Contrastive Learning (CL) has made impressive progress in natural language processing, especially in sentence representation learning. Plenty of data augmentation methods have been proposed for the generation of positive samples. However, due to the highly abstract nature of natural language, these augmentations cannot maintain the quality of generated positive samples, e.g., too easy or hard samples. To this end, we propose to improve the quality of positive examples from a data arrangement perspective and develop a novel model-agnostic approach: Dynamic Curriculum Learning based Contrastive Sentence Embedding framework (DCLCSE) for sentence embeddings. Specifically, we propose to incorporate a curriculum learning strategy to control the positive example usage. At the early learning stage, easy samples are selected to optimize the CL-based model. As the model's capability increases, we gradually select harder samples for model training, ensuring the learning efficiency of the model. Furthermore, we design a novel difficulty measurement module to calculate the difficulty of generated positives, in which the model's capability is considered for the accurate sample difficulty measurement. Based on this, we develop multiple arrangement strategies to facilitate the model learning process based on learned difficulties. Finally, extensive experiments over multiple representative models demonstrate the superiority of DCLCSE. As a byproduct, we have released the codes to facilitate other researchers.},
  archive  = {J},
  author   = {Chang Liu and Dacao Zhang and Meng Wang},
  doi      = {10.1109/TBDATA.2024.3423650},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {635-647},
  title    = {DCLCSE: Dynamic curriculum learning based contrastive learning of sentence embeddings},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive superpixel segmentation with non-uniform seed initialization. <em>TBD</em>, <em>11</em>(2), 620-634. (<a href='https://doi.org/10.1109/TBDATA.2024.3423719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Superpixel segmentation is a powerful image pre-processing tool in computer vision applications. However, fewer superpixel segmentation methods consider automatically determining the number of initial superpixels. Focusing on high-precision and connectivity, we propose a superpixel segmentation algorithm with non-uniform seed initialization. The proposed algorithm can adaptively determine the number and the position of initial seeds, and is robust to the segmentation of small objects and slender regions. First, we propose a seed initialization scheme based on the side of the circumscribed rectangle of the small object and interval boundary gradient. To enhance the regularity of superpixels, we equally added seeds for grids with sparse seed distribution. Second, we construct a weighted distance measure with search region and feature constraints, which reduces the computational complexity and enhances the precision of pixel label assignment. Finally, we quantify the disconnected regions are present in abundance, and propose a post-processing method based on the area of predefined small objects. The proposed method can significantly improve the connectivity and regularity of the generated superpixels. Extensive experiments on the widely-used BSDS and CamVid datasets demonstrate that the non-uniform seed initialization is effective, and the performance of the proposed superpixel segmentation is favorably compared with the state-of-the-art methods.},
  archive  = {J},
  author   = {Xinlin Xie and Jing Fan and Xinying Xu and Gang Xie},
  doi      = {10.1109/TBDATA.2024.3423719},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {620-634},
  title    = {Adaptive superpixel segmentation with non-uniform seed initialization},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGE: Age-gender effect on faculty career progression in american universities. <em>TBD</em>, <em>11</em>(2), 606-619. (<a href='https://doi.org/10.1109/TBDATA.2024.3423726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This study was undertaken to examine the impact of age and gender on faculty career progression in academia and to identify key performance indicators leading to attaining promotion. To explore any evidence of age-gender effect on faculty career progression, gender compositions, promotion rates, and appointment lengths at the assistant and associate professor levels are investigated. Furthermore, the underlying factors influencing faculty performance evaluation decisions are analyzed using the commercial data provided by Academic Analytics, LLC, which comprises the scholarly records of 336 793 faculty members from 472 Ph.D.-granting universities in the United States during 2011-2020. Various machine learning techniques, including ensemble learning and association rule mining, are performed to determine the important features that provide the most significant insights into academic career growth. Our results indicate strong evidence of age-gender effect on faculty career advancement and underscore the significance of journal article and citation counts for career progression in higher education.},
  archive  = {J},
  author   = {H. Rahmani and Anthony J. Olejniczak and Gary R. Weckman},
  doi      = {10.1109/TBDATA.2024.3423726},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {606-619},
  title    = {AGE: Age-gender effect on faculty career progression in american universities},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DBNetVizor: Visual analysis of dynamic basketball player networks. <em>TBD</em>, <em>11</em>(2), 591-605. (<a href='https://doi.org/10.1109/TBDATA.2024.3423721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Visual analysis has been increasingly integrated into the exploration of temporal networks, as visualization methods have the capability to present time-varying attributes and relationships of entities in an easy-to-read manner. Visualization techniques have been employed in a variety of dynamic network datasets, including social media networks, academic citation networks, and financial transaction networks. However, effectively visualizing dynamic basketball player network data, which consists of numerical networks, intensive timestamps, and subtle changes, remains a challenge for analysts. To address this issue, we propose a snapshot extraction algorithm that involves human-in-the-loop methodology to help users divide a series of networks into hierarchical snapshots for subsequent network analysis tasks, such as node exploration and network pattern analysis. Furthermore, we design and implement a prototype system, called DBNetVizor, for dynamic basketball player network data visualization. DBNetVizor integrates a graphical user interface to help users extract snapshots visually and interactively, as well as multiple linked visualization charts to display macro- and micro-level information of dynamic basketball player network data. To demonstrate the usability and efficiency of our proposed methods, we present two case studies based on dynamic basketball player network data in a competition. Additionally, we conduct an evaluation and receive positive feedback.},
  archive  = {J},
  author   = {Baofeng Chang and Guodao Sun and Sujia Zhu and Qi Jiang and Wang Xia and Jingwei Tang and Ronghua Liang},
  doi      = {10.1109/TBDATA.2024.3423721},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {591-605},
  title    = {DBNetVizor: Visual analysis of dynamic basketball player networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symbolic knowledge reasoning on hyper-relational knowledge graphs. <em>TBD</em>, <em>11</em>(2), 578-590. (<a href='https://doi.org/10.1109/TBDATA.2024.3423670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Knowledge reasoning has been widely researched in knowledge graphs (KGs), but there has been relatively less research on hyper-relational KGs, which also plays an important role in downstream tasks. Existing reasoning methods on hyper-relational KGs are based on representation learning. Though this approach is effective, it lacks interpretability and ignores the graph structure information. In this paper, we make the first attempt at symbolic reasoning on hyper-relational KGs. We introduce rule extraction methods based on both individual facts and paths, and propose a rule-based symbolic reasoning approach, HyperPath. This approach is simple and interpretable, it can serve as a baseline model for symbolic reasoning in hyper-relational KGs. We provide experimental results on almost all datasets, including five large-scale datasets and seven sub-datasets of them. Experiments show that the expressive power of the proposed model is similar to simple neural networks like convolutional networks, but not as advanced as more complex networks such as Transformer and graph convolutional networks, which is consistent with the performance of symbolic methods on KGs. Furthermore, we also analyze the impact of rule length and hyperparameters on the model's performance, which can provide insights for future research in hypergraph symbolic reasoning.},
  archive  = {J},
  author   = {Zikang Wang and Linjing Li and Daniel Dajun Zeng},
  doi      = {10.1109/TBDATA.2024.3423670},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {578-590},
  title    = {Symbolic knowledge reasoning on hyper-relational knowledge graphs},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HEART: Historically information embedding and subspace re-weighting transformer-based tracking. <em>TBD</em>, <em>11</em>(2), 566-577. (<a href='https://doi.org/10.1109/TBDATA.2024.3423672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transformers-based trackers offer significant potential for integrating semantic interdependence between template and search features in tracking tasks. Transformers possess inherent capabilities for processing long sequences and extracting correlations within them. Several researchers have explored the feasibility of incorporating Transformers to model continuously changing search areas in tracking tasks. However, their approach has substantially increased the computational cost of an already resource-intensive Transformer. Additionally, existing Transformers-based trackers rely solely on mechanically employing multi-head attention to obtain representations in different subspaces, without any inherent bias. To address these challenges, we propose HEART (Historical Information Embedding And Subspace Re-weighting Tracker). Our method embeds historical information into the queries in a lightweight and Markovian manner to extract discriminative attention maps for robust tracking. Furthermore, we develop a multi-head attention distribution mechanism to retrieve the most promising subspace weights for tracking tasks. HEART has demonstrated its effectiveness on five datasets, including OTB-100, LaSOT, UAV123, TrackingNet, and GOT-10k.},
  archive  = {J},
  author   = {Tianpeng Liu and Jing Li and Amin Beheshti and Jia Wu and Jun Chang and Beihang Song and Lezhi Lian},
  doi      = {10.1109/TBDATA.2024.3423672},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {566-577},
  title    = {HEART: Historically information embedding and subspace re-weighting transformer-based tracking},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep cross-modal hashing with ranking learning for noisy labels. <em>TBD</em>, <em>11</em>(2), 553-565. (<a href='https://doi.org/10.1109/TBDATA.2024.3423704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Deep hashing technology has recently become an essential tool for cross-modal retrieval on large-scale datasets. However, their performances heavily depend on accurate annotations to train the hashing model. In real applications, we usually only obtain low-quality label annotations owing to labor and time consumption limitations. To mitigate the performance degradation caused by noisy labels, in this paper, we propose a robust deep hashing method, called deep hashing with ranking learning (DHRL), for cross-modal retrieval. The proposed DHRL method consists of a refined semantic concept alignment module and a ranking-swapping module. In this first module, we adopt two transformers to perform the semantic alignment tasks between different modalities on a set of refined concepts, and then convert them into hash codes to reduce heterogeneous differences between multimodalities. The second module first identifies the noisy labels in the training set and ranks them according to ranking loss. Then it swaps the ranking information of different modal network branches. Unlike existing robust hashing methods for assuming noise distribution, our proposed DHRL method requires no prior assumptions for the input data. Extensive experiments on three benchmark datasets have shown that our proposed DHRL method has stronger advantages over other state-of-the-art hashing methods.},
  archive  = {J},
  author   = {Zhenqiu Shu and Yibing Bai and Kailing Yong and Zhengtao Yu},
  doi      = {10.1109/TBDATA.2024.3423704},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {553-565},
  title    = {Deep cross-modal hashing with ranking learning for noisy labels},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SGAMF: Sparse gated attention-based multimodal fusion method for fake news detection. <em>TBD</em>, <em>11</em>(2), 540-552. (<a href='https://doi.org/10.1109/TBDATA.2024.3414341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the field of fake news detection, deep learning techniques have emerged as superior performers in recent years. Nevertheless, the majority of these studies primarily concentrate on either unimodal feature-based methodologies or image-text multimodal fusion techniques, with a minimal focus on the fusion of unstructured text features and structured tabular features. In this study, we present SGAMF, a Sparse Gated Attention-based Multimodal Fusion strategy, designed to amalgamate text features and auxiliary features for the purpose of fake news identification. Compared with traditional multimodal fusion methods, SGAMF can effectively balance accuracy and inference time while selecting the most important features. A novel sparse-gated-attention mechanism has been proposed which instigates a shift in text representation conditioned on auxiliary features, thereby selectively filtering out non-essential features. We have further put forward an enhanced ALBERT for the encoding of text features, capable of balancing efficiency and accuracy. To corroborate our methodology, we have developed a multimodal COVID-19 fake news detection dataset. Comprehensive experimental outcomes on this dataset substantiate that our proposed SGAMF delivers competitive performance in comparison to the existing state-of-the-art techniques in terms of accuracy and $F_{1}$ score.},
  archive  = {J},
  author   = {Pengfei Du and Yali Gao and Linghui Li and Xiaoyong Li},
  doi      = {10.1109/TBDATA.2024.3414341},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {540-552},
  title    = {SGAMF: Sparse gated attention-based multimodal fusion method for fake news detection},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Update selective parameters: Federated machine unlearning based on model explanation. <em>TBD</em>, <em>11</em>(2), 524-539. (<a href='https://doi.org/10.1109/TBDATA.2024.3409947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated learning is a promising privacy-preserving paradigm for distributed machine learning. In this context, there is sometimes a need for a specialized process called machine unlearning, which is required when the effect of some specific training samples needs to be removed from a learning model due to privacy, security, usability, and/or legislative factors. However, problems arise when current centralized unlearning methods are applied to existing federated learning, in which the server aims to remove all information about a class from the global model. Centralized unlearning usually focuses on simple models or is premised on the ability to access all training data at a central node. However, training data cannot be accessed on the server under the federated learning paradigm, conflicting with the requirements of the centralized unlearning process. Additionally, there are high computation and communication costs associated with accessing clients’ data, especially in scenarios involving numerous clients or complex global models. To address these concerns, we propose a more effective and efficient federated unlearning scheme based on the concept of model explanation. Model explanation involves understanding deep networks and individual channel importance, so that this understanding can be used to determine which model channels are critical for classes that need to be unlearned. We select the most influential channels within an already-trained model for the data that need to be unlearned and fine-tune only influential channels to remove the contribution made by those data. In this way, we can simultaneously avoid huge consumption costs and ensure that the unlearned model maintains good performance. Experiments with different training models on various datasets demonstrate the effectiveness of the proposed approach.},
  archive  = {J},
  author   = {Heng Xu and Tianqing Zhu and Lefeng Zhang and Wanlei Zhou and Philip S. Yu},
  doi      = {10.1109/TBDATA.2024.3409947},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {524-539},
  title    = {Update selective parameters: Federated machine unlearning based on model explanation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reinforcement learning framework for N-ary document-level relation extraction. <em>TBD</em>, <em>11</em>(2), 512-523. (<a href='https://doi.org/10.1109/TBDATA.2024.3410099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Knowledge Bases (KBs) have become more complex because some facts in KBs include more than two entities. The construction and completion of these KBs require a new relation extraction task to retrieve complex facts from the text. To address this issue, we present a new N-ary Document-Level relation extraction task that involves extracting relations that 1) include an arbitrary number of entities, and 2) can span multiple sentences within a document. This new task requires inferring relation labels and entity completeness, i.e., whether the entities in the document are (insufficient to describe the relation. We propose a reinforcement learning-based relation classifier training framework that can adapt most existing binary document-level relation extractors to this task. Extensive experimental evaluation demonstrates that our proposed framework is effective in reducing the impact of noise introduced by distant supervision or unrelated sentences in the document.},
  archive  = {J},
  author   = {Chenhan Yuan and Ryan Rossi and Andrew Katz and Hoda Eldardiry},
  doi      = {10.1109/TBDATA.2024.3410099},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {512-523},
  title    = {A reinforcement learning framework for N-ary document-level relation extraction},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate network alignment via consistency in node evolution. <em>TBD</em>, <em>11</em>(2), 499-511. (<a href='https://doi.org/10.1109/TBDATA.2024.3407543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Network alignment, which integrates multiple network resources by identifying anchor nodes that exist in different networks, is beneficial for conducting comprehensive network analysis. Although there have been many studies on network alignment, most of them are limited to static scenarios and only can achieve acceptable top-$\alpha$ ($\alpha &gt; 10$) results. In the absence of considering dynamic changes in networks, accurate network alignment (i.e., top-1 result) faces two problems: 1) Missing information: focusing solely on aligning networks at a specific time leads to low top-1 performance due to the lack of information from other time periods; 2) Confusing information: ignoring temporal information and focusing on aligning networks across the entire time span leads to low top-1 performance due to inability to distinguish the neighborhood nodes of anchor nodes. In this paper, we propose a dynamic network alignment method, which aims to achieve better top-1 alignment results with consider changing network structures over time. Towards this end, we learn the representations of nodes in the changing network structure with time, and preserve the consistency of anchor node pairs during the time-evolution process. First, we employ a Structure-Time-aware module to capture network dynamics while preserving network structure and learning node representations that incorporate temporal information. Second, we ensure the global and local consistency of anchor node pairs over time by utilizing linear and similarity functions, respectively. Finally, we determine whether two nodes are anchor node pairs by maintaining consistency between global, local, and node representations. Experimental results obtained from real-world datasets demonstrate that the proposed model achieves performance comparable to several state-of-the-art methods.},
  archive  = {J},
  author   = {Qiyao Peng and Yinghui Wang and Pengfei Jiao and Huaming Wu and Wenjun Wang},
  doi      = {10.1109/TBDATA.2024.3407543},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {499-511},
  title    = {Accurate network alignment via consistency in node evolution},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unsupervised projected sample selector for active learning. <em>TBD</em>, <em>11</em>(2), 485-498. (<a href='https://doi.org/10.1109/TBDATA.2024.3407545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Active learning, as a technique, aims to effectively label specific data points while operating within a designated query budget. Nevertheless, the majority of unsupervised active learning algorithms are based on shallow linear representation and lack sufficient interpretability. Furthermore, certain diversity-based methods face challenges in selecting samples that adequately represent the entire data distribution. Inspired by these reasons, in this paper, we propose an unsupervised active learning method on orthogonal projections to construct a deep neural network model. By optimizing the orthogonal projection process, we establish the connection between projection and active learning, consequently enhancing the interpretability of the proposed method. The proposed method can efficiently project the feature space onto a spanned subspace, deriving an indicator matrix while calculating the projection loss. Moreover, we consider the redundancy among samples to ensure both data point diversity and enhancement of clustering-based algorithms. Through extensive comparative experiments on six public datasets, the results demonstrate that the proposed method can effectively select more informative and representative samples and improve performance by up to 11%.},
  archive  = {J},
  author   = {Yueyang Pi and Yiqing Shi and Shide Du and Yang Huang and Shiping Wang},
  doi      = {10.1109/TBDATA.2024.3407545},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {485-498},
  title    = {Unsupervised projected sample selector for active learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). System identification with fourier transformation for long-term time series forecasting. <em>TBD</em>, <em>11</em>(2), 474-484. (<a href='https://doi.org/10.1109/TBDATA.2024.3407568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Time-series prediction has drawn considerable attention during the past decades fueled by the emerging advances of deep learning methods. However, most neural network based methods fail in extracting the hidden mechanism of the targeted physical system. To overcome these shortcomings, an interpretable sparse system identification method without any prior knowledge is proposed in this study. This method adopts the Fourier transform to reduces the irrelevant items in the dictionary matrix, instead of indiscriminate usage of polynomial functions in most system identification methods. It shows an visible system representation and greatly reduces computing cost. With the adoption of $l_{1}$ norm in regularizing the parameter matrix, a sparse description of the system model can be achieved. Moreover, three data sets including the water conservancy data, global temperature data and financial data are used to test the performance of the proposed method. Although no prior knowledge was known about the physical background, experimental results show that our method can achieve long-term prediction regardless of the noise and incompleteness in the original data more accurately than the widely-used baseline data-driven methods. This study may provide some insight into time-series prediction investigations, and suggests that a white-box system identification method may extract the easily overlooked yet inherent periodical features and may beat neural-network based black-box methods on long-term prediction tasks.},
  archive  = {J},
  author   = {Xiaoyi Liu and Duxin Chen and Wenjia Wei and Xia Zhu and Hao Shi and Wenwu Yu},
  doi      = {10.1109/TBDATA.2024.3407568},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {474-484},
  title    = {System identification with fourier transformation for long-term time series forecasting},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imperceptible physical attack against face recognition systems via LED illumination modulation. <em>TBD</em>, <em>11</em>(2), 461-473. (<a href='https://doi.org/10.1109/TBDATA.2024.3403377'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Although face recognition starts to play an important role in our daily life, we need to pay attention that data-driven face recognition vision systems are vulnerable to adversarial attacks. However, current digital adversarial attacks and physical adversarial attacks both have drawbacks, with the former ones impractical and the latter one conspicuous, high-computational and low-executable. To address the issues, we propose a practical, executable, stealthy and low computational adversarial attack based on LED illumination modulation. To fool the systems, the proposed attack generates physically imperceptible luminance changes to human eyes through fast intensity modulation of scene LED illumination and uses the rolling shutter effect of CMOS image sensors in face recognition systems to implant luminance information perturbation to the captured face images. In summary, we present a denial-of-service (DoS) attack for face detection and an evasion attack for face verification. We also evaluate their effectiveness against well-known face detection models, Dlib, MTCNN and RetinaFace, and face verification models, Dlib, FaceNet, and ArcFace. The extensive physical experiments show that the success rates of DoS attacks against face detection models reach 97.67$\%$, 100$\%$, and 100$\%$, respectively, and the success rates of evasion attacks against all face verification models reach 100$\%$.},
  archive  = {J},
  author   = {Junbin Fang and Canjian Jiang and You Jiang and Puxi Lin and Zhaojie Chen and Yujing Sun and Siu-Ming Yiu and Zoe L. Jiang},
  doi      = {10.1109/TBDATA.2024.3403377},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {461-473},
  title    = {Imperceptible physical attack against face recognition systems via LED illumination modulation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $ \tt {zkFL}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="monospace">zkFL</mml:mi></mml:math>: Zero-knowledge proof-based gradient aggregation for federated learning. <em>TBD</em>, <em>11</em>(2), 447-460. (<a href='https://doi.org/10.1109/TBDATA.2024.3403370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. FL can be a scalable machine learning solution in Big Data scenarios. Traditional FL relies on the trust assumption of the central aggregator, which forms cohorts of clients honestly. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or insert fake clients, to manipulate the final training results. In this work, we introduce $ \tt {zkFL}$, which leverages zero-knowledge proofs to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator provides a proof per round, demonstrating to the clients that the aggregator executes the intended behavior faithfully. To further reduce the verification cost of clients, we use blockchain to handle the proof in a zero-knowledge way, where miners (i.e., the participants validating and maintaining the blockchain data) can verify the proof without knowing the clients’ local and aggregated models. The theoretical analysis and empirical results show that $ \tt {zkFL}$ achieves better security and privacy than traditional FL, without modifying the underlying FL network structure or heavily compromising the training speed.},
  archive  = {J},
  author   = {Zhipeng Wang and Nanqing Dong and Jiahao Sun and William Knottenbelt and Yike Guo},
  doi      = {10.1109/TBDATA.2024.3403370},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {447-460},
  title    = {$ \tt {zkFL}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="monospace">zkFL</mml:mi></mml:math>: Zero-knowledge proof-based gradient aggregation for federated learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An open dataset of cyber asset graphs for cybercrime research. <em>TBD</em>, <em>11</em>(2), 438-446. (<a href='https://doi.org/10.1109/TBDATA.2024.3403371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Cybercrime poses a severe threat to the entire Internet ecosystem. Various cyber assets, such as domain name, IP address, and security certificate, are staple infrastructures of cybercrime. A cyber asset graph (CAG) is a collection of closely related cyber assets held by a cybercrime gang to support online criminal activities. Analyzing CAGs provides rich data insights for cybercrime investigation and governance. This paper introduces an open dataset of CAGs comprised of 2.37 million nodes with eight types of cyber assets and 3.28 million edges with eleven types of relations. This paper introduces the dataset construction process, applied areas, and the experience of using the dataset in the ChinaVis Data Challenge 2022. This dataset contains numerous CAGs of cybercrime gangs in the real world, which is the first open dataset of CAGs for cybercrime research. This dataset can also support the development of other application-oriented areas, such as cyber asset management and cyber-physical-social system, and various graph-related research areas, such as graph theory, graph mining, and graph visualization.},
  archive  = {J},
  author   = {Xin Zhao and Shaolong Li and Ying Zhao and Shuowen Fu and Yunpeng Chen and Fangfang Zhou and Xin Huang and Yuwei Li and Zhuo Chen},
  doi      = {10.1109/TBDATA.2024.3403371},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {438-446},
  title    = {An open dataset of cyber asset graphs for cybercrime research},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-evidence based fact verification via a confidential graph neural network. <em>TBD</em>, <em>11</em>(2), 426-437. (<a href='https://doi.org/10.1109/TBDATA.2024.3403382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Fact verification tasks aim to identify the integrity of textual contents according to the truthful corpus. Existing fact verification models usually build a fully connected reasoning graph, which regards claim-evidence pairs as nodes and connects them with edges. They employ the graph to propagate the semantics of the nodes. Nevertheless, the noisy nodes usually propagate their semantics via the edges of the reasoning graph, which misleads the semantic representations of other nodes and amplifies the noise signals. To mitigate the propagation of noisy semantic information, we introduce a Confidential Graph Attention Network (CO-GAT), which proposes a node masking mechanism for modeling the nodes. Specifically, CO-GAT calculates the node confidence score by estimating the relevance between the claim and evidence pieces. Then, the node masking mechanism uses the node confidence scores to control the noise information flow from the vanilla node to the other graph nodes. CO-GAT achieves a 73.59% FEVER score on the FEVER dataset and shows the generalization ability by broadening the effectiveness to the science-specific domain.},
  archive  = {J},
  author   = {Yuqing Lan and Zhenghao Liu and Yu Gu and Xiaoyuan Yi and Xiaohua Li and Liner Yang and Ge Yu},
  doi      = {10.1109/TBDATA.2024.3403382},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {426-437},
  title    = {Multi-evidence based fact verification via a confidential graph neural network},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning based anomaly detection approach for air pollution assessment. <em>TBD</em>, <em>11</em>(2), 414-425. (<a href='https://doi.org/10.1109/TBDATA.2024.3403392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Environmental air pollution has become a cause of global concern due to its adverse effects. Unusually high concentration of air pollutants can be regarded as an anomaly indicating certain air quality problems. This paper presents a deep learning based anomaly detection approach to identify anomalous concentrations of five different air pollutants: Carbon Monoxide ($CO$), Ozone ($O_{3}$), Nitogen Oxide ($NO_{X}$) and Particulate Matters ($PM_{2.5}$, $PM_{10}$) in a real-life environmental dataset. The collected data is multivariate in nature containing hourly generated information about several air pollutants and atmospheric parameters from a non-polluted city of India. The proposed framework contains a Bidirectional Long Short Term Memory (Bi-LSTM) based predictor model with self-attention to capture the normal pollutant levels in the time series dataset. The predictor model is responsible for predicting the value at the next timestamp, corresponding to a given window of the time series data. A subsequent anomaly detector is utilized to identify the anomalous pollutant levels based on the predictions of predictor model. Anomalies detected by the proposed framework are utilized to analyze the correlation of temporal and atmospheric parameters with the anomalous concentration levels. Experimental results illustrate the predominance of proposed approach over existing approaches towards air pollution assessment.},
  archive  = {J},
  author   = {Anindita Borah},
  doi      = {10.1109/TBDATA.2024.3403392},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {414-425},
  title    = {Deep learning based anomaly detection approach for air pollution assessment},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective personalized search with heterogeneous graph based hawkes process. <em>TBD</em>, <em>11</em>(2), 402-413. (<a href='https://doi.org/10.1109/TBDATA.2024.3399606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Personalized search aims at re-ranking search results with reference to users’ background information. The state-of-the-art personalized search methods often consider both the short-term search interests from current session behaviors and the long-term search interests from previous session behaviors. However, sessions in real-world search scenarios are usually very short, and a large number of sessions contain only one query, which makes it difficult to model short-term search interests. Intuitively, apart from current session behaviors, some recent historical session behaviors could also contribute to the current search interests, and the influence of these behaviors typically decays over time. Based on this intuition, we propose a novel heterogeneous graph based Hawkes process to improve the effectiveness of personalized search. Specifically, we first construct a heterogeneous graph to model multiple relations between users, queries, and documents. Then, we propose a heterogeneous graph neural network based algorithm to encode the representations of users’ historical search behaviors. After that, we develop a multivariate Hawkes process to capture the influence of historical search behaviors on the current search intent. Our approach can dynamically model the influence of historical behaviors in a continuous time space. Thus, both the current session behaviors and the historical session behaviors can be utilized to characterize a more accurate current search intent. We evaluate our method using three real-life datasets, and the results show that our approach significantly outperforms the state-of-the-art methods in terms of several widely-used precision metrics.},
  archive  = {J},
  author   = {Xiang Wu and Hongchao Qin and Rong-Hua Li and Yuchen Meng and Huanzhong Duan and Yanxiong Lu and Yujing Gao and Fusheng Jin and Guoren Wang},
  doi      = {10.1109/TBDATA.2024.3399606},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {402-413},
  title    = {Effective personalized search with heterogeneous graph based hawkes process},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep umbra: A generative approach for sunlight access computation in urban spaces. <em>TBD</em>, <em>11</em>(2), 388-401. (<a href='https://doi.org/10.1109/TBDATA.2024.3382964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Sunlight and shadow play critical roles in how urban spaces are utilized, thrive, and grow. While access to sunlight is essential to the success of urban environments, shadows can provide shaded places to stay during the hot seasons, mitigate heat island effect, and increase pedestrian comfort levels. Properly quantifying sunlight access and shadows in large urban environments is key in tackling some of the important challenges facing cities today. In this paper, we propose Deep Umbra, a novel computational framework that enables the quantification of sunlight access and shadows at a global scale. Our framework is based on a conditional generative adversarial network that considers the physical form of cities to compute high-resolution spatial information of accumulated sunlight access for the different seasons of the year. We use data from seven different cities to train our model, and show, through an extensive set of experiments, its low overall RMSE (below 0.1) as well as its extensibility to cities that were not part of the training set. Additionally, we contribute a set of case studies and a comprehensive dataset with sunlight access information for more than 100 cities across six continents of the world.},
  archive  = {J},
  author   = {Kazi Shahrukh Omar and Gustavo Moreira and Daniel Hodczak and Maryam Hosseini and Nicola Colaninno and Marcos Lage and Fabio Miranda},
  doi      = {10.1109/TBDATA.2024.3382964},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {388-401},
  title    = {Deep umbra: A generative approach for sunlight access computation in urban spaces},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal transformer network for weather forecasting. <em>TBD</em>, <em>11</em>(2), 372-387. (<a href='https://doi.org/10.1109/TBDATA.2024.3378061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Spatio-temporal neural networks have been successfully applied to weather forecasting tasks recently. The key notion is to learn spatio-temporal features concurrently from spatial and temporal dependencies. Existing methods are mainly based on local smoothness assumptions where the features are learned by accumulating information in local spatio-temporal regions. However, the weather conditions in a certain spatio-temporal region are usually influenced by global meteorological changes and long-range historical weather conditions. Therefore, these methods that ignore the large-scale spatio-temporal effects can hardly learn effective features. In this paper, we propose a novel spatio-temporal Transformer network in weather forecasting to address the above challenges. The main idea is to leverage the Transformer architecture to carefully capture the multi-scale spatial and long-range temporal information in weather data. First, we propose to combine the global and local position encodings based on absolute geographic locations and relative geodesic distances and insert them into the spatial Transformer to extract the multi-scale spatial information in meteorological graphs. Then, we further capture the long-range temporal dependencies by a temporal Transformer where the attention mechanism is used to improve the representation ability and scalability of the models. Extensive experiments over real weather datasets demonstrate the effectiveness of our framework.},
  archive  = {J},
  author   = {Junzhong Ji and Jing He and Minglong Lei and Muhua Wang and Wei Tang},
  doi      = {10.1109/TBDATA.2024.3378061},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {372-387},
  title    = {Spatio-temporal transformer network for weather forecasting},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly-supervised cross-domain segmentation of electron microscopy with sparse point annotation. <em>TBD</em>, <em>11</em>(2), 359-371. (<a href='https://doi.org/10.1109/TBDATA.2024.3378062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Accurate segmentation of organelle instances from electron microscopy (EM) images plays an essential role in many neuroscience researches. However, practical scenarios usually suffer from high annotation costs, label scarcity, and large domain diversity. While unsupervised domain adaptation (UDA) that assumes no annotation effort on the target data is promising to alleviate these challenges, its performance on complicated segmentation tasks is still far from practical usage. To address these issues, we investigate a highly annotation-efficient weak supervision, which assumes only sparse center-points on a small subset of object instances in the target training images. To achieve accurate segmentation with partial point annotations, we introduce instance counting and center detection as auxiliary tasks and design a multitask learning framework to leverage correlations among the counting, detection, and segmentation, which are all tasks with partial or no supervision. Building upon the different domain-invariances of the three tasks, we enforce counting estimation with a novel soft consistency loss as a global prior for center detection, which further guides the per-pixel segmentation. To further compensate for annotation sparsity, we develop a cross-position cut-and-paste for label augmentation and an entropy-based pseudo-label selection. The experimental results highlight that, by simply using extremely weak annotation, e.g., 15% sparse points, for model training, the proposed model is capable of significantly outperforming UDA methods and produces comparable performance as the supervised counterpart. The high robustness of our model shown in the validations and the low requirement of expert knowledge for sparse point annotation further improve the potential application value of our model.},
  archive  = {J},
  author   = {Dafei Qiu and Shan Xiong and Jiajin Yi and Jialin Peng},
  doi      = {10.1109/TBDATA.2024.3378062},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {359-371},
  title    = {Weakly-supervised cross-domain segmentation of electron microscopy with sparse point annotation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint multi-feature information entity alignment for cross-lingual temporal knowledge graph with BERT. <em>TBD</em>, <em>11</em>(2), 345-358. (<a href='https://doi.org/10.1109/TBDATA.2024.3378113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Entity alignment is the most critical technology of knowledge fusion, which aims to identify and align entities that exist in different knowledge graphs (KGs) but represent the same real-world objects. Temporal knowledge graphs (TKGs) extend static triples into quadruples by introducing temporal information, which is more in line with the dynamic changes of the real world and is often used to enhance the performance of various applications. However, the existing entity alignment methods mainly focus on traditional KGs, ignoring the temporal information contained in TKGs may lead to misalignment between some similar entities. The latest method enhances the performance of entity alignment by learning temporal information embedding, but it does not make full use of the advantages of temporal information. In this paper, we propose a new Entity Alignment method for Cross-lingual TKG with BERT (EACTB). EACTB uses the BERT model of multi-language training to learn the semantic relevance of entity description. For temporal information, we propose a new and more efficient method to calculate the similarity of temporal information. EACTB uses graph convolution network (GCN) to embed structural information. In addition, iterative method is used to deal with the problem of insufficient training datasets. Experimental results on three cross-lingual TKGs datasets show that EACTB is significantly superior to existing methods.},
  archive  = {J},
  author   = {Luyi Bai and Xiuting Song and Lin Zhu},
  doi      = {10.1109/TBDATA.2024.3378113},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {345-358},
  title    = {Joint multi-feature information entity alignment for cross-lingual temporal knowledge graph with BERT},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ConDTC: Contrastive deep trajectory clustering for fine-grained mobility pattern mining. <em>TBD</em>, <em>11</em>(2), 333-344. (<a href='https://doi.org/10.1109/TBDATA.2024.3362195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Trajectory clustering is a cornerstone task in the field of trajectory mining. With the proliferation of deep learning, deep trajectory clustering has been widely researched to mine mobility patterns from massive unlabeled trajectories. Nevertheless, existing methods mostly ignore trajectories’ temporal regularities, which are essential for mining fine-grained mobility patterns for applications including traveling group identification, transportation mode discovering, social security emergency, etc. To fill this gap, we propose ConDTC, a contrastive deep trajectory clustering method targeting for fine-grained mobility pattern mining. Specifically, we first design a spatial-temporal trajectory representation learning method which can capture both spatial and temporal regularities of trajectories synchronously. The proposed trajectory representation model can be used as a pre-trained model to serve various downstream trajectory mining tasks. Then, we construct a contrastive trajectory clustering module which optimizes trajectory representations and clustering performance simultaneously. Experimental results on three datasets validate that ConDTC can identify fine-grained mobility patterns by clustering trajectories with similar spatial-temporal mobility patterns together while separating those with different mobility patterns apart. Actually, ConDTC outperforms all state-of-the-art competitors substantially in terms of effectiveness, efficiency and robustness.},
  archive  = {J},
  author   = {Junjun Si and Jin Yang and Yang Xiang and Li Li and Bo Tu and Rongqing Zhang},
  doi      = {10.1109/TBDATA.2024.3362195},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {333-344},
  title    = {ConDTC: Contrastive deep trajectory clustering for fine-grained mobility pattern mining},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on truth discovery: Concepts, methods, applications, and opportunities. <em>TBD</em>, <em>11</em>(2), 314-332. (<a href='https://doi.org/10.1109/TBDATA.2024.3423677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the era of data information explosion, there are different observations on an object (e.g., the height of the Himalayas) from different sources on the web, social sensing, crowd sensing, and data sensing applications. Observations from different sources on an object can conflict with each other due to errors, missing records, typos, outdated data, etc. How to discover truth facts for objects from various sources is essential and urgent. In this paper, we aim to deliver a comprehensive and exhaustive survey on truth discovery problems from the perspectives of concepts, methods, applications, and opportunities. We first systematically review and compare problems from objects, sources, and observations. Based on these problem properties, different methods are analyzed and compared in depth from observation with single or multiple values, independent or dependent sources, static or dynamic sources, and supervised or unsupervised learning, followed by the surveyed applications in various scenarios. For future studies in truth discovery fields, we summarize the code sources and datasets used in above methods. Finally, we point out the potential challenges and opportunities on truth discovery, with the goal of shedding light and promoting further investigation in this area.},
  archive  = {J},
  author   = {Shuang Wang and He Zhang and Quan Z. Sheng and Xiaoping Li and Zhu Sun and Taotao Cai and Wei Emma Zhang and Jian Yang and Qing Gao},
  doi      = {10.1109/TBDATA.2024.3423677},
  journal  = {IEEE Transactions on Big Data},
  month    = {4},
  number   = {2},
  pages    = {314-332},
  title    = {A survey on truth discovery: Concepts, methods, applications, and opportunities},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust privacy-preserving federated item ranking in online marketplaces: Exploiting platform reputation for effective aggregation. <em>TBD</em>, <em>11</em>(1), 303-309. (<a href='https://doi.org/10.1109/TBDATA.2024.3505055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Online marketplaces often collect products to sell from several other platforms (and sellers) and produce a unique ranking/score of these products to users. Keeping as private the user preferences provided in each (individual) platform is a need and a challenge at the same time. We are currently used to rating items in the marketplace itself which, in turn, can produce more effective rankings. Hence, the shaping of an effective item ranking would require a sharing of the user ratings between the individual platforms and the marketplace, thus impacting users’ privacy. In this paper, we propose the initial steps towards a change of paradigm, where the ratings are kept as private in each platform. Under this paradigm, each platform produces its rankings, then aggregated by the marketplace, in a federated fashion. To ensure that the marketplace’s rankings maintain their effectiveness, we exploit the concept of reputation of the individual platform, so that the final marketplace ranking is weighted by the reputation of each platform providing its ranking. Experiments on three datasets, covering different use cases, show that our approach can produce effective rankings, improving robustness to attacks, while keeping user preference data private within each seller platform.},
  archive  = {J},
  author   = {Guilherme Ramos and Ludovico Boratto and Mirko Marras},
  doi      = {10.1109/TBDATA.2024.3505055},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {303-309},
  title    = {Robust privacy-preserving federated item ranking in online marketplaces: Exploiting platform reputation for effective aggregation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modality and equity-aware graph pooling fusion: A bike mobility prediction study. <em>TBD</em>, <em>11</em>(1), 286-302. (<a href='https://doi.org/10.1109/TBDATA.2024.3414280'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose an equity-aware GRAph-fusion differentiable Pooling neural network to accurately predict the spatio-temporal urban mobility (e.g., station-level bike usage in terms of departures and arrivals) with Equity (GRAPE). GRAPE consists of two independent hierarchical graph neural networks for two mobility systems—one as a target graph (i.e., a bike sharing system) and the other as an auxiliary graph (e.g., a taxi system). We have designed a convolutional fusion mechanism to jointly fuse the target and auxiliary graph embeddings and extract the shared spatial and temporal mobility patterns within the embeddings to enhance prediction accuracy. To further improve the equity of bike sharing systems for diverse communities, we focus on the bike resource allocation and model prediction performance, and propose to regularize the predicted bike resource as well as the accuracy across advantaged and disadvantaged communities, and thus mitigate the potential unfairness in the predicted bike sharing usage. Our evaluation of over 23 million bike rides and 100 million taxi trips in New York City and Chicago has demonstrated GRAPE to outperform all of the baseline approaches in terms of prediction accuracy (by 15.80% for NYC and 50.55% for Chicago on average) and social equity awareness (by 32.44% and 24.43% in terms of resource fairness for NYC and Chicago, and 13.36% and 16.52% in terms of performance fairness).},
  archive  = {J},
  author   = {Xi Yang and Suining He and Kang G. Shin and Mahan Tabatabaie and Jing Dai},
  doi      = {10.1109/TBDATA.2024.3414280},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {286-302},
  title    = {Cross-modality and equity-aware graph pooling fusion: A bike mobility prediction study},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficiently transfer user profile across networks. <em>TBD</em>, <em>11</em>(1), 271-285. (<a href='https://doi.org/10.1109/TBDATA.2024.3414321'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {User profiling has very important applications for many downstream tasks. Most existing methods only focus on modeling user profiles of one social network with plenty of data. However, user profiles are difficult to acquire, especially when the data is scarce. Fortunately, we observed that similar users have similar behavior patterns in different social networks. Motivated by such observations, in this paper, we for the first time propose to study the user profiling problem from the transfer learning perspective. We design two efficient frameworks for User Profile transferring acrOss Networks, i.e., UPON and E-UPON. In UPON, we first design a novel graph convolutional networks based characteristic-aware domain attention model to find user dependencies within and between domains (i.e., social networks). We then design a dual-domain weighted adversarial learning method to address the domain shift problem existing in the transferring procedure. In E-UPON, we optimize UPON in terms of computational complexity and memory. Specifically, we design a mini-cluster gradient descent based graph representation algorithm to shrink the searching space and ensure parallel computation. Then we use an adaptive cluster matching method to adjust the clusters of users. Experimental results on Twitter-Foursquare dataset demonstrate that UPON and E-UPON outperform the state-of-the-art models.},
  archive  = {J},
  author   = {Mengting Diao and Zhongbao Zhang and Sen Su and Shuai Gao and Huafeng Cao and Junda Ye},
  doi      = {10.1109/TBDATA.2024.3414321},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {271-285},
  title    = {Efficiently transfer user profile across networks},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic recognition of cyberbullying in the web of things and social media using deep learning framework. <em>TBD</em>, <em>11</em>(1), 259-270. (<a href='https://doi.org/10.1109/TBDATA.2024.3409939'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Web of Things (WoT) is a network that facilitates the formation and distribution of information its users make. Young people nowadays, digital natives, have no trouble relating to others or joining groups online since they have grown up in a world where new technology has pushed communications to a nearly real-time level. Shared private messages, rumours, and sexual comments are all examples of online harassment that have led to several recent cases worldwide. Therefore, academics have been more interested in finding ways to recognise bullying conduct on these platforms. The effects of cyberbullying, a terrible form of online misbehaviour, are distressing. It takes several documents, but the text is predominant on social networks. Intelligent systems are required for the automatic detection of such occurrences. Most previous research has used standard machine-learning techniques to tackle this issue. The increasing pervasiveness of cyberbullying in WoT and other social media platforms is a significant cause for worry that calls for robust responses to prevent further harm. This study offers a unique method of leveraging the deep learning (DL) model binary coyote optimization-based Convolutional Neural Network (BCNN) in social networks to identify and classify cyberbullying. An essential part of this method is the combination of DL-based abuse detection and feature subset selection. To efficiently detect and address cases of cyberbullying via social media, the proposed system incorporates many crucial steps, including preprocessing, feature selection, and classification. A binary coyote optimization (BCO)-based feature subset selection method is presented to enhance classification efficiency. To improve the accuracy of cyberbullying categorization, the BCO algorithm efficiently chooses a selection of key characteristics. Cyberbullying must be tracked and classified across all internet channels, and Convolutional Neural Network (CNN) is constructed. With a best-case accuracy of 99.5% on Formspring, 99.7% on Twitter, and 99.3% on Wikipedia, the suggested algorithm successfully identified the vast majority of cyberbullying content.},
  archive  = {J},
  author   = {Fahd N. Al-Wesabi and Marwa Obayya and Jamal Alsamri and Rana Alabdan and Nojood O Aljehane and Sana Alazwari and Fahad F. Alruwaili and Manar Ahmed Hamza and A Swathi},
  doi      = {10.1109/TBDATA.2024.3409939},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {259-270},
  title    = {Automatic recognition of cyberbullying in the web of things and social media using deep learning framework},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CompanyKG: A large-scale heterogeneous graph for company similarity quantification. <em>TBD</em>, <em>11</em>(1), 247-258. (<a href='https://doi.org/10.1109/TBDATA.2024.3407573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the investment industry, it is often essential to carry out fine-grained company similarity quantification for a range of purposes, including market mapping, competitor analysis, and mergers and acquisitions. We propose and publish a knowledge graph, named CompanyKG, to represent and learn diverse company features and relations. Specifically, 1.17 million companies are represented as nodes enriched with company description embeddings; and 15 different inter-company relations result in 51.06 million weighted edges. To enable a comprehensive assessment of methods for company similarity quantification, we have devised and compiled three evaluation tasks with annotated test sets: similarity prediction, competitor retrieval and similarity ranking. We present extensive benchmarking results for 11 reproducible predictive methods categorized into three groups: node-only, edge-only, and node+edge. To the best of our knowledge, CompanyKG is the first large-scale heterogeneous graph dataset originating from a real-world investment platform, tailored for quantifying inter-company similarity.},
  archive  = {J},
  author   = {Lele Cao and Vilhelm von Ehrenheim and Mark Granroth-Wilding and Richard Anselmo Stahl and Andrew McCornack and Armin Catovic and Dhiana Deva Cavalcanti Rocha},
  doi      = {10.1109/TBDATA.2024.3407573},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {247-258},
  title    = {CompanyKG: A large-scale heterogeneous graph for company similarity quantification},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication-efficient distributed learning via sparse and adaptive stochastic gradient. <em>TBD</em>, <em>11</em>(1), 234-246. (<a href='https://doi.org/10.1109/TBDATA.2024.3407510'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Gradient-based optimization methods implemented on distributed computing architectures are increasingly used to tackle large-scale machine learning applications. A key bottleneck in such distributed systems is the high communication overhead for exchanging information, such as stochastic gradients, between workers. The inherent causes of this bottleneck are the frequent communication rounds and the full model gradient transmission in every round. In this study, we present SASG, a communication-efficient distributed algorithm that enjoys the advantages of sparse communication and adaptive aggregated stochastic gradients. By dynamically determining the workers who need to communicate through an adaptive aggregation rule and sparsifying the transmitted information, the SASG algorithm reduces both the overhead of communication rounds and the number of communication bits in the distributed system. For the theoretical analysis, we introduce an important auxiliary variable and define a new Lyapunov function to prove that the communication-efficient algorithm is convergent. The convergence result is identical to the sublinear rate of stochastic gradient descent, and our result also reveals that SASG scales well with the number of distributed workers. Finally, experiments on training deep neural networks demonstrate that the proposed algorithm can significantly reduce communication overhead compared to previous methods.},
  archive  = {J},
  author   = {Xiaoge Deng and Dongsheng Li and Tao Sun and Xicheng Lu},
  doi      = {10.1109/TBDATA.2024.3407510},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {234-246},
  title    = {Communication-efficient distributed learning via sparse and adaptive stochastic gradient},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Training large-scale graph neural networks via graph partial pooling. <em>TBD</em>, <em>11</em>(1), 221-233. (<a href='https://doi.org/10.1109/TBDATA.2024.3403380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Graph Neural Networks (GNNs) are powerful tools for graph representation learning, but they face challenges when applied to large-scale graphs due to substantial computational costs and memory requirements. To address scalability limitations, various methods have been proposed, including sampling-based and decoupling-based methods. However, these methods have their limitations: sampling-based methods inevitably discard some link information during the sampling process, while decoupling-based methods require alterations to the model's structure, reducing their adaptability to various GNNs. This paper proposes a novel graph pooling method, Graph Partial Pooling (GPPool), for scaling GNNs to large-scale graphs. GPPool is a versatile and straightforward technique that enhances training efficiency while simultaneously reducing memory requirements. GPPool constructs small-scale pooled graphs by pooling partial nodes into supernodes. Each pooled graph consists of supernodes and unpooled nodes, preserving valuable local and global information. Training GNNs on these graphs reduces memory demands and enhances their performance. Additionally, this paper provides a theoretical analysis of training GNNs using GPPool-constructed graphs from a graph diffusion perspective. It shows that a GNN can be transformed from a large-scale graph into pooled graphs with minimal approximation error. A series of experiments on datasets of varying scales demonstrates the effectiveness of GPPool.},
  archive  = {J},
  author   = {Qi Zhang and Yanfeng Sun and Shaofan Wang and Junbin Gao and Yongli Hu and Baocai Yin},
  doi      = {10.1109/TBDATA.2024.3403380},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {221-233},
  title    = {Training large-scale graph neural networks via graph partial pooling},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TAMT: Privacy-preserving task assignment with multi-threshold range search for spatial crowdsourcing applications. <em>TBD</em>, <em>11</em>(1), 208-220. (<a href='https://doi.org/10.1109/TBDATA.2024.3403374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Spatial crowdsourcing is a distributed computing paradigm that utilizes the collective intelligence of workers to perform complex tasks. How to achieve privacy-preserving task assignment in spatial crowdsourcing applications has been a popular research area. However, most of the existing task assignment schemes may reveal private and sensitive information of tasks or workers. Few schemes can support task assignment based on different attributes simultaneously, such as spatial, interest, etc. To study the above themes, in this paper, we propose one privacy-preserving task assignment scheme with multi-threshold range search for spatial crowdsourcing applications (TAMT). Specifically, we first define euclidean distance-based location search and Hamming distance-based interest search, which map the demands of the tasks and the interests of the workers into the binary vectors. Second, we deploy PKD-tree to index the task data leveraging the pivoting techniques and the triangular inequality of euclidean distance, and propose an efficient multi-threshold range search algorithm based on matrix encryption and decomposition technology. Furthermore, based on DT-PKC, we introduce a ciphertext-based secure comparison protocol to support multi-threshold range search for spatial crowdsourcing applications. Finally, comprehensive security analysis proves that our proposed TAMT is privacy-preserving. Meanwhile, theoretical analysis and experimental evaluation demonstrate that TAMT is practical and efficient.},
  archive  = {J},
  author   = {Haiyong Bao and Zhehong Wang and Rongxing Lu and Cheng Huang and Beibei Li},
  doi      = {10.1109/TBDATA.2024.3403374},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {208-220},
  title    = {TAMT: Privacy-preserving task assignment with multi-threshold range search for spatial crowdsourcing applications},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A universal and efficient multi-modal smart contract vulnerability detection framework for big data. <em>TBD</em>, <em>11</em>(1), 190-207. (<a href='https://doi.org/10.1109/TBDATA.2024.3403376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A vulnerability or error in a smart contract will lead to serious consequences including loss of assets and leakage of user privacy. Established smart contract vulnerability detection tools define vulnerabilities through symbolic execution, fuzz testing, and other methods requiring extremely specialized security knowledge. Even so, with the development of vulnerability exploitation techniques, vulnerability detection tools customized by experts cannot cope with the deformation of existing vulnerabilities or unknown vulnerabilities. The vulnerability detection based on machine learning developed in recent years studies vulnerabilities from different dimensions and designs corresponding models to achieve a high detection rate. However, these methods usually only focus on some features of smart contracts, or the model itself does not have universality. Experimental results on the publicly large-scale dataset SmartBugs-Wild demonstrate that this paper's method not only outperforms existing methods in several metrics, but also is scalable, general, and requires less domain knowledge, providing a new idea for the development of smart contract vulnerability detection.},
  archive  = {J},
  author   = {Wenjuan Lian and Zikang Bao and Xinze Zhang and Bin Jia and Yang Zhang},
  doi      = {10.1109/TBDATA.2024.3403376},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {190-207},
  title    = {A universal and efficient multi-modal smart contract vulnerability detection framework for big data},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallelly running and privacy-preserving agglomerative hierarchical clustering in outsourced cloud computing environments. <em>TBD</em>, <em>11</em>(1), 174-189. (<a href='https://doi.org/10.1109/TBDATA.2024.3403375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As a Big Data analysis technique, hierarchical clustering is helpful in summarizing data since it returns the clusters of the data and their clustering history. Cloud computing is the most suitable option to efficiently perform hierarchical clustering over numerous data. However, since compromised cloud service providers can cause serious privacy problems by revealing data, it is necessary to solve the problems prior to using the external cloud computing service. Privacy-preserving hierarchical clustering protocol in an outsourced computing environment has never been proposed in existing works. Existing protocols have several problems that limit the number of participating data owners or disclose the information of data. In this article, we propose a parallelly running and privacy-preserving agglomerative hierarchical clustering (ppAHC) over the union of datasets of multiple data owners in an outsourced computing environment, which is the first protocol to the best of our knowledge. The proposed ppAHC does not disclose any information about input and output, including the data access patterns. The proposed ppAHC is highly efficient and suitable for Big Data analysis to handle numerous data since its cost for one round is independent of the amount of data. It allows data owners without sufficient computing capability to participate in a collaborative hierarchical clustering.},
  archive  = {J},
  author   = {Jeongsu Park and Dong Hoon Lee},
  doi      = {10.1109/TBDATA.2024.3403375},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {174-189},
  title    = {Parallelly running and privacy-preserving agglomerative hierarchical clustering in outsourced cloud computing environments},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards real-time network intrusion detection with image-based sequential packets representation. <em>TBD</em>, <em>11</em>(1), 157-173. (<a href='https://doi.org/10.1109/TBDATA.2024.3403394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Machine learning (ML) and deep learning (DL) advancements have greatly enhanced anomaly detection of network intrusion detection systems (NIDS) by empowering them to analyze Big Data and extract patterns. ML/DL-based NIDS are trained using either flow-based or packet-based features. Flow-based NIDS are suitable for offline traffic analysis, while packet-based NIDS can analyze traffic and detect attacks in real-time. Current packet-based approaches analyze packets independently, overlooking the sequential nature of network communication. This results in biased models that exhibit increased false negatives and positives. Additionally, most literature-proposed packet-based NIDS capture only payload data, neglecting crucial information from packet headers. This oversight can impair the ability to identify header-level attacks, such as denial-of-service attacks. To address these limitations, we propose a novel artificial intelligence-enabled methodological framework for packet-based NIDS that effectively analyzes header and payload data and considers temporal connections among packets. Our framework transforms sequential packets into two-dimensional images. It then develops a convolutional neural network-based intrusion detection model to process these images and detect malicious activities. Through experiments using publicly available big datasets, we demonstrate that our framework is able to achieve high detection rates of 97.7% to 99% across different attack types and displays promising resilience against adversarial examples.},
  archive  = {J},
  author   = {Jalal Ghadermazi and Ankit Shah and Nathaniel D. Bastian},
  doi      = {10.1109/TBDATA.2024.3403394},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {157-173},
  title    = {Towards real-time network intrusion detection with image-based sequential packets representation},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-enabled secure collaborative model learning using differential privacy for IoT-based big data analytics. <em>TBD</em>, <em>11</em>(1), 141-156. (<a href='https://doi.org/10.1109/TBDATA.2024.3394700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {With the rise of Big data generated by Internet of Things (IoT) smart devices, there is an increasing need to leverage its potential while protecting privacy and maintaining confidentiality. Privacy and confidentiality in Big Data aims to enable data analysis and machine learning on large-scale datasets without compromising the dataset sensitive information. Usually current Big Data analytics models either efficiently achieves privacy or confidentiality. In this article, we aim to design a novel blockchain-enabled secured collaborative machine learning approach that provides privacy and confidentially on large scale datasets generated by IoT devices. Blockchain is used as secured platform to store and access data as well as to provide immutability and traceability. We also propose an efficient approach to obtain robust machine learning model through use of cryptographic techniques and differential privacy in which the data among involved parties is shared in a secured way while maintaining privacy and confidentiality of the data. The experimental evaluation along with security and performance analysis show that the proposed approach provides accuracy and scalability without compromising the privacy and security.},
  archive  = {J},
  author   = {Prakash Tekchandani and Abhishek Bisht and Ashok Kumar Das and Neeraj Kumar and Marimuthu Karuppiah and Pandi Vijayakumar and Youngho Park},
  doi      = {10.1109/TBDATA.2024.3394700},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {141-156},
  title    = {Blockchain-enabled secure collaborative model learning using differential privacy for IoT-based big data analytics},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3A multi-classification division-aggregation framework for fake news detection. <em>TBD</em>, <em>11</em>(1), 130-140. (<a href='https://doi.org/10.1109/TBDATA.2024.3378098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nowadays, as human activities are shifting to social media, fake news detection has been a crucial problem. Existing methods ignore the classification difference in online news and cannot take full advantage of multi-classification knowledges. For example, when coping with a post “A mouse is frightened by a cat,” a model that learns “computer” knowledge tends to misunderstand “mouse” and give a fake label, but a model that learns “animal” knowledge tends to give a true label. Therefore, this research proposes a multi-classification division-aggregation framework to detect fake news, named $CKA$, which innovatively learns classification knowledges during training stages and aggregates them during prediction stages. It consists of three main components: a news characterizer, an ensemble coordinator, and a truth predictor. The news characterizer is responsible for extracting news features and obtaining news classifications. Cooperating with the news characterizer, the ensemble coordinator generates classification-specifical models for the maximum reservation of classification knowledges during the training stage, where each classification-specifical model maximizes the detection performance of fake news on corresponding news classifications. Further, to aggregate the classification knowledges during the prediction stage, the truth predictor uses the truth discovery technology to aggregate the predictions from different classification-specifical models based on reliability evaluation of classification-specifical models. Extensive experiments prove that our proposed $CKA$ outperforms state-of-the-art baselines in fake news detection.},
  archive  = {J},
  author   = {Wen Zhang and Haitao Fu and Huan Wang and Zhiguo Gong and Pan Zhou and Di Wang},
  doi      = {10.1109/TBDATA.2024.3378098},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {130-140},
  title    = {3A multi-classification division-aggregation framework for fake news detection},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Heterogeneous social event detection via hyperbolic graph representations. <em>TBD</em>, <em>11</em>(1), 115-129. (<a href='https://doi.org/10.1109/TBDATA.2024.3381017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Social events reflect the dynamics of society and, here, natural disasters and emergencies receive significant attention. The timely detection of these events can provide organisations and individuals with valuable information to reduce or avoid losses. However, due to the complex heterogeneities of the content and structure of social media, existing models can only learn limited information; large amounts of semantic and structural information are ignored. In addition, due to high labour costs, it is rare for social media datasets to include high-quality labels, which also makes it challenging for models to learn information from social media. In this study, we propose two hyperbolic graph representation-based methods for detecting social events from heterogeneous social media environments. For cases where a dataset has labels, we design a Hyperbolic Social Event Detection (HSED) model that converts complex social information into a unified social message graph. This model addresses the heterogeneity of social media, and, with this graph, the information in social media can be used to capture structural information based on the properties of hyperbolic space. For cases where the dataset is unlabelled, we design an Unsupervised Hyperbolic Social Event Detection (UHSED). This model is based on the HSED model but includes graph contrastive learning to make it work in unlabelled scenarios. Extensive experiments demonstrate the superiority of the proposed approaches.},
  archive  = {J},
  author   = {Zitai Qiu and Jia Wu and Jian Yang and Xing Su and Charu Aggarwal},
  doi      = {10.1109/TBDATA.2024.3381017},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {115-129},
  title    = {Heterogeneous social event detection via hyperbolic graph representations},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature subspace learning-based binary differential evolution algorithm for unsupervised feature selection. <em>TBD</em>, <em>11</em>(1), 99-114. (<a href='https://doi.org/10.1109/TBDATA.2024.3378090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {It is a challenging task to select the informative features that can maintain the manifold structure in the original feature space. Many unsupervised feature selection methods still suffer the poor cluster performance in the selected feature subset. To tackle this problem, a feature subspace learning-based binary differential evolution algorithm is proposed for unsupervised feature selection. First, a new unsupervised feature selection framework based on evolutionary computation is designed, in which the feature subspace learning and the population search mechanism are combined into a unified unsupervised feature selection. Second, a local manifold structure learning strategy and a sample pseudo-label learning strategy are presented to calculate the importance of the selected feature subspace. Third, the binary differential evolution algorithm is developed to optimize the selected feature subspace, in which the binary information migration mutation operator and the adaptive crossover operator are designed to promote the searching for the global optimal feature subspace. Experimental results on various types of real-world datasets demonstrate that the proposed algorithm can obtain more informative feature subset and competitive cluster performance compared with eight state-of-the-art unsupervised feature selection methods.},
  archive  = {J},
  author   = {Tao Li and Yuhua Qian and Feijiang Li and Xinyan Liang and Zhi-Hui Zhan},
  doi      = {10.1109/TBDATA.2024.3378090},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {99-114},
  title    = {Feature subspace learning-based binary differential evolution algorithm for unsupervised feature selection},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning from crowds using graph neural networks with attention mechanism. <em>TBD</em>, <em>11</em>(1), 86-98. (<a href='https://doi.org/10.1109/TBDATA.2024.3378100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Crowdsourcing has been playing an essential role in machine learning since it can obtain a large number of labels in an economical and fast manner for training increasingly complex learning models. However, the application of crowdsourcing learning still faces several challenges such as the low quality of crowd labels and the urgent requirement for learning models adapting to the label noises. There have been many studies focusing on truth inference algorithms to improve the quality of labels obtained by crowdsourcing. Comparably, end-to-end predictive model learning in crowdsourcing scenarios, especially using cutting-edge deep learning techniques, is still in its infant stage. In this paper, we propose a novel graph convolutional network-based framework, namely CGNNAT, which models the correlation of instances by combining the GCN model with an attention mechanism to learn more representative node embeddings for a better understanding of the bias tendency of crowd workers. Furthermore, a specific projection processing layer is employed in CGNNAT to model the reliability of each crowd worker, which makes the model an end-to-end neural network directly trained by noisy crowd labels. Experimental results on several real-world and synthetic datasets show that the proposed CGNNAT outperforms state-of-the-art and classical methods in terms of label prediction.},
  archive  = {J},
  author   = {Jing Zhang and Ming Wu and Zeyi Sun and Cangqi Zhou},
  doi      = {10.1109/TBDATA.2024.3378100},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {86-98},
  title    = {Learning from crowds using graph neural networks with attention mechanism},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A distributed generative adversarial network for data augmentation under vertical federated learning. <em>TBD</em>, <em>11</em>(1), 74-85. (<a href='https://doi.org/10.1109/TBDATA.2024.3375150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Vertical federated learning can aggregate participant data features. To address the issue of insufficient overlapping data in vertical federated learning, this study presents a generative adversarial network model that allows distributed data augmentation. First, this study proposes a distributed generative adversarial network FeCGAN for multiple participants with insufficient overlapping data, considering the fact that the generative adversarial network can generate simulation samples. This network is suitable for multiple data sources and can augment participants’ local data. Second, to address the problem of learning divergence caused by different local distributions of multiple data sources, this study proposes the aggregation algorithm FedKL. It aggregates the feedback of the local discriminator to interact with the generator and learns the local data distribution more accurately. Finally, given the problem of data waste caused by the unavailability of nonoverlapping data, this study proposes a data augmentation method called VFeDA. It uses FeCGAN to generate pseudo features and expands more overlapping data, thereby improving the data use. Experiments showed that the proposed model is suitable for multiple data sources and can generate high-quality data.},
  archive  = {J},
  author   = {Yunpeng Xiao and Xufeng Li and Tun Li and Rong Wang and Yucai Pang and Guoyin Wang},
  doi      = {10.1109/TBDATA.2024.3375150},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {74-85},
  title    = {A distributed generative adversarial network for data augmentation under vertical federated learning},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PR3: Reversible and usability-enhanced visual privacy protection via thumbnail preservation and data hiding. <em>TBD</em>, <em>11</em>(1), 59-73. (<a href='https://doi.org/10.1109/TBDATA.2024.3375155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The image hosting platform is becoming increasingly popular due to its user-friendly features, but it is prone to causing privacy concerns. Only protecting privacy, in fact, can be easy to come true, but usability is frequently sacrificed. Visual privacy protection schemes aim to make a balance between privacy and usability, whereas they are often irreversible. Recently, some reversible visual privacy protection schemes have been proposed by preserving thumbnails (known as TPE). However, they either have excessive states in the Markov chain modeled by the scheme or cannot reverse losslessly. Meanwhile, images encrypted by existing TPE schemes can not embed additional information and thus the usability is limited to visual observation. In view of this, we pertinently propose a reversible and usability-enhanced visual privacy protection scheme (called PR3) based on thumbnail preservation and data hiding. In this scheme, we utilize the sum-preserving data embedding algorithm to substitute the the lowest seven bits of the image without changing the sum. Any data overflow resulting from the above process is stored in the vacated space of the most significant bits. The remaining space serves two purposes: embedding additional information and adjusting the image to approximate the thumbnail. Compared with existing TPE works, PR3 has fewer states in the Markov chain and supports lossless recovery of images. In addition, additional information can be embedded in the encrypted image to enhance usability.},
  archive  = {J},
  author   = {Ruoyu Zhao and Yushu Zhang and Wenying Wen and Xinpeng Zhang and Xiaochun Cao and Yong Xiang},
  doi      = {10.1109/TBDATA.2024.3375155},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {59-73},
  title    = {PR3: Reversible and usability-enhanced visual privacy protection via thumbnail preservation and data hiding},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FuzzyPPI: Large-scale interaction of human proteome at fuzzy semantic space. <em>TBD</em>, <em>11</em>(1), 47-58. (<a href='https://doi.org/10.1109/TBDATA.2024.3375149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Large-scale protein-protein interaction (PPI) network of an organism provides key insights into its cellular and molecular functionalities, signaling pathways and underlying disease mechanisms. For any organism, the total unexplored protein interactions significantly outnumbers all known positive and negative interactions. For Human, all known PPI datasets contain only $\sim\!\! 5.61$ million positive and $\sim\!\! 0.76$ million negative interactions, which is $\sim\!\! 3.1$% of potential interactions. We have implemented a distributed algorithm in Apache Spark that evaluates a Human PPI network of $\sim \!\! 180$ million potential interactions resulting from 18 994 reviewed proteins for which Gene Ontology (GO) annotations are available. The computed scores have been validated against state-of-the-art methods on benchmark datasets. FuzzyPPI performed significantly better with an average F1 score of 0.62 compared to GOntoSim (0.39), GOGO (0.38), and Wang (0.38) when tested with the Gold Standard PPI Dataset. The resulting scores are published with a web server for non-commercial use at http://fuzzyppi.mimuw.edu.pl/. Moreover, conventional PPI prediction methods produce binary results, but in fact this is just a simplification as PPIs have strengths or probabilities and recent studies show that protein binding affinities may prove to be effective in detecting protein complexes, disease association analysis, signaling network reconstruction, etc. Keeping these in mind, our algorithm is based on a fuzzy semantic scoring function and produces probabilities of interaction.},
  archive  = {J},
  author   = {Anup Kumar Halder and Soumyendu Sekhar Bandyopadhyay and Witold Jedrzejewski and Subhadip Basu and Jacek Sroka},
  doi      = {10.1109/TBDATA.2024.3375149},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {47-58},
  title    = {FuzzyPPI: Large-scale interaction of human proteome at fuzzy semantic space},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid optimization algorithm for detection of security attacks in IoT-enabled cyber-physical systems. <em>TBD</em>, <em>11</em>(1), 35-46. (<a href='https://doi.org/10.1109/TBDATA.2024.3372368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The Internet of Things (IoT) is being prominently used in smart cities and a wide range of applications in society. The benefits of IoT are evident, but cyber terrorism and security concerns inhibit many organizations and users from deploying it. Cyber-physical systems that are IoT-enabled might be difficult to secure since security solutions designed for general information/operational technology systems may not work as well in an environment. Thus, deep learning (DL) can assist as a powerful tool for building IoT-enabled cyber-physical systems with automatic anomaly detection. In this paper, two distinct DL models have been employed i.e., Deep Belief Network (DBN) and Convolutional Neural Network (CNN), considered hybrid classifiers, to create a framework for detecting attacks in IoT-enabled cyber-physical systems. However, DL models need to be trained in such a way that will increase their classification accuracy. Therefore, this paper also aims to present a new hybrid optimization algorithm called “Seagull Adapted Elephant Herding Optimization” (SAEHO) to tune the weights of the hybrid classifier. The “Hybrid Classifier + SAEHO” framework takes the feature extracted dataset as an input and classifies the network as either attack or benign. Using sensitivity, precision, accuracy, and specificity, two datasets were compared. In every performance metric, the proposed framework outperforms conventional methods.},
  archive  = {J},
  author   = {Amit Sagu and Nasib Singh Gill and Preeti Gulia and Ishaani Priyadarshini and Jyotir Moy Chatterjee},
  doi      = {10.1109/TBDATA.2024.3372368},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {35-46},
  title    = {Hybrid optimization algorithm for detection of security attacks in IoT-enabled cyber-physical systems},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-DPSDG: An edge-based differential privacy protection model for smart healthcare. <em>TBD</em>, <em>11</em>(1), 21-34. (<a href='https://doi.org/10.1109/TBDATA.2024.3366071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The edge computing paradigm has revolutionized the healthcare sector, providing more real-time medical data processing and analysis, which also poses more serious privacy and security risks that must be carefully considered and addressed. Based on differential privacy, we presented an innovative privacy-preserving model named Edge-DPSDG (Edge-Differentially Private Synthetic Data Generator) for smart healthcare under edge computing. It also develops and evolves a privacy budget allocation mechanism. In a distributed environment, the privacy budget for local medical data is personalized by computing the Shapley value and the information entropy value of each attribute in the dataset, which takes into account the trade-off between data privacy and utility. Extensive experiments on three public medical datasets are performed to evaluate the performance of Edge-DPSDG on two metrics. For utility evaluation, Edge-DPSDG shows a best 21.29% accuracy improvement compared to the state-of-the-art; our privacy budget allocation mechanism improved existing models’ accuracy by up to 6.05%. For privacy evaluation, Edge-DPSDG shows that can effectively ensure the privacy of the original datasets. In addition, Edge-DPSDG helps smooth the data, and results in a 3.99% accuracy loss decrease over the non-private model.},
  archive  = {J},
  author   = {Moli Lyu and Zhiwei Ni and Qian Chen and Fenggang Li},
  doi      = {10.1109/TBDATA.2024.3366071},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {21-34},
  title    = {Edge-DPSDG: An edge-based differential privacy protection model for smart healthcare},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-centric graph learning: A survey. <em>TBD</em>, <em>11</em>(1), 1-20. (<a href='https://doi.org/10.1109/TBDATA.2024.3489412'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The history of artificial intelligence (AI) has witnessed the significant impact of high-quality data on various deep learning models, such as ImageNet for AlexNet and ResNet. Recently, instead of designing more complex neural architectures as model-centric approaches, the attention of AI community has shifted to data-centric ones, which focuses on better processing data to strengthen the ability of neural models. Graph learning, which operates on ubiquitous topological data, also plays an important role in the era of deep learning. In this survey, we comprehensively review graph learning approaches from the data-centric perspective, and aim to answer three crucial questions: (1) when to modify graph data, (2) what part of the graph data needs modification to unlock the potential of various graph models, and (3) how to safeguard graph models from problematic data influence. Accordingly, we propose a novel taxonomy based on the stages in the graph learning pipeline, and highlight the processing methods for different data structures in the graph data, i.e., topology, feature and label. Furthermore, we analyze some potential problems embedded in graph data and discuss how to solve them in a data-centric manner. Finally, we provide some promising future directions for data-centric graph learning.},
  archive  = {J},
  author   = {Yuxin Guo and Deyu Bo and Cheng Yang and Zhiyuan Lu and Zhongjian Zhang and Jixi Liu and Yufei Peng and Chuan Shi},
  doi      = {10.1109/TBDATA.2024.3489412},
  journal  = {IEEE Transactions on Big Data},
  month    = {2},
  number   = {1},
  pages    = {1-20},
  title    = {Data-centric graph learning: A survey},
  volume   = {11},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
