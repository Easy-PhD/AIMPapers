<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCDS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcds">TCDS - 103</h2>
<ul>
<li><details>
<summary>
(2025). Multimodal discriminative network for emotion recognition across individuals. <em>TCDS</em>, <em>17</em>(5), 1323--1335. (<a href='https://doi.org/10.1109/TCDS.2025.3552124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal emotion recognition is gaining significant attention for ability to fuse complementary information from diverse physiological and behavioral signals, which benefits the understanding of emotional disorders. However, challenges arise in multimodal fusion due to uncertainties inherent in different modalities, such as complex signal coupling and modality heterogeneity. Furthermore, the feature distribution drift in intersubject emotion recognition hinders the generalization ability of the method and significantly degrades performance on new individuals. To address the above issues, we propose a cross-subject multimodal emotion robust recognition framework that effectively extracts subject-independent intrinsic emotional identification information from heterogeneous multimodal emotion data. First, we develop a multichannel network with self-attention and cross-attention mechanisms to capture modality-specific and complementary features among different modalities, respectively. Second, we incorporate contrastive loss into the multichannel attention network to enhance feature extraction across different channels, thereby facilitating the disentanglement of emotion-specific information. Moreover, a self-expression learning-based network layer is devised to enhance feature discriminability and subject alignment. It aligns samples in a discriminative space using block diagonal matrices and maps multiple individuals to a shared subspace using a block off-diagonal matrix. Finally, attention is used to merge multichannel features, and multilayer perceptron is employed for classification. Experimental results on multimodal emotion datasets confirm that our proposed approach surpasses the current state-of-the-art in terms of emotion recognition accuracy, with particularly significant gains observed in the challenging cross-subject multimodal recognition scenarios.},
  archive      = {J_TCDS},
  author       = {Minxu Liu and Donghai Guan and Chuhang Zheng and Qi Zhu},
  doi          = {10.1109/TCDS.2025.3552124},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1323--1335},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multimodal discriminative network for emotion recognition across individuals},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SIR-HCL: Semantic-inconsistency reasoning and hybrid contrastive learning for efficient cross-emotion anomaly detection. <em>TCDS</em>, <em>17</em>(5), 1310--1322. (<a href='https://doi.org/10.1109/TCDS.2025.3550645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-emotion anomaly detection is an emerging and challenging research topic in cognitive analysis field, which aims at identifying the abnormal emotion pair whose semantic patterns are inconsistent across different emotional modalities. To the best of our knowledge, this topic has yet to be well studied, which could potentially benefit lots of valuable cognitive applications such as autistic children diagnosis and criminal deception detection. To this end, this article proposes an efficient cross-emotion anomaly detection approach via semantic-inconsistency reasoning and hybrid contrastive learning (SIR-HCL), which is the first attempt to detect the anomalous emotional pairs across the audio–visual emotions. First, the proposed framework utilizes dual-branch network to obtain the deep emotional features in each modality, and then employs the shared residual block to derive the semantically compatible features. Subsequently, an efficient hybrid contrastive learning approach is designed to enlarge the semantic-inconsistency among abnormal emotional pair with different affective classes, while enhancing the semantic-consistency and increasing the feature correlation between normal emotional pair from the same affective class. At the same time, an efficient bidirectional learning scheme is employed to significantly improve the data utilization and a two-component Beta Mixture Model is adaptively utilized to reason the anomalous emotion pairs. Extensive experiments evaluated on two benchmark datasets show that the proposed SIR-HCL method can well detect the anomalous emotional pairs across audio-visual emotional data, and brings substantial improvements over the state-of-the-art competing methods.},
  archive      = {J_TCDS},
  author       = {Xin Liu and Qiyan Chen and Yiu-ming Cheung and Shu-Juan Peng},
  doi          = {10.1109/TCDS.2025.3550645},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1310--1322},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SIR-HCL: Semantic-inconsistency reasoning and hybrid contrastive learning for efficient cross-emotion anomaly detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alteration of functional brain networks during lower limb movement in parkinson's disease patients with freezing of gait. <em>TCDS</em>, <em>17</em>(5), 1301--1309. (<a href='https://doi.org/10.1109/TCDS.2025.3551600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal brain structures have been observed in Parkinson's disease (PD) patients with freezing of gait (FoG), but the neural mechanisms behind FoG are still not well understood. This study analyzed EEG data from 13 PD patients with FoG and 13 healthy controls (HCs) during a pedaling task. Eight key brain regions were selected to measure power density and connectivity across different frequency bands and time windows. Using graph theory, the study examined neural changes between FoG patients and HCs, focusing on metrics: clustering coefficient, degree, nodal efficiency, and global efficiency. FoG patients had decreased δ and θ activity in the precentral-L and frontal-mid-L regions and increased β activity in the precentral-R and postcentral regions during motor initiation (0–400 ms postcue). FoG patients also exhibited decreased connectivity in the bilateral frontal-mid, supplementary motor area (SMA), postcentral, and precentral regions in the δ and θ bands during motor initiation. During motor execution (0–1 s postcue), fewer significant connections were observed in the α band in these regions. Furthermore, FoG patients also had a decreased clustering coefficient in δ, θ, and α bands during motor initiation and in the θ band during motor execution in regions such as SMA-L, frontal-mid-R, and postcentral-R. The nodal efficiency increased in the regions of precentral-R and postcentral-R (δ band), postcentral-L (θ band) for motor initiation, and precentral-L (θ band) for motor execution. FoG in PD is characterized by the changes of brain functional network, including the decrease of δ and θ activities and the increase of β activity in some brain regions during gait initiation and the abnormal network reorganization. These findings may help to understand the neural mechanisms of FoG in PD and could guide future brain stimulation studies.},
  archive      = {J_TCDS},
  author       = {Jingting Liang and Xiangguo Yin and Mingxing Lin and Shuqin Wang and Aiqin Song and Wen Chen},
  doi          = {10.1109/TCDS.2025.3551600},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1301--1309},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Alteration of functional brain networks during lower limb movement in parkinson's disease patients with freezing of gait},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An Encoder–Decoder model based on spiking neural networks for address event representation object recognition. <em>TCDS</em>, <em>17</em>(5), 1286--1300. (<a href='https://doi.org/10.1109/TCDS.2025.3548868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Address event representation (AER) object recognition task has attracted extensive attention in neuromorphic vision processing. The spike-based and event-driven computation inherent in the spiking neural network (SNN) provides an energy-saving solution for AER object recognition. However, SNN with spike timing dependent plasticity (STDP) learning rule has not achieved satisfying AER object recognition performance. This work proposes an SNN-based encoder-decoder model to improve the recognition performance of AER objects. An STDP-based locally connected spiking neural network (LC-SNN) is proposed as an encoder to extract rich spatiotemporal features from AER event flows more flexibly. After the encoder extracts and learns primary features, we propose a fully connected spiking neural network (FC-SNN) based on the reward-modulated spike-timing-dependent plasticity (R-STDP) learning rule as a decoder to learn higher-level features for classification. In addition, we improved the winner-take-all (WTA) mechanisms and R-STDP learning rule in the decoder based on the reward and punish decision, enabling the network to perform better. The experiments are performed on the N-MNIST, MNIST-DVS, and the dynamic vision sensor (DVS) gesture datasets, improving the accuracy of the best existing plasticity-based SNN by 0.19%, 0.27%, and 1.35%, respectively.},
  archive      = {J_TCDS},
  author       = {Sichun Du and Haodi Zhu and Yang Zhang and Qinghui Hong},
  doi          = {10.1109/TCDS.2025.3548868},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1286--1300},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An Encoder–Decoder model based on spiking neural networks for address event representation object recognition},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Emotional intelligence: Abstract cognition innovation in artificial general intelligence systems. <em>TCDS</em>, <em>17</em>(5), 1272--1285. (<a href='https://doi.org/10.1109/TCDS.2025.3547934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the incorporation of abstract emotion-triggering mechanisms into artificial general intelligence (AGI) systems through the nonaxiomatic reasoning system (NARS) framework. Leveraging cognitive appraisal theory, the proposed model facilitates dynamic regulation of cognitive resources by modulating priority and durability based on goal alignment and temporal evaluation. Distinct from conventional emotion models that depend on predefined feedback mechanisms, this framework enables generalized emotional responses, thereby enhancing adaptability to complex temporal and causal dynamics. Experimental validation conducted on flappy bird and airplane combat simulation platforms illustrates the superiority of the emotion-driven NARS, which demonstrates enhanced decision-making efficiency, robust goal prioritization, and superior adaptability compared to its nonemotional counterpart. These findings underscore the potential of emotion-enabled AGI systems to advance applications in high-stakes domains, including autonomous driving and robotics, where real-time adaptability and efficient resource allocation are paramount.},
  archive      = {J_TCDS},
  author       = {Xiang Li and Yongming Li and Luyao Bai and Jingyi Zhao},
  doi          = {10.1109/TCDS.2025.3547934},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1272--1285},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Emotional intelligence: Abstract cognition innovation in artificial general intelligence systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foundational policy acquisition via multitask learning for motor skill generation. <em>TCDS</em>, <em>17</em>(5), 1260--1271. (<a href='https://doi.org/10.1109/TCDS.2025.3543350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a multitask reinforcement learning (RL) algorithm for foundational policy acquisition to generate novel motor skills. Learning the rich representation of the multitask policy is a challenge in dynamic movement generation tasks because the policy needs to cope with changes in goals or environments with different reward functions or physical parameters. Inspired by human sensorimotor adaptation mechanisms, we developed the learning pipeline to construct the encoder–decoder networks and network selection to facilitate foundational policy acquisition under multiple situations. First, we compared the proposed method with previous multitask RL methods in the standard multilocomotion tasks. The results showed that the proposed approach outperformed the baseline methods. Then, we applied the proposed method to the ball heading task using a monopod robot model to evaluate skill generation performance. The results showed that the proposed method was able to adapt to novel target positions or inexperienced ball restitution coefficients but to acquire a foundational policy network, originally learned for heading motion, which can generate an entirely new overhead kicking skill.},
  archive      = {J_TCDS},
  author       = {Satoshi Yamamori and Jun Morimoto},
  doi          = {10.1109/TCDS.2025.3543350},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1260--1271},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Foundational policy acquisition via multitask learning for motor skill generation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-subject and cross-session EEG emotion recognition based on multisource structural deep clustering. <em>TCDS</em>, <em>17</em>(5), 1245--1259. (<a href='https://doi.org/10.1109/TCDS.2025.3545666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individual fluctuations and temporal variability of electroencephalogram (EEG) data pose challenges in precisely identifying emotions. Although a model may perform well with data specific to a certain subject or session, the fluctuations in EEG data can significantly impair the model's performance on a different subject or session. To tackle this problem, current approaches synchronize the original and new subject or session feature distributions. Directly matching EEG data across individuals or sessions may undermine the inherent distinguishability due to the heterogeneity in data distribution. Instead of direct alignment, this work utilizes multisource structural deep clustering to identify the inherent structural knowledge of the target itself and regularize it through the distribution of source labels. Furthermore, the method was implemented on the intermediate output utilizing high-confidence features to improve pattern identification in the latent feature space. This led to more distinct differentiations across subdomains with varying labels. Comparative analyses were performed with state of-the-art (SOTA) models on SEED and SEED-IV datasets. The model proposed outperformed other baseline models, reaching an average accuracy of 90.69%/95.05% in a cross-subject/cross session experiment on SEED and 74.35%/78.56% in SEED-IV. This research provides a novel approach to align EEG features without the need for direct distance calculation.},
  archive      = {J_TCDS},
  author       = {Yiyuan Chen and Xiaodong Xu and Xiaowei Qin},
  doi          = {10.1109/TCDS.2025.3545666},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1245--1259},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cross-subject and cross-session EEG emotion recognition based on multisource structural deep clustering},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variability in sensory event-related potential as an early marker of cognitive impairment. <em>TCDS</em>, <em>17</em>(5), 1235--1244. (<a href='https://doi.org/10.1109/TCDS.2025.3541729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mild cognitive impairment (MCI) is a precursor to dementia and poses significant health and economic challenges. Early detection of MCI can slow disease progression and ease the burden on patients and caregivers. This study aimed to explore sensory-perception deficits and fluctuations in MCI using auditory event-related potentials (ERPs) from a prefrontal two-channel electroencephalogram (EEG) device. The study involved 573 MCI patients and 1,295 cognitively normal (CN) individuals, with cognitive decline assessed through the Seoul neuropsychological screening battery (SNSB) and the mini-mental state examination (MMSE). This study analyzed ERPs and trial-to-trial variability using the response variance curve (RVC) in neural responses to eight auditory sounds. While no significant differences were observed in ERP amplitudes and area under the curves (AUCs) for sensory (N1) and perception (P2) processing between MCI and CN groups, MCI patients showed notable differences in trial-to-trial variability, particularly in those aged 70 to 79 years. This variability remained significant even after adjusting for factors such as age, sex, years of education, and MMSE scores. The study suggests that MCI is associated with instability in preattentive auditory detection and higher-order perceptual processing. The findings highlight that people in their 70s may be in a transitional phase associating these sensory-perceptual variabilities with cognitive impairment. Given the large sample size and limitations of current neuropsychological tests, the study underscores the potential of sensory ERP measures as a supplementary tool for MCI screening.},
  archive      = {J_TCDS},
  author       = {Joel Eyamu and Wuon-Shik Kim and Kahye Kim and Kun Ho Lee and Jaeuk U. Kim},
  doi          = {10.1109/TCDS.2025.3541729},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1235--1244},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Variability in sensory event-related potential as an early marker of cognitive impairment},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved dual neural network method based on levy flight for multirobot cooperative area coverage search in 3-D unknown environments. <em>TCDS</em>, <em>17</em>(5), 1223--1234. (<a href='https://doi.org/10.1109/TCDS.2025.3541416'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on multirobot collaborative search in unknown 3-D environments, based on bio-inspired neural networks, holds significant value and importance. However, challenges arise in 3-D environments, including excessive turning and vertical movement, as well as the potential for collisions between robots. In response, we propose an improved Glasius bioinspired neural network (GBNN) that mitigates decision conflicts among robots and considers the impact of turning and vertical movement on robot decision-making. Furthermore, to address the issue of robots getting trapped in local deadlocks during the search process, we present a dual neural network algorithm based on Levy flights. In the method, dual GBNN based on Levy flight (LF-DUAL-GBNN) proposed in this article, robots obtain random target points through Levy flights and are then guided by a dual neural network to navigate to the vicinity of the target points, thus breaking free from local deadlock states. Finally, we conducted simulation experiments to validate the algorithm's effectiveness.},
  archive      = {J_TCDS},
  author       = {Fangfang Zhang and Yongqi Wang and Wenhao Wang and Jianbin Xin and Jinzhu Peng and Yaonan Wang},
  doi          = {10.1109/TCDS.2025.3541416},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1223--1234},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An improved dual neural network method based on levy flight for multirobot cooperative area coverage search in 3-D unknown environments},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributional latent variable models with an application in active cognitive testing. <em>TCDS</em>, <em>17</em>(5), 1212--1222. (<a href='https://doi.org/10.1109/TCDS.2025.3548962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive modeling commonly relies on asking participants to complete a battery of varied tests to estimate attention, working memory, and other latent variables. In many cases, these tests result in highly variable observation models. A near-ubiquitous approach is to repeat many observations for each test independently, resulting in a distribution of the outcomes from each test given to each subject. Latent variable models (LVMs), if employed, are only added after data collection. In this article, we explore the usage of LVMs to enable learning across many correlated variables simultaneously. We extend LVMs to the setting where observed data for each subject are a series of observations from many different distributions, rather than simple vectors to be reconstructed. By embedding test battery results for individuals in a latent space that is trained jointly across a population, we can leverage correlations both between disparate test data for a single participant and between multiple participants. We then propose an active learning framework that leverages this model to conduct more efficient cognitive test batteries. We validate our approach by demonstrating with real-time data acquisition that it performs comparably to conventional methods in making item-level predictions with fewer test items.},
  archive      = {J_TCDS},
  author       = {Robert Kasumba and Dom C.P. Marticorena and Anja Pahor and Geetha Ramani and Imani Goffney and Susanne M. Jaeggi and Aaron R. Seitz and Jacob R. Gardner and Dennis L. Barbour},
  doi          = {10.1109/TCDS.2025.3548962},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1212--1222},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Distributional latent variable models with an application in active cognitive testing},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Barrier offset varying-parameter dynamic learning network for solving dual-arms human-like behavior generation. <em>TCDS</em>, <em>17</em>(5), 1199--1211. (<a href='https://doi.org/10.1109/TCDS.2025.3541070'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabling robots to imitate human actions and perform tasks with high precision while avoiding potential obstacles in the environment can effectively enhance the interaction between social robots and humans. In this article, to achieve higher precision trajectory tracking and obstacle avoidance for dual-arm humanoid robots, the barrier offset varying-parameter dynamic learning neural (BOVDL) network method is proposed and applied to dual-arm humanoid behavior generation scheme. To do so, a dual-arms humanoid robot model is set up, and transformed into a constrained time-varying quadratic programming (TVQP) problem. Second, by using Lagrangian multiplier method and Karush–Kuhn–Tuchker condition, the inequality constrained TVQP is converted as a time-varying equation with a barrier parameter. Third, a varying-parameter dynamic learning network is presented to solve the time-varying equation with a barrier parameter. Computer simulation experiments are conducted to verify the feasibility, accuracy, and safety of the proposed BOVDL network method. Experimental results show that all 14 joints of the humanoid robot's arms are within the motion range of each real human arm's physical constraints. The maximum position error and velocity error between the desired trajectory and the actual trajectory of the end effector are less than $10^{-6}\ \text{m}$ magnitude and $10^{-7}\ \text{m}$ magnitude, respectively, representing a reduction of five orders of magnitude compared to the traditional varying-parameter convergent-differential neural network. Furthermore, the proposed method also enables the dual-arm humanoid robot to avoid collisions with obstacles while performing tasks, demonstrating the superiority of the proposed BOVDL network scheme.},
  archive      = {J_TCDS},
  author       = {Zhijun Zhang and Mingyang Zhang and Jinjia Guo and Haotian He},
  doi          = {10.1109/TCDS.2025.3541070},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1199--1211},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Barrier offset varying-parameter dynamic learning network for solving dual-arms human-like behavior generation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Memristor-based circuit of sequential associative memory with memory interactions and stimulus similarity effect. <em>TCDS</em>, <em>17</em>(5), 1186--1198. (<a href='https://doi.org/10.1109/TCDS.2025.3540591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most proposed memristor-based circuits of associative memory (AM) consider various mechanisms in only one AM. Few works on circuit design of sequential associative memory (SAM) have been reported. In this article, a memristor-based circuit of SAM with memory interactions and stimulus similarity effect is designed. Two associative memories, the prior associative memory (PAM) and the later associative memory (LAM), are formed successively. The PAM modifies the rate of the formation of the LAM, and the LAM, in turn, affects the strength of the PAM, which are two interactions in SAM, called transfer and retroaction. In addition, the magnitudes of transfer and retroaction are determined by the similarity of the conditioned stimuli. The above functions are realized by the ring input processing module, memory module, transfer module, and retroaction module. It has been identified in circuit analysis that the proposed circuit is power efficient and has good robustness. Furthermore, the proposed circuit has promising applications for fire rescue robots.},
  archive      = {J_TCDS},
  author       = {Dongdong Xiong and Xiaoping Wang and Yiming Jiang and Chao Yang and Man Jiang and Jingang Lai and Zhigang Zeng},
  doi          = {10.1109/TCDS.2025.3540591},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1186--1198},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Memristor-based circuit of sequential associative memory with memory interactions and stimulus similarity effect},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A marr-inspired framework for raising “Good” robots. <em>TCDS</em>, <em>17</em>(5), 1175--1185. (<a href='https://doi.org/10.1109/TCDS.2025.3540217'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our current computer and AI systems are built on neuroscience principles from almost a century ago. Recent advances in our understanding of biological computation have not crossed into computer science to catalyze advancements. We outline a multidimensional blueprint for a form of bio-inspired agent leveraging modern neuroscience principles (including the co-localization of memory and compute, plasticity, embodiment, active inference, and neurodevelopmental principles). We discuss how combining these core features could theoretically lead to cognitive agents that are aligned to our prosocial values, transparent, explainable, and energy efficient (i.e., “good” robots). In particular, we leverage Marr's tri-level framework and advocate for an “Implementation Level” consisting of embodied neuromorphic hardware, an “Algorithmic Level” consisting of Active Inference, and a “Computational Level” consisting of prosocial goals (supported by evidence of prosociality catalyzing the development of our own complex cognitive abilities). A developmental process scaffolds different prosocial computations over time. Supporting our perspective, we include simulation data demonstrating the transfer of priors between two different prosocial behaviors (computational level) via active inference (algorithmic level), supported by an embodied process (implementation level). Agent behavior is transparent and explainable throughout. We advocate for this blueprint as a guide in creating capable, ethical, and sustainable machine intelligence.},
  archive      = {J_TCDS},
  author       = {Sarah Hamburg and Alejandro Jimenez-Rodriguez and Aung Htet and Alessandro Di Nuovo},
  doi          = {10.1109/TCDS.2025.3540217},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1175--1185},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A marr-inspired framework for raising “Good” robots},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RoboGPT: An LLM-based long-term decision-making embodied agent for instruction following tasks. <em>TCDS</em>, <em>17</em>(5), 1163--1174. (<a href='https://doi.org/10.1109/TCDS.2025.3543364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic agents are tasked with mastering common sense and making long-term sequential decisions to execute daily tasks based on natural language instructions. Recent advancements in large language models (LLMs) have catalyzed efforts for complex robotic planning. However, despite their superior generalization and comprehension capabilities, LLM task plans sometimes suffer from issues of accuracy and feasibility. To address these challenges, we propose RoboGPT,11For more details, please refer to our project page https://github.com/Cwb0106/RoboGPT. an embodied agent specifically designed to make long-term decisions for instruction following tasks. RoboGPT integrates three key modules: 1) RoboPlanner, an LLM-based planning module equipped with 67k embodied planning data, breaks down tasks into logical subgoals. We compile a new robotic dataset using a template feedback-based self-instruction method to fine-tune the Llama model. RoboPlanner with strong generalization can plan hundreds of instruction following tasks; 2) RoboSkill, customized for each subgoal to improve navigation and manipulation capabilities; and 3) Re-Plan, a module that dynamically adjusts the subgoals based on real-time environmental feedback. By utilizing the precise semantic map generated by RoboSkill, the target objects can be replaced by calculating the similarity between subgoals and the objects present in the environment. Experimental results demonstrate that RoboGPT exceeds the performance of other state-of-the-art (SOTA) methods, particularly LLM-based methods, in terms of task planning rationality for hundreds of unseen daily tasks and even tasks from other domains.},
  archive      = {J_TCDS},
  author       = {Yaran Chen and Wenbo Cui and Yuanwen Chen and Mining Tan and Xinyao Zhang and Jinrui Liu and Haoran Li and Dongbin Zhao and He Wang},
  doi          = {10.1109/TCDS.2025.3543364},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1163--1174},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {RoboGPT: An LLM-based long-term decision-making embodied agent for instruction following tasks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NuRF: Nudging the particle filter in radiance fields for robot visual localization. <em>TCDS</em>, <em>17</em>(5), 1153--1162. (<a href='https://doi.org/10.1109/TCDS.2025.3553261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Can we localize a robot on a map only using monocular vision? This study presents neural radiance field (NuRF), an adaptive and nudged particle filter framework in radiance fields for six degree-of-freedom (6-DoF) robot visual localization. NuRF leverages recent advancements in radiance fields and visual place recognition. Conventional visual place recognition meets the challenges of data sparsity and artifact-induced inaccuracies. By utilizing radiance field-generated novel views, NuRF enhances visual localization performance and combines coarse global localization with the fine-grained pose tracking of a particle filter, ensuring continuous and precise localization. Experimentally, our method converges seven times faster than existing Monte Carlo-based methods and achieves localization accuracy within 1 m, offering an efficient and resilient solution for indoor visual localization.},
  archive      = {J_TCDS},
  author       = {Wugang Meng and Tianfu Wu and Huan Yin and Fumin Zhang},
  doi          = {10.1109/TCDS.2025.3553261},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1153--1162},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {NuRF: Nudging the particle filter in radiance fields for robot visual localization},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoSkill: Hierarchical open-ended skill acquisition for long-horizon manipulation tasks via language-modulated rewards. <em>TCDS</em>, <em>17</em>(5), 1141--1152. (<a href='https://doi.org/10.1109/TCDS.2025.3551298'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A desirable property of generalist robots is the ability to both bootstrap diverse skills and solve new long-horizon tasks in open-ended environments without human intervention. Recent advancements have shown that large language models (LLMs) encapsulate vast-scale semantic knowledge about the world to enable long-horizon robot planning. However, they are typically restricted to reasoning high-level instructions and lack world grounding, which makes it difficult for them to coordinately bootstrap and acquire new skills in unstructured environments. To this end, we propose AutoSkill, a hierarchical system that empowers the physical robot to automatically learn to cope with new long-horizon tasks by growing an open-ended skill library without hand-crafted rewards. AutoSkill consists of two key components: 1) an in-context skill chain generation and new skill bootstrapping guided by LLMs that inform the robot of discrete and interpretable skill instructions for skill retrieval and augmentation within the skill library; and 2) a zero-shot language-modulated reward scheme in conjunction with a meta prompter facilitates online new skill acquisition via expert-free supervision aligned with proposed skill directives. Extensive experiments conducted in both simulated and realistic environments demonstrate AutoSkill's superiority over other LLM-based planners as well as hierarchical methods in expediting online learning for novel manipulation tasks.},
  archive      = {J_TCDS},
  author       = {Zhenyang Lin and Yurou Chen and Zhiyong Liu},
  doi          = {10.1109/TCDS.2025.3551298},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1141--1152},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {AutoSkill: Hierarchical open-ended skill acquisition for long-horizon manipulation tasks via language-modulated rewards},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bio-inspired goal-directed cognitive map model for robot navigation and exploration. <em>TCDS</em>, <em>17</em>(5), 1125--1140. (<a href='https://doi.org/10.1109/TCDS.2025.3552085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of a cognitive map (CM), or spatial map, was originally proposed to explain how mammals learn and navigate their environments. Over time, extensive research in neuroscience and psychology has established the CM as a widely accepted model. In this work, we introduce a new goal-directed cognitive map (GDCM) model that takes a nontraditional approach to spatial mapping for robot navigation and path planning. Unlike conventional models, GDCM does not require complete environmental exploration to construct a graph for navigation purposes. Inspired by biological navigation strategies, such as the use of landmarks, Euclidean distance, random motion, and reward-driven behavior. The GDCM can navigate complex, static environments efficiently without needing to explore the entire workspace. The model utilizes known cell types (head direction, speed, border, grid, and place cells) that constitute the CM, arranged in a unique configuration. Each cell model is designed to emulate its biological counterpart in a simple, computationally efficient way. Through simulation-based comparisons, this innovative CM graph-building approach demonstrates more efficient navigation than traditional models that require full exploration. Furthermore, GDCM consistently outperforms several established path planning and navigation algorithms by finding better paths.},
  archive      = {J_TCDS},
  author       = {Matthew A. Hicks and Tingjun Lei and Chaomin Luo and Daniel W. Carruth and Zhuming Bi},
  doi          = {10.1109/TCDS.2025.3552085},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1125--1140},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A bio-inspired goal-directed cognitive map model for robot navigation and exploration},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient multitask reinforcement learning via task-specific action correction. <em>TCDS</em>, <em>17</em>(5), 1110--1124. (<a href='https://doi.org/10.1109/TCDS.2025.3543694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask reinforcement learning (MTRL) holds potential for building general-purpose agents, enabling them to generalize across a variety of tasks. However, MTRL may still be susceptible to conflicts between tasks. A primary reason for this problem is that a universal policy struggles to balance short-term and dense learning signals across various tasks, e.g., distinct reward functions in reinforcement learning. In social cognitive theory, internalized future goals, as a form of cognitive representations, can effectively mitigate potential short-term conflicts in multitask settings. Considering the benefits of future goals, we propose a novel and general framework called task-specific action correction (TSAC) from the goal perspective as an orthogonal research to previous MTRL methods. Specifically, to avoid myopia, TSAC introduces goal-oriented sparse rewards and decomposes policy learning into two separate policies: a shared policy (SP) and an action correction policy (ACP). The SP outputs a short-term perspective action based on guiding dense rewards. To alleviate conflicts resulting from excessive focus on specific tasks’ details in SP, the ACP incorporates goal-oriented sparse rewards, enabling an agent to adopt a long-term perspective to output a correction action and achieve generalization across tasks. Finally, the actions output by SP and ACP are combined based on the action correction function to form a final action that interact with the environment. Extensive experiments conducted on meta-world and multitask StarCraft II multiagent scenarios demonstrate that TSAC outperforms existing state-of-the-art methods, achieving significant improvements in sample efficiency, generalization and effective action execution across tasks.},
  archive      = {J_TCDS},
  author       = {Jinyuan Feng and Min Chen and Zhiqiang Pu and Tenghai Qiu and Jianqiang Yi and Jie Zhang},
  doi          = {10.1109/TCDS.2025.3543694},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1110--1124},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Efficient multitask reinforcement learning via task-specific action correction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two heads are better than one: Collaboration-oriented multiagent exploration system. <em>TCDS</em>, <em>17</em>(5), 1098--1109. (<a href='https://doi.org/10.1109/TCDS.2025.3530945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous exploration in unknown environments is a complex and formidable challenge that requires effective collaboration among multiple agents under partially observable conditions. Due to limited observations and inefficient collaboration, multiagent exploration often suffers from excessively long exploration paths. To address this issue, this article proposes a collaboration-oriented multiagent exploration system (COMAE). To effectively understand and leverage the interagent relationships, this article introduces collaboration-oriented observation (COO). In addition to the basic connectivity graph, the COO further constructs collaboration-oriented node features and an interaction graph to enhance the overall strategic understanding of multiagent. To improve collaboration among agents, this article designs an attention-based sequential network (ASN) to predict strategic actions. Additionally, a novel collaborative exploration reward (CER) is proposed to further prevent noncollaborative behaviors during the exploration process. Extensive experiments demonstrate that the proposed method enhances collaboration among agents and significantly reduces exploration distances.},
  archive      = {J_TCDS},
  author       = {Yang Liu and Peng Zhang and Hangyou Yu and Pingping Zhang and Jie Zhao and Dong Wang and Huchuan Lu},
  doi          = {10.1109/TCDS.2025.3530945},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1098--1109},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Two heads are better than one: Collaboration-oriented multiagent exploration system},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human–Robot sharing operation in cotransporting for nonholonomic mobile robot. <em>TCDS</em>, <em>17</em>(5), 1087--1097. (<a href='https://doi.org/10.1109/TCDS.2024.3493116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, human–robot sharing operation in cotransporting for a nonholonomic mobile robot is studied. Sharing operation weights of human and nonholonomic mobile robot are proposed to avoid dynamical obstacles in an unknown environment, which assigns operation weights to human and robot based on sensory information of environment, the balance of task between “following the human” and “autonomous obstacle avoidance” is further realized. A local multimodal obstacle avoidance method based on 2-D Lidar is proposed, which deals with the interference of unrelated obstacles. In addition, we designed a mechanism to estimate human motion intention and integrated it into the nonholonomic mobile robotic platform for following the human. The experimental results show that the proposed method can effectively deal with the problem of human–robot cotransporting for a nonholonomic mobile robot in an unknown environment, can avoid obstacle and ensure human's operation comfort in cotransporting.},
  archive      = {J_TCDS},
  author       = {Nan Feng and Xiong Guo and Xinbo Yu and Shuang Zhang and Wei He},
  doi          = {10.1109/TCDS.2024.3493116},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1087--1097},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Human–Robot sharing operation in cotransporting for nonholonomic mobile robot},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal perception for indoor mobile robotics navigation and safe manipulation. <em>TCDS</em>, <em>17</em>(5), 1074--1086. (<a href='https://doi.org/10.1109/TCDS.2024.3481457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor mobile robotics (IMR) has gained significant attention due to its potential applications in various domains, such as healthcare, logistics, and domestic assistance. However, navigating through indoor environments and performing safe manipulations still pose intractable challenges in terms of navigation accuracy and obstacle avoidance. To solve these issues, this article presents an artificial intelligence (AI) embodied multimodal perception framework for IMR intelligent navigation and safe manipulation. To ensure the navigation accuracy and robustness, we employ the complementary forward RGB camera, downward QR vision sensor, and wheel encoder measurements in a unified framework. The visual residuals and wheel odometry residuals are jointly minimized to estimate the robot states. To guarantee the safety of robotic manipulation tasks, we have developed an AI model that integrates transformer network with convolutional neural network, to associate the long-range RGB & depth patches and aggregate the multiscale obstacle features, enabling the precise detection and segmentation of obstacles in RGB-D images. Afterwards, the depths of detected obstacles are regressed, providing the robot with crucial information for collision avoidance. Eventually, we design a refined robot manipulation system that dynamically adjusts the robot behavior to ensure effective collision avoidance and to minimize potential damage to its mechanical components by constantly evaluating the spatial relationships between the robot and its surroundings. By incorporating advanced obstacle detection and the avoidance mechanism, mobile robots can navigate reliably in indoor environments with a reduced risk of collisions and real-time decision making. The presented method has been evaluated on the developed IMR platform. On the collected dataset, the estimated IMR absolute position and orientation errors are less than 0.18 m and 5${}^{\boldsymbol{\circ}}$, respectively. Besides, it achieves 89% $mAP$ on obstacle detection. The maximum of the estimated obstacle relative depth & orientation errors are less than 0.4 m and 2${}^{\boldsymbol{\circ}}$, respectively, which proves competitiveness against the state-of-the-art in both robot navigation and safe manipulation.},
  archive      = {J_TCDS},
  author       = {Yinlong Zhang and Yuanhao Liu and Shuai Liu and Wei Liang and Chu Wang and Kai Wang},
  doi          = {10.1109/TCDS.2024.3481457},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1074--1086},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multimodal perception for indoor mobile robotics navigation and safe manipulation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CS-SLAM: A lightweight semantic SLAM method for dynamic scenarios. <em>TCDS</em>, <em>17</em>(5), 1061--1073. (<a href='https://doi.org/10.1109/TCDS.2024.3462651'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {SLAM systems typically rely on the assumption of scene rigidity. However, in real-world applications, robots often need to operate in dynamic environments, presenting unique challenges to the stability of SLAM systems. Efficient and lightweight SLAM systems play an important role in enabling interactions between robots and their environments. To enhance their applicability in dynamic environments, a lightweight semantic dynamic SLAM framework CS-SLAM has been proposed. First, the article designs a lightweight semantic segmentation network, Cross-SegNet, to remove dynamic feature points. This network includes a lightweight feature learning module, Cross Block, which effectively detects dynamic objects while maintaining a lightweight design, thereby improving the processing efficiency and accuracy of the SLAM system. Second, a spatiotemporal consistency-based auxiliary mask algorithm has been proposed, which compares the mask mapped from the previous frame to the current frame and the mask from the Cross-SegNet segmentation. By calculating the intersection over union (IoU), segmentation results are analyzed and supplemented to enhance the efficiency of removing dynamic feature points. Qualitative and quantitative evaluations on public datasets and real-world scenarios demonstrate the robustness and effectiveness of the proposed approach comparing to existing methods compared to existing methods.},
  archive      = {J_TCDS},
  author       = {Zhendong Guo and Na Dong and Zehui Zhang and Xiaoming Mai and Donghui Li},
  doi          = {10.1109/TCDS.2024.3462651},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1061--1073},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {CS-SLAM: A lightweight semantic SLAM method for dynamic scenarios},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Where to learn: Embodied perception learning planned by vision-language models. <em>TCDS</em>, <em>17</em>(5), 1050--1060. (<a href='https://doi.org/10.1109/TCDS.2025.3539665'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied learning plays a crucial role in transferring self-learning agents to adapt to the environment. Existing embodied learning methods primarily rely on reinforcement learning (RL) exploration policy to collect inaccurate perceptual result samples for improving perceptual capabilities. However, RL-based exploration policies encounter several challenges such as the need for substantial data for training and the struggle to keep the diversity of the collected samples. In this article, we propose an embodied learning method that employs vision-language models (VLMs) as task planners, code planners, and path planners. Specifically, our method employs layout knowledge of the VLMs to decompose the embodied learning task into multiple subtasks and then convert each subtask into executable code, which will be executed to guide the agent to explore and collect the diverse samples in different types of rooms. Additionally, VLMs incorporate an external database to identify regions that enhance perceptual capabilities, and the agent will explore these poor perception regions to collect samples that can improve the perception performance. Experimental results demonstrate the effectiveness of our approach without the need for additional training.},
  archive      = {J_TCDS},
  author       = {Juan Wang and Di Guo and Huaping Liu},
  doi          = {10.1109/TCDS.2025.3539665},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1050--1060},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Where to learn: Embodied perception learning planned by vision-language models},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Special issue on embodied AI in indoor robotics: Bridging perception, interaction, and autonomy. <em>TCDS</em>, <em>17</em>(5), 1047--1049. (<a href='https://doi.org/10.1109/TCDS.2025.3595370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCDS},
  author       = {Yaran Chen and Chengguang Yang and Chaomin Luo and Dongbin Zhao},
  doi          = {10.1109/TCDS.2025.3595370},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {10},
  number       = {5},
  pages        = {1047--1049},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Guest editorial: Special issue on embodied AI in indoor robotics: Bridging perception, interaction, and autonomy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DG-NBV: A cognitive framework for direct generation of next best view in continuous view space. <em>TCDS</em>, <em>17</em>(4), 1035--1045. (<a href='https://doi.org/10.1109/TCDS.2024.3521438'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The next best view (NBV) plays an important role in the 3-D object reconstruction. Existing NBV methods mainly adopt the generate-and-test strategy to select the best view in the discrete view space. This affects the adaptation to diverse objects. To solve this problem, a new NBV paradigm to directly generate the NBV in the continuous view space according to the input point cloud is proposed. Specifically, a point cloud feature extraction module with learnable view and position tokens is presented. These tokens are added to the neighborhood and the position features of the point cloud to fully mine global contextual information, enhancing the feature representation. The predicted view from the proposed network is linked to a pretrained view evaluation network. By duplicating this prediction view and then concatenating the duplication result with the extracted point cloud feature, the evaluation network is endowed with the ability to evaluate arbitrary views. Take the evaluation score of the evaluation network corresponding to the predicted view as the supervision signal, the network is trained. In this way, an effective solution of the NBV selection in the continuous view space is obtained. Accordingly, the adaptability to different objects is reinforced. Experimental results on the ShapeNet and MIT CSAIL datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_TCDS},
  author       = {Zhicheng Liu and Zhiqiang Cao and Jianjie Li and Peiyu Guan and Junzhi Yu},
  doi          = {10.1109/TCDS.2024.3521438},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1035--1045},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {DG-NBV: A cognitive framework for direct generation of next best view in continuous view space},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized reinforcement learning for multiple robotic fish in cooperative pursuit task. <em>TCDS</em>, <em>17</em>(4), 1022--1034. (<a href='https://doi.org/10.1109/TCDS.2025.3540071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The control of multirobot systems, particularly in the pursuit-evasion (PE) with multiple robots, has gained significant attention in both academic and nonacademic settings. However, the collaborative operation of multirobotic fish systems encounters substantial challenges due to the complex underwater environment and unique movement mode. In this article, we propose a multiagent reinforcement learning (MARL) approach to develop a viable strategy for underwater cooperative pursuit. Initially, considering the hydrodynamic model and motion characteristics of robotic fish, we construct a specific simulation environment with multiple fish-like agents, which provides a highly realistic state transition model. Next, we develop a MARL-based strategy learning framework that incorporates appropriate reward functions and agent actions for policy learning. Finally, a series of comprehensive simulations and practical experiments are conducted to validate the effectiveness of the proposed method and confirm its successful application in underwater pursuit scenarios. These findings offer valuable insights for further research in underwater multiple robot systems.},
  archive      = {J_TCDS},
  author       = {Yukai Feng and Zhengxing Wu and Jian Wang and Sijie Li and Yupei Huang and Junzhi Yu and Min Tan},
  doi          = {10.1109/TCDS.2025.3540071},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1022--1034},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Decentralized reinforcement learning for multiple robotic fish in cooperative pursuit task},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual reinforcement learning based on multiview optimization aggregation. <em>TCDS</em>, <em>17</em>(4), 1011--1021. (<a href='https://doi.org/10.1109/TCDS.2025.3540115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although recent research has made some progress in deep reinforcement learning based on raw pixels, the low sample efficiency remains a key challenge in this field. Existing solutions often focus solely on extracting more effective state representations in the representation learning stage and overlook how to better utilize these state representations in the policy learning stage. To address this, a simple and sample-efficient visual reinforcement learning method based on multiview optimization aggregation (MVOA-VRL) is proposed for pixel-based off-policy reinforcement learning frameworks. This method enables the agent to concurrently focus on learning and utilizing state representations. Specifically, MVOA-VRL acquires multiple views of samples through random crop and adaptive intensity adjustment. It then introduces optimization aggregation methods separately in the representation learning and reinforcement learning modules to aggregate the similarities, actions, and state values of multiple samples from different views. MVOA-VRL aims to promote the agent's learning of effective representations and stable policies. Experimental results on continuous control tasks in the DMControl environment show that, compared with state-of-the-art methods, MVOA-VRL achieves higher scores and significantly improves sample efficiency.},
  archive      = {J_TCDS},
  author       = {Xuesong Wang and Ruyi Lu and Hengrui Zhang and Yuhu Cheng},
  doi          = {10.1109/TCDS.2025.3540115},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1011--1021},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Visual reinforcement learning based on multiview optimization aggregation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated mental fatigue detection from electroencephalogram using joint time-frequency representation based on multivariate iterative filtering. <em>TCDS</em>, <em>17</em>(4), 1000--1010. (<a href='https://doi.org/10.1109/TCDS.2025.3538947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driver drowsiness detection is a crucial technology for enhancing road safety and preventing accidents caused by fatigue. This article proposes a method for analyzing electroencephalogram (EEG) signals during driving tasks to assess the driver's mental state. Due to the nonstationary nature of EEG, the conventional Fourier spectrum is not well suited for spectral estimation of EEG. To address this, the study employs a multivariate iterative filtering (MIF) technique to decompose multichannel EEG signals into narrowband amplitude-frequency modulated components. The instantaneous amplitude and frequency are estimated using the discrete energy separation algorithm (DESA), and a joint time-frequency representation (JTFR) based on DESA is applied to estimate the spectral content of multichannel EEG. Mental states associated with drowsiness are identified using the joint marginal spectrum and an artificial neural network classifier. The proposed MIF-based framework was validated on two EEG datasets, achieving classification accuracies of 95.03$\pm$1.08% and 98.33$\pm$1.51%, respectively. These results demonstrate the potential of the method in preventing accidents caused by drowsy or distracted driving.},
  archive      = {J_TCDS},
  author       = {Kritiprasanna Das and Ram Bilas Pachori},
  doi          = {10.1109/TCDS.2025.3538947},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {1000--1010},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Automated mental fatigue detection from electroencephalogram using joint time-frequency representation based on multivariate iterative filtering},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved few-shot learning based on triplet metric for motor imagery EEG classification. <em>TCDS</em>, <em>17</em>(4), 987--999. (<a href='https://doi.org/10.1109/TCDS.2025.3539398'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motor imagery-based brain–computer interface (MI-BCI) technology establishes a connection between human intention and external devices in active rehabilitation. However, obtaining a mass of labeled EEG data is often difficult due to the strict requirement of experimental environment and the necessity for highly cooperative subjects, which makes the application of few-shot learning of EEG classification particularly important. Therefore, we propose a method that combines few-shot learning with triplet metric learning, aiming to maintain strong generalization capabilities of the model with limited samples. First, we pretrain a base model using large auxiliary dataset, and then fine-tune it with a small number of labeled samples from the test subjects to obtain a specific model. During the training process, metric learning between anchor samples and positive/negative samples are employed to gradually converge similar samples, creating clearer class boundaries. Then the feature information of the samples is enhanced through an attention mechanism to obtain their essential features. The proposed framework was evaluated using two publicly available datasets and obtained classification accuracies of 68.29% and 84.40%, respectively, representing enhancements of 1.04% and 1.28% over existing state-of-the-art methods. In conclusion, experimental results indicate that our proposed approach can improve the effectiveness of MI-BCI rehabilitation training.},
  archive      = {J_TCDS},
  author       = {Qingshan She and Chengjun Li and Tongcai Tan and Feng Fang and Yingchun Zhang},
  doi          = {10.1109/TCDS.2025.3539398},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {987--999},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Improved few-shot learning based on triplet metric for motor imagery EEG classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAST: Multiagent safe transformer for reinforcement learning. <em>TCDS</em>, <em>17</em>(4), 976--986. (<a href='https://doi.org/10.1109/TCDS.2025.3533744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Safety remains a crucial challenge in the application of reinforcement learning. Multiagent safe reinforcement learning (MASRL) is an emerging field aiming to learn safe control policies that maximize cumulative rewards while satisfying the safety constraints of multiagent systems. However, existing research is limited and faces challenges such as environmental nonstationarity and the curse of dimensionality in action spaces, hindering the balance between performance and safety. To address these, this article proposes a multiagent safe reinforcement learning algorithm based on Transformer (MAST). The constrained optimization problem is transformed into an unconstrained one using the Lagrangian method. We also propose the multiagent total advantage decomposition theorem, establishing the connection between MASRL and sequence models. A Transformer-based framework is proposed, where a Transformer-based actor network generates joint actions in parallel during training while producing actions autoregressively during inference. Empirical evaluations on the safe multiagent MuJoCo (MAMuJoCo) benchmark show that MAST outperforms state-of-the-art algorithms by 13.06%. Our attention-based reward and safety critics achieve a 22.10% increase in rewards and an 83.58% reduction in safety costs. Additionally, the Transformer-based actor improves performance by 53.60%–111.93% compared to RNN-based methods.},
  archive      = {J_TCDS},
  author       = {Suhang Wei and Xianwei Wang and Xiang Feng and Huiqun Yu},
  doi          = {10.1109/TCDS.2025.3533744},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {976--986},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {MAST: Multiagent safe transformer for reinforcement learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring supervised contrastive learning for skeleton-based temporal action segmentation. <em>TCDS</em>, <em>17</em>(4), 964--975. (<a href='https://doi.org/10.1109/TCDS.2025.3532694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based temporal action segmentation (STAS) takes long human skeleton sequences as input and predicts action categories at the frame level. Current STAS methods primarily use frame-wise cross-entropy loss, which focuses only on the relationships among one-hot label logits and overlooks the importance of the quality of frame-wise representations. To this end, we propose a novel framework called supervised contrastive skeleton-based temporal action segmentation (SCSAS) that optimizes representation learning. Specifically, our framework constructs a frame-level embedding space using a simple projection head and optimizes this space using three novel contrastive losses. These losses enhance the semantic relationships at the frame and segment levels by pulling together representations of the same activities and pushing apart those of different actions. Moreover, we introduce a confidence-based hard anchor sampling strategy to enhance the efficiency of contrastive learning. Finally, a boundary refinement module is also introduced to fully exploit the advantages of optimized representation. Our method seamlessly integrates with existing STAS methods and consistently enhances their performance across various datasets, without additional inference costs. With the incorporation of the boundary refinement branch, early STAS methods even achieve performance comparable to the state-of-the-art methods.},
  archive      = {J_TCDS},
  author       = {Bowen Chen and Haoyu Ji and Hanwei Ma and Ruihan Lin and Wei Nie and Weihong Ren and Zhiyong Wang and Honghai Liu},
  doi          = {10.1109/TCDS.2025.3532694},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {964--975},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Exploring supervised contrastive learning for skeleton-based temporal action segmentation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight facial attractiveness prediction using dual label distribution. <em>TCDS</em>, <em>17</em>(4), 953--963. (<a href='https://doi.org/10.1109/TCDS.2025.3529177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial attractiveness prediction (FAP) aims to assess facial attractiveness automatically based on human esthetic perception. Previous methods using deep convolutional neural networks have improved the performance, but their large-scale models have led to a deficiency in efficiency. In addition, most methods fail to take full advantage of the dataset. In this article, we present a novel end-to-end FAP approach that integrates dual label distribution and lightweight design. The manual ratings, attractiveness score, and standard deviation are aggregated explicitly to construct a dual-label distribution to make the best use of the dataset, including the attractiveness distribution and the rating distribution. Such distributions, as well as the attractiveness score, are optimized under a joint learning framework based on the label distribution learning (LDL) paradigm. The data processing is simplified to a minimum for a lightweight design, and MobileNetV2 is selected as our backbone. Extensive experiments are conducted on two benchmark datasets, where our approach achieves promising results and succeeds in balancing performance and efficiency. Ablation studies demonstrate that our delicately designed learning modules are indispensable and correlated. Additionally, the visualization indicates that our approach can perceive facial attractiveness and capture attractive facial regions to facilitate semantic predictions. The code is available at https://github.com/enquan/2D_FAP.},
  archive      = {J_TCDS},
  author       = {Shu Liu and Enquan Huang and Ziyu Zhou and Yan Xu and Xiaoyan Kui and Tao Lei and Hongying Meng},
  doi          = {10.1109/TCDS.2025.3529177},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {953--963},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Lightweight facial attractiveness prediction using dual label distribution},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SELM: From efficient autonomous exploration to long-term monitoring in semantic level. <em>TCDS</em>, <em>17</em>(4), 938--952. (<a href='https://doi.org/10.1109/TCDS.2025.3531367'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maintaining up-to-date environmental models from initial deployment through long-term autonomy in service is critical for applications such as navigation and task planning. To address the challenges of persistent monitoring in unknown environments, we introduce a two-stage monitoring strategy, termed the semantic-level autonomous exploration and long-term environment monitoring (SELM) framework. In the first stage, we introduce a novel semantic exploration method to adapt to new environments quickly. Leveraging the semantic information within the incrementally constructed 3-D scene graph (3-DSG), we combine the next-best-view (NBV) selection with room semantics, introducing a more efficient and comprehensive approach for multiroom indoor environment exploration. In addition, the exploration provides patrol routes, the room distance–connectivity graph, and complete environment initial states for subsequential monitoring. The monitoring stage aims to persistently patrol to update the world model in the presence of dynamic changes, including changes in objects’ positions. We formulate the long-term monitoring problem as the partially observable Markov decision process (POMDP) to cope with the environmental uncertainty. To solve the POMDP, we propose the graph attention bidirectional long short-term memory proximal policy optimization (GABPPO) algorithm for the optimal patrol strategy. The feasibility and effectiveness of the proposed SELM framework are verified through extensive experiments.},
  archive      = {J_TCDS},
  author       = {Fang Lang and Yongsen Qin and Yinchuan Wang and Jin Liu and Chaoqun Wang and Wei Song and Qiuguo Zhu and Rui Song},
  doi          = {10.1109/TCDS.2025.3531367},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {938--952},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SELM: From efficient autonomous exploration to long-term monitoring in semantic level},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TRANSIT-EEG—A framework for cross-subject classification with subject specific adaptation. <em>TCDS</em>, <em>17</em>(4), 923--937. (<a href='https://doi.org/10.1109/TCDS.2025.3529669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) is pivotal in monitoring and analyzing cerebral activity across diverse domains, including medical diagnostics, cognitive neuroscience, and brain–computer interfaces. However, the inherent intricacy of EEG signals and their subject-specific characteristics pose formidable challenges in devising robust and generalizable classification models. Traditional EEG signal classification paradigms rely on extensive subject-specific datasets. Also, the domain adaption for new subjects often leads to “catastrophic forgetting,” thereby diminishing the performance of model trained on prior subjects. This article proposes a novel framework, transfer, and robust adaptation of new subjects in EEG technology (TRANSIT-EEG), designed to adapt adeptly to new subjects. TRANSIT-EEG demonstrates resilience to subject-specific artifacts by integrating synthetic data generation using the proposed subject-specific augmentation model - individualized diffusion probabilistic model (IDPM). Also, it employs a robust self organising graph attention transformer (SOGAT) that dynamically constructs a graph for each subject, fostering a more accurate classification. Moreover, TRANSIT-EEG introduces adapter-based finetuning using low-rank adaptation (LoRA) for new subjects, enriching the adaptation process. The TRANSIT-EEG framework presents a promising avenue for advancing the realm of EEG signal classification. Evaluation of widely studied datasets, specifically focusing on two significant tasks, SEED for emotion recognition and PhyAat for auditory activity recognition, substantiates the efficacy and versatility of TRANSIT-EEG. This validation indicates a substantial stride toward achieving more generalizable and accurate EEG signal classification.},
  archive      = {J_TCDS},
  author       = {Chirag Ahuja and Divyashikha Sethia},
  doi          = {10.1109/TCDS.2025.3529669},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {923--937},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {TRANSIT-EEG—A framework for cross-subject classification with subject specific adaptation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTAS: An adaptive critical scenarios generation method for decision boundary assessment. <em>TCDS</em>, <em>17</em>(4), 908--922. (<a href='https://doi.org/10.1109/TCDS.2025.3527639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent algorithms have been widely applied in autonomous decision-making systems (ADMSs). Despite the success of these technologies in practical applications, our understanding of the underlying mechanisms of system decisions remains limited. A key factor in studying the decision of ADMSs is the decision boundary (DB), which helps us understand the decision behavior and reflects the safety-critical aspects and decision robustness of the system. However, this depends on the ability to generate critical scenarios near the system's DB. To address this issue, this study proposes a critical boundary scenarios (CBSs) generation method called decision tree-assisted adaptive sampling (DTAS), these CBSs effectively cover the DB of the ADMSs, thereby fully characterizing the DB. In this study, we focus solely on the input and output of the system, treating the ADMS as a completely black-box, making DTAS suitable to any black-box ADMSs with discrete outputs. Additionally, we propose a local DB description method based on decision rule optimization. These decision rules improve the interpretability of complex DB by describing parameter regions. The proposed methods are conducted extensive experiments on standard benchmarks and actual autonomous systems. The experiment performance exhibits the effectiveness of our approaches.},
  archive      = {J_TCDS},
  author       = {Hui Lu and Yuanhang Hu and Shiqi Wang and Ping Zhou and Shi Cheng},
  doi          = {10.1109/TCDS.2025.3527639},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {908--922},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {DTAS: An adaptive critical scenarios generation method for decision boundary assessment},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain–Eye collaboration target detection and localization in remote sensing image. <em>TCDS</em>, <em>17</em>(4), 896--907. (<a href='https://doi.org/10.1109/TCDS.2024.3523018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) reflects brain mechanisms, and existing research leverages EEG-based brain–computer interfaces for remote sensing image target detection. While most studies focus on achieving target image recognition, there has been less emphasis on target localization, which explores spatial information to enhance detection efficiency. An effective approach for target localization is through eye tracking. In this article, we propose a brain-eye collaboration framework (BECF) that enables both target image recognition and localization. We begin by decoding EEG-based event-related potential (ERP) features using the xDAWN+RG method for target image recognition. Additionally, we implement a region division strategy to achieve both target image recognition and coarse-grained localization, which subsequently guides fine-grained localization. To effectively leverage spatial attention information from the EEG modality and positional information from the EYE modality, we introduced a multimodal network to integrate EEG and EYE modalities into the multimodal messages. Furthermore, we achieve fine-grained target localization through a matching process between multimodal-based potential areas and eye-tracking-based potential areas. For evaluation, we conducted target image detection tasks on our dataset. Notably, the balanced accuracy (BA) of target detection using the division strategy reached 81.32%, with a BA-based information transfer rate of 76.32 bits/min, outperforming other methods and significantly improving detection efficiency.},
  archive      = {J_TCDS},
  author       = {Jianan Han and Longjie Ma and Li Zhu and Jiajia Tang and Wanzeng Kong},
  doi          = {10.1109/TCDS.2024.3523018},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {896--907},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Brain–Eye collaboration target detection and localization in remote sensing image},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual memory schemas for localized image memorability prediction. <em>TCDS</em>, <em>17</em>(4), 884--895. (<a href='https://doi.org/10.1109/TCDS.2025.3533112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual memory schemas (VMS) capture the regions of scene images that cause that scene to be remembered, providing a two-dimensional memorability map that indicates the parts of a given scene that match to mental schemas held in the mind. Despite the advantage of determining which parts of an image lead to remembering said image, VMS prediction capabilities lag behind those of single-score memorability. Compared with predicting single-score ratings for the likelihood of a person remembering an image, VMS prediction is a significantly harder task, due to increased computational complexity, minimal model development compared with single score, and lack of relevant data. In this work, we aim to improve methods for two-dimensional memorability prediction. We first significantly increase the size of a database containing VMS maps obtained from participants in a scene memorization experiment, and then we develop an architecture that leverages existing single-score image memorability datasets to predict VMS maps. Our final model, dual-feedback VMS (DF-VMS) significantly outperforms existing VMS prediction models, with a performance increase of 11.8%. Additionally, we explore the semantic structures that are actually captured by visual memory schemas, determining the combination of scene elements that lead to remembering that scene.},
  archive      = {J_TCDS},
  author       = {Cameron Kyle-Davidson and Adrian G. Bors and Karla K. Evans},
  doi          = {10.1109/TCDS.2025.3533112},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {884--895},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Visual memory schemas for localized image memorability prediction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering brain network modules in major depression during naturalistic music perception with block term decomposition. <em>TCDS</em>, <em>17</em>(4), 874--883. (<a href='https://doi.org/10.1109/TCDS.2024.3523020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Major depressive disorder (MDD) has been linked to altered brain networks and might be relieved by music therapy. Yet, the neurophysiological basis, especially the functional network mechanism, of music therapy on depression remains poorly understood. Here, we apply a novel dynamic module detection method based on block term decomposition (BTD) to examine the reorganization of time-varying topological network structures during music listening in MDD using electroencephalography (EEG). Specifically, temporal adjacency matrices generated using a sliding-window technique form a three-way tensor. The multilinear rank-$(L,L,1)$ BTD is applied to directly derive hidden network modules with specific time evolution from the temporally concatenated tensors for each frequency band. After temporal correlation analysis with musical features extracted from music stimuli, we identify several frequency-dependent network modules with specific temporal patterns modulated by musical features. These modular networks encompass subnetworks of default mode, frontoparietal, language, and sensorimotor networks involved in the delta, alpha, and beta bands, exhibiting significantly different modulations by music between healthy control and MDD groups. These results implicate that the altered oscillatory modular networks might affect the dynamic processing of musical features for MDD patients, which could offer valuable perspectives on revealing the neural mechanisms of music therapy for MDD.},
  archive      = {J_TCDS},
  author       = {Yongjie Zhu and Yuxing Hao and Jia Liu and Fengyu Cong},
  doi          = {10.1109/TCDS.2024.3523020},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {874--883},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Uncovering brain network modules in major depression during naturalistic music perception with block term decomposition},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDAG-net: Multidomain association-guided network for image-based long-term visual localization. <em>TCDS</em>, <em>17</em>(4), 859--873. (<a href='https://doi.org/10.1109/TCDS.2024.3525180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the case of long-term changing environment, long-term visual localization is a challenging problem in autonomous driving and mobile robots. Due to the influence of season, illumination and other changing weather conditions, the traditional image retrieval methods are difficult to achieve ideal results in long-term visual localization. Therefore, inspired by the human brain associative recognition function, an image retrieval based on a multidomain association-guided network is proposed to solve the long-term visual localization problem. The key idea is to extract the discriminative domain-invariant features in different scenes through multidomain image transformation of the perceptual network and the conceptual network. In addition, in order to better associate image features of different scenes in the conceptual network and guide the perceptual network to obtain more robust domain invariant features, an association-guided module is designed without the need for external datasets. On this basis, the domain feature loss function and the guidance mechanism of the loss function are introduced to assist these two network models training to obtain better performance. Finally, experiments are carried out on the CMU-Seasons dataset and the RobotCar-Seasons dataset. Compared with some state-of-the-art methods, the proposed method improved the high-precision localization result of urban, suburban, and park scenes in the CMU-Seasons dataset by 1.5%, 0.5%, and 0.7%, respectively, which also can verify the effectiveness of the proposed method under various seasonal and illumination conditions.},
  archive      = {J_TCDS},
  author       = {Fawei Ge and Yunzhou Zhang and Li Wang and Yanhai Tan and Sonya Coleman and Dermot Kerr},
  doi          = {10.1109/TCDS.2024.3525180},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {859--873},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {MDAG-net: Multidomain association-guided network for image-based long-term visual localization},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiobject graph affordance network: Goal-oriented planning through learned compound object affordances. <em>TCDS</em>, <em>17</em>(4), 847--858. (<a href='https://doi.org/10.1109/TCDS.2024.3520086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning object affordances is an effective tool in the field of robot learning. While the data-driven models investigate affordances of single or paired objects, there is a gap in the exploration of affordances of compound objects composed of an arbitrary number of objects. We propose the multiobject graph affordance network, which models complex compound object affordances by learning the outcomes of robot actions that facilitate interactions between an object and a compound. Given the depth images of the objects, the object features are extracted via convolution operations and encoded in the nodes of graph neural networks. Graph convolution operations are used to encode the state of the compounds, which are used as input to decoders to predict the outcome of the object-compound interactions. After learning the compound object affordances, given different tasks, the learned outcome predictors are used to plan sequences of stack actions that involve stacking objects on top of each other, inserting smaller objects into larger containers, and passing through ringlike objects through poles. We showed that our system successfully modeled the affordances of compound objects that include concave and convex objects, in both simulated and real-world environments. We benchmarked our system with a baseline model to highlight its advantages.},
  archive      = {J_TCDS},
  author       = {Tuba Girgin and Emre Uğur},
  doi          = {10.1109/TCDS.2024.3520086},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {847--858},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multiobject graph affordance network: Goal-oriented planning through learned compound object affordances},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust gaze-based intention prediction for real-world scenarios. <em>TCDS</em>, <em>17</em>(4), 835--846. (<a href='https://doi.org/10.1109/TCDS.2024.3519904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing intention prediction scenarios in daily life primarily focus on 2-D screens, while the process of intention expression in 3-D scenarios remains largely unexplored. We first analyze eye-tracking data from both 2-D and 3-D scenarios to reveal differences in cognitive load. To address the increased error and redundant gaze points in 3-D scenarios, we propose a gaze region model combined with a clustering method based on density and ordering principles, providing a robust representation of visual attention. Additionally, we integrate this visual attention representation with advanced classifiers for intention prediction. The results indicate that when intentions are expressed in a 3-D scenario, subjects’ cognitive load is reduced, facilitating their understanding and expression of intentions, ultimately improving the accuracy of intention prediction. Simultaneously, an evaluation of existing visual attention representation models related to intention prediction is conducted. Our proposed 3-D visual attention model, as part of the intention prediction framework, improves accuracy to 94.50%. To validate the theory and model, we introduce the ADLIP Gaze dataset, which consists of data from 102 individuals. These findings are expected to provide theoretical explanations and methods for intention prediction and efficient human–robot interaction in 3-D scenarios.},
  archive      = {J_TCDS},
  author       = {Zihang Yin and Zhonghua Wan and Mingxuan Yang and Yi Xiong and Wei Wang and Shiqian Wu},
  doi          = {10.1109/TCDS.2024.3519904},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {835--846},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Robust gaze-based intention prediction for real-world scenarios},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A neurodynamic-diversity-based spiking network model for text classification. <em>TCDS</em>, <em>17</em>(4), 823--834. (<a href='https://doi.org/10.1109/TCDS.2024.3523338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language models have been undergoing rapid growth and remarkable success, while requiring massive computing resource. The brain-inspired spiking neural networks (SNNs), with advantages of better biological interpretability and less energy consumption, provides a likely alternative to process language tasks in a more sustainable way. However, there are still major difficulties in representing and processing text information with SNN-based models. Comprehensively exploring the neurodynamic diversity, we propose a spiking neural network model that could taking respective advantages from both integrator and resonator neurons to address text classification tasks. With collaborations of these two dynamically different spiking neurons, our network model outperforms previous SNN-based text classification model in an all-round way with much less time steps, and even shows some extent of potential to reach the performance of a topologically equivalent artificial neural network.},
  archive      = {J_TCDS},
  author       = {Yuguo Liu and Wenyu Chen and Hanwen Liu and Yun Zhang and Malu Zhang and Hong Qu},
  doi          = {10.1109/TCDS.2024.3523338},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {823--834},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A neurodynamic-diversity-based spiking network model for text classification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the mechanisms of bidirectional plasticity from temporal feature stimuli in cerebellar motor learning: A ca2+-mediated simulation. <em>TCDS</em>, <em>17</em>(4), 809--822. (<a href='https://doi.org/10.1109/TCDS.2024.3517694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cerebellum plays a vital role in motor learning. The delay eyeblink conditioning is a standard protocol for studying cerebellar function from both computational and experimental perspectives. Ca2+-mediated bidirectional plastic changes between parallel fibers and Purkinje cells are regarded as the most important modulation that can dominate cerebellar motor learning. However, the mechanism of such modulation is unclear and difficult to uncover with experimental methods in vivo. In this study, we propose a biologically plausible learning rule for parallel fibers-Purkinje cells (PFs-PCs) bidirectional synaptic plasticity based on the inositol 1,4,5-trisphosphate (IP3)-βCaMKII-AMPAR cascade mediated by Ca2+. We simulate the process of AMPA receptor phosphorylation and dephosphorylation which are influenced by the concentration of regenerative Ca2+ and IP3 mediated by the coactivation of parallel fiber and climbing fiber to Purkinje cell. Using this model, Purkinje cells can not only learn the responses of single interstimulus interval (ISI), sequential double ISIs, and two sessions of different ISIs, but also show excitatory responses after short ISI afferents, consistent with our observation. In addition, the conditioned response maxima of the simulation results all appear before the expected unconditioned stimulus input, which confirms the biological experimental findings. These results suggest that PF-PC plasticity based on the IP3-Ca2+-AMPAR cascade may be the key to revealing unknown mechanisms of cerebellar motor learning and our model can reproduce subtle details of the motor learning process of delay eyeblink conditioning.},
  archive      = {J_TCDS},
  author       = {Tao Xu and Zhikun Wang and Jiaqing Chen and Jiajia Huang and Guosong Wu and Ya Ke and Wing Ho Yung},
  doi          = {10.1109/TCDS.2024.3517694},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {809--822},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Exploring the mechanisms of bidirectional plasticity from temporal feature stimuli in cerebellar motor learning: A ca2+-mediated simulation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performer: A high-performance global-local model-augmented with dual network interaction mechanism. <em>TCDS</em>, <em>17</em>(4), 794--808. (<a href='https://doi.org/10.1109/TCDS.2024.3519629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In deep learning, convolutional neural networks (CNNs) focus on local information through convolutional kernels, while transformers attend to global information using self-attention mechanisms. The union of these distinct approaches enables a more comprehensive extraction of image features. However, the feature map dimensions of CNN and Transformer differ, leading to dimension mismatch issues when combining these architectures. Additionally, the parameter size of the hybrid model integrating both architectures remains large, making it difficult to train. To further augmenting the interpretation of complex image patterns, we present Performer, a dual-network architecture that seamlessly combines CNNs and transformers, resulting in a novel and efficient representation learning model. In the Performer model, we innovate by devising a unique interaction methodology for CNN and transformer architectures to enhance the image feature extraction capabilities mutually. To counteract issue of dimensionality mismatch, we also introduce a refined transformer block, a advancement over the transformer block of ViT. To validate the effectiveness of Performer, we conduct extensive experiments on both classification and segmentation tasks. Performer achieve an accuracy of 83.37% on the ImageNet-200 dataset. For semantic segmentation, Performer excels on the CamVid and Hippocampus datasets. On CamVid, our model achieves a mean intersection over union (mIoU) of 63.27% and pixel accuracy of 92.11%, demonstrating superior performance in capturing fine details and handling complex scenes effectively. The code is available at https://github.com/hlf-thh/Performer.},
  archive      = {J_TCDS},
  author       = {Dayu Tan and Linfeng Hua and Rui Hao and Qi Xu and Yansen Su and Chunhou Zheng and Weimin Zhong},
  doi          = {10.1109/TCDS.2024.3519629},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {794--808},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Performer: A high-performance global-local model-augmented with dual network interaction mechanism},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A gradient descent-based backend feedback adaptive motion planning algorithm for autonomous mobile robots. <em>TCDS</em>, <em>17</em>(4), 784--793. (<a href='https://doi.org/10.1109/TCDS.2024.3519319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces an innovative motion planning algorithm for autonomous mobile robots, specifically focusing on quadrotor unmanned aerial vehicles (UAVs), utilizing a gradient descent-enhanced frontend and backend architecture. A trajectory planning algorithm is proposed for the front-end part. It relies on backend optimization feedback and memorized jump points. The algorithm builds on the jump point search (JPS) algorithm and introduces an obstacle table and jump point table. A new heuristic function is proposed, which emphasizes the weight of obstacle proportion in order to avoid getting stuck in local optimal paths. In the backend trajectory optimization part, a backend space-time trajectory optimization method based on gradient descent is proposed, and an optimization objective function is designed to ensure the smoothness and safety of the UAV trajectory. The simulation results show that the algorithm proposed in this article has significant advantages for improving real-time performance and environmental adaptability compared with the method based on ESDF and the EGO-planner. The actual flight experiments show that the proposed algorithm can avoid UAVs getting stuck in local optima during path planning. Notably, the proposed methodology also holds promise for application in path planning for other autonomous robots.},
  archive      = {J_TCDS},
  author       = {Gang Li and Si-Cheng Wang and Bin Cheng and Zhong-Pan Zhu},
  doi          = {10.1109/TCDS.2024.3519319},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {784--793},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A gradient descent-based backend feedback adaptive motion planning algorithm for autonomous mobile robots},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG-based cognitive load estimation of acoustic parameters for data sonification. <em>TCDS</em>, <em>17</em>(4), 771--783. (<a href='https://doi.org/10.1109/TCDS.2025.3525492'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sonification is a data visualization technique which expresses data attributes via psychoacoustic parameters, which are nonspeech audio signals used to convey information. This article investigates the binary estimation of cognitive load induced by psychoacoustic parameters conveying the focus level of an astronomical image via electroencephalogram (EEG) embeddings. Employing machine learning and deep learning methodologies, we demonstrate that EEG signals are reliable for 1) binary estimation of cognitive load; 2) isolating easy versus difficult visual-to-auditory perceptual mappings; and 3) capturing perceptual similarities among psychoacoustic parameters. Our key findings reveal that 1) EEG embeddings can reliably measure cognitive load, achieving a peak F1-score of 0.98; 2) extreme focus levels are easier to detect via auditory mappings than intermediate ones; and 3) psychoacoustic parameters inducing comparable cognitive load levels tend to generate similar EEG encodings.},
  archive      = {J_TCDS},
  author       = {Gulshan Sharma and Surbhi Madan and Maneesh Bilalpur and Abhinav Dhall and Ramanathan Subramanian},
  doi          = {10.1109/TCDS.2025.3525492},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {771--783},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {EEG-based cognitive load estimation of acoustic parameters for data sonification},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multidocument summarization via graph representation learning. <em>TCDS</em>, <em>17</em>(4), 759--770. (<a href='https://doi.org/10.1109/TCDS.2024.3519181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of multidocument summarization (MDS) is to generate a comprehensive and concise summary from multiple documents, which should not only be grammatically correct but also semantically contains the refined content of the overall texts. Existing summarizers based on sequential pretrained large language models often cognize documents as linear sequences, which overlook the hierarchical structure correlations of sentences and paragraphs within or between documents. Additionally, those models also have limitations in handling long text input. To alleviate these two problems, a multidocument summarization model is proposed, with a heterogeneous graph of sentences, paragraphs and documents, called HeterMDS, to uncover deep semantic meanings and local–global context within documents. By integrating large language model and graph encoder with bootstrapped graph latents, the proposed HeterMDS can learn a semantically rich document representation and generate a coherent, concise and fact-consistent summary. It can be flexibly applied to current pretrained language models, effectively improving their performance in MDS. Extensive experiment results can verify the effectiveness of the proposed HeterMDS and its contained modules, and demonstrate its competitiveness against the state-of-the-art models.},
  archive      = {J_TCDS},
  author       = {Gui-Huang Zeng and Yan-Qin Liu and Chun-Yang Zhang and Hai-Chun Cai and C. L. Philip Chen},
  doi          = {10.1109/TCDS.2024.3519181},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {759--770},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Adaptive multidocument summarization via graph representation learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating human intention into multirobot decision making via Brain–Computer interface enabled shared autonomy. <em>TCDS</em>, <em>17</em>(4), 746--758. (<a href='https://doi.org/10.1109/TCDS.2024.3518544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multirobot systems tend to have higher execution efficiency when performing tasks such as mapping, search, and space exploration. Still, due to the influence of sensor measurement error, the decision-making of multirobot systems usually has deviations that are difficult to eliminate. In this unfavorable situation, the advantage of human experience can generally guide the multirobot system in making the correct decision. This study proposed a high-resolution brain–computer interface (BCI) paradigm and constructed a human intention probability model through graph neural networks. This allows for the preservation of richer interactive information, capturing the inherent uncertainty and preference features of human intention. Meanwhile, a BCI-enabled shared autonomy strategy integrating probabilistic human intention through opinion dynamics is introduced, ensuring the collaborative participation of humans and robots in decision-making. Experimental results show that the shared autonomy approach significantly improves decision-making accuracy compared to the initial multirobot estimate. Further analysis shows that this approach greatly outperforms traditional BCI strategies, showing promise for human–multirobot cooperation in complex task environments.},
  archive      = {J_TCDS},
  author       = {Wei Dai and Yaru Liu and Huimin Lu and Zongtan Zhou},
  doi          = {10.1109/TCDS.2024.3518544},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {746--758},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Integrating human intention into multirobot decision making via Brain–Computer interface enabled shared autonomy},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancements in automated assessment and diagnosis of autism spectrum disorder through multimodality sensing technologies: Survey of the last decade. <em>TCDS</em>, <em>17</em>(4), 727--745. (<a href='https://doi.org/10.1109/TCDS.2025.3574145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autism spectrum disorder (ASD) is a complex neurodevelopmental disorder characterized by difficulties in social interaction, communication, and repetitive behavior patterns. Traditional research approaches have primarily focused on studying autism using single-modal data analysis, such as relying solely on audio, video, and neuro signals. However, recent advancements in technology, cognitive science, and artificial intelligence (AI) have provided opportunities to explore the potential benefits of multisensory integration and fusion of modalities in understanding autism patterns. This survey makes three key contributions to advancing the future of ASD diagnosis and intervention. First, it provides a comprehensive review of recent advancements in multimodal sensing technologies, detailing primary modalities, data cleaning and synchronization techniques, feature extraction, and fusion methodologies to integrate diverse sensory data. Second, it classifies assistive technologies into three major categories: 1) computer-based systems; 2) virtual reality simulations; and 3) robotic interactions, analyzing their applications for cross-referencing symptoms and enabling real-time interventions in skills assessment and therapy. Third, it identifies critical challenges related to data collection, sensor synchronization, standardizing assessment paradigms, and real-time processing demands, proposing actionable future directions to improve diagnostic precision, scalability, and adaptability. These contributions underscore the transformative potential of multimodal sensing systems to revolutionize ASD assessment and diagnosis by enabling comprehensive, objective, and tailored solutions for diverse individuals across the autism spectrum.},
  archive      = {J_TCDS},
  author       = {Athmar N. M. Shamhan and Marwa Qaraqe and Dena Al-Thani},
  doi          = {10.1109/TCDS.2025.3574145},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {727--745},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Advancements in automated assessment and diagnosis of autism spectrum disorder through multimodality sensing technologies: Survey of the last decade},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging the gap: Cognitive science perspectives and artificial intelligence for prediction and detection of specific learning disabilities. <em>TCDS</em>, <em>17</em>(4), 711--726. (<a href='https://doi.org/10.1109/TCDS.2025.3562665'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Specific learning disabilities are complex neurodevelopmental disorders that significantly impact an individual’s academic achievements, social interactions, and overall well-being. Both cognitive science and technological developments geared toward artificial intelligence have helped us better understand and serve people with specific learning disabilities. This review article examines cognitive science and artificial intelligence studies, focusing on predicting and detecting the most prevalent specific learning disabilities, namely dyslexia, dysgraphia, and dyscalculia in children. It aims to establish a correlation between cognitive research and artificial intelligence techniques that can benefit the affected population. Understanding both domains enables the development of more effective artificial intelligence technologies grounded in cognitive science principles. Artificial intelligence can revolutionize early prediction, detection, and interventions for specific learning disabilities. The success of this effort lies in the collaboration among data scientists, clinicians, and domain experts.},
  archive      = {J_TCDS},
  author       = {Subha Sreekumar and Lijiya A and Rajith K Ravindren},
  doi          = {10.1109/TCDS.2025.3562665},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {8},
  number       = {4},
  pages        = {711--726},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Bridging the gap: Cognitive science perspectives and artificial intelligence for prediction and detection of specific learning disabilities},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain network reorganization in response to multilevel mental workload in simulated flight tasks. <em>TCDS</em>, <em>17</em>(3), 698--709. (<a href='https://doi.org/10.1109/TCDS.2024.3511394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various real-world situations, inappropriate mental workload (MWL) can impair task performance and may cause operational safety risks. Growing efforts have been made to reveal the underlying neural mechanisms of MWL. However, most studies have been limited to well-controlled cognitive tasks, overlooking the exploration of the underlying neural mechanisms in close-to-real human–machine interaction tasks. Here, we investigated the brain network reorganization in response to MWL in a close-to-real simulated flight task. Specifically, a dual-task (primary flight simulation + secondary auditory choice reaction time task) design flight simulation paradigm to mimic real-flight cognitive challenges was introduced to induce varying levels of MWL. The perceived subjective task difficulty and secondary task performance validated the effectiveness of our experimental design. Moreover, multilevel MWL classification was performed to delve into the changes of functional connectivity (FC) in response to different MWL and achieved satisfactory performance (three levels, accuracy $=$ 71.85%). Further inspection of the discriminative FCs highlighted the importance of frontal and parietal-occipital brain regions in MWL modulation. Additional graph theoretical analysis revealed increased information transfer efficiency across distributed brain regions with the increase of MWL. Overall, our research offers valuable insights into the neural mechanisms underlying MWL, with potential implications for improving safety in aviation contexts.},
  archive      = {J_TCDS},
  author       = {Kuijun Wu and Jingjia Yuan and Xianliang Ge and Ioannis Kakkos and Linze Qian and Sujie Wang and Yamei Yu and Chuantao Li and Yu Sun},
  doi          = {10.1109/TCDS.2024.3511394},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {698--709},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Brain network reorganization in response to multilevel mental workload in simulated flight tasks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Location-guided head pose estimation for fisheye image. <em>TCDS</em>, <em>17</em>(3), 682--697. (<a href='https://doi.org/10.1109/TCDS.2024.3506060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera with a fisheye or ultra-wide lens covers a wide field of view that cannot be modeled by the perspective projection. Serious fisheye lens distortion in the peripheral region of the image leads to degraded performance of the existing head pose estimation models trained on undistorted images. This article presents a new approach for head pose estimation that uses the knowledge of head location in the image to reduce the negative effect of fisheye distortion. We develop an end-to-end convolutional neural network to estimate the head pose with the multitask learning of head pose and head location. Our proposed network estimates the head pose directly from the fisheye image without the operation of rectification or calibration. We also created a fisheye-distorted version of the three popular head pose estimation datasets, BIWI, 300W-LP, and AFLW2000 for our experiments. Experimental results show that our network remarkably improves the accuracy of head pose estimation compared with other state-of-the-art one-stage and two-stage methods.},
  archive      = {J_TCDS},
  author       = {Bing Li and Dong Zhang and Cheng Huang and Yun Xian and Ming Li and Dah-Jye Lee},
  doi          = {10.1109/TCDS.2024.3506060},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {682--697},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Location-guided head pose estimation for fisheye image},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of audio trigger’s frequency on autonomous sensory meridian response. <em>TCDS</em>, <em>17</em>(3), 672--681. (<a href='https://doi.org/10.1109/TCDS.2024.3506039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous sensory meridian response (ASMR) is an experience-dependent sensation in response to audio and audio–visual triggers. The acoustical characteristics of audio trigger have been speculated to be in connection with ASMR. To explore the effect of audio trigger’s frequency on ASMR and then to discover ASMR’s mechanism, the ASMR phenomenon under random-frequency audio, high-frequency audio, low-frequency audio, original audio, white-noise and rest were analyzed by EEG. The differential entropy and power spectral density were applied to quantitative analysis. The results suggest the audio’s frequency can modulate the brain activities on θ, α, β, γ, and high γ frequencies. Moreover, ASMR responder and nonresponder may be more sensitive to low-frequency audio and white-noise by suppressing brain activities of central areas in γ and high γ frequencies. Further, for ASMR responders, ASMR evoked by low-frequency audio trigger may involve more attentional selection or semantic processing and may not alter the brain functions in information processing and execution.},
  archive      = {J_TCDS},
  author       = {Lili Li and Zhiqing Wu and Zhongliang Yu and Zhibin He and Zhizhong Wang and Liyu Lin and Shaolong Kuang},
  doi          = {10.1109/TCDS.2024.3506039},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {672--681},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The effect of audio trigger’s frequency on autonomous sensory meridian response},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A biomathematical model for classifying sleep stages using deep learning techniques. <em>TCDS</em>, <em>17</em>(3), 659--671. (<a href='https://doi.org/10.1109/TCDS.2024.3503767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A biomathematical model is a framework that calculates corresponding indices based on biological and physiological parameters, and can be used to study the fatigue states of submarine crew members during long-duration operations. Submarine personnel are prone to fatigue and decreased vigilance, leading to unnecessary risks. Sleep quality plays a crucial role in assessing human vigilance; however, traditional biomathematical models generally categorize human sleep into two different pressure stages based on circadian rhythms. To accurately classify sleep stages based on physiological signals, this article proposes a novel deep learning architecture using single-channel EEG signals. This architecture comprises four modules: beginning with a feature preliminary extraction module employing a multiscale convolutional neural network (MSCNN), followed by a feature aggregation module combining reparameterizable large kernel network with temporal convolutions network (RepLKnet), then utilizing a multivariate weighted recurrent network as the tensor encoder (MWRN), and finally, decoding with a dynamic graph convolutional neural network (DGCNN). The output is provided by a final classifier. We assessed the effectiveness of the proposed model using two publicly available datasets. The results demonstrate that our model surpasses current leading benchmarks.},
  archive      = {J_TCDS},
  author       = {Ruijie He and Wei Tong and Miaomiao Zhang and Guangyu Zhu and Edmond Q. Wu},
  doi          = {10.1109/TCDS.2024.3503767},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {659--671},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A biomathematical model for classifying sleep stages using deep learning techniques},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial–Temporal spiking feature pruning in spiking transformer. <em>TCDS</em>, <em>17</em>(3), 644--658. (<a href='https://doi.org/10.1109/TCDS.2024.3500018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are known for brain-inspired architecture and low power consumption. Leveraging biocompatibility and self-attention mechanism, Spiking Transformers become the most promising SNN architecture with high accuracy. However, Spiking Transformers still faces the challenge of high training costs, such as a 51$M$ network requiring 181 training hours on ImageNet. In this work, we explore feature pruning to reduce training costs and overcome two challenges: high pruning ratio and lightweight pruning methods. We first analyze the spiking features and find the potential for a high pruning ratio. The majority of information is concentrated on a part of the spiking features in spiking transformer, which suggests that we can keep this part of the tokens and prune the others. To achieve lightweight, a parameter-free spatial–temporal spiking feature pruning method is proposed, which uses only a simple addition-sorting operation. The spiking features/tokens with high spike accumulation values are selected for training. The others are pruned and merged through a compensation module called Softmatch. Experimental results demonstrate that our method reduces training costs without compromising image classification accuracy. On ImageNet, our approach reduces the training time from 181 to 128 h while achieving comparable accuracy (83.13% versus 83.07%).},
  archive      = {J_TCDS},
  author       = {Zhaokun Zhou and Kaiwei Che and Jun Niu and Man Yao and Guoqi Li and Li Yuan and Guibo Luo and Yuesheng Zhu},
  doi          = {10.1109/TCDS.2024.3500018},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {644--658},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Spatial–Temporal spiking feature pruning in spiking transformer},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interaction is worth more explanations: Improving Human–Object interaction representation with propositional knowledge. <em>TCDS</em>, <em>17</em>(3), 631--643. (<a href='https://doi.org/10.1109/TCDS.2024.3496566'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting human–object interactions (HOI) presents a formidable challenge, necessitating the discernment of intricate, high-level relationships between humans and objects. Recent studies have explored HOI vision-and-language modeling (HOI-VLM), which leverages linguistic information inspired by cross-modal technology. Despite its promise, current methodologies face challenges due to the constraints of limited annotation vocabularies and suboptimal word embeddings, which hinder effective alignment with visual features and consequently, the efficient transfer of linguistic knowledge. In this work, we propose a novel cross-modal framework that leverages external propositional knowledge which harmonize annotation text with a broader spectrum of world knowledge, enabling a more explicit and unambiguous representation of complex semantic relationships. Additionally, considering the prevalence of multiple complexities due to the symbiotic or distinctive relationships inherent in one HO pair, along with the identical interactions occurring with diverse HO pairs (e.g., “human ride bicycle” versus “human ride horse”). The challenge lies in understanding the subtle differences and similarities between interactions involving different objects or occurring in varied contexts. To this end, we propose the Jaccard contrast strategy to simultaneously optimize cross-modal representation consistency across HO pairs (especially for cases where multiple interactions occur), which encompasses both vision-to-vision and vision-to-knowledge alignment objectives. The effectiveness of our proposed method is comprehensively validated through extensive experiments, showcasing its superiority in the field of HOI analysis.},
  archive      = {J_TCDS},
  author       = {Feng Yang and Yichao Cao and Xuanpeng Li and Weigong Zhang},
  doi          = {10.1109/TCDS.2024.3496566},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {631--643},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Interaction is worth more explanations: Improving Human–Object interaction representation with propositional knowledge},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMART: Sequential multiagent reinforcement learning with role assignment using transformer. <em>TCDS</em>, <em>17</em>(3), 615--630. (<a href='https://doi.org/10.1109/TCDS.2024.3504256'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent reinforcement learning (MARL) has received increasing attention and been used to solve cooperative multiagent decision-making and learning control tasks. However, the high complexity of the joint action space and the nonstationary learning process are two major problems that negatively impact on the sample efficiency and solution quality of MARL. To this end, this article proposes a novel approach named sequential MARL with role assignment using transformer (SMART). By learning the effects of different actions on state transitions and rewards, SMART realizes the action abstraction of the original action space and the adaptive role cognitive modeling of multiagent, which reduces the complexity of the multiagent exploration and learning process. Meanwhile, SMART uses causal transformer networks to update role assignment policy and action selection policy sequentially, alleviating the influence of nonstationary multiagent policy learning. The convergence characteristic of SMART is theoretically analyzed. Extensive experiments on the challenging Google football and StarCraft multiagent challenge are conducted, demonstrating that compared with mainstream MARL algorithms such as MAT and HAPPO, SMART achieves a new state-of-the-art performance. Meanwhile, the learned policies through SMART have good generalization ability when the number of agents changes.},
  archive      = {J_TCDS},
  author       = {Yixing Lan and Hao Gao and Xin Xu and Qiang Fang and Yujun Zeng},
  doi          = {10.1109/TCDS.2024.3504256},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {615--630},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SMART: Sequential multiagent reinforcement learning with role assignment using transformer},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling task engagement to regulate reinforcement learning-based decoding for online brain control. <em>TCDS</em>, <em>17</em>(3), 606--614. (<a href='https://doi.org/10.1109/TCDS.2024.3492199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain–machine interfaces (BMIs) offer significant promise for enabling paralyzed individuals to control external devices using their brain signals. One challenge is that during the online brain control (BC) process, subjects may not be completely immersed in the task, particularly when multiple steps are needed to achieve a goal. The decoder indiscriminately takes the less engaged trials as training data, which might decrease the decoding accuracy. In this article, we propose an alternative kernel RL-based decoder that trains online with continuous parameter update. We model neural activity from the medial prefrontal cortex (mPFC), a reward-related brain region, to represent task engagement. This information is incorporated into a stochastic learning rate using an exponential model, which measures the relevancy of neural data. The proposed algorithm was evaluated in the experiment where rats performed a cursor-reaching BC task. We found the neural activities from mPFC contained the engagement information which was negatively correlated with trial response time. Moreover, compared to the RL method without task engagement modeling, our proposed method enhanced the training efficiency. It used half of the training data to achieve the same reconstruction accuracy of the cursor trajectory. The results demonstrate the potential of our RL framework for improving online BC tasks.},
  archive      = {J_TCDS},
  author       = {Xiang Zhang and Xiang Shen and Yiwen Wang},
  doi          = {10.1109/TCDS.2024.3492199},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {606--614},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Modeling task engagement to regulate reinforcement learning-based decoding for online brain control},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developmental networks with foveation. <em>TCDS</em>, <em>17</em>(3), 592--605. (<a href='https://doi.org/10.1109/TCDS.2024.3492181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The foveated nature of the human vision system (HVS) means the acuity on the retina peaks at the center of the fovea and gradually descends to the periphery with increasing eccentricity. Foveation is general-purpose, meaning the fovea is more often used than the periphery. Self-generated saccades dynamically project the fovea to different parts of the visual world so that the high-acuity fovea can process interested parts at different times. It is still unclear why biological vision uses foveation. This work is the first foveated neural network as far as we are aware, but it has a limited scope. We study two subjects here as follows. 1) We design a biological density of cones (BDOCs) foveation method for image warping to simulate a biologically plausible foveated retina using a commonly available uniform-pixel camera. 2) The subject of this article is not specific to tasks, but we choose a challenging task, visual navigation, as an example of quantitative and spatiotemporal tasks, and compare it with deep learning. Our experimental results showed that 1) the BDOC foveation is logically and visually correct; and 2) the developmental network (DN) performs better than deep learning in a surprising way and foveation helps both network types.},
  archive      = {J_TCDS},
  author       = {Xiang Wu and Juyang Weng},
  doi          = {10.1109/TCDS.2024.3492181},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {592--605},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Developmental networks with foveation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data augmentation for seizure prediction with generative diffusion model. <em>TCDS</em>, <em>17</em>(3), 577--591. (<a href='https://doi.org/10.1109/TCDS.2024.3489357'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation (DA) can significantly strengthen the electroencephalogram (EEG)-based seizure prediction methods. However, existing DA approaches are just the linear transformations of original data and cannot explore the feature space to increase diversity effectively. Therefore, we propose a novel diffusion-based DA method called DiffEEG. DiffEEG can fully explore data distribution and generate samples with high diversity, offering extra information to classifiers. It involves two processes: the diffusion process and the denoised process. In the diffusion process, the model incrementally adds noise with different scales to EEG input and converts it into random noise. In this way, the representation of data can be learned. In the denoised process, the model utilizes learned knowledge to sample synthetic data from random noise input by gradually removing noise. The randomness of input noise and the precise representation enable the synthetic samples to possess diversity while ensuring the consistency of feature space. We compared DiffEEG with original, down-sampling, sliding windows and recombination methods, and integrated them into five representative classifiers. The experiments demonstrate the effectiveness and generality of our method. With the contribution of DiffEEG, the multiscale CNN achieves state-of-the-art performance, with an average sensitivity, FPR, AUC of 95.4%, 0.051/h, 0.932 on the CHB-MIT database and 93.6%, 0.121/h, 0.822 on the Kaggle database.},
  archive      = {J_TCDS},
  author       = {Kai Shu and Le Wu and Yuchang Zhao and Aiping Liu and Ruobing Qian and Xun Chen},
  doi          = {10.1109/TCDS.2024.3489357},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {577--591},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Data augmentation for seizure prediction with generative diffusion model},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task and motion planning of service robot arm in unknown environment based on virtual voxel-semantic space. <em>TCDS</em>, <em>17</em>(3), 564--576. (<a href='https://doi.org/10.1109/TCDS.2024.3489773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A task and motion planning method for service robot arm based on 3-D voxel-semantic maps is proposed, which can realize virtual environment mapping, manipulator planning, and grasping tasks in unknown environments. First of all, a complete point cloud scene is obtained and spliced. Mask region-based convolutional neural network (RCNN) network is used to complete object detection and instance segmentation. A voxel-semantic hybrid map composed of 3-D point cloud, semantic information, and 3-D computer aided design (CAD) model is constructed. Second, an improved A* algorithm is proposed to plan the optimal path of robot arm end-effector. The Bezier curve interpolation is introduced to obtain the smooth trajectory. Third, the grasping poses of the robot gripper corresponding to different geometries are explored. Semantic-driven spatial task planning is achieved by decomposing robotic arm pick and place tasks. Finally, the effectiveness and rapidity of the proposed algorithm are verified in virtual space and real physical space, respectively.},
  archive      = {J_TCDS},
  author       = {Lipeng Wang and Xiaochen Wang and Junjun Huang and Mengjie Liu},
  doi          = {10.1109/TCDS.2024.3489773},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {564--576},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Task and motion planning of service robot arm in unknown environment based on virtual voxel-semantic space},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Kernel-based Actor–Critic learning framework for autonomous brain control on trajectory. <em>TCDS</em>, <em>17</em>(3), 554--563. (<a href='https://doi.org/10.1109/TCDS.2024.3485078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reinforcement learning (RL)-based brain–machine interfaces (BMIs) hold promise for restoring motor functions in paralyzed individuals. These interfaces interpret neural activity to control external devices through trial-and-error. In brain control (BC) tasks, subjects control the device continuously moving in space by imagining their own limb movement, in which the subject can change direction at any position before reaching the target. Such multistep BC tasks span a large space both in neural state and over a sequence of movements. However, conventional RL decoders face challenges in efficient exploration and limited guidance from delayed rewards. In this article, we propose a kernel-based actor–critic learning framework for multistep BC tasks. Our framework integrates continuous trajectory control (actor) and internal continuous state value estimation (critic) from medial prefrontal cortex (mPFC) activity. We evaluate our algorithm's performance in a BC three-lever discrimination task using data from two rats, comparing it to a kernel RL decoder with internal binary rewards and delayed external rewards. Experimental results show that our approach achieves faster convergence, shorter target-acquisition time, and shorter distances to targets. These findings highlight the potential of our algorithm for clinical applications in multistep BC tasks.},
  archive      = {J_TCDS},
  author       = {Zhiwei Song and Xiang Zhang and Shuhang Chen and Jieyuan Tan and Yiwen Wang},
  doi          = {10.1109/TCDS.2024.3485078},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {554--563},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Kernel-based Actor–Critic learning framework for autonomous brain control on trajectory},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive environment generation for continual learning: Integrating constraint logic programming with deep reinforcement learning. <em>TCDS</em>, <em>17</em>(3), 540--553. (<a href='https://doi.org/10.1109/TCDS.2024.3485482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a novel framework that combines constraint logic programming (CLP) with deep reinforcement learning (DRL) to create adaptive environments for continual learning. We focus on two challenging domains: Sudoku puzzles and scheduling problems, where environment complexity evolves based on the agent's performance. By integrating CLP, we dynamically adjust problem difficulty in response to the agent's learning trajectory, ensuring a progressively challenging environment that fosters enhanced problem-solving skills. Empirical results across 500 000 episodes show substantial improvements in solve rates, increasing from 6% to 86% for sudoku puzzles and 7% to 79% for scheduling problems, alongside significant reductions in the average steps required to solve each problem. The proposed adaptive environment generation demonstrates the potential of CLP in advancing RL agents’ continual learning capabilities by dynamically regulating complexity, thus improving their adaptability and learning efficiency. This framework contributes to the broader fields of reinforcement learning and procedural content generation by introducing an innovative approach to continual adaptation in complex environments.},
  archive      = {J_TCDS},
  author       = {Youness Boutyour and Abdellah Idrissi},
  doi          = {10.1109/TCDS.2024.3485482},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {540--553},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Adaptive environment generation for continual learning: Integrating constraint logic programming with deep reinforcement learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A task-oriented deep learning approach for human localization. <em>TCDS</em>, <em>17</em>(3), 525--539. (<a href='https://doi.org/10.1109/TCDS.2024.3485886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio-based human sensing has attracted substantial research attention due to its wide range of applications, including e-healthcare monitoring, indoor security, and industrial surveillance. However, most existing studies rely on fixed receivers to capture wireless signal perturbations. This article introduces UH-Sense, the first human sensing system using an unmanned aerial vehicle (UAV) equipped with an omnidirectional antenna to measure signal strength from surrounding WiFi access points (APs). UH-Sense addresses the challenge of multisource UAV-induced noise with a novel data-driven learning-based approach that denoises corrupted data without prior knowledge of noise characteristics. Furthermore, we develop a localization model based on radio tomography imaging (RTI) that localizes humans without collecting the fingerprint database. We demonstrate that UH-Sense is readily deployable on commodity platforms and evaluate its performance in different real-world environments including irregular AP deployment and nonline-of-sight (NLOS) scenarios. Experimental results show that UH-Sense achieves a high detection performance with an average F1 score of 0.93 and yields similar or even better localization performance than that of using clean data (i.e., data collected at a fixed receiver), which has not been achieved by any of the state-of-the-art denoising methods.},
  archive      = {J_TCDS},
  author       = {Yu-Jia Chen and Wei Chen and Sai Qian Zhang and Hai-Yan Huang and H.T. Kung},
  doi          = {10.1109/TCDS.2024.3485886},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {525--539},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A task-oriented deep learning approach for human localization},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous estimation of human motion intention and time-varying arm stiffness for enhanced Human–Robot interaction. <em>TCDS</em>, <em>17</em>(3), 510--524. (<a href='https://doi.org/10.1109/TCDS.2024.3480854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in physiological human motor control research indicate that human endpoint stiffness magnitude increases linearly with grasp force. Based on these findings, a scheme was proposed in this article to integrate the linear quadratic estimation (LQE) filter with the stiffness model inferred from grasp force, which can simultaneously estimate the human arm's stiffness and motion intention. Then, an online variable impedance controller (VIC) was designed based on these estimations for physical human–robot interaction (pHRI). The proposed stiffness model and estimation method were validated through experiments using a planar robotic interface. To assess its performance in practical pHRI tasks, the implementation of human arm stiffness and intention estimation combining with VIC was extended to teleoperation peg-in-hole and robot-assisted rehabilitation tasks. The experimental results demonstrate that the proposed method can effectively estimate human motion intention and arm stiffness simultaneously. Compared to existing methods, the proposed VIC enhances pHRI in terms of increased flexibility, effective guidance, and reduced human effort.},
  archive      = {J_TCDS},
  author       = {Huayang Wu and Chengzhi Zhu and Long Cheng and Chenguang Yang and Yanan Li},
  doi          = {10.1109/TCDS.2024.3480854},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {510--524},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Simultaneous estimation of human motion intention and time-varying arm stiffness for enhanced Human–Robot interaction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PDRL: Towards deeper states and further behaviors in unsupervised skill discovery by progressive diversity. <em>TCDS</em>, <em>17</em>(3), 495--509. (<a href='https://doi.org/10.1109/TCDS.2024.3471645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present progressive diversity reinforcement learning (PDRL), an unsupervised reinforcement learning (URL) method for discovering diverse skills. PDRL encourages learning behaviors that span multiple steps, particularly by introducing “deeper states”—states that require a longer sequence of actions to reach without repetition. To address the challenges of weak skill diversity and weak exploration in partially observable environments, PDRL employs two indications for skill learning to foster exploration and skill diversity, emphasizing each observation and subtrajectory's accuracy compared to its predecessor. Skill latent variables are represented by mappings from states or trajectories, helping to distinguish and recover learned skills. This dual representation promotes exploration and skill diversity without additional modeling or prior knowledge. PDRL also integrates intrinsic rewards through a combination of observations and subtrajectories, effectively preventing skill duplication. Experiments across multiple benchmarks show that PDRL discovers a broader range of skills compared to existing methods. Additionally, pretraining with PDRL accelerates fine-tuning in goal-conditioned reinforcement learning (GCRL) tasks, as demonstrated in Fetch robotic manipulation tasks.},
  archive      = {J_TCDS},
  author       = {Ziming He and Chao Song and Jingchen Li and Haobin Shi},
  doi          = {10.1109/TCDS.2024.3471645},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {495--509},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {PDRL: Towards deeper states and further behaviors in unsupervised skill discovery by progressive diversity},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Functional connectivity patterns learning for EEG-based emotion recognition. <em>TCDS</em>, <em>17</em>(3), 480--494. (<a href='https://doi.org/10.1109/TCDS.2024.3470248'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuroscience research reveals that different emotions are associated with different functional connectivity structures of brain regions. However, many existing electroencephalography (EEG)-based emotion recognition methods use these connectivity patterns broadly without distinguishing between specific emotions. Additionally, the nonstationarity of EEG signals often results in high variations across different periods, leading models to extract time-specific features instead of emotional features. This article proposes a functional connectivity patterns learning network (FCPL) for EEG-based emotion recognition to address these challenges. FCPL includes a coefficient branch, a graph construction module, and a period domain adversarial module. These components capture individual characteristics and specific emotional connectivity patterns and reduce period-related variations, respectively. FCPL achieves state-of-the-art results: 42.04%/28.81% for seven-class subject-dependent/independent experiments on the MPED dataset, 97.45%/89.88% for subject-dependent/independent experiments on the SEED dataset, and 95.98%/96.19% for valence/arousal subject-dependent experiments and 67.90%/65.60% for valence/arousal subject-independent experiments on the DREAMER dataset. This work advances the exploration of functional connectivity structures in EEG signals from coarse-grained emotion-related patterns to fine-grained emotional distinctions, promoting neuroscience, and EEG-based emotion recognition technologies.},
  archive      = {J_TCDS},
  author       = {Chongxing Shi and C. L. Philip Chen and Shuzhen Li and Tong Zhang},
  doi          = {10.1109/TCDS.2024.3470248},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {480--494},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Functional connectivity patterns learning for EEG-based emotion recognition},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HDMTK: Full integration of hierarchical decision-making and tactical knowledge in multiagent adversarial games. <em>TCDS</em>, <em>17</em>(3), 465--479. (<a href='https://doi.org/10.1109/TCDS.2024.3470068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of adversarial games, existing decision-making algorithms primarily rely on reinforcement learning, which can theoretically adapt to diverse scenarios through trial and error. However, these algorithms often face the challenges of low effectiveness and slow convergence in complex wargame environments. Inspired by how human commanders make decisions, this article proposes a novel method named full integration of hierarchical decision-making and tactical knowledge (HDMTK). This method comprises an upper reinforcement learning module and a lower multiagent reinforcement learning (MARL) module. To enable agents to efficiently learn the cooperative strategy, in HDMTK, we separate the whole task into explainable subtasks and devise their corresponding subgoals for shaping the online rewards based on tactical knowledge. Experimental results on the wargame simulation platform “MiaoSuan” show that, compared to the advanced MARL methods, HDMTK exhibits superior performance and faster convergence in the complex scenarios.},
  archive      = {J_TCDS},
  author       = {Wei Li and Boling Hu and Aiguo Song and Kaizhu Huang},
  doi          = {10.1109/TCDS.2024.3470068},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {465--479},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {HDMTK: Full integration of hierarchical decision-making and tactical knowledge in multiagent adversarial games},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing dimensional image emotion detection with a low-resource dataset via two-stage training. <em>TCDS</em>, <em>17</em>(3), 455--464. (<a href='https://doi.org/10.1109/TCDS.2024.3465602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image emotion analysis has gained notable attention owing to the growing importance of computationally modeling human emotions. Most previous studies have focused on classifying the feelings evoked by an image into predefined emotion categories. Compared with these categorical approaches which cannot address the ambiguity and complexity of human emotions, recent studies have taken dimensional approaches to address these problems. However, there is still a limitation in that the number of dimensional datasets is significantly smaller for model training, compared with many available categorical datasets. We propose four types of frameworks that use categorical datasets to predict emotion values for a given image in the valence–arousal (VA) space. Specifically, our proposed framework is trained to predict continuous emotion values under the supervision of categorical labels. Extensive experiments demonstrate that our approach showed a positive correlation with the actual VA values of the dimensional dataset. In addition, our framework improves further when a small number of dimensional datasets are available for the fine-tuning process.},
  archive      = {J_TCDS},
  author       = {SangEun Lee and Seoyun Kim and Yubeen Lee and Jufeng Yang and Eunil Park},
  doi          = {10.1109/TCDS.2024.3465602},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {6},
  number       = {3},
  pages        = {455--464},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Enhancing dimensional image emotion detection with a low-resource dataset via two-stage training},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-laplacian-processing-based multimodal localization backend for robots and autonomous systems. <em>TCDS</em>, <em>17</em>(2), 436--453. (<a href='https://doi.org/10.1109/TCDS.2024.3468712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous localization and mapping (SLAM) for positioning of robots and autonomous systems (RASs) and mapping of their surrounding environments is a task of major significance in various applications. However, the main disadvantage of traditional SLAM is that the deployed backend modules suffer from accumulative error caused by sharp viewpoint changes, diverse weather conditions, etc. As such, to improve the localization accuracy of the moving agents, we propose a cost-effective and loosely coupled relocalization backend, deployed on top of original SLAM algorithms, which exploits the topologies of poses and landmarks generated either by camera, LiDAR, or mechanical sensors, to couple and fuse them. This novel fusion scheme enhances the decision-making ability and adaptability of autonomous systems, akin to human cognition, by elaborating graph Laplacian processing concept with Kalman filters. Initially designed for cooperative localization of active road users, this approach optimally combines multisensor information through graph signal processing and Bayesian estimation for self-positioning. Conducted experiments were focused on evaluating how our approach can improve the positioning of autonomous ground vehicles, as prominent examples of RASs equipped with sensing capabilities, in challenging outdoor environments. More specifically, experiments were carried out using the CARLA simulator to generate different types of driving trajectories and environmental conditions, as well as real automotive data captured by an operating vehicle in Langen, Germany. Evaluation study demonstrates that localization accuracy is greatly improved both in terms of overall trajectory error as well as loop closing accuracy for each sensor fusion configuration.},
  archive      = {J_TCDS},
  author       = {Nikos Piperigkos and Christos Anagnostopoulos and Aris S. Lalos and Petros Kapsalas and Duong Van Nguyen},
  doi          = {10.1109/TCDS.2024.3468712},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {436--453},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Graph-laplacian-processing-based multimodal localization backend for robots and autonomous systems},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Touch gesture recognition-based physical Human–Robot interaction for collaborative tasks. <em>TCDS</em>, <em>17</em>(2), 421--435. (<a href='https://doi.org/10.1109/TCDS.2024.3466553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human–robot collaboration (HRC) has recently attracted increasing attention as a vital component of next-generation automated manufacturing and assembly tasks, yet physical human–robot interaction (pHRI)—which is an inevitable component of collaboration—is often limited to rudimentary touches. This article therefore proposes a deep-learning-based pHRI method that utilizes predefined types of human touch gestures as intuitive communicative signs for collaborative tasks. To this end, a touch gesture network model is first designed upon the framework of the gated recurrent unit (GRU) network, which accepts a set of ground-truth dynamic responses (energy change, generalized momentum, and external joint torque) of robot manipulators under the action of known types of touch gestures and learns to predict the five representative touch gesture types and the corresponding link toward a random touch gesture input. After training the GRU-based touch gesture model using a collected dataset of dynamic responses of a robot manipulator, a total of 35 outputs (five gesture types with seven links each) is recognized with 96.94% accuracy. The experimental results of recognition accuracy correlated with the touch gesture types, and their strength results are shown to validate the performance and disclose the characteristics of the proposed touch gesture model. An example of an IKEA chair assembly task is also presented to demonstrate a collaborative task using the proposed touch gestures. By developing the proposed pHRI method and demonstrating its applicability, we expect that this method can help position physical interaction as one of the key modalities for communication in real-world HRC applications.},
  archive      = {J_TCDS},
  author       = {Dawoon Jung and Chengyan Gu and Junmin Park and Joono Cheong},
  doi          = {10.1109/TCDS.2024.3466553},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {421--435},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Touch gesture recognition-based physical Human–Robot interaction for collaborative tasks},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fatigue state recognition system for miners based on a multimodal feature extraction and fusion framework. <em>TCDS</em>, <em>17</em>(2), 410--420. (<a href='https://doi.org/10.1109/TCDS.2024.3461713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fatigue factor is widely recognized as a primary contributor to accidents in the mining industry. Proactively recognizing fatigue states in miners before starting work can effectively establish a safety boundary for both miners safety and coal mine production. Therefore, this study designs a fatigue state recognition system for miners based on a multimodal extraction and fusion framework. First, the system is equipped with various sensors, a core processor and a display to collect and process physiological data such as electrocardiogram (ECG), electrodermal activity (EDA), blood pressure (BP), blood oxygen saturation (SpO${}_{2}$), skin temperature (SKT), as well as facial data, and to present fatigue state, respectively. Second, based on the multimodal feature extraction and fusion framework, after the necessary preprocessing steps, the system extracts physiological features by time and frequency domain analysis, extracts facial features by ResNeXt-50 and gated recurrent unit (GRU), and fuses multifeatures by Transformer+. Finally, in the comprehensive laboratory for coal-related programs of Xi’an University of Science and Technology, we test the system and build a multimodal dataset, and the results demonstrate an average accuracy of 93.15%.},
  archive      = {J_TCDS},
  author       = {Hongguang Pan and Shiyu Tong and Xuqiang Wei and Bingyang Teng},
  doi          = {10.1109/TCDS.2024.3461713},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {410--420},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Fatigue state recognition system for miners based on a multimodal feature extraction and fusion framework},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The methodology of quantitative social intention evaluation and robot gaze behavior control in multiobjects scenario. <em>TCDS</em>, <em>17</em>(2), 400--409. (<a href='https://doi.org/10.1109/TCDS.2024.3461335'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article focuses on the multiple objects selection problem for the robot in social scenarios, and proposes a novel methodology composed of quantitative social intention evaluation and gaze behavior control. For the social scenarios containing various persons and multimodal social cues, a combination of the entropy weight method (EWM) and gray correlation-order preference by similarity to the ideal solution (GC-TOPSIS) model is proposed to fuse the multimodal social cues, and evaluate the social intention of candidates. According to the quantitative evaluation of social intention, a robot can generate the interaction priority among multiple social candidates. To ensure this interaction selection mechanism in behavior level, an optimal control framework composed of model predictive controller (MPC) and online Gaussian process (GP) observer is employed to drive the eye-head coordinated gaze behavior of robot. Through the experiments conducted on the Xiaopang robot, the availability of the proposed methodology can be illustrated. This work enables robots to generate social behavior based on quantitative intention perception, which could bring the potential to explore the sensory principles and biomechanical mechanism underlying the human-robot interaction, and broaden the application of robot in the social scenario.},
  archive      = {J_TCDS},
  author       = {Haoyu Zhu and Xiaorui Liu and Hang Su and Wei Wang and Jinpeng Yu},
  doi          = {10.1109/TCDS.2024.3461335},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {400--409},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The methodology of quantitative social intention evaluation and robot gaze behavior control in multiobjects scenario},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-centric functional-connectivity-based cofluctuation-guided subcortical connectivity network construction. <em>TCDS</em>, <em>17</em>(2), 390--399. (<a href='https://doi.org/10.1109/TCDS.2024.3462709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subcortical regions can be functionally organized into connectivity networks and are extensively communicated with the cortex via reciprocal connections. However, most current research on subcortical networks ignores these interconnections, and networks of the whole brain are of high dimensionality and computational complexity. In this article, we propose a novel cofluctuation-guided subcortical connectivity network construction model based on edge-centric functional connectivity (FC). It is capable of extracting the cofluctuations between the cortex and subcortex and constructing dynamic subcortical networks based on these interconnections. Blind source separation approaches with domain knowledge are designed for dimensionality reduction and feature extraction. Great reproducibility and reliability were achieved when applying our model to two sessions of functional magnetic resonance imaging (fMRI) data. Cortical areas having synchronous communications with the cortex were detected, which was unable to be revealed by traditional node-centric FC. Significant alterations in connectivity patterns were observed when dealing with fMRI of subjects with and without Parkinson's disease, which were further correlated to clinical scores. These validations demonstrated that our model provided a promising strategy for brain network construction, exhibiting great potential in clinical practice.},
  archive      = {J_TCDS},
  author       = {Qinrui Ling and Aiping Liu and Taomian Mi and Piu Chan and Xun Chen},
  doi          = {10.1109/TCDS.2024.3462709},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {390--399},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Edge-centric functional-connectivity-based cofluctuation-guided subcortical connectivity network construction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neighborhood-curiosity-based exploration in multiagent reinforcement learning. <em>TCDS</em>, <em>17</em>(2), 379--389. (<a href='https://doi.org/10.1109/TCDS.2024.3460368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient exploration in cooperative multiagent reinforcement learning is still tricky in complex tasks. In this article, we propose a novel multiagent collaborative exploration method called neighborhood-curiosity-based exploration (NCE), by which agents can explore not only novel states but also new partnerships. Concretely, we use the attention mechanism in graph convolutional networks to perform a weighted summation of features from neighbors. The calculated attention weights can be regarded as an embodiment of the relationship among agents. Then, we use the prediction errors of the aggregated features as intrinsic rewards to facilitate exploration. When agents encounter novel states or new partnerships, NCE will produce large prediction errors, resulting in large intrinsic rewards. In addition, agents are more influenced by their neighbors and only interact directly with them in multiagent systems. Exploring partnerships between agents and their neighbors can enable agents to capture the most important cooperative relations with other agents. Therefore, NCE can effectively promote collaborative exploration even in environments with a large number of agents. Our experimental results show that NCE achieves significant performance improvements on the challenging StarCraft II micromanagement (SMAC) benchmark.},
  archive      = {J_TCDS},
  author       = {Shike Yang and Ziming He and Jingchen Li and Haobin Shi and Qingbing Ji and Kao-Shing Hwang and Xianshan Li},
  doi          = {10.1109/TCDS.2024.3460368},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {379--389},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Neighborhood-curiosity-based exploration in multiagent reinforcement learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCANet: Cross-modality comprehensive feature aggregation network for indoor scene semantic segmentation. <em>TCDS</em>, <em>17</em>(2), 366--378. (<a href='https://doi.org/10.1109/TCDS.2024.3455356'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic segmentation of indoor scenes based on RGB and depth information has been a persistent and enduring research topic. However, how to fully utilize the complementarity of multimodal features and achieve efficient fusion remains a challenging research topic. To address this challenge, we proposed an innovative cross-modal comprehensive feature aggregation network (CCANet) to achieve high-precision semantic segmentation of indoor scenes. In this method, we first propose a bidirectional cross-modality feature rectification (BCFR) module to complement each other and remove noise in both channel and spatial correlations. After that, the adaptive criss-cross attention fusion (CAF) module is designed to realize multistage deep multimodal feature fusion. Finally, a multisupervision strategy is applied to accurately learn additional details of the target, guiding the gradual refinement of segmentation maps. By conducting thorough experiments on two openly accessible datasets of indoor scenes, the results demonstrate that CCANet exhibits outstanding performance and robustness in aggregating RGB and depth features.},
  archive      = {J_TCDS},
  author       = {Zhang Zihao and Yang Yale and Hou Huifang and Meng Fanman and Zhang Fan and Xie Kangzhan and Zhuang Chunsheng},
  doi          = {10.1109/TCDS.2024.3455356},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {366--378},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {CCANet: Cross-modality comprehensive feature aggregation network for indoor scene semantic segmentation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A behavioral decision-making model of learning and memory for mobile robot triggered by curiosity. <em>TCDS</em>, <em>17</em>(2), 352--365. (<a href='https://doi.org/10.1109/TCDS.2024.3454779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning and memorizing behavioral decision in the process of environmental cognition to guide future decision is an important aspect of research and application in mobile robotics. Traditional rule-based behavioral decision approaches have difficulty in adapting to complex and changing environments. The offline decision-making approaches lead to poor adaptability to dynamic environments, while behavioral decision-making based on reinforcement learning relies on data acquisition, and the learned knowledge cannot guide mobile robots to quickly adapt to new environments. To address this issue, this article proposes a brain-inspired behavioral decision model that can perform incremental learning by simulating the logical structure of memory classification in the brain, as well as the memory conversion mechanisms of hippocampus, prefrontal cortex, and anterior cingulate cortex. The model interacts with the environment through semisupervised learning and learns the current decision online, simulating the memory function of humans to enable mobile robots to adapt to changing environments. In addition, an internal reward mechanism driven by curiosity is designed, simulating the reinforcement mechanism of curiosity in human memory, encoding the memory of unfamiliar behavioral decisions for mobile robots, and consolidating the memory of frequently made behavioral decisions, improving the learning and memory capacity of mobile robots in environmental cognition. The feasibility of the proposed model is verified by physical experiments in different environments.},
  archive      = {J_TCDS},
  author       = {Dongshu Wang and Qi Liu and Xulin Gao and Lei Liu},
  doi          = {10.1109/TCDS.2024.3454779},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {352--365},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A behavioral decision-making model of learning and memory for mobile robot triggered by curiosity},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying longitudinal intermediate phenotypes between genotypes and clinical score via exclusive relationship-induced association analysis in alzheimer's disease. <em>TCDS</em>, <em>17</em>(2), 340--351. (<a href='https://doi.org/10.1109/TCDS.2024.3451232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a widely focused topic, brain imaging genetics has achieved great successes in the diagnosis of complex brain disorders. In clinical application, the imaging phenotypes affected via genetic factors will change over time. A clinical score-relevant exclusive relationship-induced multimodality learning (CS-ERMM) framework is proposed for integrating longitudinal neuroimage, genetics, and clinical score data. Specifically, first, the exclusive lasso term is used to construct the exclusive multimodality learning method, which can convey the unique information at a specific time point. The relationship-induced term is then introduced to automatically learn the relatedness among the multiple time-points from data, which explores the association between genotypes and longitudinal imaging phenotypes to facilitate the understanding of the degenerative process. Finally, the clinical score outcomes are integrated into such association model, which discovers longitudinal phenotypic markers associated with the Alzheimer's disease risk single nucleotide polymorphism that are relevant to clinical score outcomes. We also design a proximal alternating optimization strategy to solve the constructed CS-ERMM model. Extensive experimental results on brain imaging genetic data from the Alzheimer's disease neuroimaging initiative dataset have validated that our method outperforms several competing approaches, which achieve strong associations and identify important consistent markers across longitudinal phenotypes related to genetic risk biomarkers for disease interpretation.},
  archive      = {J_TCDS},
  author       = {Meiling Wang and Wei Shao and Shuo Huang and Daoqiang Zhang},
  doi          = {10.1109/TCDS.2024.3451232},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {340--351},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Identifying longitudinal intermediate phenotypes between genotypes and clinical score via exclusive relationship-induced association analysis in alzheimer's disease},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive-learning-based assist-as-needed control for ankle rehabilitation. <em>TCDS</em>, <em>17</em>(2), 328--339. (<a href='https://doi.org/10.1109/TCDS.2024.3455795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a progressive-learning-based assist-as-needed (AAN) control scheme for ankle rehabilitation. To quantify the training performance, a fuzzy logic (FL) system is established to generate a holistic metric based on multiple kinematic and dynamic indicators. Subsequently, a cost function that contains both the tracking error and robot stiffness is constructed. A novel learning scheme is then proposed to enhance subjects’ engagement, leveraging the FL metric to uphold a declining trend in the robot's stiffness. The system stability is analyzed using the Lyapunov theory, the control ultimate bounds are specified and the effects of parameter tuning are discussed. Experiments are conducted on an ankle robot and the minimal assist-as-needed (MAAN) scheme is adopted for comparison. With a training session consisting of 11 trials, the quantitative performance evaluations, individual error convergences, progressive stiffness learning and human–robot interaction are evaluated. It is shown that within eight trials under the progressive AAN and MAAN, the robot assistive torques have an average reduction of 13.45% and 20.25% while subjects’ active torques are increased by 56.53% and 58.39%, respectively. During the late stage of training, the progressive AAN further improves two criteria by 9.44% and 6.29%, while the MAAN partially loses subjects’ participation (active torques are reduced by 36.38%) due to the occurrence of motion adaption.},
  archive      = {J_TCDS},
  author       = {Kun Qian and Zhenhong Li and Yihui Zhao and Jie Zhang and Xianwen Kong and Samit Chakrabarty and Zhiqiang Zhang and Sheng Quan Xie},
  doi          = {10.1109/TCDS.2024.3455795},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {328--339},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Progressive-learning-based assist-as-needed control for ankle rehabilitation},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pretrained dynamics learning of numerous heterogeneous robots and Gen2Real transfer. <em>TCDS</em>, <em>17</em>(2), 315--327. (<a href='https://doi.org/10.1109/TCDS.2024.3454240'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring dynamics is vital for robotic learning and serves as the foundation for planning and control. This article addresses two essential inquiries: How can one develop a model that encompasses a vast array of diverse robotic dynamics? Is it possible to establish a model that alleviates the burdens of data collection and domain expertise necessary for constructing specific robot models? We explore the dynamics present in a dataset containing numerous serial articulated robots and introduce a novel concept, “Gen2Real,” to transfer simulated, generalized models to physical, and specialized robots. By randomizing dynamics parameters, topological configurations, and model dimensions, we generate an extensive dataset that corresponds to varying properties, connections, and quantities of robotic links. A structure adapted from the generative pretrained transformer is employed to approximate the dynamics of a multitude of heterogeneous robots. Within Gen2Real, we transfer the pretrained model to a target robot using distillation to enable real-time computation. The results corroborate the superiority of the proposed method in terms of accurately learning an immense scope of robotic dynamics, managing commonly encountered disturbances, and exhibiting versatility in transferring to distinct robots.},
  archive      = {J_TCDS},
  author       = {Dengpeng Xing and Yiming Yang and Jiale Li},
  doi          = {10.1109/TCDS.2024.3454240},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {315--327},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Pretrained dynamics learning of numerous heterogeneous robots and Gen2Real transfer},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain compensatory mechanisms during the prolonged cognitive task: FNIRS and eye-tracking study. <em>TCDS</em>, <em>17</em>(2), 303--314. (<a href='https://doi.org/10.1109/TCDS.2024.3453590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of maintaining cognitive performance under fatigue is crucial in fields requiring high concentration and efficiency to successfully complete critical tasks. In this context, the study of compensatory mechanisms that help the brain overcome fatigue is particularly important. This research investigates the correlations between physiological, behavioral, and subjective measures while considering the impact of fatigue on the performance of working memory tasks. A combined approach of functional near-infrared spectroscopy (fNIRS) and eye-tracking was used to reconstruct brain functional networks based on fNIRS data and analyze them in terms of network characteristics such as global clustering coefficient and global efficiency. Results showed a significant increase in subjective fatigue but no significant change in performance during the experiment. The study confirmed that despite fatigue, subjects can maintain performance through compensatory mechanisms, increasing mental effort, with the level of compensation depending on the task's complexity. Furthermore, the study showed that compensatory effort maintains the efficiency of the frontoparietal network, and the degree of compensatory effort is related to the difference in response times between high- and low-complexity tasks.},
  archive      = {J_TCDS},
  author       = {A. A. Badarin and V. M. Antipov and V. V. Grubov and A. V. Andreev and E. N. Pitsik and S. A. Kurkin and A. E. Hramov},
  doi          = {10.1109/TCDS.2024.3453590},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {303--314},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Brain compensatory mechanisms during the prolonged cognitive task: FNIRS and eye-tracking study},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Location reasoning of target objects based on human common sense and robot experiences. <em>TCDS</em>, <em>17</em>(2), 287--302. (<a href='https://doi.org/10.1109/TCDS.2024.3442862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The location reasoning of target objects in robot-operated environment is a challenging task. Objects that robots need to interact with are often located at a distance or are contained within containers, making them inaccessible for direct observation by the robot. The uncertainty of the storage location of the target objects and the lack of reasoning ability present considerable challenges. In this article, we propose a method for semantic localization of robot-operated objects based on human common sense and robot experiences. Instead of reasoning the object storage locations solely based on the category of the target object, a probabilistic ontology model is introduced to represent uncertain knowledge in the task of object localization, which combines the expressive power of classical first-order logic and the inference capability of Bayesian inference. The target location is then estimated using the probabilistic ontologies with dynamic integration of human common sense and robot experiences. Experimental results in both simulation and real-world environments demonstrate the effectiveness of the proposed integration of human common sense and robot experiences in the task of semantic localization of robot-operated objects.},
  archive      = {J_TCDS},
  author       = {Yueguang Ge and Yinghao Cai and Shuo Wang and Shaolin Zhang and Tao Lu and Haitao Wang and Junhang Wei},
  doi          = {10.1109/TCDS.2024.3442862},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {287--302},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Location reasoning of target objects based on human common sense and robot experiences},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal emotion fusion mechanism and empathetic responses in companion robots. <em>TCDS</em>, <em>17</em>(2), 271--286. (<a href='https://doi.org/10.1109/TCDS.2024.3442203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability of humanoid robots to exhibit empathetic facial expressions and provide corresponding responses is essential for natural human–robot interaction. To enhance this, we integrate the GPT3.5 model with a facial expression recognition model, creating a multimodal emotion recognition system. Additionally, we address the challenge of realistically mimicking human facial expressions by designing the physical structure of a humanoid robot. Initially, we develop a humanoid robot capable of adjusting the positions of its facial organs and neck through servo displacement to achieve more natural facial expressions. Subsequently, to overcome the current limitation where emotional interaction robots struggle to accurately recognize user emotions, we introduce a coupled generative pretrained transformer (GPT)-based multimodal emotion recognition method that utilizes both text and images, thereby enhancing the robot's emotion recognition accuracy. Finally, we integrate the GPT-3.5 model to generate empathetic responses based on recognized user emotional states and language text, which are then mapped onto the robot to enable empathetic expressions that can achieve a more comfortable human–machine interaction experience. Experimental results on benchmark databases demonstrate that the performance of the coupled GPT-based multimodal emotion recognition method using text and images outperforms other approaches, and it possesses unique empathetic response capabilities relative to alternative methods.},
  archive      = {J_TCDS},
  author       = {Xiaofeng Liu and Qincheng Lv and Jie Li and Siyang Song and Angelo Cangelosi},
  doi          = {10.1109/TCDS.2024.3442203},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {271--286},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Multimodal emotion fusion mechanism and empathetic responses in companion robots},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV coverage path planning of multiple disconnected regions based on cooperative optimization algorithms. <em>TCDS</em>, <em>17</em>(2), 259--270. (<a href='https://doi.org/10.1109/TCDS.2024.3442957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the coverage path planning problem when an unmanned aerial vehicle (UAV) surveys an unknown site composed of multiple isolated areas. The problem is typically non-deterministic polynomial-time hard(NP-hard) and cannot be easily solved, especially when considering the scale of each area. By decomposing the problem into two cascaded subproblems—1) covering a specific polygon area; and 2) determining the optimal visiting order of different areas—an approximate solution can be found more efficiently. First, the target areas are approximated as convex polygons, and the coverage pattern is designed based on four control points. Then, the optimal visiting order is determined based on a state defined by area indices and control points. We propose two different optimization methods to solve this problem. The first method is a direct extension of the genetic algorithm, using a customized coding method. The second method is a reinforcement learning-based (RL-based) approach that solves the problem as a variant of the traveling salesman problem (TSP) through end-to-end policy training. The simulation results indicate that the proposed methods can provide solutions to the multiple-area coverage problem with competitive optimality and efficiency.},
  archive      = {J_TCDS},
  author       = {Yang Lyu and Shuyue Wang and Tianmi Hu and Quan Pan},
  doi          = {10.1109/TCDS.2024.3442957},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {259--270},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {UAV coverage path planning of multiple disconnected regions based on cooperative optimization algorithms},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Biomimetic spiking neural network based on monolayer 2-D synapse with short-term plasticity for auditory brainstem processing. <em>TCDS</em>, <em>17</em>(2), 247--258. (<a href='https://doi.org/10.1109/TCDS.2024.3450915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the sound localization of species, short-term depression (STD) plays an important role in maintaining interaural timing difference (ITD) sensitivity. In this article, a biomimetic spiking neural network (SNN) utilizing 2-D synaptic devices for mimicking biological sound localization is presented. A two-terminal monolayer device is used as the artificial synapse, whose temporal conductance change mimics the STD of a synapse. Alpha synaptic current and leaky integrate-and-fire (LIF) neuron models are used for realistic cortical operation. Lateral inhibition and superior olivary nucleus (SON) are adopted to increase the acuteness, to compensate for the interaural level difference (ILD)-induced disturbance, and to enlarge the sound intensity range. By combining solid-state STD synapses and bio-plausible cortical models with an ITD-based coincidence detection mechanism to mimic the auditory brainstem processing, our SNN achieved sound localization with a human-level resolution of 1°.},
  archive      = {J_TCDS},
  author       = {Jieun Kim and Peng Zhou and Unbok Wi and Bomin Joo and Donguk Choi and Myeong-Lok Seol and Sravya Pulavarthi and Linfeng Sun and Heejun Yang and Woo Jong Yu and Jin-Woo Han and Sung-Mo Kang and Bai-Sun Kong},
  doi          = {10.1109/TCDS.2024.3450915},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {247--258},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Biomimetic spiking neural network based on monolayer 2-D synapse with short-term plasticity for auditory brainstem processing},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Programmable bionic control circuit based on central pattern generator. <em>TCDS</em>, <em>17</em>(2), 233--246. (<a href='https://doi.org/10.1109/TCDS.2024.3388152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The central pattern generator (CPG) involves a group of neurons that produce rhythmic signals in a coordinated manner. Currently, CPG circuits capable of efficient online programming are rarely found in the literature. To address this issue, this article proposes a memristive control circuit based on CPG. First, an online amplification module is designed to adjust the positive and negative amplification coefficients. On the basis of this structure, a CPG unit circuit controlling a joint is proposed. According to the topology of CPG network model, a CPG network circuit composed of multiple units is devised. This network can coordinate multiple joints to produce a gait. In this article, the circuit is applied to generate the activity pattern of fish swimming. PSPICE simulation results demonstrate that four units can realize the basic swimming patterns of a robot fish. Through memristor programming, the circuit can achieve smooth online switching of robot fish swimming patterns. Moreover, hardware implementation proves the practicality of the circuit.},
  archive      = {J_TCDS},
  author       = {Qinghui Hong and Qing Li and Jia Li and Jingru Sun and Sichun Du},
  doi          = {10.1109/TCDS.2024.3388152},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {4},
  number       = {2},
  pages        = {233--246},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Programmable bionic control circuit based on central pattern generator},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinforcement-learning-based multi-unmanned aerial vehicle optimal control for communication services with limited endurance. <em>TCDS</em>, <em>17</em>(1), 219--231. (<a href='https://doi.org/10.1109/TCDS.2024.3441865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the service path problem of multi-unmanned aerial vehicle (multi-UAV) providing communication services to multiuser in urban environments with limited endurance. Our goal is to learn an optimal multi-UAV centralized control policy that will enable UAVs to find the illumination areas in urban environments through curiosity-driven exploration and harvest energy to continue providing communication services to users. First, we propose a reinforcement learning (RL)-based multi-UAV centralized control strategy to maximize the accumulated communication service score. In the proposed framework, curiosity can act as an internal incentive signal, allowing UAVs to explore the environment without any prior knowledge. Second, a two-phase exploring protocol is proposed for practical implementation. Compared to the baseline method, our proposed method can achieve a significantly higher accumulated communication service score in the exploitation-intensive phase. The results demonstrate that the proposed method can obtain accurate service paths over the baseline method and handle the exploration-exploitation tradeoff well.},
  archive      = {J_TCDS},
  author       = {Lu Dong and Pinle Ding and Xin Yuan and Andi Xu and Jie Gui},
  doi          = {10.1109/TCDS.2024.3441865},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {219--231},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Reinforcement-learning-based multi-unmanned aerial vehicle optimal control for communication services with limited endurance},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An impedance recognition framework based on electromyogram for physical Human–Robot interaction. <em>TCDS</em>, <em>17</em>(1), 205--218. (<a href='https://doi.org/10.1109/TCDS.2024.3442172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In physical human–robot interaction (pHRI), the interaction profiles, such as impedance and interaction force are greatly influenced by the operator's muscle activities, impedance and interaction force between the robot and the operator. Actually, parameters of interaction profiles are easy to be measured, such as position, velocity, acceleration, and muscle activities. However, the impedance cannot be directly measured. In some areas, it is difficult to capture the force information, especially where the force sensor is hard to be attached on the robots. In this sense, it is worth developing a feasible and simple solution to recognize the impedance parameters by exploring the potential relationship among the above mentioned interaction profiles. To this end, a framework of impedance recognition based on different time-based weight membership functions with broad learning system (TWMF-BLS) is developed for stable/unstable pHRI. Specifically, a linear weight membership function and a nonlinear weight membership function are proposed for stable and unstable pHRI by using the hybrid features for estimating the interaction force. And then the human arm impedance can be estimated without a biological model or a robot's model. Experimental results have demonstrated the feasibility and effectiveness of the proposed approach.},
  archive      = {J_TCDS},
  author       = {Jing Luo and Chaoyi Zhang and Chao Zeng and Yiming Jiang and Chenguang Yang},
  doi          = {10.1109/TCDS.2024.3442172},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {205--218},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {An impedance recognition framework based on electromyogram for physical Human–Robot interaction},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A derivative topic propagation model based on multidimensional cognition and game theory. <em>TCDS</em>, <em>17</em>(1), 189--204. (<a href='https://doi.org/10.1109/TCDS.2024.3432337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given that emotional content spreads more widely than rational content in social networks, as well as the complexity of user cognition and the interaction of derivative topics, this article proposes a derivative topic dissemination model that integrates multidimensional cognition and game theory. First, regarding the issue of user emotional reactions in mining topics. In this article, we quantify the affective influence among users by considering user behaviors as continuous conversations through conversation-level sentiment analysis and the proximity centrality of social networks. Second, considering that user behavior is influenced by multidimensional cognition, this article proposes a method based on S(Sensibility) R(Rationality) 2vec to simulate the dialectical relationship between sensibility and rationality in the user decision-making process. Finally, considering the cooperative and competitive relationship among derived topics, this article uses evolutionary game theory to analyze the topic life cycle and quantify its impact on user behavior by time discretization method. Accordingly, we propose a CG-back-propagation (BP) model incorporating a BP neural network to efficiently simulate the nonlinear relationship of user behavior. Experiments show that the model can not only effectively tap the influence of multidimensional cognition on users’ retweeting behavior, but also effectively perceive the propagation dynamics of derived topics.},
  archive      = {J_TCDS},
  author       = {Qian Li and Long Gao and Wenyi Xi and Tun Li and Rong Wang and Junwei Ge and Yunpeng Xiao},
  doi          = {10.1109/TCDS.2024.3432337},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {189--204},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {A derivative topic propagation model based on multidimensional cognition and game theory},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Channel-selection-based temporal convolutional network for patient-specific epileptic seizure detection. <em>TCDS</em>, <em>17</em>(1), 179--188. (<a href='https://doi.org/10.1109/TCDS.2024.3433551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since sudden and recurrent epileptic seizures seriously affect people's lives, computer-aided automatic seizure detection is crucial for precise diagnosis and prompt treatment. A novel seizure detection algorithm named channel selection-based temporal convolutional network (CS-TCN) was proposed in this article. First, electroencephalogram (EEG) recordings were segmented into 2-s intervals and features were extracted from both the time and frequency domains. Then, the expanded fisher score channel selection method was employed to select channels that contribute the most to seizure detection. Finally, the features from selected EEG channels were fed into the TCN to capture inherent temporal dependencies of EEG signals and detect seizure events. Children Hospital Boston and Massachusetts Institute of Technology (CHB-MIT) and Siena datasets were used to verify the detection performance of the CS-TCN algorithm, achieving sensitivities of 98.56% and 98.88%, and specificities of 99.80% and 99.88% in samplewise analysis, respectively. In eventwise analysis, the algorithm achieved sensitivities of 97.57% and 95.00%, with delays of 6.91 and 18.62 s, and FDR/h of 0.11 and 0.39, respectively. These results surpassed state-of-the-art few-channel algorithms for both datasets. CS-TCN algorithm offers excellent performance while simplifying model complexity and computational requirements, thus showcasing its potential for facilitating seizure detection in home environments.},
  archive      = {J_TCDS},
  author       = {Guangming Wang and Xiyuan Lei and Wen Li and Won Hee Lee and Lianchi Huang and Jialin Zhu and Shanshan Jia and Dong Wang and Yang Zheng and Hua Zhang and Badong Chen and Gang Wang},
  doi          = {10.1109/TCDS.2024.3433551},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {179--188},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Channel-selection-based temporal convolutional network for patient-specific epileptic seizure detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GLADA: Global and local associative domain adaptation for EEG-based emotion recognition. <em>TCDS</em>, <em>17</em>(1), 167--178. (<a href='https://doi.org/10.1109/TCDS.2024.3432752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition based on electroencephalography (EEG) has significant advantages in terms of reliability and accuracy. However, individual differences in EEG limit the ability of sentiment classifiers to generalize across subjects. Furthermore, due to the nonstationarity of EEG, subject signals can vary with time, an important challenge for temporal emotion recognition. Several emotion recognition methods have been developed that consider the alignment of conditional distributions, but do not balance the weights of conditional and marginal distributions. In this article, we propose a novel approach to generalize emotion recognition models across individuals and time, i.e., global and local associative domain adaptation (GLADA). The proposed method consists of three parts: 1) deep neural networks are used to extract deep features from emotional EEG data; 2) considering that marginal and conditional distributions between domains can contribute to adaptation differently, a method that combines coarse-grained adversarial adaptation and fine-grained adversarial adaptation is used to narrow the domain distance of the joint distribution in the EEG data between subjects (i.e., reduce intersubject variability), and the weights of the marginal and conditional distributions are automatically balanced using dynamic balancing factors; and 3) domain adaptation is used to accelerate model convergence. Using GLADA, subject-independent EEG emotion recognition is improved by reducing the influence of the subject’s personal information on EEG emotion. Experimental results demonstrate that the GLADA model effectively addresses the domain transfer problem, resulting in improved performance across multiple EEG emotion recognition tasks.},
  archive      = {J_TCDS},
  author       = {Tianxu Pan and Nuo Su and Jun Shan and Yang Tang and Guoqiang Zhong and Tianzi Jiang and Nianming Zuo},
  doi          = {10.1109/TCDS.2024.3432752},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {167--178},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {GLADA: Global and local associative domain adaptation for EEG-based emotion recognition},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implementing brain-like fear generalization and emotional arousal associated with memory. <em>TCDS</em>, <em>17</em>(1), 155--166. (<a href='https://doi.org/10.1109/TCDS.2024.3425845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion plays an important role in human life. In recent years, memristor-based emotion circuits have been proposed extensively, but few circuits simulate the neural circuity that generates specific emotions in the limbic system. In this article, a memristor-based circuit of brain-like fear generalization is proposed. It is described from two dimensions of perception and higher cognition, respectively, both of which are realized by simulating the limbic system of human brain. The main difference between these two dimensions lies in the circuit design of the hippocampus module. Moreover, the memory enhancement effect caused by fear is one of the reasons for the phenomenon of fear generalization. That is, high arousal of fear leads to enhanced memory. Herein, the memristor-based circuit associated with different emotional arousal and memory is designed. The simulation results in SPICE show that the circuit is able to implement the brain-like fear generalization and the emotional memory under different arousal. The circuit design of these neural networks may provide some references for the field of brain-like robots.},
  archive      = {J_TCDS},
  author       = {Mei Guo and Douyin Zhang and Wenhai Guo and Gang Dou and Junwei Sun},
  doi          = {10.1109/TCDS.2024.3425845},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {155--166},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Implementing brain-like fear generalization and emotional arousal associated with memory},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic prediction of disturbance caused by interfloor sound events. <em>TCDS</em>, <em>17</em>(1), 147--154. (<a href='https://doi.org/10.1109/TCDS.2024.3424457'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a direct correlation between noise and human health, while negative consequences may vary from sleep disruption and stress to hearing loss and reduced productivity. Despite its undeniable relevance, the underlying process governing the relationship between unpleasant sound events, and the annoyance they may cause has not been systematically studied yet. In this context, this work focuses on the disturbance caused by interfloor sound events, i.e., the audio signals transmitted through the floors of a building. Activities such as walking, running, using household appliances or other daily actions generate sounds that can be heard by those on an adjacent floor. To this end, we implemented a suitable dataset including diverse interfloor sound events annotated according to the perceived disturbance. Subsequently, we propose a framework able to quantify similarities exhibited by interfloor sound events starting from standardized time-frequency representations, which are processed by a Siamese neural network composed of a series of convolutional layers. Such similarities are then employed by a $k$-medoids regression scheme making disturbance predictions based on interfloor sound events with neighboring latent representations. After thorough experiments, we demonstrate the effectiveness of such a framework and its superiority over popular regression algorithms. Last but not least, the proposed solution offers interpretable predictions, which may be meaningfully utilized by human experts.},
  archive      = {J_TCDS},
  author       = {Stavros Ntalampiras and Alessandro Scalambrino},
  doi          = {10.1109/TCDS.2024.3424457},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {147--154},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Automatic prediction of disturbance caused by interfloor sound events},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpikingViT: A multiscale spiking vision transformer model for event-based object detection. <em>TCDS</em>, <em>17</em>(1), 130--146. (<a href='https://doi.org/10.1109/TCDS.2024.3422873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras have unique advantages in object detection, capturing asynchronous events without continuous frames. They excel in dynamic range, low latency, and high-speed motion scenarios, with lower power consumption. However, aggregating event data into image frames leads to information loss and reduced detection performance. Applying traditional neural networks to event camera outputs is challenging due to event data's distinct characteristics. In this study, we present a novel spiking neural networks (SNNs)-based object detection model, the spiking vision transformer (SpikingViT) to address these issues. First, we design a dedicated event data converting module that effectively captures the unique characteristics of event data, mitigating the risk of information loss while preserving its spatiotemporal features. Second, we introduce SpikingViT, a novel object detection model that leverages SNNs capable of extracting spatiotemporal information among events data. SpikingViT combines the advantages of SNNs and transformer models, incorporating mechanisms such as attention and residual voltage memory to further enhance detection performance. Extensive experiments have substantiated the remarkable proficiency of SpikingViT in event-based object detection, positioning it as a formidable contender. Our proposed approach adeptly retains spatiotemporal information inherent in event data, leading to a substantial enhancement in detection performance.},
  archive      = {J_TCDS},
  author       = {Lixing Yu and Hanqi Chen and Ziming Wang and Shaojie Zhan and Jiankun Shao and Qingjie Liu and Shu Xu},
  doi          = {10.1109/TCDS.2024.3422873},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {130--146},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {SpikingViT: A multiscale spiking vision transformer model for event-based object detection},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prepulse inhibition and prestimulus nonlinear brain dynamics in childhood: A lyapunov exponent approach. <em>TCDS</em>, <em>17</em>(1), 115--129. (<a href='https://doi.org/10.1109/TCDS.2024.3418841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The acoustic startle reflex (ASR) relies on the sensorimotor system and is affected by aging, sex, and psychopathology. ASR can be modulated by the prepulse inhibition (PPI) paradigm, which achieves the inhibition of reactivity to a startling stimulus (pulse) following a weak prepulse stimulus. Additionally, neurophysiological studies have found that brain activity is characterized by irregular patterns with high complexity, which however reduces with age. Our study investigated the relationship between prestartle nonlinear dynamics and PPI in healthy children versus adults. Fifty-six individuals took part in the experiment: 31 children and adolescents and 25 adults. Participants heard 51 pairs of tones (prepulse and startle) with a time difference of 30 to 500 ms. Subsequently, we assessed neural complexity by computing the largest Lyapunov exponent (LLE) during the prestartle period and assessed PPI by analyzing the poststartle event-related potentials (ERPs). Results showed higher neural complexity for children compared to adults, in line with previous research showing reduced complexity in the physiological signals in aging. As expected, PPI (as reflected in the P50 and P200 components) was enhanced in adults compared to children, potentially due to the maturation of the ASR for the former. Interestingly, prestartle complexity was correlated with the P50 component in children only, but not in adults, potentially due to the different stage of sensorimotor maturation between age groups. Overall, our study offers novel contributions for investigating brain dynamics, linking nonlinear with linear measures. Our findings are consistent with the loss of neural complexity in aging, and suggest differentiated links between nonlinear and linear metrics in children and adults.},
  archive      = {J_TCDS},
  author       = {Anastasios E. Giannopoulos and Ioanna Zioga and Vaios Ziogas and Panos Papageorgiou and Georgios N. Papageorgiou and Charalabos Papageorgiou},
  doi          = {10.1109/TCDS.2024.3418841},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {115--129},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Prepulse inhibition and prestimulus nonlinear brain dynamics in childhood: A lyapunov exponent approach},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regulating temporal neural coding via fast and slow synaptic dynamics. <em>TCDS</em>, <em>17</em>(1), 102--114. (<a href='https://doi.org/10.1109/TCDS.2024.3417477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The NMDA receptor (NMDAR), as a ubiquitous type of synapse in neural systems of the brain, presents slow dynamics to modulate neural spiking activity. For the cerebellum, NMDARs have been suggested for contributing complex spikes in Purkinje cells (PCs) as a mechanism for cognitive activity, learning, and memory. Recent experimental studies are debating the role of NMDAR in PC dendritic input, yet it remains unclear how the distribution of NMDARs in PC dendrites can affect their neural spiking coding properties. In this work, a detailed multiple-compartment PC model was used to study how slow-scale NMDARs together with fast-scale AMPA, regulate neural coding. We find that NMDARs act as a band-pass filter, increasing the excitability of PC firing under low-frequency input while reducing it under high frequency. This effect is positively related to the strength of NMDARs. For a response sequence containing a large number of regular and irregular spiking patterns, NMDARs reduce the overall regularity under high-frequency input while increasing the local regularity under low-frequency. Moreover, the inhibitory effect of NMDA receptors during high-frequency stimulation is associated with a reduced conductance of large conductance calcium-activated potassium (BK) channel. Taken together, our results suggest that NMDAR plays an important role in the regulation of neural coding strategies by utilizing its complex dendritic structure.},
  archive      = {J_TCDS},
  author       = {Yuanhong Tang and Lingling An and Xingyu Zhang and Huiling Huang and Zhaofei Yu},
  doi          = {10.1109/TCDS.2024.3417477},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {102--114},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Regulating temporal neural coding via fast and slow synaptic dynamics},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The distinction between object recognition and object identification in brain connectivity for Brain–Computer interface applications. <em>TCDS</em>, <em>17</em>(1), 89--101. (<a href='https://doi.org/10.1109/TCDS.2024.3417299'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition and object identification are complex cognitive processes where information is integrated and processed by an extensive network of brain areas. However, although object recognition and object identification are similar, they are considered separate functions in the brain. Interestingly, the difference between object recognition and object identification has still not been characterized in a way that brain–computer interface (BCI) applications can detect or use. Hence, in this study, we investigated neural features during object recognition and identification tasks through functional brain connectivity. We conducted an experiment involving 25 participants to explore these neural features. Participants completed two tasks: an object recognition task, where they determined whether a target object belonged to a specified category, and an object identification task, where they identified the target object among four displayed images. Our aim was to discover reliable features that could distinguish between object recognition and identification. The results demonstrate a significant difference between object recognition and identification in the participation coefficient (PC) and clustering coefficient (CC) of delta activity in the visual and temporal regions of the brain. Further analysis at the category level shows that this coefficient differs for different categories of objects. Utilizing these discovered features for binary classification, the accuracy for the animal category reached 80.28%. The accuracy for flower and vehicle categories also improved when combining the PC and CC, although no improvement was observed for the food category. Overall, what we have found is a feature that might be able to be used to differentiate between object recognition and identification within a BCI object recognition system. Further, it may help BCI object recognition systems to determine a user’s intentions when selecting an object.},
  archive      = {J_TCDS},
  author       = {Daniel Leong and Thomas Do and Chin-Teng Lin},
  doi          = {10.1109/TCDS.2024.3417299},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {89--101},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {The distinction between object recognition and object identification in brain connectivity for Brain–Computer interface applications},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-subject emotion recognition from multichannel EEG signals using multivariate decomposition and ensemble learning. <em>TCDS</em>, <em>17</em>(1), 77--88. (<a href='https://doi.org/10.1109/TCDS.2024.3417534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotions are mental states that determine the behavior of a person in society. Automated identification of a person's emotion is vital in different applications such as brain–computer interfaces (BCIs), recommender systems (RSs), and cognitive neuroscience. This article proposes an automated approach based on multivariate fast iterative filtering (MvFIF) and an ensemble machine learning model to recognize cross-subject emotions from electroencephalogram (EEG) signals. The multichannel EEG signals are initially decomposed into multichannel intrinsic mode functions (MIMFs) using the MvFIF. The features, such as differential entropy (DE), dispersion entropy (DispEn), permutation entropy (PE), spectral entropy (SE), and distribution entropy (DistEn), are extracted from MIMFs. The binary atom search optimization (BASO) technique is employed to reduce the dimension of the feature space. The light gradient boosting machine (LGBM), extreme learning machine (ELM), and ensemble bagged tree (EBT) classifiers are used to recognize different human emotions using the features of EEG signals. The results demonstrate that the LGBM classifier has achieved the highest average accuracy of 99.50% and 98.79%, respectively, using multichannel EEG signals from the GAMEEMO and DREAMER databases for cross-subject emotion recognition (ER). Compared to other multivariate signal decomposition algorithms, the MvFIF-based method has demonstrated higher accuracy in recognizing emotions using multichannel EEG signals. The proposed (MvFIF+DE+BASO+LGBM) technique outperforms the existing state-of-the-art methods in ER using EEG signals.},
  archive      = {J_TCDS},
  author       = {Raveendrababu Vempati and Lakhan Dev Sharma and Rajesh Kumar Tripathy},
  doi          = {10.1109/TCDS.2024.3417534},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {77--88},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Cross-subject emotion recognition from multichannel EEG signals using multivariate decomposition and ensemble learning},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling thoughts: A review of advancements in EEG brain signal decoding into text. <em>TCDS</em>, <em>17</em>(1), 61--76. (<a href='https://doi.org/10.1109/TCDS.2024.3462452'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conversion of brain activity into text using electroencephalography (EEG) has gained significant traction in recent years. Many researchers are working to develop new models to decode EEG signals into text form. Although this area has shown promising developments, it still faces numerous challenges that necessitate further improvement. It is important to outline this area's recent developments and future research directions to provide a comprehensive understanding of the current state of technology, guide future research efforts, and enhance the effectiveness and accessibility of EEG-to-text systems. In this review article, we thoroughly summarize the progress in EEG-to-text conversion. First, we talk about how EEG-to-text technology has grown and what problems the field still faces. Second, we discuss existing techniques used in this field. This includes methods for collecting EEG data, the steps to process these signals, and the development of systems capable of translating these signals into coherent text. We conclude with potential future research directions, emphasizing the need for enhanced accuracy, reduced system constraints, and the exploration of novel applications across varied sectors. By addressing these aspects, this review aims to contribute to developing more accessible and effective brain–computer interface (BCI) technology for a broader user base.},
  archive      = {J_TCDS},
  author       = {Saydul Akbar Murad and Nick Rahimi},
  doi          = {10.1109/TCDS.2024.3462452},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {61--76},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Unveiling thoughts: A review of advancements in EEG brain signal decoding into text},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mental workload assessment using deep learning models from EEG signals: A systematic review. <em>TCDS</em>, <em>17</em>(1), 40--60. (<a href='https://doi.org/10.1109/TCDS.2024.3460750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mental workload (MWL) assessment is crucial in information systems (IS), impacting task performance, user experience, and system effectiveness. Deep learning offers promising techniques for MWL classification using electroencephalography (EEG), which monitors cognitive states dynamically and unobtrusively. Our research explores deep learning's potential and challenges in EEG-based MWL classification, focusing on training inputs, cross-validation methods, and classification problem types. We identify five types of EEG-based MWL classification: within-subject, cross subject, cross session, cross task, and combined cross task and cross subject. Success depends on managing dataset uniqueness, session and task variability, and artifact removal. Despite the potential, real-world applications are limited. Enhancements are necessary for self-reporting methods, universal preprocessing standards, and MWL assessment accuracy. Specifically, inaccuracies are inflated when data are shuffled before splitting to train and test sets, disrupting EEG signals’ temporal sequence. In contrast, methods such as the time-series cross validation and leave-session-out approach better preserve temporal integrity, offering more accurate model performance evaluations. Utilizing deep learning for EEG-based MWL assessment could significantly improve IS functionality and adaptability in real time based on user cognitive states.},
  archive      = {J_TCDS},
  author       = {Kunjira Kingphai and Yashar Moshfeghi},
  doi          = {10.1109/TCDS.2024.3460750},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {40--60},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Mental workload assessment using deep learning models from EEG signals: A systematic review},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Speech imagery decoding using EEG signals and deep learning: A survey. <em>TCDS</em>, <em>17</em>(1), 22--39. (<a href='https://doi.org/10.1109/TCDS.2024.3431224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech imagery (SI)-based brain–computer interface (BCI) using electroencephalogram (EEG) signal is a promising area of research for individuals with severe speech production disorders. Recent advances in deep learning (DL) have led to significant improvements in this domain. However, there is a lack of comprehensive review that covers the application of DL methods for decoding imagined speech via EEG. In this article, we survey SI and DL literature to address critical questions regarding preferred paradigms, preprocessing necessity, optimal input formulations, and current trends in DL-based techniques. Specifically, we first search major databases across science and engineering disciplines for relevant studies. Then, we analyze the DL-based techniques applied in SI decoding from five main perspectives: dataset, preprocessing, input formulation, DL architecture, and performance evaluation. Moreover, we summarize the key findings of this work and propose a set of practical recommendations. Finally, we highlight the practical challenges of DL-based imagined speech decoding and suggest future research directions.},
  archive      = {J_TCDS},
  author       = {Liying Zhang and Yueying Zhou and Peiliang Gong and Daoqiang Zhang},
  doi          = {10.1109/TCDS.2024.3431224},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {22--39},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Speech imagery decoding using EEG signals and deep learning: A survey},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sensorimotor integration: A review of neural and computational models and the impact of parkinson’s disease. <em>TCDS</em>, <em>17</em>(1), 3--21. (<a href='https://doi.org/10.1109/TCDS.2024.3520976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sensorimotor integration (SMI) is a complex process that allows humans to perceive and interact with their environment. Any impairment in SMI may impact the day-to-day functioning of humans, particularly evident in Parkinson’s Disease (PD). SMI is critical to accurate perception and modulation of motor outputs. Therefore, understanding the associated neural pathways and mathematical underpinnings is crucial. In this article, a systematic review of the proposed neural and computational models associated with SMI is performed. While the neural models discuss the neural architecture and regions, the computational models explore the mathematical or computational mechanisms involved in SMI. The article then explores how PD may impair SMI, reviewing studies that discuss deficits in the perception of various modalities, pointing to an SMI impairment. This helps in understanding the nature of SMI deficits in PD. Overall, the review offers comprehensive insights into the basis of SMI and the effect of PD on SMI, enabling clinicians to better understand the SMI mechanisms and facilitate the development of targeted therapies to mitigate SMI deficits in PD.},
  archive      = {J_TCDS},
  author       = {Yokhesh K. Tamilselvam and Jacky Ganguly and Mandar S. Jog and Rajni V. Patel},
  doi          = {10.1109/TCDS.2024.3520976},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {3--21},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Sensorimotor integration: A review of neural and computational models and the impact of parkinson’s disease},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial: 2025 new year message from the editor-in-chief. <em>TCDS</em>, <em>17</em>(1), 2. (<a href='https://doi.org/10.1109/TCDS.2025.3533704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TCDS},
  author       = {Huajin Tang},
  doi          = {10.1109/TCDS.2025.3533704},
  journal      = {IEEE Transactions on Cognitive and Developmental Systems},
  month        = {2},
  number       = {1},
  pages        = {2},
  shortjournal = {IEEE Trans. Cogn. Develop. Syst.},
  title        = {Editorial: 2025 new year message from the editor-in-chief},
  volume       = {17},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
