<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TSC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tsc">TSC - 238</h2>
<ul>
<li><details>
<summary>
(2025). Comment on “RCME: A reputation incentive committee consensus-based for matchmaking encryption in IoT healthcare”. <em>TSC</em>, <em>18</em>(5), 3362-3363. (<a href='https://doi.org/10.1109/TSC.2025.3601977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Yang et al. proposed a reputation incentive committee consensus-based matchmaking scheme (IEEE Transactions on Services Computing, 2024), claiming to achieve indistinguishability under adaptive chosen ciphertext attacks (IND-CCA2). In this work, we present a plaintext recovery attack against their scheme under the adaptive chosen ciphertext attack model, analyze the design and proof flaws enabling the attacks, and suggest a countermeasure to achieve the IND-CCA2 security.},
  archive      = {J_TSC},
  author       = {Jiseung Kim and Hyung Tae Lee},
  doi          = {10.1109/TSC.2025.3601977},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3362-3363},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Comment on “RCME: A reputation incentive committee consensus-based for matchmaking encryption in IoT healthcare”},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wind-aware service provisioning strategy for multi-package drone delivery. <em>TSC</em>, <em>18</em>(5), 3348-3361. (<a href='https://doi.org/10.1109/TSC.2025.3592374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, drone delivery has drawn significant attention for its promising potential in solving the last-mile delivery problem. As a novel service paradigm, Drone-as-a-Service (DaaS) is emerging as an effective way to address service provisioning problems within complex delivery networks. However, existing DaaS composition frameworks often fail to consider the sharing of delivery services and are not applicable to multi-package drone delivery tasks. Meanwhile, as one of the most critical real-world environmental factors for drones, the impact of dynamic wind conditions is not adequately considered by existing studies. This may lead to Quality of Service (QoS) degradation of delivery services. To address the above issues, in this paper, we propose a wind-aware service provisioning strategy for multi-package drone delivery. Given the advantages of Edge Computing (EC) in handling such dynamic factors due to its low latency and high reliability, we first establish a spatio-temporal DaaS model based on service sharing according to the edge-based drone delivery system. Then, we propose a novel wind-aware drone delivery service provisioning strategy for multi-package delivery to minimize energy consumption of drones. The proposed strategy consists of two phases: service sharing and service composition. In the service sharing phase, the service sharing plan is generated by an improved genetic algorithm. In the service composition phase, the edge server dynamically generates the service composition plan through our proposed policy iteration based DaaS composition method. Experimental results using real delivery network and wind data demonstrate that our strategy is able to reduce delivery energy consumption of drone by about 10.1%.},
  archive      = {J_TSC},
  author       = {Jia Xu and Hao Liu and Xiao Liu and Azadeh Ghari Neiat and Xuejun Li and Yun Yang},
  doi          = {10.1109/TSC.2025.3592374},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3348-3361},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Wind-aware service provisioning strategy for multi-package drone delivery},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trident: A provider-oriented resource management framework for serverless computing platforms. <em>TSC</em>, <em>18</em>(5), 3334-3347. (<a href='https://doi.org/10.1109/TSC.2025.3603867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing has become increasingly popular due to its flexible and hassle-free service, relieving users from traditional resource management burdens. However, the shift in responsibility has led to unprecedented challenges for serverless providers in managing virtual machines (VMs) and serving heterogeneous function instances. Serverless providers need to purchase, provision and manage VM instances from IaaS providers, aiming to minimize VM provisioning costs while ensuring compliance with Service Level Objectives (SLOs). In this paper, we propose Trident, a provider-oriented resource management framework for serverless computing platforms. Trident optimizes three major serverless computing provisioning problems for serverless providers: workload prediction, VM provisioning, and function placement. Specifically, Trident introduces a novel dynamic model selection algorithm for more accurate workload prediction. With the prediction results, Trident then carefully designs a hierarchical reinforcement learning (HRL)-based approach for VM provisioning with a mix of types and configurations. To further improve resource utilization, Trident employs an effective collocation placement strategy for efficient function container scheduling. Evaluations on the Azure Function dataset demonstrate that Trident maintains the lowest probability of violating SLOs while simultaneously achieving substantial cost savings of up to 71.8% in provisioning expense compared to state-of-the-art methods from industry and academia.},
  archive      = {J_TSC},
  author       = {Botao Zhu and Yifei Zhu and Chen Chen and Linghe Kong},
  doi          = {10.1109/TSC.2025.3603867},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3334-3347},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Trident: A provider-oriented resource management framework for serverless computing platforms},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TPI-LLM: Serving 70B-scale LLMs efficiently on low-resource mobile devices. <em>TSC</em>, <em>18</em>(5), 3321-3333. (<a href='https://doi.org/10.1109/TSC.2025.3596892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {LLM serving is shifting from cloud to edge due to privacy concerns over user interaction data. However, mobile devices struggle with very limited computing power and memory, requiring collaboration among multiple devices to run LLM apps. The mainstream solution, pipeline parallelism, is inefficient for such cases because mobile devices typically run only one inference task at a time. This article argues that tensor parallelism, despite its high communication cost, can better fit such scenarios. We introduce TPI-LLM, a compute and memory-efficient tensor parallel inference system designed to run 70B-scale LLMs on low-resource mobile devices. It keeps sensitive raw data local on users’ devices and employs a sliding window memory scheduler to dynamically manage layer weights. It overlaps disk I/O with computation and communication, enabling efficient operation of large models on memory-limited devices. Extensive experiments show that TPI-LLM reduces token latency by 80%–90% compared to Transformers, Accelerate, and Galaxy. It also cuts the peak memory footprint by 90%, requiring just 3.1 GiB of memory for 70B-scale models.},
  archive      = {J_TSC},
  author       = {Zonghang Li and Wenjiao Feng and Mohsen Guizani and Hongfang Yu},
  doi          = {10.1109/TSC.2025.3596892},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3321-3333},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {TPI-LLM: Serving 70B-scale LLMs efficiently on low-resource mobile devices},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards fairness exploration and optimization for digital service networks. <em>TSC</em>, <em>18</em>(5), 3307-3320. (<a href='https://doi.org/10.1109/TSC.2025.3595214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital service networks often face the challenge of Service-Oriented Fairness (SOF), where service nodes with varying levels of activity may receive unequal treatment. This article takes the recommendation service system as a representative case to explore and mitigate the impact of SOF. The SOF issue in the recommendation service system can be abstracted as User-Oriented Fairness (UOF), where service models often exhibit bias toward a small group of users, resulting in significant unfairness in the quality of recommendations. Existing research on UOF faces three major limitations, and no single approach effectively addresses all of them. Limitation 1: Post-processing methods fail to address the root cause of the UOF issue. Limitation 2: Some in-processing methods rely heavily on unstable user similarity calculations under severe data sparsity problems. Limitation 3: Other in-processing methods overlook the disparate treatment of individual users within user groups. In this article, we propose a novel Individual Reweighting for User-Oriented Fairness framework, namely IR-UOF, to address all the aforementioned limitations. The motivation behind IR-UOF is to introduce an in-processing strategy that addresses the UOF issue at the individual level without the need to explore user similarities. We first conduct extensive experiments on three real-world recommendation service datasets using four backbone recommendation models to demonstrate the effectiveness of IR-UOF in mitigating UOF and improving recommendation fairness. Furthermore, we select two general digital service datasets to prove that IR-UOF can be extended to tackle the general SOF issue in other types of digital service networks. In summary, the IR-UOF framework achieves optimal model performance across all datasets, while improving fairness by at least 3.8% in recommendation systems and 24.7% in general service systems.},
  archive      = {J_TSC},
  author       = {Zhongxuan Han and Li Zhang and Chaochao Chen and Xiaolin Zheng and Yuyuan Li and Shuiguang Deng and Guanjie Cheng and Schahram Dustdar},
  doi          = {10.1109/TSC.2025.3595214},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3307-3320},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Towards fairness exploration and optimization for digital service networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a trust ecosystem for crowdsourcing IoT services: A macro perspective. <em>TSC</em>, <em>18</em>(5), 3292-3306. (<a href='https://doi.org/10.1109/TSC.2025.3604379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust plays a crucial role in crowdsourcing Internet of Things (IoT), as it can be used to select trustworthy participants to improve the quality of crowdsourced services and strengthen system security. While traditional research has focused on micro-level aspects, including trust computation and propagation, a comprehensive macro-level trust analysis remains underexplored. In this paper, we propose a macroscopic trust ecosystem analysis framework for crowdsourcing IoT services. We first construct a Trust Ecosystem Model (TEM), where trust clusters serve as an abstraction to capture and quantify overall trust characteristics based on their size and structure. To analyze the dynamic evolution of TEM, we propose a Percolation-based Trust Ecosystem Analysis Model (P-TEAM), which maps the formation of trust clusters to a joint site-bond percolation process. Thus, the study of TEM evolution can be reframed into an investigation of how trust clusters evolve as users’ trust attributes change. Through P-TEAM, we identify the critical thresholds associated with trust attributes that trigger trust phase transitions in crowdsourcing IoT services, which act as key metrics for evaluating the ecosystem’s robustness macroscopically. Finally, we further evaluate the trust ecosystem beyond these thresholds by calculating the proportions of trusted giant components. We validate our approach on directed networks, using both synthetic and real-world datasets. The experimental results further substantiate our findings and provide valuable insights into constructing a healthy and sustainable trust ecosystem for crowdsourcing IoT services.},
  archive      = {J_TSC},
  author       = {Dianjie Lu and Guijuan Zhang and Yu Guo and Xiaohua Jia},
  doi          = {10.1109/TSC.2025.3604379},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3292-3306},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Towards a trust ecosystem for crowdsourcing IoT services: A macro perspective},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-varying microservice orchestration with routing for dynamic call graphs via multi-scale deep reinforcement learning. <em>TSC</em>, <em>18</em>(5), 3276-3291. (<a href='https://doi.org/10.1109/TSC.2025.3597631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight microservices as a software architecture have been widely adopted in online application development. However, in highly-concurrent microservice scenarios, frequent data communications, complex call dependencies, and dynamic delay requirements bring great challenges to efficient microservice orchestration. In this case, service deployment and request routing are interactively-coupled in multi-instance modeling, and cannot be locally optimized effectively, thereby enlarging the difficulty for collaborative orchestration. To accommodate time-varying request properties and dynamic microservice multiplexing, orchestration schemes are frequently adapted to real-time parallel request queues, further complicating the difficulty. Nevertheless, most previous work failed to propose appropriate models and methods for the above issues. Therefore, this paper investigates the online microservice orchestration with probabilistic routing for dynamic call graphs in clouds. First, we formulate the time-slot-based joint optimization problem as a Markov Decision Process. The open Jackson queuing networks are used to accurately establish multi-instance models and analyze the request queuing, processing, and communicating delays. Then, we propose an efficient curiosity-driven deep reinforcement learning algorithm, which meticulously implements instance-level orchestration through multi-dimensional collaborative decisions and multi-time-scale trigger events. Finally, through comprehensive trace-driven experiments, our proposed approach significantly outperforms other baselines in terms of orchestration cost and resource utilization.},
  archive      = {J_TSC},
  author       = {Yi Hu and Liangbo Hou and Junhui Hu and Mingyuan Ren and Menglan Hu and Chao Cai and Kai Peng},
  doi          = {10.1109/TSC.2025.3597631},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3276-3291},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Time-varying microservice orchestration with routing for dynamic call graphs via multi-scale deep reinforcement learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task offloading and resource scheduling in mobile edge-cloud computing based on edge competition and task prediction. <em>TSC</em>, <em>18</em>(5), 3262-3275. (<a href='https://doi.org/10.1109/TSC.2025.3592390'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the emerging cloud-edge-end computing networks, edge servers possess more constrained resources and face greater task offloading pressure than centralized cloud servers due to the surge in mobile applications and data. Concurrently, the presence of multiple edge service providers introduces additional challenges, including competition among servers, disordered resource pricing, and a lack of coordination in edge and cloud resource allocation. To address these issues, we propose a novel approach aimed at optimizing task deployment, resource pricing, and system coordination. First, we develop a competitiveness model to facilitate efficient edge-side task allocation while addressing the challenges of resource pricing under competitive conditions. Second, we design a transformer-based task prediction model to enhance the accuracy of resource demand forecasting, thereby enabling more effective edge-cloud resource allocation. To achieve these objectives, the system’s interaction is structured into two distinct stages. This division simplifies the problem-solving process and ensures that the long-term goal of maximizing benefits for all stakeholders—edge service providers, cloud providers, and end-users—is achieved. The proposed solution not only improves task offloading efficiency and resource utilization, but also promotes fair competition and pricing transparency across the system.},
  archive      = {J_TSC},
  author       = {Shujuan Tian and Keke Xu and Shuhuan Xiang and Xingxia Dai and Zhu Xiao and Li Zeng},
  doi          = {10.1109/TSC.2025.3592390},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3262-3275},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Task offloading and resource scheduling in mobile edge-cloud computing based on edge competition and task prediction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sudoku: Scalable high-density cloud rendering multi-client architecture. <em>TSC</em>, <em>18</em>(5), 3249-3261. (<a href='https://doi.org/10.1109/TSC.2025.3607449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, WebRTC, a transmitted protocol key for real-time interactions, is widely adopted in cloud-rendering areas, especially for the video-streaming part. Traditional end-to-end cloud rendering clients are becoming inadequate due to increasing server densities, particularly in areas like offline eSports and commercial promotions. Addressing this, we introduce Sudoku, a novel WebRTC-based solution for high-density cloud rendering. This implementation stands out by segregating the control and data planes. The control plane, on the CPU, adopts a single-process strategy, eliminating inter-process communication and enhancing scalability. In contrast, the data plane on the GPU introduces a fine-grained dataflow control, isolating each game stream to minimize resource contention and improve stream playback quality. Sudoku also integrates direct rendering technology, leveraging data locality for faster decoding and rendering. Our evaluations reveal that Sudoku dramatically surpasses existing models, cutting decoding and rendering times by 93.3% and 52.6%, respectively, and achieving a 5.76-fold density increase. So far, Sudoku has already been used at Intel for cloud-rendering performance tests and demonstrates its efficacy and potential for broader applications. This development signifies a major leap in the field, offering improved efficiency and scalability for high-density cloud rendering setups.},
  archive      = {J_TSC},
  author       = {Yun Wang and Bing Deng and Xia Jiang and Xuyan Hu and Dongjie Tang and Hongyu Zhang and Randy Xu and Yong Yao and Yijin Sun and Zhengwei Qi},
  doi          = {10.1109/TSC.2025.3607449},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3249-3261},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Sudoku: Scalable high-density cloud rendering multi-client architecture},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stability-oriented heterogeneous application re-deployment in mobile edge computing. <em>TSC</em>, <em>18</em>(5), 3235-3248. (<a href='https://doi.org/10.1109/TSC.2025.3592382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Mobile Edge Computing (MEC), various heterogeneous applications have being deployed on edge servers in close proximity to end-users for the low-latency responses. In this circumstance, since the resources on edge servers are limited, it is critical to deploy these applications on suitable edge servers. However, due to the heterogeneity of the applications and the mobility of end-users in real MEC circumstances, the requests each edge server received may undergo temporal fluctuations in both views of quantity and type. In other words, it is crucial to re-deploy these heterogeneous applications to match these dynamic circumstances, instead of permanent deployments without adjustments. Nevertheless, frequent re-deployment causes service interruptions and resource wastage, leading to system instability. Existing approaches struggle to handle redeployment effectively in heterogeneous, dynamic, and stability-critical MEC environments. In this paper, we first formulate the Edge Application Re-Deployment problem on the basis of constrained multi-objective optimization and prove its $\mathcal {NP}$-hardness. Then we propose an optimal re-deployment approach based on the Integer Programming technique for small-scale edge application re-deployment scenarios. And we also propose a Decompose-Solve-Merge approximation approach which balances the effectiveness and efficiency with a configurable parameter for large-scale scenarios. Extensive experiments on a real-world data set evaluate our novel approaches against four existing representative approaches. Additionally, we perform the ablation experiment to validate the effectiveness of our approaches and explore the impact of configurable parameter on the performance. The results show the superior performance of our approaches on re-deployment in terms of heterogeneous, dynamic, and stability.},
  archive      = {J_TSC},
  author       = {Gaofeng Zhang and Sheng Jia and Liqiang Xu and Benzhu Xu and Wenming Wu and Liping Zheng},
  doi          = {10.1109/TSC.2025.3592382},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3235-3248},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Stability-oriented heterogeneous application re-deployment in mobile edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPAVM: A SFC placement and VNF migration framework for VNF instance reuse in vehicle-infrastructure collaboration. <em>TSC</em>, <em>18</em>(5), 3221-3234. (<a href='https://doi.org/10.1109/TSC.2025.3592379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of Vehicle-Infrastructure Collaboration (VIC), this study addresses the critical challenge of optimizing Service Function Chains (SFCs) deployment within resource-constrained and delay-sensitive vehicular edge networks. By leveraging Virtual Network Function (VNF) technology, which shifts network services from traditional hardware to a more agile, container-based edge computing architecture, we aim to enhance the Quality of Service (QoS) for vehicular users. SFCs, composed of multiple VNFs arranged in a specific sequence, are pivotal for delivering a range of functional services essential for QoS enhancement. Our objective is to optimize the placement of SFCs by reusing VNF instances to minimize both delay and operational costs. The reuse of VNF instances, however, introduces two significant challenges: the effective placement of SFCs within vehicular edge networks and the optimization of VNF instance containers positioning. To address these challenges, we propose a robust framework called SPAVM (Service Placement and VNF Migration), which consists of two primary components: one for SFC placement and the other for VNF migration. For SFC placement, we introduce the Service Placement based on VNF Instance Reuse algorithm (SPVIR), which maximizes the utilization of existing VNF container resources. For VNF migration, we propose the VNF Instance Migration algorithm (VIMA), which considers edge server connectivity to determine optimal migration targets for VNF instance containers. Extensive experiments validate the proposed algorithms’ performance, demonstrating their effectiveness in reducing delay and costs, thereby enhancing the overall efficiency of vehicular edge networks.},
  archive      = {J_TSC},
  author       = {Shihong Hu and Kaixin Zhang and Zhihao Qu and Baoliu Ye},
  doi          = {10.1109/TSC.2025.3592379},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3221-3234},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {SPAVM: A SFC placement and VNF migration framework for VNF instance reuse in vehicle-infrastructure collaboration},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SpaceCache+: Towards pervasive content delivery via low-earth orbit mega-constellations. <em>TSC</em>, <em>18</em>(5), 3206-3220. (<a href='https://doi.org/10.1109/TSC.2025.3592385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging Low-Earth Orbit (LEO) mega-constellations face challenges such as limited bandwidth and highly variable user demand, which can degrade network performance and lead to inefficient satellite resource utilization. One promising solution is to enable Content Delivery Networks (CDNs) within LEO satellites by deploying cache-equipped satellites. However, many existing approaches rely on inter-satellite links, which are not widely used in practice and are typically activated only when terrestrial ground station coverage is insufficient. Furthermore, the dynamic coverage patterns of satellites and diverse regional content preferences add to the complexity of efficient CDN deployment in space. To address these challenges, we propose SpaceCache+, a satellite-based CDN framework. We introduce a new metric, user benefit, that jointly captures user coverage and latency reduction to assess the effectiveness of cache satellite deployment. Recognizing that deployment typically occurs incrementally, we formulate the User Benefit-centric Cache Satellite Deployment problem and design an efficient heuristic solution. To enhance content placement, we also propose a cache replacement policy based on zero-shot meta-learning, which adapts to both regional content popularity and satellite mobility. We evaluate the performance of SpaceCache+ using real-world constellation settings with CDN traces. Compared with benchmark strategies, SpaceCache+ improves user benefit and cache hit ratio by up to 66.29% and 77.12%, respectively.},
  archive      = {J_TSC},
  author       = {Songshi Dou and Shengyu Zhang and Zhenglong Li and Jinxian Wu and Xianhao Chen and Kwan L. Yeung},
  doi          = {10.1109/TSC.2025.3592385},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3206-3220},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {SpaceCache+: Towards pervasive content delivery via low-earth orbit mega-constellations},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SNN-IoT: Efficient partitioning and enabling of deep spiking neural networks in IoT services. <em>TSC</em>, <em>18</em>(5), 3191-3205. (<a href='https://doi.org/10.1109/TSC.2025.3592380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs), due to their inherent biological plausibility and energy-saving characteristics, naturally align with the requirements of IoT services. However, current SNNs require a multi-layer structure to achieve effective applications across various fields. The multi-layer deep SNNs with massive model parameters demand computational resources, rendering them incompatible with resource-constrained IoT devices. To address this problem, in this work, a deep SNN partitioning framework called SNN-IoT is proposed to run complex SNN models on IoT devices. The SNN-IoT first partitions a full deep SNN model into smaller sub-models, leveraging the event-driven sparsity of SNNs and channel-level firing patterns to distribute filters with lower levels of spike activity onto devices with more constrained resources. The SNN model partitioning and deployment is formulated as an optimization problem and is solved using a greedy search assignment mechanism. Furthermore, a channel-wise pruning method exploits the varying degrees of channel activity, effectively reducing each sub-model’s size and computational load without compromising performance. Extensive experiments conducted on four non-neuromorphic and two neuromorphic datasets have demonstrated that the SNN-IoT framework not only efficiently partitions deep SNNs and enables their deployment on IoT devices but also significantly reduces the inference latency and energy consumption for IoT services. The experiment uses 9 Raspberry Pi-4B as the IoT devices, and results show that SNN-IoT may reduce the average latency and energy consumption by about 60.7% and 49.9%, respectively, while maintaining the inference accuracy.},
  archive      = {J_TSC},
  author       = {Xin Du and WenTao Tong and Linshan Jiang and Di Yu and Zhiliang Wu and Qiang Duan and Shuiguang Deng},
  doi          = {10.1109/TSC.2025.3592380},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3191-3205},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {SNN-IoT: Efficient partitioning and enabling of deep spiking neural networks in IoT services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Slarm: SLA-aware, reliable and efficient transaction dissemination for permissioned blockchains. <em>TSC</em>, <em>18</em>(5), 3177-3190. (<a href='https://doi.org/10.1109/TSC.2025.3592411'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The blockchain paradigm has attracted diverse applications to be deployed upon. However, no service-level agreement (SLA) mechanism has been proposed to enforce the SLA disseminating deadlines to commit blockchain transactions, although these transactions are often interactively submitted by clients and desire short SLA deadlines (e.g., tens of seconds). Existing peer-to-peer (P2P) multicast protocols for blockchains take the unidirectional approach to disseminate transactions regardless of their SLA deadlines, making transactions easily violate their deadlines. Moreover, these protocols are vulnerable to malicious P2P nodes, and their protocol messages (e.g., SLA-stringent transactions) are vulnerable to deferring attacks. We propose Slarm, the first bidirectional P2P multicast protocol for permissioned blockchains, which conservatively adjusts transactions’ dissemination speed to satisfy their SLA deadlines according to the trustworthy SLA feedback of previously disseminated transactions. Slarm guarantees transactions’ SLAs in a decentralized way and defends against the deferring attacks using TEE. Evaluation of Slarm with five notable P2P multicast protocols and five diverse real-world applications shows that: even with transaction spikes and attacked nodes, Slarm achieves a much higher transaction SLA satisfaction rate with reasonably high commit throughput.},
  archive      = {J_TSC},
  author       = {Ji Qi and Tianxiang Shen and Jianyu Jiang and Xusheng Chen and Xiapu Luo and Fengwei Zhang and Heming Cui},
  doi          = {10.1109/TSC.2025.3592411},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3177-3190},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Slarm: SLA-aware, reliable and efficient transaction dissemination for permissioned blockchains},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Serving LLM in distributed GPU cluster with fine-grain pipeline constraints. <em>TSC</em>, <em>18</em>(5), 3164-3176. (<a href='https://doi.org/10.1109/TSC.2025.3603576'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Large Language Models (LLMs) continue to advance, their parameter sizes are growing exponentially—far outpacing hardware capabilities. This widening gap necessitates distributed computing through pipeline parallelism for efficient inference. However, the uneven distribution of requests across pipeline stages creates significant performance bottlenecks in real-world deployments. To address this challenge, we present Planck, a performance optimization framework specifically designed for distributed LLM inference. Planck implements fine-grained control through two key mechanisms: a progressive SLO allocation strategy that dynamically adjusts time constraints based on workload patterns, and stage-specific performance controllers that prevent bottlenecks before they cascade through the system. By intelligently balancing resources across pipeline stages, Planck effectively eliminates queue buildup—essentially preventing traffic congestion before it forms. Evaluation using diverse workloads in real cloud environments demonstrates that Planck reduces P99 tail latency by up to 18% and decreases the longest queue lengths by as much as 47.8% across pipeline stages, significantly improving both system responsiveness and resource utilization.},
  archive      = {J_TSC},
  author       = {Yanying Lin and Shijie Peng and Shuaipeng Wu and Yanbo Li and Chengzhi Lu and Kejiang Ye and Chengzhong Xu},
  doi          = {10.1109/TSC.2025.3603576},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3164-3176},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Serving LLM in distributed GPU cluster with fine-grain pipeline constraints},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure optimization with preferred skyline predicate on incomplete data. <em>TSC</em>, <em>18</em>(5), 3152-3163. (<a href='https://doi.org/10.1109/TSC.2025.3592527'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outsourcing data storage and computations to cloud servers offers a cost-effective solution for remote data management and query processing. However, ensuring the privacy of sensitive information remains a critical concern, and existing secure algorithms rely on data completeness where all attribute values are valid to ignore the dominance issues under intransitivity and cyclicity. This paper addresses the challenge of executing secure skyline predicates on outsourced incomplete data, while keeping the dataset, queries, and results confidential from the cloud servers. We propose a novel secure dominance under incomplete data as a core component of various query types. To balance security and efficiency, we introduce two filtering methods around access patterns. Additionally, we present two secure skyline extensions concerning dimension and skyband to produce meaningful skylines. The proposed solutions are empirically evaluated for efficiency and scalability on diverse datasets, demonstrating the practical viability of our approach.},
  archive      = {J_TSC},
  author       = {Yu Chen and Rongmao Chen and Shaojing Fu and Xinyi Huang and Mingwu Zhang and Yuexiang Yang},
  doi          = {10.1109/TSC.2025.3592527},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3152-3163},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Secure optimization with preferred skyline predicate on incomplete data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure and privacy preserving scalable GDPR enforcement system. <em>TSC</em>, <em>18</em>(5), 3137-3151. (<a href='https://doi.org/10.1109/TSC.2025.3603041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protecting personal data has become a major issue of concern in the whole world. As per the General Data Protection Regulation (GDPR), Data Owners (DO) are given back control over their data, which is maintained by the Service Providers (SP). However, SP needs to be fully GDPR compliant. In the existing GDPR enforcement technologies, DO is severely facing the scalability issue considering it has to manage separate keys for different data. Clearly, it would be highly impractical to manage such a huge and proportionally increased amount of keys for resource constrained devices of DO. Furthermore, the integrity of the data received by the Data User (DU) is not addressed in the existing works. We propose a proxy re-encryption based secure protocol where DO has to manage only one key. We propose a threat model to analyze our proposed protocol. In this analysis, we use a random oracle model to assess security and privacy. We use Burrows-Abadi-Needham (BAN) logic to verify the correctness. Simulation through AVISPA and ProVerif, formal verification tools, also confirms the security and privacy of the proposed protocol. A testbed simulation is conducted to estimate the computational, communication, and storage requirements, ensuring the practicality of the proposed protocol.},
  archive      = {J_TSC},
  author       = {Mou Dutta and Subhasish Dhal and Hema Gohain},
  doi          = {10.1109/TSC.2025.3603041},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3137-3151},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Secure and privacy preserving scalable GDPR enforcement system},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing makespan via optimizing service applications scheduling without runtime estimation. <em>TSC</em>, <em>18</em>(5), 3123-3136. (<a href='https://doi.org/10.1109/TSC.2025.3608216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient scheduling of service applications is critical for improving cluster resource utilization while minimizing makespan and application completion time. However, existing schedulers often struggle with coordinating task placement on worker machines due to the lack of runtime estimations. This limitation leads to two major performance issues: the non-synchronization problem and the contention-oblivious problem, both of which result in suboptimal application completion times. To address these challenges, Morbius is proposed, a scheduler that explicitly leverages the spatial structure of service applications to enhance scheduling decisions. Morbius adopts an all-or-nothing scheduling policy, ensuring that all tasks of an application are scheduled to run simultaneously, thereby effectively mitigating the non-synchronization problem. Within each priority queue, Morbius follows a shortest total time first policy, which facilitates contention-aware scheduling. Moreover, Morbius incorporates work conservation and starvation avoidance policies to better handle execution uncertainties and further improve application completion times. A prototype of Morbius is implemented on Yarn and evaluated in two environments: a homogeneous cluster with 36 machines and a heterogeneous cluster with 122 machines. Experimental results show that Morbius significantly outperforms existing approaches, improving average application completion time by up to 10.41× and reducing makespan by over 32.80%.},
  archive      = {J_TSC},
  author       = {Libin Liu and Zhixiong Niu and Xiuting Xu},
  doi          = {10.1109/TSC.2025.3608216},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3123-3136},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Reducing makespan via optimizing service applications scheduling without runtime estimation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $R^{3}$: A building block for disordering-tolerant load balancing in data center networks. <em>TSC</em>, <em>18</em>(5), 3106-3122. (<a href='https://doi.org/10.1109/TSC.2025.3603578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Packet-level load balancing has shown its massive potential for long in utilizing super high bisection bandwidth of data center network (DCN). This kind of potential, however, has still not been completely transformed into huge performance enhancement of data transmission. The fundamental reason is that packet-level load balancing can fully utilize the parallel paths of underlying physical network, but suffer from the problem of packet disordering transmission, which greatly impairs the flow-level transmission performance of DCN. This paper explores the root cause of performance impairment generated by packet disordering transmission, and proposes $R^{3}$, a solution focusing on “recognizably releasing redundant acknowledgements” as a building block for data center packet-level load balancer. In $R^{3}$’s heart, the source leaf switch perceives the global packet loss information and selectively intercepts the redundant acknowledgement packets, thus avoiding the TCP-driven end-host from experiencing frequent window reductions and unnecessary packet retransmissions. Experimental results of numerous simulation tests and real implementations show that, after integrating $R^{3}$ into the representative data center packet-level load balancing schemes, the transmission performances of both delay-sensitive and throughput-oriented data center flows are significantly improved. Furthermore, $R^{3}$ is merely implemented by switch, leaving the end hosts and the deployed load balancing scheme totally unchanged.},
  archive      = {J_TSC},
  author       = {Tao Zhang and Yuchen Wang and Yuanzhen Hu and Jinbin Hu and Haotian Jing and Yangfan Li and Xidao Luan},
  doi          = {10.1109/TSC.2025.3603578},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3106-3122},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {$R^{3}$: A building block for disordering-tolerant load balancing in data center networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving blockchain-enabled parametric insurance via remote sensing and IoT. <em>TSC</em>, <em>18</em>(5), 3093-3105. (<a href='https://doi.org/10.1109/TSC.2025.3570857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Insurance, a popular approach of financial risk management, has suffered from the issues of high operational costs, opaqueness, inefficiency and a lack of trust. Recently, blockchain-enabled parametric insurance through authorized data sources (e.g., remote sensing and IoT) aims to overcome these issues by automating the underwriting and claim processes of insurance policies on a blockchain. However, the openness of blockchain platforms raises a concern of user privacy, as the private user data in insurance claims on a blockchain may be exposed to outsiders. In this paper, we propose a privacy-preserving parametric insurance framework based on succinct zero-knowledge proofs (zk-SNARKs), whereby an insuree submits a zero-knowledge proof (without revealing any private data) for the validity of an insurance claim and the authenticity of its data sources to a blockchain for transparent verification. Moreover, we extend the recent zk-SNARKs to support robust privacy protection for multiple heterogeneous data sources and improve its efficiency to cut the incurred gas cost by 80%. As a proof-of-concept, we implemented a working prototype of bushfire parametric insurance on real-world blockchain platform Ethereum, and present extensive empirical evaluations.},
  archive      = {J_TSC},
  author       = {Mingyu Hao and Keyang Qian and Sid Chi-Kin Chau},
  doi          = {10.1109/TSC.2025.3570857},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3093-3105},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Privacy-preserving blockchain-enabled parametric insurance via remote sensing and IoT},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-aware joint DNN model deployment and partitioning optimization for collaborative edge inference services. <em>TSC</em>, <em>18</em>(5), 3079-3092. (<a href='https://doi.org/10.1109/TSC.2025.3607117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge inference (EI) has emerged as a promising paradigm to address the growing limitations of cloud-based Deep Neural Network (DNN) inference services, such as high response latency, limited scalability, and severe data privacy exposure. However, deploying DNN models on resource-constrained edge devices introduces additional challenges, including limited computation/storage resources, dynamic service demands, and heightened privacy risks. To address these challenges, this paper presents a novel privacy-aware optimization framework that jointly tackles DNN model deployment, user-server association, and model partitioning, aiming to minimize long-term average inference delay under resource and privacy constraints. We formulate the problem as a complex, NP-hard stochastic optimization problem, emphasizing its inherent complexity. To efficiently handle system dynamics and computational complexity, we adopt a Lyapunov-based approach to transform the long-term objective into tractable per-slot decisions. Additionally, we introduce a coalition formation game model to facilitate adaptive user-server association, and design a greedy algorithm for model deployment within each coalition. Extensive simulations demonstrate that the proposed algorithm significantly reduces inference delay while consistently satisfying privacy constraints, outperforming baselines across diverse scenarios.},
  archive      = {J_TSC},
  author       = {Zhipeng Cheng and Xiaoyu Xia and Hong Wang and Minghui Liwang and Ning Chen and Xuwei Fan and Xianbin Wang},
  doi          = {10.1109/TSC.2025.3607117},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3079-3092},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Privacy-aware joint DNN model deployment and partitioning optimization for collaborative edge inference services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing blockchain shard allocations service: A multi-objective evolutionary perspective. <em>TSC</em>, <em>18</em>(5), 3065-3078. (<a href='https://doi.org/10.1109/TSC.2025.3597191'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sharding is one of the most effective techniques for addressing scalability challenges in blockchain systems. However, existing sharding schemes often fail to balance security and scalability, primarily due to unavoidable cross-shard communication costs. Some schemes rely on additional roles like TEEs or alliances to streamline cross-shard consensus, introducing security risks such as hardware attacks or node collusion. Others mitigate cross-shard consensus costs by periodically distributing nodes or states based on predefined rules, yet inefficient distribution rules lead to poor scalability. In response, this article proposes SAC, a novel sharding allocation service that efficiently trades scalability and security via a two-stage allocation strategy. First, SAC employs lightweight state graph clustering to group frequently interacting states within the same shards based on historical transaction data, reducing cross-shard transactions significantly. Second, it formulates node allocation as a multi-objective evolutionary problem (MoSA) that jointly maximizes system throughput, minimizes confirmation latency, and balances malicious node distribution. Next, SAC selects FV-MOEA as the foundational solver for MoSA after comprehensive preliminary experiments. Based on this, SAC proposes an improved algorithm, LeFV, to explore optimal shard allocation solutions. Specifically, LeFV retains and mutates low-contributing but potentially high-quality solutions to enhance population diversity. It allows for a wider exploration of shard allocations, thereby identifying optimal ones that effectively balance scalability and security of the sharding system. Extensive experiments on a sophisticated blockchain emulator demonstrate that SAC outperforms two advanced state-of-the-art methods in balancing scalability and security.},
  archive      = {J_TSC},
  author       = {Kun Zhou and Hongbing Cheng and Zhicheng Xu and Jie Gao and Huixin Chen},
  doi          = {10.1109/TSC.2025.3597191},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3065-3078},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Optimizing blockchain shard allocations service: A multi-objective evolutionary perspective},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective unlearning in recommender systems via preference guided pareto exploration. <em>TSC</em>, <em>18</em>(5), 3052-3064. (<a href='https://doi.org/10.1109/TSC.2025.3593906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems typically collect and analyze user data, which raises the risk of privacy invasion. User-sensitive information can be leaked from the user portrait, e.g., user embedding, within recommender models. Therefore, the task of recommendation unlearning has been widely studied, aiming to eliminate the influence of target data on recommender models. This paper explores the extended concept of unlearning, which seeks to remove sensitive user information while retaining the essential information for recommendation purposes. Previous studies have primarily focused on extended unlearning in isolation, e.g., attribute unlearning. However, users often need to fulfill multiple unlearning objectives simultaneously. Therefore, we bridge this gap by introducing post-training multi-objective unlearning, which allows the concurrent fulfillment of multiple unlearning objectives while preserving recommendation performance. Note that the objectives may conflict with each other, leading to the compromise of one objective when minimizing the overall objective value. To address this challenge, we introduce a Pareto exploration approach that incorporates the recommendation performance as optimization guidance, allowing us to obtain the Pareto optimal solution through the trade-off between conflicting objectives. To adapt to practical scenarios where data is not accessible post-training, we utilize a data-free regularization to guide recommendation performance. We conducted extensive experiments on three real-world datasets, which demonstrate the effectiveness of our proposed method.},
  archive      = {J_TSC},
  author       = {Yuyuan Li and Yizhao Zhang and Weiming Liu and Xiaohua Feng and Zhongxuan Han and Chaochao Chen and Chenggang Yan},
  doi          = {10.1109/TSC.2025.3593906},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3052-3064},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Multi-objective unlearning in recommender systems via preference guided pareto exploration},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPDA: A massively parallel learning and dependency-aware scheduling algorithm for data processing clusters. <em>TSC</em>, <em>18</em>(5), 3038-3051. (<a href='https://doi.org/10.1109/TSC.2025.3592427'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of large-scale machine learning, large-scale clusters are extensively used for data processing jobs. However, the state-of-the-art heuristic-based and Deep Reinforcement Learning (DRL) based job scheduling mechanisms are facing challenges such as slow training speed and under-exploitation of jobs’ complex dependencies. We propose MPDA, a Massively Parallel learning and Dependency-Aware scheduling algorithm, consisting of a fast-training mechanism and a novel dependency-aware policy network, GATNetwork, to address these two challenges respectively. The fast-training mechanism is a two-level massively parallel training method that can significantly accelerate the training process and maximally utilize the resources of the cluster. Additionally, its decoupled learning and interacting design enables hybrid-workload training for MPDA, which guarantees the generalization and robustness of MPDA. The GATNetwork exploits the dependencies among stages/jobs using Graph Attention Network (GAT) and Long Short-Term Memory (LSTM) networks to improve the performance of the scheduling policy. The experiments show that MPDA accelerates the training speed by one to two orders of magnitude and achieves better scheduling performance, i.e., lower average job completion time, compared with existing scheduling algorithms.},
  archive      = {J_TSC},
  author       = {Qing Li and Xingchi Chen and Jiawei Guo and Fa Zhu and Achyut Shankar and Fayez Alqahtani and Kamalakanta Muduli and Bo Yi and Yong Jiang},
  doi          = {10.1109/TSC.2025.3592427},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3038-3051},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {MPDA: A massively parallel learning and dependency-aware scheduling algorithm for data processing clusters},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model caching and application offloading for mobile edge intelligence network with learning-and-optimization approach. <em>TSC</em>, <em>18</em>(5), 3022-3037. (<a href='https://doi.org/10.1109/TSC.2025.3592381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Intelligence is a promising computing paradigm for mobile users to access Artificial Intelligence (AI) services. It seamlessly integrates AI online inference processes with Mobile Edge Computing (MEC), delivering low-latency services through application offloading. However, previous works often overlooked the need to pre-deploy relevant AI models on the edge server and disregarded the impact of model deployment on service performance. Furthermore, even with proper model deployment, service efficiency still depends heavily on the resource allocation strategies of the edge server. To this end, we propose a hybrid Deep Reinforcement Learning (DRL) approach, termed SA2CNN, which jointly optimizes discrete decisions on AI model caching and application offloading, along with continuous allocation of bandwidth and frequency resources, to minimize user energy cost and task latency. We first formulate the aforementioned challenges into a Markov decision process, then decompose it into two low-complexity sub-problems: the Discrete Destination Selection (DDS) problem and the Continuous Resources Allocation (CRA) problem. DRL is responsible for outputting the DDS actions, while CRA sub-problem results are solved by convex optimization. Furthermore, we decouple the caching and offloading decisions across time slots to eliminate the impact of model deployment on task performance, and employ a deep convolutional neural network to effectively learn the underlying temporal dependencies. Simulations show that our approach reduces service cost by 52.1% to 68.1% compared to the baselines, exhibiting significant performance enhancements.},
  archive      = {J_TSC},
  author       = {Ziyu Peng and Yu Qiu and Gaocai Wang},
  doi          = {10.1109/TSC.2025.3592381},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3022-3037},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Model caching and application offloading for mobile edge intelligence network with learning-and-optimization approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MicroForge: An integrated platform for accelerating experiments in microservice research with the universal evolution model. <em>TSC</em>, <em>18</em>(5), 3008-3021. (<a href='https://doi.org/10.1109/TSC.2025.3608160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid adoption of microservice architectures has spurred research into addressing various challenges in microservice system evolution, such as service deployment and task offloading. While numerous solutions have been proposed by researchers, validating these approaches requires extensive experimentation, including both algorithmic testing and simulations across different Microservice-Based Applications (MBAs). However, current research lacks sufficient physical experimentation, which is essential for evaluating how these solutions perform when deployed on actual computing infrastructure with running microservice instances. This gap exists mainly because conducting physical experiments is complex and costly, primarily due to two barriers: the lack of standardized problem modeling approaches and the absence of suitable experimental platforms. To address these challenges, the Universal Evolution Model (UEM) was proposed that unifies different microservice research problems within a common evolution framework. This model frames research problems as approaches to evolve microservice systems under specific constraints to achieve desired objectives. Based on UEM and the widely-adopted Monitor-Analyze-Plan-Execute over a shared Knowledge (MAPE-K) model, a general Microservice System Evolution Architecture was designed that standardizes experimental processes across diverse research scenarios. Building on these foundations, MicroForge was presented, an open-source experimental platform equipped with tools for accelerating experiments. The platform incorporates RescueService, a carefully curated MBA. Through comprehensive evaluation across various MBAs, the functional effectiveness of MicroForge was validated. Additionally, a practical user guide was provided to facilitate the platform’s adoption and utilization.},
  archive      = {J_TSC},
  author       = {Xiang He and Teng Wang and Zihang Su and Zhenxiang Zhao and Yin Chen and Jihong Yan and Zhongjie Wang},
  doi          = {10.1109/TSC.2025.3608160},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {3008-3021},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {MicroForge: An integrated platform for accelerating experiments in microservice research with the universal evolution model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Magneto: Load-balanced key-value service for write-intensive workloads. <em>TSC</em>, <em>18</em>(5), 2994-3007. (<a href='https://doi.org/10.1109/TSC.2025.3592425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-performance key-value (KV) storage is critical for the cloud, providing KV services to various cloud applications. A key challenge for KV services is that workloads of cloud applications are often write-intensive and exhibit highly-skewed characteristics, which result in load imbalance among storage servers thus lowering system performance. To address this problem, prior works set up an in-switch write-back caching mechanism, which adopts a centralized controller to balance writes. However, the limited bandwidth between the switches and the controller is a severe bottleneck for achieving high performance. In this paper, we present Magneto, a novel key-value service architecture for the cloud. At the core of Magneto is a delayed-write mechanism in the switch data plane to absorb frequent write queries for hot items. It effectively balances the load under write-intensive workloads without involving the controller. Magneto also designs a reliability mechanism to ensure system reliability during switch state transitions and failures. We implement a prototype using an FPGA-integrated switch, which has high packet-processing performance and contains enough memory to provide both the in-switch cache and the write buffer. Extensive evaluation shows that Magneto can achieve 8.4x system throughput gains compared to baseline systems when handling a skewed workload consisting of 70% reads and 30% writes. Moreover, it can reduce the load on back-end servers up to 56% in total.},
  archive      = {J_TSC},
  author       = {Yuanhang Gao and Yingwen Chen and Xiangrui Yang and Huan Zhou and Shihua Tang and Ming Xu},
  doi          = {10.1109/TSC.2025.3592425},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2994-3007},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Magneto: Load-balanced key-value service for write-intensive workloads},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). J$\text{C}^{5}$A: Service delay minimization for aerial MEC-assisted industrial cyber-physical systems. <em>TSC</em>, <em>18</em>(5), 2976-2993. (<a href='https://doi.org/10.1109/TSC.2025.3592419'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of the sixth generation (6G) and industrial Internet of Things (IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of sensor devices. To address the limited resources of IIoT sensor devices, uncrewed aerial vehicle (UAV)-assisted mobile edge computing (MEC) has emerged as a promising solution, providing flexible and cost-effective services in close proximity of IIoT sensor devices (ISDs). However, leveraging aerial MEC to meet the delay-sensitive and computation-intensive requirements of the ISDs could face several challenges, including the limited communication, computation and caching (3C) resources, stringent offloading requirements for 3C services, and constrained on-board energy of UAVs. To address these issues, we first present a collaborative aerial MEC-assisted ICPS architecture by incorporating the computing capabilities of the macro base station (MBS) and UAVs. We then formulate a service delay minimization optimization problem (SDMOP). Since the SDMOP is proved to be an NP-hard problem, we propose a joint computation offloading, caching, communication resource allocation, computation resource allocation, and UAV trajectory control approach (J$\text{C}^{5}$A). Specifically, J$\text{C}^{5}$A consists of a block successive upper bound minimization method of multipliers (BSUMM) for computation offloading and service caching, a convex optimization-based method for communication and computation resource allocation, and a successive convex approximation (SCA)-based method for UAV trajectory control. Moreover, we theoretically prove the convergence and polynomial complexity of J$\text{C}^{5}$A. Simulation results demonstrate that the proposed approach can achieve superior system performance compared to the benchmark approaches and algorithms.},
  archive      = {J_TSC},
  author       = {Geng Sun and Jiaxu Wu and Zemin Sun and Long He and Jiacheng Wang and Dusit Niyato and Abbas Jamalipour and Shiwen Mao},
  doi          = {10.1109/TSC.2025.3592419},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2976-2993},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {J$\text{C}^{5}$A: Service delay minimization for aerial MEC-assisted industrial cyber-physical systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Is your AI truly yours? leveraging blockchain for copyrights, provenance, and lineage. <em>TSC</em>, <em>18</em>(5), 2960-2975. (<a href='https://doi.org/10.1109/TSC.2025.3586091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Artificial Intelligence (AI) integrates into diverse areas, particularly in content generation, ensuring rightful ownership and ethical use becomes paramount, AI service providers are expected to prioritize responsibly sourcing training data and obtaining licenses from data owners. However, existing studies primarily center on safeguarding static copyrights, which simply treat metadata/datasets as non-fungible items with transferable/trading capabilities, neglecting the dynamic nature of training procedures that can shape an ongoing trajectory. In this paper, we present IBis, a blockchain-based framework tailored for AI model training workflows. Our design can dynamically manage copyright compliance and data provenance in decentralized AI model training processes, ensuring that intellectual property rights are respected throughout iterative model enhancements and licensing updates. Technically, IBis integrates on-chain registries for datasets, licenses and models, alongside off-chain signing services to facilitate collaboration among multiple participants. Further, IBis provides APIs designed for seamless integration with existing contract management software, minimizing disruptions to established model training processes. We implement IBis using Daml on the Canton blockchain. Evaluation results showcase the feasibility and scalability of IBis across varying numbers of users, datasets, models, and licenses.},
  archive      = {J_TSC},
  author       = {Qin Wang and Guangsheng Yu and Yilin Sai and H.M.N. Dilum Bandara and Shiping Chen},
  doi          = {10.1109/TSC.2025.3586091},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2960-2975},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Is your AI truly yours? leveraging blockchain for copyrights, provenance, and lineage},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IRS-assisted hyperspectral image processing in satellite edge computing services. <em>TSC</em>, <em>18</em>(5), 2946-2959. (<a href='https://doi.org/10.1109/TSC.2025.3595166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of satellite technology has significantly enhanced satellite computing service capabilities, particularly in terms of its application potential for complex tasks such as hyperspectral image (HSI) processing. Satellite edge computing (SEC) substantially improves processing efficiency by transferring task processing to the satellite. At the same time, intelligent reflective surfaces (IRS) reduce the pressure on ground service center communication resources by optimizing communication links between satellites on the ground. However, existing works mainly optimize general computing tasks, resulting in limited performance when processing HSI tasks. This paper proposes an IRS-assisted HSI processing SEC system to achieve the optimal balance between HSI processing accuracy and system energy consumption. We formulate an optimization problem as a joint task covering HSI offloading, band selection, and IRS phase shift optimization to achieve optimal overall performance. To address the problem, we propose the joint feature iterative optimization (JFIO) framework for HSI processing, which generates optimized task offloading solutions through graph attention networks, utilizes multi-feature attention capsule networks to achieve efficient band selection, and combines this with IRS modules to optimize communication link conditions. Extensive experiments on various datasets demonstrate that the proposed framework achieves an excellent balance between accuracy and energy consumption, with its performance significantly outperforming other baseline methods.},
  archive      = {J_TSC},
  author       = {Xiaoteng Yang and Jie Feng and Lei Liu and Qingqi Pei and Shahid Mumtaz and Keqin Li and Schahram Dustdar},
  doi          = {10.1109/TSC.2025.3595166},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2946-2959},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {IRS-assisted hyperspectral image processing in satellite edge computing services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating functional and structural semantics for web API recommendation via multi meta-path aggregation. <em>TSC</em>, <em>18</em>(5), 2932-2945. (<a href='https://doi.org/10.1109/TSC.2025.3599574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing popularity of Internet services, developers can reuse or mix these services to create new applications (such as mashups) to improve development efficiency. In previous studies, several web API recommendation methods have been proposed, some of which rely on mashup demand information to extract necessary functional features. These methods often incorporate Quality of Service (QoS) assessments of network services to enhance recommendations. Others utilize graph neural networks to extract implicit structural semantics from the web API network and apply them to build downstream tasks. Despite the significant contributions of these methods in advancing web API recommendations, the challenge lies in how to better extract structural information in highly sparse conditions and combine it with functional information to optimize recommendation performance. In this paper, we construct a mashup-tag-API graph based on tag similarity to address the sparsity between mashups and web APIs. We also design a method to jointly incorporate structural and functional information. Structural information is mined through multiple meta-path aggregation mechanisms. Mashups are aggregated into different clusters based on structural information. Developers’ requirement features are then extracted using functional semantic components and mapped to groups of similar mashups. Finally, the model recommends highly compatible network APIs based on historical call records from these groups. Comprehensive testing using real-world datasets from ProgrammableWeb demonstrates that our method outperforms baseline methods in terms of accuracy, recall, and MAP.},
  archive      = {J_TSC},
  author       = {Chunyan Sang and Yahao Liu and Shigen Liao and Junhao Wen},
  doi          = {10.1109/TSC.2025.3599574},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2932-2945},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Integrating functional and structural semantics for web API recommendation via multi meta-path aggregation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HG-PAD: Heterogeneous graph structure learning aided performance anomaly diagnosis in microservice systems. <em>TSC</em>, <em>18</em>(5), 2916-2931. (<a href='https://doi.org/10.1109/TSC.2025.3603218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microservice architecture offers great scalability and flexibility to the development of online servicessystems. Performance anomalies, which happen frequently due to code bugs or runtime environmentmisconfiguration, can severely damage the system availability and cause great losses. However, it is challenging to detect performance anomalies and locate their root causes considering the large volume of monitoring data (e.g., metrics and traces) and the complex dynamic interdependence between heterogeneous services. Against these challenges, we propose HG-PAD, an automatic performance anomaly diagnosis (PAD) framework for microservice systems. We build the multi-relation heterogeneous graph to model the intricate dependency between services. We further design a structure learning mechanism combining graph neural network (GNN) and node embedding learning to capture the dynamic and latent dependencies. Based on the optimized dependency graph, we devise a Conditional Variational Auto-Encoder (CVAE) based unsupervised anomaly detection method and a graph attention network (GAT) based root cause localization method for accurate anomaly diagnosis. We use datasets of different scales based on real world applications to verify the effectiveness of HG-PAD, and the experimental results show that HG-PAD achieves better diagnostic performance compared with existing baseline methods.},
  archive      = {J_TSC},
  author       = {Jian Yang and Zian Wang and Shuangwu Chen and Huasen He and Yunpeng Hou and Xiaofeng Jiang},
  doi          = {10.1109/TSC.2025.3603218},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2916-2931},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {HG-PAD: Heterogeneous graph structure learning aided performance anomaly diagnosis in microservice systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FrigateBird: Decoupling Metadata/Data services for continuously fast object storage. <em>TSC</em>, <em>18</em>(5), 2904-2915. (<a href='https://doi.org/10.1109/TSC.2025.3594340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, ride-hailing service has tens of millions of registered drivers and hundreds of millions of registered passengers, serving tens of millions of rides per day. Different from social media applications like Facebook and LinkedIn, ride hailing needs to read/write/query a large number of small objects (like photos and audio/video pieces) always fast, so that it can support critical online computations such as face comparison and sentiment analysis on audio/video records. This is of particular importance for ride-hailing service to recognize and avoid potential dangers. Existing object stores (like Haystack and Tectonic) usually store object data in files and place object metadata in a separate key-value store (like RocksDB), which is unsuitable for ride-hailing service mainly because the objects’ file-related information is placed together with the object data. This severely affects the I/O performance of object storage: first, for crash consistency, the writes of object data and object metadata must be conducted in separate phases of one transaction, which significantly increases I/O latency; second, the space of deleted objects needs to be reclaimed via compaction, which could sharply lower I/O performance when the system is busy in serving normal read/write requests. This paper describes FrigateBird, a continuously fast object store for ride-hailing service. FrigateBird differs from existing object stores in three aspects. First, we present a metadata/data decoupled service architecture for object storage, where the rich metadata service realizes efficient queries and updates of object metadata, and the raw data service purely performs disk I/O to read/write object data from/to raw disks. Second, we propose a rich metadata structure (called RichMeta) taking the write operation logs as part of object metadata, which allows FrigateBird to simultaneously write the object data to raw disks (without filesystem overhead) and write the metadata to a key-value store, guaranteeing crash consistency by checking whether the transaction is completed and rolling back if not. Third, we design a compaction-free deletion mechanism which can efficiently delete an object by only updating the metadata without involving the data service, so that FrigateBird can efficiently support ride-hailing service’s frequent delete operations while avoiding data-migration-caused performance hiccup. Evaluation shows that FrigateBird outperforms the state-of-the-art object stores by up to $3.36\times$ and $27.9\times$ in the mean I/O latency for normal and in-compaction scenarios, respectively.},
  archive      = {J_TSC},
  author       = {Yiming Zhang and Kekun Hu and Hailiang Chen and Gang Dong and Rengang Li},
  doi          = {10.1109/TSC.2025.3594340},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2904-2915},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {FrigateBird: Decoupling Metadata/Data services for continuously fast object storage},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault-tolerant cost-efficient scheduling for energy and deadline-constrained IoT workflows in edge-cloud continuum. <em>TSC</em>, <em>18</em>(5), 2892-2903. (<a href='https://doi.org/10.1109/TSC.2025.3599497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing brings computation closer to Internet-of-Things (IoT) data sources, reducing latency but increasing energy consumption and susceptibility to node failures. The cloud platform provides extensive computational capabilities, but comes with significant costs and communication delays due to network congestion. The edge-cloud continuum strategically combines these approaches to mitigate their individual drawbacks. However, effectively scheduling IoT workflows to minimize costs while adhering to strict requirements for latency, energy efficiency, and reliability remains a major challenge in real-time IoT applications. To address these challenges, we propose the Reliable Energy-constrained Cost-aware Real-time (RECR) algorithm for optimizing IoT workflow scheduling across the edge-cloud continuum. RECR minimizes monetary costs and enhances reliability while adhering to strict energy and deadline constraints. We also introduce RECR-D, a fault-tolerant extension that employs adaptive task duplication to manage transient and permanent failures, with reliability rigorously modeled using Continuous-Time Markov Chains (CTMCs) to integrate dynamic failure behavior. Extensive simulations demonstrate that RECR reduces workflow monetary costs by approximately 21% and improves deadline adherence by 37% compared to state-of-the-art algorithms. Furthermore, RECR-D improves compliance with reliability and energy constraints by 27% and by up to 208%, respectively, highlighting its robust performance in dynamic, failure-prone environments. These contributions significantly advance workflow management for IoT applications, proving crucial for real-time traffic control and video analytics in smart cities, ensuring timely processing and lower costs. They are also vital for remote patient monitoring and medical imaging analysis in healthcare, improving reliability and meeting deadlines for patient safety.},
  archive      = {J_TSC},
  author       = {Ahmad Taghinezhad-Niar and Javid Taheri},
  doi          = {10.1109/TSC.2025.3599497},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2892-2903},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Fault-tolerant cost-efficient scheduling for energy and deadline-constrained IoT workflows in edge-cloud continuum},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extending IoT devices network lifetime using IoT service prediction and recommendation. <em>TSC</em>, <em>18</em>(5), 2879-2891. (<a href='https://doi.org/10.1109/TSC.2025.3599467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {IoT devices continue to be an integral part of our everyday lives, providing convenience, efficiency, and enhanced capabilities. With limited storage, computational, and energy resources, the optimized management of these resources becomes essential for resource-constrained devices. The proposed solution is a two-phase framework for optimizing IoT resource management. First, we apply a Time-Series analysis Machine Learning (ML) technique to predict upcoming user service requests based on their historical interactions and preferences within a specified Region of Interest (RoI). Second, using anticipated user requests, we design a recommendation system to suggest a set of devices that serve those service requests based on the user’s Quality of Service (QoS) and security preferences, while addressing common challenges such as the cold-start problem, device availability, and energy consumption. The validity of our framework is characterized by an improved device network lifetime, with devices being available up to 95% of the service time, a 20% increase in the number of user requests fulfilled, and the ability to provide more service with fewer devices. These results highlight that predicting future user service requests, along with a service recommendation method, is a novel way to optimize the resource management process.},
  archive      = {J_TSC},
  author       = {Damilola Alao and Victorine Clotilde Wakam Younang and Amartya Sen},
  doi          = {10.1109/TSC.2025.3599467},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2879-2891},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Extending IoT devices network lifetime using IoT service prediction and recommendation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ESR-MHFL: Edge server reallocation for multi-hierarchical federated learning. <em>TSC</em>, <em>18</em>(5), 2865-2878. (<a href='https://doi.org/10.1109/TSC.2025.3606219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) enables efficient and privacy-preserving Edge Intelligence (EI) in Mobile Edge Computing (MEC). However, implementing FL-enabled EI services faces critical challenges, including data and device heterogeneity, limited network resources, uneven distribution of network infrastructure, etc., which may intensify with increasing system scale. These challenges are particularly acute in multi-provider environments where edge servers are suboptimally allocated across federations, leading to degraded convergence and increased training costs. In this article, we present a novel Multiple Hierarchical Federated Learning (MHFL) architecture for large-scale FL and design an Edge Server Reallocation scheme (ESR-MHFL) to enhance training efficiency by optimally redistributing edge servers among federations based on their contribution to model convergence. We first develop a closed-form analysis model for MHFL to quantify training time, computation, and communication costs. To improve training efficiency, we analyze the impacts of edge server allocation on convergence and formulate server reallocation as a multi-item auction problem with theoretical guarantees. We then propose ESR-MHFL, which leverages Coalition Structure Generation (CSG) and greedy matching methods to simplify the reallocation problem and enhance efficiency. Extensive numerical simulations demonstrate that ESR-MHFL not only improves model accuracy while reducing training cost but also exhibits strong compatibility with existing client selection methods, achieving improved training efficiency. The total economic expenditure combining all components.},
  archive      = {J_TSC},
  author       = {Tianao Xiang and Yuanguo Bi and Lin Cai and Chong Yu and Mingjian Zhi and Rongfei Zeng and Tom H. Luan},
  doi          = {10.1109/TSC.2025.3606219},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2865-2878},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {ESR-MHFL: Edge server reallocation for multi-hierarchical federated learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EPPQ: Efficient and privacy-preserving $k$NN query processing for outsourced high-dimensional data. <em>TSC</em>, <em>18</em>(5), 2851-2864. (<a href='https://doi.org/10.1109/TSC.2025.3600124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive schemes have been conducted on the development of efficient and privacy-preserving $k$NN query algorithms in data outsourcing scenarios. However, existing researches primarily address low-dimensional data, posing scalability challenges in higher dimensions. To tackle this issue, we propose an efficient and privacy-preserving $k$NN query scheme for outsourced high-dimensional data (EPPQ), emphasizing the complete lifecycle from secure dimensionality reduction of high-dimensional data to secure $k$NN query on the reduced-dimensional data. Specifically, in the secure dimensionality reduction phase: on the one hand, EPPQ integrates principal component analysis (PCA) for dimensionality reduction to minimize computational overhead. On the other hand, to address privacy concerns during the process of PCA, by incorporating differential privacy (DP), we propose the Privacy-Preserving Data Dimensionality Reduction Algorithm based on PCA (PDDRP). In the secure $k$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math>NN query phase: for one thing, EPPQ facilitates the index of the reduced-dimensional data by k-d tree. To enhance index efficiency, we innovatively propose plaintexts-based distance calculation definitions (PDC definitions) and construct an efficient variant of k-d tree (Ek-d tree), for the first time. For another, the Paillier homomorphic encryption (PHE) technique is leveraged to safeguard privacy when outsourcing Ek-d tree to untrusted cloud servers. Additionally, for ciphertexts-based distance calculations and comparisons, we design the Secure Precomputed Distance protocol (SPCD) and Secure Comparison protocol (SCOM). Finally, we creatively present the Privacy-Preserving $k$NN Query Algorithm based on Ek-d tree (PKQKT) for efficient and secure $k$NN query. Comprehensive security analysis demonstrates that the EPPQ scheme meets the required security properties under the honest-but-curious model. Extensive experiments confirms that EPPQ achieves high computational efficiency and query accuracy.},
  archive      = {J_TSC},
  author       = {Jing Wang and Haiyong Bao and Rongxing Lu and Cheng Huang and Menghong Guan and Lu Xing},
  doi          = {10.1109/TSC.2025.3600124},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2851-2864},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {EPPQ: Efficient and privacy-preserving $k$NN query processing for outsourced high-dimensional data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing game policy optimization in mobile crowdsourcing: A reinforcement learning approach. <em>TSC</em>, <em>18</em>(5), 2837-2850. (<a href='https://doi.org/10.1109/TSC.2025.3592388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Crowd Sensing (MCS) is a widely adopted approach for data collection across diverse applications. However, as the number of tasks and participants in MCS continues to grow, task allocation and dynamic pricing challenges have become increasingly complex. Existing research primarily focuses on single-task allocation problems, often overlooking the diversity and complexity inherent in multi-task, multi-worker scenarios. To address these challenges, this paper proposes the Enhanced Heuristic Search with Tabu and Local Search (EH-STLS) algorithm, alongside a Kolmogorov-Arnold Deep Q Network (KDQN) model, both grounded in a Stackelberg game framework. The EH-STLS algorithm employs a multi-objective optimization framework that combines tabu search and local search strategies to improve the efficiency of worker-task matching while ensuring high task quality. The KDQN model views task publishers as leaders and treats crowdsourcing platforms, encryption agencies, and workers as followers to achieve optimal dynamic pricing and utility allocation. Extensive experiments on synthetic datasets generated from real-world data reveal that the proposed methods substantially outperforms the baseline algorithms regarding task allocation quality and pricing efficiency, achieving up to a 21.7% increase in task quality and a 16.4% improvement in pricing effectiveness.},
  archive      = {J_TSC},
  author       = {Dihong Luo and Yingjie Wang and Haojun Teng and Bingyi Xie and Meimei Sun and Zhipeng Cai},
  doi          = {10.1109/TSC.2025.3592388},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2837-2850},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Enhancing game policy optimization in mobile crowdsourcing: A reinforcement learning approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling the awareness of video perceived quality for short-form video streaming. <em>TSC</em>, <em>18</em>(5), 2822-2836. (<a href='https://doi.org/10.1109/TSC.2025.3592391'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, fueled by the rapid advances in high-speed mobile networks, streaming short-form videos over mobile devices (e.g., TikTok) has become ubiquitous among mobile users. Despite the widespread application, our investigation based on a real video data source revealed that a large proportion of short videos watched by viewers have suboptimal video quality (e.g., with low VMAF scores), which indicates that the Quality-of-Experience (QoE) is in fact far from optimal. This problem is primarily due to the lack of awareness of video quality optimization based on the features of the video content such as the scene complexity. To tackle this problem, this work develops a novel system called Quality Aware Short Video Streaming (QASVS), which adopts machine learning techniques to learn the video content features and then automatically generate quality-driven bitrate decision models to optimize the perceived video quality and QoE. Extensive evaluations show that QASVS is able to improve the video quality by 11.1%∼27.9% while significantly reducing the playback rebuffering compared to the state-of-the-art streaming algorithms. Therefore, QASVS is able to provide an effective way for streaming vendors to deliver high-performance short-video services.},
  archive      = {J_TSC},
  author       = {Guanghui Zhang and Jing Guo and Mengbai Xiao and Hui Yuan and Dongxiao Yu and Xiuzhen Cheng},
  doi          = {10.1109/TSC.2025.3592391},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2822-2836},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Enabling the awareness of video perceived quality for short-form video streaming},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E-log: Fine-grained elastic log-based anomaly detection and diagnosis for databases. <em>TSC</em>, <em>18</em>(5), 2808-2821. (<a href='https://doi.org/10.1109/TSC.2025.3594870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Database Management Systems (DBMS) form the backbone of modern large-scale software systems, where reliable anomaly detection and diagnosis are essential for ensuring system availability. However, existing log-based methods often impose significant performance overhead by collecting large volumes of logs, which is impractical for DBMS requiring high read/write throughput. This paper addresses a critical yet underexplored challenge: how to balance logging granularity with runtime efficiency for effective anomaly management in databases. We present E-Log, a novel fine-grained elastic log-based framework for anomaly detection and diagnosis. E-Log intelligently adjusts the amount and detail of logging based on system state—maintaining lightweight logging during normal operation for efficient anomaly detection, and triggering rich, informative logging only upon anomaly suspicion for accurate diagnosis. This adaptive strategy significantly reduces runtime overhead while preserving diagnostic precision. We implement E-Log on Apache IoTDB and evaluate it using benchmarks including TSBS, TPCx-IoT, and IoT-Bench. Experimental results show that E-Log improves anomaly detection accuracy by 3.15% and diagnosis performance by 9.32% compared to state-of-the-art methods. Moreover, it reduces log storage size by 43.53% and increases average write throughput by 26.22%. These results highlight E-Log’s potential to enable efficient, accurate, and scalable anomaly management in high-performance database systems.},
  archive      = {J_TSC},
  author       = {Lingzhe Zhang and Tong Jia and Xinyu Tan and Xiangdong Huang and Mengxi Jia and Hongyi Liu and Zhonghai Wu and Ying Li},
  doi          = {10.1109/TSC.2025.3594870},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2808-2821},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {E-log: Fine-grained elastic log-based anomaly detection and diagnosis for databases},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient seamless task offloading based on edge-terminal collaborative for AIoT elastic computing services. <em>TSC</em>, <em>18</em>(5), 2794-2807. (<a href='https://doi.org/10.1109/TSC.2025.3592386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial Intelligence of Things (AIoT) utilizes a combination of computing, storage, and networking resources to provide highly reliable and low-latency information services to the industrial production processes. However, with the increasing integration of numerous smart terminals into real-time sensing, autonomous decision-making, and precision manufacturing execution systems, the current task scheduling pattern appears to be insufficient to meet the latency requirements of computationally intensive tasks. To address the above challenge, this paper presents a collaborative edge-terminal task offloading scheme. First, the Task Backlog and Multi-slot Scheduling (TBMS) problem is converted from a long-term offloading problem to a single timeslot scheduling problem by Lyapunov optimization. Then, to simplify the problem, the single timeslot problem is decomposed into three subproblems: the local resource allocation problem, the server resource allocation problem, and the indicator weight selection problem. The two resource allocation problems are proved to be convex, which have been solved by using the Bisection method and the Karush-Kuhn-Tucker (KKT) method, respectively. For the indicator weight selection problem, we proposed the enhanced jumping spider optimization algorithm that integrates the elite opposition-based learning strategy. Extensive experiments show that the proposed algorithm can alleviate the computing pressure of the terminal device. Compared with the traditional methods, the offload system cost is effectively reduced by at least 58.8% and the average execution success rate is increased by at least 6%.},
  archive      = {J_TSC},
  author       = {Jing Wang and Yuhuai Peng and Xinyu Zhang and Lei Liu and Shahid Mumtaz and Mohsen Guizani and Schahram Dustdar},
  doi          = {10.1109/TSC.2025.3592386},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2794-2807},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Efficient seamless task offloading based on edge-terminal collaborative for AIoT elastic computing services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic function placement and request scheduling of serverless workflows in edge environment. <em>TSC</em>, <em>18</em>(5), 2781-2793. (<a href='https://doi.org/10.1109/TSC.2025.3592226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, edge computing has emerged as a promising solution for deploying IoT applications that demand minimal latency. By leveraging Function as a Service (FaaS) at the edge, it is possible to achieve efficient and scalable computing capabilities. However, implementing serverless deployment at the edge presents challenges such as auto-scaling, resource management, and mitigating cold-start delays, particularly due to the limited resources available. These challenges are even more significant in workflow-based applications, where tasks are interdependent. This article introduces a dynamic approach for executing serverless workflows at the edge, consisting of three key components: initial function placement, request scheduling, and dynamic adjustment. The initial placement leverages the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to deploy function instances across edge nodes. Request scheduling, on the other hand, distributes requests among these instances using a pattern graph matching algorithm. Finally, the dynamic adjustment component periodically refines placement and scheduling strategies to adapt to changing demands, utilizing a local search technique known as simulated annealing. Evaluation results indicate that the proposed solution reduces the average makespan of workflows by up to 86% compared to state-of-the-art methods.},
  archive      = {J_TSC},
  author       = {Behrooz Zolfaghari and Saeid Abrishami and Abbas Rasoolzadegan and Bahman Javadi},
  doi          = {10.1109/TSC.2025.3592226},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2781-2793},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Dynamic function placement and request scheduling of serverless workflows in edge environment},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic food delivery problem based on spatial crowdsourcing. <em>TSC</em>, <em>18</em>(5), 2768-2780. (<a href='https://doi.org/10.1109/TSC.2025.3592378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, crowdsourcing online food delivery (COFD) services have been increasingly popular, in which a number of crowdsourced riders are recruited to deliver food orders for those geographically dispersed customers. Due to the dynamics and uncertainty of food orders and riders, it is challenging to design an immediate allocation mechanism to recruit suitable riders and plan paths, with the goal of maximizing the total profit of all recruited riders. To address this challenge, we first formalize a crowdsourcing online food delivery problem with the goal of maximizing the expectation of long-term profit of all riders. Then, an efficient heuristic-based algorithm is proposed in which order-exchange and order-transfer policies are applied. Finally, we conduct extensive experiments on synthetic and real-world datasets to evaluate our proposed algorithm, whose results show that the proposed policies and algorithm are more effective compared to the baselines in terms of total profit, total distance, and average waiting time.},
  archive      = {J_TSC},
  author       = {Lichen Zhang and Yangyang Zhang and Tong Li},
  doi          = {10.1109/TSC.2025.3592378},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2768-2780},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Dynamic food delivery problem based on spatial crowdsourcing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disruption-aware microservice re-orchestration for cost-efficient multi-cloud deployments. <em>TSC</em>, <em>18</em>(5), 2754-2767. (<a href='https://doi.org/10.1109/TSC.2025.3604373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-cloud environments enable a cost-efficient scaling of cloud-native applications across geographically distributed virtual nodes with different pricing models. In this context, the resource fragmentation caused by frequent changes in the resource demands of deployed microservices, along with the allocation or termination of new and existing microservices, increases the deployment cost. Therefore, re-orchestrating deployed microservices on a cheaper configuration of multi-cloud nodes offers a practical solution to restore the cost efficiency of deployment. However, the rescheduling procedure causes frequent service interruptions due to the continuous termination and rebooting of the containerized microservices. Moreover, it may potentially interfere with and delay other deployment operations, compromising the stability of the running applications. To address this issue, we formulate a multi-objective integer linear programming (ILP) problem that computes a microservice rescheduling solution capable of providing minimum deployment cost without significantly affecting the service continuity. At the same time, the proposed formulation also preserves the quality of service (QoS) requirements, including latency, expressed through microservice co-location constraints. Additionally, we present a heuristic algorithm to approximate the optimal solution, striking a balance between cost reduction and service disruption mitigation. We integrate the proposed approach as a custom plugin of the Kubernetes (K8s) scheduler. Results reveal that our approach significantly reduces multi-cloud deployment costs and service disruptions compared to the benchmark schemes, while ensuring QoS requirements are consistently met.},
  archive      = {J_TSC},
  author       = {Marco Zambianco and Silvio Cretti and Domenico Siracusa},
  doi          = {10.1109/TSC.2025.3604373},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2754-2767},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Disruption-aware microservice re-orchestration for cost-efficient multi-cloud deployments},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DiffMSR: A multi-semantic graph diffusion model for service recommendation. <em>TSC</em>, <em>18</em>(5), 2740-2753. (<a href='https://doi.org/10.1109/TSC.2025.3596891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of cloud computing and service computing, service recommendation systems play a crucial role in helping users efficiently filter the appropriate services. However, the sparsity of service data and the presence of noise in interactions make it extremely challenging to accurately capture user preferences. Existing service recommendation methods based on Graph Neural Networks (GNNs) primarily rely on ID aggregation, often neglecting the richness of textual semantics and are susceptible to interaction noise, resulting in suboptimal modeling of user-service relationships. Although Large Language Models (LLMs) demonstrate remarkable advantages in capturing textual semantics, current methods struggle to effectively align structural representations with textual representations, limiting improvements in recommendation performance. To address these challenges, we propose an innovative multi-semantic graph diffusion model for service recommendation, DiffMSR, which aims to align textual and structural representations while learning the generation process of interaction graphs in a denoising manner. This approach mitigates data sparsity and effectively reduces noise interference. Specifically, the model leverages LLMs to capture the textual semantic features of service descriptions and integrates them with structured semantic information from knowledge graphs. Through cross-semantic contrastive learning, it achieves heterogeneous semantic alignment. Furthermore, the model introduces a multisemantic diffusion-based generation framework, which iteratively denoises to construct high-quality user-service interaction graphs. This significantly enhances the multi-semantic awareness of user representations, thereby improving recommendation performance. Experiments on public service datasets demonstrate that DiffMSR outperforms existing state-of-the-art baseline methods, achieving improvements of 4.13% and 6.37% in recommendation accuracy and recall, respectively.},
  archive      = {J_TSC},
  author       = {Xiang Xie and Jianxun Liu and Buqing Cao and Wenyu Zhao and Sheng Lin and Min Shi and Jinjun Chen},
  doi          = {10.1109/TSC.2025.3596891},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2740-2753},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {DiffMSR: A multi-semantic graph diffusion model for service recommendation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning to hash for time-aware QoS prediction based on VQ-VAE. <em>TSC</em>, <em>18</em>(5), 2726-2739. (<a href='https://doi.org/10.1109/TSC.2025.3592409'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In mobile edge computing (MEC) environments, with the increase in the number of web services possessing the same or similar functions, the prediction of nonfunctional quality of service (QoS) indicators turns increasingly vital in satisfying the diverse needs of different users. Despite the significant achievements of QoS prediction approaches such as collaborative filtering (CF), they often fail to obtain informative representations due to complex user-service-time QoS data. Moreover, changing network conditions and overlooked temporal factors lead to reduced accuracy. In response to these challenges, this paper proposes a novel deep learning to hash method for time-aware QoS prediction built on a VQ-VAE (named Pred$_{QoS}$<mml:math><mml:msub><mml:mrow/><mml:mrow><mml:mi>Q</mml:mi><mml:mi>o</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math>). In the proposed Pred$_{QoS}$<mml:math><mml:msub><mml:mrow/><mml:mrow><mml:mi>Q</mml:mi><mml:mi>o</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math>, inspired by advanced CF approaches, we make QoS predictions at a specific moment for the target user through its similar timeslots. Specifically, we train a codebook to discretize the continuous latent representation obtained by the encoder based on vector quantization (VQ), which is motivated by the generative vector-quantized variational autoencoder (VQ-VAE) model, allowing us to derive compact binary codes representing the QoS data. Then, the similarities between QoS data are determined, helping to make effective and efficient predictions. Furthermore, the time-aware Pred$_{QoS}$<mml:math><mml:msub><mml:mrow/><mml:mrow><mml:mi>Q</mml:mi><mml:mi>o</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math> approach incorporates a temporal factor by training on multiple QoS data (including the user, service, and time dimensions). As a result, the discrete hash codes (i.e., QoS data representations) derived from the encoder can fully uncover the time factor’s dynamic impact on the QoS data, thereby yielding significantly improved prediction performance. In summary, the Pred$_{QoS}$<mml:math><mml:msub><mml:mrow/><mml:mrow><mml:mi>Q</mml:mi><mml:mi>o</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math> approach learns compact hash codes for original QoS data while taking time into account, enabling accurate predictions to be produced through similar timeslots for the target users. Finally, comprehensive experiments carried out on the real-world WS-DREAM dataset affirm the exceptional performance of the Pred$_{QoS}$<mml:math><mml:msub><mml:mrow/><mml:mrow><mml:mi>Q</mml:mi><mml:mi>o</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math>.},
  archive      = {J_TSC},
  author       = {Lingzhen Kong and Xiyuan Hu and Lianyong Qi and Xiaolong Xu and Yiwen Zhang and Lina Yao and Xuyun Zhang},
  doi          = {10.1109/TSC.2025.3592409},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2726-2739},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Deep learning to hash for time-aware QoS prediction based on VQ-VAE},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning and feedback control based container auto-scaling for cloud native micro-services. <em>TSC</em>, <em>18</em>(5), 2714-2725. (<a href='https://doi.org/10.1109/TSC.2025.3596887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Kubernetes-based Cloud Native platforms, allocating containers to micro-services elastically according to workload changes is benefical to minimizing resource cost while stabling response times. However, inaccurate performance models for multi-container systems, along with coarse-grained container-based allocation, cause performance fluctuations. In this paper, deep learning, traditional Jackson Queuing Network (JQN) and feedback control are integrated to devise a container provisioning algorithm which leverages the neural networks’ ability to fit nonlinear performance models, the real-time responsiveness of feedback control, and the precise prediction of micro-service interactions offered by the JQN. The proposal is evaluated on a real Kubernetes based Cloud Native cluster. Experimental results illustrate that the container cost is decreased by 10.94%$\sim$11.36% while satifisfying Service Level Agreements (SLA) in terms of 95th accessing-path response times.},
  archive      = {J_TSC},
  author       = {Zhicheng Cai and Hang Wu and Xu Jiang and Xiaoping Li and Rajkumar Buyya},
  doi          = {10.1109/TSC.2025.3596887},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2714-2725},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Deep learning and feedback control based container auto-scaling for cloud native micro-services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-locality-aware task assignment and scheduling for distributed job executions. <em>TSC</em>, <em>18</em>(5), 2701-2713. (<a href='https://doi.org/10.1109/TSC.2025.3594158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the data-locality-aware task assignment and scheduling problem for distributed job executions. Our goal is to minimize job completion times without prior knowledge of future job arrivals. We propose an Optimal Balanced Task Assignment algorithm (OBTA), which achieves minimal job completion times while significantly reducing computational overhead through efficient narrowing of the solution search space. To balance performance and efficiency, we extend the approximate Water-Filling (WF) algorithm, providing a rigorous proof that its approximation factor equals the number of task groups in a job. We also introduce a novel heuristic, Replica-Deletion (RD), which outperforms WF by leveraging global optimization techniques. To further enhance scheduling efficiency, we incorporate job ordering strategies based on a shortest-estimated-time-first policy, reducing average job completion times across workloads. Extensive trace-driven evaluations validate the effectiveness and scalability of the proposed algorithms.},
  archive      = {J_TSC},
  author       = {Hailiang Zhao and Xueyan Tang and Peng Chen and Jianwei Yin and Shuiguang Deng},
  doi          = {10.1109/TSC.2025.3594158},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2701-2713},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Data-locality-aware task assignment and scheduling for distributed job executions},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAppCheat: Detecting cheating robots for DApps on multiple blockchains. <em>TSC</em>, <em>18</em>(5), 2687-2700. (<a href='https://doi.org/10.1109/TSC.2025.3596896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed a rapid increase in the number of blockchain-based decentralized applications (DApps). As reported by DAppRadar, there are more than 5,000 DApps with more than 17.2 million daily Unique Active Wallets (users). However, it is also reported that some robots are used to manipulate the ranking, attract more users, cheat investors, etc. Hence, it is necessary to detect those robots. Unlike traditional robots or spam detection on Internet, each blockchain has its specific data structure with the impacts of exchanges and whales, leading to the challenges of detecting DApp robots. In this paper, we conduct the first systematic investigation on DApps robots, named DAppCheat. We first collect and release the first multi-blockchain DApp-user dataset, including 4,857 DApps and 99,758,959 users from Ethereum, EOSIO, TRON, and BSC. We propose a general parent account mechanism for multiple blockchains in order to find anonymous user collusion. We define the creating account weight and the used DApp volume weight to reduce the impacts of exchanges and whales. Extensive experimental results show the effectiveness of DAppCheat.},
  archive      = {J_TSC},
  author       = {Peilin Zheng and Xiapu Luo and Weilin Zheng and Zibin Zheng},
  doi          = {10.1109/TSC.2025.3596896},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2687-2700},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {DAppCheat: Detecting cheating robots for DApps on multiple blockchains},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing offloading for digital twinning empowered industrial IoT. <em>TSC</em>, <em>18</em>(5), 2673-2686. (<a href='https://doi.org/10.1109/TSC.2025.3596626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Digital Twin (DT) represents a rapidly advancing technological innovation within the Industrial Internet of Things (IIoT) domain. DT leverages the power of simulation, machine learning, and data mining to facilitate optimal decision-making for physical objects. However, the creation of a dynamic and living digital counterpart comes at a considerable cost. It requires continuous massive data updating and processing every time the physical object changes. As most data collected by IIoT devices are in their original form, such as images and videos, transmitting such data to remote cloud computing will result in large delays. Furthermore, data processing is often a computationally intensive operation, such as image recognition and video coding, making it impractical to perform processing tasks directly in IIoT devices. To overcome this problem, we introduced the Multi-access/mobile Edge Computing (MEC) architecture to enhance capabilities of DT-enabled IIoT devices. IIoT devices can leverage the extra computing resources in MEC to process raw data, transmitting only the calculation results to update the digital counterpart. To efficiently allocate resources between IIoT devices and MEC, we propose a double auction-based resource allocation scheme. The IIoT devices can purchase computing power from MEC, and an iterative double auction scheme is applied to achieve system efficiency within this market. Furthermore, we propose the Win or Learn Fast Algorithm Policy Hill Climbing (Wolf-PHC) algorithm, which enables agents to improve their strategies continuously through participation in auctions. Simulation results demonstrate that this algorithm accelerates the process of market equilibrium convergence.},
  archive      = {J_TSC},
  author       = {Weibo Qin and Ying Wang and Haipeng Yao and Jiaqi Xu and Tianle Mai and Yunjie Liu and Zehui Xiong and F. Richard Yu},
  doi          = {10.1109/TSC.2025.3596626},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2673-2686},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Computing offloading for digital twinning empowered industrial IoT},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CATScaler: A convolution-augmented transformer scaling framework for cloud-native applications. <em>TSC</em>, <em>18</em>(5), 2659-2672. (<a href='https://doi.org/10.1109/TSC.2025.3592383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient container scaling is crucial for enhancing the availability and scalability of cloud-native applications through adaptive resource management. In cloud computing, the default autoscaling feature of Kubernetes scales pods only when the cluster or application exceeds a predefined threshold. However, this reactive approach often leads to significant resource waste during demand fluctuations because it cannot predict future workload changes and adjust resources in advance. This paper presents CATScaler, a novel Convolution-Augmented Transformer Scaler designed to proactively optimize resource allocation in serverless environments. CATScaler is a proactive approach composed of two modules: workload prediction and elastic auto-scaling. In the prediction module, we develop a convolution-augmented transformer to accurately predict workload changes at both local and global levels. Additionally, we incorporate reversible instance normalization to mitigate the shift caused by the difference between workload data and training data. In the auto-scaling module, we implement an instance-counting method to handle the nonlinear relationships between variables. Experiments using two real datasets from Alibaba Cloud and Huawei Cloud demonstrate the effectiveness of CATScaler. The tests conducted on a cluster of 4 servers demonstrated that CATScaler reduced response time latency by 1.1× compared to Kubernetes’ default scaler and decreased service violation rates by 3.2×.},
  archive      = {J_TSC},
  author       = {Fan’an Meng and Hongjun Dai and Guoqing Cong and Bo Zhu and Hailiang Zhao},
  doi          = {10.1109/TSC.2025.3592383},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2659-2672},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {CATScaler: A convolution-augmented transformer scaling framework for cloud-native applications},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Calling out trustless users: A trust propagation scheme for decentralized trust management. <em>TSC</em>, <em>18</em>(5), 2646-2658. (<a href='https://doi.org/10.1109/TSC.2025.3601226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trust management has been widely employed to determine a user’s trustworthiness based on evaluations from other entities, and trustless users are those with low trustworthiness due to dishonest or malicious behaviors. To overcome the defects of traditional centralized trust management, decentralized trust management has been proposed, leveraging blockchain to store trust data, e.g., user interaction evaluations, securely. However, blockchain-based decentralized trust management usually suffers from throughput limitation, hindering timely record users’ trust data, delaying expose trustless users. As a result, trustless users may still interact with others in the system with the outdated trustworthiness, undermining the reliability and fairness of the system. Furthermore, decentralized pseudonymous networks suffer from a trust cold-start problem due to lacking users’ prior interaction history or endorsements from trusted third parties, making it hard for newly joined users to assess the trustworthiness. To address these issues, we propose TUES in this paper, an efficient Trustless User Exposure Scheme. TUES stores trust data in a trust blockchain collectively maintained by all users. To efficiently expose trustless users, we design a dynamic consensus mechanism for TUES. This dynamic consensus mechanism integrates three novel consensus algorithms, efficiently utilizing network throughput to record trust data of trustless users and ensure the consistency of the blockchain. Additionally, TUES includes a multi-signature-based scheme to allocate initial trust values to users, thus resolving the trust cold-start problem in decentralized pseudonymous networks. Analysis and experiments show that TUES improves the efficiency of exposing trustless users while maintaining the consistency of the trust blockchain. It also increases the cost for adversaries conducting Sybil, whitewashing and Byzantine attacks.},
  archive      = {J_TSC},
  author       = {Yong Yu and Haochen Yang and Yannan Li and Robert H. Deng},
  doi          = {10.1109/TSC.2025.3601226},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2646-2658},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Calling out trustless users: A trust propagation scheme for decentralized trust management},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARMM: Sandwich-attack resilient automated market maker. <em>TSC</em>, <em>18</em>(5), 2632-2645. (<a href='https://doi.org/10.1109/TSC.2024.3376195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular decentralized exchange (DEX) model is the automated market maker (AMM), supporting billions of USD of daily trading volume. Notably, a sandwich attack is a predatory trading strategy upon DEX, where adversaries extract on a monthly basis over 10 M USD from the benign traders. Previous solutions provided sandwich-attack resilience primarily through complex trader operations or costly order matching, which are user-unfriendly and inefficient on blockchain. In this work, we propose a new AMM design—the Autonomous Reordered Market Maker (ARMM) with integrated sandwich-attack resilience. Specifically, we provide a prediction resistant on-chain reordering algorithm to randomly shuffle and reassemble transactions, reducing the probability of a successful sandwich attack. Our empirical evaluation suggests that ARMM can prevent a profitable single-block sandwich attack with 90.16%, and increase the sandwich attack victim’s revenue with a probability of 96%. In addition, we simulate the profit of adversaries in cross-block sandwich attacks and find that ARMM can even keep the adversary from profiting and increase the victim’s revenue with a probability of 49.60% and 94.60%, respectively. Besides, our analysis shows that ARMM can avoid 100% of the perfect single-block attacks and has a lower limit probability of 33.50% in avoiding imperfect single-block attacks from adversary miners.},
  archive      = {J_TSC},
  author       = {Shunrong Jiang and Jingwei Chen and Jinpeng Li and Zhihui Liu and Yonggang Li},
  doi          = {10.1109/TSC.2024.3376195},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2632-2645},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {ARMM: Sandwich-attack resilient automated market maker},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective multi-scale contrastive learning system for online group recommendation services in event-based social networks. <em>TSC</em>, <em>18</em>(5), 2616-2631. (<a href='https://doi.org/10.1109/TSC.2025.3593346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On event-based social platforms such as Meetup and Douban, online groups serve as more than virtual communities for users to share experiences, they also provide an essential pathway for users to discover and participate in offline events. As the number of groups grows, it imposes the need of the study of online group recommendation. Despite there being many existing approaches to solve this problem, they all ignore the phenomenon that the groups that users participate in often contain a number of similar users. This phenomenon implies that similar users play a crucial role in identifying the groups that users are likely to join. In order to exploit similar users to improve the recommendation performance, we propose an effective multi-scale contrastive learning system for online Group Recommendation services, which is with a two-Tower model in event-based social networks (Tower4GR). Specifically, we first adopt the two-tower model to capture the interactive signals within the sequences and groups. We then incorporate the features of similar users into the sequence encoder, and aggregate the relevant users’ features into the group encoder, through which the preferred groups of similar users are more likely to be discovered by the target user. Finally, we propose an effective multi-scale contrastive learning framework for the two-tower architecture. It derives self-supervision signals from both same-scale data and cross-scale data, thereby extracting more meaningful data patterns. Moreover, the framework strengthens the cooperative associations between two towers. Extensive experiments on three real-world datasets from Meetup demonstrate the superiority of our proposed model over existing state-of-the-art models.},
  archive      = {J_TSC},
  author       = {Xiaomei Huang and Zhiheng Zhou and Jianjun Li and Neal N. Xiong and Yugen Yi and Jin Liu and Guoqiong Liao},
  doi          = {10.1109/TSC.2025.3593346},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2616-2631},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {An effective multi-scale contrastive learning system for online group recommendation services in event-based social networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate and interpretable log-based fault diagnosis using large language models. <em>TSC</em>, <em>18</em>(5), 2602-2615. (<a href='https://doi.org/10.1109/TSC.2025.3599494'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Log-based fault diagnosis is essential for ensuring system reliability and resilience. However, current methods only provide fault diagnosis results without explanations, which undermines their credibility. Large language models (LLMs) have extensive pre-trained knowledge and show potential in log analysis, yet they cannot be directly applied to log-based fault diagnosis due to limited specialized capabilities and domain-specific insights. Furthermore, LLMs have limitations in context length and are too diverse to select a suitable one. To address these issues, this paper presents LogInsight, a framework that enables accurate and interpretable log-based fault diagnosis using LLMs. We fine-tune a medium-sized, open-source LLM to incorporate domain expertise and leverage its interpretive capability. Additionally, we design a Fault-Oriented Log Summary (FOLS) module to extract essential information from log sequences, mitigating LLMs’ context length limitation. Extensive evaluations on two public datasets and a real-world production dataset demonstrate LogInsight’s superiority over state-of-the-art methods in both performance and interpretability.},
  archive      = {J_TSC},
  author       = {Yongqian Sun and Shiyu Ma and Tong Xiao and Yongxin Zhao and Xuhui Cai and Wei Dong and Yue Shen and Yao Zhao and Shenglin Zhang and Jing Han and Dan Pei},
  doi          = {10.1109/TSC.2025.3599494},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2602-2615},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Accurate and interpretable log-based fault diagnosis using large language models},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A utility-optimal reverse posted pricing mechanism for online mobile crowdsensing task allocation. <em>TSC</em>, <em>18</em>(5), 2588-2601. (<a href='https://doi.org/10.1109/TSC.2025.3592426'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to traditional mechanism design, the posted pricing mechanism can quickly determine the winning user and ensure the revenue of the seller through a predetermined price. Additionally, the posted pricing mechanism inherently possesses economic properties such as truthfulness and individual rationality. These properties make it an ideal method for solving online task allocation problems for mobile crowdsensing services (MCSs). The challenge in posted pricing mechanism design is being able to find reasonable posted prices under complex MCS task constraints. This article presents an innovative posted pricing mechanism to solve a general point of interest (POI)-based online MCS task allocation problem. We transform the problem into an integer programming model with the goal of maximizing the total utility of the system while satisfying various constraints. We prove that under any user arrival order, there must exist a posted price structure that can ensure that the total utility of the system is approximately optimal, with an approximation ratio of $1/(d+1)$ in the worst case. With the support of theoretical analysis, the posted price calculation can be completed using only a simple gradient descent algorithm. Compared with existing methods, our solution achieves very good results in terms of total utility and the task completion ratio, indicating that it can effectively improve the efficiency and service quality of MCSs.},
  archive      = {J_TSC},
  author       = {Jixian Zhang and Xuelin Yang and Peng Chen and Zhemin Wang and Weidong Li and Zhenli He and Keqin Li},
  doi          = {10.1109/TSC.2025.3592426},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2588-2601},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A utility-optimal reverse posted pricing mechanism for online mobile crowdsensing task allocation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A two-stage learning-driven many-objective memetic algorithm for solving the workflow scheduling problem in cloud environment. <em>TSC</em>, <em>18</em>(5), 2574-2587. (<a href='https://doi.org/10.1109/TSC.2025.3596977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing complex workflow application and computational resources requirement, distributed computing has attracted growing attention. Meanwhile, cloud computing has emerged as a prominent solution due to its elasticity, heterogeneity, and on-demand capabilities. However, data security and execution reliability in cloud are still urgent issues that need to be addressed. Based on the data encryption and task redundancy mechanism, this article presented a many-objective workflow scheduling problem (RSWSP) with the objectives of minimizing the execution time, cost, risk, and non-reliability. Then, a two-stage learning-driven many-objective memetic algorithm (TMMA) with tailored designs is introduced to address the RSWSP. First, several problem-specific heuristics are employed for cooperative initialization, generating a diverse set of initial solutions. Second, a two-stage global diversification approach is implemented to explore the problem space, which clusters the population into sub-populations and adoptive selects leader solutions based on the state of the population. In addition, a learning-driven local intensification strategy is incorporated for exploitation, encompassing six neighbor search operators and a Q-learning-based selection mechanism. Extensive experiments have been conducted to validate the performance of TMMA. The statistical comparison reveals that the TMMA is superior to state-of-the-art algorithms in solving the RSWSP in terms of solution quality and robustness.},
  archive      = {J_TSC},
  author       = {Shuo Qin and Dechang Pi and Zhongshi Shao},
  doi          = {10.1109/TSC.2025.3596977},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2574-2587},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A two-stage learning-driven many-objective memetic algorithm for solving the workflow scheduling problem in cloud environment},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A service-oriented optimization framework for edge caching with revenue maximization and QoS guarantees. <em>TSC</em>, <em>18</em>(5), 2559-2573. (<a href='https://doi.org/10.1109/TSC.2025.3596801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of mobile applications and data-intensive services, such as augmented reality and real-time analytics, necessitates efficient content delivery mechanisms in Mobile Edge Computing (MEC) environments. MEC enhances service responsiveness by caching content closer to end users; however, the constrained storage capacities of edge servers pose challenges in maintaining optimal Quality of Service (QoS). This paper presents a novel service-oriented content caching framework that optimizes resource allocation and revenue generation while ensuring QoS compliance. We introduce a dynamic fee-based pricing model that adapts service charges based on content retrieval latency, incentivizing improved service performance. The caching optimization problem is formulated as an Integer Linear Programming (ILP) model, and a computationally efficient approximation algorithm leveraging Linear Programming (LP) relaxation and rounding techniques is proposed to derive near-optimal solutions. Additionally, a resource expansion model is integrated to dynamically extend storage capacity in response to evolving service demands, ensuring scalable and cost-effective content provisioning. Extensive theoretical analysis and simulations validate the proposed approach, demonstrating a 22.4% increase in total service revenue and a 31.7% reduction in average content access delay compared to baseline strategies. This work contributes to services computing by providing a mathematically rigorous and computationally efficient framework for dynamic content management in edge networks.},
  archive      = {J_TSC},
  author       = {Chia-Cheng Hu and Jiao-Yan Zeng},
  doi          = {10.1109/TSC.2025.3596801},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2559-2573},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A service-oriented optimization framework for edge caching with revenue maximization and QoS guarantees},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reinforcement learning-based population hyper-heuristic for energy-efficient cloud workflow scheduling problem. <em>TSC</em>, <em>18</em>(5), 2545-2558. (<a href='https://doi.org/10.1109/TSC.2025.3589126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As cloud computing undergoes rapid development, cloud workflow scheduling has gained significant attention as a critical area of research. However, the substantial energy consumption of cloud data centers in handling diverse workflow applications highlights the need for effective energy optimization strategies to promote green computing. This research investigates an energy-efficient cloud workflow scheduling problem that explores the inherent relationship between scheduling schemes, host load, and energy consumption. To tackle this problem, a reinforcement learning-based population hyper-heuristic algorithm is proposed to provide an effective solution. Specifically, six low-level heuristics (LLHs) tailored to the characteristics of the problem are developed and refined. Meanwhile, Q-learning is employed as a high-level strategy for the intelligent selection of LLHs. Moreover, an adaptive local search strategy is introduced to improve inferior individuals. Additionally, an experience-driven task-resource mapping approach is employed to realize efficient resource allocation. To mitigate the challenge of infeasible solutions, a region-constrained insertion mechanism is devised to effectively minimize their occurrence. Subsequently, a solution feasibility reconstruction strategy is introduced to adjust and optimize any remaining infeasible solutions. Extensive experiments on various workflow applications of different scales and types demonstrate the superiority of the proposed algorithm over existing approaches in terms of energy optimization and exhibit significant practical application potential.},
  archive      = {J_TSC},
  author       = {Yuanzhuang Li and Yifan He and Jian Lin and Zhan Xu and Shiyu Zhang},
  doi          = {10.1109/TSC.2025.3589126},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2545-2558},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A reinforcement learning-based population hyper-heuristic for energy-efficient cloud workflow scheduling problem},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel reciprocal dual-channel preference extraction and refinement network for category-aware service recommendation. <em>TSC</em>, <em>18</em>(5), 2531-2544. (<a href='https://doi.org/10.1109/TSC.2025.3601040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommender systems (SRSs) aim to predict the subsequent content in which users may be interested based on their past usage history. Existing solutions on SRSs focus on modeling sequential characteristics of user-service interactions and achieve promising performance. However, they do not take full advantage of one key factor that usually influences user behaviors: the category of services. It is necessary yet challenging to leverage category information due to two significant reasons. First, bundling relationships exist between services/categories, which is vital for the prediction of user behaviors but hard to mine and encode. Second, since interest preferences and category preferences are closely related, their dynamic evolution has to be studied simultaneously. To tackle the above challenges, we propose a novel Dual-channel Preference Extraction and Refinement Network (DPERN) to extract users’ multi-faceted preferences toward more accurate recommendation. For the former challenge, we leverage the co-occurrence information of services and categories to represent their intrinsic relationships and then adopt the graph embedding method to jointly pre-train their embeddings. For the latter challenge, we design dual preference extractors, each leveraging both service and category information, to capture interest preferences and category preferences, respectively. Moreover, we devise a preference refinement network to model the interaction between two extracted preferences, to enhance preference representations. Experimental results on three public datasets have demonstrated the effectiveness of the proposed DPERN model.},
  archive      = {J_TSC},
  author       = {Shuxiang Xu and Qibu Xiang and Yushun Fan and Jia Zhang},
  doi          = {10.1109/TSC.2025.3601040},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2531-2544},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A novel reciprocal dual-channel preference extraction and refinement network for category-aware service recommendation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A heuristic big data processing multi task efficient deployment method based on QoS aware clustering and bayesian classification in cloud environment. <em>TSC</em>, <em>18</em>(5), 2517-2530. (<a href='https://doi.org/10.1109/TSC.2025.3592228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient deployment of Big Data processing tasks in cloud environments is the basic core function of Big Data processing, which refers to the effective deployment of tasks to the computing resources of cloud platforms, achieving high-performance and high-throughput data processing. In this process, task deployment needs to consider load balancing on the cloud platform to ensure that tasks can be evenly deployed to each computing node. However, currently in the process of providing services on cloud platforms, the available resources of all hosts will be automatically and dynamically readjusted, and it cannot be guaranteed that each task will be deployed to the host with the most remaining resources. This load imbalance in the platform will result in computational results that cannot be returned to users in a timely and effective manner. So, a heuristic multi-task efficient deployment approach for Big Data processing based on QoS awareness and Bayesian classification in cloud environments called QBC is proposed. The QBC first performs long-run QoS awareness on hosts in the cloud; Then, based on user task requirements, selects host nodes that meet QoS constraints to form a candidate set, and performs Bayesian classification to find the host node which has highest a posteriori probability to serve as the clustering center; third, designs an objective function based on euclidean spatial distance to acquire the optimum host clustering set in the candidate set; Finally, deploys the user’s tasks to this optimal host cluster set. The experimental results show that this approach implements optimization of long-run load balancing in Big Data cloud platforms with minimal resource consumption, enhances the ability of the cloud platform to provide external support, and thus promotes efficient deployment of multitasking in Big Data processing under cloud computing. The proposed QBC based framework reduces the overall Energy Consumption by an average of 47.98%, MakeSpan by an average of 24.42%, Total Cost by an average of 30.17%, Average Waiting Time by an average of 36.92%, and the Throughput is increased by an average of 41.93% as compared to the existing algorithms.},
  archive      = {J_TSC},
  author       = {ZiYuan Yu and Peng Yang and WanJing Wu and Liang Tan and DanLian Ye and Kun She},
  doi          = {10.1109/TSC.2025.3592228},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2517-2530},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A heuristic big data processing multi task efficient deployment method based on QoS aware clustering and bayesian classification in cloud environment},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A game-theoretic approach for microservice request dispatching in mobile edge computing systems. <em>TSC</em>, <em>18</em>(5), 2503-2516. (<a href='https://doi.org/10.1109/TSC.2025.3602905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of the mobile edge computing paradigm enables the deployment of microservices on edge servers, which greatly improves the quality of services and reduces network transmission costs. However, due to limited computing and storage resources, an individual edge server can host only a limited number of microservice instances. Moreover, user mobility often results in uneven distribution of service requests in mobile edge computing systems. To this end, it is a key problem to dispatch microservice requests to appropriate edge servers to minimize the average service response time. Current solutions to this problem rely on centralized methods and suffer from serious problems of single point of failure, error-proneness, difficult expansion, low robustness, etc. To resolve these problems, this paper proposes a decentralized game-theoretic approach for dispatching microservice requests effectively and efficiently in mobile edge computing systems. Specifically, we formulate the request dispatching problem as a decentralized non-cooperative game and propose a decentralized request dispatching algorithm that can find the Nash equilibrium through finite iterations. We conduct a series of experiments to demonstrate that our approach beats benchmarking approaches with close-to-optimal performance and high efficiency measured by convergence time.},
  archive      = {J_TSC},
  author       = {Hongyue Wu and Bowen Shi and Qiang He and Guangming Cui and Shizhan Chen and Zhiyong Feng and Albert Y. Zomaya and Shuiguang Deng},
  doi          = {10.1109/TSC.2025.3602905},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2503-2516},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A game-theoretic approach for microservice request dispatching in mobile edge computing systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A blockchain-based cross-domain data transmission scheme for industrial internet of things with edge-cloud computing. <em>TSC</em>, <em>18</em>(5), 2489-2502. (<a href='https://doi.org/10.1109/TSC.2025.3603155'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of the Industrial Internet of Things (IIoT) and widespread adoption of smart devices have profoundly reshaped traditional industrial production and management. Facilitated cross-domain data transmission between these devices has greatly boosted intelligence and efficiency in IIoT. Yet despite progress in cross-domain transmission, existing schemes still face severe challenges: complex hierarchical architectures, cross-domain trust issues, and high computational costs. To tackle these problems, we propose a blockchain-based cross-domain data transmission scheme for IIoT, integrated with edge-cloud computing. First, we enhance cross-domain transmission in edge-cloud environments by developing a lightweight blockchain-assisted framework, which cuts down redundant entity interactions. Second, we address inter-domain trust in IIoT by establishing trust relationships and computing relationship keys. Finally, we introduce a lightweight blockchain-based authentication and key agreement protocol to simplify cross-domain data transmission between smart devices. Security analysis shows the proposed scheme achieves strong security in the real-or-random model, effectively resisting various potential threats. Performance analysis and blockchain simulations further confirm its practical applicability for IIoT deployment.},
  archive      = {J_TSC},
  author       = {Xingxing Chen and Qingfeng Cheng and Xiaofeng Chen and Xiangyang Luo},
  doi          = {10.1109/TSC.2025.3603155},
  journal      = {IEEE Transactions on Services Computing},
  month        = {9-10},
  number       = {5},
  pages        = {2489-2502},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A blockchain-based cross-domain data transmission scheme for industrial internet of things with edge-cloud computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating IoT and 6G: Applications of edge intelligence, challenges, and future directions. <em>TSC</em>, <em>18</em>(4), 2471-2488. (<a href='https://doi.org/10.1109/TSC.2025.3586152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge intelligence (EI) entails deploying artificial intelligence algorithms at the network’s edge. Utilizing edge computing infrastructure enables local data processing and decision-making, resulting in decreased latency, bandwidth consumption, and improved privacy security. With increasing the number of Internet of Things (IoT) devices and the development of 6G communication technologies, there is a growing demand for fast, efficient, and low-latency data processing, which has led to the rise of EI. This survey comprehensively reviews and analyzes the current state of research on EI from the perspective of technological development, with a particular focus on the following key aspects: (1) We review the basic concepts of EI and its distinctions from traditional cloud computing and edge computing; (2) We explore the technological framework of EI in detail, including key technologies such as edge computing and federated learning, and analyze how these technologies integrate with modern communication technologies like IoT devices and 6G networks; (3) We discuss the challenges faced when implementing EI technologies, such as data privacy and security issues, device resource constraints, and propose corresponding solutions or research directions. The survey also outlines the main research directions and technical challenges driving the future development of EI, providing valuable insights and guidance for researchers and practitioners in the field.},
  archive      = {J_TSC},
  author       = {Qiang He and Jinqiu Lin and Hui Fang and Xingwei Wang and Min Huang and Xiushuang Yi and Keping Yu},
  doi          = {10.1109/TSC.2025.3586152},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2471-2488},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Integrating IoT and 6G: Applications of edge intelligence, challenges, and future directions},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unlocking complexity: Harnessing value entropy for advanced multidimensional utility evaluation in service ecosystems. <em>TSC</em>, <em>18</em>(4), 2456-2470. (<a href='https://doi.org/10.1109/TSC.2025.3577481'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of smart services in daily life drives the rapid emergence of service ecosystems across various domains such as E-commerce, cloud manufacturing, and crowdsourcing. Evaluating the utility of these ecosystems is challenging due to complex characteristics like diverse crowd intelligence, cascading service network effects, and the interplay of individual interests. To address these challenges, this study introduces an innovative utility evaluation model that integrates individual and systemic factors with multidimensional metrics, reconciling micro and macro perspectives. This model effectively addresses potential conflicts between individual and systemic benefits, supporting the continuous learning and evolution of agents within service ecosystems. It employs value entropy to precisely model and interpret complex nonlinear emergence phenomena. The model’s universal framework offers high customizability and broad applicability, facilitating specific adaptations across different service ecosystem scenarios. Additionally, a visual multi-agent system simulation tool has been developed to adjust agent attributes and cooperative topologies, allowing for the observation of system responses. Empirical validation confirms the model’s accuracy and provides a mechanistic explanation for emergence phenomena in human societies. The findings demonstrate the model’s high coordination and effectiveness in managing both linear and nonlinear characteristics, presenting a powerful and flexible tool for utility evaluation in service ecosystems..},
  archive      = {J_TSC},
  author       = {Xiangning Yu and Xiao Xue and Deyu Zhou and Gang Wang and Zhiyong Feng},
  doi          = {10.1109/TSC.2025.3577481},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2456-2470},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Unlocking complexity: Harnessing value entropy for advanced multidimensional utility evaluation in service ecosystems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards cost-optimal policies for DAGs to utilize IaaS clouds with online learning. <em>TSC</em>, <em>18</em>(4), 2439-2455. (<a href='https://doi.org/10.1109/TSC.2025.3536305'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Premier cloud service providers (CSPs) offer on-demand and spot instances (i.e., virtual machines) with time-varying features in availability and price. While interacting with a CSP, what concerns users is the process of cost-effectively utilizing these instances, possibly in addition to self-owned instances. A job in data-intensive applications is represented by a directed acyclic graph (DAG) whose nodes represent tasks and the DAG can further be transformed into a chain of tasks. The key to achieving cost efficiency is determining the allocation of a specific deadline to each task, as well as the allocation of different types of instances to the task. In this paper, we make some mild assumptions on the usage of various instances and give an analytical understanding of the expected behaviors while utilizing instances. Based on this, we propose informed heuristic policies to determine the allocation of deadlines and instances. The policies are parametric to support the usage of online learning to infer the optimal values against the dynamics of cloud markets. Finally, intuitive greedy policies are used as baselines to validate the effectiveness of the proposed analytical solutions. The cost improvement is up to 23.58% when spot and on-demand instances are considered and up to 50.08% when self-owned instances are also considered.},
  archive      = {J_TSC},
  author       = {Xiaohu Wu and Han Yu and Giuliano Casale and Guanyu Gao},
  doi          = {10.1109/TSC.2025.3536305},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2439-2455},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Towards cost-optimal policies for DAGs to utilize IaaS clouds with online learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthesis rather than redemption: A win-win rewards program by self-assembling fragments into an item in the metaverse. <em>TSC</em>, <em>18</em>(4), 2427-2438. (<a href='https://doi.org/10.1109/TSC.2025.3583105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern business activities often rely on rewards programs as a common means of incentivizing consumers. However, in certain cases, it can be a lose-lose situation for both consumers and businesses. For consumers, operators may alter or terminate rewards programs. For operators, rewards programs are difficult to be trusted by consumers and can come with many potential costs. The metaverse is perceived as a utopian-style virtual social system, where brands or individuals, called as operators, can establish stores/stops to conduct business activities. That is, there are also rewards programs in the metaverse, and if people simply replicate existing programs, it may lead to the situations mentioned above. To this end, we aim to propose a win-win rewards program in the metaverse to alleviate the situation. It suggests that consumers can obtain items in rewards programs through self-assembly rather than through the redeem controlled by the operator. Operators cannot make modifications to the rewards program once it is launched. Therefore, this rewards program is trustworthy, and operators also divest themselves from the redemption process in the rewards program, thereby reducing many additional costs. Meanwhile, we implement a specific prototype from a technical perspective to match the proposed rewards program in the metaverse. It specifically implements the decomposition of items prepared by the operator into fragments. Once consumers obtain a sufficient number of fragments, they can assemble them into a complete item without the involvement of the operator.},
  archive      = {J_TSC},
  author       = {Ruoyu Zhao and Moting Su and Wenying Wen and Fengshu Li and Yushu Zhang},
  doi          = {10.1109/TSC.2025.3583105},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2427-2438},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Synthesis rather than redemption: A win-win rewards program by self-assembling fragments into an item in the metaverse},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stabilizing quality of wi-fi-based location services using high-performance distributed stream processing and data pipelines. <em>TSC</em>, <em>18</em>(4), 2417-2426. (<a href='https://doi.org/10.1109/TSC.2025.3586154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location-based Systems (LBS) are popular for delivering customized information. However, some issues, such as place credibility, the efficiency of position calculations, and communication latency, pose challenges for indoor LBS. This work proposes the High-performance Perspective Platform (H3P) to help network managers understand network users’ information. The H3P provides indoor positioning services based on Wi-Fi 6 (IEEE 802.11ax) to stabilize service quality and ensure high computation efficiency for rapid service response. It utilizes Apache Kafka and Apache Zookeeper clusters on Kubernetes to handle large amounts of data. Wi-Fi usage data is transmitted to Kafka’s distributed real-time data streaming to enhance position credibility and the immediacy of position calculations. The data structure is also optimized to improve computation efficiency. Experimental results show that H3P improves data latency by up to 69% and data insertion latency by up to 73%. Additionally, H3P offers more stability and efficiency than Chang et al., 2012 in terms of data transmission, with an improvement of approximately 16.15%. This allows administrators to manage the network with user-friendly interfaces and a smooth user experience.},
  archive      = {J_TSC},
  author       = {Chen-Kun Tsung and Ching-Hsien Hsu and Jung-Chun Liu and Nguyen Gia Nhu and Chun Hsiung and Xin-Ting Zhang and Chao-Tung Yang},
  doi          = {10.1109/TSC.2025.3586154},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2417-2426},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Stabilizing quality of wi-fi-based location services using high-performance distributed stream processing and data pipelines},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sparse-FCL: Sparse federated continual learning for evolving mobile edge computing environments. <em>TSC</em>, <em>18</em>(4), 2403-2416. (<a href='https://doi.org/10.1109/TSC.2025.3583174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) leverages embedded devices to deliver low-latency and cost-effective intelligent services, such as autonomous driving. Federated learning allows MEC devices to collaboratively learn a global model without exposing private data. However, the dynamic real-world environments that MEC devices operate in can lead to catastrophic forgetting of previously learned knowledge. While continual learning has been applied in federated learning to retain knowledge about past data, there are significant barriers when extending it for evolving MEC environments due to unbearable communication and computation costs for resource-constrained devices. We address this by proposing Sparse-FCL, a federated continual learning framework that uses sparse training to reduce training overhead and improve model performance. Specifically, to retain the generalization knowledge of specific tasks, we introduce a progressive neuron selection via multi-device collaboration module, which gradually selects the key neurons that show importance on multiple devices in each continual learning task. In addition, we propose a task-adaptive topology exploration module, aiming to provide more pliable dynamic sparse training configurations for the scenarios of federated continual learning. Experiments on FCL benchmarks demonstrate Sparse-FCL’s superior accuracy under high sparsity levels, and it also achieves reductions of 80.4%, 89.5%, and 19.7% in communication, computation, and storage overhead, respectively, compared to existing federated continual learning methods.},
  archive      = {J_TSC},
  author       = {Yan Liu and Feiyang Liu and Bin Guo and Zhouyangzi Zhang and Ruonan Xu and Zhiwen Yu},
  doi          = {10.1109/TSC.2025.3583174},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2403-2416},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Sparse-FCL: Sparse federated continual learning for evolving mobile edge computing environments},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLICE: Energy-efficient satellite-ground co-inference via layer-wise scheduling optimization. <em>TSC</em>, <em>18</em>(4), 2388-2402. (<a href='https://doi.org/10.1109/TSC.2025.3577451'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Low Earth Orbit (LEO) satellites are facilitating the provision of Deep Neural Networks (DNNs)-inherent services to achieve ubiquitous coverage via satellite computing. However, the computational demands and energy consumption of DNN models present significant challenges for satellite computing with limited power and computation resources. Based on the layered characteristics of DNN models, a satellite-ground co-inference strategy has been introduced, which executes certain layers on satellites and the remaining layers on ground servers. Determining the optimal layers for in-orbit processing, however, is non-trivial due to the under-explored energy consumption of satellite computing across different models and restricted yet varying communication conditions of satellite-ground links. In this article, we first conduct a comprehensive measurement to uncover energy consumption of satellite computing across different layers and models. By summarizing the key observations, we develop a layer-specific energy consumption model tailored to diverse DNN architectures and kernels. We then investigate the energy-efficient satellite-ground co-inference problem and formulate it as an integer-nonlinear programming problem, which presents high computational complexity. To tackle these difficulties, we propose a satellite-ground co-inference algorithm that employs a branch-and-bound strategy, combined with the Sobol sequence and Lagrange multiplier, to reduce complexity and ensure stability across diverse DNN architectures. To evaluate the proposed algorithm, we conduct experiments based on real-world satellite parameters. The results demonstrate that our proposed algorithm can achieve an average energy savings of 96% under various data volumes compared to the existing benchmarks.},
  archive      = {J_TSC},
  author       = {Yijie Chen and Qiyang Zhang and Ruolin Xing and Yuanzhe Li and Xiao Ma and Yiran Zhang and Ao Zhou and Shangguang Wang},
  doi          = {10.1109/TSC.2025.3577451},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2388-2402},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {SLICE: Energy-efficient satellite-ground co-inference via layer-wise scheduling optimization},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust dynamic edge service placement under spatio-temporal correlated demand uncertainty. <em>TSC</em>, <em>18</em>(4), 2372-2387. (<a href='https://doi.org/10.1109/TSC.2025.3570873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing enables Service Providers (SPs) to enhance user experience by placing services closer to the network edge. However, cost-effectively provisioning edge resources to meet uncertain and varying demand is a critical challenge. This paper introduces a novel two-stage, multi-period robust optimization model for edge service placement and workload allocation, aiming to minimize SPs’ operating costs while ensuring service quality. The salient feature of this model is its ability to leverage dynamic service placement and spatio-temporal correlations in demand uncertainties to mitigate the conservatism of traditional robust approaches optimized for worst-case scenarios. In our model, resource reservation is determined preemptively in the first stage, while dynamic service placement and workload allocation are adaptively optimized in the second stage after uncertainties are revealed. To address the computational challenges posed by integer recourse variables in the resulting tri-level adjustable robust optimization problem, we develop a novel iterative decomposition-based approach with guaranteed finite convergence to an exact optimal solution. Extensive numerical results validate the efficacy of the proposed model and approach.},
  archive      = {J_TSC},
  author       = {Jiaming Cheng and Duong Thuy Anh Nguyen and Duong Tung Nguyen},
  doi          = {10.1109/TSC.2025.3570873},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2372-2387},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Robust dynamic edge service placement under spatio-temporal correlated demand uncertainty},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RECaching: Cost-effective edge caching for cloud storage with differentiated regional workloads. <em>TSC</em>, <em>18</em>(4), 2358-2371. (<a href='https://doi.org/10.1109/TSC.2025.3576647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For cloud storage, edge caching not only reduces the latency of delivering data to requesters due to closer delivery distances, but also potentially brings cost-savings to cloud users due to cheaper edge resources. However, cost-effective caching requires the exact knowledge of future requests, which is hard for cloud users to obtain in advance. It raises a risk of incurring more costs due to unpopular redundant replicas if data is blindly cached at the edge. To overcome this challenge, in this paper, we propose an online cost-effective caching algorithm based on reinforcement learning to dynamically make decisions of edge caching and caching lifespan online without any knowledge of the future. Further, we propose several mechanisms to enhance the cost performance of the proposed algorithm, to make up for possible mistakes made at the beginning of the learning and avoid cost-ineffective periods in caching lifespans. We then analyze the performance of caching decisions given by the proposed algorithm with enhancements. Finally, we conduct extensive simulations driven by real-world traces under prevalent pricing schemes of both the cloud and the edge, which reveals that significant cost-savings and superiority over benchmark algorithms can be achieved.},
  archive      = {J_TSC},
  author       = {Mingyu Liu and Li Pan and Shijun Liu},
  doi          = {10.1109/TSC.2025.3576647},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2358-2371},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {RECaching: Cost-effective edge caching for cloud storage with differentiated regional workloads},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality of experience in crowdsourced energy services. <em>TSC</em>, <em>18</em>(4), 2342-2357. (<a href='https://doi.org/10.1109/TSC.2025.3576997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel Quality of Experience (QoE) metric as a key criterion for optimizing the composition of energy services within a crowdsourced IoT environment. Two novel composition approaches, namely, Importance-based and Heuristic-based, are proposed to ensure the highest QoE for consumers. The Importance-based approach prioritizes time slots based on their significance. The Heuristic-based approach considers the importance of time slots and the availability of services to maximize QoE while minimizing service provisioning costs. We conduct extensive experiments using real-world datasets to evaluate the effectiveness and efficiency of the proposed approaches. The results demonstrate that both approaches enhance consumer satisfaction by optimizing energy allocation, with the Heuristic-based approach outperforming the Importance-based method in minimizing rewards.},
  archive      = {J_TSC},
  author       = {Amani Abusafia and Athman Bouguettaya and Abdallah Lakhdari},
  doi          = {10.1109/TSC.2025.3576997},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2342-2357},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Quality of experience in crowdsourced energy services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QLP-DCS: A quality-aware, low-cost, and privacy-preserving data collection service for mobile crowd sensing. <em>TSC</em>, <em>18</em>(4), 2326-2341. (<a href='https://doi.org/10.1109/TSC.2025.3565374'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the service of Mobile Crowd Sensing (MCS), High-quality Data Collection (HDC), Bilateral Location Privacy Preservation (BLPP), and sensing cost are three pivotal issues. It is widely believed that HDC necessitates the recruitment of workers with high Quality of Service (QoS), which is related to the sensing data capabilities of the recruited workers and the worker-task distances. However, submitting high-quality data demands more resources from the workers, incurring higher costs. Meanwhile, BLPP techniques, aiming to conceal the locations of the workers and tasks, may impede the evaluation of the workers’ QoS. Therefore, there is still a lack of a low-cost and BLPP high QoS data collection research. Motivated by this, we propose a Quality-Aware, Low-Cost, and Privacy-Preserving Data Collection Service (QLP-DCS) for MCS. First, we propose a matrix perturbation-based approach to achieve BLPP while preserving the partial order relationship of distances. Subsequently, we employ the Upper Confidence Bound indexes-based reverse auction recruiting workers to balance exploration and exploitation with the low sensing cost. Then, we propose a multi-level truth discovery approach and establish an effective trust verification mechanism. Theoretical analysis and extensive experiments validate the superior performance of our QLP-DCS.},
  archive      = {J_TSC},
  author       = {Yajiang Huang and Jialin Guo and Shihao Yang and Jiali Liu and Anfeng Liu and Jianheng Tang and Tian Wang and Mianxiong Dong and Houbing Song},
  doi          = {10.1109/TSC.2025.3565374},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2326-2341},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {QLP-DCS: A quality-aware, low-cost, and privacy-preserving data collection service for mobile crowd sensing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving image retrieval in cloud computing via adaptive secret keys and self-supervised block-augmented pretraining. <em>TSC</em>, <em>18</em>(4), 2310-2325. (<a href='https://doi.org/10.1109/TSC.2025.3576692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-Preserving Image Retrieval (PPIR) enables searching for similar images on cloud servers while safeguarding image privacy. PPIR uploads encrypted images to servers to address privacy concerns and then employs deep neural networks for retrieval on extracted features from cipher-images. However, current PPIR encrypts all images with the same secret keys to maintain consistent feature spaces, lacking support for adaptive keys where distinct images are encrypted with various keys. To this end, we propose a new PPIR scheme to support adaptive keys while keeping stable feature spaces. Specifically, we design ingenious image encryption to align with feature extraction during the JPEG compression process, incorporating encryptable orthogonal transformations, shuffling, stream cipher, and sign encryption operations. Our approach extracts well-designed absolute value sequences of local blocks and global histogram features from cipher-images, ensuring stable feature spaces under adaptive keys. To enhance model generalization performance, we employ Self-Supervised Contrastive Learning (SSCL) to build a pretraining model and propose a straightforward yet efficient block-sampling augmentation technique for the structured features to drive SSCL. Moreover, our retrieval model implements a dual-attention structure to capture dependencies among local block sequences and import scores of global features. Extensive experiments on four datasets demonstrate that our approach achieves superior retrieval accuracy compared to existing schemes and maintains excellent retrieval performance under adaptive keys, effectively preserving image privacy.},
  archive      = {J_TSC},
  author       = {Qihua Feng and Zhixun Lu and Litian Zhang and Chaozhuo Li and Feiran Huang and Jian Weng and Philip S. Yu},
  doi          = {10.1109/TSC.2025.3576692},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2310-2325},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Privacy-preserving image retrieval in cloud computing via adaptive secret keys and self-supervised block-augmented pretraining},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing profit and delay in computing power network via deep deterministic policy gradient: A task decomposition and computing path optimization approach. <em>TSC</em>, <em>18</em>(4), 2295-2309. (<a href='https://doi.org/10.1109/TSC.2025.3570862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the contemporary landscape of computationally intensive applications, Computing Power Network (CPN) offers a solution to enhance computational efficiency and cost-effectiveness by integrating and sharing computing resources. However, with the surge in task volume within multi-user environments, effectively scheduling these tasks to optimize system profit and delay presents a significant challenge. This article introduces an optimization approach leveraging Deep Deterministic Policy Gradient (DDPG) to enhance CPN performance through task decomposition and computing path optimization. We initially construct a multi-layer CPN system model encompassing cloud computing, edge computing, and terminal device layers. Subsequently, we integrate a novel mechanism for convex optimization-based task decomposition, enabling intelligent subdivision of tasks into sub-tasks and dynamic allocation to suitable nodes within the network. Furthermore, we devise a Convex Optimization Task Decomposition-based Multi-Agent Deep Deterministic Policy Gradient (CO-MADDPG) algorithm, empowering multiple computing tasks as independent agents to learn and identify optimal offloading paths and computing nodes, thereby minimizing delay and maximizing system profit. A series of simulation experiments validate the effectiveness of the CO-MADDPG algorithm in handling concurrent tasks, demonstrating its capability to reduce task completion times, enhance system revenue, and maintain adaptability and stability across varying task demands.},
  archive      = {J_TSC},
  author       = {Bo Ma and Xiaosen Hu and Yexin Pan and Qin Lu and Chuanhuang Li},
  doi          = {10.1109/TSC.2025.3570862},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2295-2309},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Optimizing profit and delay in computing power network via deep deterministic policy gradient: A task decomposition and computing path optimization approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing consistency in distributed data services: The CP-raft protocol for high-performance and fault-tolerant replication. <em>TSC</em>, <em>18</em>(4), 2281-2294. (<a href='https://doi.org/10.1109/TSC.2025.3577505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raft is a popular consistency protocol in distributed data services due to its ease of implementation. However, log entries in Raft cannot be outof-order processed even if they do not have dependencies, which limits the system performance in high-concurrency scenarios. To address this challenge, researchers propose out-of-order Raft (OORaft) protocols that support out-of-order apply (OOApply). Existing OORaft protocols adopt the Paxos-style election and replication process to merge missing entries on leader candidates. It leads to problems such as extra overhead on dependency analysis, availability when the network is partitioned, and incomplete verification. This paper proposes a concise paralleled Raft called CPRaft, which is the first OORaft protocol to focus on dependency analysis overhead and to use full TLA+ validation for the leader election process. Specifically, 1) CP-Raft proposes a Raft-aligned three-step election method that simplifies the understanding difficulty and solves the availability problem when the network is partitioned. 2) CP-Raft applies an efficient leader-side bitmap-based dependency analysis and representation method to break through the performance bottleneck caused by dependency analysis costs. 3) CP-Raft discusses why existing methods cannot achieve OORaft correctness verification using TLA+ in a limited time and uses phased verification methods to ensure its correctness. Finally, we implement CP-Raft based on an open-source Raft protocol and discuss its potential performance bottlenecks in various scenarios. The experimental results under high dependency strength workloads demonstrate that CP Raft achieves 1.5 transaction per second (TPS) performance of DP-Raft and 2 of ParallelRaft-CE. It also provides better availability than state-of-the-art OORaft protocols.},
  archive      = {J_TSC},
  author       = {Haiwen Du and Kai Wang and Yulei Wu and Hongke Zhang},
  doi          = {10.1109/TSC.2025.3577505},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2281-2294},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Optimizing consistency in distributed data services: The CP-raft protocol for high-performance and fault-tolerant replication},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online workload scheduling for social welfare maximization in the computing continuum. <em>TSC</em>, <em>18</em>(4), 2267-2280. (<a href='https://doi.org/10.1109/TSC.2025.3570845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing ecosystems are shifting toward a computing continuum paradigm designed to handle the diverse and dynamic nature of computing resources spread across various locations. It demonstrates significant potential in providing high-bandwidth and low-latency services for users. However, as a large number of users request services from distributed computing continuum systems, it is critical to schedule numerous delay-sensitive, fractional workloads and maximum parallelism-bound jobs to appropriate backend resources, e.g., cloud container instances. In addition, the scheduling strategy also needs to maximize the social welfare that incorporates the utilities of jobs and the revenue of service providers. However, current workload scheduling algorithms are based on simple heuristics and lack performance guarantees. Due to the unpredictability of online requests, the distribution of requests should not be assumed. Therefore, designing an online workload scheduling strategy without assumptions on request distributions is essential for balancing the online workload. This work first establishes a spatiotemporal integrated resource pool to reflect the computational resources provided by distributed computing continuum systems. Then, several pseudo-social welfare functions and marginal cost functions are constructed, where the latter is used to estimate the marginal cost of provisioning services to each newly arrived job based on the current resource surplus. We propose an online workload scheduling strategy named OnSocMax to solve the above problems. It operates by following the solutions to several convex pseudo-social welfare maximization problems and is proven to be $\alpha$-competitive for some $\alpha$ with a value of at least 2. The evaluation results demonstrate that OnSocMax outperforms several benchmark strategies in maximizing social welfare.},
  archive      = {J_TSC},
  author       = {Hailiang Zhao and Ziqi Wang and Guanjie Cheng and Wenzhuo Qian and Peng Chen and Jianwei Yin and Schahram Dustdar and Shuiguang Deng},
  doi          = {10.1109/TSC.2025.3570845},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2267-2280},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Online workload scheduling for social welfare maximization in the computing continuum},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online popularity prediction service via minimal substitution reinforcement learning for social networks. <em>TSC</em>, <em>18</em>(4), 2254-2266. (<a href='https://doi.org/10.1109/TSC.2025.3586094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the key challenges of current online social platforms is predicting the size of information cascades, also known as popularity prediction or cascade prediction. Accurate popularity prediction can benefit various fields, including news distribution, market decisions, and rumor detection. However, existing popularity prediction approaches concentrate more on the historical sequences of single messages, overlooking the interactions between message diffusion and the dynamic nature of social networks, which limits the timeliness and accuracy of predictions. To address this, we propose an online popularity prediction service based on minimal substitution reinforcement learning called MSRL. Specifically, we explore a substitution theory and design a minimal substitution reinforcement learning method that models diffusion as message substitution and considers mutual information diffusion. That helps the model gain a broader perspective, allowing it to fully exploit the cooperative, competitive, or dependent relationships between information diffusions. Furthermore, the reinforcement learning scheme enables the service to dynamically adjust its parameters to respond to the dynamic social network environment in real-time. Finally, extensive experiments on real-world datasets show that the MSRL outperforms state-of-the-art methods regarding accuracy and service agility.},
  archive      = {J_TSC},
  author       = {Ranran Wang and Yin Zhang and Henning Meyerhenke and Zhiliang Feng and Sabita Maharjan and Yan Zhang},
  doi          = {10.1109/TSC.2025.3586094},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2254-2266},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Online popularity prediction service via minimal substitution reinforcement learning for social networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On-demand and scalable topology control service for LEO satellite network evolving. <em>TSC</em>, <em>18</em>(4), 2238-2253. (<a href='https://doi.org/10.1109/TSC.2025.3576690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inter-Satellite Links (ISLs) are pivotal for delivering global connectivity services and optimizing resource utilization in 6G and beyond. However, delivering effective topology control services through ISL provisioning faces critical challenges in sustainability and reliability. Reducing ISLs can conserve energy and extend satellite battery life for Low-Earth-Orbit (LEO) satellites where replacing batteries is impractical. Conversely, increasing ISLs can enhance service reliability but may lead to uneven traffic distribution, overloading nodes, and accelerating battery degradation, ultimately degrading the quality of 6G services. To tackle this dilemma, we propose TASRI—a service-oriented framework for Traffic-Aware, Sustainable, and Reliable ISL provisioning. TASRI provides a dynamic topology control service by partitioning network topologies into logical zones, enabling flexible ISL activation and deactivation to adapt to varying service demands, ensuring efficient resource utilization and dynamic service orchestration. Using a sustainability-oriented weight model, we formulate the topology control service optimization problem and introduce a scalable on-demand topology evolving algorithm with a bounded approximation ratio. Extensive real-world deployment-based simulation results show that, compared to the state-of-the-art, our TASRI can substantially reduce battery life consumption, while achieving comparable reliability and excellent scalability with considerably fewer ISLs or ISL handovers.},
  archive      = {J_TSC},
  author       = {Long Chen and Yi Ching Chou and Haoyuan Zhao and Hengzhi Wang and Feng Wang and Hao Fang and Sami Ma and Feilong Tang and Linghe Kong and Jiangchuan Liu},
  doi          = {10.1109/TSC.2025.3576690},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2238-2253},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {On-demand and scalable topology control service for LEO satellite network evolving},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view trace clustering based on graph convolutional networks in process mining. <em>TSC</em>, <em>18</em>(4), 2226-2237. (<a href='https://doi.org/10.1109/TSC.2025.3577480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining techniques can extract process models from event logs produced by information systems. However, in flexible environments, simply using existing methods often leads to complex process models that are hard to understand, due to the less structured and greater complexity of processes in real life. Trace clustering is a pre-processing technique that enhances the effectiveness of model mining by partitioning similar behaviors in logs. In this article, we present a Multi-view Trace Clustering method, named MTC, that improves the homogeneity of trace subclusters. Our method consists of three parts: (1) We use trace profiles to depict the traces from different views, and each profile would be transformed into a graph based on the k-nearest neighbor algorithm; (2) A fusion graph is designed to capture the information among these graphs based on an attention coefficient matrix, and then the graph convolutional networks are used to encode all graphs for obtaining the common representation; (3) We also enhance the characterization of the common representation with an inner decoder. Finally, we adopt k-means to cluster the traces in the log based on the common representation. Extensive experiments using multiple datasets illustrate that MTC significantly surpasses state-of-the-art trace clustering methods.},
  archive      = {J_TSC},
  author       = {Leilei Lin and Yunuo Cao and Wenlong Chen and Zan Zong and Chen Qian and Lijie Wen},
  doi          = {10.1109/TSC.2025.3577480},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2226-2237},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Multi-view trace clustering based on graph convolutional networks in process mining},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent reinforcement learning for freshness-aware data sensing model in vehicular crowdsensing systems. <em>TSC</em>, <em>18</em>(4), 2212-2225. (<a href='https://doi.org/10.1109/TSC.2025.3583250'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular Crowdsensing (VCS) is a promising paradigm for supporting urban sensing services, where Service Providers (SPs) engage Mobile Vehicles (MVs) to perform data sensing tasks with specific objectives. However, existing studies have predominantly focused on data sensing quality in terms of data collection completeness and geographic fairness, while largely neglecting the important aspect of data freshness. Moreover, effective mechanisms for optimizing data freshness through coordination of the behaviors of both SPs and MVs are still lacking. Accordingly, this paper proposes a Freshness-Aware Data Sensing (FDS) model by considering heterogeneous data freshness, varying sensing capabilities of MVs, and limited budgets of SPs. The FDS is formulated as a two-stage game model, where SPs and MVs iteratively determine their pricing and sensing strategies in a self-interested manner to maximize their individual gains. Further, we develop a multi-agent reinforcement learning-based approach to learn the pricing strategies based on historical observations, which allows SPs to make pricing decisions without global knowledge. Additionally, given the pricing strategies of SPs, the optimal solution for each MV is derived. Finally, we build the simulation model based on realistic vehicular traces, where the simulation results demonstrate the superiority of the proposed algorithm in various scenarios.},
  archive      = {J_TSC},
  author       = {Penglin Dai and Xin Wang and Yue Xiang and Xiao Wu and Junhua Wang and Kai Liu},
  doi          = {10.1109/TSC.2025.3583250},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2212-2225},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Multi-agent reinforcement learning for freshness-aware data sensing model in vehicular crowdsensing systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear structure analysis of embeddings for bias disparity reduction in collaborative filtering. <em>TSC</em>, <em>18</em>(4), 2201-2211. (<a href='https://doi.org/10.1109/TSC.2025.3570852'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems personalize user experiences by filtering large volumes of information, and further shape user behavior. Collaborative filtering (CF), a widely used algorithm in this domain, learns user preferences from user-item interactions. However, recent studies indicate that certain CF algorithms can excessively amplify inherent data biases, manifesting as bias disparity. In this study, we examine the latent factor model (LFM), a state-of-the-art CF method that represents users and items as vectors in a shared latent space. By applying linear dimensionality reduction techniques with strong interpretability (such as principal component analysis) to LFM embeddings trained on real-world data, we identify specific axes that encode biases. We then demonstrate the application of these linear relationships in mitigating bias disparity. Experimental results show that our proposed approach can reduce bias disparity with only a slight decrease in recommendation accuracy—an average of $5-8\%$ with principal component analysis and $1-3\%$ with independent component analysis.},
  archive      = {J_TSC},
  author       = {Hiroki Okamura and Keisuke Maeda and Ren Togo and Takahiro Ogawa and Miki Haseyama},
  doi          = {10.1109/TSC.2025.3570852},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2201-2211},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Linear structure analysis of embeddings for bias disparity reduction in collaborative filtering},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Knowledge transfer service for multi-granular traffic prediction in smart cities. <em>TSC</em>, <em>18</em>(4), 2188-2200. (<a href='https://doi.org/10.1109/TSC.2025.3583232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is essential for the efficient resource allocation and management of smart cities. However, many cities face challenges in accessing sufficient traffic data due to high collection costs and privacy concerns, which impacts the accuracy of predictions. To address the issue of data sparsity, transfer learning methods have been explored with promising outcomes. Nonetheless, key challenges remain in determining which knowledge to transfer and how to transfer it effectively. In this paper, we propose a Knowledge Transfer Service for multi-granular traffic prediction in smart cities, leveraging Multi-Granular Spatiotemporal Pattern Transfer Learning (MGSTPAT). MGSTPAT utilizes meta-learning to acquire rich, multi-granularity spatiotemporal knowledge from data-abundant source cities and initializes robust models. Additionally, our Embedded Adaptive Clustering module generates task-specific multi-granularity patterns, while the Granularity-Aware Pattern Attention mechanism enables target cities with limited data to select the most relevant patterns, enhancing prediction accuracy. Extensive experiments on real-world datasets demonstrate the effectiveness of our approach in addressing traffic prediction challenges across cities.},
  archive      = {J_TSC},
  author       = {Jiqian Mo and Junyang Chen and Zhiguo Gong},
  doi          = {10.1109/TSC.2025.3583232},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2188-2200},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Knowledge transfer service for multi-granular traffic prediction in smart cities},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KeAD: Knowledge-enhanced graph attention network for accurate anomaly detection. <em>TSC</em>, <em>18</em>(4), 2172-2187. (<a href='https://doi.org/10.1109/TSC.2025.3583241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anomaly detection has emerged as one of the core research topics to support workflow applications across domains. To differentiate anomalies from normal patterns of workflows, Graph Neural Networks (GNNs) models have been introduced. These models leverage time series data to construct graph structures, in order to explicitly capture task dependencies among industrial Internet of Things (IoT) devices, and thus, to identify deviations from predicted behaviours as anomalies. However, existing forecasting-based anomaly detection methods may not accurately detect certain anomalies, as they rely solely on historical sensory data while seldom considering the valuable information embedded in domain knowledge. To address this limitation, this paper proposes a Knowledge-enhanced graph attention-based Anomaly Detection (KeAD) method. Specifically, a knowledge-enhanced graph structure is constructed by incorporating domain-specific knowledge to represent spatio-temporal dependencies between IoT devices. Based on which, a knowledge-enhanced graph attention-based forecasting network is developed to predict the future behaviours of IoT devices. Anomalies, such as those caused by cyber-attacks in workflows, are detected by analyzing deviations from these predicted behaviours in conjunction with domain-specific knowledge. A case study is presented, along with extensive experiments conducted on publicly available datasets. Evaluation results demonstrate that KeAD outperforms the state-of-the-art techniques in terms of anomaly detection accuracy.},
  archive      = {J_TSC},
  author       = {Yi Li and Zhangbing Zhou and Pu Sun and Shuiguang Deng and Xiao Sun and Xiao Xue and Sami Yangui and Walid Gaaloul},
  doi          = {10.1109/TSC.2025.3583241},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2172-2187},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {KeAD: Knowledge-enhanced graph attention network for accurate anomaly detection},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint time-frequency pseudo anomalies for multimodal electrocardiogram quality assessment in healthcare service computing. <em>TSC</em>, <em>18</em>(4), 2158-2171. (<a href='https://doi.org/10.1109/TSC.2025.3583116'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiogram(ECG) signal analysis is crucial in healthcare service computing. Ensuring accurate assessment of ECG signal quality is vital to prevent wastage of transmission bandwidth and ineffective analysis caused by noise. This enables the efficient utilization of service resources. However, existing ECG signal quality assessment(SQA) methods primarily focus on single-modal learning, overlooking the interrelation of ECG in a multimodal feature space and failing to effectively exploit available information for pattern mining. In this paper, we model the SQA for ECG as an anomaly detection problem and propose a multimodal unsupervised SQA method. It jointly explores the boundaries between high-quality ECG and noise in both the time and frequency domains by introducing time-frequency pseudo anomalies. Specifically, we first simulate real ECG noise from the time-domain using a combination of a series of noises and convert it to the frequency-domain to form time-frequency pseudo-anomalies. Next, we map the time-frequency pseudo anomalies onto hyperspheres and jointly refine the hyperspheres learned only from high-quality ECG samples in both feature spaces. Finally, the noise score is defined as the distance from the joint time-frequency features to the center of the hypersphere. Multiple experiments on various real-world ECG datasets validate the superior performance of our proposed method.},
  archive      = {J_TSC},
  author       = {Xunhua Huang and Liang Xi and Haoyi Fan and Fengbin Zhang and Xu Yu and Lei Liu and Mianxiong Dong and Mohsen Guizani},
  doi          = {10.1109/TSC.2025.3583116},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2158-2171},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Joint time-frequency pseudo anomalies for multimodal electrocardiogram quality assessment in healthcare service computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint task offloading and migration optimization in UAV-enabled dynamic MEC networks. <em>TSC</em>, <em>18</em>(4), 2143-2157. (<a href='https://doi.org/10.1109/TSC.2025.3576644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAV-enabled multi-access edge computing (MEC) is expanding possibilities for integrated space-air-ground networks, especially in the 5 G era and beyond. In this scenario, tasks from mobile users (MUs) are offloaded to nearby UAVs for execution, with results returned upon completion. However, the unpredictable mobility of MUs, coupled with dynamic network conditions and fluctuating resource availability, can degrade the reliability of communication links, leading to increased delivery latency, particularly for tasks involving large computational results. To meet stringent QoS requirements, adaptive task migration across UAVs is essential to minimize latency. To address this issue, in this paper, we first investigate Computation Task MiGration (CTMiG) problem in UAV-enabled dynamic MEC networks, focusing on joint optimization of task-serving (offloading and migration) decisions to reduce latency for all MUs. We propose the ILCTS algorithm, an imitation learning-based joint optimization method that adaptively adjusts scheduling strategies in response to environmental changes. An improved PPO algorithm is first proposed to train a policy and generate expert data, followed by generative adversarial imitation learning to imitate the data and continuously explore new ones through online learning to enhance the policy. Experimental results demonstrate that our algorithm achieves superior performance in training accuracy and average latency compared to other representative methods.},
  archive      = {J_TSC},
  author       = {Liang Wang and Bingnan Shen and Lianbo Ma and Yao Zhang and Yingnan Zhao and Hongzhi Guo and Zhiwen Yu and Bin Guo},
  doi          = {10.1109/TSC.2025.3576644},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2143-2157},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Joint task offloading and migration optimization in UAV-enabled dynamic MEC networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint data placement and service deployment in distributed cloud-edge environment. <em>TSC</em>, <em>18</em>(4), 2129-2142. (<a href='https://doi.org/10.1109/TSC.2025.3586092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How to efficiently deploying the service components of a data-intensive application on cloud and edge servers to minimize its latency is one of the main challenges for service providers. Most existing studies consider either service deployment or data placement, rather than their joint optimization. This work considers the driving relationship between data and services in a heterogeneous environment including remote cloud and nearby edge servers, and aims to obtain a desired data placement and service deployment scheme while meeting user requirements for service quality. First, we formulate the problem and decouple data placement from service deployment by polynomial reduction. Then, a priority-based data placement strategy is proposed, which can generate a data placement scheme. After that, the original problem is transformed into a classical assignment problem, and a service deployment strategy based on an improved Hungarian algorithm is proposed to obtain a service deployment scheme. Then, a dynamic adjustment strategy based on response weight is proposed to dynamically adjust the data placement and service deployment scheme in order to reduce response latency, and obtain the final scheme. Finally, a series of comparative experiments were conducted, pitting our algorithms against several baseline and SOTA algorithms. The results show that the proposed algorithms, in comparison to other algorithms, is capable of generating superior data placement and service deployment schemes to significantly reduce response latency.},
  archive      = {J_TSC},
  author       = {Pengwei Wang and Jingtan Jia and Chao Fang and Guobing Zou and Zhijun Ding},
  doi          = {10.1109/TSC.2025.3586092},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2129-2142},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Joint data placement and service deployment in distributed cloud-edge environment},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated resource allocation for sequential task offloading in edge computing. <em>TSC</em>, <em>18</em>(4), 2115-2128. (<a href='https://doi.org/10.1109/TSC.2025.3589122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge computing, end devices (EDs) containerize tasks with the necessary resources and offload subsets to a nearby high-capacity edge server (ES) to improve efficiency. Most existing research focuses on inseparable task offloading to minimize response times or resource allocation to reduce energy consumption. However, task execution can be speeded up with excessive computing and network resources, it will increase energy consumption and incur unnecessarily high costs. Besides, complex applications like autonomous driving often partition sequential tasks to improve performance, necessitating a joint optimization of sequential task offloading and multi-resource allocation. In this paper, we introduce a Stackelberg game-based framework to model the interplay between these elements. EDs, acting as leaders, determine the offloading breakpoints of sequential tasks and the locality for processing. The ES, as the follower, uses the Karush-Kuhn-Tucker (KKT) conditions and a Boundary-constrained quasi-Particle Swarm Optimization (Bc-qPSO) algorithm to refine computing and network resource allocation, aiming to reduce system costs effectively. Our simulations show that the proposed algorithms reduce cost by approximately 10%-20% compared to traditional methods, highlighting their potential for improving the efficiency of edge computing systems.},
  archive      = {J_TSC},
  author       = {Meiyan Teng and Xin Li and Xuyun Zhang and Yanling Bu and Kun Zhu and Adnan Mahmood and Jie Wu and Quan Z. Sheng},
  doi          = {10.1109/TSC.2025.3589122},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2115-2128},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Integrated resource allocation for sequential task offloading in edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlxVRM: Enabling online configuring memory via virtualization on programmable data plane. <em>TSC</em>, <em>18</em>(4), 2103-2114. (<a href='https://doi.org/10.1109/TSC.2025.3583131'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programmable data plane (PDP) has emerged as a powerful platform for line-rate packet processing, utilizing on-chip register memory to execute stateful applications. Yet most existing efforts concentrate on static approaches for allocating register memory, necessitating switch restarting and service interruption. Despite the availability of research on sharing memory for concurrent applications, the rigid requirement of limiting memory sharing to the same pipeline stages hampers application flexibility and poses scalability challenges. To address this limitation, we present FlxVRM, a flexible register memory virtualization layer for data plane P4 programs which supports high-flexibility sharing of register memory for concurrent applications on PDP. FlxVRM enables memory allocation at any stage and location of the pipeline on PDP for each application at run time. To reduce resource usage during virtualization in the data plane pipeline, FlxVRM further merges different tables and actions with similar structures within P4 programs. Additionally, FlxVRM provides a compiler to generate data plane programs for virtualization as well as the control plane API configuration. A prototype of FlxVRM is implemented based on P4 hardware switches with Intel Tofino ASIC. Our experiment results show that FlxVRM significantly improves the allocatable memory space for applications by up to 50%, while reducing the resource of the table up to 68%.},
  archive      = {J_TSC},
  author       = {Mimi Qian and Lin Cui and Fung Po Tso and Yuhui Deng and Zhen Zhang and Weijia Jia},
  doi          = {10.1109/TSC.2025.3583131},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2103-2114},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {FlxVRM: Enabling online configuring memory via virtualization on programmable data plane},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fidelity-aware inference services in DT-assisted edge computing via service model retraining. <em>TSC</em>, <em>18</em>(4), 2089-2102. (<a href='https://doi.org/10.1109/TSC.2025.3586126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Digital Twin (DT) technique enables seamless integrations between the physical and virtual worlds. By continuously synchronizing DTs with their physical counterparts, DTs can provide accurate reflections of physical objects and facilitate high-fidelity inference services based on service models. Orthogonal to the DT technology, Mobile Edge Computing (MEC) has been envisioning as a promising paradigm for providing intelligent services to users while meeting stringent delay and accuracy requirements. In this paper, we investigate fidelity-aware inference services in a DT-assisted MEC network where there are multiple source DTs providing new updated training data to service models often. We jointly schedule mobile devices to upload their update data to their DTs, and choose service models for retraining using their updated source DT data over a given time horizon. We further assume that the previous version of each service model can still serve its users during its retraining period, while a retrained service model can provide high-fidelity services to its users. To this end, we first formulate two novel optimization problems: the model instance placement problem that assigns model instances to cloudlets in an MEC network so that the total placement cost of all service models is minimized, and the cumulative utility maximization problem to maximize the cumulative fidelity of all service models over a given time horizon, by jointly scheduling mobile devices to upload their update data to their DTs and service models to be trained using their updated source DT data at each time slot. We then formulate an integer linear programming (ILP) solution for the model instance placement problem when the problem size is small; otherwise we develop an approximate solution to the problem, at the expense of moderate resource violations. We also devise an efficient online algorithm for the cumulative utility maximization problem. We finally evaluate the performance of the proposed algorithms via simulations, and the simulation results demonstrate that the proposed algorithms are promising.},
  archive      = {J_TSC},
  author       = {Xuan Ai and Weifa Liang and Yuncan Zhang and Wenzheng Xu},
  doi          = {10.1109/TSC.2025.3586126},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2089-2102},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Fidelity-aware inference services in DT-assisted edge computing via service model retraining},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedCLR+: Tackling onboard label constraints for accurate federated satellite computing. <em>TSC</em>, <em>18</em>(4), 2075-2088. (<a href='https://doi.org/10.1109/TSC.2025.3583150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of Low Earth Orbit (LEO) satellites, particularly with the increasing deployment of intelligent computing capabilities using commercial off-the-shelf (COTS) hardware, presents significant opportunities to enhance the quality of in-orbit services. However, the current onboard conditions remain insufficient to enhance model accuracy by increasing model size, and inadequate accuracy hampers the effectiveness of in-orbit services. The satellite-ground federated learning (FL) paradigm, leveraging collaborative fine-tuning, offers a promising solution to continuously improve onboard model performance. Prior studies have focused on optimizing fine-tuning under constraints like limited bandwidth and computational resources, they often overlook two critical challenges: the scarcity and skewness of labeled onboard data and the long revisit cycles of satellites. To address these challenges and better support in-orbit services, this article designs a realistic simulation methodology for the onboard fine-tuning process and conducts a comprehensive measurement study. Based on insights from the measurement results, we propose an efficient satellite-ground federated fine-tuning system, FedCLR+. In this system, we design a FedCLR algorithm to enhance system accuracy through representation optimization. Additionally, we propose a hybrid bias-compensated strategy to further mitigate accuracy loss by enriching the diversity of aggregation information. Experimental results show that FedCLR+ significantly enhances accuracy by up to 21.61×, reduces transmission volume by an average of 7.29%, and maintaining acceptable additional overhead compared to baselines.},
  archive      = {J_TSC},
  author       = {Chen Yang and Qiyang Zhang and Qibo Sun and Shufeng Ouyang and Ao Zhou and Shangguang Wang and Mengwei Xu},
  doi          = {10.1109/TSC.2025.3583150},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2075-2088},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {FedCLR+: Tackling onboard label constraints for accurate federated satellite computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature noise resilient for QoS prediction with probabilistic deep supervision. <em>TSC</em>, <em>18</em>(4), 2062-2074. (<a href='https://doi.org/10.1109/TSC.2025.3577425'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate Quality of Service (QoS) prediction is essential for enhancing user satisfaction in web recommendation systems, yet existing prediction models often overlook feature noise, focusing predominantly on label noise. In this paper, we present the Probabilistic Deep Supervision Network (PDS-Net), a robust framework designed to effectively identify and mitigate feature noise, thereby improving QoS prediction accuracy. PDS-Net operates with a dual-branch architecture: the main branch utilizes a decoder network to learn a Gaussian-based prior distribution from known features, while the second branch derives a posterior distribution based on true labels. A key innovation of PDS-Net is its condition-based noise recognition loss function, which enables precise identification of noisy features in objects (users or services). Once noisy features are identified, PDS-Net refines the feature’s prior distribution, aligning it with the posterior distribution, and propagates this adjusted distribution to intermediate layers, effectively reducing noise interference. Extensive experiments conducted on two real-world QoS datasets demonstrate that PDS-Net consistently outperforms existing models, achieving an average improvement of 8.91% in MAE on Dataset D1 and 8.32% on Dataset D2 compared to the state-of-the-art. These results highlight PDS-Net’s ability to accurately capture complex user-service relationships and handle feature noise, underscoring its robustness and versatility across diverse QoS prediction environments.},
  archive      = {J_TSC},
  author       = {Ziliang Wang and Xiaohong Zhang and Ze Shi Li and Sheng Huang and Meng Yan},
  doi          = {10.1109/TSC.2025.3577425},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2062-2074},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Feature noise resilient for QoS prediction with probabilistic deep supervision},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EProbe: EBPF-enhanced accurate container status probing in cloud-native systems. <em>TSC</em>, <em>18</em>(4), 2047-2061. (<a href='https://doi.org/10.1109/TSC.2025.3576658'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-native systems enhance scalability and availability by leveraging containers. For better container management and scheduling, existing container management systems (e.g., Kubernetes) employ readiness and liveness probes to probe container status. However, failures in these probing systems can lead to traffic scheduling errors, application failures, or even cluster crashes, due to real-time probing issues or implementation bugs. To address these shortcomings, we propose a non-intrusive, real-time container status probing system based on eBPF (extended Berkeley Packet Filter), called eProbe. eProbe intercepts packets passing through containers to probe container status in real time within the operating system kernel, without additional instrumentation or modifications to existing container management systems. Our evaluations show that eProbe can accurately probe container status, whereas Kubernetes probes can only partially achieve this. Moreover, eProbe incurs a low overhead, consuming only 0.03% to 0.26% additional memory and 0.22% to 0.33% additional CPU, while introducing a 2.48% increase in response time for cloud-native applications.},
  archive      = {J_TSC},
  author       = {Wanqi Yang and Pengfei Chen},
  doi          = {10.1109/TSC.2025.3576658},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2047-2061},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {EProbe: EBPF-enhanced accurate container status probing in cloud-native systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). East: Efficient and accurate secure inference framework for transformer. <em>TSC</em>, <em>18</em>(4), 2038-2046. (<a href='https://doi.org/10.1109/TSC.2025.3577491'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer has been successfully used in practical applications due to its powerful advantages. However, users’ input is leaked to the model provider during the service. With people’s attention to privacy, privacy-preserving Transformer inference is on the demand of such services. Secure protocols for non-linear functions are crucial in privacy-preserving Transformer inference, which are not well studied. Thus, designing practical secure protocols for non-linear functions is hard but significant to model performance. In this work, we propose a framework East to enable efficient and accurate secure Transformer inference. First, we propose a new oblivious piecewise polynomial evaluation algorithm and apply it to the activation functions, which reduces the runtime and communication of GELU by over 1.5× and 2.5×, compared to prior arts. Second, the secure protocols for softmax and layer normalization are carefully designed to faithfully maintain the desired functionality. Third, several optimizations are conducted in detail to enhance the overall efficiency. We applied East to BERT and the results show that the inference accuracy remains consistent with the plaintext inference without fine-tuning. Compared to Iron, we achieve about 1.8× lower communication within 1.2× lower runtime.},
  archive      = {J_TSC},
  author       = {Yuanchao Ding and Hua Guo and Yewei Guan and Weixin Liu and Jiarong Huo and Zhenyu Guan and Xiyong Zhang},
  doi          = {10.1109/TSC.2025.3577491},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2038-2046},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {East: Efficient and accurate secure inference framework for transformer},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMSDRec: Dynamic structure-aware graph masked autoencoder and spatiotemporal diffusion for next-POI recommendation. <em>TSC</em>, <em>18</em>(4), 2024-2037. (<a href='https://doi.org/10.1109/TSC.2025.3570853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of smart devices has accelerated location-based services, creating vast trajectory data for Point-of-Interest (POI) recommendations. However, user omissions or privacy concerns often result in incomplete trajectory data, compromising sequential patterns and spatiotemporal relationships. Existing solutions using graph structures, self-supervised learning (SSL), or spatiotemporal contexts still face two limitations: (1) graph-based and SSL methods produce suboptimal trajectory representations due to respective inherent constraints; (2) noise interference persists when modeling distorted spatiotemporal signals. To mitigate these issues, we propose a Dynamic Structure-aware Graph Masked Autoencoder and Spatiotemporal Diffusion for Next-POI Recommendation (DMSDRec). Specifically, we introduce a dynamic structure-aware improved graph masked autoencoder that adaptively and dynamically distills global transitional information for self-supervised augmentation. It naturally avoids the noise introduced by existing SSL methods’ dependency on manual views augmentation. Meanwhile, the masked reconstruction task synergistically enhances trajectory representations by capturing deeper cross-sequence dependencies. Additionally, we propose an effective latent-space spatiotemporal diffusion denoising method. First, we employ graph structures to model spatiotemporal relationships, utilizing higher-order structural information to alleviate the linear spatiotemporal relationship deviations caused by incomplete trajectories. Building on this, we implement diffusion models in the latent space to systematically identify and remove noise from spatiotemporal representations. Through experimental results on two real-world datasets, we demonstrate the superiority of our proposed DMSDRec in terms of recommendation accuracy and robustness.},
  archive      = {J_TSC},
  author       = {Yue Li and Jun Zeng and Haoran Tang and Junhao Wen and Min Gao and Wei Zhou},
  doi          = {10.1109/TSC.2025.3570853},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2024-2037},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {DMSDRec: Dynamic structure-aware graph masked autoencoder and spatiotemporal diffusion for next-POI recommendation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decentralized request dispatch for edge-clouds: A diffusion-based reinforcement learning paradigm. <em>TSC</em>, <em>18</em>(4), 2010-2023. (<a href='https://doi.org/10.1109/TSC.2025.3577458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-cloud systems have the potential to achieve ubiquitous computing by providing services in close proximity to users that submit service requests. The key challenge is how to efficiently orchestrate services and dispatch requests to satisfy the Quality of Service (QoS) requirements of users in dynamic edge-cloud environments. With the benefit of efficiently adapting to uncertainty, Reinforcement Learning (RL) based approaches are proposed to solve the request dispatch problem in edge-cloud environments. However, existing RL based approaches are often constrained by inexpressive policies that make highly suboptimal decisions in the field of request dispatch for edge-clouds. To enhance the effectiveness of RL in guaranteeing the QoS requirements of users, this paper presents D2Sched, a novel scheduling framework that represents the policy networks of Multi-Agent Deep Reinforcement Learning (MADRL) as diffusion models to generate request dispatch decisions. To improve the valid probability of generated dispatch decisions, D2Sched coordinates all agents for the resource competition among different requests by carefully considering the availability of system resources and latency targets of requests. Extensive experiments using synthetic and real traces demonstrate that D2Sched can improve the average system throughput under QoS requirements of users by up to 20.1% compared to representative baselines.},
  archive      = {J_TSC},
  author       = {Yaqiong Peng and Haocheng Peng and Wei Wang},
  doi          = {10.1109/TSC.2025.3577458},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {2010-2023},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Decentralized request dispatch for edge-clouds: A diffusion-based reinforcement learning paradigm},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data re-outsourcing detection with latency-constraint for edge storage. <em>TSC</em>, <em>18</em>(4), 1996-2009. (<a href='https://doi.org/10.1109/TSC.2025.3579235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge storage has become a widely used solution for providing low-latency data access services, which motivates data owners to outsource data on geographically distributed edge nodes to deliver a positive user experience. Nevertheless, various security concerns raise in terms of data availability. Among them, edge data geo-location verification becomes a prominent concern when the data is out of owners’ control, since outsourced data may be re-outsourced to other economical yet unknown third-party devices by dishonest edge nodes for saving storage space and pocketing the difference. Existing geo-localization approaches for cloud architectures can not be practically applied to identify such re-outsourcing behaviors due to the uniqueness of edge storage. To close this gap, we make the first attempt to investigate the edge data re-outsourcing detection (EDRD) problem, enabling the data owner to inspect if outsourced data is consistently cached on the rented edge nodes with agreed geo-location. We leverage timed Challenge-Response mechanisms for data possession proof while measuring verification latency to detect re-outsourcing behaviors by comparing with re-outsourcing detection threshold $\mathbb {C}$. We prove that the edge node whose verification latency exceeds $\mathbb {C}$ is dishonest. To obtain the optimal $\mathbb {C}$, we formulate the threshold determination (TD) problem and transform it to an easy-to-handle form for problem complexity reduction. Then, a preference-based approach named TD-P is developed to efficiently address the transformed TD problem. On top of that, we propose a $\mathbb {C}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="double-struck">C</mml:mi></mml:math>-aware edge data re-outsourcing detection scheme entitled EDRD-$\mathbb {C}$ to tackle the EDRD problem effectively. The efficiency and effectiveness of TD-P and EDRD-$\mathbb {C}$ are verified by extensive theoretical analysis and experimental evaluations on both simulated and real platforms. Notably, EDRD-$\mathbb {C}$ achieves 100% detection accuracy by sacrificing a reasonable amount of computing resources and 92.97% in the worst case.},
  archive      = {J_TSC},
  author       = {Yao Zhao and Youyang Qu and Yong Xiang and Feifei Chen and Longxiang Gao},
  doi          = {10.1109/TSC.2025.3579235},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1996-2009},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Data re-outsourcing detection with latency-constraint for edge storage},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crowdsourcing to service users: Work for yourself and get reward. <em>TSC</em>, <em>18</em>(4), 1982-1995. (<a href='https://doi.org/10.1109/TSC.2022.3173959'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous service providers rely on crowdsourcing from service users, rather than a less-specific, more public group, to provide better services to the users themselves. In the majority of studies on the incentive mechanism for crowdsourcing, however, the users’ intrinsic desire/demand for better service, which could have been exploited to enhance their involvement in crowdsourcing, has been largely overlooked. Therefore, conclusions are limited regarding the optimal incentives and the benefits of crowdsourcing on the service quality and thus the market share. In this paper, we study the incentive mechanism for crowdsourcing that combines a financial reward in the form of service price discounts and users’ intrinsic demand for better service. Our focus is on leveraging the users dual role, i.e., the interdependence between the service and the users. We show that the dynamic market converges to a unique equilibrium under mild conditions, with the consideration of varying service usage levels and privacy concerns of the users. Besides, counter-intuitively, failure to take into account the users’ intrinsic reward leads to too little extrinsic incentive. Moreover, our results showed how the competition reshapes the markets, which cannot be intuitively or trivially predicted without a thorough analysis.},
  archive      = {J_TSC},
  author       = {Jing Hou and Li Sun and Tao Shu},
  doi          = {10.1109/TSC.2022.3173959},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1982-1995},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Crowdsourcing to service users: Work for yourself and get reward},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative service caching, task offloading, and resource allocation in caching-assisted mobile edge computing. <em>TSC</em>, <em>18</em>(4), 1966-1981. (<a href='https://doi.org/10.1109/TSC.2025.3586093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) revolutionizes the traditional cloud-based computing paradigm by moving resources in proximity to the network edge, aiming to cater to the rigorous requirements of emerging latency-sensitive applications. However, the escalating resource demands intensify the competition among user devices (UDs). Thus, it is essential to coordinate task offloading and resource scheduling while ensuring fairness among users in MEC. Despite the crucial role of user fairness in motivating task offloading in MEC, it is often overlooked in existing literature. Therefore, we in this paper propose a caching-enhanced MEC framework and formulate a collaborative service caching, task offloading, and multi-resource allocation problem to maximize average user satisfaction. Multiple factors contribute to the difficulty in solving the optimization problem, including constrained resource capabilities, user mobility, service heterogeneity, and spatial demand coupling. Consequently, we transform the origin problem into two distinct subproblems – the service caching and task offloading problem, and the multi-resource allocation problem, respectively. Then, the Advantage Actor-Critic (A2C) based approach is proposed to address the former problem, while a Lagrangian duality-based approach is adopted to tackle the latter problem. The simulation results demonstrate the superior performance of the proposed solution in comparison to several baseline methods.},
  archive      = {J_TSC},
  author       = {Chaogang Tang and Yao Ding and Shuo Xiao and Zhenzhen Huang and Huaming Wu},
  doi          = {10.1109/TSC.2025.3586093},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1966-1981},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Collaborative service caching, task offloading, and resource allocation in caching-assisted mobile edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CASE: Minimizing attack surfaces based on context-aware system call enforcement. <em>TSC</em>, <em>18</em>(4), 1952-1965. (<a href='https://doi.org/10.1109/TSC.2025.3577497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Invoking system calls in exploit implementation is a typical approach to compromising a system. A key objective of these attacks is to manipulate program execution paths, with a specific focus on invoking targeted system calls. Our study introduces Context-Aware System Call Enforcement (CASE), a software-based approach meticulously crafted to shrink the attack surface associated with system call-based exploits. CASE achieves this by rigorously validating the context, mainly backward function call paths and runtime stack states, to ensure the legitimacy of system call invocations. Our strategy incorporates innovative elements, including anchored entry points, return address-based validation, and frame size checks. We formalize our approach by creating NP-hard challenges for potential attackers and complete with a proof-of-concept (PoC) implementation that shields against attacks. Our PoC implementation introduces minimal overhead, less than 2%, for context validation. Simultaneously, it adeptly identifies and halts attacks of varying complexities, ranging from simple examples to real-world servers.},
  archive      = {J_TSC},
  author       = {Man-Ni Hsu and Tsung-Han Liu and Hsuan-Ying Lee and Chun-Ying Huang},
  doi          = {10.1109/TSC.2025.3577497},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1952-1965},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {CASE: Minimizing attack surfaces based on context-aware system call enforcement},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Battery swapping tour optimization problem in dockless electric bike sharing service systems with distance-aware user incentives. <em>TSC</em>, <em>18</em>(4), 1938-1951. (<a href='https://doi.org/10.1109/TSC.2025.3562337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dockless electric bike (E-bike) sharing has emerged as a modern and eco-friendly mode of urban transportation, providing convenient services for users. Typically, the service provider deploys a truck from a central depot to visit multiple parking locations, replacing low-energy batteries in E-bikes and returning to the depot. However, the cost of visiting numerous locations can be substantial. One effective strategy is to aggregate low-energy E-bikes together, prompting the adoption of incentive mechanism to encourage users to relocate their bikes to the proper parking locations, albeit at an additional incentive cost. The challenge lies in balancing the tour cost of the truck and the incentive cost for E-bike users. To address this issue, this paper formulates an online battery swapping optimization problem, which is NP-hard and cannot be approximated within a competitive factor of 2. For the offline and online problems, the paper proposes a $\frac{20}{3}$-approximation algorithm and an $O(\epsilon ^{-1})$-competitive algorithm. The simulation results using the real-world data demonstrate that the proposed algorithms outperform existing baselines by identifying suitable battery swapping locations to minimize both tour and incentive costs for service providers.},
  archive      = {J_TSC},
  author       = {Chun-An Yang and Shih-Chieh Chen and Jian-Jhih Kuo and Yi-Hsuan Peng and Yu-Wen Chen and Ming-Jer Tsai},
  doi          = {10.1109/TSC.2025.3562337},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1938-1951},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Battery swapping tour optimization problem in dockless electric bike sharing service systems with distance-aware user incentives},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A resource management strategy for fluid equilibrium in edge-cloud market supporting AIGC services. <em>TSC</em>, <em>18</em>(4), 1922-1937. (<a href='https://doi.org/10.1109/TSC.2025.3583154'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating demands for Artificial Intelligence-generated content (AIGC) services greatly require computing resources. The edge-cloud market offers an effective solution for AIGC services by integrating, managing, and trading distributed computing resources. Within this novel service market, participants contribute idle resources to support AIGC services to earn income, creating a more flexible market environment. Meanwhile, the generation quality and computing resource requirements of AIGC services are related to input prompts. Therefore, this relationship introduces new challenges, such as the information uncertainty in input prompts, the inability to model resource continuity, and high-dimensional complexity for optimization. In this paper, we propose a resource fluid equilibrium management strategy for supporting AIGC services within edge-cloud market, termed FluE. To address the challenge of information uncertainty in user prompts, we measure the content value of AIGC prompts by information entropy and introduce a redundancy reduction approach to focus on meaningful information in prompts. To tackle the challenge of the inability to model the continuity provision of computing resources, we utilize the fluid model to ensure seamless resource provision and facilitate a more balanced management of computing resources. To address the challenge of high-dimensional complexity of strategy optimization, we develop a diffusion-based algorithm named ReDiff to reconstruct the target strategy distribution and generate precise and effective optimization decisions. We evaluate our proposed scheme under a dynamic resource provisioning environment. Based on the DiffusionDB dataset, the publicly available real trace of AIGC service prompt, our ReDiff algorithm achieves up to 69.8% and 77.4% improvements in average social welfare compared to LySAC and CD-PPO, respectively.},
  archive      = {J_TSC},
  author       = {Xiaofei Wang and Chenxuan Hou and Chao Qiu and Xiaoxu Ren and Zehui Xiong and Haipeng Yao and Dusit Niyato},
  doi          = {10.1109/TSC.2025.3583154},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1922-1937},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A resource management strategy for fluid equilibrium in edge-cloud market supporting AIGC services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A logical reasoning network for high-order complementary cloud API recommendation. <em>TSC</em>, <em>18</em>(4), 1906-1921. (<a href='https://doi.org/10.1109/TSC.2025.3579133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud API recommender system has emerged as a promising solution to address the overload problem caused by the overwhelming growth of cloud APIs, aiming to improve software development efficiency. However, the incremental development nature of the service-oriented software necessitates an adaptive complementary cloud API recommender system, while existing systems purely focus on generating single-function, high-quality, and personalized recommendations based on retrieval content, quality of service, or user preferences, and overlook the practical scenarios that recommend high-order complementary cloud APIs for developers. This article addresses this gap by providing a logical reasoning network for high-order complementary cloud API recommendation (LRN4HCAR). We first construct three relation graphs to characterize cloud API co-invocation, function co-occurrence, and substitute relations, capturing the strong complementarity, weak complementarity, and non-substitute relations among cloud APIs. Next, we develop a high-order complementary logical reasoning network with three sub-networks: strong complementary logical reasoning, weak complementary logical reasoning, and non-substitute logical reasoning. This network enables the recommendation of cloud APIs with high-order complementary relationships while excluding substitutes. Utilizing LRN4HCAR, we evaluate classic and SOTA baselines across various complementary recommendation scenarios on two real-world datasets. Experimental results demonstrate that LRN4HCAR outperforms others in all experimental settings for complementary cloud API recommendations. We also verify the ability of LRN4HCAR to handle substitute noise and improve the visibility of long-tail cloud APIs in the recommendation results. The implementation code has made publicly available at https://github.com/hey-mem/LRN4HCAR.},
  archive      = {J_TSC},
  author       = {Zhen Chen and Denghui Xie and Mengyao Wu and Yueshen Xu and Dianlong You},
  doi          = {10.1109/TSC.2025.3579133},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1906-1921},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A logical reasoning network for high-order complementary cloud API recommendation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight I/O throttling service to improve the user experience of mobile devices. <em>TSC</em>, <em>18</em>(4), 1893-1905. (<a href='https://doi.org/10.1109/TSC.2025.3576691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most frequently occurring operations, I/Os significantly affect the application launching time and frame rate of mobile devices, hence influencing the user experience. However, the response speed of I/O requests is still the bottleneck in practice. This paper shows that high I/O latency is usually due to the congestion inside Flash instead of the system layer. Unfortunately, Flash is treated as a closed box device and cannot be modified after delivery. In this paper, we propose a novel service to address this issue without an intra-Flash modification. Specifically, this paper proposes a lightweight I/O throttling framework in mobile systems named FlashDAM. This service throttles the I/O flow to make way for I/Os that may block the foreground application. FlashDAM is the first work that proves that proper I/O throttling positively affects the user experience, contrary to the common belief. Furthermore, this paper proposes FlashDAM$^+$, an enhanced version of FlashDAM. By coordinating I/O throttling and compression, FlashDAM’s effect in the system layer is minimized. We have implemented FlashDAM on real mobile devices. Experimental results illustrate that the app launching speed and frame rate are enhanced by 72% and 45% separately compared to the state-of-the-art. When enabling the compression feature of FlashDAM, that is, FlashDAM$^+$, screen jank and application launch latency are further reduced by 9.5% and 11.4%, respectively, under heavy background I/O load.},
  archive      = {J_TSC},
  author       = {Changlong Li and Zongwei Zhu and Yuyangjun Lu and Chao Wang and Xuehai Zhou and Edwin Hsing-Mean Sha},
  doi          = {10.1109/TSC.2025.3576691},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1893-1905},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A lightweight I/O throttling service to improve the user experience of mobile devices},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid NOMA-OMA framework for multi-user offloading in mobile edge computing system. <em>TSC</em>, <em>18</em>(4), 1881-1892. (<a href='https://doi.org/10.1109/TSC.2025.3576699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the integration of mobile edge computing (MEC) and non-orthogonal multiple access (NOMA) has gained significant attention for its potential to reduce energy consumption and offloading latency in future wireless networks. While NOMA can enhance system capacity, accommodating multiple users on the same channel may lead to decoding inaccuracies and reduced offloading accuracy. To tackle these problems, this article proposes a multi-user offloading model that combines NOMA and orthogonal multiple access (NOMA-OMA) to optimize resource allocation. Users are divided into groups based on their geographical locations, with each group further divided into subgroups. OMA is used within each subgroup, while NOMA is employed between different subgroups to achieve joint multi-user offloading. We divide the optimization problem into two sub-problems, namely power and time allocation between different subgroups and delay allocation within the same subgroup. Closed-form expressions for the two sub-problems are derived. The proposed method achieves optimal system energy consumption while increasing the number of users and maintaining low system complexity. Simulation results demonstrate the effectiveness of the proposed method.},
  archive      = {J_TSC},
  author       = {Furong Chai and Qi Zhang and Haipeng Yao and Xiangjun Xin and Ran Gao and Di Wu and F. Richard Yu},
  doi          = {10.1109/TSC.2025.3576699},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1881-1892},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A hybrid NOMA-OMA framework for multi-user offloading in mobile edge computing system},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fair and efficient resource allocation algorithm for cloud rendering jobs. <em>TSC</em>, <em>18</em>(4), 1869-1880. (<a href='https://doi.org/10.1109/TSC.2025.3570847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service level agreements (SLAs) formulated by cloud rendering service providers and users are varied, as users may have diverse performance requirements for their own jobs. This leads to a complex issue that cloud resources need to be allocated to rendering jobs in an appropriate and effective manner to satisfy users’ diverse SLAs. To address this issue, in this article, we propose a novel fair and efficient resource allocation algorithm, which aims to maximize the execution efficiency of rendering service applications while satisfying users’ diverse SLAs. First, to satisfy users’ diverse SLAs, we propose a rigorous definition of weighted acceleration ratio fairness, whose guiding principle is that the execution speed of a rendering job should be proportional to its weight determined by users’ SLAs. Then, under the guidance of the proposed principle of acceleration ratio fairness, we formulate a new algorithm to fairly allocate resources to rendering jobs. Lastly, to improve execution efficiency and coordinate efficiency and fairness, we propose a fair and efficient resource allocation algorithm with relaxing fairness in resource competitive and non-competitive situations separately for rendering service applications. With extensive experiments that involve real rendering application workloads, we validate the effectiveness of our algorithms in improving execution efficiency and satisfying users’ diverse SLAs.},
  archive      = {J_TSC},
  author       = {Xiulin Li and Li Pan and Shijun Liu and Xiangxu Meng},
  doi          = {10.1109/TSC.2025.3570847},
  journal      = {IEEE Transactions on Services Computing},
  month        = {7-8},
  number       = {4},
  pages        = {1869-1880},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A fair and efficient resource allocation algorithm for cloud rendering jobs},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards hybrid architectures for big data analytics: Insights from spark-MPI integration. <em>TSC</em>, <em>18</em>(3), 1852-1868. (<a href='https://doi.org/10.1109/TSC.2025.3562342'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-Performance Data Analytics (HPDA) combines high-performance computing (HPC) with data analytics to uncover patterns and insights in dual-intensive applications that are both data-intensive and compute-intensive. Traditional Big Data frameworks and HPC technologies often struggle to address these demands independently, prompting researchers to explore their integration. Spark, known for its efficient in-memory computing with RDDs, and MPI, a foundational standard in HPC, are prominent candidates for such integration. This survey explores the integration of Spark and MPI for HPDA, highlighting their potential for unified data processing and computation. We first classify application workloads and review the characteristics and limitations of traditional frameworks. Then, we analyze the challenges and requirements of integrated architectures, focusing on the specific implementations of typical middleware-level architectures. Through comparative analysis, we highlight their advantages and limitations. Finally, we present application examples, outline key challenges and future research directions, and briefly discuss progress in integration approaches for other technology combinations.},
  archive      = {J_TSC},
  author       = {Mengbing Zhou and Qiuyan Li and Mingyuan Cai and Chengzhong Xu and Yang Wang},
  doi          = {10.1109/TSC.2025.3562342},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1852-1868},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Towards hybrid architectures for big data analytics: Insights from spark-MPI integration},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TREAT: Temporal and relational attention-based tensor representation learning for ethereum phishing users. <em>TSC</em>, <em>18</em>(3), 1838-1851. (<a href='https://doi.org/10.1109/TSC.2025.3568277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Ethereum blockchain platform has witnessed a surge in crypto-cybercrimes, particularly phishing attacks, resulting in significant financial losses. Analyzing the Ethereum transaction network to detect phishing users poses a set of critical challenges, including network sparsity, dynamic network fluctuations, large-scale data and significant class imbalance. Existing literature in this area primarily leverages traditional feature engineering or network representation learning to retrieve crucial information from transaction records to identify suspected users. However, these methods mainly rely on manually handcrafted features or conventional node representation learning while ignoring the inherent network sparsity and dynamic fluctuations. Hence, to alleviate these challenges, this article introduces a novel tensor-based representation learning framework, TREAT (Temporal and Relational Attention-based Tensor Representation Learning). TREAT models the Ethereum transaction network as a 3-dimensional tensor to preserve structural, transactional, and temporal aspects in a standalone architecture, thereby observing the rich correlation among these. The framework is coupled with a two-way self-attention mechanism alongside a rank-based tensor decomposition to comprehend the underlying evolving transaction interaction patterns while addressing the network sparsity. A Graph Neural Network layer with edge attention elevates the final representation quality, thereby yielding a 3%~4% improvement in F1-Score with respect to the existing baselines.},
  archive      = {J_TSC},
  author       = {Medhasree Ghosh and Raju Halder and Joydeep Chandra},
  doi          = {10.1109/TSC.2025.3568277},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1838-1851},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {TREAT: Temporal and relational attention-based tensor representation learning for ethereum phishing users},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tensor-based factorial hidden markov model for cyber-physical-social services. <em>TSC</em>, <em>18</em>(3), 1825-1837. (<a href='https://doi.org/10.1109/TSC.2025.3565382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development and widespread application of information, computer, and communication technologies, Cyber-Physical-Social Systems (CPSS) have gained increasing importance and attention. To enable intelligent applications and provide better services for CPSS users, efficient data analytical models are crucial. This paper presents a novel data analytic framework for CPSS services. First, a Tensor-Based Factorial Hidden Markov Model (T-FHMM) is introduced to comprehensively analyze multi-user activity features, enhancing CPSS activity analytics. A tensor-based Forward-Backward algorithm is then designed for T-FHMM to efficiently perform evaluation tasks using multiple probabilistic computing micro-services. Additionally, a tensor-based Baum-Welch algorithm is developed to accurately learn model parameters via parameter optimization micro-services. Furthermore, a tensor-based Viterbi algorithm is implemented with specific micro-services to improve prediction tasks. Finally, the comprehensive performance of the proposed model and algorithms is validated on three open datasets through self-comparison and other-comparison. Experimental results demonstrate that the proposed method outperforms compared methods in terms of accuracy, precision, recall, and F1-score.},
  archive      = {J_TSC},
  author       = {Zhixing Lu and Laurence T. Yang and Azreen Azman and Shunli Zhang and Fang Zhou},
  doi          = {10.1109/TSC.2025.3565382},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1825-1837},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Tensor-based factorial hidden markov model for cyber-physical-social services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synthetic privacy-preserving trajectories with semantic-aware dummies for location-based services. <em>TSC</em>, <em>18</em>(3), 1811-1824. (<a href='https://doi.org/10.1109/TSC.2025.3556642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trajectory synthesis with a series of fake locations has been deemed as a promising obfuscation technology to preserve the individual privacy of users in Location-Based Services (LBSs). However, a number of previous approaches fail to take into consideration the geographic distance and motion direction of the real locations to synthesize trajectories. As a result, most of them always cannot represent the statistical characteristics of real trajectories in a privacy-preserving manner, and thus suffer from various attacks through data analysis. To tackle this issue, this paper presents SPSD, a novel privacy-preserving trajectory synthesis approach with a $k$-anonymous guarantee, through extracting the semantic, geographic and directional similarity of locations from the real trajectories to create plausible trajectories. SPSD first classifies all historical trajectory data into a series of sets for location identity, by introducing the visiting time and visiting duration, which can clearly represent the semantic information of locations. Then, $ 4k$ locations and $ 2k$ of $ 4k$ ones have been selected from each set to act as the initial disguises of each corresponding real location, with quantitative semantic and geographic similarities, respectively. In order to find enough fake locations for each real location in less time, the candidate locations have been narrowed down to $k$ in direction recovery through step-by-step screening, with the $k$-anonymous property. Experiment results built on the real-world trajectory datasets indicate that SPSD has outperformed the previous approaches in terms of semantic similarity, directional accuracy and security resistance to synthesize privacy-preserving trajectories at the tolerable time cost.},
  archive      = {J_TSC},
  author       = {Haojun Huang and Hao Sun and Weimin Wu and Chen Wang and Wuwu Liu and Wang Miao and Geyong Min},
  doi          = {10.1109/TSC.2025.3556642},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1811-1824},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Synthetic privacy-preserving trajectories with semantic-aware dummies for location-based services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Split computing for mobile devices: Energy and latency perspective. <em>TSC</em>, <em>18</em>(3), 1798-1810. (<a href='https://doi.org/10.1109/TSC.2025.3564885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To tackle the difficulties of running sophisticated deep neural network (DNN) models on mobile devices, split computing presents a viable solution by offloading computations to the edge server. Current split computing schemes typically aim to lower either inference latency or energy use separately; however, optimizing both simultaneously is quite challenging due to numerous shifting factors, such as intensive continuous DNN model inferences, DNN model traits, and device/network conditions. Moreover, in practical applications, edge server overload might lead to substantial queuing delays, adding complexity to the optimization process. This article outlines a joint optimization problem that simultaneously seeks to minimize both inference latency and energy consumption, with a distinct inclusion of queue clearance latency for an accurate analysis of the continuously generated DNN model inferences. To address this intricate optimization challenge, we introduce a low-complexity heuristic algorithm that sets split point decisions based on the residual energy of mobile devices for each DNN inference cycle. Upon evaluation, our proposed algorithm demonstrates notable improvements by reducing inference latency by between 73.37% and 99.39%, and cutting down energy usage by between 39.97% and 94.67% compared to fully local processing on mobile devices.},
  archive      = {J_TSC},
  author       = {Daeyoung Jung and Jaewook Lee and Hyeonjae Jeong and Dongju Cha and Heewon Kim and Sangheon Pack},
  doi          = {10.1109/TSC.2025.3564885},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1798-1810},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Split computing for mobile devices: Energy and latency perspective},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Service migration for delay-sensitive IoT applications in edge networks. <em>TSC</em>, <em>18</em>(3), 1782-1797. (<a href='https://doi.org/10.1109/TSC.2025.3547221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Internet of Things (IoT) applications prompts extraordinary demands for the collaboration of large amounts of computational resources provided by IoT devices in edge networks, and these applications are mostly delay-sensitive. Generally, these resources are encapsulated as IoT services. Thereafter, IoT applications can be performed, such that the collaboration of their sub-tasks is achieved through the composition of functionally complementary and geographically contiguous IoT services. The status of computational resources in IoT devices may change continuously along with their occupancy and release by IoT services. Considering the resource-scarceness of IoT devices, when the workload of IoT devices increases due to more services to be processed, certain IoT devices may hardly have enough remaining resources to co-host more instances of certain IoT services prescribed by forthcoming IoT applications with strict constraints. As a result, the delay satisfaction of both on-running and forthcoming IoT applications may be negatively impacted, or even hardly be satisfied any longer. To solve this issue, this paper proposes a rEsource-Efficient service Configuration ($E^{2}$rC) mechanism, which aims to optimize the configuration of computational resources provided by IoT devices with respect to complex requirements prescribed by IoT applications, through service migration techniques. This service migration problem is formulated as markov multi-phases decisions, which is solved through our enhanced Deep Reinforcement Learning (DRL) approach with a two-layer Q-network. Extensive experiments have been conducted upon the dataset of our testbed system. Evaluation results show that our $E^{2}$rC is more efficient than the state-of-art counterparts in satisfying delay constraints of IoT applications, while reducing the energy consumption and improving the resource utilization efficiency of IoT devices.},
  archive      = {J_TSC},
  author       = {Xiaocui Li and Zhangbing Zhou and Yasha Wang and Shuiguang Deng and Patrick C. K. Hung},
  doi          = {10.1109/TSC.2025.3547221},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1782-1797},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Service migration for delay-sensitive IoT applications in edge networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SECTest: An integrated testing platform for QoS in satellite edge clouds. <em>TSC</em>, <em>18</em>(3), 1769-1781. (<a href='https://doi.org/10.1109/TSC.2025.3565368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of satellite computing capabilities, the diversity of satellite communication services imposes varied quality of service (QoS) requirements. Limited satellite resources necessitate remote deployment and updates of running services for QoS testing, increasing testing difficulty. Existing testing tools are limited in functionality or reliant on specific infrastructures, failing to meet the QoS testing needs of edge cloud services in mobile satellite scenarios. In this paper, we present SECTest, an integrated testing platform for QoS in satellite edge clouds. More precisely, SECTest can integrate changes in satellite network topology, create and manage satellite edge cloud cluster testing environments on heterogeneous edge devices, customize experiments for users, support deployment and scaling of various integrated testing tools, provide test data persistence function to manage data life cycle and store data hierarchically, and publish and visualize test results. We have built a real satellite edge cloud cluster based on Kubernetes, integrating both physical and virtual machines, and deploying a variety of integrated testing tools using containerization technology. Currently, we have evaluated the quality of service in terms of processing latency, packet drop rate, throughput, and average response time for object detection microservice applications, web microservice applications, and data transfer tasks. To demonstrate SECTest's scalability in testing network communication protocols, we evaluated the performance of HTTP and gRPC in microservice communication within the cluster. Our experimental results validate SECTest's ability to test key service quality metrics in a real satellite edge cloud cluster.},
  archive      = {J_TSC},
  author       = {Guogen Zeng and Juan Luo and Yufeng Zhang and Ying Qiao and Shuyang Teng and Keqin Li},
  doi          = {10.1109/TSC.2025.3565368},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1769-1781},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {SECTest: An integrated testing platform for QoS in satellite edge clouds},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seamless graph task scheduling over dynamic vehicular clouds: A hybrid methodology for integrating pilot and instantaneous decisions. <em>TSC</em>, <em>18</em>(3), 1753-1768. (<a href='https://doi.org/10.1109/TSC.2025.3562340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular clouds (VCs) play a crucial role in the Internet-of-Vehicles (IoV) ecosystem by securing essential computing resources for a wide range of tasks. This paPertackles the intricacies of resource provisioning in dynamic VCs for computation-intensive tasks, represented by undirected graphs for parallel processing over multiple vehicles. We model the dynamics of VCs by considering multiple factors, including varying communication quality among vehicles, fluctuating computing capabilities of vehicles, uncertain contact duration among vehicles, and dynamic data exchange costs between vehicles. Our primary goal is to obtain feasible assignments between task components and nearby vehicles, called templates, in a timely manner with minimized task completion time and data exchange overhead. To achieve this, we propose a hybrid graph task scheduling (P-HTS) methodology that combines offline and online decision-making modes. For the offline mode, we introduce an approach called risk-aware pilot isomorphic subgraph searching (RA-PilotISS), which predicts feasible solutions for task scheduling in advance based on historical information. Then, for the online mode, we propose time-efficient instantaneous isomorphic subgraph searching (TE-InstaISS), serving as a backup approach for quickly identifying new optimal scheduling template when the one identified by RA-PilotISS becomes inapplicable due to changing conditions. Through comprehensive experiments, we demonstrate the superiority of our proposed hybrid mechanism compared to state-of-the-art methods in terms of various evaluative metrics, e.g., time efficiency such as the delay caused by seeking for possible templates and task completion time, as well as cost function, upon considering different VC scales and graph task topologies.},
  archive      = {J_TSC},
  author       = {Bingshuo Guo and Minghui Liwang and Xiaoyu Xia and Li Li and Zhenzhen Jiao and Seyyedali Hosseinalipour and Xianbin Wang},
  doi          = {10.1109/TSC.2025.3562340},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1753-1768},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Seamless graph task scheduling over dynamic vehicular clouds: A hybrid methodology for integrating pilot and instantaneous decisions},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust federated learning for privacy preservation and efficiency in edge computing. <em>TSC</em>, <em>18</em>(3), 1739-1752. (<a href='https://doi.org/10.1109/TSC.2025.3562359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as a key enabler of privacy-preserving distributed model training in edge computing environments, crucial for service-oriented applications such as personalized healthcare, smart cities, and intelligent assistants. However, existing privacy-preserving FL methods are susceptible to multiple privacy leakage attacks (MPLA), where adversaries infer sensitive information through repeated gradient updates. This paper proposes a Robust and Communication-Efficient Federated Learning (RCFL) framework designed to enhance privacy protection and communication efficiency in edge-based service environments. RCFL integrates a global privacy-preserving mechanism with an innovative privacy encoding strategy that minimizes privacy risks over multiple data releases while significantly reducing communication overhead. The proposed framework’s theoretical analysis demonstrates its ability to maintain differential privacy across numerous interactions, ensuring robust model convergence and efficiency. Experimental results using MNIST and CIFAR-10 datasets reveal that RCFL can lower the MPLA success rate from 88.56% to 42.57% compared to state-of-the-art methods, while reducing communication costs by over 90%. These findings underscore RCFL’s potential to enhance security, efficiency, and scalability in service-oriented edge computing applications.},
  archive      = {J_TSC},
  author       = {Hao Zhou and Hua Dai and Geng Yang and Yang Xiang},
  doi          = {10.1109/TSC.2025.3562359},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1739-1752},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Robust federated learning for privacy preservation and efficiency in edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable service recommendation: A multi-modal adversarial method for personalized recommendation under uncertain missing modalities. <em>TSC</em>, <em>18</em>(3), 1724-1738. (<a href='https://doi.org/10.1109/TSC.2025.3556640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized recommendation is of paramount importance in online content platforms like Kuai and Tencent. To ensure accurate recommendations, it is crucial to consider multi-modal information in both items and user-user/item interactions. While existing works on multimedia recommendation have made strides in leveraging multi-modal contents to enrich item representations, many of them overlook the practical scenario of multiple modality missing. As a result, the performance of recommendation systems can be significantly compromised in such cases. In this paper, we introduce a novel multi-modal adversarial method called $MMAM$, which aims to provide reliable personalized recommendation services even in the presence of uncertain missing modalities. The core idea behind $MMAM$ is to design a generator that can effectively encode both user-user/item interactions and multi-modal contents, taking into account various missing cases. The generator is trained to learn transferable features from different combinations of missing modalities in order to deceive a discriminative classifier. Additionally, we propose a modal discriminator that can classify the missing cases of multi-modalities, further enhancing the capability of the model. Moreover, a well-equipped predictor utilizes the transferable features to predict potential user interests. To improve the prediction accuracy, we design a type discriminator that enhances the classification of link types. By employing a mini-max game between the generator and the discriminators, $MMAM$ successfully obtains transferable features that encompass multi-modal contents, even when facing uncertain missing modalities. We conduct extensive experiments on industrial datasets, including Kuai and Tencent. Comparing with state-of-the-art approaches, MMAM achieves improvements in personalized recommendation tasks under uncertain missing modalities. MMAM holds promise for enhancing multi-modal personalized recommendations in real-world applications.},
  archive      = {J_TSC},
  author       = {Junyang Chen and Ruohan Yang and Jingcai Guo and Huan Wang and Kaishun Wu and Liangjie Zhang},
  doi          = {10.1109/TSC.2025.3556640},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1724-1738},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Reliable service recommendation: A multi-modal adversarial method for personalized recommendation under uncertain missing modalities},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReFrame: A resource-friendly cloud-assisted on-device deep learning framework for vision services. <em>TSC</em>, <em>18</em>(3), 1711-1723. (<a href='https://doi.org/10.1109/TSC.2025.3552328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-assisted Internet of Things (IoT) device deployment of deep neural networks (DNNs) promotes On-device deep learning to provide users with ubiquitous high-quality services by solving the contradiction between insufficient IoT device resources and intensive demand for high-performance DNN resources. However, most existing methods optimize DNNs by considering one or two terms of transmission, computation, and storage resources, but do not consider all three terms at the same time in cloud-assisted IoT device deployment and updating DNNs. To this end, we propose a non-learnable module-based ResNet and a cloud-assisted on-device deep learning framework, ReFrame, based on the consideration of three indicators: model transmission parameters, computation resources, and storage resources. In the proposed method, we first specify that some parameters in DNNs are non-learnable and randomly initialized, so that, these parameters can be saved and reproduced with a few random seeds. By doing so, the cloud only transmits random seeds and learnable parameters to reduce the number of parameter transmissions. Second, we reduce the computation resource consumption of the model by introducing computation-friendly operators, such as pooling, to replace vanilla convolutions. Finally, since random seeds are used to save non-learnable model parameters, on IoT devices we only need to store random seeds and learnable parameters to reproduce the well-trained model. Compared with saving the complete model, our method greatly reduces IoT device storage resource consumption. Experimental results on image classification, object detection, and semantic segmentation tasks demonstrate the effectiveness of the proposed method. Specifically, on the CIFAR-10, our proposed method reduces approximately 89% of FLOPs and 90% of transmitted data in the prototype system compared to ResNet-18.},
  archive      = {J_TSC},
  author       = {Jianhang Xie and Chuntao Ding and Qingji Guan and Ao Zhou and Yidong Li},
  doi          = {10.1109/TSC.2025.3552328},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1711-1723},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {ReFrame: A resource-friendly cloud-assisted on-device deep learning framework for vision services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random coding responses for resisting side-channel attacks in client-side deduplicated cloud storage. <em>TSC</em>, <em>18</em>(3), 1697-1710. (<a href='https://doi.org/10.1109/TSC.2025.3568252'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Side-channel attacks are widespread in client-side deduplication systems, compromising the privacy of outsourced data. The adversary may infer the existence status of data via the deterministic relations between duplication requests and responses to launch side-channel attacks. Random Response (RARE) is one of the state-of-the-art approaches to overcome this issue, where the cloud server returns the randomized deduplication response for two requests at once to mitigate the risk of side-channel attacks. However, it still has some inherent limitations on communication efficiency and security. In this paper, we propose Random Coding Responses (RACORE), a lightweight and secure multi-chunk coding algorithm to address the limitations of RARE. RACORE achieves efficient multi-chunk coding and obfuscation based on the linear mapping induced by a specially constructed pseudo-random matrix. Compared to existing schemes, RACORE can strike a flexible balance between security and performance by adjusting parameters. Further, we present an enhanced composite matrix generation strategy to extend the coding matrix. Based on this strategy, we design an enhanced coding algorithm RACORE$^+$ to improve efficiency. Besides, we put forward a novel redundant chunk selection method to enhance the security of RACORE and RACORE$^+$. Rigorous security analysis and extensive experimental evaluation demonstrate that both RACORE and RACORE$^+$ can effectively resist side-channel attacks while reducing overhead compared to existing schemes.},
  archive      = {J_TSC},
  author       = {Guanxiong Ha and Yuchen Chen and Zhipeng Cai and Chunfu Jia and Xuan Shan},
  doi          = {10.1109/TSC.2025.3568252},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1697-1710},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Random coding responses for resisting side-channel attacks in client-side deduplicated cloud storage},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality of experience and reliability-aware task offloading and scheduling for multi-user mobile-edge computing systems. <em>TSC</em>, <em>18</em>(3), 1683-1696. (<a href='https://doi.org/10.1109/TSC.2025.3552338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile-edge computing (MEC) has received wide attention recently due to its efficacy in alleviating the computation stress of mobile devices (MDs), which is realized by offloading workloads from MD users to nearby edge servers (ESs). Prior work has studied related task offloading and scheduling problems and proposed many approaches. However, none of these approaches considers the reliability issue in MEC systems which may suffer soft errors during task execution as well as bit errors during task offloading simultaneously. Targeting optimization on a multi-user MEC system, in this article we investigate the task offloading and scheduling problem of maximizing system quality of experience (QoE) under a certain reliability requirement. With the consideration of the combinatorial nature of this problem, we propose to decompose the original problem into i) a task-to-ES assignment problem with fixed task offloading decision, for satisfying system reliability constraint, ii) a computing resource allocation problem with fixed task offloading and assignment decisions, for maximizing system QoE, and iii) a task offloading optimization problem to find the best offloading decision that achieves the maximum QoE under the reliability constraint using our task assignment and resource allocation methods. In order to solve these sub-problems, we further design a reliability-aware task-to-ES assignment algorithm, a QoE-optimum resource allocation algorithm, and a binary particle swarm optimization based task offloading algorithm. We perform extensive simulations and testbed experiments to validate the efficacy of the proposed scheme. Simulation and testbed results show that the proposed scheme greatly outperforms four benchmark approaches and it achieves up to 63.2% and 43.1% increase in the average QoE (quantified by offloading utility), respectively.},
  archive      = {J_TSC},
  author       = {Junlong Zhou and Xiangpeng Hou and Yue Zeng and Peijin Cong and Weiming Jiang and Song Guo},
  doi          = {10.1109/TSC.2025.3552338},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1683-1696},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Quality of experience and reliability-aware task offloading and scheduling for multi-user mobile-edge computing systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Puncturable signature and applications in privacy-aware data reporting for VDTNs. <em>TSC</em>, <em>18</em>(3), 1669-1682. (<a href='https://doi.org/10.1109/TSC.2025.3562318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vehicular digital twin networks (VDTNs), digital twin (DT) can assist the vehicle in data handling and report traffic data to the management server, thereby providing enhanced and scalable services for intelligent transport systems. However, the reported data may suffer from forgery and eavesdropping attacks due to the transmission on the open channel. In addition, a critical threat in VDTNs is the physical vehicle capture attack, namely, an adversary is capable of compromising the vehicle to obtain the current secret key, which can break the reliability of historical reported data and make the services provided by DT unavailable. Puncturable signature (PS) is a promising solution to eliminate these concerns, despite that the existing PS constructions have non-negligible false-positive errors and impose a significant cost on practical deployments. In this article, we design a novel PS and apply it to privacy-aware data reporting protocol (PA-DRP) for VDTNs. Specifically, the designed PS adopts a derivation-based way to achieve puncturing functionality, which is free from false-positive errors while extremely reducing the storage overhead of the secret keys. Meanwhile, we employ the designed PS to construct PA-DRP that enjoys authentication and forward security. Additionally, PA-DRP not only allows DT to remove privacy-sensitive information from the signed data but also provides fuzzy identity for protecting the real identity of the vehicle. Furthermore, the security analysis and performance evaluation demonstrate that the designed PS and PA-DRP not only can withstand various security and privacy assaults for VDTNs but also are efficient and practical.},
  archive      = {J_TSC},
  author       = {Chenhao Wang and Yang Ming and Hang Liu and Songnian Zhang and Rongxing Lu},
  doi          = {10.1109/TSC.2025.3562318},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1669-1682},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Puncturable signature and applications in privacy-aware data reporting for VDTNs},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proof of random leader: A fast and manipulation-resistant proof-of-authority consensus algorithm for permissioned blockchains using verifiable random function. <em>TSC</em>, <em>18</em>(3), 1655-1668. (<a href='https://doi.org/10.1109/TSC.2025.3536315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proof of Authority (PoA) is a widely adopted consensus algorithm for permissioned blockchain networks, where a group of trusted entities governs the network. PoA is known for achieving rapid consensus with minimal computational and energy requirements. However, existing PoA variants such as Aura and Clique suffer from low transaction throughput in high workload conditions and provide limited randomness in leader selection. They are also vulnerable to time and order manipulation attacks. To overcome these limitations, this paper introduces a novel PoA-based consensus algorithm called Proof of Random Leader (PoRL), which utilizes a verifiable random function to enhance transaction throughput, improve scalability, and ensure fair and unpredictable leader selection. The proposed PoRL algorithm was implemented in Python and evaluated using a network of six consensus nodes with varying computational capabilities. The performance of PoRL was assessed based on key metrics, including security, consistency, availability, fault tolerance, block time, and transaction throughput. Experimental results indicate that PoRL achieves lower consensus times and higher transaction throughput compared to Aura and Clique, making it a more efficient solution for permissioned blockchain networks. The findings of this study provide valuable insights for blockchain practitioners in selecting the most suitable PoA implementation based on their specific network requirements.},
  archive      = {J_TSC},
  author       = {Md. Mainul Islam and Mpyana Mwamba Merlec and Hoh Peter IN},
  doi          = {10.1109/TSC.2025.3536315},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1655-1668},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Proof of random leader: A fast and manipulation-resistant proof-of-authority consensus algorithm for permissioned blockchains using verifiable random function},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-enhanced federated expanded graph learning for secure QoS prediction. <em>TSC</em>, <em>18</em>(3), 1641-1654. (<a href='https://doi.org/10.1109/TSC.2025.3559613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current state-of-the-art QoS prediction methods face two main limitations. First, most existing QoS prediction approaches are centralized, gathering all user-service invocation QoS records for training and optimization, which causes privacy breaches. While some federated learning-based methods consider user privacy in a distributed way, they either directly upload local trained parameters or use simple encryption for global aggregation at the central server, thus failing to truly protect user privacy. Second, existing federated learning-based methods neglect distributed user-service topology and latent behavior-attribute correlations, compromising QoS prediction accuracy. To address these limitations, we propose a novel framework named Privacy-Enhanced Federated Expanded Graph Learning (PE-FGL) for secure QoS prediction. It first conducts user-service expansion on the invocation graph with advanced privacy-preserving techniques, upgrading first-order local QoS invocations to high-order interaction relationships. Then, it extracts hybrid features from the expanded invocation graph via deep learning and graph residual learning. Finally, a two-layer secure mechanism of federated parameters aggregation is designed to enable collaborative learning among users through local parameter segmentation and global aggregation, achieving effective and secure QoS prediction. Extensive experiments on WS-DREAM demonstrate effective QoS prediction across multiple metrics while preserving privacy in user-service invocations.},
  archive      = {J_TSC},
  author       = {Guobing Zou and Zhi Yan and Shengxiang Hu and Yanglan Gan and Bofeng Zhang and Yixin Chen},
  doi          = {10.1109/TSC.2025.3559613},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1641-1654},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Privacy-enhanced federated expanded graph learning for secure QoS prediction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PFedCal: Lightweight personalized federated learning with adaptive calibration strategy. <em>TSC</em>, <em>18</em>(3), 1627-1640. (<a href='https://doi.org/10.1109/TSC.2025.3553707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is a promising artificial intelligence framework that enables clients to collectively train models with data privacy. However, in real-world scenarios, to construct practical FL frameworks, several challenges have to be addressed, including statistical heterogeneity, constrained resources, and fairness. Therefore, we first investigate an aggregation gap caused by statistical heterogeneity during local model initialization, which not only causes additional computational overhead for clients but also leads to the degradation of fairness. To bridge this gap, we propose pFedCal, a novel personalized federated learning with lightweight adaptive calibration strategy that performs calibration compensation through the prior knowledge of clients. Specifically, we introduce compensation for each client at the model initialization, with the compensation derived from the global gradient and the latest gradient bias. To enhance the calibration effect, we introduce a smoothing-based calibration strategy, and we design an adaptive calibration strategy. A representative example demonstrates that the proposed calibration and smoothing strategies improve fairness for clients. The theoretical analysis indicates that with an appropriate learning rate, pFedCal converges to a first-order stationary point for non-convex loss functions. Comprehensive experimental results show that pFedCal achieves faster convergence, higher accuracy, and improved fairness than the state-of-the-art methods.},
  archive      = {J_TSC},
  author       = {Dongshang Deng and Xuangou Wu and Tao Zhang and Chaocan Xiang and Wei Zhao and Minrui Xu and Jiawen Kang and Zhu Han and Dusit Niyato},
  doi          = {10.1109/TSC.2025.3553707},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1627-1640},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {PFedCal: Lightweight personalized federated learning with adaptive calibration strategy},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online traffic allocation for video service providers in cloud-edge cooperative systems. <em>TSC</em>, <em>18</em>(3), 1618-1626. (<a href='https://doi.org/10.1109/TSC.2025.3565363'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, with the popularity of live video applications, VSPs (video service providers) begin to use cloud servers to enhance user experience and reduce operational costs. In this paper, we consider VSPs leveraging a cloud-edge cooperative model to deliver video services for cost reduction. Since bandwidth costs make up a significant portion of VSPs’ operating expenses, we mainly consider bandwidth cost optimizations in traffic allocation. In addition, the QoE (quality of experience) is also very important, while the latency has a larger impact on QoE. Thus our traffic allocation approach aims to strike a fine balance between minimizing bandwidth cost and bounding the latency experienced by clients. Such a trade-off is difficult to optimize with some prevailing bandwidth billing methods such as the 95th percentile bandwidth billing. We quantify such a trade-off by constructing a linear bandwidth cost optimization problem. We first describe the offline version of the optimization problem, and then design an online greedy algorithm that considers minimizing the current bandwidth cost at each time slot. By applying the Lyapunov optimization framework, we design another online algorithm based on the original greedy one. We prove that the time average delay achieved by our online algorithm is smaller than the upper bound we set when certain conditions are satisfied. Through extensive simulation experiments, we show that the proposed online algorithm can significantly reduce both the bandwidth cost and the time average delay of clients.},
  archive      = {J_TSC},
  author       = {Zhiwei Zhou and Li Pan and Shijun Liu},
  doi          = {10.1109/TSC.2025.3565363},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1618-1626},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Online traffic allocation for video service providers in cloud-edge cooperative systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OblivTime: Oblivious and efficient interval skyline query processing over encrypted time-series data. <em>TSC</em>, <em>18</em>(3), 1602-1617. (<a href='https://doi.org/10.1109/TSC.2025.3553698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series data is prevalent in many applications like smart homes, smart grids, and healthcare. And it is now increasingly common to store and query time-series data in the cloud. Despite the benefits, data privacy concerns in such outsourced services are pressing, making it imperative to embed privacy assurance mechanisms from the outset. Most existing related works have been focused on querying for different types of aggregate statistics. In this article, we instead focus on the secure support for advanced interval skyline queries, which allow to identify time series that are not dominated by any other time series within a query time interval. This is valuable for time-series data analytics in applications like remote health monitoring (e.g., identifying patients with high heart rates in a certain week). We present OblivTime, a new system framework for oblivious and efficient interval skyline query processing over encrypted time-series data. OblivTime is built from a synergy of time-series data analytics, lightweight cryptography, and GPU parallel computing, achieving stronger security guarantees and lower online query latency over the state-of-the-art prior work. Extensive experiments demonstrate that OblivTime can achieve up to $666\times$ speedup in online query latency over the state-of-the-art prior work.},
  archive      = {J_TSC},
  author       = {Huajie Ouyang and Yifeng Zheng and Songlei Wang and Zhongyun Hua},
  doi          = {10.1109/TSC.2025.3553698},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1602-1617},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {OblivTime: Oblivious and efficient interval skyline query processing over encrypted time-series data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSCCL: A framework for enhancing mashup service clustering with contrastive learning. <em>TSC</em>, <em>18</em>(3), 1588-1601. (<a href='https://doi.org/10.1109/TSC.2025.3565389'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining high-quality service function vectors and aggregating neighborhood features in service association graph are prevalent methods for Mashup service clustering. However, existing methods often focus on enhancing the service functional feature extraction while overlooking distinctions among different services when creating service function vectors. Additionally, neighborhood feature aggregation is typically considered within a single association graph, lacking contrast optimization of different association features. To address these challenges, we propose a novel framework, MSCCL (Mashup Service Clustering with Contrastive Learning). MSCCL consists of two core components: a service function vector generation module and a neighborhood feature aggregation module. Contrastive learning is employed to enhance vector quality and optimize feature aggregation in both modules. We present a service clustering method within MSSCL that combines techniques from BERT (Bidirectional Encoder Representations from Transformers) and GAT (Graph Attention Networks). Compared to state-of-the-art methods, this approach reduces DBI by 2.03% to 12.58%, while enhancing SC, NMI, and Purity by 2.24% to 15.47%, 3.34% to 11.39%, and 2.58% to 13.65%, respectively. Furthermore, the experiments demonstrate that the popular models for service function vector generation and neighborhood feature aggregation can all be integrated into MSSCL. After being integrated into MSSCL, the clustering performance of these models was significantly improved, highlighting the effectiveness and generalizability of MSSCL.},
  archive      = {J_TSC},
  author       = {Qiang Hu and Haoquan Qi and Shengzhi Du and Pengwei Wang},
  doi          = {10.1109/TSC.2025.3565389},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1588-1601},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {MSCCL: A framework for enhancing mashup service clustering with contrastive learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Monitoring inter-drone service interference for resilient operations. <em>TSC</em>, <em>18</em>(3), 1573-1587. (<a href='https://doi.org/10.1109/TSC.2025.3568245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel service-based framework for drone service resilience. Our framework monitors inter-drone interference that may lead to drone service failure. We present a novel drone service interference taxonomy to formally identify different interference types in a skyway network. We then propose a heuristic-based approach that leverages spatio-temporal proximity analysis to detect the occurrence of inter-drone interference. In addition, we present an interference severity assessment to quantify their impact on drone services’ efficiency. We conduct a set of experiments using real-world datasets to evaluate the effectiveness and efficiency of our proposed approach. The results indicate that the proposed heuristic-based approach detects the occurrence of inter-drone interferences with an accuracy of 95%. In addition, the proposed method is $\approx$70% more efficient than the baseline exhaustive approach and $\approx$48% faster than the K-means approach.},
  archive      = {J_TSC},
  author       = {Syeda Amna Rizvi and Athman Bouguettaya and Amani Abusafia and Abdallah Lakhdari and Vejaykarthy Srithar},
  doi          = {10.1109/TSC.2025.3568245},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1573-1587},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Monitoring inter-drone service interference for resilient operations},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ML-based intrusion detection as a service: Traffic split offloading and cost allocation in a multi-tier architecture. <em>TSC</em>, <em>18</em>(3), 1557-1572. (<a href='https://doi.org/10.1109/TSC.2025.3563680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An Intrusion Detection System (IDS) employing machine learning (ML) solutions is crucial for identifying network intrusions. To minimize operational expenses and enhance performance, enterprises have begun outsourcing IDS management to service providers, giving rise to the concept of Intrusion Detection as a Service (IDaS). Earlier research primarily aimed at enhancing the accuracy of ML-based IDS models or expediting their computational process. However, from the service provider’s perspective, an optimal architecture ensuring minimal computation cost and processing delay is crucial to increasing revenue. This study evaluates the performance of IDaS in a multi-tier architecture, utilizing traffic split offloading to enhance performance by mapping three in-sequence ML-based IDS tasks (pre-processing, binary detection, multi-class classification) to the architectures as the offloading destinations. We employ a simulated annealing-based traffic offloading and cost allocation (SA-TOCA) algorithm to determine the offloading ratio for each traffic path and the cost requirements for each tier. The results indicate that the edge-cloud architecture is 15% and four times more cost-effective compared to the fog-edge and fog-cloud architectures, respectively, and it demonstrates superior performance in minimizing processing delays. Offloading the majority of traffic to the edge and the remainder to the cloud proves to be an efficient strategy, reducing both computation costs and average delays.},
  archive      = {J_TSC},
  author       = {Didik Sudyana and Yuan-Cheng Lai and Ying-Dar Lin and Piotr Chołda},
  doi          = {10.1109/TSC.2025.3563680},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1557-1572},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {ML-based intrusion detection as a service: Traffic split offloading and cost allocation in a multi-tier architecture},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-cost data offloading strategy with deep reinforcement learning for internet of things. <em>TSC</em>, <em>18</em>(3), 1543-1556. (<a href='https://doi.org/10.1109/TSC.2024.3404347'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread adoption of the Internet of Things (IoT) and various smart medical devices, the volume of medical data has dramatically increased, making the processing of medical Internet of Things (IoMT) data increasingly challenging. Due to the integration of edge computing and cloud computing, IoMT can allocate increased computing and storage resources in proximity to the terminal, addressing the low-latency requirements of computationally intensive tasks. While existing initiatives have shifted services to edge servers, they have not taken into account the joint impact of task priorities and mobile computing services on Mobile Edge Computing (MEC) networks. Fortunately, the rapidly advancing field of Artificial Intelligence (AI) has proven effective in some resource allocation applications in recent years. In this article, we propose a mobile edge computing-based intelligent healthcare multitasking processing system aimed at addressing the issue of service prioritization in medical scenarios. Considering energy consumption and latency, we present a multi-objective task-aware service offloading algorithm under the framework of end-edge-cloud collaborative IoMT systems, employing deep deterministic policy gradients (DDPG). Adaptability to the diversity of different services is achieved through dynamic adjustments based on various business types and system requirements. Finally, the effectiveness of DDPG for IoMT is validated using real-world data.},
  archive      = {J_TSC},
  author       = {Qiang He and Zheng Feng and Zhixue Chen and Tianhang Nan and Kexin Li and Huiming Shen and Keping Yu and Xingwei Wang},
  doi          = {10.1109/TSC.2024.3404347},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1543-1556},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Low-cost data offloading strategy with deep reinforcement learning for internet of things},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LEAGAN: A decentralized version-control framework for upgradeable smart contracts. <em>TSC</em>, <em>18</em>(3), 1529-1542. (<a href='https://doi.org/10.1109/TSC.2025.3562323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart contracts are integral to decentralized systems like blockchains and enable the automation of processes through programmable conditions. However, their immutability, once deployed, poses challenges when addressing errors or bugs. Existing solutions, such as proxy contracts, facilitate upgrades while preserving application integrity. Yet, proxy contracts bring issues such as storage constraints and proxy selector clashes - along with complex inheritance management. This article introduces a novel upgradeable smart contract framework with version control, named ”decentraLized vErsion control and updAte manaGement in upgrAdeable smart coNtracts (LEAGAN).” LEAGAN is the first decentralized updatable smart contract framework that employs data separation with Incremental Hash (IH) and Revision Control System (RCS). It updates multiple contract versions without starting anew for each update, and reduces time complexity, and where RCS optimizes space utilization through differentiated version control. LEAGAN also introduces the first status contract in upgradeable smart contracts, and which reduces overhead while maintaining immutability. In Ethereum Virtual Machine (EVM) experiments, LEAGAN shows 40% better space utilization, 30% improved time complexity, and 25% lower gas consumption compared to state-of-the-art models. It thus stands as a promising solution for enhancing blockchain system efficiency.},
  archive      = {J_TSC},
  author       = {Gulshan Kumar and Rahul Saha and Mauro Conti and William Johnston Buchanan},
  doi          = {10.1109/TSC.2025.3562323},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1529-1542},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {LEAGAN: A decentralized version-control framework for upgradeable smart contracts},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latency-aware joint task offloading and energy control for cooperative mobile edge computing. <em>TSC</em>, <em>18</em>(3), 1515-1528. (<a href='https://doi.org/10.1109/TSC.2025.3553708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the application of the Internet of Things (IoT), existing cloud edge collaboration technologies face the problem of poor coordination of heterogeneous resources. In this article, we propose CFEMC, which is a novel Cloud-Fog-Edge Multi-layer Collaboration resource scheduling framework for IoT. First, we design a collaborative resource scheduling framework based on semi-distributed artificial intelligence. It can achieve collaborative optimization of cloud/edge computing resource allocation under the constraints of high reliability and low latency. Second, we present a workflow applications scheduling strategy based on the proposed collaborative resource scheduling framework. This can solve the problem of unstable computing performance and transmission bandwidth during the scheduling process. Finally, the extensive and real data supported simulation results show that CFEMC has advantages in terms of energy consumption, delay and throughput compared with other benchmark strategies. Against CEC Hu et al. 2023 and PSO Zeng et al. 2022, the average throughput increases by 16.37% and 24.21%, and the total queuing delay decreases by 54.23% and 58.12%, respectively.},
  archive      = {J_TSC},
  author       = {Weibei Fan and Fu Xiao and Yao Pan and Xiaobai Chen and Lei Han and Shui Yu},
  doi          = {10.1109/TSC.2025.3553708},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1515-1528},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Latency-aware joint task offloading and energy control for cooperative mobile edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KubeSPT: Stateful pod teleportation for service resilience with live migration. <em>TSC</em>, <em>18</em>(3), 1500-1514. (<a href='https://doi.org/10.1109/TSC.2025.3564888'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container orchestration systems, such as Kubernetes, streamline containerized application deployment. As more and more applications are being deployed in Kubernetes, there is an increasing need for rescheduling - relocating a running pod to different nodes - due to system upgrades, node failures, and load-balancing optimizations. Live migration, which transfers services from source nodes to target nodes with minimal downtime, is the ideal support for rescheduling. However, implementing live migration for pods that run stateful services is challenging, because Kubernetes manages pods as stateless. First, the current pod’s network namespace initialization process causes a mismatch in the network state between the migrated pod and internal containers. Second, migrating the memory state results in extended downtime. Third, Kubernetes operations on pods do not consider preserving the state of the pods. Therefore, we propose KubeSPT to achieve live migration of stateful pods in rescheduling scenarios. First, we synchronize the network state of pods and internal containers by controlling packet flow and implement fast service redirection. Second, we introduce a Hot Data and Lazy-Restore method for memory restoration to reduce migration downtime. Finally, we decouple pod migration operations from other Kubernetes operations to ensure compatibility with live migration. Experimental results show that KubeSPT reduces downtime by 86% –93% compared to current rescheduling methods.},
  archive      = {J_TSC},
  author       = {Hansheng Zhang and Song Wu and Hao Fan and Zhuo Huang and Weibin Xue and Chen Yu and Shadi Ibrahim and Hai Jin},
  doi          = {10.1109/TSC.2025.3564888},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1500-1514},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {KubeSPT: Stateful pod teleportation for service resilience with live migration},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating deep spiking Q-network into hypergame-theoretic deceptive defense for mitigating malware propagation in edge intelligence-enabled IoT systems. <em>TSC</em>, <em>18</em>(3), 1487-1499. (<a href='https://doi.org/10.1109/TSC.2025.3562355'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) systems are susceptible to compromise due to malware propagation, leading to the data breach and information theft. In this paper, we propose a proactive deception-oriented hypergame-theoretic malware propagation-mitigation (DHMPM) model between IoT nodes and edge devices under asymmetric information in edge intelligence (EI)-enabled IoT systems. We then explore malware-propagated deceptive defense strategies based on deep reinforcement learning. Specifically, IoT nodes and edge devices continually adjust their strategies based on obtained utilities under beliefs perceived by uncertainties from the game environment and system dynamics. Built upon the proposed game DHMPM, we next apply spiking neural networks (SNNs) into deep Q-network to form hypergame-theoretic deep spiking Q-network (HGDSQN), practically converging to the optimal malware-propagated deceptive defense strategy in EI-enabled IoT systems. Such SNNs can simulate biological brains with the pulse communication mechanism and break through the bottleneck of temporal processing in traditional models with deep neural networks, realizing intelligent decision-making and real-time malware defense. We eventually perform experimental simulations that assess the effect of attack arrival probability and learning rate on the optimal learning strategy selection, demonstrating the effectiveness of the proposed HGDSQN algorithm.},
  archive      = {J_TSC},
  author       = {Yizhou Shen and Carlton Shepherd and Chuadhry Mujeeb Ahmed and Shigen Shen and Shui Yu},
  doi          = {10.1109/TSC.2025.3562355},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1487-1499},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Integrating deep spiking Q-network into hypergame-theoretic deceptive defense for mitigating malware propagation in edge intelligence-enabled IoT systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving user QoE via joint trajectory and resource optimization in multi-UAV assisted MEC. <em>TSC</em>, <em>18</em>(3), 1472-1486. (<a href='https://doi.org/10.1109/TSC.2025.3568283'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a promising network architecture, Mobile Edge Computing (MEC), has been proven that can effectively reduce the end-to-end latency and the energy consumption. The Unmanned Aerial Vehicle (UAV) assisted MEC network, where the UAV can provide the computation offloading services for the mobile users, can further alleviate the huge deployment cost of static edge servers. However, it remains unsolved how multiple cooperative flying UAVs serve the ground users, especially considering that these UAVs may share the same wireless channel and can communicate with the users while flying. In this paper, we first propose the Age of Task (AoT) metric to measure the quality of experience, and then formulate the joint optimization problem to minimize the worst AoT among all the users. Based on the block coordinate descent (BCD) method, this problem is transformed into three non-convex programming sub-problems (i.e., the UAV-user association sub-problem, the UAV trajectory planning sub-problem and the transmit power optimization sub-problem). Specifically, the successive convex approximation (SCA) technique is exploited iteratively to deal with the non-convexity in the UAV trajectory and transmit power optimization. Numerical results show that the proposed scheme outperforms the benchmark offloading schemes in terms of AoT.},
  archive      = {J_TSC},
  author       = {Yang Gao and Jun Tao and Yifan Xu and Zuyan Wang and Yu Gao and Meiling Wang},
  doi          = {10.1109/TSC.2025.3568283},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1472-1486},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Improving user QoE via joint trajectory and resource optimization in multi-UAV assisted MEC},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit supervision-assisted graph collaborative filtering for third-party library recommendation. <em>TSC</em>, <em>18</em>(3), 1459-1471. (<a href='https://doi.org/10.1109/TSC.2025.3562349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Third-party libraries (TPLs) play a crucial role in software development. Utilizing TPL recommender systems can aid software developers in promptly finding useful TPLs. A number of TPL recommendation approaches have been proposed and among them graph neural network (GNN)-based recommendation is attracting the most attention. However, GNN-based approaches generate node representations through multiple convolutional aggregations, which is prone to introducing noise, resulting in the over-smoothing issue. In addition, due to the high sparsity of labelled data, node representations may be biased in real-world scenarios. To address these issues, this paper presents a TPL recommendation method named Implicit Supervision-assisted Graph Collaborative Filtering (ISGCF). Specifically, it takes the App-TPL interaction relationships as input and employs a popularity-debiased method to generate denoised App and TPL graphs. This reduces the noise introduced during graph convolution and alleviates the over-smoothing issue. It also employs a novel implicitly-supervised loss function to exploit the labelled data to learn enhanced node representations. Extensive experiments on a large-scale real-world dataset demonstrate that ISGCF achieves a significant performance advantage over other state-of-the-art TPL recommendation methods in Recall, NDCG and MAP. The experiments also validate the superiority of ISGCF in mitigating the over-smoothing problem.},
  archive      = {J_TSC},
  author       = {Lianrong Chen and Mingdong Tang and Naidan Mei and Fenfang Xie and Guo Zhong and Qiang He},
  doi          = {10.1109/TSC.2025.3562349},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1459-1471},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Implicit supervision-assisted graph collaborative filtering for third-party library recommendation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HGDRec:Next POI recommendation based on hypergraph neural network and diffusion model. <em>TSC</em>, <em>18</em>(3), 1445-1458. (<a href='https://doi.org/10.1109/TSC.2025.3562352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, next Point-of-Interest (POI) recommendation is essential for many location-based services, aiming to predict the most likely POI a user will visit next. Current research employs graph-based and sequential methods, which have significantly improved performance. However, there are still limitations: numerous methods overlook the fact that user intent is constantly changing and complex. Furthermore, prior studies have seldom addressed spatiotemporal correlations while considering differences in user behavior patterns. Additionally, implicit feedback contains noise. To address these issues, we propose a recommender model named HGDRec for the next POI recommendation. Specifically, we introduce an approach for extracting trajectory intent by integrating multi-dimensional trajectory representations to achieve a multi-level understanding of user trajectories. Then, by analyzing users’ long trajectories, we construct global hypergraph structures across spatiotemporal regions to comprehensively capture user behavior patterns. Additionally, to further optimize trajectory intent representation, we employ a feature optimization method based on the improved diffusion model. Extensive experiments on three real-world datasets validate the superiority of HGDRec over the state-of-the-art methods.},
  archive      = {J_TSC},
  author       = {Yinchen Pan and Jun Zeng and Ziwei Wang and Haoran Tang and Junhao Wen and Min Gao},
  doi          = {10.1109/TSC.2025.3562352},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1445-1458},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {HGDRec:Next POI recommendation based on hypergraph neural network and diffusion model},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning with blockchain-enhanced machine unlearning: A trustworthy approach. <em>TSC</em>, <em>18</em>(3), 1428-1444. (<a href='https://doi.org/10.1109/TSC.2025.3553709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing need to comply with privacy regulations and respond to user data deletion requests, integrating machine unlearning into IoT-based federated learning has become imperative. This article introduces an innovative framework that melds blockchain with federated learning, ensuring an immutable record of unlearning requests and actions. Our approach not only bolsters the trustworthiness and integrity of the federated learning model but also adeptly addresses efficiency and security challenges typical in IoT environments. Key contributions include a certification mechanism for the unlearning process, enhancement of data security and privacy, and optimization of data management. Experimental results on MNIST and CIFAR-10 datasets demonstrate the effectiveness of our approach, achieving 0% accuracy for unlearned classes while maintaining 77.74% and 42.65% overall model accuracy for MNIST and CIFAR-10, respectively. Our time complexity analysis shows that the blockchain integration introduces only 2 seconds of overhead per epoch, highlighting the practicality of our solution for IoT applications.},
  archive      = {J_TSC},
  author       = {Xuhan Zuo and Minghao Wang and Tianqing Zhu and Lefeng Zhang and Shui Yu and Wanlei Zhou},
  doi          = {10.1109/TSC.2025.3553709},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1428-1444},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Federated learning with blockchain-enhanced machine unlearning: A trustworthy approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing LLM QoS through cloud-edge collaboration: A diffusion-based multi-agent reinforcement learning approach. <em>TSC</em>, <em>18</em>(3), 1412-1427. (<a href='https://doi.org/10.1109/TSC.2025.3562362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) are widely used across various domains, but deploying them in cloud data centers often leads to significant response delays and high costs, undermining Quality of Service (QoS) at the network edge. Although caching LLM request results at the edge using vector databases can greatly reduce response times and costs for similar requests, this approach has been overlooked in prior research. To address this, we propose a novel Vector database-assisted cloud-Edge collaborative LLM QoS Optimization (VELO) framework that caches LLM request results at the edge using vector databases, thereby reducing response times for subsequent similar requests. Unlike methods that modify LLMs directly, VELO leaves the LLM's internal structure intact and is applicable to various LLMs. Building on VELO, we formulate the QoS optimization problem as a Markov Decision Process (MDP) and design an algorithm based on Multi-Agent Reinforcement Learning (MARL). Our algorithm employs a diffusion-based policy network to extract the LLM request features, determining whether to request the LLM in the cloud or retrieve results from the edge's vector database. Implemented in a real edge system, our experimental results demonstrate that VELO significantly enhances user satisfaction by simultaneously reducing delays and resource consumption for edge users of LLMs. Our DLRS algorithm improves performance by 15.0% on average for similar requests and by 14.6% for new requests compared to the baselines.},
  archive      = {J_TSC},
  author       = {Zhi Yao and Zhiqing Tang and Wenmian Yang and Weijia Jia},
  doi          = {10.1109/TSC.2025.3562362},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1412-1427},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Enhancing LLM QoS through cloud-edge collaboration: A diffusion-based multi-agent reinforcement learning approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Electricity cost minimization for multi-workflow allocation in geo-distributed data centers. <em>TSC</em>, <em>18</em>(3), 1397-1411. (<a href='https://doi.org/10.1109/TSC.2025.3562325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Worldwide, Geo-distributed Data Centers (GDCs) provide computing and storage services for massive workflow applications, resulting in high electricity costs that vary depending on geographical locations and time. How to reduce electricity costs while satisfying the deadline constraints of workflow applications is important in GDCs, which is determined by the execution time of servers, power, and electricity price. Determining the completion time of workflows with different server frequencies can be challenging, especially in scenarios with heterogeneous computing resources in GDCs. Moreover, the electricity price is also different in geographical locations and may change dynamically. To address these challenges, we develop a geo-distributed system architecture and propose an Electricity Cost aware Multiple Workflows Scheduling algorithm (ECMWS) for servers of GDCs with fixed frequency and power. ECMWS comprises four stages, namely workflow sequencing, deadline partitioning, task sequencing, and resource allocation where two graph embedding models and a policy network are constructed to solve the Markov Decision Process (MDP). After statistically calibrating parameters and algorithm components over a comprehensive set of workflow instances, the proposed algorithms are compared with the state-of-the-art methods over two types of workflow instances. The experimental results demonstrate that our proposed algorithm significantly outperforms other algorithms, achieving an improvement of over 15% while maintaining an acceptable computational time.},
  archive      = {J_TSC},
  author       = {Shuang Wang and He Zhang and Tianxing Wu and Yueyou Zhang and Wei Emma Zhang and Quan Z. Sheng},
  doi          = {10.1109/TSC.2025.3562325},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1397-1411},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Electricity cost minimization for multi-workflow allocation in geo-distributed data centers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-cloud continuum orchestration of critical services: A smart-city approach. <em>TSC</em>, <em>18</em>(3), 1381-1396. (<a href='https://doi.org/10.1109/TSC.2025.3568251'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart-city services are typically developed as closed systems within each city's vertical, communicating and interacting with cloud services while remaining isolated within each provider's domain. With the emergence of 5G private domains and the introduction of new M2M services focusing on autonomous systems, there is a shift from the cloud-based approach to a distributed edge computing paradigm, in a continuum orchestration. However, an essential component is missing. Current orchestration tools, designed for cloud-based deployments, lack robust workload isolation, fail to meet timing constraints, and are not tailored to the resource-constrained nature of edge devices. Therefore, new orchestration methods are needed to support MEC environments. This article addresses this gap. We propose an orchestration platform and its algorithms to facilitate the seamless orchestration of both cloud and edge-based services, encompassing both critical and non-critical services. This work extends the current Kubernetes orchestration platform to include a novel location-specific resource definition, a custom scheduler to accommodate real-time and legacy services, continuous service monitoring to detect sub-optimal states, and a refined load balancing mechanism that prioritizes the fastest response times.},
  archive      = {J_TSC},
  author       = {Rodrigo Rosmaninho and Duarte Raposo and Pedro Rito and Susana Sargento},
  doi          = {10.1109/TSC.2025.3568251},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1381-1396},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Edge-cloud continuum orchestration of critical services: A smart-city approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-view deep learning approach for predictive business process monitoring. <em>TSC</em>, <em>18</em>(3), 1368-1380. (<a href='https://doi.org/10.1109/TSC.2025.3562344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive business process monitoring (PBPM) is particularly valuable in dynamic business environments, and it can help organisations mitigate risks and optimise resource allocation. An interesting task in PBPM is next activity prediction (NAP), which allows the prediction of future activities that will be executed at a certain time based on ongoing business processes. Existing methods typically only utilise the order information of traces when predicting the next activity, without fully leveraging the attribute information present in the logs. Given the usefulness of these for NAP, combining them can help neural networks gain a deeper understanding of the actual business process. In this study, we propose a dual-view deep learning approach to fully extract and fuse the aforementioned two aspects of information. First, we treated traces as sequential texts and extracted the trace order information based on a long short-term memory based self-attention network. Then, we treated traces as unstructured images and captured the implicit attribute fusion information among events using a 12-layer residual network. Finally, two parts of information were fused for NAP. Experiments on 12 real-life event logs prove that the proposed approach is superior to state-of-the-art approaches, exhibiting good performance in accuracy, macro-precision, macro-recall, macro-F1-score, and macro-Gmean.},
  archive      = {J_TSC},
  author       = {Binbin Chen and Shuangyao Zhao and Qiang Zhang and Chunhua Tang and Leilei Lin},
  doi          = {10.1109/TSC.2025.3562344},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1368-1380},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Dual-view deep learning approach for predictive business process monitoring},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRL-based joint optimization of wireless charging and computation offloading for multi-access edge computing. <em>TSC</em>, <em>18</em>(3), 1352-1367. (<a href='https://doi.org/10.1109/TSC.2025.3556614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless-powered multi-access edge computing (WP-MEC), as a promising computing paradigm with the great potential for breaking through the power limitations of wireless devices, is facing the challenges of reliable task offloading and charging power allocation. Towards this end, we formulate a joint optimization problem of wireless charging and computation offloading in socially-aware D2D-assisted WP-MEC to maximize the utility, characterized by wireless devices’ residual energy and the strength of social relationship. To address this problem, we propose a deep reinforcement learning (DRL)-based approach with hybrid actor-critic networks including three actor networks and one critic network as well as with Proximal Policy Optimization (PPO) updating policy. Further, to prevent the policy collapse, we adopt the PPO-clip algorithm which limits the update steps to enhance the stability of algorithm. The experimental results show that the proposed algorithm can achieved superior convergence performance and, meanwhile, improves the average utility efficiently compared to other baseline approaches.},
  archive      = {J_TSC},
  author       = {Xinyuan Zhu and Fei Hao and Lianbo Ma and Changqing Luo and Geyong Min and Laurence T. Yang},
  doi          = {10.1109/TSC.2025.3556614},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1352-1367},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {DRL-based joint optimization of wireless charging and computation offloading for multi-access edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DIST: Distributed learning-based energy-efficient and reliable task scheduling and resource allocation in fog computing. <em>TSC</em>, <em>18</em>(3), 1336-1351. (<a href='https://doi.org/10.1109/TSC.2025.3568255'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents DIST, a novel distributed reinforcement learning-based (DRL) framework for energy-efficient and reliable task scheduling and resource allocation in fog computing, low-latency computing solutions driven by the rapid deployment of IoT devices, and time-sensitive applications. DIST is built based on a novel distributed Q-learning to enable fog nodes to learn an optimal strategy to balance energy consumption, task execution time, and system reliability. The main novelty includes a cooperative Dynamic Voltage and Frequency Scaling-enabled task scheduling policy that dynamically adjusts node energy level to ensure power consumption reduction without sacrificing deadline adherence or reliability. The results demonstrate that DIST reduces energy consumption by up to 52.26%, realizes 38% higher success rates, and reduces task wait times by up to 46.77%, compared with state-of-the-art algorithms.},
  archive      = {J_TSC},
  author       = {Elyas Oustad and Abolfazl Younesi and Mohsen Ansari and Sepideh Safari and Mohammad Arman Soleimani and Jörg Henkel and Alireza Ejlali},
  doi          = {10.1109/TSC.2025.3568255},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1336-1351},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {DIST: Distributed learning-based energy-efficient and reliable task scheduling and resource allocation in fog computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative edge server placement for maximizing QoS with distributed data cleaning. <em>TSC</em>, <em>18</em>(3), 1321-1335. (<a href='https://doi.org/10.1109/TSC.2025.3552337'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of contaminated data on Internet of Things (IoT) devices has the potential to undermine the accuracy of data-driven decision-making by altering the distribution of original data. Existing data cleaning methods primarily depend on cloud center or cloud-edge cooperation, leading to prolonged data transmission delays and reduced cleaning accuracy. In this study, we identify edge server placement as a crucial step aligned with data cleaning and view the collaborative edge server placement with distributed data cleaning (SPDC) as a holistic problem. We comprehensively quantify the complexity of our issue through the analysis of numerous scenarios. To address this problem, we introduce a novel distributed collaborative edge framework comprising two key stages: server placement and data cleaning. We propose an optimized clustering algorithm for the former, considering the data distribution on the IoT layer and the constraints of the edge layer. For the latter, we introduce a gossip-based data cleaning algorithm that fully utilizes edge collaboration to enhance data cleaning accuracy. The algorithm exhibits an approximate performance complexity of O($\ln m$), where $m$ represents the number of users’ tasks. Both theoretical analysis and experimental results reveal that our algorithm an average improvement in data cleaning accuracy of 9.02% and a reduction in delay of 36.61%, surpassing the performance of state-of-the-art works in various scenarios.},
  archive      = {J_TSC},
  author       = {Yuzhu Liang and Mujun Yin and Wenhua Wang and Qin Liu and Liang Wang and Xi Zheng and Tian Wang},
  doi          = {10.1109/TSC.2025.3552337},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1321-1335},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Collaborative edge server placement for maximizing QoS with distributed data cleaning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Business process remaining time prediction based on incremental event logs. <em>TSC</em>, <em>18</em>(3), 1308-1320. (<a href='https://doi.org/10.1109/TSC.2025.3562338'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predictive Process Monitoring (PPM) aims to predict the future state of running process instances to enable timely interventions to mitigate potential risks. As one of the most fundamental tasks in PPM, process remaining time prediction focuses on preventing timeout occurrences. While various deep learning-based approaches have been developed for this purpose, they often rely on pre-established static prediction models and struggle to maintain accurate predictions when the process undergoes dynamic changes, such as an expanding sales channels. To tackle this challenge, this paper proposes an incremental process remaining time prediction framework by continuously updating the prediction model based on an incremental event log. Specifically, a feature selection strategy is first introduced to extract effective features from event logs. Leveraging effective features can significantly improve the prediction quality by capturing the changes in process information. Then, three incremental log-based updating mechanisms, including period-based updating, quantity-based updating, and concept-drift-based updating, along with a reconstruction strategy, are proposed to dynamically adjust the prediction model in response to business changes. Finally, LSTM, Transformer, and Auto-encoder models are adapted and integrated into the proposed framework. The approach has been implemented and publicly released. Experimental evaluation using nine real-life event logs demonstrate that the proposed framework and its three instantiations (i.e., LSTM-based, Transformer-based, and Auto-encoder-based ones) outperform state-of-the-art techniques in terms of prediction accuracy.},
  archive      = {J_TSC},
  author       = {Na Guo and Cong Liu and Qi Mo and Jian Cao and Chun Ouyang and Xixi Lu and Qingtian Zeng},
  doi          = {10.1109/TSC.2025.3562338},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1308-1320},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Business process remaining time prediction based on incremental event logs},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoBDA: Model-driven reference architecture for automated big data analysis framework. <em>TSC</em>, <em>18</em>(3), 1293-1307. (<a href='https://doi.org/10.1109/TSC.2025.3536310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The formidable growth of the Internet of Things and quality of service requirements accelerate demands for edge-based data-driven services, a major goal of Industry 4.0 and Society 5.0. However, Big Data analysis (BDA) comprehends a diverged stepped process that consumes excessive semantic knowledge and extensive makespan. These limitations hamper the meaningful adoption of BDA and hinder achieving the Industry 4.0 and Society 5.0 goals. Therefore, a solution featuring agile, able to deliver edged-user-friendly automated BDA (AutoBDA), is one of the preferred ways to address the aforementioned concerns. Moreover, BDA is an evolving field in data science. Nevertheless, ad-hoc architectures inherit severe adaptability constraints. Furthermore, solutions that address unique or domain-specific requirements became accustomed to practice; however, they constrain inclusivity. Therefore, we perceived that a holistic modeling approach featuring inclusive and agile facilitates achieving the Industry 4.0 and Society 5.0 goals. Software reference architecture (SRA) is a well-known holistic approach and can alleviate ad-hoc concerns. Automatic service composition (ASC) is preferred to automate diverged-stepped processes. Therefore, we proposed a model-driven holistic SRA for the ASC-based AutoBDA, featuring inclusive and agile up to domain-independent data mining and machine learning requirements. Experiments demonstrate that our proposal efficiently and effectively achieved our objectives.},
  archive      = {J_TSC},
  author       = {Akila Siriweera and Incheon Paik},
  doi          = {10.1109/TSC.2025.3536310},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1293-1307},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {AutoBDA: Model-driven reference architecture for automated big data analysis framework},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Auto.gov: Learning-based governance for decentralized finance (DeFi). <em>TSC</em>, <em>18</em>(3), 1278-1292. (<a href='https://doi.org/10.1109/TSC.2025.3553700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized finance (DeFi) is an integral component of the blockchain ecosystem, enabling a range of financial activities through smart-contract-based protocols. Traditional Decentralized finance (DeFi) governance typically involves manual parameter adjustments by protocol teams or token holder votes, and is thus prone to human bias and financial risks, undermining the system's integrity and security. While existing efforts aim to establish more adaptive parameter adjustment schemes, there remains a need for a governance model that is both more efficient and resilient to significant market manipulations. In this paper, we introduce “Auto.gov”, a learning-based governance framework that employs a Deep Q-network (DQN) Reinforcement learning (RL) strategy to perform semi-automated, data-driven parameter adjustments. We create a DeFi environment with an encoded action-state space akin to the Aave lending protocol for simulation and testing purposes, where Auto.gov has demonstrated the capability to retain funds that would have otherwise been lost to price oracle attacks. In tests with real-world data, Auto.gov outperforms the benchmark approaches by at least 14% and the static baseline model by tenfold, in terms of the preset performance metric—protocol profitability. Overall, the comprehensive evaluations confirm that Auto.gov is more efficient and effective than traditional governance methods, thereby enhancing the security, profitability, and ultimately, the sustainability of DeFi protocols.},
  archive      = {J_TSC},
  author       = {Jiahua Xu and Yebo Feng and Daniel Perez and Benjamin Livshits},
  doi          = {10.1109/TSC.2025.3553700},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1278-1292},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Auto.gov: Learning-based governance for decentralized finance (DeFi)},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial impostors: An efficient and scalable scheme for location privacy preservation. <em>TSC</em>, <em>18</em>(3), 1262-1277. (<a href='https://doi.org/10.1109/TSC.2025.3562354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The progress of location-based services has led to severe concerns about location privacy leakage. However, existing methods are still incompetent for efficient and scalable location privacy preservation (LPP). They are often vulnerable to inference attacks with side information, or hard to be implemented due to the high computational complexity. In this article, we pursue the high protection quality with low computational complexity. We propose a scalable LPP method based on the paradigm of counterfeiting locations. To make fake locations extremely plausible, we forge them by synthesizing artificial impostors. The so-called artificial impostors refer to the synthesized traces that have similar semantic features to the actual traces, e.g., similar transition patterns, and do not contain any protected location, i.e., the exact location that needs to be protected. We devise two dedicated techniques: the station-based synthesis method and the population-level semantic model. We conduct the experiments on real datasets of two cities (Shanghai of China and Asturias of Spain) to validate the quality of privacy preservation, utility loss, and scalability of the proposed method. Based on these two datasets, the experimental results show that our method achieves the privacy preservation quality of 97.68% and 96.24%, respectively, and the time spent on building generators is only 144.96 seconds and 136.08 seconds, respectively. The experimental results also show that our method achieves a good trade-off between privacy and utility. Our study would give the research community new insights into improving the practicality of LPP paradigm via counterfeiting locations.},
  archive      = {J_TSC},
  author       = {Hao Tang and Kunfeng Chen and Zhiyang Xie and Cheng Wang},
  doi          = {10.1109/TSC.2025.3562354},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1262-1277},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Artificial impostors: An efficient and scalable scheme for location privacy preservation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARRQP: Anomaly resilient real-time QoS prediction framework with graph convolution. <em>TSC</em>, <em>18</em>(3), 1245-1261. (<a href='https://doi.org/10.1109/TSC.2025.3565376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of modern service-oriented architecture, ensuring Quality of Service (QoS) is of paramount importance. The ability to predict QoS values in advance empowers users to make informed decisions, ensuring that the chosen service aligns with their expectations. This harmonizes seamlessly with the core objective of service recommendation, which is to adeptly steer users towards services tailored to their distinct requirements and preferences. However, achieving accurate and real-time QoS predictions in the presence of various issues and anomalies, including outliers, data sparsity, grey sheep instances, and cold start scenarios, remains a challenge. Current state-of-the-art methods often fall short when addressing these issues simultaneously, resulting in performance degradation. In response, in this article, we introduce an Anomaly-Resilient Real-time QoS Prediction framework (called ARRQP). Our primary contributions encompass proposing an innovative approach to QoS prediction aimed at enhancing prediction accuracy, with a specific emphasis on improving resilience to anomalies in the data. ARRQP utilizes the power of graph convolution techniques, a powerful tool in graph-based machine learning, to capture intricate relationships and dependencies among users and services. By leveraging graph convolution, our framework enhances its ability to model and seize complex relationships within the data, even when the data is limited or sparse. ARRQP integrates both contextual information and collaborative insights, enabling a comprehensive understanding of user-service interactions. By utilizing robust loss functions, this approach effectively reduces the impact of outliers during the training of the predictive model. Additionally, we introduce a method for detecting grey sheep users or services that is resilient to sparsity. These grey sheep instances are subsequently treated separately for QoS prediction. Furthermore, we address the cold start problem as a distinct challenge by emphasizing contextual features over collaborative features. This approach allows us to effectively handle situations where newly introduced users or services lack historical data. Experimental results on the publicly available benchmark WS-DREAM 1 dataset demonstrate the framework's effectiveness in achieving accurate and timely QoS predictions, even in scenarios where anomalies abound.},
  archive      = {J_TSC},
  author       = {Suraj Kumar and Soumi Chattopadhyay},
  doi          = {10.1109/TSC.2025.3565376},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1245-1261},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {ARRQP: Anomaly resilient real-time QoS prediction framework with graph convolution},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end deep learning QoS prediction model based on temporal context and feature fusion. <em>TSC</em>, <em>18</em>(3), 1232-1244. (<a href='https://doi.org/10.1109/TSC.2025.3562324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing end-to-end quality of service (QoS) prediction methods based on deep learning often use one-hot encodings as features, which are input into neural networks. It is difficult for the networks to learn the information that is conducive to prediction. Aiming at the above problem, an end-to-end deep learning QoS prediction model based on a temporal context and feature fusion is proposed. In the proposed model, three blocks are designed for QoS prediction. Firstly, a user-service encoding conversion block is designed to convert the one-hot encodings of users and services into the latent features of users and services, which can make full use of the data in sparse matrices. Then a time feature extraction block is designed to extract time features based on the time-varying characteristics of QoS values. Finally, the time features are fused with the latent features of users and services to predict QoS values. The experimental results show that on existing datasets, the proposed model has better prediction accuracy than other advanced methods in response time and throughput.},
  archive      = {J_TSC},
  author       = {Peiyun Zhang and Jiajun Fan and Yutong Chen and Wenjun Huang and Haibin Zhu and Qinglin Zhao},
  doi          = {10.1109/TSC.2025.3562324},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1232-1244},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {An end-to-end deep learning QoS prediction model based on temporal context and feature fusion},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reliable distributed-cloud storage based on permissioned blockchain. <em>TSC</em>, <em>18</em>(3), 1216-1231. (<a href='https://doi.org/10.1109/TSC.2025.3565388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional single-cloud storage suffers from single point of failure, leading to low data availability. As a result, it fails to meet users’ demands for reliable cloud storage services. Therefore, the current cloud storage paradigm has shifted to distributed-cloud storage (e.g., multi-cloud storage, JointCloud storage), where users store multiple replicas of data across multiple Cloud Service Providers (CSPs). However, this imposes significant storage pressure on CSPs. To reduce costs and maximize profits, some malicious CSPs may delete user data, undermining trust in cloud services and hindering the growth of the cloud computing industry. To address this issue, we propose a novel distributed-cloud storage based on permissioned blockchain, which effectively reduces storage costs while ensuring data availability. Firstly, we integrate Byzantine Fault Tolerance in permissioned blockchain with erasure coding (EC) to replace the traditional multi-cloud multi-replica storage approach. This integration significantly reduces storage costs while providing an efficient means for data recovery. Based on blockchain, we further propose a data integrity auditing approach that eliminates reliance on semi-trusted third-party auditors and enables decentralized data integrity verification. Combined with this auditing approach, our EC-based data recovery approach ensures data availability while enhancing users’ trust in distributed-cloud storage. Theoretical analysis indicates that our scheme reduces storage overhead from $O(n)$ to $O(1)$ with $n$ CSPs while ensuring data availability. Meanwhile, experimental results demonstrate that computational overhead is reduced by approximately 78% compared to traditional multi-cloud multi-replica storage, achieving the cost-effective and highly reliable distributed-cloud storage.},
  archive      = {J_TSC},
  author       = {Kaimin Zhang and Xingwei Wang and Bo Yi and Min Huang and Lin Qiu and Enliang Lv and Jingjing Guo},
  doi          = {10.1109/TSC.2025.3565388},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1216-1231},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A reliable distributed-cloud storage based on permissioned blockchain},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A paradigm shift in service research: The case of service composition. <em>TSC</em>, <em>18</em>(3), 1213-1215. (<a href='https://doi.org/10.1109/TSC.2025.3552345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in artificial intelligence, particularly in machine learning and neural networks, have significantly influenced various domains, including service computing. Large Language Models (LLMs) are at the forefront of this transformation, introducing new paradigms for automation and decision-making. This paper examines the evolving impact of LLMs on service composition, a fundamental problem in service computing. By analyzing shifts in research approaches, methodologies, and system architectures, we highlight how LLM-driven automation challenges traditional composition techniques. The discussion provides insights into emerging opportunities, limitations, and research directions, emphasizing the need to rethink service composition in the era of AI-driven automation.},
  archive      = {J_TSC},
  author       = {Marco Aiello},
  doi          = {10.1109/TSC.2025.3552345},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1213-1215},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A paradigm shift in service research: The case of service composition},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel cross-chain hierarchical federated learning framework for enhancing service security and communication efficiency. <em>TSC</em>, <em>18</em>(3), 1199-1212. (<a href='https://doi.org/10.1109/TSC.2025.3562329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional federated learning (FL) uploads local models to a central server for model aggregation and suffers from server centralization. While blockchain-based FL addresses the issue of centralization, new challenges arise, including limited scalability of a single chain, expensive overhead of blockchain consensus, and inconsistent quality of uploaded models. This article proposes a new cross-chain-based FL (CBFL) framework. Specifically, we propose a three-layer cross-chain FL architecture consisting of a task-releasing chain, a relay chain, and local model uploading chains. The task-releasing chain is used for task issuers to release FL tasks and global model aggregation. The local model uploading chain manages local devices, stores local models and aggregates these local models. To verify the quality of local models, we propose a dual-criteria model quality inspection method based on cross entropy and cosine similarity to exclude substandard local models. We also propose hierarchical FL before global model aggregation to further reduce the communication overhead. Moreover, multi-signature is used to ensure the consistent transmission of models in the cross-chain process. Experiments corroborate that the proposed CBFL improves performance by about 50% compared to the existing BFL framework. Moreover, the proposed dual-criteria model quality inspection method has better robustness than Krum and Trimmed Mean.},
  archive      = {J_TSC},
  author       = {Li Duan and He Huang and Chao Li and Wei Ni and Bo Cheng},
  doi          = {10.1109/TSC.2025.3562329},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1199-1212},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A novel cross-chain hierarchical federated learning framework for enhancing service security and communication efficiency},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A developer-focused genetic algorithm for IoT application placement in the computing continuum. <em>TSC</em>, <em>18</em>(3), 1185-1198. (<a href='https://doi.org/10.1109/TSC.2025.3556641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of the Internet of Things (IoT) paradigm has led to an interest in applying it not only in tasks for the general public but also to stringent domains such as healthcare. However, the developers of these next-generation IoT applications must consider additional non-functional requirements related to the criticality of the processes they automate, such as low response times or low deployment costs, as well as technical constraints, which include organizational, legal and policy-related constraints on where data can be processed or stored. While the Computing Continuum paradigm emerges as a valuable alternative for placing such applications, identifying the deployments that satisfy all these requirements becomes a tough challenge. The NP-hard nature of the problem makes it impractical to manually find such a deployment, and traditional approaches fail to consider the technical constraints. In this article, we present the Genetic Algorithm for Application Placement (GAAP), an evolutionary computing-based meta-heuristic designed to help IoT application developers find deployments that satisfy their Quality of Service, business and technical constraints. Our evaluation of an Internet of Medical Things use case shows that GAAP supports larger scenarios than traditional approaches and gives IoT application developers more options while providing better scalability.},
  archive      = {J_TSC},
  author       = {Juan Luis Herrera and Alejandro Moya and Javier Berrocal and Juan Manuel Murillo and Elena Navarro},
  doi          = {10.1109/TSC.2025.3556641},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1185-1198},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A developer-focused genetic algorithm for IoT application placement in the computing continuum},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A continuous authentication framework for securing metaverse identities. <em>TSC</em>, <em>18</em>(3), 1171-1184. (<a href='https://doi.org/10.1109/TSC.2025.3553711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Metaverse, continuous authentication is essential for verifying the ongoing connection between a user’s physical identity and avatar, ensuring secure access to various services. This process is crucial for confirming identities, maintaining security, and preventing unauthorized activities that could compromise legitimate services. However, traditional biometric-based authentication methods are susceptible to threats such as impersonation, replay attacks, and disguise, primarily due to the difficulty in directly using biometric information to represent the connection between virtual and physical identities. To address these challenges, some studies have proposed using blockchain schemes to mitigate security threats. Despite this, these approaches often encounter issues like insufficient network protection for authentication connections, prolonged data processing times, and latency. To overcome these limitations, we propose a secure continuous authentication framework that leverages standard protocols such as QUIC and JWT to verify user identities efficiently. Our approach employs embedding models on edge devices to generate and transmit biometric data. In contrast, a deep learning-based model on the server validates the user’s credentials, ensuring both high performance and availability. Experimental results show that our QUIC and JWT-based protocol delivers superior security and effectiveness compared to traditional biometric approaches and blockchain-based methods, achieving an AUC of 0.97, an EER of 3.77, and an F1 score of 0.96.},
  archive      = {J_TSC},
  author       = {Sangsoo Han and Eunbi Hwang and YoonSik Kim and Taekyoung Kwon},
  doi          = {10.1109/TSC.2025.3553711},
  journal      = {IEEE Transactions on Services Computing},
  month        = {5-6},
  number       = {3},
  pages        = {1171-1184},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A continuous authentication framework for securing metaverse identities},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-domain survey on time-criticality in cloud computing. <em>TSC</em>, <em>18</em>(2), 1152-1170. (<a href='https://doi.org/10.1109/TSC.2025.3539197'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional cloud services and infrastructures are mainly designed to maximize utilization of resources and provide best-effort Quality-of-Service levels. However, many emerging use cases in both public and private cloud computing scenarios are time-critical in nature. For example, automated vehicles, smart cities, and automated factories, are all application domains characterized by the need for highly reliable and consistent low-latency services. The incorporation of predictable execution properties in cloud solutions is essential to meet these requirements. This paper provides an overview of the current research landscape in cloud computing, summarizing the key aspects to enable support of time-critical applications. The paper explores various levels of the typical cloud software stack: machine virtualization and containers, resource management and orchestration, fault tolerance, serverless computing, data storage and management, and communications.},
  archive      = {J_TSC},
  author       = {R. Andreoli and R. Mini and P. Skarin and H. Gustafsson and J. Harmatos and L. Abeni and T. Cucinotta},
  doi          = {10.1109/TSC.2025.3539197},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1152-1170},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A multi-domain survey on time-criticality in cloud computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When differential privacy meets query control: A hybrid framework for practical range query leakage quantification and mitigation. <em>TSC</em>, <em>18</em>(2), 1137-1151. (<a href='https://doi.org/10.1109/TSC.2024.3517316'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encrypted range schemes are becoming increasingly attractive for commercial databases, as they allow for confidential query service on encrypted databases hosted on remote servers. These schemes, by design, leak specific patterns such as access, volume, and search patterns. However, they are vulnerable to leakage-abuse attacks (LAAs) that exploit these patterns to reconstruct the plaintext databases. In response, the query control paradigms have emerged, with our preceding framework, RangeQC, being a notable example. These paradigms probe deeper into the intricacies of granular user query access control, advancing beyond past scheme-level efforts and acting as sentinels against the inadvertent leakage of delicate data patterns. While RangeQC aimed to regulate high-leakage queries through query control, it encountered usability impediments. Acknowledging that query control alone might be insufficient, we introduce an additional layer of protection in our evolved framework, RangeQC+. This fusion model combines query control with differential privacy-based data perturbation, a proactive strategy to muddle query responses and yield obfuscated leakage patterns. Complementing this approach, RangeQC+ incorporates refined, noise-resistant leakage metrics for accurate pattern analysis. Through comprehensive assessments and comparative analysis, RangeQC+ consistently showcases a balanced blend of enhanced performance, robust privacy, and user-friendly functionality.},
  archive      = {J_TSC},
  author       = {Xinyan Li and Yuefeng Du and Cong Wang},
  doi          = {10.1109/TSC.2024.3517316},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1137-1151},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {When differential privacy meets query control: A hybrid framework for practical range query leakage quantification and mitigation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VPFLI: Verifiable privacy-preserving federated learning with irregular users based on single server. <em>TSC</em>, <em>18</em>(2), 1124-1136. (<a href='https://doi.org/10.1109/TSC.2024.3520867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is widely used in neural network-based deep learning, which allows multiple users to jointly train a model without disclosing their data. However, the data quality of the users is not uniform, and some users with poor computing ability and outdated equipments called irregular ones may collect low-quality data and thus reduce the accuracy of the global model. In addition, the untrusted server may return wrong aggregation results to cheat the users. To solve these problems, we propose a verifiable privacy-preserving FL protocol with irregular users (VPFLI) based on single server. The protocol is privacy-preserving for the untrusted server and it is proved secure based on drop-tolerant homomorphic encryption. For low-quality datasets, their proportion would be decreased in the aggregation results in order to ensure the accuracy of the global model. Also, the aggregation results can be effectively verified by the users based on linear homomorphic hash. Moreover, VPFLI is proposed based on single server, which is more applicable in reality compare with the previous ones based on two non-colluding servers. The experiments show that VPFLI improves the accuracy of the model from 83.5% to 91.5% based on MNIST dataset compared to the traditional FL protocols.},
  archive      = {J_TSC},
  author       = {Yanli Ren and Yerong Li and Guorui Feng and Xinpeng Zhang},
  doi          = {10.1109/TSC.2024.3520867},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1124-1136},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {VPFLI: Verifiable privacy-preserving federated learning with irregular users based on single server},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vertical federated unlearning via backdoor certification. <em>TSC</em>, <em>18</em>(2), 1110-1123. (<a href='https://doi.org/10.1109/TSC.2025.3536312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical Federated Learning (VFL) offers a novel paradigm in machine learning, enabling distinct entities to train models cooperatively while maintaining data privacy. This method is particularly pertinent when entities possess datasets with identical sample identifiers but diverse attributes. Recent privacy regulations emphasize an individual's right to be forgotten, which necessitates the ability for models to unlearn specific training data. The primary challenge is to develop a mechanism to eliminate the influence of a specific client from a model without erasing all relevant data from other clients. Our research investigates the removal of a single client's contribution within the VFL framework. We introduce an innovative modification to traditional VFL by employing a mechanism that inverts the typical learning trajectory with the objective of extracting specific data contributions. This approach seeks to optimize model performance using gradient ascent, guided by a pre-defined constrained model. We also introduce a backdoor mechanism to verify the effectiveness of the unlearning procedure. Our method avoids fully accessing the initial training data and avoids storing parameter updates. Empirical evidence shows that the results align closely with those achieved by retraining from scratch. Utilizing gradient ascent, our unlearning approach addresses key challenges in VFL, laying the groundwork for future advancements in this domain.},
  archive      = {J_TSC},
  author       = {Mengde Han and Tianqing Zhu and Lefeng Zhang and Huan Huo and Wanlei Zhou},
  doi          = {10.1109/TSC.2025.3536312},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1110-1123},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Vertical federated unlearning via backdoor certification},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). User preference oriented service caching and task offloading for UAV-assisted MEC networks. <em>TSC</em>, <em>18</em>(2), 1097-1109. (<a href='https://doi.org/10.1109/TSC.2025.3536319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles (UAVs) have emerged as a new and flexible paradigm to offer low-latency and diverse mobile edge computing (MEC) services for user equipment (UE). To minimize the service delay, caching is introduced in UAV-assisted MEC networks to bring service contents closer to UEs. However, UAV-assisted MEC is challenged by the heavy communication overhead introduced by service caching and UAV’s limited energy capacity. In this article, we propose an online algorithm, OOA, that jointly optimizes caching and offloading decisions for UAV-assisted MEC networks, to minimize the overall service delay. Specifically, to improve the caching effectiveness and reduce the caching overhead, OOA employs a greedy algorithm to dynamically make caching decisions based on UEs’ preferences on services and UAVs’ historical trajectories, with the goal of maximizing the probability of successful offloading. To realize the rational utilization of energy from a long-term perspective, OOA decomposes the online problem into a series of single-slot problems by scaling the UAV’s energy constraint into the objective, and iteratively optimizes UAV trajectory and task offloading at each time slot. Theoretical analysis proves that OOA converges to a suboptimal solution with polynomial time complexity. Extensive simulations based on real world data further show that OOA can reduce the service delay by up to 33% while satisfying the UAV’s energy constraint, compared to three state-of-the-art algorithms.},
  archive      = {J_TSC},
  author       = {Ruiting Zhou and Yifeng Huang and Yufeng Wang and Lei Jiao and Haisheng Tan and Renli Zhang and Libing Wu},
  doi          = {10.1109/TSC.2025.3536319},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1097-1109},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {User preference oriented service caching and task offloading for UAV-assisted MEC networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-driven pattern mining on incremental data for stream analyzing service. <em>TSC</em>, <em>18</em>(2), 1081-1096. (<a href='https://doi.org/10.1109/TSC.2025.3536359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern mining, one of the data analysis approaches, provides meaningful assistance for various business services, such as product recommendation and marketing. However, certain real-world data contain uncertain characteristics, and some business services want to consider the uncertainty of data. Uncertain pattern mining is an advanced technique for discovering more useful patterns from uncertainty-driven data with uncertain information about items. However, although many business services create and process incremental data in real-time, most of the previous uncertain pattern mining techniques have limitations in analyzing incremental data since they mainly focus on processing static data. To address the limitations, we present a list-based uncertain pattern mining method that effectively analyzes incremental uncertainty-driven data in real time by scanning stream data only once. In addition, uncertainty-driven data analytics can be executed efficiently due to the list structure that is effective in construction and mining. The tests of performance for runtime, memory consumption, and scalability are performed using real datasets and synthetic datasets, which illustrate that the suggested technique reveals outstanding performance compared to state-of-the-art algorithms. The additional case study evaluations with concept-drifting tests as well as accuracy and significance tests demonstrate the practical applications of the algorithm and the quality of the extracted results.},
  archive      = {J_TSC},
  author       = {Myungha Cho and Hanju Kim and Yoonji Baek and Seungwan Park and Doyoon Kim and Doyoung Kim and Chanhee Lee and Bay Vo and Witold Pedrycz and Unil Yun},
  doi          = {10.1109/TSC.2025.3536359},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1081-1096},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Uncertainty-driven pattern mining on incremental data for stream analyzing service},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TrustPay: A dual-layer blockchain-based framework for trusted service transaction. <em>TSC</em>, <em>18</em>(2), 1068-1080. (<a href='https://doi.org/10.1109/TSC.2025.3534619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web service-oriented transactions have become an integral part of the Internet economy, with the mainstream transaction patterns relying primarily on cloud service markets. However, traditional service transaction methods have deficiencies in terms of both trust and scalability. Distrust between service provider (SP) and consumer (SC), particularly around online payments and data security, impedes the further growth of service transactions. Although blockchain-based transaction mechanisms have made notable progress in addressing trust issues, they still face performance bottlenecks. To tackle these challenges, this paper introduces TrustPay, a service transaction framework that leverages a dual-layer blockchain structure consisting of a parent chain and multiple subchains. The framework partitions the subchain network based on business, with each subchain dedicated to storing service invocation records generated within a specific business unit. The smart contract deployed on the parent chain will settle the invocation records in all subchain networks as transaction records and facilitate automatic transfers among blockchain accounts. This design leverages blockchain's inherent reliability while improving its scalability in large-scale scenarios. Additionally, a novel consensus protocol, REFEREE, is introduced and applied to the subchain network, ensuring efficient recording of invocation data and trusted verification among participants, further enhancing both trust and performance. Comparative experiments and analysis show that TrustPay's dual-layer blockchain structure and REFEREE protocol are not only reliable but also outperform baseline methods in terms of efficiency.},
  archive      = {J_TSC},
  author       = {Shengye Pang and Xinkui Zhao and Shuyi Yu and Jintao Chen and Shuiguang Deng and Jianwei Yin},
  doi          = {10.1109/TSC.2025.3534619},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1068-1080},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {TrustPay: A dual-layer blockchain-based framework for trusted service transaction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards close-to-zero runtime collection overhead: Raft-based anomaly diagnosis on system faults for distributed storage system. <em>TSC</em>, <em>18</em>(2), 1054-1067. (<a href='https://doi.org/10.1109/TSC.2024.3521675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed storage systems are fundamental infrastructures of today’s large-scale software systems such as cloud systems. Diagnosing anomalies in distributed storage systems is essential for maintaining software availability. Existing anomaly diagnosis approaches mainly rely on the run-time data including monitoring data and application logs. However, collecting and analyzing the run-time data requires huge computing, storage, and management costs. Typically, more fine-grained run-time data can reveal more symptoms of anomalies, but on the contrary, requires more computing, storage, and management costs. As a result, solving the anomaly diagnosis problem is a balancing between the quality of run-time data and system overhead or cost. In this paper, we take into account both data quality and system overhead or cost by introducing a new type of run-time data-Raft logs. Raft logs are naturally produced by distributed storage systems and collecting raft logs will not bring any extra system overhead. To verify the ability of Raft logs in reflecting anomalies, we conduct a comprehensive study on the interconnection between the anomalies and Raft logs. Based on the study, we propose an effective Raft-Based Anomaly Diagnosis approach named RBAD. For evaluation, we expose the first open-sourced comprehensive dataset with multiple runtime data containing both Raft logs, application logs and monitoring data. Experiments based on this dataset demonstrate RBAD’s superiority, outperforming monitoring-based methods by 15.38% and log-based methods by 53.10%.},
  archive      = {J_TSC},
  author       = {Lingzhe Zhang and Tong Jia and Mengxi Jia and Hongyi Liu and Yong Yang and Zhonghai Wu and Ying Li},
  doi          = {10.1109/TSC.2024.3521675},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1054-1067},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Towards close-to-zero runtime collection overhead: Raft-based anomaly diagnosis on system faults for distributed storage system},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TF-DDRL: A transformer-enhanced distributed DRL technique for scheduling IoT applications in edge and cloud computing environments. <em>TSC</em>, <em>18</em>(2), 1039-1053. (<a href='https://doi.org/10.1109/TSC.2025.3528346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous increase of IoT applications, their effective scheduling in edge and cloud computing has become a critical challenge. The inherent dynamism and stochastic characteristics of edge and cloud computing, along with IoT applications, necessitate solutions that are highly adaptive. Currently, several centralized Deep Reinforcement Learning (DRL) techniques are adapted to address the scheduling problem. However, they require a large amount of experience and training time to reach a suitable solution. Moreover, many IoT applications contain multiple interdependent tasks, imposing additional constraints on the scheduling problem. To overcome these challenges, we propose a Transformer-enhanced Distributed DRL scheduling technique, called TF-DDRL, to adaptively schedule heterogeneous IoT applications. This technique follows the Actor-Critic architecture, scales efficiently to multiple distributed servers, and employs an off-policy correction method to stabilize the training process. In addition, Prioritized Experience Replay (PER) and Transformer techniques are introduced to reduce exploration costs and capture long-term dependencies for faster convergence. Extensive results of practical experiments show that TF-DDRL, compared to its counterparts, significantly reduces response time, energy consumption, monetary cost, and weighted cost by up to 60%, 51%, 56%, and 58%, respectively.},
  archive      = {J_TSC},
  author       = {Zhiyu Wang and Mohammad Goudarzi and Rajkumar Buyya},
  doi          = {10.1109/TSC.2025.3528346},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1039-1053},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {TF-DDRL: A transformer-enhanced distributed DRL technique for scheduling IoT applications in edge and cloud computing environments},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SHELDB: Client storage aware homomorphic encrypted database processing framework with low communication overhead. <em>TSC</em>, <em>18</em>(2), 1026-1038. (<a href='https://doi.org/10.1109/TSC.2025.3539189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Database as a service (DBaaS) in cloud raises severe concern in terms of data security. Storing data in encrypted form may confirm data confidentiality. But, database processing cannot be supported in this encrypted form. Theoretically, homomorphic encryption is a solution to support direct encrypted data processing. In this work, we highlight one of the major challenges of FHE- encrypted query processing that demands huge data transfer requirements from cloud to client for final decryption at the end of SQL query execution. We show in light of Chosen Plaintext Attack (CPA) that in spite of performing conditional filtering through SQL queries over encrypted databases, the size of the resultant dataset cannot be less than the original size of the database. In this work, we make an effort to propose a new encrypted query processing framework termed as SHELDB which supports client storage compatibility and low communication overhead using block-wise final result transmission from cloud to client by extending the concept of TOP operator implementation in standard SQL. However, it is to be noted that realization of such optimization is not straightforward because of circuit-based implementation requirements with FHE gates. Our experimental demonstration shows that the proposed framework is capable of executing all TPC-C standard SQL queries with the aid of 8-core parallel processing within $\sim 12.65$ minutes for an encrypted database of $768 \times 9$ size with 16-bits elements each. Though the computation time is linear with the number of rows, we have explored map-reduce type parallel processing techniques to reduce the timing requirements for databases with larger rows. Consequently, our new query processing framework reduces the communication overhead from m to $\delta k$ rows ($1 \leq \delta \leq block$) where the encrypted database contains m rows, $\delta$ is the number of blocks to be transmitted each time with $k$ = ($m/block$) rows. In spite of $k$ being a controllable parameter according to client storage and $\delta$ is dependent on the query parameters, final security analysis explains why the proposed technique is general database attack-resistant.},
  archive      = {J_TSC},
  author       = {Tanusree Parbat and Ayantika Chatterjee},
  doi          = {10.1109/TSC.2025.3539189},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1026-1038},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {SHELDB: Client storage aware homomorphic encrypted database processing framework with low communication overhead},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable large model for unlabeled anomaly detection with trio-attention U-transformer and manifold-learning siamese discriminator. <em>TSC</em>, <em>18</em>(2), 1012-1025. (<a href='https://doi.org/10.1109/TSC.2025.3536306'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To identify pattern deviations in large-scale industrial infrastructures, anomaly detection is crucial yet challenging. Previous research has not adequately addressed the characteristics and deployment considerations in these complex scenarios. In this paper, we present InoU, a scalable anomaly detection framework to process unlabeled multivariate time-series data. We incorporate a VAE filter to ease impacts from noisy components in training materials. We propose a scalable trio-attention U-Transformer to construct the typical representation of high-dimensional streams and produce pseudo labels that enable the later training process. The ultra perception and intra-/ inter-flow attention mechanisms are delicately designed to aggregate information from different flows with variable granularities while keeping a global view of the data. Its nested structure helps to maintain high efficiency even when the model is scaled down. We introduce a Siamese discriminator that projects target data into manifolds, and collates discrepancies at the embedding level. This paradigm elevates detection performance far beyond segment-wise error comparison in prior works. We apply contrastive and adversarial learning techniques to optimize manifold projection and detection performance when processing unseen samples. Extensive experiments on five large-scale datasets demonstrate the effectiveness of InoU with an average F1-Score improvement of 5.58%, significantly outperforming the state-of-the-art.},
  archive      = {J_TSC},
  author       = {Muyan Yao and Dan Tao and Peng Qi and Ruipeng Gao},
  doi          = {10.1109/TSC.2025.3536306},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {1012-1025},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Scalable large model for unlabeled anomaly detection with trio-attention U-transformer and manifold-learning siamese discriminator},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PriVeriFL: Privacy-preserving and aggregation-verifiable federated learning. <em>TSC</em>, <em>18</em>(2), 998-1011. (<a href='https://doi.org/10.1109/TSC.2024.3451183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning provides a collaborative way to build machine learning models without sharing private data. However, attackers might infer private information from model updates submitted by participants, and the aggregator might maliciously forge the final aggregation results. Federated learning still faces data privacy and aggregation integrity challenges. In this paper, we combine inference attacks and information theory to analyze the sensitivity of different bits of model parameters. We conclude that not all bits of model parameters will leak privacy. This realization inspires us to propose a novel low-expansion homomorphic aggregation scheme based on Paillier homomorphic encryption (PHE) for safeguarding participants’ data privacy. Building upon this, we develop PriVeriFL-A, a privacy-preserving and aggregation-verifiable federated learning scheme that combines homomorphic hash function and signature. To prevent collusion attacks between the aggregator and malicious participants, we further improve our PHE-based scheme into a threshold PHE-based one, named PriVeriFL-B. Compared with the privacy-preserving federated learning scheme based on classic PHE, PriVeriFL-A reduces the communication overhead to 1.65%, and the encryption/decryption computation overhead to 0.88%. Both PriVeriFL-A and PriVeriFL-B can effectively verify the integrity of the global model, while maintaining an almost negligible communication overhead for integrity verification and protecting the privacy of participants’ data.},
  archive      = {J_TSC},
  author       = {Lulu Wang and Mirko Polato and Alessandro Brighente and Mauro Conti and Lei Zhang and Lin Xu},
  doi          = {10.1109/TSC.2024.3451183},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {998-1011},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {PriVeriFL: Privacy-preserving and aggregation-verifiable federated learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online service placement, task scheduling, and resource allocation in hierarchical collaborative MEC systems. <em>TSC</em>, <em>18</em>(2), 983-997. (<a href='https://doi.org/10.1109/TSC.2025.3536307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) pushes cloud computing capabilities to the network edge, which provides real-time processing and caching flexibility for service-based applications. Conventionally, the individual node solution is insufficient to tackle the increasing computation workload and provide diverse services, especially for unpredictable spatiotemporal service request patterns. To address this, we first propose a hierarchical collaborative computing (HCC) framework to serve users’ demands by reaping sufficient computing capability in Cloud, ubiquitous service area in edge layer, and idle resources in device layer. To better unleash the benefits of HCC and pursue long-term performance, we investigate heterogeneity-aware resource management by collaborative service placement, task scheduling, and resource allocation both in-node and cross-node. We then propose an online optimization framework that first decouples the decisions across different slots. For each instant mixed integer non-linear programming problem, we introduce the surrogate Lagrangian relaxation method to reduce complexity and design hybrid numerical techniques to solve the subproblems. Theoretical analysis and extensive simulation results demonstrate the efficiency of the HCC framework in decreasing system cost on devices, and our proposed algorithms can effectively utilize the resources in the collaborative space to achieve the trade-off between system cost minimization and service placement cost stability.},
  archive      = {J_TSC},
  author       = {An Du and Jie Jia and Schahram Dustdar and Jian Chen and Xingwei Wang},
  doi          = {10.1109/TSC.2025.3536307},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {983-997},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Online service placement, task scheduling, and resource allocation in hierarchical collaborative MEC systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSCNet: Multi-scale network with convolutions for long-term cloud workload prediction. <em>TSC</em>, <em>18</em>(2), 969-982. (<a href='https://doi.org/10.1109/TSC.2025.3536313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate workload prediction is crucial for resource allocation and management in large-scale cloud data centers. While many approaches have been proposed, most existing methods are based on Recurrent Neural Networks (RNNs) or their variants, focusing on short-term cloud workload prediction without considering or identifying the long-term changes and different periodic patterns of cloud workloads. Due to variations in user demands or workload dynamics, cloud workloads that appear stable in the short term often exhibit distinct patterns in the long term. This can lead to a significant decline in prediction accuracy for existing methods when applied to long-term cloud workload forecasting. To address these challenges and overcome the limitations of current approaches, we propose a Multi-Scale Network with Convolutions (MSCNet) for accurate long-term cloud workload prediction. MSCNet employs multi-scale modeling of the original cloud workload to effectively extract multi-scale features and different periodic patterns, learning the long-term dependencies among the cloud workload. Our core component, the Multi-Scale Block, combines the Multi-Scale Patch Block, Transformer Encoder, and Multi-Scale Convolutions Block for comprehensive multi-scale learning. This enables MSCNet to adaptively learn both short-term and long-term features and patterns of cloud workloads, resulting in accurate long-term cloud workload predictions. Extensive experiments are conducted using real-world cloud workload data from Alibaba, Google, and Azure to validate the effectiveness of MSCNet. The experimental results demonstrate that MSCNet achieves accurate long-term cloud workload prediction with a computational complexity of $O(L^{2}d)$, outperforming existing state-of-the-art methods.},
  archive      = {J_TSC},
  author       = {Feiyu Zhao and Weiwei Lin and Shengsheng Lin and Shaomin Tang and Keqin Li},
  doi          = {10.1109/TSC.2025.3536313},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {969-982},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {MSCNet: Multi-scale network with convolutions for long-term cloud workload prediction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MedShield: A fast cryptographic framework for private multi-service medical diagnosis. <em>TSC</em>, <em>18</em>(2), 954-968. (<a href='https://doi.org/10.1109/TSC.2025.3526369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The substantial progress in privacy-preserving machine learning (PPML) facilitates outsourced medical computer-aided diagnosis (MedCADx) services. However, existing PPML frameworks primarily concentrate on enhancing the efficiency of prediction services, without exploration into diverse medical services such as medical segmentation. In this article, we propose MedShield, a pioneering cryptographic framework for diverse MedCADx services (i.e., multi-service, including medical imaging prediction and segmentation). Based on a client-server (two-party) setting, MedShield efficiently protects medical records and neural network models without fully outsourcing. To execute multi-service securely and efficiently, our technical contributions include: 1) optimizing computational complexity of matrix multiplications for linear layers at the expense of free additions/subtractions; 2) introducing a secure most significant bit protocol with crypto-friendly activations to enhance the efficiency of non-linear layers; 3) presenting a novel layer for upscaling low-resolution feature maps to support multi-service scenarios in practical MedCADx. We conduct a rigorous security analysis and extensive evaluations on benchmarks (MNIST and CIFAR-10) and real medical records (breast cancer, liver disease, COVID-19, and bladder cancer) for various services. Experimental results demonstrate that MedShield achieves up to $2.4\times$, $4.3\times$, and $2\times$ speed up for MNIST, CIFAR-10, and medical datasets, respectively, compared with prior work when conducting prediction services. For segmentation services, MedShield preserves the precision of the unprotected version, showing a 1.23% accuracy improvement.},
  archive      = {J_TSC},
  author       = {Fuyi Wang and Jinzhi Ouyang and Xiaoning Liu and Lei Pan and Leo Yu Zhang and Robin Doss},
  doi          = {10.1109/TSC.2025.3526369},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {954-968},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {MedShield: A fast cryptographic framework for private multi-service medical diagnosis},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LogNotion: Highlighting massive logs to assist human reading and decision making. <em>TSC</em>, <em>18</em>(2), 940-953. (<a href='https://doi.org/10.1109/TSC.2025.3528327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Massive logs contain crucial information about the working status of software systems, which contributes to anomaly detection and troubleshooting. For engineers, it is a laborious task to manually inspect raw logs to know the system running status, and therefore an automated log summarization tool can be helpful. However, due to the specificity of logs in terms of grammar, vocabulary and semantics, existing natural language-based methods cannot perform well in log analysis. To address these issues, we propose LogNotion, a general log summarization framework that highlights the log messages to assist human reading and decision making. We first explore the role played by triplets in log analysis, and propose a triplet extraction method based on sequence tagging and component alignment, in which the specificity of logs is fully taken into account. Then, we propose an unsupervised log summarization method to extract both regular and noteworthy information based on triplets. Comprehensive experiments are conducted on seven real-world log datasets and the results show that LogNotion improves the average ROUGE-1 by 0.26, recall by 0.12, and compression ratio by 2.13%, compared to state-of-the-art log summarization tools. The helpfulness, readability and generalizability are also verified through human evaluation and cross-dataset tests.},
  archive      = {J_TSC},
  author       = {Guojun Chu and Jingyu Wang and Tao Sun and Qi Qi and Haifeng Sun and Zirui Zhuang and Jianxin Liao},
  doi          = {10.1109/TSC.2025.3528327},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {940-953},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {LogNotion: Highlighting massive logs to assist human reading and decision making},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Li-MSA: Power consumption prediction of servers based on few-shot learning. <em>TSC</em>, <em>18</em>(2), 926-939. (<a href='https://doi.org/10.1109/TSC.2025.3541555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power consumption prediction is one of the keys to optimize the energy consumption of servers. Existing traditional regression-based methods are too simple and poorly generalized, while popular deep learning methods require too much data. Therefore, they are difficult to be widely generalized. In this study, we propose a framework of linear interpolation multi-head sparse temporal pattern attention (Li-MSA) based on few-shot learning for power consumption prediction of servers with small-scale datasets in environments such as cloud data centers or edge computing. First, the interpolation reconstruction module extends and smooths the data. Then, the embedding learning module is used to narrow the scope of the hypothesis space. Finally, the multi-head sparse temporal pattern attention module emphasizes features and predicts power consumption. The results of the experiments show that Li-MSA outperforms the best results among the other methods for two datasets with different time steps in the RMSE metric by 15.34%, 17.35%, 18.18%, 6.28%, 4.05%, 7.73%.},
  archive      = {J_TSC},
  author       = {Saiqin Long and Yuan Li and Zhetao Li and Guoqi Xie and Weiwei Lin and Kenli Li},
  doi          = {10.1109/TSC.2025.3541555},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {926-939},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Li-MSA: Power consumption prediction of servers based on few-shot learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and privacy-preserving reconfigurable authentication scheme for IoT devices. <em>TSC</em>, <em>18</em>(2), 912-925. (<a href='https://doi.org/10.1109/TSC.2025.3536314'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) has revolutionized connectivity by enabling a large number of devices to autonomously exchange real-time data over the Internet. However, IoT devices used in public spaces are vulnerable to physical and cloning attacks. To address this issue, researchers have introduced the concept of physical-unclonable functions (PUFs) to enhance security in IoT applications. While PUF-based security solutions typically rely on static challenge-response behavior, many practical applications require dynamic or reconfigurable PUFs. For instance, PUF-based key storage may require updating or revoking secrets, and protection against modeling attacks, where an attacker can derive a PUF model from a set of challenge-response pairs (CRPs) using learning capabilities. In this paper, we introduce LR-OPUF, a reconfigurable one-time PUF, and propose a lightweight and privacy-preserving authentication scheme based on this LR-OPUF foundation. One notable feature of our authentication scheme is that it enables a device to prove its legitimacy to a semi-honest verifier without disclosing the CRPs. Through security and performance analyses, we demonstrate that our approach not only ensures vital security aspects but also exhibits high computational efficiency.},
  archive      = {J_TSC},
  author       = {Prosanta Gope and Fei Hongming and Biplab Sikdar},
  doi          = {10.1109/TSC.2025.3536314},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {912-925},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Lightweight and privacy-preserving reconfigurable authentication scheme for IoT devices},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging consortium blockchain for secure cross-domain data sharing in supply chain networks. <em>TSC</em>, <em>18</em>(2), 897-911. (<a href='https://doi.org/10.1109/TSC.2025.3544130'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supply Chain Networks (SCNs) play a vital role in achieving strategic decision-making for production and distribution facilities, aiming to meet market demands and gain competitive advantages. With the application of new-generation information technology in the supply chain, enterprises within SCNs generate a substantial volume of relevant business data. Sharing this data among SCN enterprises can effectively reduce operating costs, optimize business processes, and enhance the overall efficiency of the supply chain. However, effective data sharing among SCN participants faces challenges, such as data leakage, data quality assurance, and fair data value allocation. To address these challenges, this paper proposes a secure cross-domain data sharing model in SCNs (named SCN-CDSM) based on consortium blockchain technology. The model introduces trust, enables cross-domain data exchange, and promotes cooperation among supply chain enterprises. To ensure privacy, group signatures and access control smart contracts are designed, along with an approach to reduce blockchain throughput limitations. Furthermore, a sharing incentive mechanism utilizing the Stackelberg game model based on data value is designed to foster fairness and collaboration. Extensive numerical simulations are conducted to demonstrate the effectiveness of the proposed schemes, achieving both security and efficiency in data sharing within SCNs.},
  archive      = {J_TSC},
  author       = {Runqun Xiong and Jing Cheng and Xirui Dong and Jiahang Pu and Feng Shan},
  doi          = {10.1109/TSC.2025.3544130},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {897-911},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Leveraging consortium blockchain for secure cross-domain data sharing in supply chain networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning schema embeddings for service link prediction: A coupled matrix-tensor factorization approach. <em>TSC</em>, <em>18</em>(2), 883-896. (<a href='https://doi.org/10.1109/TSC.2025.3541552'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schema information is increasingly crucial to improve service discovery, recommendation, and composition, addressing link sparsity and lack of explainability inherent in methods relying solely on triples. However, existing approaches predominantly utilize schema information as a rigid filtering mechanism, equivalent to fixed conditions that lack the capability to adaptively adjust based on model learning. This paper introduces a novel learnable schema-aware knowledge embedding framework that enhances service link prediction by synergizing entity, relation, and type embeddings through a coupled matrix-tensor factorization model. To our knowledge, this is the first approach that couples entity and relation embeddings to enable adaptive learning of Schema Embeddings (SchemaE). Our framework is both expressive and easy to use, with the capability to generalize to existing bilinear models. Within this framework, we further propose the schema prompt method for embedding isolated nodes, which typically suffer from sparse relations or the absence of neighbors, leading to biased representation often overlooked in existing works. Despite embedding schema information, our model remains lightweight due to the introduction of a parameter-efficient strategy via type assists. We conduct extensive experiments on four public datasets, including comparisons with existing SOTA models, parameter analysis, performance validation on extended models, and visualization. The experimental results confirm the effectiveness and efficiency of the proposed model.},
  archive      = {J_TSC},
  author       = {Jing Yang and Xiaofen Wang and Laurence T. Yang and Yuan Gao and Shundong Yang and Xiaokang Wang},
  doi          = {10.1109/TSC.2025.3541552},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {883-896},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Learning schema embeddings for service link prediction: A coupled matrix-tensor factorization approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale service mesh orchestration with probabilistic routing in cloud data centers. <em>TSC</em>, <em>18</em>(2), 868-882. (<a href='https://doi.org/10.1109/TSC.2025.3526373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service mesh architectures are emerging as a promising microservice paradigm for developing online cloud applications. However, in large-scale microservice scenarios, frequent service communications, intricate call dependencies, and stringent latency requirements bring great pressure to efficient service mesh orchestration. In this case, the problems of service deployment and request routing based on service mesh architectures are tightly-coupled and interdependent, and cannot be effectively optimized individually, enlarging the difficulty for collaborative orchestration. When microservice multiplexing, parallel dependencies, and multi-instance modeling are considered, the difficulty is further aggravated. Nonetheless, most existing work failed to propose appropriate models and methods for the above challenges. Therefore, this article studies the large-scale service mesh orchestration with probabilistic routing and constrained bandwidths for parallel call graphs. We leverage the open Jackson queuing network theory to capture crucial microservices and analyze request processing, queuing, and communication latency for massive user requests in a fine-grained way. Then, this article proposes an efficient three-stage heuristic, which achieves elegant multi-instance consolidation and probabilistic multi-queue routing to reduce response latency and cost. We also provide the algorithm complexity and mathematical analysis of the performance. Finally, extensive trace-driven experiments are performed to validate the superiority of our proposed algorithm over other baselines.},
  archive      = {J_TSC},
  author       = {Kai Peng and Yi Hu and Haonan Ding and Haoxuan Chen and Liangyuan Wang and Chao Cai and Menglan Hu},
  doi          = {10.1109/TSC.2025.3526373},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {868-882},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Large-scale service mesh orchestration with probabilistic routing in cloud data centers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint trajectory optimization and resource allocation in UAV-MEC systems: A lyapunov-assisted DRL approach. <em>TSC</em>, <em>18</em>(2), 854-867. (<a href='https://doi.org/10.1109/TSC.2025.3544124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC), as a highly promising technology, effectively processes computation-intensive tasks by offloading them to edge servers. Utilizing the advantages of Unmanned Aerial Vehicles (UAVs) in deployment flexibility and broad coverage, UAV-assisted edge computing can significantly enhance system efficiency. This paper studies a scenario where a UAV-MEC system serves multiple Mobile Users (MUs) with random task arrivals and movements. We minimize the energy consumption of MUs by jointly optimizing UAV trajectory and resource allocation for MUs subjected to the UAV energy limit. The problem is formulated as a multi-stage Mixed-Integer Nonlinear Programming (MINLP) problem. To address this, we propose an algorithm called JTORA integrated Deep Reinforcement Learning (DRL) and Lyapunov optimization techniques. Specifically, we initially transform the multi-stage MINLP problem into a deterministic optimization problem utilizing Lyapunov techniques and decompose the original problem into two sub-problems in parallel. Through DRL, we solve the first sub-problem of trajectory and communication resources optimization. For the second sub-problem involving computing resource allocation, convex optimization is employed to get the optimal solution. Theoretical analysis and experimental results demonstrate that the JTORA algorithm can effectively reduce the energy consumption of MUs while ensuring UAV endurance.},
  archive      = {J_TSC},
  author       = {Ying Chen and Yaozong Yang and Yuan Wu and Jiwei Huang and Lian Zhao},
  doi          = {10.1109/TSC.2025.3544124},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {854-867},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Joint trajectory optimization and resource allocation in UAV-MEC systems: A lyapunov-assisted DRL approach},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint service deployment and task offloading for datacenters with edge heterogeneous servers. <em>TSC</em>, <em>18</em>(2), 839-853. (<a href='https://doi.org/10.1109/TSC.2025.3539199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computing (MEC) can improve execution efficiency and reduce overhead for offloading computing tasks to edge servers with more resources. In the microservice system, the current research only considers the cross segment communication cost of computing tasks, does not consider the case of the same end, and ignores the discovery and invocation optimization of associated services. In this paper, we propose CACO, which is a novel content-aware classification offloading framework for MEC based on correlation matrix. CACO first designs an adaptive service discovery model, which can make timely response and adjustment to the changes of the external environment. It then investigates an efficient affinity matrix based service discovery algorithm, which expresses the association relationship between services by constructing a service association matrix. In addition, CACO constructs a relational model by giving different weight coefficients to the delay and energy loss, which improves the delay and energy loss of message processing in a satisfying manner. Simulation results indicate that CACO reduces the total traffic of redundant messages by 46.2% $\sim$76.5%, respectively compared with state-of-the-art solutions. Testbed benchmarks show that it can also improve the stability by reducing control overhead by 34.5% $\sim$81.6% .},
  archive      = {J_TSC},
  author       = {Fu Xiao and Weibei Fan and Lei Han and Tie Qiu and Xiuzhen Cheng},
  doi          = {10.1109/TSC.2025.3539199},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {839-853},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Joint service deployment and task offloading for datacenters with edge heterogeneous servers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent transaction generation control for permissioned blockchain-based services. <em>TSC</em>, <em>18</em>(2), 828-838. (<a href='https://doi.org/10.1109/TSC.2025.3528318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the permissioned blockchain technology has been proposed to ensure data integrity in distributed systems, the low throughput and high latency have been recognized as major issues. In some applications, the data, available later than allowed time, can be useless, so the effective throughput is newly considered, defined as the average number of transactions per second, committed within the required latencies. For maximizing the effective throughput, we propose a novel intelligent transaction generation control (i-TGC) method to determine the transaction generation for each client. To improve performance in the dynamic environment of blockchain services based on real-time information, we employ the reinforcement learning (RL) for the i-TGC algorithm. Our experiment results show the i-TGC outperforms the probabilistic transaction generation control (p-TGC), which generates transactions randomly with the optimal probability that maximizes the effective throughput. We also verify the performance of the i-TGC for various environments with different block sizes, block generation timeout, traffic patterns, and the number of clients. The i-TGC can be a way to accelerate the usage of the permissioned blockchain for latency-sensitive services.},
  archive      = {J_TSC},
  author       = {Dongsun Kim and Sinwoong Yun and Sungho Lee and Jemin Lee and Dusit Niyato},
  doi          = {10.1109/TSC.2025.3528318},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {828-838},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Intelligent transaction generation control for permissioned blockchain-based services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated contrastive learning for cross-domain recommendation. <em>TSC</em>, <em>18</em>(2), 812-827. (<a href='https://doi.org/10.1109/TSC.2025.3528325'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional cross-domain recommendation models, which require centrally collecting varieties of original data from users, usually meet a challenge that users are reluctant to provide their real feedbacks because of privacy concerns. Thus, federated learning is incorporated into cross-domain recommendation, since it aggregates parameters of local models trained on user sides to train a global recommendation model, instead of centralized data collection. However, the deviations between the global model and local ones, which are caused by users’ data with non-independent and identical distributions, significantly challenge existing federated learning-based models in terms of alleviating data sparsity and cold-start problems. This article proposes a novel end-to-end federated contrastive learning-based model towards cross-domain recommendation, namely ${{Fed-CLR}}$. It first uses an inference model to characterize interaction distributions of users in source domain(s), then reconstructs historical interactions of users in target domain(s) with a generative model, and finally performs federated contrastive learning at model level (including inner-model and inter-model) to help reduce deviations between the global model and local ones. Particularly, a constraint mechanism, namely ${{Con-Mec}}$, is proposed to achieve consistency reinforcement from the aspect of inner- and inter-models. The experimental results on three real-world datasets not only show that ${{Fed-CLR}}$ outperforms the state-of-the-art peers, but also demonstrate that ${{Fed-CLR}}$ achieves a faster convergence speed than classical federated learning-based models.},
  archive      = {J_TSC},
  author       = {Qingren Wang and Yuchuan Zhao and Yi Zhang and Yiwen Zhang and Shuiguang Deng and Yun Yang},
  doi          = {10.1109/TSC.2025.3528325},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {812-827},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Federated contrastive learning for cross-domain recommendation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing federated learning through layer-wise aggregation over non-IID data. <em>TSC</em>, <em>18</em>(2), 798-811. (<a href='https://doi.org/10.1109/TSC.2025.3536309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, federated learning (FL) has been widely adopted to train deep neural networks (DNNs) among massive devices without revealing their local data in edge computing (EC). To relieve the communication bottleneck of the central server in FL, hierarchical federated learning (HFL), which leverages edge servers as intermediaries to perform model aggregation among devices in proximity, comes into being. Nevertheless, the existing HFL systems may not perform training effectively due to bandwidth constraints and non-IID issues on devices. To conquer these challenges, we introduce an HFL system with device-edge assignment and layer selection, namely Heal. Specifically, Heal organizes all the devices into a hierarchical structure (i.e., device-edge assignment) and enables each device to forward only a sub-model with several valuable layers for aggregation (i.e., layer selection). This processing procedure is called layer-wise aggregation. To further save communication resource and improve the convergence performance, we then design an iteration-based algorithm to optimize the development of our layer-wise aggregation strategy by considering the data distribution as well as resource constraints among devices. Extensive experiments on both the physical platform and the simulated environment show that Heal accelerates DNN training by about 1.4–12.5×, and reduces the network traffic consumption by about 31.9–64.1%, compared with the existing HFL systems.},
  archive      = {J_TSC},
  author       = {Yang Xu and Ying Zhu and Zhiyuan Wang and Hongli Xu and Yunming Liao},
  doi          = {10.1109/TSC.2025.3536309},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {798-811},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Enhancing federated learning through layer-wise aggregation over non-IID data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient and accuracy-aware DNN inference with IoT device-edge collaboration. <em>TSC</em>, <em>18</em>(2), 784-797. (<a href='https://doi.org/10.1109/TSC.2025.3536311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limited energy and computing resources of Internet of Things (IoT) devices, the collaboration of IoT devices and edge servers is considered to handle the complex deep neural network (DNN) inference tasks. However, the heterogeneity of IoT devices and the various accuracy requirements of inference tasks make it difficult to deploy all the DNN models in edge servers. Moreover, a large-scale data transmission is engaged in collaborative inference, resulting in an increased demand on spectrum resource and energy consumption. To address these issues, in this paper, we first design an accuracy-aware multi-branch DNN inference model and quantify the relationship between branch selection and inference accuracy. Then, based on the multi-branch DNN model, we aim to minimize the energy consumption of devices by jointly optimizing the selection of DNN branches and partition layers, as well as the computing and communication resources allocation. The proposed problem is a mixed-integer nonlinear programming problem. We propose a hierarchical approach to decompose the problem, and then solve it with a proportional integral derivative based searching algorithm. Experimental results demonstrate our proposed scheme has better inference performance and can reduce the total energy consumption up to 65.3$\%$, compared to other collaboration schemes.},
  archive      = {J_TSC},
  author       = {Wei Jiang and Haichao Han and Daquan Feng and Liping Qian and Qian Wang and Xiang-Gen Xia},
  doi          = {10.1109/TSC.2025.3536311},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {784-797},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Energy-efficient and accuracy-aware DNN inference with IoT device-edge collaboration},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient online computation of business process state from trace prefixes via N-gram indexing. <em>TSC</em>, <em>18</em>(2), 770-783. (<a href='https://doi.org/10.1109/TSC.2025.3547235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the following problem: Given a process model and an event log containing trace prefixes of ongoing cases of a process, map each case to its corresponding state (i.e., marking) in the model. This state computation operation is a building block of other process mining operations, such as log animation and short-term simulation. An approach to this state computation problem is to perform a token-based replay of each trace prefix against the model. However, when a trace prefix does not strictly follow the behavior of the model, token replay may produce a state that is not reachable from the initial state of the process. An alternative approach is to first compute an alignment between the trace prefix of each ongoing case and the model, and then replay the aligned trace prefix. However, (prefix-)alignment is computationally expensive. This paper proposes a method that, given a trace prefix of an ongoing case, computes its state in constant time on the length of the trace using an index that represents states as $n$-grams. An empirical evaluation shows that the proposed approach has an accuracy comparable to that of the prefix-alignment approach, while achieving a throughput of hundreds of thousands of traces per second.},
  archive      = {J_TSC},
  author       = {David Chapela-Campa and Marlon Dumas},
  doi          = {10.1109/TSC.2025.3547235},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {770-783},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Efficient online computation of business process state from trace prefixes via N-gram indexing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamically scheduling deadline-constrained interleaved workflows on heterogeneous computing systems. <em>TSC</em>, <em>18</em>(2), 758-769. (<a href='https://doi.org/10.1109/TSC.2025.3536317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous computing systems are extensively utilized to execute a wide range of time-critical services, which encompass numerous interdependent tasks organized in the form of workflows. In practice, the dynamic arrival of workflows often interleaves with their execution, leading to resource contention among multiple workflows and potentially causing QoS (Quality of Service) degradation. However, compared to the extensive research on single workflow scheduling, interleaved workflow scheduling has received relatively less attention. Moreover, the challenge of effectively scheduling limited computing resources to promptly complete consecutively arriving workflows remains underexplored, despite its practical importance. To fill this gap, this work proposes a method called Urgency-based List Scheduling (ULS) for dynamically scheduling deadline-constrained interleaved workflows. In ULS, a novel task property called urgency is introduced to prioritize tasks from multiple workflows by capturing real-time execution information, and each newly arrived workflow is scheduled with the outstanding tasks of prior workflows based on a list-based strategy to make more informed decisions. Extensive evaluation experiments are performed and the findings illustrate that ULS can achieve a reduction of at least 68% in deadline miss rates and 77% in overall tardiness compared to existing methods.},
  archive      = {J_TSC},
  author       = {Kun Cai and Quanwang Wu and Mengchu Zhou and Chao Chen and Junhao Wen and Shouguang Wang},
  doi          = {10.1109/TSC.2025.3536317},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {758-769},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Dynamically scheduling deadline-constrained interleaved workflows on heterogeneous computing systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DRMQ: Dynamic resource management for enhanced QoS in collaborative edge-edge industrial environments. <em>TSC</em>, <em>18</em>(2), 743-757. (<a href='https://doi.org/10.1109/TSC.2025.3539201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the fast-developing industrial environments, extensive focus on resource management within Mobile Edge Computing (MEC) aims to ensure low-latency QoS, however, some tasks offloaded to the cloud still experience high latency. Additionally, high energy consumption, poor link reliability, and excessive processing delays are intolerable for industrial applications. Compared to general servers, edge computing devices based on Arm architecture exhibit lower latency and higher energy efficiency. This highlights the need for improved heterogeneous Collaborative Edge-Edge Industrial Environments (CEIE) and precise multi-user QoS metrics. Thus, we focus on dynamic resource management within the CEIE architecture to better satisfy diverse industrial applications, formulating a multi-stage Mixed Integer Nonlinear Programming (MINLP) problem to minimize system costs. To reduce the computational complexity of solving the MINLP, we decompose the original problem into multi-user task offloading, Communication Resource Allocation (CmRA), and Computational Resource Allocation (CpRA) problems. These transformed problems are then tackled using DRMQ: an integrated learning optimization approach that combines model-free, priority experience replay-based Double Deep Q-Network (iDDQN) with model-based optimization, accelerating the Q-value function's convergence speed and reducing training time. Extensive simulations show that our proposed optimization scheme can reduce the average weighted system cost by at least 43.168% . Moreover, testbed experiments demonstrate that the proposed algorithm can reduce the average system cost by at least 42.650% in real-world applications, outperforming existing methods.},
  archive      = {J_TSC},
  author       = {Fengyi Huang and Wenhua Wang and Qin Liu and Wentao Fan and Jianxiong Guo and Weijia Jia and Jiannong Cao and Tian Wang},
  doi          = {10.1109/TSC.2025.3539201},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {743-757},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {DRMQ: Dynamic resource management for enhanced QoS in collaborative edge-edge industrial environments},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeFedGCN: Privacy-preserving decentralized federated GCN for recommender system. <em>TSC</em>, <em>18</em>(2), 729-742. (<a href='https://doi.org/10.1109/TSC.2025.3536320'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated recommender system (RS), a prevailing distributed paradigm, has been spawning significant interest in exploiting locally stored but tremendous data to predict items best aligned with clients. However, federated RS suffers severely from a single point of failure due to the dependency on the central server, leading to potential denial of service (DoS) attacks. To address this security weakness, in this paper, we propose a decentralized privacy-preserving federated graph convolutional network for RS, dubbed DeFedGCN. Specifically, DeFedGCN aggregates local updates by a decentralized consensus-reaching process and customizes local models for personalized recommendation, where the aggregation is enhanced by local differential privacy to resist model inversion attacks. More importantly, to promote the recommendation performance, DeFedGCN conducts a sub-graph expansion based on the private set interaction to explore high-order interactions among clients and items. Theoretical analysis confirms the effectiveness and privacy guarantee of DeFedGCN. Additionally, we conduct extensive experiments on four widespread real-world databases. The recommendation performance of DeFedGCN outperforms the state-of-the-art federated RS algorithms without security protection against DoS attacks by up to 7.4%.},
  archive      = {J_TSC},
  author       = {Qian Chen and Zilong Wang and Mengqing Yan and Haonan Yan and Xiaodong Lin and Jianying Zhou},
  doi          = {10.1109/TSC.2025.3536320},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {729-742},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {DeFedGCN: Privacy-preserving decentralized federated GCN for recommender system},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for scheduling applications in serverless and serverful hybrid computing environments. <em>TSC</em>, <em>18</em>(2), 718-728. (<a href='https://doi.org/10.1109/TSC.2024.3520864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing has gained popularity as a novel cloud execution model for applications in recent times. Businesses constantly try to leverage this new paradigm to add value to their revenue streams. The serverless eco-system accommodates many application domains successfully. However, its inherent properties such as cold start delays and relatively high per unit charges appear as a shortcoming for certain application workloads, when compared to a traditional Virtual Machine (VM) based execution scenario. A few research works exist, that study how serverless computing could be used to mitigate the challenges in a VM based cluster environment, for certain applications. In contrast, this work proposes a generalized framework for determining which workloads are best able to reap benefits of a serverless computing environment. In essence, we present a potential hybrid scheduling solution for exploiting the benefits of both a serverless and a VM based serverful computing environment. Our proposed framework leverages the actor-critic based deep reinforcement learning architecture coupled with the proximal policy optimization technique, in determining the best scheduling decision for workload executions. Extensive experiments conducted demonstrate the effectiveness of such a solution, in terms of user cost and application performance, with improvements of up to 44% and 11% respectively.},
  archive      = {J_TSC},
  author       = {Anupama Mampage and Shanika Karunasekera and Rajkumar Buyya},
  doi          = {10.1109/TSC.2024.3520864},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {718-728},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Deep reinforcement learning for scheduling applications in serverless and serverful hybrid computing environments},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning for mobility-aware digital twin migrations in edge computing. <em>TSC</em>, <em>18</em>(2), 704-717. (<a href='https://doi.org/10.1109/TSC.2025.3528331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decade witnessed an explosive growth on the number of IoT devices (objects/suppliers), including portable mobile devices, autonomous vehicles, sensors and intelligence appliances. To realize the digital representations of objects, Digital Twins (DTs) are key enablers to provide real-time monitoring, behavior simulations and predictive decisions for objects. On the other hand, Mobile Edge Computing (MEC) has been envisioned as a promising paradigm to provide delay-sensitive services for mobile users (consumers) at the network edge, e.g., real-time healthcare, AR/VR, online gaming, smart cities, and so on. In this paper, we study a novel DT migration problem for high quality service provisioning in an MEC network with the mobility of both suppliers and consumers for a finite time horizon, with the aim to minimize the sum of the accumulative DT synchronization cost of all suppliers and the total service cost of all consumers requesting for different DT services. To this end, we first show that the problem is NP-hard, and formulate an integer linear programming solution to the offline version of the problem. We then develop a Deep Reinforcement Learning (DRL) algorithm for the DT migration problem, by considering the system dynamics and heterogeneity of different resource consumptions, mobility traces of both suppliers and consumers, and workloads of cloudlets. We finally evaluate the performance of the proposed algorithms through experimental simulations. Simulation results demonstrate that the proposed algorithms are promising.},
  archive      = {J_TSC},
  author       = {Yuncan Zhang and Luying Wang and Weifa Liang},
  doi          = {10.1109/TSC.2025.3528331},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {704-717},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Deep reinforcement learning for mobility-aware digital twin migrations in edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BlockEmulator: An emulator enabling to test blockchain sharding protocols. <em>TSC</em>, <em>18</em>(2), 690-703. (<a href='https://doi.org/10.1109/TSC.2025.3547222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous blockchain simulators have been proposed to allow researchers to simulate mainstream blockchains. However, we have not yet found a testbed that enables researchers to develop and evaluate their new consensus algorithms or new protocols for blockchain sharding systems. To fill this gap, we developed BlockEmulator, which is designed as an experimental platform, particularly for emulating blockchain sharding mechanisms. BlockEmulator adopts a lightweight blockchain architecture so developers can only focus on implementing their new protocols or mechanisms. Using layered modules and useful programming interfaces offered by BlockEmulator, researchers can implement a new protocol with minimum effort. Through experiments, we test various functionalities of BlockEmulator in two steps. First, we prove the correctness of the emulation results yielded by BlockEmulator by comparing the theoretical analysis with the observed experiment results. Second, other experimental results demonstrate that BlockEmulator can facilitate measuring a series of metrics, including throughput, transaction confirmation latency, cross-shard transaction ratio, the queuing status of transaction pools, workload distribution across blockchain shards, etc. We have made BlockEmulator open-source in Github.},
  archive      = {J_TSC},
  author       = {Huawei Huang and Guang Ye and Qinglin Yang and Qinde Chen and Zhaokang Yin and Xiaofei Luo and Jianru Lin and Jian Zheng and Taotao Li and Zibin Zheng},
  doi          = {10.1109/TSC.2025.3547222},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {690-703},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {BlockEmulator: An emulator enabling to test blockchain sharding protocols},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavior tomographer: Identifying hidden cybercrimes by behavior interior structure modeling. <em>TSC</em>, <em>18</em>(2), 673-689. (<a href='https://doi.org/10.1109/TSC.2025.3539194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying hidden cybercrimes is a challenging task, as these behaviors are often carefully planned by criminals with counter-surveillance awareness. Existing solutions for cybercrime detection struggle to uncover enough clues to identify hidden criminal behaviors. Malicious behaviors are concealed beneath benign behaviors, and the boundaries between malicious and benign behaviors in the representation space are blurred to evade mainstream deep learning-based security authentication models. We introduce a behavior tomographer (BT) to reconstruct the behavior structure from three slices: agent, event, and attribute slices, enabling more granular detection of hidden cybercrimes. The core idea of BT is to reconstruct interior information about behavior structure from multiple slices, much like computed tomography in modern medicine enables the reconstruction of internal body. It enables the extraction of discriminative information from intricate interior associations between behavioral attributes rather than surface information meticulously crafted by criminals. Our experiments are conducted on two representative cybercrime datasets. Promising experimental results demonstrate that BT outperforms state-of-the-art models on key metrics, achieving around 0.99 AUC-ROC and approximately 0.9 AUC-PR. Moreover, BT notably excels at low false positive rates, showcasing its high effectiveness for real-world applications.},
  archive      = {J_TSC},
  author       = {Cheng Wang and Hangyu Zhu},
  doi          = {10.1109/TSC.2025.3539194},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {673-689},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Behavior tomographer: Identifying hidden cybercrimes by behavior interior structure modeling},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BackFillMe: An energy and performance efficient virtual machine scheduler for IaaS datacenters. <em>TSC</em>, <em>18</em>(2), 660-672. (<a href='https://doi.org/10.1109/TSC.2025.3539190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backfilling refers to the practice of allowing small jobs to be completed ahead of schedule as long as they do not cause the first job in the line to wait. Users are expected to offer estimates of how long jobs will take to complete in order to make these decisions possible, and these projections are often based on historical data. However, predictions are very hard and may not be accurate, particularly in cloud computing scenarios where jobs or applications run on Virtual Machines (VMs). In addition, scheduling and consolidation techniques can improve the energy efficiency and performance of applications. Consolidation involves VM migrations that can have a negative impact on workload performance and users’ costs. Backfilling can be used as an alternative technique for consolidation (short-term) and/or can be used along with consolidation (long-term). Backfilling methods are well-utilised in single computing systems, but are relatively unexplored in cloud resource allocation. A backfilling-based resource allocation and consolidation technique is proposed. Using real workloads from the Google cluster traces, we investigate the impact of backfilling on infrastructure energy efficiency and performance. For 12583 heterogeneous servers and approximately three million jobs that belong to three different applications, we observed that approximately 19% energy savings and 6% workload performance improvements are achievable using the backfilling approach. Furthermore, our evaluation suggests that using VM runtime as a criterion for the backfilling approach is approximately 3.56%–7.78% more energy and 1.91%–3.38% more performance efficient than using priority as a backfilling criterion.},
  archive      = {J_TSC},
  author       = {Muhammad Zakarya and Lee Gillam and Mohammad Reza Chalak Qazani and Ayaz Ali Khan and Khaled Salah and Omer Rana},
  doi          = {10.1109/TSC.2025.3539190},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {660-672},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {BackFillMe: An energy and performance efficient virtual machine scheduler for IaaS datacenters},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Availability and reliability of core networks (4G/5G) from a deployment standpoint. <em>TSC</em>, <em>18</em>(2), 647-659. (<a href='https://doi.org/10.1109/TSC.2025.3528332'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {4G LTE core networks or service based 5G core networks may be created as a set of virtual network functions (VNFs) i.e., network service (NS). The NS can be deployed with the use of cloud computing platform. A virtual machine (VM) with specialized software is denoted as VNF. In this present work, via mathematical modelling, we derive the NS availability considering the placement of core network nodes in a single virtual machine (SVM) as well as in multiple virtual machines (MVM). We consider the failure perspective of host node, VMs, and core network nodes in the availability analysis. We also look at NS reliability in terms of the placement of VNFs of NS in SVM as well as in MVM. After that, we examine the availability and reliability of SVM based NS and MVM based NS. Then, we compare the availability as well as the reliability considering SVM based NS and MVM based NS. Comparison results show that an SVM based NS deployment can lead to a more than 14% gain in availability and more than 100% gain in reliability with respect to an MVM based NS deployment.},
  archive      = {J_TSC},
  author       = {Priyatosh Mandal},
  doi          = {10.1109/TSC.2025.3528332},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {647-659},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Availability and reliability of core networks (4G/5G) from a deployment standpoint},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient replication-based aggregation verification and correctness assurance scheme for federated learning. <em>TSC</em>, <em>18</em>(2), 633-646. (<a href='https://doi.org/10.1109/TSC.2024.3520833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning(FL), enabling multiple clients collaboratively to train a model via a parameter server, is an effective approach to address the issue of data silos. However, due to the self-interest and laziness of servers, they may not correctly aggregate the global model parameters, which will cause the final model trained to deviate from the training goal. In the existing proposals, the cryptography-based verification scheme involves heavy computation overheads. On the other hand, the replication-based verification method, relying on a dual-server architecture, can ensure the correctness of aggregation and reduce computation overheads, but incur at least twice the communication cost as that of the task itself. To address these issues, we propose a novel replication-based aggregation scheme for FL, which enables efficient verification and stronger correctness assurance. The scheme employs a main-secondary server architecture, which allows the secondary servers to partakes in aggregation tasks at a predetermined probability, consequently mitigating the validation overhead. Moreover, we resort to the game theory and design a Learning Contract to impose penalties on dishonest servers, enforcing rational servers to correctly compute global model parameters. Under the use of Betrayal Contract to prevent collusion among servers, we further design a training game to efficiently verify global model parameters and ensure their correctness. Finally, we analyze the correctness of the proposed scheme and demonstrate that the computational overhead of our scheme is $\frac{{n + 1}}{{2n}}$ of the previous replication-based validation scheme, obtaining a significant reduction in communication cost, where $n$ means the training rounds. Experimental results further validate our deduction.},
  archive      = {J_TSC},
  author       = {Shihong Wu and Yuchuan Luo and Shaojing Fu and Yingwen Chen and Ming Xu},
  doi          = {10.1109/TSC.2024.3520833},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {633-646},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {An efficient replication-based aggregation verification and correctness assurance scheme for federated learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An anonymous, trust and fairness based privacy preserving service construction framework in mobile crowdsourcing. <em>TSC</em>, <em>18</em>(2), 618-632. (<a href='https://doi.org/10.1109/TSC.2025.3536318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of mobile smart devices with ever-improving sensing capacities means that Mobile Crowd Sensing (MCS) can economically provide a large-scale and flexible solution. However, existing MCSs face threats to privacy and fairness when recruiting workers due to information sensitivity, uncertainty about worker behavior, and budget constraints. To address the above issues, we propose an Anonymity, Trust, and Fairness in Privacy Protection (ATFPP) service construction framework to cost-effectively improve the quality of data at MCS. The main innovations are as follows: Firstly, on anonymity, in order to protect the privacy of workers, we propose a Privacy-Preserving (PP) framework based on an anonymous three-party platform, which realizes a full-process privacy-preserving scheme for workers. Second, on trust, we design more efficient Truth Discovery (TD) algorithm and adopt multifactor trust assessment method to identify more trustworthy workers. In addition, in terms of fairness, the fair distribution of compensation is realized through reasonable budget and approximate Shapley method. Finally, the proposed ATFPP scheme is theoretically proven to be correct and effective. Simulations based on real-world datasets illustrate that our ATFPP service construction scheme outperforms the state-of-the-art method in terms of both privacy protection and data quality.},
  archive      = {J_TSC},
  author       = {Xuechi Chen and Bochang Yang and Qian He and Shaobo Zhang and Tian Wang and Houbing Song and Anfeng Liu},
  doi          = {10.1109/TSC.2025.3536318},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {618-632},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {An anonymous, trust and fairness based privacy preserving service construction framework in mobile crowdsourcing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptively bias-extended non-negative latent factorization of tensors model for accurately representing the dynamic QoS data. <em>TSC</em>, <em>18</em>(2), 603-617. (<a href='https://doi.org/10.1109/TSC.2025.3544123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying quality-of-service (QoS) data are usually utilized for Web service evaluation and selection. To accurately estimate the unknown information in time-varying QoS data, it is crucial to capture the temporal patterns hidden in the known data. The Non-negative Latent Factorization of Tensors (NLFT) model has performed well in describing the temporal patterns in time-varying QoS data. However, it assigns a single bias to each dimension of the target QoS tensor, making it suffer from estimation accuracy loss when describing the fluctuations of time-varying QoS data. To address this vital issue, this paper proposes an Adaptively Bias-extended NLFT (ABNT) model based on the fuzzy logic with two-fold ideas: a) extending the linear biases on each dimension of tensor for describing the complex fluctuations of QoS data precisely, b) building a fuzzy logic-incorporated particle swarm optimization algorithm to establish a self-adaptation mechanism for the count of extended linear biases and regularization coefficients. Detailed algorithms and analyses are provided for the proposed ABNT model. Empirical studies on two practical time-varying QoS datasets indicate that the estimation accuracy of the ABNT model outperforms that of state-of-the-art QoS data estimation models (with an average 23.94% improvement in MAE).},
  archive      = {J_TSC},
  author       = {Xiuqin Xu and Mingwei Lin and Xin Luo and Zeshui Xu},
  doi          = {10.1109/TSC.2025.3544123},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {603-617},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {An adaptively bias-extended non-negative latent factorization of tensors model for accurately representing the dynamic QoS data},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive and interpretable congestion control service based on multi-objective reinforcement learning. <em>TSC</em>, <em>18</em>(2), 586-602. (<a href='https://doi.org/10.1109/TSC.2024.3517328'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The need for an adaptive congestion control (CC) service is crucial due to the heterogeneity of systems and the diversity of applications. Traditional CC methods often fail to adaptively balance throughput and delay, struggling to meet the varied demands of different network applications. In this work, we introduce Auto, a novel CC service that employs Multi-Objective Reinforcement Learning (MORL) to transcend these limitations. Unlike conventional approaches, Auto optimizes policies within a single model to cater to all potential preferences for balancing throughput and delay, making it ideal for diverse and heterogeneous network environments. To enhance operational transparency, we developed an interpretation algorithm that translates MORL into a human- readable decision tree, essential for service computing where clarity and interpretability are crucial. Furthermore, Auto allows users to explicitly set flow priorities and target sending rates, meeting varied application demands. Our extensive evaluations show that Auto not only consistently outperforms existing CC methods in diverse network conditions but also exhibits robustness to stochastic packet loss and rapid network changes. These capabilities establish Auto as a pioneering solution for next-generation congestion control in networking services.},
  archive      = {J_TSC},
  author       = {Jiacheng Liu and Xu Li and Feilong Tang and Peng Li and Long Chen and Jiadi Yu and Yanmin Zhu and Pheng-Ann Heng and Laurence T. Yang},
  doi          = {10.1109/TSC.2024.3517328},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {586-602},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {An adaptive and interpretable congestion control service based on multi-objective reinforcement learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaFlow: Learning and utilizing workflows for enhanced service recommendation in dynamic environments. <em>TSC</em>, <em>18</em>(2), 572-585. (<a href='https://doi.org/10.1109/TSC.2025.3547219'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service provisioning represents a nuanced form of recommendation, offering a bundle of services (APIs) tailored to the specifics needs of an application (mashup) as defined by the developer, significantly easing development efforts. Unlike standard product recommendations, service recommendations face unique challenges, including cold-start, long-tail phenomena, constraints, dynamic environments, and workflows. While the first four issues have seen some resolution in the literature, the workflow mining and integration among services remains underexplored. In this article, we focus on this gap by introducing AdaFlow, a model designed to understand and leverage service workflows within mashups, identifying viable service patterns for recommendations. AdaFlow employs a Graph Neural Network (GNN)-based framework, AdaptiveNN, to capture and learn service interactions. This learned workflow knowledge feeds into a dynamic GNN, enhancing service evolution representations that inform our recommendation process. Moreover, AdaFlow exhibits superior performance in managing dynamic and imbalanced scenarios.},
  archive      = {J_TSC},
  author       = {Mingyi Liu and Gensheng Wu and Hanchuan Xu and Jian Wang and Xiaofei Xu and Zhongjie Wang},
  doi          = {10.1109/TSC.2025.3547219},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {572-585},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {AdaFlow: Learning and utilizing workflows for enhanced service recommendation in dynamic environments},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A QoS prediction framework via utility maximization and region-aware matrix factorization. <em>TSC</em>, <em>18</em>(2), 557-571. (<a href='https://doi.org/10.1109/TSC.2025.3541554'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the surge of Web services, users are more concerned about Quality-of-Service (QoS) information when choosing Web services with similar functionalities. Today, effectively and accurately predicting QoS values is a tough challenge. Typically, traditional methods only use the QoS values provided by users to predict the missing QoS values, ignoring the arbitrariness of some users in providing observed QoS values and failing to consider the existence of anomalous QoS values with contingencies caused by some unstable Web services. Taking into account the above, this article proposes HyLoReF-us, a new framework for QoS prediction. HyLoReF-us uses the user reputation to measure the trustworthiness of users and the service reputation to measure the stability of web services. First, considering the utility generated by the invocation between users and Web services, HyLoReF-us employs a Logit model to calculate the user reputation and service reputation. Second, after combining the location information of users and services, as well as their reputations, HyLoReF-us obtains QoS predictions through an improved Matrix Factorization (MF) model. Finally, a series of experiments were conducted on the standard WS-DREAM dataset. Experimental results show that HyLoReF-us outperforms current state-of-the-art or baseline methods at Matrix Densities (MD) from 5% to 30%.},
  archive      = {J_TSC},
  author       = {Xia Chen and Yugen Du and Guoxing Tang and Fan Chen and Yingwei Luo and Hanting Wang},
  doi          = {10.1109/TSC.2025.3541554},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {557-571},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A QoS prediction framework via utility maximization and region-aware matrix factorization},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A malicious information popularity prediction model based on user influence. <em>TSC</em>, <em>18</em>(2), 543-556. (<a href='https://doi.org/10.1109/TSC.2025.3544122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social networks, studying methods for predicting the popularity of malicious information can help improve the ability to predict online public opinion. This paper proposes a malicious information popularity prediction model based on user influence, targeting the cooperative adversarial nature of malicious information propagation, the problem of assessing user influence in malicious information propagation space, and the complexity of malicious information propagation space. First, regarding the cooperative adversarial nature of the malicious information propagation process, considering that user behavior is influenced by both malicious and positive information during the propagation process, evolutionary game theory and multiple linear regression are introduced, and internal and external behavioral factors of the user are synthesized to construct influential functions that quantify malicious information and positive information. Meanwhile, the influence matrix is introduced when quantifying information to construct a weighted malicious information propagation network further. Second, regarding the problem of assessing user influence in the malicious information propagation space, considering the advantages of PageRank in measuring the importance of web pages and combining the timeliness of malicious information propagation, an improved algorithm T-PageRank (Timeliness-PageRank) based on timeliness is proposed. Introducing the time decay factor into the PageRank algorithm effectively enhances the accuracy and timeliness of the influence assessment of malicious information propagation. Finally, regarding the complexity of the propagation space of malicious information and considering that Graph Attention Network (GAT) can effectively capture complex relationships between nodes, combined with user influence, a malicious information popularity prediction model based on GAT is constructed. The model learns the complex interaction between users by using GAT and updates the feature representation of users so that it can be used for subsequent malicious information popularity prediction tasks. The experiment shows that the model can not only accurately assess the influence of users but also effectively predict the popularity of malicious information propagation.},
  archive      = {J_TSC},
  author       = {Tun Li and Yan Tang and Rong Xie and Yuqi Weng and Qian Li and Rong Wang and Chaolong Jia and Yunpeng Xiao},
  doi          = {10.1109/TSC.2025.3544122},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {543-556},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A malicious information popularity prediction model based on user influence},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid optimization framework for age of information minimization in UAV-assisted MCS. <em>TSC</em>, <em>18</em>(2), 527-542. (<a href='https://doi.org/10.1109/TSC.2025.3528339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {UAVs-enabled Mobile Crowdsensing (UMCS) has gained considerable attention recently, but it is challenging to meet the data collection needs of the entire city using only the UAV with limited energy. Furthermore, how to effectively minimize Age-of-Information (AoI) and ensure data quality has not been well solved in previous studies. Therefore, this paper proposes a hybrid optimization framework for AoI minimization, which recruits massive distributed workers as the main force for data collection, while the UAV acts as a data collection collaborator and is more inclined to fly to the SNs that cannot establish connections with workers, To mitigate the potential security threats incurred by dishonest workers of the MCS system, we first provide a Greedy-based Multi-worker Task Assignment (GMTA) strategy, aiming to assign more urgent data collection tasks to reliable workers under workload constraints. Then, we propose a Deep-Reinforcement-Learning-based Global AoI Minimization (DRL-GAM) strategy for the UAV path planning to find a set of optimal actions to minimize the global AoI. Based on the real dataset, our simulation experiments show that compared with traditional strategies, our DRL-GAM strategy can reduce the global AoI by an average of 6.49%$\sim$68.21% in various network sizes, and is more stable for the average standard deviation is only 51.75% of other strategies.},
  archive      = {J_TSC},
  author       = {Yuxin Liu and Qingyong Deng and Zhiwen Zeng and Anfeng Liu and Zhetao Li},
  doi          = {10.1109/TSC.2025.3528339},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {527-542},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A hybrid optimization framework for age of information minimization in UAV-assisted MCS},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A false positive resilient distributed trust management framework for collaborative intrusion detection systems. <em>TSC</em>, <em>18</em>(2), 513-526. (<a href='https://doi.org/10.1109/TSC.2025.3539202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative Intrusion Detection System (CIDS) protect large networks against distributed attacks. However, a CIDS is vulnerable to insider attacks that decrease the mutual trust among the CIDS nodes. Most existing trust management approaches rely on a central authority, trusted third parties or network peers for managing trust. The current techniques are prone to high false positives and vulnerable to various reputation attacks. For instance, device attestation manages trust among CIDS nodes by verifying the integrity of a node’s hardware and software configuration. However, it lacks real-time monitoring of the dynamic state, limiting its effectiveness against ongoing attacks and malware. Therefore, incorporating the system’s dynamic state in the trust framework is crucial, but it causes false positives requiring corrective mechanisms. To address these challenges, this paper proposes a blockchain-based integrated trust management framework for CIDS, incorporating the device’s genome attestation, the system’s dynamic parameters, and a false positive resilient reputation mechanism. By storing the reputation scores on the blockchain, the framework alleviates the need for a third party for trust management and thus mitigates attacks applicable to reputation-based systems. The paper performs a comprehensive security and performance analysis of the proposed framework to gauge its efficiency and study the effects of a penalty on a node’s reputation during the recovery and rally phases. We also study the impact of false positives on the reputation of a node. The results show that Hyperledger Fabric offers lower transaction latency and low CPU utilization compared to Ethereum Blockchain.},
  archive      = {J_TSC},
  author       = {Kadhim Hayawi and Imran Makhdoom and Saifullah Khalid and Richard Adeyemi Ikuesan and Mohammed Kaosar and Ishfaq Ahmad},
  doi          = {10.1109/TSC.2025.3539202},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {513-526},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A false positive resilient distributed trust management framework for collaborative intrusion detection systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A customized genetic algorithm for SLA-aware service provisioning in infrastructure-less vehicular cloud networks. <em>TSC</em>, <em>18</em>(2), 499-512. (<a href='https://doi.org/10.1109/TSC.2025.3528317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular Ad-hoc Networks (VANETs) and in-vehicle networks offer complementary perspectives on Intelligent Transportation Systems (ITS), enabling communication between vehicles and within individual vehicles, respectively. While VANETs focus on vehicle-to-vehicle communication, the growing demand for dynamic resource sharing and data processing across a fleet of vehicles highlights the need for Vehicular Cloud Networks (VCNs). VCNs, despite their lack of fixed infrastructure and the continuous mobility of vehicles, provide a promising solution for improving resource management and data sharing, making them critical for achieving efficient Service Level Agreements (SLAs) in infrastructure-less environments. This article addresses these challenges by employing a hierarchical clustering technique and proposing a novel mathematical formulation for resource provisioning in infrastructure-less vehicular clouds. The formulation considers diverse criteria, including provider and requester mobility, data volume, and service delay tolerance, to ensure SLA adherence. A customized genetic algorithm is used to solve the maximization problem, incorporating a grouping mechanism for efficient problem solving. Simulations using the NS2 network simulator and the IBM CPLEX optimization tool validate the feasibility of the proposed approach and demonstrate its superior performance compared to the other methods.},
  archive      = {J_TSC},
  author       = {Farhoud Jafari Kaleibar and Marc St-Hilaire and Masoud Barati},
  doi          = {10.1109/TSC.2025.3528317},
  journal      = {IEEE Transactions on Services Computing},
  month        = {3-4},
  number       = {2},
  pages        = {499-512},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A customized genetic algorithm for SLA-aware service provisioning in infrastructure-less vehicular cloud networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Telemedicine monitoring system based on Fog/Edge computing: A survey. <em>TSC</em>, <em>18</em>(1), 479-498. (<a href='https://doi.org/10.1109/TSC.2024.3506473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Telemedicine Monitoring (TM) integrates mobile communication technology and Internet of Things (IoT) technology for health monitoring and data management. Amidst the escalating demand for telemedicine, traditional cloud computing struggles to guarantee real-time performance and data privacy. To address these challenges, we systematically survey the application of fog and edge computing technologies in TM systems. We focus on the following key aspects: (1) We delve into the theoretical foundations of fog and edge computing, underscoring their salient advantages including low latency, location awareness, high mobility, and more. (2) We elaborate on the architecture of a TM system hinged on fog and edge computing. (3) We outline key challenges facing fog/edge computing-based TM systems, including bandwidth limitations, low latency, data security, privacy, heterogeneity, and reliability. (4) We discuss the need for future advancements in the realms of security defense capability, system adaptability, and convergence of scheduling algorithms to refine the construction of the TM system and stimulate the development of telemedicine.},
  archive      = {J_TSC},
  author       = {Qiang He and Zhaolin Xi and Zheng Feng and Yueyang Teng and Lianbo Ma and Yuliang Cai and Keping Yu},
  doi          = {10.1109/TSC.2024.3506473},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {479-498},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Telemedicine monitoring system based on Fog/Edge computing: A survey},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TFEGRU: Time-frequency enhanced gated recurrent unit with attention for cloud workload prediction. <em>TSC</em>, <em>18</em>(1), 467-478. (<a href='https://doi.org/10.1109/TSC.2024.3517324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of cloud workload is crucial for effective resource allocation in cloud computing. However, due to the complexity and high dimensionality of workloads in the cloud environment, achieving precise workload prediction is a complex and challenging problem. Current approaches to cloud workload prediction mainly rely on deep learning methods based on the Recurrent Neural Network (RNN), which struggle to capture the long-term dependencies inherent in workloads effectively. To tackle these challenges and overcome the limitations of existing methods, we propose an effective approach Time-Frequency Enhanced Gated Recurrent Unit with Attention (TFEGRU) for cloud workload prediction. First, we design a Time-Frequency Enhanced Block (TFEB) to capture complex workload patterns and extract features from both the frequency and temporal domains. Next, we integrate channel independent strategy and channel embedding into the model to adapt to high-dimensional workloads and enhance predictive performance. Finally, we apply a Gated Recurrent Unit (GRU) in conjunction with a multi-head self-attention mechanism to achieve accurate workload prediction. To validate the effectiveness of TFEGRU, comprehensive experiments are conducted using real-world traces from Google and Alibaba cloud data centers. The experimental results demonstrate that TFEGRU achieves accurate and efficient predictions across diverse cloud workloads, outperforming existing state-of-the-art methods.},
  archive      = {J_TSC},
  author       = {Feiyu Zhao and Weiwei Lin and Shengsheng Lin and Haocheng Zhong and Keqin Li},
  doi          = {10.1109/TSC.2024.3517324},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {467-478},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {TFEGRU: Time-frequency enhanced gated recurrent unit with attention for cloud workload prediction},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TaxiGuider: Pick-up service recommendation via multiple spatial-temporal trajectories. <em>TSC</em>, <em>18</em>(1), 453-466. (<a href='https://doi.org/10.1109/TSC.2024.3512952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle GPS devices provide abundant trajectory data that can be exploited to generate helpful pick-up service recommendations for taxi drivers. However, existing trajectory clustering approaches struggle to perform well on trajectory data with different distribution characteristics (e.g., dense in downtown and discrete in suburbs) simultaneously. Additionally, current prediction models mainly focus on subsection prediction but fail to produce accurate multisection predictions. To this end, we propose a recommendation framework, namely TaxiGuider, that can generate accurate pick-up cluster recommendations for taxi drivers. First, historical pick-up points are extracted from the entire vehicle trajectory data after preprocessing. Then, a graph Laplacian-based multiple spatial-temporal clustering approach is presented to generate clusters that can effectively match the distribution of trajectory data. Furthermore, a pick-up frequency prediction model that employs a multi-head attention mechanism is proposed to produce accurate multisection predictions that can help taxi drivers make comprehensive considerations for their next destination. Finally, top $N$ clusters with the highest predicted pick-up frequency are recommended to the target taxis according to their request. Experimental results on real-world datasets suggest that TaxiGuider outperforms state-of-the-art approaches in terms of both subsection and multisection predictions. Moreover, it produces pick-up cluster recommendations with superior prediction and classification accuracy simultaneously.},
  archive      = {J_TSC},
  author       = {Zhipeng Zhang and Mianxiong Dong and Kaoru Ota and Yao Zhang and Yonggong Ren},
  doi          = {10.1109/TSC.2024.3512952},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {453-466},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {TaxiGuider: Pick-up service recommendation via multiple spatial-temporal trajectories},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task offloading and resource pricing based on game theory in UAV-assisted edge computing. <em>TSC</em>, <em>18</em>(1), 440-452. (<a href='https://doi.org/10.1109/TSC.2024.3512936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the limited battery capacity and computational resources of mobile devices, computation-intensive tasks generated by mobile devices can be offloaded to edge servers for processing. This paper investigates the multi-user task offloading and resource pricing issues in Autonomous aerial vehicle (AAV)-assisted Multi-Access Edge Computing (MEC) systems. The optimization objectives is optimizing the utility of the server and the utility of the Edge Users (EUs), with decision variables encompassing the offloading strategies of EUs and the pricing strategies of the server. We divide the entire optimization problem into two parts. When optimizing the server's utility, server energy consumption is a crucial metric; hence, in the first part, we formulate the user allocation problem with the goal of minimizing the server's overall energy consumption. Utilizing game theory, we transform the user allocation problem into a multi-user non-cooperative game and prove the existence of a Nash Equilibrium (NE). The Game-based User Allocation (GBUA) algorithm is proposed to obtain the user allocation strategy. After addressing the user allocation problem, we consider the simultaneous optimization of both server and EUs utility. Therefore, in the second part, we model the server and EUs's engagement using the Stackelberg game model and employ backward induction to verify the presence of a Stackelberg Equilibrium (SE). Additionally, we propose the Resource Pricing and Task Offloading (RPATO) algorithm, based on game theory, to obtain the SE solution. Finally, extensive experiments are conducted to validate the effectiveness of the proposed algorithms, and numerous comparative algorithms are tested to prove the advancement and innovation of our proposed algorithms.},
  archive      = {J_TSC},
  author       = {Zhuoyue Chen and Yaozong Yang and Jiajie Xu and Ying Chen and Jiwei Huang},
  doi          = {10.1109/TSC.2024.3512936},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {440-452},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Task offloading and resource pricing based on game theory in UAV-assisted edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task allocation with geography-context-capacity awareness in distributed burstable billing edge-cloud systems. <em>TSC</em>, <em>18</em>(1), 427-439. (<a href='https://doi.org/10.1109/TSC.2024.3506475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new real-time interactive services, such as virtual and augmented reality, demand significantly higher network bandwidth and quality, which the traditional centralized cloud struggles to meet. In addition, centralized optimization management becomes inefficient as the scale of the scene continues to expand. In response, edge cloud systems have emerged, but distributed geographic locations, burstable billing business models, and large numbers of servers in large-scale scenarios pose new challenges for resource management. In this article, we propose GeoCC, a novel strategy to save bandwidth overhead in burstable billing edge cloud systems. GeoCC addresses challenges through a dual approach. First, a geography-aware graph construction and partitioning algorithm is used to organize server resources, and a large number of servers are reasonably divided into multiple server pools for parallel processing. Second, it introduces an enhanced burstable billing optimization mechanism that considers contextual factors and adaptive bandwidth capacity. Experiments based on real data from an edge cloud operator demonstrate the effectiveness of GeoCC. Compared with the baseline, GeoCC can effectively reduce bandwidth peaks, decreasing bandwidth costs by an average of 28.30% and up to 81.83% at the 95th percentile billing.},
  archive      = {J_TSC},
  author       = {Shihao Shen and Chenfei Gu and Yuanze Li and Chao Qiu and Xiaofei Wang and Rui Tan and Cheng Zhang and Wenyu Wang},
  doi          = {10.1109/TSC.2024.3506475},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {427-439},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Task allocation with geography-context-capacity awareness in distributed burstable billing edge-cloud systems},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Serv-HU: Service hand-off for UAV-as-a-service. <em>TSC</em>, <em>18</em>(1), 414-426. (<a href='https://doi.org/10.1109/TSC.2024.3521684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a UAV Service Hand-off scheme (Serv-HU) for the UAV-as-a-Service (UaaS) platform to provide seamless UAV services to the end-users. Traditionally, a service provider of a UaaS platform serves a limited application area due to the unavailability of adequate resources such as UAVs. Failing to deliver the service by the service providers for the requested entire application area by the end-user affects the reputation of the service providers. Consequently, the service delivery for a partial application area impacts the overall business, which is unacceptable for a Service-Oriented Architecture. To address this issue, we design a service hand-off scheme that enables the service providers to serve the entire requested application area by the end users with the help of other available service providers. We consider the presence of two types of service providers – Primary (PSP) and Secondary (SSP) in a UaaS platform. We apply a two-stage approach for the UAV service delivery to the end-users. In the first stage, a PSP optimally selects the SSPs for serving the uncovered application area by the PSP. The end-users request the service from the PSP, and on failing to provide the service for the entire application area, the PSP makes the service available from the optimally selected SSPs. In the second stage, we design an optimal pricing strategy that helps in determining the price charged to the end-users considering the involvement of PSPs and SSPs. We apply the Lagrangian multiplier method and Karush-Kuhn-Tucker (KKT) conditions to achieve the outcomes of these two stages. The simulation results depict that the charged price is reduced by $10.3 - 12.7\%$ while we apply the optimal SSP selection strategy as compared to the random selection of SSPs.},
  archive      = {J_TSC},
  author       = {Arijit Roy and Veera Manikantha Rayudu Tummala and Vinay Yadam},
  doi          = {10.1109/TSC.2024.3521684},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {414-426},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Serv-HU: Service hand-off for UAV-as-a-service},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RMDF-CV: A reliable multi-source data fusion scheme with cross validation for quality service construction in mobile crowd sensing. <em>TSC</em>, <em>18</em>(1), 399-413. (<a href='https://doi.org/10.1109/TSC.2024.3506482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Crowd Sensing is a prevalent and efficient paradigm for multi-source data collection, where Multi-source Data Fusion (MDF) plays a crucial role in constructing quality data collection services. Current MDF methods often require the majority of participating sensing sources to be credible, or assume that the workers’ credibility is either prior known or easily calculable. However, due to the presence of uncredible environments and the problem of Information Elicitation Without Verification (IEWV), these methods are impractical. It may lead to a vicious cycle where the recruitment of uncredible workers affects the quality of the estimated truth, which can further lead to misjudgments of worker credibility, thereby exacerbating the quality of subsequent recruitment. In this article, a Reliable Multi-source Data Fusion scheme with Cross Validation (RMDF-CV) is proposed to obtain reliable truth for service construction. Specifically, we first introduce the Combinatorial Multi-Armed Bandit (CMAB) model to recruit high-credibility workers by balancing exploration and exploitation. Then, we establish three-stage truth data through three different data sources: Unmanned Aerial Vehicles, credible workers, and Deep Matrix Factorization. Theoretical analyses and extensive simulations confirm the excellent performance of our RMDF-CV scheme.},
  archive      = {J_TSC},
  author       = {Kejia Fan and Jialin Guo and Runsheng Li and Yuanye Li and Anfeng Liu and Jianheng Tang and Tian Wang and Mianxiong Dong and Houbing Song},
  doi          = {10.1109/TSC.2024.3506482},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {399-413},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {RMDF-CV: A reliable multi-source data fusion scheme with cross validation for quality service construction in mobile crowd sensing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving competitive detour tasking in spatial crowdsourcing. <em>TSC</em>, <em>18</em>(1), 385-398. (<a href='https://doi.org/10.1109/TSC.2024.3511992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial crowdsourcing (SC) has recently emerged as a new crowdsourcing service paradigm, where workers move physically to designated locations to perform tasks. Most SC systems perform task assignment based on the spatial proximity between task locations and worker locations. Under such a strategy, workers can only perform tasks near them, which may result in low social welfare (i.e., the total profit of the platform and workers). In contrast, the newly emerging strategy of competitive task assignment (CTA) stimulates workers to compete for their preferred tasks, allowing optimization of the overall profit of SC systems. Among others, one novel CTA setting is competitive detour tasking, which allows workers to compete for tasks that need them to make detours from their original travel paths. However, it requires collecting each worker’s bidding profile which may expose private information. In light of this, in this article, we design, implement, and evaluate PrivCO, a new system framework enabling privacy-preserving competitive detour tasking services in SC. PrivCO delicately bridges state-of-the-art competitive detour tasking algorithms with lightweight cryptography, providing strong protections for workers’ bidding profiles. Extensive experiments over real-world datasets demonstrate that while offering strong security guarantees, PrivCO achieves social welfare comparable to the plaintext domain.},
  archive      = {J_TSC},
  author       = {Yifeng Zheng and Menglun Zhou and Songlei Wang and Zhongyun Hua and Jinghua Jiang and Yansong Gao},
  doi          = {10.1109/TSC.2024.3511992},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {385-398},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Privacy-preserving competitive detour tasking in spatial crowdsourcing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of heterogeneous device task runtime based on edge server-oriented deep neuro-fuzzy system. <em>TSC</em>, <em>18</em>(1), 372-384. (<a href='https://doi.org/10.1109/TSC.2024.3520869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the runtime of tasks is of great significance as it can help users better understand the future runtime consumption of the tasks and make decisions for their heterogeneous devices, or be applied to task scheduling. Learning features from user task history data for predicting task runtime is a mainstream method. However, this method faces many challenges when applied to edge intelligence. In the Big Data era, user devices and data features are constantly evolving, necessitating frequent model retrains. Meanwhile, the noisy data from these devices requires robust methods for valuable insight extraction. In this paper, we propose an edge server-oriented deep neuro-fuzzy system (ESODNFS) that can be trained and inferred on edge servers, for providing users with task runtime prediction services. We divided the dataset and trained it on multiple improved adaptive-network-based fuzzy inference system units (ANFISU), and finally conducted joint training on a deep neural network (DNN). By partitioning the dataset, we reduced the number of parameters for each ANFISU, and at the same time, multiple units can be trained in parallel, supporting fast training and iteration. Additionally, the application of fuzzy inference can effectively learn the features in noisy data and make accurate predictions. The experimental results show that ESODNFS can accurately predict the runtime of real tasks. Compared with other DNN and DNFS, it can achieve good prediction results while reducing training time by over 35%.},
  archive      = {J_TSC},
  author       = {Haijie Wu and Weiwei Lin and Wangbo Shen and Xiumin Wang and C. L. Philip Chen and Keqin Li},
  doi          = {10.1109/TSC.2024.3520869},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {372-384},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Prediction of heterogeneous device task runtime based on edge server-oriented deep neuro-fuzzy system},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PetTC: Pairwise joint embedding based contrastive tensor completion for network traffic monitoring services. <em>TSC</em>, <em>18</em>(1), 357-371. (<a href='https://doi.org/10.1109/TSC.2024.3517331'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network traffic matrices often suffer from incompleteness and sparsity due to various factors, including network device policies and system limitations. The incompleteness can undermine the reliability and accuracy of network traffic monitoring services, negatively impacting downstream tasks such as network planning and fault diagnosis. Our focus is on network traffic data recovery, intending to infer missing traffic data from partial measurements accurately. Although tensor completion algorithms are quite effective in recovering traffic data, existing models often overlook cross-domain traffic relationships and fail to account for the order and distribution of traffic, leading to reduced recovery accuracy. To overcome these limitations, we propose a new contrastive tensor completion model that utilizes pairwise joint embedding. This model employs innovative techniques, including a cross-domain embedding module to avoid information homogeneity and enhance model expressiveness, a contrastive module to preserve the order and distribution of traffic volumes, and an injective interaction module to map entry embeddings into the numerical space, ensuring convergence and retaining the original numerical distribution. Experiments on three real-world network traffic datasets show that our model significantly reduces the error in missing traffic data recovery compared to other existing models while maintaining traffic order and distribution.},
  archive      = {J_TSC},
  author       = {Haoxuan Wang and Kun Xie and Jigang Wen and Guangxing Zhang and Wei Liang and Gaogang Xie and Kenli Li},
  doi          = {10.1109/TSC.2024.3517331},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {357-371},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {PetTC: Pairwise joint embedding based contrastive tensor completion for network traffic monitoring services},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Participation-dependent privacy preservation in cross-silo federated learning. <em>TSC</em>, <em>18</em>(1), 342-356. (<a href='https://doi.org/10.1109/TSC.2024.3506479'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cross-silo federated learning (FL), clients of common interest cooperatively train a global model without sharing local sensitive data, but they still face potential privacy leakage due to privacy threats from malicious attackers. Although some articles have proposed effective privacy-preserving mechanisms for FL (such as differential privacy (DP)), clients in cross-silo FL are usually different companies or organizations who may behave selfishly to optimize their own benefits. In this article, we study DP-based cross-silo FL where clients selfishly decide their participation levels (i.e., data sizes for model trainings) and privacy leakage tolerance levels to trade off between model accuracy loss and privacy loss, and we model clients’ interactions as a participation-dependent privacy preservation game. It is challenging to analyze the game since the comprehensive impact of participation levels and privacy leakage tolerance levels on model accuracy is unclear and the behaviors of heterogeneous clients are coupled in a highly complex manner. To capture the impact of participation and privacy preservation behaviors, we first characterize the optimality gap of DP-based cross-silo FL for both convex and non-convex models, where the privacy leakage tolerance levels and the participation levels are coupled nonlinearly. We model clients’ costs based on the optimality gap, and prove that clients’ selfish participation-dependent privacy preservation game is a potential game. To analyze the optimal strategies of heterogeneous clients in a stable state, we derive the closed-form expression for the unique Nash equilibrium (NE), where clients may choose full participation or partial participation, and the equilibrium privacy preservation strategy depends on clients’ accuracy-privacy preference ratios. We analyze the social efficiency of the NE by calculating the price of anarchy (PoA) and show that the PoA increases with the number of clients and the heterogeneity of clients’ model accuracy preferences. To improve the social efficiency achieved at equilibrium, we design a socially efficient incentive mechanism that allows clients with large model accuracy preferences to compensate clients with small model accuracy preferences. Extensive experiments verify our theoretical results for both the convex and non-convex models as well as both the i.i.d. data distribution case and the non-i.i.d. data distribution case.},
  archive      = {J_TSC},
  author       = {Yanling Qin and Xiangping Zheng and Qian Ma and Guocheng Liao and Xu Chen},
  doi          = {10.1109/TSC.2024.3506479},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {342-356},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Participation-dependent privacy preservation in cross-silo federated learning},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online layer-aware joint request scheduling, container placement, and resource provision in edge computing. <em>TSC</em>, <em>18</em>(1), 328-341. (<a href='https://doi.org/10.1109/TSC.2024.3504237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containers have emerged as a pivotal tool for service deployment in edge computing. Before running the container, an image composed of several layers must exist locally. Recent strategies have utilized layer-sharing in images to reduce deployment delays. However, existing research only focuses on a single aspect of container orchestration, like container placement, neglecting the joint optimization of the entire orchestration process. To fill in such gaps, this article introduces an online strategy that considers layer-aware container orchestration, encompassing request scheduling, container placement, and resource provision. The goal is to reduce costs, adapt to evolving user demands, and adhere to system constraints. We present an online optimization problem that accounts for various real-world factors in orchestration, including container and server expenses. An online algorithm is proposed, integrating a regularization-based approach and stepwise rounding to address this optimization problem efficiently. The regularization approach separates time-dependent container placement and server wake-up costs, requiring only current information and past decisions. The stepwise rounding process generates feasible solutions that meet system constraints, reducing computational costs. Additionally, a competitive ratio proof is provided for the proposed algorithm. Extensive evaluations demonstrate that our approach achieves about 20% performance enhancement compared to baseline algorithms.},
  archive      = {J_TSC},
  author       = {Zhenzheng Li and Jiong Lou and Zhiqing Tang and Jianxiong Guo and Tian Wang and Weijia Jia and Wei Zhao},
  doi          = {10.1109/TSC.2024.3504237},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {328-341},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Online layer-aware joint request scheduling, container placement, and resource provision in edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ObliuSky: Oblivious user-defined skyline query processing in the cloud. <em>TSC</em>, <em>18</em>(1), 314-327. (<a href='https://doi.org/10.1109/TSC.2024.3512945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of cloud computing has spurred the popularity of storing and querying databases in the cloud. Among others, skyline queries play an important role in the database field due to its usefulness in multi-criteria decision support systems. To accommodate the tailored needs of users, user-defined skyline query has recently emerged, allowing users to define custom preferences in their skyline queries. However, user-defined skyline query services, if deployed in the cloud, may raise critical privacy concerns as the outsourced databases and skyline queries may contain proprietary/privacy-sensitive information. In light of the above, this paper presents ObliuSky, a new solution enabling oblivious user-defined skyline query processing in the cloud. ObliuSky departs from prior work by not only providing confidentiality protection for the content of the outsourced database, the user-defined skyline queries, and the query results, but also hiding the data patterns (e.g., user-defined dominance relations among database points and search access patterns) which may indirectly cause data leakages. We formally analyze the security guarantees and conduct extensive performance evaluations. The results show that while achieving much stronger security guarantees than the state-of-the-art prior work, ObliuSky is superior in database and query encryption efficiency, and scalable in oblivious query processing.},
  archive      = {J_TSC},
  author       = {Yifeng Zheng and Weibo Wang and Songlei Wang and Zhongyun Hua and Yansong Gao},
  doi          = {10.1109/TSC.2024.3512945},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {314-327},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {ObliuSky: Oblivious user-defined skyline query processing in the cloud},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Next PoI recommendation based on graph convolutional networks and multiple context-awareness. <em>TSC</em>, <em>18</em>(1), 302-313. (<a href='https://doi.org/10.1109/TSC.2024.3463500'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next Point-of-interest recommendation involves modeling user interactions with Point-of-interests (PoIs) to analyze user behavior patterns and suggest future scenarios. Data sparsity problems in PoI recommendations can significantly impact the performance of the recommendation model. This paper introduces the Graph Convolutional Network and Multiple Context-Aware PoI Recommendation model (GMCA). First, we present a weighted graph convolutional network that aims to capture the optimal representations of users and PoIs within the user-PoI interaction graph. Second, we employ a fine-grained approach to analyze user check-in records and cluster them into multiple user activity centers. Furthermore, we incorporate time, location, and social context information into the matrix decomposition process. Third, User activity centers are constructed by clustering user check-in records, and the geographical influence of PoI location on user behavioral patterns is explored using probabilistic factor decomposition. The evaluation of the GMCA model on the Yelp and Gowalla datasets shows a significant improvement in Precision@10 indicators. Specifically, there is a 13.85% increase in Precision@10 on the Yelp dataset and a 9.01% increase on the Gowalla dataset. The effectiveness of the GMCA model has been confirmed through numerous experiments conducted on two public datasets.},
  archive      = {J_TSC},
  author       = {Wei Zhou and Cheng Fu and Chunyan Sang and Min Gao and Junhao Wen},
  doi          = {10.1109/TSC.2024.3463500},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {302-313},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Next PoI recommendation based on graph convolutional networks and multiple context-awareness},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective deep reinforcement learning for function offloading in serverless edge computing. <em>TSC</em>, <em>18</em>(1), 288-301. (<a href='https://doi.org/10.1109/TSC.2024.3489443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Function offloading problems play a crucial role in optimizing the performance of applications in serverless edge computing (SEC). Existing research has extensively explored function offloading strategies based on optimizing a single objective. However, a significant challenge arises when users expect to optimize multiple objectives according to the relative importance of these objectives. This challenge becomes particularly pronounced when the relative importance of the objectives dynamically shifts. Consequently, there is an urgent need for research into multi-objective function offloading methods. In this paper, we redefine the SEC function offloading problem as a dynamic multi-objective optimization issue and propose a novel approach based on Multi-objective Reinforcement Learning (MORL) called MOSEC. MOSEC can coordinately optimize three objectives, i.e., application completion time, User Device (UD) energy consumption, and user cost. To reduce the impact of extrapolation errors, MOSEC integrates a Near-on Experience Replay (NER) strategy during the model training. Furthermore, MOSEC adopts our proposed Earliest First (EF) scheme to maintain the policies learned previously, which can efficiently mitigate the catastrophic policy forgetting problem. Extensive experiments conducted on various generated applications demonstrate the superiority of MOSEC over state-of-the-art multi-objective optimization algorithms.},
  archive      = {J_TSC},
  author       = {Yaning Yang and Xiao Du and Yutong Ye and Jiepin Ding and Ting Wang and Mingsong Chen and Keqin Li},
  doi          = {10.1109/TSC.2024.3489443},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {288-301},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Multi-objective deep reinforcement learning for function offloading in serverless edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-granularity weighted federated learning for heterogeneous edge computing. <em>TSC</em>, <em>18</em>(1), 270-287. (<a href='https://doi.org/10.1109/TSC.2024.3495532'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL), an advanced variant of distributed machine learning, enables clients to collaboratively train a model without sharing raw data, thereby enhancing privacy, security, and reducing communication overhead. However, in edge computing scenarios, there is an increasing trend towards diversity, heterogeneity, and complexity in clients’ data and models. The fundamental challenges, such as non-independent and identically distributed (non-IID) data and multi-granularity data accompanied by model heterogeneity, have become more evident and pose challenges to collaborative training among clients. In this paper, we refine the FL framework and propose the Multi-granularity Weighted Federated Learning (MGW-FL), emphasizing efficient collaborative training among clients with varied data granularities and diverse model scales across distinct data distributions. We introduce a distance-based FL mechanism designed for homogeneous clients, providing personalized models to mitigate the negative effects that non-IID data might have on model aggregation. Simultaneously, we propose an attention-weighted FL mechanism enhanced by a prior attention mechanism, facilitating knowledge transfer across clients with heterogeneous data granularities and model scales. Furthermore, we provide theoretical analyses of the convergence properties of the proposed MGW-FL method for both convex and non-convex models. Experimental results on five benchmark datasets demonstrate that, compared to baseline methods, MGW-FL significantly improves accuracy by almost 150% and convergence efficiency by nearly 20% on both IID and non-IID data.},
  archive      = {J_TSC},
  author       = {Yunfeng Zhao and Chao Qiu and Shangxuan Cai and Zhicheng Liu and Yu Wang and Xiaofei Wang and Qinghua Hu},
  doi          = {10.1109/TSC.2024.3495532},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {270-287},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Multi-granularity weighted federated learning for heterogeneous edge computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MEC-enabled task replication with resource allocation for reliability-sensitive services in 5G mMTC networks. <em>TSC</em>, <em>18</em>(1), 253-269. (<a href='https://doi.org/10.1109/TSC.2024.3517341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for connectivity in 5G networks has led to a focus on massive machine-type communication (mMTC) in mobile edge computing (MEC) for IoTs. However, the proliferation of IoT devices has resulted in densely deployed networks and led to a high volume of task offloading to the same edge servers simultaneously. As a consequence, mMTC applications may experience service congestion, negatively impacting service reliability. To enhance the service reliability of latency-sensitive applications, task replication with resource allocation is proposed in MEC, in which a task can be sent simultaneously to multiple computing nodes. Task replication can reduce task latency and improve service reliability at the cost of consuming more computation resources. However, unconstrained task replication may result in too many uploading links, leading to severe costs in network operation. To handle the above challenge, we propose a constrained stochastic optimization problem by task replication with wireless resource block (RB) allocation and edge server queue management. To ensure queue stability while minimizing cost, we design one strategy based on the Lyapunov optimization framework. Accordingly, we further model RB allocation as a mean-field game (MFG) due to the intensive coupling of the RB pool for massive users. Tractable partial differential equations are used to analyze MFG equilibrium, and we derive the optimal edge server queue management based on a given task replication strategy and RB allocation scheme. Our theoretical analysis demonstrates that our algorithm closely approaches the optimal overall costs within a small gap, and simulation results show that our strategy generates a significantly lower cumulative cost than other alternative strategies.},
  archive      = {J_TSC},
  author       = {Rui Huang and Wushao Wen and Zhi Zhou and Chongwu Dong and Xu Chen},
  doi          = {10.1109/TSC.2024.3517341},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {253-269},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {MEC-enabled task replication with resource allocation for reliability-sensitive services in 5G mMTC networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning-to-adaptation for security service in industrial IoT: An AI-enabled slice-specific solution. <em>TSC</em>, <em>18</em>(1), 239-252. (<a href='https://doi.org/10.1109/TSC.2024.3505785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network slicing is the key enabler for the 5G Industrial Internet of Things (IIoT), allowing tailored services and security guarantees for vertical industries. With the advent of 5G-Advanced (5G-A) and 6G era, the number of slices will increase significantly, leading to more diverse security requirements given different slice features. To provide adaptive security management spanning multiple slices in IIoT, this paper proposes a novel slice-specific secure IIoT (SSIOT) architecture with an AI-enabled solution. The SSIOT architecture separates the control and data planes, where the control plane orchestrates the Security Service Function Chains (SSFC) across network slices and the data plane analyzes the slice-specific features like traffic patterns, resource SLA guarantees, and Virtual Security Network Function (VSNF) dependencies. To extract these spatial-temporal features from the dynamic IIoT environments, we facilitate the powerful deep reinforcement learning (DRL) methods and propose a structural GS2L approach. GS2L is maliciously designed with the core principles of graph convolutional network (GCN) and Gated Recurrent Unit (GRU), enabling a thorough understanding of physical resource distribution and the request dynamics across slices. Extensive experiments are conducted in diverse IIoT slices with the real-world USNet and fat-tree topologies. Simulation results demonstrate that GS2L outperforms state-of-the-art learning and heuristic benchmarks, showcasing an overall 15.2% improvement with efficient and stable resource utilization.},
  archive      = {J_TSC},
  author       = {Zhiwei Wei and Bing Li and Rongqing Zhang and Lingyang Song},
  doi          = {10.1109/TSC.2024.3505785},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {239-252},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Learning-to-adaptation for security service in industrial IoT: An AI-enabled slice-specific solution},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KA$^{2}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>SE: Key-aggregation authorized searchable encryption scheme for data sharing in wireless sensor networks. <em>TSC</em>, <em>18</em>(1), 226-238. (<a href='https://doi.org/10.1109/TSC.2024.3491378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a promising technology, key-aggregation searchable encryption with constant computation overhead is especially suitable for sensor nodes with limited computation resources in wireless sensor networks. However, in most of the existing key-aggregation searchable encryption schemes, the authorized aggregation key is generated in a deterministic way. As a result, these schemes suffer from “Key Forge Attack” and “Trapdoor Forge Attack” that we proposed and hence fail to support the security property as they claimed (which is an important goal to be achieved in key-aggregation searchable encryption schemes). To fix these flaws, in this paper, we identify the security challenges related to key-aggregation searchable encryption and propose a lightweight key-aggregation authorized searchable encryption scheme based on attribute-based encryption, called KA$^{2}$SE. It enables a data owner to share encrypted data with an authorized query user by issuing only a single authorized aggregation key, and the authorized query user only needs to submit a single trapdoor to the cloud server to perform keyword search. We formulate the security definitions for KA$^{2}$SE and prove its security. Finally, empirical evaluations demonstrate that KA$^{2}$ SE is computationally efficient in comparison with existing schemes.},
  archive      = {J_TSC},
  author       = {Haijiang Wang and Jianting Ning and Wei Wu and Chao Lin and Kai Zhang},
  doi          = {10.1109/TSC.2024.3491378},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {226-238},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {KA$^{2}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup></mml:math>SE: Key-aggregation authorized searchable encryption scheme for data sharing in wireless sensor networks},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intent-guided bilateral long and short-term information mining with contrastive learning for sequential recommendation. <em>TSC</em>, <em>18</em>(1), 212-225. (<a href='https://doi.org/10.1109/TSC.2024.3520868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current sequential recommendation systems mainly focus on mining information related to users to make personalized recommendations. However, there are two subjects in the user historical interaction sequence: users and items. We believe that mining sequence information only from the users’ perspective is limited, ignoring effective information from the perspective of items, which is not conducive to alleviating the data sparsity problem. To explore potential links between items and use them for recommendation, we propose Intent-guided Bilateral Long and Short-Term Information Mining with Contrastive Learning for Sequential Recommendation (IBLSRec), which interpretively integrates three kinds of information mined from the sequence: user preferences, user intentions, and potential relationships between items. Specifically, we model the potential relationships between interactive items from a long-term and short-term perspective. The short-term relationship between items is regarded as noise; the long-term relationship between items is regarded as a stable common relationship and integrated with the user's personalized preferences. In addition, user intent is used to guide the modeling of user preferences to refine the representation of user preferences further. A large number of experiments on four real data sets validate the superiority of our model.},
  archive      = {J_TSC},
  author       = {Junhui Niu and Wei Zhou and Fengji Luo and Yihao Zhang and Jun Zeng and Junhao Wen},
  doi          = {10.1109/TSC.2024.3520868},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {212-225},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Intent-guided bilateral long and short-term information mining with contrastive learning for sequential recommendation},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible computing: A new framework for improving resource allocation and scheduling in elastic computing. <em>TSC</em>, <em>18</em>(1), 198-211. (<a href='https://doi.org/10.1109/TSC.2024.3489433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the advent of cloud computing, Elastic Computing (EC) has become the standard architecture for resource allocation and scheduling. EC typically allocates computing resources based on predefined specifications, such as virtual machine or container flavors. However, these flavors are often constrained by fixed CPU-to-memory ratios, which frequently fail to match the actual resource needs of applications. As a result, cloud providers experience high resource allocation rates nearing saturation ($&gt; $80%) but with low utilization ($&lt; $25%). This study introduces Flexible Computing (FC), a novel approach to resource allocation and scheduling. Unlike EC, FC allocates resources based on an application resource usage profile, derived from the historical resource consumption of workloads, rather than relying on fixed specifications. Additionally, FC incorporates a real-time performance degradation detection mechanism to address performance issues caused by the noisy-neighbor effect when colocated workloads interfere with each other. FC dynamically adjusts resource allocation according to actual usage, ensuring that application performance meets Service Level Agreements (SLAs), while preventing resource waste and performance degradation from improper resource over-commitment. Large-scale experimental validations conducted on the FC architecture within Huawei Cloud data centers demonstrate that, compared to EC, FC can reduce computing resource consumption by over 33% while managing the same workloads. Furthermore, FC's real-time performance degradation detection model achieves a prediction error of less than 5% across various testing environments, highlighting its commercial viability.},
  archive      = {J_TSC},
  author       = {Weipeng Cao and Jiongjiong Gu and Zhong Ming and Zhiyuan Cai and Yuzhao Wang and Changping Ji and Zhijiao Xiao and Yuhong Feng and Ye Liu and Liang-Jie Zhang},
  doi          = {10.1109/TSC.2024.3489433},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {198-211},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Flexible computing: A new framework for improving resource allocation and scheduling in elastic computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FeDistSlice: Federated policy distillation for collaborative intelligence in multi-tenant RAN slicing. <em>TSC</em>, <em>18</em>(1), 184-197. (<a href='https://doi.org/10.1109/TSC.2024.3517334'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Deep Reinforcement Learning (FDRL) for Radio Access Network (RAN) Slicing offers a promising approach for optimizing resource allocation and network performance, while also preserving data privacy for multiple tenants. However, the inherently non-independent and identically distributed (non-IID) nature of data, stemming from the diverse services and unique characteristics of RAN slices, poses significant challenges. This heterogeneity can disrupt the standard assumptions FDRL makes, leading to model training inefficiencies and potentially suboptimal slicing decisions. Addressing this non-IID challenge is imperative to harness the full potential of FDRL in RAN slicing and to ensure seamless, adaptive, and efficient resource sharing among the tenants. Hence, we propose FeDistSlice, a federated distillation slicing framework wherein multiple decision agents collaborate in real time, optimizing resource allocation tailored to each tenant's specific characteristics. Motivated by collaborative intelligence, we introduced a customized mutual policy distillation (MPD) strategy to foster collaboration across multiple tenants. This innovation allows for the creating of personalized models tailored to each agent's unique requirements and context. Through MPD, these models can collaboratively learn and refine their policies by leveraging insights from other agents within the network. Simulation results show that FeDistSlice converges more effectively and achieves increased robustness to non-IID data.},
  archive      = {J_TSC},
  author       = {Guolin Sun and Daniel Ayepah-Mensah and Huan Chen and Gordon Owusu Boateng and Guisong Liu},
  doi          = {10.1109/TSC.2024.3517334},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {184-197},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {FeDistSlice: Federated policy distillation for collaborative intelligence in multi-tenant RAN slicing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced tube-based sampling for accurate network distance measurement with minimal sampling scheduling overhead. <em>TSC</em>, <em>18</em>(1), 169-183. (<a href='https://doi.org/10.1109/TSC.2024.3506477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in demand for latency-sensitive services has propelled network distance measurement to the forefront of networking research. Utilizing the low-rank structure of full network data, the tensor completion method can efficiently estimate network distance from partially sampled distance data measured from a small set of node pairs. However, its performance is affected by sampling algorithm limitations, including unreliability and high overhead in dynamic networks. To tackle these challenges, we propose tube-based sampling as an alternative to point-based sampling, utilizing a partition-based algorithm to incorporate randomness for improved reliability. Additionally, we introduce a Tube Length Identification Algorithm to dynamically adjust tube length based on network status, balancing scheduling overhead reduction with estimation accuracy. Experimental results on three real network distance datasets, compared against 13 baseline algorithms, demonstrate the high accuracy and low scheduling overhead of our approach.},
  archive      = {J_TSC},
  author       = {Jiazheng Tian and Cheng Wang and Kun Xie and Jigang Wen and Gaogang Xie and Kenli Li and Wei Liang},
  doi          = {10.1109/TSC.2024.3506477},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {169-183},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Enhanced tube-based sampling for accurate network distance measurement with minimal sampling scheduling overhead},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient parameter synchronization for peer-to-peer distributed learning with selective multicast. <em>TSC</em>, <em>18</em>(1), 156-168. (<a href='https://doi.org/10.1109/TSC.2024.3506480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in distributed machine learning show theoretically and empirically that, for many models, provided that workers will eventually participate in the synchronizations, $i)$ the training still converges, even if only $p$ workers take part in each round of synchronization, and $ii)$ a larger $p$ generally leads to a faster rate of convergence. These findings shed light on eliminating the bottleneck effects of parameter synchronization in large-scale data-parallel distributed training and have motivated several optimization designs. In this paper, we focus on optimizing the parameter synchronization for peer-to-peer distributed learning, where workers broadcast or multicast their updated parameters to others for synchronization, and propose SelMcast, a suite of expressive and efficient multicast receiver selection algorithms, to achieve the goal. Compared with the state-of-the-art (SOTA) design, which randomly selects exactly $p$ receivers for each worker’s multicast in a bandwidth-agnostic way, SelMcast chooses receivers based on the global view of their available bandwidth and loads, yielding two advantages, i.e., accelerated parameter synchronization for higher utilization of computing resources and enlarged average $p$ values for faster convergence. Comprehensive evaluations show that SelMcast is efficient for both peer-to-peer Bulk Synchronous Parallel (BSP) and Stale Synchronous Parallel (SSP) distributed training, outperforming the SOTA solution significantly.},
  archive      = {J_TSC},
  author       = {Shouxi Luo and Pingzhi Fan and Ke Li and Huanlai Xing and Long Luo and Hongfang Yu},
  doi          = {10.1109/TSC.2024.3506480},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {156-168},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Efficient parameter synchronization for peer-to-peer distributed learning with selective multicast},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient hierarchical federated services for heterogeneous mobile edge. <em>TSC</em>, <em>18</em>(1), 140-155. (<a href='https://doi.org/10.1109/TSC.2024.3495501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As 6G networks actively advance edge intelligence, Federated Learning (FL) emerges as a key technology that enables data sharing while preserving data privacy and fostering collaboration among edge devices for intelligent service learning. However, the multi-dimensional heterogeneous and hierarchical network architecture brings many challenges to FL deployment, including selecting appropriate nodes for model training and designing effective methods for model aggregation. Compared with most studies that focus on solving individual problems within 6G, this paper proposes an efficient deployment scheme named hierarchical heterogeneous FL (HHFL), which comprehensively considers various influencing factors. First, the deployment of HHFL over 6G is modeled amid the heterogeneity of communications, computation, and data. An optimization problem is then formulated, aiming to minimize deployment costs in terms of latency and energy consumption. Subsequently, to tackle this optimization challenge, we design an intelligent FL deployment framework, consisting of a hierarchical aggregation deployment (HAD) component for hierarchical FL aggregation structure construction and an adaptive node selection (ANS) component for selecting diverse clients based on multi-dimensional discrepancy criteria. Experimental results demonstrate that our proposed framework not only adapts to various application requirements but also outperforms existing technologies by achieving superior learning performance, reduced latency, and lower energy consumption.},
  archive      = {J_TSC},
  author       = {Shengyuan Liang and Qimei Cui and Xueqing Huang and Borui Zhao and Yanzhao Hou and Xiaofeng Tao},
  doi          = {10.1109/TSC.2024.3495501},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {140-155},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Efficient hierarchical federated services for heterogeneous mobile edge},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Crowdsourcing home healthcare service: Matching caretakers with caregivers for jointly rostering and routing. <em>TSC</em>, <em>18</em>(1), 126-139. (<a href='https://doi.org/10.1109/TSC.2024.3517344'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce crowdsourcing home healthcare service (CHHS) systems, where caregivers (including nurses, personal care attendants, and housekeepers) from different locations (rather than centralized institutions) offer diverse home healthcare services to caretakers at home. Powered by cloud computing, the CHHS system enables real-time, dynamic, and large-scale matching between caretakers and caregivers based on their preferences and constraints, and determines caregivers’ rostering and routing plans, involving the NP-hard nurse rostering problem (NRP) and the vehicle routing problem (VRP). This work firstly creates a mathematical programming model to jointly roster and route for the CHHS, maximizing the matching scores of caregivers and caretakers based on the preferred features through analytic hierarchy process (AHP), and minimizing caregivers’ overtime and routing costs, under constraints of caregiver skills, regulations, and vehicle routing. The proposed matching score mechanism assigns weights to caretaker preferences, enhancing pairing with preferred caregivers and reducing dissatisfaction. This work proposes a hybrid genetic algorithm with variable neighborhood search (GAVNS), respectively tailored to handle the rostering and routing aspects of CHHS. Simulation indicates that the GAVNS lowers costs by approximately 38% and 26% in rural and city cases, respectively, and outperforms standalone GA and VNS, achieving a 3% additional cost reduction and consistently yielding feasible solutions.},
  archive      = {J_TSC},
  author       = {Chun-Cheng Lin and Yi-Chun Peng and Zhen-Yin Annie Chen and Pei-Yu Liu},
  doi          = {10.1109/TSC.2024.3517344},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {126-139},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Crowdsourcing home healthcare service: Matching caretakers with caregivers for jointly rostering and routing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous management of machine learning-based application behavior. <em>TSC</em>, <em>18</em>(1), 112-125. (<a href='https://doi.org/10.1109/TSC.2024.3486226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern applications are increasingly driven by Machine Learning (ML) models whose non-deterministic behavior is affecting the entire application life cycle from design to operation. The pervasive adoption of ML is urgently calling for approaches that guarantee a stable non-functional behavior of ML-based applications over time and across model changes. To this aim, non-functional properties of ML models, such as privacy, confidentiality, fairness, and explainability, must be monitored, verified, and maintained. Existing approaches mostly focus on i) implementing solutions for classifier selection according to the functional behavior of ML models, ii) finding new algorithmic solutions, such as continuous re-training. In this paper, we propose a multi-model approach that aims to guarantee a stable non-functional behavior of ML-based applications. An architectural and methodological approach is provided to compare multiple ML models showing similar non-functional properties and select the model supporting stable non-functional behavior over time according to (dynamic and unpredictable) contextual changes. Our approach goes beyond the state of the art by providing a solution that continuously guarantees a stable non-functional behavior of ML-based applications, is ML algorithm-agnostic, and is driven by non-functional properties assessed on the ML models themselves. It consists of a two-step process working during application operation, where model assessment verifies non-functional properties of ML models trained and selected at development time, and model substitution guarantees continuous and stable support of non-functional properties. We experimentally evaluate our solution in a real-world scenario focusing on non-functional property fairness.},
  archive      = {J_TSC},
  author       = {Marco Anisetti and Claudio A. Ardagna and Nicola Bena and Ernesto Damiani and Paolo G. Panero},
  doi          = {10.1109/TSC.2024.3486226},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {112-125},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Continuous management of machine learning-based application behavior},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Content-specific and buffer-based migration schemes for fog computing. <em>TSC</em>, <em>18</em>(1), 98-111. (<a href='https://doi.org/10.1109/TSC.2024.3506474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing and network function virtualization (NFV) technologies reduce latency and provide scalable services at the network edge. However, the limited resources of edge nodes pose a challenge in handling high traffic volumes from requests that demand high computation, extended lifetime, and low latency. To address this challenge, dynamic load migration schemes for NFV-based fog paradigms are proposed here. First, a content-specific scheme that diffuses excess loads to nearby locations hosting relevant virtual network functions (VNFs) of the requests. Second, a buffer scheme reserves a dedicated node to absorb loads from saturated nodes in the proximity of terminals. Third, a hybrid scheme integrates both strategies by initially migrating loads to the pre-allocated buffer to provide immediate relief without searching for candidate nodes. Upon the buffer saturation, it switches to the content-specific phase to distribute loads to the proximate nodes. The network introduces a novel request model comprised of dependent and independent VNFs of varying resource demands. Dependent VNFs are collectively mapped on a primary node while distributing independent VNFs across neighboring secondary nodes, forming a structured ring topology mapping method. These schemes enhance migration success rates, and reduce migration iterations, service downtime, and cost, as compared to prominent solutions.},
  archive      = {J_TSC},
  author       = {Mohammed A. Jasim},
  doi          = {10.1109/TSC.2024.3506474},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {98-111},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Content-specific and buffer-based migration schemes for fog computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic data generation and optimization for digital twin network. <em>TSC</em>, <em>18</em>(1), 85-97. (<a href='https://doi.org/10.1109/TSC.2024.3522504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of new applications such as AR/VR, cloud gaming, and vehicular networks, traditional network management solutions are no longer cost-effective. Digital Twin Network (DTN) creates a real-time virtual twin of the physical network, which improves the network's stability, security, and operational efficiency. AI models have been used to model complex network environments in DTN, whose quality mainly depends on the model architecture and data. This paper proposes an automatic data generation and optimization method for DTN called AutoOPT, which focuses on generating and optimizing data for data-driven DTN AI modeling through data-centric AI. The data generation stage generates data in small networks based on scale-independent indicators, which helps DTN AI models generalize to large networks. The data optimization stage automatically filters out high-quality data through seed sample selection and incremental optimization, which helps enhance the accuracy and generalization of DTN AI models. We apply AutoOPT to the DTN performance modeling scenario and evaluate it on simulated and real network data. The experimental results show that AutoOPT is more cost-efficient than state-of-the-art solutions while achieving similar results, and it can automatically select high-quality data for scenarios that require data quality improvement.},
  archive      = {J_TSC},
  author       = {Mei Li and Cheng Zhou and Lu Lu and Yan Zhang and Tao Sun and Danyang Chen and Hongwei Yang and Zhiqiang Li},
  doi          = {10.1109/TSC.2024.3522504},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {85-97},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {Automatic data generation and optimization for digital twin network},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ARAScaler: Adaptive resource autoscaling scheme using ETimeMixer for efficient cloud-native computing. <em>TSC</em>, <em>18</em>(1), 72-84. (<a href='https://doi.org/10.1109/TSC.2024.3522815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The container resource autoscaling techniques offer scalability and continuity for microservices operating in cloud-native computing environments. However, they manage resources inefficiently, causing resource waste and overload under complex workload patterns. In addition, these techniques fail to prevent oscillations caused by dynamic workloads, increasing the operational complexity. Therefore, we propose an adaptive resource autoscaling scheme (ARAScaler) to ensure the stability and resource efficiency of microservices with minimal scaling events. ARAScaler predicts future workloads using enhanced TimeMixer (ETimeMixer) applied with the convolutional method. Additionally, ARAScaler segments the predicted workload to identify burst, nonburst, dynamic, and static states and scales by calculating the optimal number of container instances for each identified state. The offline simulation results using seven cloud-workload trace datasets demonstrate the high prediction accuracy of ETimeMixer and the superior scaling performance of ARAScaler. The ARAScaler achieved a resource utilization of approximately 70% or higher with few updates and recorded the fewest resource overload instances compared to existing container resource autoscaling techniques.},
  archive      = {J_TSC},
  author       = {Byeonghui Jeong and Young-Sik Jeong},
  doi          = {10.1109/TSC.2024.3522815},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {72-84},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {ARAScaler: Adaptive resource autoscaling scheme using ETimeMixer for efficient cloud-native computing},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An active defense adjudication method based on adaptive anomaly sensing for mimic IoT. <em>TSC</em>, <em>18</em>(1), 57-71. (<a href='https://doi.org/10.1109/TSC.2024.3436673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security issues in the Internet of Things (IoT) are inevitable. Uncertain threats, such as known vulnerabilities and backdoors exist within IoT, and traditional passive network security technologies are ineffective against uncertain threats. To address the above issues, we propose an active defense adjudication method based on adaptive anomaly sensing for mimic IoT. The method constructs a mimic IoT active defense architecture, improving system security and reliability despite prevailing security threats. In addition, an intelligent anomaly sensing algorithm is integrated into the adjudication module of the mimic IoT active defense architecture to support arbitration. An adaptive anomaly sensing model based on multi-feature selection is used to determine the anomaly score of the IoT device outputs, and this model fully considers the reliability of the adjudication data and improves the accuracy of the adjudication. Finally, we conduct a comparative analysis of the proposed adjudication algorithm against three others via a mimic power communication IoT system as an application scenario. The experimental results show that our algorithm can improve security and reduce the failure rate of the mimic IoT system.},
  archive      = {J_TSC},
  author       = {Sisi Shao and Tiansheng Gu and Yijun Nie and Zongkai Ji and Fei Wu and Zhongjie Ba and Yimu Ji and Kui Ren and Guozi Sun},
  doi          = {10.1109/TSC.2024.3436673},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {57-71},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {An active defense adjudication method based on adaptive anomaly sensing for mimic IoT},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ADSS: An available-but-invisible data service scheme for fine-grained usage control. <em>TSC</em>, <em>18</em>(1), 43-56. (<a href='https://doi.org/10.1109/TSC.2024.3495498'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The demand for mobile terminals to participate in data services is increasingly vital. The General Data Protection Regulation (GDPR) has established several principled requirements for data services. Existing studies focusing on data service put emphasis on data privacy and accessibility. However, they face challenges in achieving data forgetability and portability on mobile devices under GDPR and lack consideration of usage control. In this article, we propose ADSS, an app-level data service scheme for mobile devices that can be available-but-invisible and guarantee fine-grained usage control. ADSS addresses the challenges by executing the logic of data usage in the Trusted Execution Environment (TEE) and managing the TEE states (i.e., data usage states) in the blockchain smart contracts. It not only satisfies the requirements of GDPR, ensuring strong security and confidentiality guarantees, but also enables the functionality of “pay-per-use”. We implement a prototype of the ADSS framework based on ARM Trustzone and conduct experimental evaluations. The results demonstrate that our scheme brings high efficiency compared with other data service schemes and exhibits feasibility on mobile-grade devices.},
  archive      = {J_TSC},
  author       = {Hao Wang and Jun Wang and Chunpeng Ge and Yuhang Li and Lu Zhou and Zhe Liu and Weibin Wu and Mingsheng Cao},
  doi          = {10.1109/TSC.2024.3495498},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {43-56},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {ADSS: An available-but-invisible data service scheme for fine-grained usage control},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A resource-efficient multiple recognition services framework for IoT devices. <em>TSC</em>, <em>18</em>(1), 29-42. (<a href='https://doi.org/10.1109/TSC.2024.3512949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying the convolutional neural network (CNN) model on Internet of Things (IoT) devices to provide diverse recognition services has received increasing attention. Due to the limited storage, computing, and other resources of IoT devices, it has become mainstream to first train the CNN model on the edge/cloud server and then send the trained CNN to the IoT device. However, most existing related methods suffer from two limitations, (i) low performance due to service interference or insufficient mutual assistance, and (ii) large memory resources and switching resource overhead. To this end, this article proposes a resource-efficient multiple recognition services framework for IoT devices. The proposed framework is based on the edge server-assisted IoT device training of the CNN model, and the framework includes a deeper weight adaptation (DeepWAdapt) algorithm to mitigate service interference. The DeepWAdapt algorithm consists of a set of learnable masks, and by inserting these masks into the appropriate layers of the CNN model, it mitigates mutual interference between services caused by training a single CNN model for multiple services. Each service has a specific set of masks. These learnable masks work like keys for each service, selecting appropriate and specific features for each service from a shared feature set. Experimental results demonstrate that the DeepWAdapt outperforms other state-of-the-art methods on image-level classification services and pixel-level dense prediction services. Specifically, when executing 40 services based on ResNet18, the proposed DeepWAdapt achieves 66.82% F1-score on the CelebA dataset, which is +2.61% F1-score than the previous state-of-the-art result. In addition, compared with the routing method, our proposed DeepWAdapt also reduces network transmission traffic by approximately 35%.},
  archive      = {J_TSC},
  author       = {Chuntao Ding and Zhuo Liu and Ao Zhou and Jinhui Yu and Yidong Li and Shangguang Wang},
  doi          = {10.1109/TSC.2024.3512949},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {29-42},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A resource-efficient multiple recognition services framework for IoT devices},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reinforcement learning based framework for holistic energy optimization of sustainable cloud data centers. <em>TSC</em>, <em>18</em>(1), 15-28. (<a href='https://doi.org/10.1109/TSC.2024.3495495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of cloud data centers has led to a rise in energy consumption, with the associated carbon emissions posing a further threat to the environment. Cloud providers are increasingly moving towards sustainable data centers powered by renewable energy sources (RES). The existing approaches fail to efficiently coordinate IT and cooling resources in such data centers due to the intermittent nature of RES and the complexity of state and action spaces among different devices, resulting in poor holistic energy efficiency. In this paper, a reinforcement learning (RL) based framework is proposed to optimize the holistic energy consumption of sustainable cloud data centers. First, a joint prediction method MTL-LSTM is developed to accurately evaluate both energy consumption and thermal status of each physical machine (PM) under different optimization scenarios to improve the state space information of the RL algorithm. Then, this framework designs a novel energy-aware approach named BayesDDQN, which leverages Bayesian optimization to synchronize the adjustments of VM migration and cooling parameter within the hybrid action space of the Double Deep Q-Network (DDQN) for achieving the holistic energy optimization. Moverover, the pre-cooling technology is integrated to further alleviate hotspot by making full use of RES. Experimental results demonstrate that the proposed RL-based framework achieves an average reduction of 2.83% in holistic energy consumption and 4.74% in brown energy, which also reduces cooling energy consumption by 13.48% with minimal occurrences of hotspots. Furthermore, the proposed MTL-LSTM method reduces the root mean square error (RMSE) of energy consumption and inlet temperature predictions by nearly half compared to LSTM and XGBoost.},
  archive      = {J_TSC},
  author       = {Daming Zhao and Jiantao Zhou and Jidong Zhai and Keqin Li},
  doi          = {10.1109/TSC.2024.3495495},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {15-28},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A reinforcement learning based framework for holistic energy optimization of sustainable cloud data centers},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A game-based computation offloading with imperfect information in multi-edge environments. <em>TSC</em>, <em>18</em>(1), 1-14. (<a href='https://doi.org/10.1109/TSC.2024.3517336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) can augment the capability of Internet of Things (IoT) mobile devices (MDs) through offloading the computation-intensive tasks to their adjacent servers. Synergistic computation offloading among MEC servers is one possible solution to reduce the completion time of system during peak hours. However, due to the large number of servers and the long distance between base stations (BSs), synchronizing the information of all servers takes a long time, which is not applicable to the fluctuant environments. Meanwhile, each server from different BSs is typically selfish and rational, and can only obtain the imperfect information from its adjacent servers, which is a challenge for computation offloading among servers from a global perspective. This article proposes a game-based computation offloading scheme with imperfect information in multi-edge environments. First, a non-cooperative game with imperfect information is designed to analyze the complex interactions during synergistic computation offloading among MEC servers. Second, a Synergistic Balancing Offloading Algorithm (SBOA) through distributed decision-making manner to obtain the optimal offloading decision is proposed, which guarantees that the game converges to a Nash Equilibrium (NE) point. Extensive simulation results reveal the fast convergence of SBOA. As the percentage of high-load servers rises and the number of heavy tasks increases, SBOA performs better than other benchmark algorithms in terms of timeliness, effectiveness, and system completion time.},
  archive      = {J_TSC},
  author       = {Bing Lin and Jie Weng and Xing Chen and Yun Ma and Ching-Hsien Hsu},
  doi          = {10.1109/TSC.2024.3517336},
  journal      = {IEEE Transactions on Services Computing},
  month        = {1-2},
  number       = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Serv. Comput.},
  title        = {A game-based computation offloading with imperfect information in multi-edge environments},
  volume       = {18},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
