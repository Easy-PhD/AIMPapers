<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc">TC - 288</h2>
<ul>
<li><details>
<summary>
(2025). Optimizing multi-DNN parallel inference performance in MEC networks: A resource-aware and dynamic DNN deployment scheme. <em>TC</em>, <em>74</em>(11), 3938-3952. (<a href='https://doi.org/10.1109/TC.2025.3605749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Multi-access Edge Computing (MEC) has empowered Internet of Things (IoT) devices and edge servers to deploy sophisticated Deep Neural Network (DNN) applications, enabling real-time inference. Many concurrent inference requests and intricate DNN models demand efficient multi-DNN inference in MEC networks. However, the resource-limited IoT device/edge server and expanding model size force models to be dynamically deployed, resulting in significant undesired energy consumption. In addition, parallel multi-DNN inference on the same device complicates the inference process due to the resource competition among models, increasing the inference latency. In this paper, we propose a Resource-aware and Dynamic DNN Deployment (R3D) scheme with the collaboration of end-edge-cloud. To mitigate resource competition and waste during multi-DNN parallel inference, we develop a Resource Adaptive Management (RAM) algorithm based on the Roofline model, which dynamically allocates resources by accounting for the impact of device-specific performance bottlenecks on inference latency. Additionally, we design a Deep Reinforcement Learning (DRL)-based online optimization algorithm that dynamically adjusts DNN deployment strategies to achieve fast and energy-efficient inference across heterogeneous devices. Experiment results demonstrate that R3D is applicable in MEC environments and performs well in terms of inference latency, resource utilization, and energy consumption.},
  archive      = {J_TC},
  author       = {Tong Zheng and Yuanguo Bi and Guangjie Han and Xingwei Wang and Yuheng Liu and Yufei Liu and Xiangyi Chen},
  doi          = {10.1109/TC.2025.3605749},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3938-3952},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing multi-DNN parallel inference performance in MEC networks: A resource-aware and dynamic DNN deployment scheme},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ecomap: Sustainability-driven optimization of multi-tenant DNN execution on edge servers. <em>TC</em>, <em>74</em>(11), 3925-3937. (<a href='https://doi.org/10.1109/TC.2025.3604487'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing systems struggle to efficiently manage multiple concurrent deep neural network (DNN) workloads while meeting strict latency requirements, minimizing power consumption, and maintaining environmental sustainability. This paper introduces Ecomap, a sustainability-driven framework that dynamically adjusts the maximum power threshold of edge devices based on real-time carbon intensity. Ecomap incorporates the innovative use of mixed-quality models, allowing it to dynamically replace computationally heavy DNNs with lighter alternatives when latency constraints are violated, ensuring service responsiveness with minimal accuracy loss. Additionally, it employs a transformer-based estimator to guide efficient workload mappings. Experimental results using NVIDIA Jetson AGX Xavier demonstrate that Ecomap reduces carbon emissions by an average of 30% and achieves a 25% lower carbon delay product (CDP) compared to state-of-the-art methods, while maintaining comparable or better latency and power efficiency.},
  archive      = {J_TC},
  author       = {Varatheepan Paramanayakam and Andreas Karatzas and Dimitrios Stamoulis and Iraklis Anagnostopoulos},
  doi          = {10.1109/TC.2025.3604487},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3925-3937},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Ecomap: Sustainability-driven optimization of multi-tenant DNN execution on edge servers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCC: Synchronization congestion control for multi-tenant learning over geo-distributed clouds. <em>TC</em>, <em>74</em>(11), 3911-3924. (<a href='https://doi.org/10.1109/TC.2025.3604486'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed machine learning over geo-distributed clouds enables joint training of data located in different regions, alleviating the burden of transferring large volumes of training datasets, which greatly saves bandwidth. However, the limited capacity of WAN links slows down the inter-cloud communications, which significantly decelerates the synchronization of distributed machine learning over geo-distributed clouds. Besides, the multi-tenancy in clouds results in multiple training tasks running simultaneously, whose synchronizations consistently compete for the limited WAN bandwidth with each other, which further aggravates the training performance of each task. While existing works optimize synchronizations through techniques like gradient compression, multi-resource interleaving and so on, none of them targets at the synchronization congestion especially due to multi-tenant learning, which results in inferior training performance. To solve these problems, we propose a simple but effective scheme, SCC, for fast and efficient multi-tenant learning via synchronization congestion control. SCC monitors the cross-cloud network conditions and evaluates the synchronization congestion level based on the round-trip transmission time for each synchronization. Then SCC alleviates synchronization congestion via controlling the synchronization frequency according to the synchronization congestion level in a probabilistic way. Extensive experiments are conducted within our testbeds consisted of 16 NVIDIA V100 GPUs to evaluate the performance of SCC, and comparison results show that SCC can reduce the average training completion time and makespan by up to 28.6% and 43.2% over SAP-SGD [1]. Targeted experiments are conducted to demonstrate the effectiveness and robustness of SCC.},
  archive      = {J_TC},
  author       = {Chengxi Gao and Fuliang Li and Kejiang Ye and Yang Wang and Pengfei Wang and Xingwei Wang and Chengzhong Xu},
  doi          = {10.1109/TC.2025.3604486},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3911-3924},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SCC: Synchronization congestion control for multi-tenant learning over geo-distributed clouds},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reputation-based energy-efficient transaction propagation mechanism for blockchain-enabled multi-access edge computing. <em>TC</em>, <em>74</em>(11), 3897-3910. (<a href='https://doi.org/10.1109/TC.2025.3604480'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain strengthens reliable collaboration among entities through its transparency, immutability, and traceability, leading to its integration into Multi-access Edge Computing (MEC) and promoting the development of a trusted JointCloud. However, existing transaction propagation mechanisms require MEC devices to consume significant computing resources for complex transaction verification, increasing their vulnerability to malicious attacks. Adversaries can exploit this by flooding the blockchain network with spam transactions, aiming to deplete device energy and disrupt system performance. To cope with these issues, this paper proposes a reputation-based energy-efficient transaction propagation mechanism that alleviates spam transaction attacks while reducing computing resources and energy consumption. Firstly, we design a subjective logic-based reputation scheme that assesses node trust by integrating local and recommended opinions and incorporates opinion acceptance to counteract false evidence. Then, we optimize the transaction verification method by adjusting transaction discard and verification probabilities based on the proposed reputation scheme to curb the propagation of spam transactions and reduce verification consumption. Finally, we enhance the transaction transmission strategy by prioritizing nodes with higher reputations, enhancing both resilience to spam transactions and transmission reliability. A series of simulations demonstrates the effectiveness of the proposed mechanism.},
  archive      = {J_TC},
  author       = {Xijia Lu and Qiang He and Xingwei Wang and Jaime Lloret and Peichen Li and Ying Qian and Min Huang},
  doi          = {10.1109/TC.2025.3604480},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3897-3910},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A reputation-based energy-efficient transaction propagation mechanism for blockchain-enabled multi-access edge computing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-power multiplier designs by leveraging correlations of 2$\times$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>×</mml:mo></mml:math>2 encoded partial products. <em>TC</em>, <em>74</em>(11), 3888-3896. (<a href='https://doi.org/10.1109/TC.2025.3604478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multipliers, particularly those with small bit widths, are essential for modern neural network (NN) applications. In addition, multiple-precision multipliers are in high demand for efficient NN accelerators; therefore, recursive multipliers used in low-precision fusion schemes are gaining increasing attention. In this work, we design exact recursive multipliers based on customized approximate full adders (AFAs) for low-power purposes. Initially, the partial products (PPs) encoded by 2$\times$2 multiplications are analyzed, which reveals the correlations among adjacent PPs. Based on these correlations, we propose 4$\times$4 recursive multiplier architectures where certain full adders (FAs) can be simplified without affecting the correctness of the multiplication. Manually and synthesis tool-based FA simplifications are performed separately. The obtained 4$\times$4 multipliers are then used to construct 8$\times$8 multipliers based on a low-power recursive architecture. Finally, the proposed signed and unsigned 4$\times$4 and 8$\times$8 multipliers are evaluated using a 28nm CMOS technology. Compared with DesignWare (DW) multipliers, the proposed signed and unsigned 4$\times$4 multipliers achieve power reductions of 16.5% and 11.6%, respectively, without compromising area or delay; alternatively, the delay can be reduced by 20.9% and 39.4%, respectively, without compromising power or area. For signed and unsigned 8$\times$8 multipliers, the maximum power reductions are 9.7% and 13.7%, respectively, albeit with a trade-off in area.},
  archive      = {J_TC},
  author       = {Ao Liu and Siting Liu and Hui Wang and Qin Wang and Fabrizio Lombardi and Zhigang Mao and Honglan Jiang},
  doi          = {10.1109/TC.2025.3604478},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3888-3896},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Low-power multiplier designs by leveraging correlations of 2$\times$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>×</mml:mo></mml:math>2 encoded partial products},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-path bound for parallel tasks with conditional branches. <em>TC</em>, <em>74</em>(11), 3873-3887. (<a href='https://doi.org/10.1109/TC.2025.3604469'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel execution and conditional execution are increasingly prevalent in modern embedded systems. In real-time scheduling, a fundamental problem is how to upper-bound the response times of a task. Recent work applied the multi-path technique to reduce the response time bound for tasks with parallel execution, but left tasks with conditional execution as an open problem. This paper focuses on upper-bounding response times for tasks with both parallel execution and conditional execution using the multi-path technique. By designing a delicate abstraction regarding the multiple paths of various conditional branches, we derive a new response time bound. We further apply this response time bound into the scheduling of multiple parallel tasks with conditional branches. Experiments demonstrate that the proposed bound significantly advances the state-of-the-art, reducing the response time bound by 9.4% and improving the schedulability by 31.2% on average.},
  archive      = {J_TC},
  author       = {Qingqiang He and Nan Guan and Zhe Jiang and Mingsong Lv},
  doi          = {10.1109/TC.2025.3604469},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3873-3887},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multi-path bound for parallel tasks with conditional branches},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The metric relationship between extra connectivity and extra diagnosability of multiprocessor systems. <em>TC</em>, <em>74</em>(11), 3860-3872. (<a href='https://doi.org/10.1109/TC.2025.3604468'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As multiprocessor systems scale up, $h$-extra connectivity and $h$-extra diagnosability serve as two pivotal metrics for assessing the reliability of the underlying interconnection networks. To ensure that each component of the survival graph holds no fewer than $h + 1$ vertices, the $h$-extra connectivity and $h$-extra diagnosability have been proposed to characterize the fault tolerability and self-diagnosing capability of networks, respectively. Many efforts have been made to establish the quantifiable relationship between these metrics but it is less than optimal. This work addresses the flaws of the existing results and proposes a novel proof to determine the metric relationship between $h$-extra connectivity and $h$-extra diagnosability under the PMC and MM* models. Our approach overcomes the defect of previous results by abandoning the network’s regularity and independence number. Furthermore, we apply the suggested metric to establish the $h$-extra diagnosability of a new network class, named generalized exchanged X-cube-like network $GEXC(s,t)$, which takes dual-cube-like network, generalized exchanged hypercube, generalized exchanged crossed cube, and locally generalized exchanged twisted cube as special cases. Finally, we propose the $h$-extra diagnosis strategy ($h$-EDS) and design two self-diagnosis algorithms AhED-PMC and AhED-MM*, and then conduct experiments on $GEXC(s,t)$ and the real-world network DD-$g648$ to show the high accuracy and superior performance of the proposed algorithms.},
  archive      = {J_TC},
  author       = {Yifan Li and Shuming Zhou and Sun-Yuan Hsieh and Qifan Zhang},
  doi          = {10.1109/TC.2025.3604468},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3860-3872},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The metric relationship between extra connectivity and extra diagnosability of multiprocessor systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sketching for heavy item-oriented data stream mining with memory constraints. <em>TC</em>, <em>74</em>(11), 3845-3859. (<a href='https://doi.org/10.1109/TC.2025.3604467'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate and fast data stream mining is critical to many tasks, including real-time series analysis for mobile sensor data, big data management and machine learning. Various heavy-oriented item detection tasks, such as identifying heavy hitters, heavy changers, persistent items, and significant items, have garnered considerable attention from both industry and academia. Unfortunately, as data stream speeds continue to increase and the available memory, particularly in L1 cache, remains limited for real-time processing, existing schemes face challenges in simultaneously achieving high detection accuracy, memory efficiency, and fast update throughput, as we reveal. To tackle this conundrum, we propose a versatile and elegant sketch framework named Tight-Sketch, which supports a spectrum of heavy-based detection tasks. Recognizing that, in practice, most items are cold (non-heavy/persistent/significant), we implement distinct eviction strategies for different item types. This approach allows us to swiftly discard potentially cold items while offering enhanced protection to hot ones (heavy/persistent/significant). Additionally, we introduce an eviction method based on stochastic decay, ensuring that Tight-Sketch incurs only small one-sided errors without overestimation. To further enhance detection accuracy under extremely constrained memory allocations, we introduce Tight-Opt, a variant incorporating two optimization strategies. We conduct extensive experiments across various detection tasks to demonstrate that Tight-Sketch significantly outperforms existing methods in terms of both accuracy and update speed. Furthermore, by utilizing Single Instruction Multiple Data (SIMD) instructions, we enhance Tight-Sketch’s update throughput by up to 36%. We also implement Tight-Sketch on FPGA to validate its practicality and low resource overhead in hardware deployments.},
  archive      = {J_TC},
  author       = {Weihe Li and Paul Patras},
  doi          = {10.1109/TC.2025.3604467},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3845-3859},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient sketching for heavy item-oriented data stream mining with memory constraints},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliability-aware optimization of task offloading for UAV-assisted edge computing. <em>TC</em>, <em>74</em>(11), 3832-3844. (<a href='https://doi.org/10.1109/TC.2025.3604463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed aerial vehicles (UAV) are widely used for edge computing in poor infrastructure scenarios due to their deployment flexibility and mobility. In UAV-assisted edge computing systems, multiple UAVs can cooperate with the cloud to provide superior computing capability for diverse innovative services. However, many service-related computational tasks may fail due to the unreliability of UAVs and wireless transmission channels. Diverse solutions were proposed, but most of them employ time-driven strategies which introduce unwanted decision waiting delays. To address this problem, this paper focuses on a task-driven reliability-aware cooperative offloading problem in UAV-assisted edge-enhanced networks. The issue is formulated as an optimization problem which jointly optimizes UAV trajectories, offloading decisions, and transmission power, aiming to maximize the long-term average task success rate. Considering the discrete-continuous hybrid action space of the problem, a dependence-aware latent-space representation algorithm is proposed to represent discrete-continuous hybrid actions. Furthermore, we design a novel deep reinforcement learning scheme by combining the representation algorithm and a twin delayed deep deterministic policy gradient algorithm. We compared our proposed algorithm with four alternative solutions via simulations and a realistic Kubernetes testbed-based setup. The test results show how our scheme outperforms the other methods, ensuring significant improvements in terms of task success rate.},
  archive      = {J_TC},
  author       = {Hao Hao and Changqiao Xu and Wei Zhang and Xingyan Chen and Shujie Yang and Gabriel-Miro Muntean},
  doi          = {10.1109/TC.2025.3604463},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3832-3844},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reliability-aware optimization of task offloading for UAV-assisted edge computing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A computation-quantized training framework to generate accuracy lossless QNNs for one-shot deployment in embedded systems. <em>TC</em>, <em>74</em>(11), 3818-3831. (<a href='https://doi.org/10.1109/TC.2025.3603732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantized Neural Networks (QNNs) have received increasing attention, since they can enrich intelligent applications deployed on embedded devices with limited resources, such as mobile devices and AIoT systems. Unfortunately, the numerical and computational discrepancies between training systems (i.e., servers) and deployment systems (e.g., embedded ends) may lead to large accuracy drop for QNNs in real deployments. We propose a Computation-Quantized Training Framework (CQTF), which simulates deployment-time fixed-point computation during training to enable one-shot, lossless deployment. The training procedure of CQTF is built upon a well-formulated quantization-specific numerical representation that quantifies both numerical and computational discrepancies between training and deployment. Leveraging this representation, forward propagation executes all computations in quantization mode to simulate deployment-time inference, while backward propagation identifies and mitigates gradient vanishing through an efficient floating-point gradient update scheme. Benchmark-based experiments demonstrate the efficiency of our approach, which can achieve no accuracy loss from training to deployment. Compared with existing five frameworks, the deployed accuracy of CQTF can be improved by up to 18.41%.},
  archive      = {J_TC},
  author       = {Xingzhi Zhou and Wei Jiang and Jinyu Zhan and Lingxin Jin and Lin Zuo},
  doi          = {10.1109/TC.2025.3603732},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3818-3831},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A computation-quantized training framework to generate accuracy lossless QNNs for one-shot deployment in embedded systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing in-network computing deployment via collaboration across planes. <em>TC</em>, <em>74</em>(11), 3805-3817. (<a href='https://doi.org/10.1109/TC.2025.3603730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The new paradigm of In-network computing (INC) permits service computation to be executed within network paths, rather than solely on dedicated servers. Although the programmable data plane has showcased notable performance advantages for INC application deployments, its effectiveness is constrained by resource limitations, potentially impeding the expressiveness and scalability of these deployments. Conversely, delegating computational tasks to the control plane, supported by general-purpose servers with abundant resources, offers increased flexibility. Nonetheless, this strategy compromises efficiency to a considerable extent, particularly when the system operates under heavy load. To simultaneously exploit the efficiency of data plane and the flexibility of control plane, we propose Carlo, a cross-plane collaborative optimization framework to support the network-wide deployment of multiple INC applications across both the control and data plane. Carlo first analyzes resource requirements of various INC applications across different planes. It then establishes mathematical models for resource allocation in cross-plane and automatically generates solutions using proposed algorithms. We have implemented the prototype of Carlo on Intel Tofino ASIC switches and DPDK. Experimental results demonstrate that Carlo can effectively trade off between computation time and deployment performance while avoiding performance degradation.},
  archive      = {J_TC},
  author       = {Xiaoquan Zhang and Lin Cui and WaiMing Lau and Fung Po Tso and Yuhui Deng and Weijia Jia},
  doi          = {10.1109/TC.2025.3603730},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3805-3817},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enhancing in-network computing deployment via collaboration across planes},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unleashing the power of differential fault attacks on QARMAv2. <em>TC</em>, <em>74</em>(11), 3792-3804. (<a href='https://doi.org/10.1109/TC.2025.3603728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {QARMAv2, a family of lightweight block ciphers introduced in ToSC 2023, is an evolution of the original QARMA design, specifically constructed to accommodate more extended tweak values while simultaneously enhancing security measures. In this paper, for the first time, we present differential fault analysis (DFA) of all the QARMAv2 variants by introducing an approach to utilize the fault propagation patterns at the nibble level, with the goal of identifying relevant faulty ciphertexts and vulnerable fault positions. Introducing six random nibble faults strategically into the ($r-1$)-th and ($r-2$)-th backward rounds of the $r$-round QARMAv2-64 significantly reduces the secret key space from $2^{128}$ to $2^{32}$. Additionally, when targeting QARMAv2-128-128, it demands the introduction of six random nibble faults to effectively reduce the secret key space from $2^{128}$ to a remarkably reduced $2^{24}$. To conclude, we also explore the potential extension of our methods to conduct DFA on other versions of QARMAv2. To the best of our knowledge, this marks the first instance of a differential fault attack targeting the QARMAv2 tweakable block cipher family, signifying an important direction in cryptographic analysis.},
  archive      = {J_TC},
  author       = {Soumya Sahoo and Debasmita Chakraborty and Santanu Sarkar},
  doi          = {10.1109/TC.2025.3603728},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3792-3804},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Unleashing the power of differential fault attacks on QARMAv2},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Load balancing scheduling for batch-ordered job-store: Online vs. offline. <em>TC</em>, <em>74</em>(11), 3778-3791. (<a href='https://doi.org/10.1109/TC.2025.3603725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient resource utilization is crucial in real-world applications, especially for balancing loads across machines handling specific job types. This paper introduces a novel batch-ordered job-store scheduling model, where jobs in a batch are scheduled sequentially, with their operations allocated in a round-robin fashion across two scenarios. We establish that this problem is NP-hard and analyze it in both online and offline settings. In the online case, we first examine the exclusive scenario, where operations within the same job must be scheduled on different machines, and show that a load greedy (LG) algorithm achieves a tight competitive ratio of $2-\frac{1}{m}$, with $m$ representing the number of machines. Next, we consider the circular scenario, which requires maintaining the circular order of operations across ordered machines. In this context, we analyze potential anomalies in load distribution during local optimality achieved by the ordered load greedy (OLG) algorithm and provide bounds on the occurrence of these anomalies and the maximum load in each local scheduling round. In the offline case, we abstract each OLG scheduling process as a generalized circular sequence alignment (CSA) problem and develop a dynamic programming-based matching (DPM) algorithm to solve it. To further enhance load balancing, we develop a dynamic programming-based optimization (DPO) algorithm to schedule multiple jobs simultaneously in both scenarios. Experimental results confirm the efficiency of DPM for the CSA problem, and we validate the load balancing effectiveness of both online and offline algorithms using real traffic datasets. These theoretical findings and algorithmic implementations lay a solid groundwork for future practical advancements.},
  archive      = {J_TC},
  author       = {Mengbing Zhou and Yang Wang and Bocong Zhao and Chengzhong Xu},
  doi          = {10.1109/TC.2025.3603725},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3778-3791},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Load balancing scheduling for batch-ordered job-store: Online vs. offline},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information sharing in multi-tenant metaverse via intent-driven multicasting. <em>TC</em>, <em>74</em>(11), 3763-3777. (<a href='https://doi.org/10.1109/TC.2025.3603720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-tenant metaverse enables multiple users in a common virtual world to interact with each other online. Information sharing will occur when interactions between a user and the environment are multicast to other users by an interactive metaverse (IM) service. However, ineffective information-sharing strategies intensify competitions among users for limited resources in networks, and fail to interpret optimization intent prompts conveyed in high-level natural languages, ultimately diminishing user immersion. In this paper, we explore reliable information sharing in a multi-tenant metaverse with time-varying resource capacities and costs, where IM services are unreliable and alter the volumes of data processed by them, while the service provider dynamically adjusts global intent to minimize multicast delays and costs. To this end, we first formulate the information sharing problem as a Markov decision process and show its NP-hardness. Then, we propose a learning-based system GTP, which combines the proximal policy optimization reinforcement learning with feature extraction networks, including graph attention network and gated recurrent unit, and a Transformer encoder for multi-feature comparison to process a sequence of incoming multicast requests without the knowledge of future arrival information. The GTP operates through three modules: a deployer that allocates primary and backup IM services across the network to minimize a weighted goal of server computation costs and communication distances between users and services, an intent extractor that dynamically infers provider intent conveyed in natural language, and a router that constructs on-demand multicast routing trees adhering to users, the provider, and network constraints. We finally conduct theoretical and empirical analysis on the proposed algorithms for the system. Experimental results show that the proposed algorithms are promising, and superior to their comparison baseline algorithms.},
  archive      = {J_TC},
  author       = {Yu Qiu and Min Chen and Weifa Liang and Lejun Ai and Dusit Niyato},
  doi          = {10.1109/TC.2025.3603720},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3763-3777},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Information sharing in multi-tenant metaverse via intent-driven multicasting},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EABE-PUFPH: Efficient attribute-based encryption with reliable policy updating under full policy hiding. <em>TC</em>, <em>74</em>(11), 3750-3762. (<a href='https://doi.org/10.1109/TC.2025.3603717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ciphertext-policy attribute-based encryption (CP-ABE) has garnered significant attention for enabling fine-grained access control over encrypted data in cloud environments. However, in traditional CP-ABE schemes, access policies are transmitted in plaintext, which can lead to sensitive information leakage. To mitigate this risk, hiding access policies has become essential. Under the condition of full hidden access policies, realizing efficient and accurate decryption and dynamic policy updating has become an urgent challenge. To tackle these challenges, we present an efficient attribute-based encryption with reliable policy updating under full policy hiding (EABE-PUFPH) scheme, which effectively integrates full policy hiding with policy updating capabilities. Furthermore, we conduct a rigorous security analysis and performance evaluation of the EABE-PUFPH scheme. Evaluation results show that the EABE-PUFPH scheme achieves full hidden access policies without affecting decryption efficiency, and its efficiency surpasses other similar schemes that achieve full policy hiding.},
  archive      = {J_TC},
  author       = {Chenghao Gu and Jiguo Li and Yichen Zhang and Yang Lu and Jian Shen},
  doi          = {10.1109/TC.2025.3603717},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3750-3762},
  shortjournal = {IEEE Trans. Comput.},
  title        = {EABE-PUFPH: Efficient attribute-based encryption with reliable policy updating under full policy hiding},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FSA-hash: Flow-size-aware sketch hashing for software switches. <em>TC</em>, <em>74</em>(11), 3736-3749. (<a href='https://doi.org/10.1109/TC.2025.3603716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern data centers and enterprise networks, software switches have become critical components for achieving flexible and efficient network management. Due to resource constraints in software switches, sketches have emerged as a promising approach for network traffic measurement. However, their accuracy is often impacted by hash collisions. Existing hash functions treat all collisions equally, failing to account for the differing impacts of collisions involving elephant flows versus mouse flows. We propose FSA-Hash, a novel flow-size-aware hashing scheme that separates elephant flows from each other and from mouse flows, minimizing the most detrimental collisions. FSA-Hash is designed based on two insights: separating elephant flows from mouse flows avoids overestimating mouse flows, while separating elephant flows from each other enables accurate heavy-hitter detection. We implement FSA-Hash using machine learning models trained on network traffic data (LFSA-Hash), and also design a lightweight online variant (OLFSA-Hash) that learns the hash model solely from sketch queries on the software switch, obviating traffic collection overheads. Evaluations across four sketches and two tasks demonstrate FSA-Hash’s superior accuracy over standard hash functions. Moreover, OLFSA-Hash closely matches LFSA-Hash’s performance, making it an attractive option for adaptively refining the hash model without monitoring traffic.},
  archive      = {J_TC},
  author       = {Fuliang Li and Kejun Guo and Yiming Lv and Jiaxing Shen and Yuting Liu and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2025.3603716},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3736-3749},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FSA-hash: Flow-size-aware sketch hashing for software switches},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NetKG: Synthesizing interpretable network router configurations with knowledge graph. <em>TC</em>, <em>74</em>(11), 3722-3735. (<a href='https://doi.org/10.1109/TC.2025.3603712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced router configuration synthesizers aim to prevent network outages by automatically synthesizing configurations that implement routing protocols. However, the lack of interpretability makes operators uncertain about how low-level configurations are synthesized and whether the automatically generated configurations correctly align with routing intents. This limitation restricts the practical deployment of synthesizers. In this paper, we present NetKG, an interpretable configuration synthesis tool. $(i)$ NetKG leverages a knowledge graph as the intermediate representation for configurations, reformulating the configuration synthesis problem as a configuration knowledge completion task; $(ii)$ NetKG regards network intents as query tasks that need to be satisfied in the current configuration space, achieving this through knowledge reasoning and completion; $(iii)$ NetKG explains the synthesis process and the consistency between configuration and intent through the configuration knowledge involved in reasoning and completion. We show that NetKG can scale to realistic networks and automatically synthesize intent-compliant configurations for static routes, OSPF, and BGP. It can explain the consistency between configuration and intent at different granularities through a visual interface. Experimental results indicate that NetKG synthesizes configurations in 2 minutes for a network with up to 197 routers, which is 7.37x faster than the SMT-based synthesizer.},
  archive      = {J_TC},
  author       = {Zhenbei Guo and Fuliang Li and Peng Zhang and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2025.3603712},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3722-3735},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NetKG: Synthesizing interpretable network router configurations with knowledge graph},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable encrypted deduplication based on location-hiding secret sharing of data keys. <em>TC</em>, <em>74</em>(11), 3710-3721. (<a href='https://doi.org/10.1109/TC.2025.3603710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encrypted deduplication is attractive because it can provide high storage efficiency while protecting data privacy. Most existing schemes achieve encrypted deduplication against brute-force attacks (BFAs) based on server-aided encryption. Unfortunately, the centralized key server in server-aided encryption can potentially become a single point of failure. To this end, distributed server-aided encryption is presented, which splits a system-level master key into multiple shares and distributes them across several key servers. However, it is hard to improve security and scalability with this method simultaneously. This paper presents a secure and scalable encrypted deduplication scheme ScalaDep. ScalaDep achieves a new design paradigm centered on location-hiding secret sharing of data keys. As the number of deployed key servers increases, the attack cost of adversaries increases while the number of requests handled by each key server decreases, enhancing both scalability and security. Furthermore, we propose a two-phase duplicate detection method for our paradigm, which utilizes short hashes and key identifiers to achieve secure duplicate detection against BFAs. Additionally, based on the allreduce algorithm, ScalaDep enables all key servers to collaboratively record the number of client requests and resist online BFAs by enforcing rate limiting. Security analysis and performance evaluation demonstrate the security and efficiency of ScalaDep.},
  archive      = {J_TC},
  author       = {Guanxiong Ha and Yuchen Chen and Chunfu Jia and Keyan Chen and Rongxi Wang and Qiaowen Jia},
  doi          = {10.1109/TC.2025.3603710},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3710-3721},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scalable encrypted deduplication based on location-hiding secret sharing of data keys},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An on-board executable pareto-based iterated local search algorithm for embedded multi-core processor task scheduling. <em>TC</em>, <em>74</em>(11), 3696-3709. (<a href='https://doi.org/10.1109/TC.2025.3603699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advancement of wearable electronic technology has facilitated the integration of smart wearable devices into artificial intelligence (AI)-driven medical assisted diagnosis. Embedded multi-core processors (MPs) have gradually emerged as pivotal hardware components for smart wearable medical diagnostic devices due to their high performance and flexibility. However, embedded MPs face the challenge of balancing performance, power consumption, and load-balancing. In response, we introduce a Pareto-based iterated local search (PILS) algorithm for task scheduling, which systematically optimizes multiple objectives, alongside a task list model to reduce the dimension of the decision space and enhance scheduling performance. In addition, we present a two-stage discretization scheme to ensure that the proposed algorithm offers meaningful guidance throughout the scheduling process. Simulation and on-board testing results show that the proposed algorithm effectively optimizes energy consumption, task execution time, and load-balancing in embedded MPs task scheduling, indicating the potential of the proposed algorithm in enhancing the performance of smart wearable medical diagnostic devices powered by embedded MPs.},
  archive      = {J_TC},
  author       = {Qinglin Zhao and Lixin Zhang and Qi Pan and Kunbo Cui and Mingqi Zhao and Fuze Tian and Bin Hu},
  doi          = {10.1109/TC.2025.3603699},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3696-3709},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An on-board executable pareto-based iterated local search algorithm for embedded multi-core processor task scheduling},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Thermal elasticity-aware host resource provision for carbon efficiency on virtualized servers. <em>TC</em>, <em>74</em>(11), 3682-3695. (<a href='https://doi.org/10.1109/TC.2025.3603698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Servers in modern data centers face increasing challenges from energy inefficiency and thermal-related outages, both of which significantly contribute to their overall carbon footprint. These challenges often arise from a lack of coordination between computational resource provisioning and thermal management capabilities. This paper introduces the concept of thermal elasticity, a system’s intrinsic ability to absorb thermal stress without requiring additional cooling, as a guiding metric for sustainable thermal management. Building on this, we propose a collaborative in-band and out-of-band resource provisioning framework that adjusts CPU allocation based on real-time thermal feedback. By leveraging a machine learning model and runtime monitoring, the framework dynamically provisions CPU clusters to virtual machines co-located on the same host. Evaluations on real servers with multiple workloads show that our method reduces peak power consumption from 5.2% to 9.6%, and lowers peak temperatures between 4${^{\boldsymbol{\circ}}}$C and 6.5${^{\boldsymbol{\circ}}}$C (up to 40${^{\boldsymbol{\circ}}}$C in extreme cases). Carbon emissions are also reduced from 7% to 37% during SPEC benchmark runs. These results highlight the framework’s potential to alleviate stress on power and cooling infrastructure, thereby enhancing energy efficiency, reducing carbon footprint, and improving service continuity during thermal challenges.},
  archive      = {J_TC},
  author       = {Da Zhang and Haojun Xia and Xiaotong Wang and Yanchang Feng and Haohao Liu and Bibo Tu},
  doi          = {10.1109/TC.2025.3603698},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3682-3695},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Thermal elasticity-aware host resource provision for carbon efficiency on virtualized servers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TeraPool: A physical design aware, 1024 RISC-V cores shared-l1-memory scaled-up cluster design with high bandwidth main memory link. <em>TC</em>, <em>74</em>(11), 3667-3681. (<a href='https://doi.org/10.1109/TC.2025.3603692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared L1-memory clusters of streamlined instruction processors (processing elements - PEs) are commonly used as building blocks in modern, massively parallel computing architectures (e.g. GP-GPUs). Scaling out these architectures by increasing the number of clusters incurs computational and power overhead, caused by the requirement to split and merge large data structures in chunks and move chunks across memory hierarchies via the high-latency global interconnect. Scaling up the cluster reduces buffering, copy, and synchronization overheads. However, the complexity of a fully connected cores-to-L1-memory crossbar grows quadratically with Processing Element (PE)-count, posing a major physical implementation challenge. We present TeraPool, a physically implementable, ${\boldsymbol &gt;} 1000$ floating-point-capable RISC-V PEs scaled-up cluster design, sharing a Multi-MegaByte ${\boldsymbol &gt;} 4000$-banked L1 memory via a low latency hierarchical interconnect (1-7/9/11 cycles, depending on target frequency). Implemented in 12 nm FinFET technology, TeraPool achieves near-gigahertz frequencies (910 MHz) typical, 0.80 V/25 $^{\boldsymbol{\circ}}$C. The energy-efficient hierarchical PE-to-L1-memory interconnect consumes only 9-13.5 pJ for memory bank accesses, just 0.74-1.1${\boldsymbol \times}$ the cost of a FP32 FMA. A high-bandwidth main memory link is designed to manage data transfers in/out of the shared L1, sustaining transfers at the full bandwidth of an HBM2E main memory. At 910 MHz, the cluster delivers up to 1.89 single precision TFLOP/s peak performance and up to 200 GFLOP/s/W energy efficiency (at a high IPC/PE of 0.8 on average) in benchmark kernels, demonstrating the feasibility of scaling a shared-L1 cluster to a thousand PEs, four times the PE count of the largest clusters reported in literature.},
  archive      = {J_TC},
  author       = {Yichao Zhang and Marco Bertuletti and Chi Zhang and Samuel Riedel and Diyou Shen and Bowen Wang and Alessandro Vanelli-Coralli and Luca Benini},
  doi          = {10.1109/TC.2025.3603692},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3667-3681},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TeraPool: A physical design aware, 1024 RISC-V cores shared-l1-memory scaled-up cluster design with high bandwidth main memory link},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRRQ: Privacy-preserving resilient RkNN query over encrypted outsourced multiattribute data. <em>TC</em>, <em>74</em>(11), 3652-3666. (<a href='https://doi.org/10.1109/TC.2025.3603688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional reverse k-nearest neighbor (RkNN) query schemes typically assume that users are available online in real-time for interactive key reception, overlooking scenarios where users might be offline. Moreover, existing privacy-preserving RkNN query schemes primarily focus on user features or spatial data, neglecting the significance of user reputation values. To address these limitations, we propose a privacy-preserving resilient RkNN query scheme over encrypted outsourced multi-attribute data (PRRQ). Specifically, to mitigate the challenges posed by resilient online presence (i.e., non-real-time online) of users for interactive key reception, we incorporate a non-interactive key exchange (NIKE) protocol and the Diffie-Hellman two-party key exchange algorithm to propose a multi-party NIKE algorithm (2K-NIKE), facilitating non-interactive key reception for multiple users. Considering the privacy leakage issues, PRRQ encodes original multi-attribute data (i.e., spatial, feature, and reputation values) alongside query requests based on formalized criteria. Additionally, we integrate the proposed 2K-NIKE and the improved symmetric homomorphic encryption (iSHE) algorithms to encrypt them. Furthermore, catering to the requirements of ciphertext-based RkNN queries, we propose a private RkNN query eligibility-checking (PREC) algorithm and a private reputation-verifying (PRRV) algorithm, which validate the compliance of encrypted outsourced multi-attribute data with query requests. Security analysis demonstrates that PRRQ achieves simulation-based security under an honest-but-curious model. Experimental results show that PRRQ offers superior computational efficiency compared to comparative schemes.},
  archive      = {J_TC},
  author       = {Jing Wang and Haiyong Bao and Na Ruan and Qinglei Kong and Cheng Huang and Hong-Ning Dai},
  doi          = {10.1109/TC.2025.3603688},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3652-3666},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PRRQ: Privacy-preserving resilient RkNN query over encrypted outsourced multiattribute data},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Concurrent linguistic error detection (CLED): A new methodology for error detection in large language models. <em>TC</em>, <em>74</em>(11), 3638-3651. (<a href='https://doi.org/10.1109/TC.2025.3603682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of Large Language Models (LLMs) requires dependable operation in the presence of errors in the hardware (caused by for example radiation) as this has become a pressing concern. At the same time, the scale and complexity of LLMs limit the overhead that can be added to detect errors. Therefore, there is a need for low-cost error detection schemes. Concurrent Error Detection (CED) uses the properties of a system to detect errors, so it is an appealing approach. In this paper, we present a new methodology and scheme for error detection in LLMs: Concurrent Linguistic Error Detection (CLED). Its main principle is that the output of LLMs should be valid and generate coherent text; therefore, when the text is not valid or differs significantly from the normal text, it is likely that there is an error. Hence, errors can potentially be detected by checking the linguistic features of the text generated by LLMs. This has the following main advantages: 1) low overhead as the checks are simple and 2) general applicability, so regardless of the LLM implementation details because the text correctness is not related to the LLM algorithms or implementations. The proposed CLED has been evaluated on two LLMs: T5 and OPUS-MT. The results show that with a 1% overhead, CLED can detect more than 87% of the errors, making it suitable to improve LLM dependability at low cost.},
  archive      = {J_TC},
  author       = {Jinhua Zhu and Javier Conde and Zhen Gao and Pedro Reviriego and Shanshan Liu and Fabrizio Lombardi},
  doi          = {10.1109/TC.2025.3603682},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3638-3651},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Concurrent linguistic error detection (CLED): A new methodology for error detection in large language models},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid approach to refine WCRT bounds for DAG scheduling using anomaly classification. <em>TC</em>, <em>74</em>(11), 3623-3637. (<a href='https://doi.org/10.1109/TC.2025.3603674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the performance demands and stringent timing requirements of safety-critical systems like avionics and autonomous vehicles, research has focused on providing timing guarantees for the scheduling of Directed Acyclic Graph (DAG) tasks in multicore systems. The structural complexity and timing anomalies make this problem challenging. Existing methods bound the Worst-Case Response Time (WCRT) of tasks through static analysis, but these bounds are complicated, difficult to validate, and often remain pessimistic for many scheduling scenarios. Runtime enforcement of scheduling constraints can be effective in eliminating timing anomalies and providing timing guarantees; however, it is unnecessary for anomaly-free scheduling scenarios, leading to non-work-conserving schedules and deliberate utilization loss. This paper proposes a hybrid approach to identify timing anomalies in DAG scheduling scenarios within a system, providing tighter WCRT solutions. The static analysis first offers a sufficient anomaly test to directly identify some anomaly-free DAG scheduling scenarios. Leveraging a wide range of scheduling data collected from the running system or its simulator, we then apply a machine learning approach to train a binary classification model, achieving an accuracy of 99.5%. Identifying the anomaly status enables the application of more precise WCRT bounds for different scheduling scenarios, leading to improved system performance. Specifically, we shorten the WCRT bounds for anomaly-free DAG scheduling by an average of up to 21.58%, with a maximum reduction of up to 55.47% compared to the state-of-the-art method.},
  archive      = {J_TC},
  author       = {Nan Chen and Xiaotian Dai and Alan Burns and Iain Bate},
  doi          = {10.1109/TC.2025.3603674},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3623-3637},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A hybrid approach to refine WCRT bounds for DAG scheduling using anomaly classification},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Caravan: Incentive-driven account migration via transaction aggregation in sharded blockchain. <em>TC</em>, <em>74</em>(11), 3609-3622. (<a href='https://doi.org/10.1109/TC.2025.3603672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain sharding is a promising solution for scalability but struggles to reach the expected performance due to the high ratio of cross-shard transactions. Account migration has emerged as a critical approach to optimizing shard performance. However, existing migration solutions suffer from inefficient handling of queued withdrawal transactions from a migrating account and inadequate priority mechanism for migration transaction, resulting in prolonged transaction makespan and reduced system throughput. This paper proposes Caravan, a novel blockchain sharding system for optimizing account migration. First, Caravan proposes a transaction aggregation-based migration scheme to efficiently handle withdrawal congestion post-migration. It incorporates a multi-level Merkle tree and cross-shard synchronization protocol to ensure cross-shard security. Second, Caravan presents an economic incentive-driven priority mechanism that motivates miners to perform transaction aggregation and prioritize migration transactions by increasing the associated revenue. Furthermore, its gas recycling strategy enables users to finance migration costs without awareness or extra expenses. Finally, we develop the Caravan prototype, deploy it on Alibaba Cloud, and experiment with real Ethereum transactions. The results show that compared to the state-of-the-art account migration schemes, Caravan significantly mitigates the transaction surge caused by migration, achieving up to a 3.2× throughput improvement and a 65% reduction in transaction confirmation latency. And users share considerable migration costs without extra expenses, significantly reduce system costs.},
  archive      = {J_TC},
  author       = {Yu Tao and Shouchen Zhou and Lu Zhou and Zhe Liu},
  doi          = {10.1109/TC.2025.3603672},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3609-3622},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Caravan: Incentive-driven account migration via transaction aggregation in sharded blockchain},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic bin packing with heterogeneous dependent bins for regionless in geo-distributed clouds. <em>TC</em>, <em>74</em>(11), 3596-3608. (<a href='https://doi.org/10.1109/TC.2025.3602297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud service providers use geo-distributed datacenters to provide resources and services to clients located in different regions. However, uneven population density leads to unbalanced development of geo-distributed datacenters and cloud service providers face a shortage of land resources to further develop datacenters in densely populated regions. Thus, it is a real challenge for cloud service providers to meet the increasing demand from clients in affluent regions with saturated resources and to better utilize underutilized data centers in other regions. To address this challenge, we study an online resource allocation problem in geo-distributed clouds, whose goal is to assign each user request upon arrival to an appropriate geographic cloud region to minimize the resulting peak utilization of resource pools with different cost coefficients. To this end, we formulate the problem as a dynamic bin packing problem with heterogeneous dependent bins where user requests correspond to items to be packed and heterogeneous cloud resources are bins. To solve this online problem with high uncertainty, we propose a simulation based memetic algorithm to generate robust offline proactive policies based on historical data, which enable fast decision making for online packing. Our experiments based on realistic data show that the proposed approach leads to a reduction in total costs of up to 15% compared to the current practice, while being much faster for decision making compared to a popular online method.},
  archive      = {J_TC},
  author       = {Yinuo Li and Jin-Kao Hao and Liwei Song},
  doi          = {10.1109/TC.2025.3602297},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3596-3608},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dynamic bin packing with heterogeneous dependent bins for regionless in geo-distributed clouds},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel indirect methodology based on execution traces for grading functional test programs. <em>TC</em>, <em>74</em>(11), 3582-3595. (<a href='https://doi.org/10.1109/TC.2025.3600005'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing functional test programs for hardware testing is time-consuming and experience-wise. A functional test program’s quality is usually assessed only through expensive fault simulation campaigns during early development. This paper presents indirect quality measurements of fault detection capabilities of functional test programs to reduce the total cost of fault simulation in the early development stages. We present a methodology that analyzes the instruction trace generated by running functional test programs on-chip and building its control and dataflow graph. We use the graph to identify potential flaws that affect the program’s fault detection capabilities. We present different graph-based techniques to measure the programs’ quality indirectly. By exploiting standard debugging formats, we individuate instructions in the source code that affect the graph-based measurements. We perform experiments on an automotive device manufactured by STMicroelectronics, running functional test programs of different natures. Our results show that our metric allows test engineers to develop better functional test programs without basing their development solely on fault simulation campaigns.},
  archive      = {J_TC},
  author       = {Francesco Angione and Paolo Bernardi and Andrea Calabrese and Lorenzo Cardone and Stefano Quer and Claudia Bertani and Vincenzo Tancorre},
  doi          = {10.1109/TC.2025.3600005},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3582-3595},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A novel indirect methodology based on execution traces for grading functional test programs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient delay-bounded dependent task offloading with service caching at edges. <em>TC</em>, <em>74</em>(11), 3568-3581. (<a href='https://doi.org/10.1109/TC.2025.3598749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We are now embracing an era of edge computing and artificial intelligence, and the combination of the two has spawned a new field of research called edge intelligence. Massive amounts of data is generated at the edge of network, which relies on artificial intelligence to realize its potential. Meanwhile, artificial intelligence is able to flourish when processing diverse edge data. However, the computation and storage resources of edge servers are not unlimited. For some large-scale intelligent applications, it is difficult to meet their service quality requirements by directly offloading the entire application to a nearby server for processing. Due to the heterogeneity of server resources in edge environments, how to balance the workload among edge servers to provide better services also becomes complicated. The goal of this paper is to minimize the total cost of offloading large-scale applications consisting of many dependent tasks in an edge system. We formulate the Dependent task Offloading with Service Caching (DOSC) problem, which is proved to be NP-hard. A dynamic planning-based algorithm is introduced to solve fixed-DOSC, in which some services are pre-configured on the edge server, and other services can not be downloaded from the remote cloud. We also present a theoretical analysis on the performance guarantee of the dynamic planning-based algorithm. Then, we propose a near-optimal algorithm using the Gibbs sampling to solve the general DOSC problem. Testbed experiments and trace-driven simulations are conducted to verify the performance of our algorithm. Our algorithm, shown to be the most effective in terms of cost, considers both service caching and task dependencies when task offloading in comparison to other baseline algorithms.},
  archive      = {J_TC},
  author       = {Yu Liang and Sheng Zhang and Jie Wu},
  doi          = {10.1109/TC.2025.3598749},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3568-3581},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Cost-efficient delay-bounded dependent task offloading with service caching at edges},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved error bounds for floating-point quotients. <em>TC</em>, <em>74</em>(11), 3559-3567. (<a href='https://doi.org/10.1109/TC.2025.3585341'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $x_{0},y_{1},\dots,y_{k}$ be nonzero floating-point numbers in base $\beta\geq 2$ and precision $p\geq 1$. Let $z:=x_{0}/y_{1}/\dots/y_{k}$, whereby the divisions are evaluated from left to right, and let $\widehat{z}$ be the corresponding floating-point evaluation according to the IEEE 754 standard in rounding to nearest. We prove that, in absence of underflow and overflow, $|\widehat{z}-z|\leq k{\bf u}|z|$ provided that $k\leq\sqrt{\omega/\beta}\ {\bf u}^{-1/3}$. Here ${\bf u}:=\frac{1}{2}\beta^{1-p}$ denotes the relative rounding error unit and $\omega:=2$ if $\beta$ is even and $\omega:=1$ if $\beta$ is odd. Thus, the relative rounding error of $k$ consecutive floating-point divisions is bounded by $k{\bf u}$. This improves on the classical Wilkinson-type bound $\gamma_{k}:=k{\bf u}/(1-k{\bf u})$.},
  archive      = {J_TC},
  author       = {Florian Bünger},
  doi          = {10.1109/TC.2025.3585341},
  journal      = {IEEE Transactions on Computers},
  month        = {11},
  number       = {11},
  pages        = {3559-3567},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improved error bounds for floating-point quotients},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TeeRollup: Efficient rollup design using heterogeneous TEE. <em>TC</em>, <em>74</em>(10), 3546-3558. (<a href='https://doi.org/10.1109/TC.2025.3596698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rollups have emerged as a promising approach to improving blockchains’ scalability by offloading transaction execution off-chain. Existing rollup solutions either leverage complex zero-knowledge proofs or optimistically assume execution correctness unless challenged. However, these solutions suffer from high gas costs and significant withdrawal delays, hindering their adoption in decentralized applications. This paper introduces TeeRollup, an efficient rollup protocol that leverages Trusted Execution Environments (TEEs) to achieve both low gas costs and short withdrawal delays. Sequencers (i.e., system participants) execute transactions within TEEs and upload signed execution results to the blockchain with confidential keys of TEEs. Unlike most TEE-assisted blockchain designs, TeeRollup adopts a practical threat model where the integrity and availability of TEEs may be compromised. To address these issues, we first introduce a distributed system of sequencers with heterogeneous TEEs, ensuring system security even if a certain proportion of TEEs are compromised. Second, we propose a challenge mechanism to solve the redeemability issue caused by TEE unavailability. Furthermore, TeeRollup incorporates Data Availability Providers (DAPs) to reduce on-chain storage overhead and uses a laziness penalty mechanism to regulate DAP behavior. We implement a prototype of TeeRollup in Golang, using the Ethereum test network, Sepolia. Our experimental results indicate that TeeRollup outperforms zero-knowledge rollups (ZK-rollups), reducing on-chain verification costs by approximately 86% and withdrawal delays to a few minutes.},
  archive      = {J_TC},
  author       = {Xiaoqing Wen and Quanbi Feng and Hanzheng Lyu and Jianyu Niu and Yinqian Zhang and Chen Feng},
  doi          = {10.1109/TC.2025.3596698},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3546-3558},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TeeRollup: Efficient rollup design using heterogeneous TEE},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallelization strategies for DeepMD-kit using OpenMP: Enhancing efficiency in machine learning-based molecular simulations. <em>TC</em>, <em>74</em>(10), 3534-3545. (<a href='https://doi.org/10.1109/TC.2025.3595078'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DeepMD-kit enables deep learning-based molecular dynamics (MD) simulations that require efficient parallelization to leverage modern HPC architectures. In this work, we optimize DeepMD-kit using advanced OpenMP strategies to improve scalability and computational efficiency on an ARMv8 processor-based server. Our optimizations include data parallelism for neural network inference, force calculation acceleration, NUMA-aware memory management, and synchronization reductions, leading to up to $4.1\boldsymbol{\times}$ speedup and 82% higher memory bandwidth efficiency compared to the baseline implementation. Strong scaling analysis demonstrates superlinear speedup at mid-range core counts, with improved workload balancing and vectorized computations. However, challenges remain at ultra-large scales due to increasing synchronization overhead.},
  archive      = {J_TC},
  author       = {Qi Du and Feng Wang and Chengkun Wu},
  doi          = {10.1109/TC.2025.3595078},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3534-3545},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Parallelization strategies for DeepMD-kit using OpenMP: Enhancing efficiency in machine learning-based molecular simulations},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-Radix/Mixed-radix NTT multiplication Algorithm/Architecture co-design over fermat modulus. <em>TC</em>, <em>74</em>(10), 3519-3533. (<a href='https://doi.org/10.1109/TC.2025.3590972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polynomial multiplication using Number Theoretic Transform (NTT) is crucial in lattice-based post-quantum cryptography (PQC) and fully homomorphic encryption (FHE), with modulus $q$ significantly affecting performance. Fermat moduli of the form $2^{2^{n}}+1$, such as 65537, offer efficiency gains due to simplified modular reduction and powers-of-2 twiddle factors in NTT. While Fermat moduli have been directly applied or explored for incorporation into existing schemes, Fermat NTT-based polynomial multiplication designs remain underexplored in fully exploiting the benefits of Fermat moduli. This work presents a high-radix/mixed-radix NTT architecture tailored for Fermat moduli, which improves the utilization of the powers-of-2 twiddle factors in large transform sizes. In most cases, our design achieves a 30%–85% reduction in DSP area-time product (ATP) and a 70%–100% reduction in BRAM ATP compared to state-of-the-art designs with smaller or equivalent modulus, while maintaining competitive LUT and FF ATP, underscoring the potential of Fermat NTT-based polynomial multipliers in lattice-based cryptography.},
  archive      = {J_TC},
  author       = {Yile Xing and Guangyan Li and Zewen Ye and Ryan W. L. Luk and Donglong Chen and Hong Yan and Ray C. C. Cheung},
  doi          = {10.1109/TC.2025.3590972},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3519-3533},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-Radix/Mixed-radix NTT multiplication Algorithm/Architecture co-design over fermat modulus},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EC2P: Cost-effective cross-chain payments via hubs resisting the abort attack. <em>TC</em>, <em>74</em>(10), 3504-3518. (<a href='https://doi.org/10.1109/TC.2025.3590960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-chain technology facilitates the interoperability among isolated blockchains, where users can transfer and exchange coins. While the heterogeneity between Turing-complete (TC) blockchains like Ethereum and non-Turing-complete (NTC) blockchains like Bitcoin presents a significant challenge for cross-chain transactions. Payment Channel Hubs (PCHs) offer a promising solution for enabling TC-NTC cross-chain payments with high throughput and low confirmation delays. However, existing schemes still face two key challenges: (i) significant computation and communication overhead for variable-amount payment, and (ii) limited unlinkability, i.e., vulnerable to the abort attack. This paper proposes EC2P, the first TC-NTC cross-chain PCH that achieves variable-amount payment unlinkability while resisting the abort attack and minimizing reliance on non-interactive zero-knowledge (NIZK) proofs. EC2P introduces two protocols: the NTC-to-TC and TC-to-NTC payment protocols. The NTC-to-TC payment protocol replaces the traditional puzzle-promise and puzzle-solve paradigm with a semi-blind approach, where only one side is blinded and the blinded side’s interactions are eliminated. This achieves unlinkability and resists the abort attack without NIZK. The TC-to-NTC payment protocol enhances the paradigm by utilizing Turing-complete functionality to constrain the inability to carry out an abort attack. Through rigorous security analysis, we show that EC2P is secure and variable-amount payment unlinkable while resisting the abort attack. We implement EC2P on Ethereum and Bitcoin test networks. Our evaluation demonstrates that EC2P outperforms both in terms of communication and computation overhead and reduces communication costs by 3 orders of magnitude compared to existing variable-amount methods.},
  archive      = {J_TC},
  author       = {Danlei Xiao and Shaobo Xu and Chuan Zhang and Licheng Wang and Xiulong Liu and Liehuang Zhu},
  doi          = {10.1109/TC.2025.3590960},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3504-3518},
  shortjournal = {IEEE Trans. Comput.},
  title        = {EC2P: Cost-effective cross-chain payments via hubs resisting the abort attack},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LAShards: Low-overhead and self-adaptive MRC construction for non-stack algorithms. <em>TC</em>, <em>74</em>(10), 3490-3503. (<a href='https://doi.org/10.1109/TC.2025.3590811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared cache systems have become increasingly crucial, especially in cloud services, where the Miss Ratio Curve (MRC) is a widely used tool for evaluating cache performance. The MRC depicts the relationship between the cache miss ratio and cache size, indicating how cache performance trends with varying cache sizes. Recent advancements have enabled efficient MRC construction for stack replacement policies. For non-stack policies, miniature simulation downsizes the actual cache size and data stream through spatially hashed sampling, providing a general method for MRC construction. However, this approach still faces significant challenges. Firstly, constructing an MRC requires numerous mini-caches to obtain miss ratios, consuming significant cache resources, leading to tremendous memory and computing overhead. Secondly, it cannot adapt to the dynamic I/O workloads, resulting in less precise MRC. To address these issues, we propose LAShards, a low-overhead and self-adaptive MRC construction method for non-stack replacement policies. The key idea behind LAShards is to exploit the locality and burstiness in access patterns. It can statically reduce memory usage and dynamically adapt to workloads. Compared to previous works, LAShards can save up to $20\boldsymbol{\times}$ of memory resources, and increase throughput by up to $10\boldsymbol{\times}$.},
  archive      = {J_TC},
  author       = {Sanle Zhao and Yujuan Tan and Zhaoyang Zeng and Jing Yu and Zhuoxin Bai and Ao Ren and Xianzhang Chen and Duo Liu},
  doi          = {10.1109/TC.2025.3590811},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3490-3503},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LAShards: Low-overhead and self-adaptive MRC construction for non-stack algorithms},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRECIOUS: Approximate real-time computing in MLC-MRAM based heterogeneous CMPs. <em>TC</em>, <em>74</em>(10), 3476-3489. (<a href='https://doi.org/10.1109/TC.2025.3590809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enhancing quality of service (QoS) in approximate-computing (AC) based real-time systems, without violating power limits is becoming increasingly challenging due to contradictory constraints, i.e., power consumption and time criticality, as multicore computing platforms are becoming heterogeneous. To fulfill these constraints and optimise system QoS, AC tasks should be judiciously mapped on such platforms. However, prior approaches rarely considered the problem of AC task deployment on heterogeneous platforms. Moreover, the majority of prior approaches typically neglect the runtime architectural phenomena, which can be accounted for along with the approximation tolerance of the applications to enhance the QoS. We present PRECIOUS, a novel hybrid offline-online approach that first schedules AC real-time tasks on a heterogeneous multicore with an objective to maximise QoS and determines the appropriate cluster for each task constrained by a system-wide power limit, deadline, and task-dependency. At runtime, PRECIOUS introduces novel architectural techniques for the AC tasks, where tasks are executed on a heterogeneous platform equipped with multilevel-cell (MLC)-MRAM based last-level cache to improve energy efficiency and performance by prudentially leveraging storage density of MLC-MRAM while ameliorating associated high write latency and write energy. Our novel block management for the MLC-MRAM cache further improves performance of the system, which we exploit opportunistically to enhance system QoS, and turn off processor cores during the dynamically generated slacks. PRECIOUS-Offline achieves up to 76% QoS for a specific task-set, surpassing prior art, whereas PRECIOUS-Online enhances QoS by 9.0% by reducing cache miss-rate by 19% on a 64-core heterogeneous system without incurring any energy overhead over a conventional MRAM based cache design.},
  archive      = {J_TC},
  author       = {Sangeet Saha and Shounak Chakraborty and Sukarn Agarwal and Magnus Själander and Klaus McDonald-Maier},
  doi          = {10.1109/TC.2025.3590809},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3476-3489},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PRECIOUS: Approximate real-time computing in MLC-MRAM based heterogeneous CMPs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A highly reliable multiplexing scheme in hypercube-structured hierarchical networks. <em>TC</em>, <em>74</em>(10), 3462-3475. (<a href='https://doi.org/10.1109/TC.2025.3589732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design and optimization of network topologies play a critical role in ensuring the performance and efficiency of high-performance computing (HPC) systems. Traditional topology designs often fall short in satisfying the stringent requirements of HPC environments, particularly with respect to fault tolerance, latency, and bandwidth. To address these limitations, we propose a novel class of hierarchical networks, termed Hypercube-Structured Hierarchical Networks (HHNs). This architecture generalizes and extends existing architectures such as half hypercube networks and complete cubic networks, while also introducing previously unexplored hierarchical designs. HHNs exhibit several advantages, particularly in high-performance computing. Most notably, their high connectivity enables efficient parallel data processing, and their hierarchical structure supports scalability to accommodate growing computational demands. Furthermore, we present a unicast routing strategy and a broadcast algorithm for HHNs. A fault-tolerant algorithm is also designed based on the construction of disjoint paths. Experimental evaluations demonstrate that HHNs consistently outperform mainstream architectures in critical performance metrics, including scalability, latency, and robustness to failures.},
  archive      = {J_TC},
  author       = {Xuanli Liu and Zhenjiang Dong and Weibei Fan and Mengjie Lv and Xueli Sun and Jin Qi and Sun-Yuan Hsieh},
  doi          = {10.1109/TC.2025.3589732},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3462-3475},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A highly reliable multiplexing scheme in hypercube-structured hierarchical networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The case for secure miniservers beyond the edge. <em>TC</em>, <em>74</em>(10), 3448-3461. (<a href='https://doi.org/10.1109/TC.2025.3589691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beyond edge devices can function off the power grid and without batteries, making them suitable for deployment in hard-to-reach environments. As the energy budget is extremely tight, energy-hungry long-distance communication required for offloading computation or reporting results to a server becomes a significant limitation. Based on the observation that the energy required for communication decreases with shorter distances, this paper makes a case for the deployment of secure beyond edge miniservers. These are strategically positioned, lightweight local servers designed to support beyond edge devices without compromising the privacy of sensitive information. We demonstrate that even for relatively small scale representative computations – which are more likely to fit into the tight power budget of a beyond edge device for local processing – deploying a beyond edge miniserver can lead to higher performance. To this end, we consider representative deployment scenarios of practical importance, including but not limited to agricultural systems or building structures, where beyond edge miniservers enable highly energy-efficient real-time data processing.},
  archive      = {J_TC},
  author       = {Salonik Resch and Hüsrev Cılasun and Zamshed I. Chowdhury and Masoud Zabihi and Yang Lv and Jian-Ping Wang and Sachin S. Sapatnekar and Ismail Akturk and Ulya R. Karpuzcu},
  doi          = {10.1109/TC.2025.3589691},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3448-3461},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The case for secure miniservers beyond the edge},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A highly scalable network architecture for optical data centers. <em>TC</em>, <em>74</em>(10), 3433-3447. (<a href='https://doi.org/10.1109/TC.2025.3589688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical Data Center Networks (ODCNs) are high-performance interconnect architectures in parallel and distributed computing, providing higher bandwidth and lower power consumption. However, current optical DCNs struggle to achieve both high scalability and incremental scalability simultaneously. In this paper, we propose an extended Exchanged hyperCube, denoted by ExCube, which is a highly scalable network architecture for optical data centers. Firstly, we detail the address scheme and constructing method for ExCube, including exponential, linear, and composite scalability, which can adapt to different scalability requirements. ExCube boasts flexible scalability modes, including exponential, linear, and composite scalability, meeting diverse scalability requirements. In particular, the diameter of ExCube remains unchanged as its size increases linearly, indicating superior incremental scalability. Secondly, an efficient routing algorithm with linear time complexity is presented to determine the shortest path between any two different ToRs in ExCube. Additionally, we propose a per-flow scheduling algorithm based on the disjoint paths to enhance the performance of ExCube. The optical devices in ExCube are identical to those in existing optical DCNs, such as WaveCube and OSA, facilitating its construction. Experimental results demonstrate that ExCube outperforms WaveCube in terms of throughput and reduces data transmission time by 5%-35%. Further analysis reveals that ExCube maintains comparable performance to WaveCube across several critical metrics, including low diameter and link complexity. Compared with advanced networks, the overall cost-effectiveness and energy efficiency of ExCube have been reduced by 36.7% and 46.5%, respectively.},
  archive      = {J_TC},
  author       = {Weibei Fan and Yao Pan and Fu Xiao and Pinchang Zhang and Lei Han and Sun-Yuan Hsieh},
  doi          = {10.1109/TC.2025.3589688},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3433-3447},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A highly scalable network architecture for optical data centers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GATe: Efficient graph attention network acceleration with near-memory processing. <em>TC</em>, <em>74</em>(10), 3419-3432. (<a href='https://doi.org/10.1109/TC.2025.3588317'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Attention Network (GAT) has gained widespread adoption thanks to its exceptional performance in processing non-Euclidean graphs. The critical components of a GAT model involve aggregation and attention, which cause numerous main-memory access, occupying significant inference time. Recently, much research has proposed near-memory processing (NMP) architectures to accelerate aggregation. However, graph attention requires additional operations distinct from aggregation, making previous NMP architectures less suitable for supporting GAT, as they typically target aggregation-only workloads. In this paper, we propose GATe, a practical and efficient GAT accelerator with NMP architecture. To the best of our knowledge, this is the first time that accelerates both attention and aggregation computation on DIMM. We unify feature vector access to eliminate the two repetitive memory accesses to source nodes caused by the sequential phase-by-phase execution of attention and aggregation. Next, we refine the computation flow to reduce data dependencies in concatenation and softmax, which lowers on-chip memory usage and communication overhead. Additionally, we introduce a novel sharding method that enhances data reusability of high-degree nodes. Experiments show that GATe achieves substantial speedup of GAT attention and aggregation phases up to 6.77${\boldsymbol\times}$ and 2.46${\boldsymbol\times}$, with average to 3.69${\boldsymbol\times}$ and 2.24${\boldsymbol\times}$, respectively, compared to state-of-the-art NMP works GNNear and GraNDe.},
  archive      = {J_TC},
  author       = {Shiyan Yi and Yudi Qiu and Guohao Xu and Lingfei Lu and Xiaoyang Zeng and Yibo Fan},
  doi          = {10.1109/TC.2025.3588317},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3419-3432},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GATe: Efficient graph attention network acceleration with near-memory processing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trajectory optimization and power allocation for multi-UAV wireless networks: A communication-based multi-agent deep reinforcement learning approach. <em>TC</em>, <em>74</em>(10), 3404-3418. (<a href='https://doi.org/10.1109/TC.2025.3587976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed Aerial Vehicles (UAVs) play a crucial role in next-generation mobile communication systems, serving as aerial base stations to provide services when ground base stations fail to meet coverage requirements. However, trajectory planning and power allocation for collaborative UAVs as Aerial Base Stations (UAV-ABSs) face several challenges, including energy limitations, flight time constraints, high optimization complexity due to dynamic environment interactions, and insufficient decision-making information. To address these challenges, this paper proposes a multi-agent reinforcement learning algorithm, namely Communication Actor Centralized Attention Critic Algorithm (CATEN), to jointly optimize the flight trajectory and power allocation strategies of UAV-ABSs. The proposed algorithm aims to maximize the number of users meeting Quality of Service (QoS) requirements while minimizing UAV-ABSs energy consumption. To achieve this, firstly, an information sharing mechanism is designed to improve the collaboration efficiency among UAV-ABSs. It leverages distributed storage, intelligent scheduling of UAV-ABSs interaction experiences, and gating units to enhance information screening and fusion. Secondly, a multi-head attention critic network is proposed to capture correlations among UAV-ABSs from different subspaces. This allows the network to prioritize value information, reduce redundancy, and strengthen UAV-ABSs collaboration and decision-making capabilities. Simulation results demonstrate that CATEN achieves better performance in terms of the number of served users and energy consumption compared to existing algorithms, exhibiting good robustness and adaptability in dynamic environments.},
  archive      = {J_TC},
  author       = {Zimeng Yuan and Yuanguo Bi and Yanbo Fan and Yuheng Liu and Lianbo Ma and Liang Zhao and Qiang He},
  doi          = {10.1109/TC.2025.3587976},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3404-3418},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Trajectory optimization and power allocation for multi-UAV wireless networks: A communication-based multi-agent deep reinforcement learning approach},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WOLF: Weight-level OutLier and fault integration for reliable LLM deployment. <em>TC</em>, <em>74</em>(10), 3390-3403. (<a href='https://doi.org/10.1109/TC.2025.3587957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of Transformer-based large language models (LLMs) is presenting significant challenges for their deployment, primarily due to their enormous parameter sizes and intermediate results, which create a bottleneck in memory capacity for effective inference. Compared to traditional DRAM, Non-Volatile Memory (NVM) technologies such as Resistive Random-Access Memory (RRAM) and Phase-Change Memory (PCM) offer higher integration density, making them promising alternatives. However, before NVM can be widely adopted, its reliability issues, particularly manufacturing defects and endurance faults, must be addressed. In response to the limited memory capacity and reliability challenges of deploying LLMs in NVM, we introduce a novel low-overhead weight-level map, named Wolf. Wolf not only integrates the addresses of faulty weights to support efficient fault tolerance but also includes the addresses of outlier weights in LLMs. This allows for tensor-wise segmented quantization of both outliers and regular weights, enabling lower-bitwidth quantization. The Wolf framework uses a Bloom Filter-based map to efficiently manage outliers and faults. By employing shared hashes for outliers and faults and specific hashes for faults, Wolf significantly reduces the area overhead. Building on Wolf, we propose a novel fault tolerance method that resolves the observed issue of clustering critical incorrect outliers and fully leverages the inherent resilience of LLMs to improve fault tolerance capabilities. As a result, Wolf achieves segment-wise INT4 quantization with enhanced accuracy. Moreover, Wolf can adeptly handle Bit Error Rates as high as $1 {\boldsymbol{\times}} 10^{-2}$ without compromising accuracy, in stark contrast to the state-of-the-art approach where accuracy declines by more than 20%.},
  archive      = {J_TC},
  author       = {Chong Wang and Wanyi Fu and Jiangwei Zhang and Shiyao Li and Rui Hou and Jian Yang and Yu Wang},
  doi          = {10.1109/TC.2025.3587957},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3390-3403},
  shortjournal = {IEEE Trans. Comput.},
  title        = {WOLF: Weight-level OutLier and fault integration for reliable LLM deployment},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high-efficiency parallel mechanism for canonical polyadic decomposition on heterogeneous computing platform. <em>TC</em>, <em>74</em>(10), 3377-3389. (<a href='https://doi.org/10.1109/TC.2025.3587623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Canonical Polyadic decomposition (CPD) obtains the low-rank approximation for high-order multidimensional tensors through the summation of a sequence of rank-one tensors, greatly reducing storage and computation overhead. It is increasingly being used in the lightweight design of artificial intelligence and big data processing. The existing CPD technology exhibits inherent limitations in simultaneously achieving high accuracy and high efficiency. In this paper, a heterogeneous computing method for CPD is proposed to optimize computing efficiency with guaranteed convergence accuracy. Specifically, a quasi-convex decomposition loss function is constructed and the extreme points of the Kruskal matrix rows have been solved. Further, the massively parallelized operators in the algorithm are extracted, a software-hardware integrated scheduling method is designed, and the deployment of CPD on heterogeneous computing platforms is achieved. Finally, the memory access strategy is optimized to improve memory access efficiency. We tested the algorithm on real-world and synthetic sparse tensor datasets, numerical experimental results show that compared with the state-of-the-art method, the proposed method has a higher convergence accuracy and computing efficiency. Compared to the standard CPD parallel library, the method achieves efficiency improvements of tens to hundreds of times while maintaining the same accuracy.},
  archive      = {J_TC},
  author       = {Xiaosong Peng and Laurence T. Yang and Xiaokang Wang and Debin Liu and Jie Li},
  doi          = {10.1109/TC.2025.3587623},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3377-3389},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A high-efficiency parallel mechanism for canonical polyadic decomposition on heterogeneous computing platform},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable and efficient multi-path transmission based on disjoint paths in data center networks. <em>TC</em>, <em>74</em>(10), 3362-3376. (<a href='https://doi.org/10.1109/TC.2025.3587618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-path transmission enables load balancing and improves network performance in data center networks (DCNs). It increases the possibility of network congestion and makes traditional network traffic engineering methods inefficient due to the uneven distribution of network traffic in data centers. In this paper, we present a reliable and efficient Disjoint paths based Multi-Path Transmission scheme (DMPT) that selects distributed requests through topology awareness. Firstly, we propose disjoint path construction algorithms through rigorous theoretical proof, aiming at the different transmission requirements of DCNs. Secondly, we offer an optimal solution to the disjoint multi-path selection problem, which is aimed at the trade-off between link load and transmission time. Furthermore, DMPT can split the flow over multiple transmission paths based on the link status. Finally, extensive experiments are executed for DMPT on a novel EHDC of DCN that is based on exchanged hypercube. The experimental results show that DMPT can reduce the average running time by 18.6%, and the average path length is close to the optimal path. Furthermore, it achieves significant improvements in balancing network link traffic and facilitating deployment, which also reflects the advantages of topology aware multiplexing in practice.},
  archive      = {J_TC},
  author       = {Weibei Fan and Yao Pan and Fu Xiao and Mengjie Lv and Lei Han and Shui Yu},
  doi          = {10.1109/TC.2025.3587618},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3362-3376},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reliable and efficient multi-path transmission based on disjoint paths in data center networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a unified framework for modeling and analyzing user-defined online non-preemptive scheduling policies. <em>TC</em>, <em>74</em>(10), 3347-3361. (<a href='https://doi.org/10.1109/TC.2025.3587514'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a unified formal framework, called ReTA, that allows users to define scheduling problems using a user-friendly domain-specific language (DSL) and automatically obtain response times of jobs in return. ReTA supports user-defined online scheduling policies (beyond work-conserving or priority-based scheduling) for heterogeneous computing resource types with multiple instances per type (e.g., multiple CPU cores, GPUs, DSPs, and FPGAs on one single chip), thus supporting global, partitioned, and clustered scheduling. In the current version of ReTA, we focus on non-preemptive periodic tasks as these are susceptible to scheduling anomalies and hence harder to analyze. ReTA performs response-time analysis by constructing a timed labeled transition system (TLTS) from the domain model as a basis for performing a reachability analysis enriched with efficient state-space reduction techniques. Our empirical evaluations show that ReTA identifies up to 50 times more schedulable task sets than fixed-point iteration-based analyses. With a runtime on the order of a few minutes, ReTA produces highly accurate results two-orders of magnitude faster than an exact Timed Automata-based analysis in UPPAAL (e.g., for systems with 16 cores and 32 tasks).},
  archive      = {J_TC},
  author       = {Pourya Gohari and Jeroen Voeten and Mitra Nasri},
  doi          = {10.1109/TC.2025.3587514},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3347-3361},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards a unified framework for modeling and analyzing user-defined online non-preemptive scheduling policies},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scavenger+: Revisiting space-time tradeoffs in key-value separated LSM-trees. <em>TC</em>, <em>74</em>(10), 3332-3346. (<a href='https://doi.org/10.1109/TC.2025.3587513'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are widely used in storage systems but face significant challenges, such as high write amplification caused by compaction. KV-separated LSM-trees address write amplification but introduce significant space amplification, a critical concern in cost-sensitive scenarios. Garbage collection (GC) can reduce space amplification, but existing strategies are often inefficient and fail to account for workload characteristics. Moreover, current key-value (KV) separated LSM-trees overlook the space amplification caused by the index LSM-tree. In this paper, we systematically analyze the sources of space amplification in KV-separated LSM-trees and propose Scavenger+, which achieves a better performance-space trade-off. Scavenger+ introduces (1) an I/O-efficient garbage collection scheme to reduce I/O overhead, (2) a space-aware compaction strategy based on compensated size to mitigate index-induced space amplification, and (3) a dynamic GC scheduler that adapts to system load to make better use of CPU and storage resources. Extensive experiments demonstrate that Scavenger+ significantly improves write performance and reduces space amplification compared to state-of-the-art KV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.},
  archive      = {J_TC},
  author       = {Jianshun Zhang and Fang Wang and Jiaxin Ou and Yi Wang and Ming Zhao and Sheng Qiu and Junxun Huang and Baoquan Li and Peng Fang and Dan Feng},
  doi          = {10.1109/TC.2025.3587513},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3332-3346},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scavenger+: Revisiting space-time tradeoffs in key-value separated LSM-trees},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ML-PTA: A two-stage ML-enhanced framework for accelerating nonlinear DC circuit simulation with pseudo-transient analysis. <em>TC</em>, <em>74</em>(10), 3319-3331. (<a href='https://doi.org/10.1109/TC.2025.3587470'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct current (DC) analysis lies at the heart of integrated circuit design in seeking DC operating points. Although pseudo-transient analysis (PTA) methods have been widely used in DC analysis in both industry and academia, their initial parameters and stepping strategy require expert knowledge and labor tuning to deliver efficient performance, which hinders their further applications. In this paper, we leverage the latest advancements in machine learning to deploy PTA with more efficient setups for different problems. More specifically, active learning, which automatically draws knowledge from other circuits, is used to provide suitable initial parameters for PTA solver, and then calibrate on-the-fly to further accelerate the simulation process using TD3-based reinforcement learning (RL). To expedite model convergence, we introduce dual agents and a public sampling buffer in our RL method to enhance sample utilization. To further improve the learning efficiency of the RL agent, we incorporate imitation learning to improve reward function and introduce supervised learning to provide a better dual-agent rotation strategy. We make the proposed algorithm a general out-of-the-box SPICE-like solver and assess it on a variety of circuits, demonstrating up to 3.10$\boldsymbol\times$ reduction in NR iterations for the initial stage and 285.71$\boldsymbol\times$ for the RL stage.},
  archive      = {J_TC},
  author       = {Zhou Jin and Wenhao Li and Haojie Pei and Xiaru Zha and Yichao Dong and Xiang Jin and Xiao Wu and Dan Niu and Wei W. Xing},
  doi          = {10.1109/TC.2025.3587470},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3319-3331},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ML-PTA: A two-stage ML-enhanced framework for accelerating nonlinear DC circuit simulation with pseudo-transient analysis},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UKFaaS: Lightweight, high-performance and secure FaaS communication with unikernel. <em>TC</em>, <em>74</em>(10), 3305-3318. (<a href='https://doi.org/10.1109/TC.2025.3586031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unikernel is a promising runtime for serverless computing with its lightweight and isolated architecture. It offers a secure and efficient environment for applications. However, famous serverless frameworks like Knative have introduced heavyweight component sidecars to assist function instance deployment in a non-intrusive manner. But the sidecar not only hinders the throughput of unikernel function services but also consumes excessive memory resources. Moreover, the intricate network communication pathways among various services pose significant challenges for deploying unikernels in production serverless environments. Although shared-memory based communication on the same server can solve the communication bottleneck of unikernel-based function instances. The situation where malicious programs on the server make the shared memory untrustworthy limits the deployment of such technologies. We propose UKFaaS, a lightweight and high-performance serverless framework. UKFaaS leverages the advantages of customized operating systems through unikernel and it non-intrusively integrates sidecar functionality into the unikernel, avoiding the overhead of sidecar request forwarding. Additionally, UKFaaS innovatively implements data communication between unikernels in the same server to eliminate VM-Exit bottlenecks in RPC (remote process call) based on VMFUNC without relying on memory sharing. The preliminary experimental results indicate that UKFaaS can realize $1.8\boldsymbol{\times}$-$3.5\boldsymbol{\times}$ request throughput per second (RPS) compared with the advanced serverless system FaasFlow, UaaF and Nightcore in the Google online boutique microservice benchmark.},
  archive      = {J_TC},
  author       = {Zhenqian Chen and Yuchun Zhan and Peng Hu and Xinkui Zhao and Muyu Yang and Siwei Tan and Lufei Zhang and Liqiang Lu and Jianwei Yin and Zuoning Chen},
  doi          = {10.1109/TC.2025.3586031},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3305-3318},
  shortjournal = {IEEE Trans. Comput.},
  title        = {UKFaaS: Lightweight, high-performance and secure FaaS communication with unikernel},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RV-CURE: A RISC-V capability architecture for full memory safety. <em>TC</em>, <em>74</em>(10), 3291-3304. (<a href='https://doi.org/10.1109/TC.2025.3586029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memory-safety violations remain persistent in the real world. Although a tagged-pointer concept has demonstrated significant practical potential, prior work has shown scalability limitations in both performance and security. In this paper, we revisit the tagged-pointer design based on our observation that a pointer tag, stored in a pointer address, can be associated with security metadata and used as a hash to look up a hash table that stores associated metadata. To realize our idea as a new tagging-based memory-capability model, we investigate a hardware-software co-design approach. First, we develop a generalized tagging method, data-pointer tagging (DPT), to ensure full memory safety. DPT assigns a 16-bit tag to each memory object and associates that tag with the object’s capability metadata. On a memory access, DPT then performs a capability check using its associated metadata and validates the access. Furthermore, we design a RISC-V capability architecture, RV-CURE, that implements hardware extensions for DPT and thus enables robust, efficient capability enforcement. Altogether, we prototype a RISC-V evaluation framework, in which we launch FPGA instances running the Linux OS and conduct a full-system simulation. Our evaluation shows that RV-CURE imposes 9.5$-$ 19.6% runtime overhead for the SPEC 2017 C/C++ workloads while ensuring strong memory safety.},
  archive      = {J_TC},
  author       = {Yonghae Kim and Anurag Kar and Jaewon Lee and Jaekyu Lee and Hyesoon Kim},
  doi          = {10.1109/TC.2025.3586029},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3291-3304},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RV-CURE: A RISC-V capability architecture for full memory safety},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaptDQC: Adaptive distributed quantum computing with quantitative performance analysis. <em>TC</em>, <em>74</em>(10), 3277-3290. (<a href='https://doi.org/10.1109/TC.2025.3586027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present AdaptDQC, an adaptive compiler framework for optimizing distributed quantum computing (DQC) under diverse performance metrics and inter-chip communication (ICC) architectures. AdaptDQC leverages a novel spatial-temporal graph model to describe quantum circuits, model ICC architectures, and quantify critical performance metrics in DQC systems, yielding a systematic and adaptive approach to constructing circuit-partitioning and chip-mapping strategies that admit hybrid ICC architectures and are optimized against various objectives. Experimental results on a collection of benchmarks show that AdaptDQC outperforms state-of-the-art compiler frameworks: It reduces, on average, the communication cost by up to 35.4% and the latency by up to 38.4%.},
  archive      = {J_TC},
  author       = {Debin Xiang and Liqiang Lu and Siwei Tan and Xinghui Jia and Zhe Zhou and Guangyu Sun and Mingshuai Chen and Jianwei Yin},
  doi          = {10.1109/TC.2025.3586027},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3277-3290},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AdaptDQC: Adaptive distributed quantum computing with quantitative performance analysis},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FlashDecoding++Next: High throughput LLM inference with latency and memory optimization. <em>TC</em>, <em>74</em>(10), 3263-3276. (<a href='https://doi.org/10.1109/TC.2025.3585339'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the Large Language Model (LLM) becomes increasingly important in various domains, the performance of LLM inference is crucial to massive LLM applications. However, centering around the computational efficiency and the memory utilization, the following challenges remain unsolved in achieving high-throughput LLM inference: (1) Synchronous partial softmax update. The softmax operation requires a synchronous update operation among each partial softmax result, leading to $\sim$20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference tends to be flat, leading to under-utilized computation and 50% performance loss after padding zeros in previous designs (e.g., cuBLAS, CUTLASS, etc.). (3) Memory redundancy caused by activations. Dynamic allocation of activations during inference leads to redundant storage of useless variables, bringing 22% more memory consumption. We present FlashDecoding++Next, a high-throughput inference engine supporting mainstream LLMs and hardware backends. To tackle the above challenges, FlashDecoding++Next creatively proposes: (1) Asynchronous softmax with unified maximum. FlashDecoding++Next introduces a unified maximum technique for different partial softmax computations to avoid synchronization. Based on this, a fine-grained pipelining is proposed, leading to 1.18$\boldsymbol{\times}$ and 1.14$\boldsymbol{\times}$ for the prefill and decode phases in LLM inference, respectively. (2) Flat GEMM optimization with double buffering. FlashDecoding++Next points out that flat GEMMs with different shapes face varied bottlenecks. Then, techniques like double buffering are introduced, resulting in up to 52% speedup for the flat GEMM operation. (3) Buffer reusing and unified memory management. FlashDecoding++Next reuses the pre-allocated activation buffers throughout the inference process to remove redundancy. Based on that, we unify the management of different types of storage to further exploit the reusing opportunity. The memory optimization enables up to 1.57$\boldsymbol{\times}$ longer sequence to be processed. FlashDecoding++Next demonstrates remarkable throughput improvement, delivering up to 68.88$\boldsymbol{\times}$ higher throughput compared to the HuggingFace [1] implementation. On average, FlashDecoding++Next achieves 1.25$\boldsymbol{\times}$ and 1.46$\boldsymbol{\times}$ higher throughput compared to vLLM [2] and TensorRT-LLM [3] on mainstream LLMs.},
  archive      = {J_TC},
  author       = {Guohao Dai and Ke Hong and Qiuli Mao and Xiuhong Li and Jiaming Xu and Haofeng Huang and Hongtu Xia and Xuefei Ning and Shengen Yan and Yun Liang and Yu Wang},
  doi          = {10.1109/TC.2025.3585339},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3263-3276},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FlashDecoding++Next: High throughput LLM inference with latency and memory optimization},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sifter: An efficient operator auto-tuner with speculative design space exploration for deep learning compiler. <em>TC</em>, <em>74</em>(10), 3251-3262. (<a href='https://doi.org/10.1109/TC.2024.3441820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning compiler can automatically optimize operators. It provides higher flexibility compared to vendor libraries. However, existing DNN operator tuning methods mostly rely on search-based approaches, which still face challenges such as large design spaces and long tuning times. To address these issues, we propose Sifter, an efficient DNN operator auto-tuner with speculative design space exploration. By training and analyzing decision trees, we extract shared characteristics of high-quality schedules and summarize them as pruning rules. Applying these rules during the optimization allows us to speculatively explore the design space, minimize unnecessary hardware measurements, and shorten the optimization time without compromising the optimization result. We conducted experiments on three different platforms with various operators and models. The results demonstrate that Sifter reduces 52% of redundant schedules and shortens the optimization time by 41% while maintaining operator optimization performance at the state-of-the-art level.},
  archive      = {J_TC},
  author       = {Qianhe Zhao and Rui Wang and Yi Liu and Hailong Yang and Zhongzhi Luan and Depei Qian},
  doi          = {10.1109/TC.2024.3441820},
  journal      = {IEEE Transactions on Computers},
  month        = {10},
  number       = {10},
  pages        = {3251-3262},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Sifter: An efficient operator auto-tuner with speculative design space exploration for deep learning compiler},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid redundancy for reliable task offloading in collaborative edge computing. <em>TC</em>, <em>74</em>(9), 3238-3250. (<a href='https://doi.org/10.1109/TC.2025.3587620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative edge computing enables task execution on the computing resources of geo-distributed edge nodes. One of the key challenges in this field is to realize reliable task offloading by deciding whether to execute tasks locally or delegate them to neighboring nodes while ensuring task reliability. Achieving reliable task offloading is essential for preventing task failures and maintaining optimal system performance. Existing works commonly rely on task redundancy strategies, such as active or passive redundancy. However, these approaches lack adaptive redundancy mechanisms to respond to changes in the network environment, potentially resulting in resource wastage from excessive redundancy or task failures due to insufficient redundancy. In this work, we introduce a novel approach called Hybrid Redundancy for Task Offloading (HRTO) to optimize task latency and reliability. Specifically, HRTO utilizes deep reinforcement learning (DRL) to learn a task offloading policy that maximizes task success rates. With this policy, edge nodes dynamically adjust task redundancy levels based on real-time network load conditions and meanwhile assess whether the task instance is necessary for re-execution in case of task failure. Extensive experiments on real-world network topologies and a Kubernetes-based testbed evaluate the effectiveness of HRTO, showing a 14.6% increase in success rate over the benchmarks.},
  archive      = {J_TC},
  author       = {Hao Guo and Lei Yang and Qingfeng Zhang and Jiannong Cao},
  doi          = {10.1109/TC.2025.3587620},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3238-3250},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hybrid redundancy for reliable task offloading in collaborative edge computing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient LUT6-based montgomery modular multiplication using radix-16 booth method. <em>TC</em>, <em>74</em>(9), 3223-3237. (<a href='https://doi.org/10.1109/TC.2025.3587619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores more efficient LUT6-Based multiplier for Montgomery Modular Multiplication (MMM) on FPGA. Firstly, we analyze and compare the LUT6-Cost of multipliers using previous Radix-4/8/16 Booth methods. Based on the LUT6, we propose an improved LUT6-Based Radix-16 Booth method for Coarsely Integrated Operand Scanning (CIOS) MMM. When the value of multiples remains unchanged for a period of time, this method uses LUT6-Based SDP-RAM to store multiples instead of MUX to select, which effectively reduces the LUT6-Cost of Decode. Secondly, we propose a new LUT6-Based CIOS MMM using the proposed method. Finally, we explore the trade-off between area and latency, present an efficient LUT6-Based MMM hardware design (LUT6-MM) to accelerate CIOS MMM. The LUT6-MM is a scalable, parametric, and reconfigurable design, it is implemented on Xilinx Virtex-7 FPGA. When performing 1024/2048-bit MMM, the results show that the area*latency products (ALPs) of LUT6-MM ($w_{1}$ =32, $w_{2}$ =128) are 51.1%/48.8% of the previous state-of-art scalable and parametric LUT6-Based reference (Non-Reconfigurable, Not using DSP).},
  archive      = {J_TC},
  author       = {Yujun Xie and Yuan Liu},
  doi          = {10.1109/TC.2025.3587619},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3223-3237},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An efficient LUT6-based montgomery modular multiplication using radix-16 booth method},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNA: A general dynamic neural network accelerator. <em>TC</em>, <em>74</em>(9), 3210-3222. (<a href='https://doi.org/10.1109/TC.2025.3587617'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the demonstrated superiority, dynamic neural networks (NNs), which adapt their network structures to different inputs, have been recognized as an optimized alternative to conventional static NNs. However, researchers have not explored the implications of dynamic NN on neural processing unit (NPU) architecture design. Consequently, we analyze the characteristics and inefficient sources of executing dynamic NNs on existing hardware. From our analysis, existing NPUs, designed for static NNs, cannot effectively handle the execution of dynamic operator and agent-dependent data loading in dynamic NNs. To this end, we present DNA, an efficient accelerator optimized to deal with the challenges of running general dynamic NNs. Firstly, to improve the execution efficiency of dynamic operators, we propose a transverter-based online scheduling strategy to rapidly generate efficient scheduling for each dynamic operator. Secondly, to mitigate hardware idleness caused by the non-deterministic and agent-dependent data access patterns in dynamic NNs, we propose a novel predictor-based prefetching strategy that achieves effective data preloading with negligible cost. We implemented our accelerator, DNA, by integrating an additional online scheduler into a typical many-core baseline accelerator. According to our evaluation of various dynamic NNs, DNA achieves $3.48\boldsymbol\times$ speedup and $3.03\boldsymbol\times$ energy savings over the baseline accelerator.},
  archive      = {J_TC},
  author       = {Lian Liu and Jinxin Yu and Mengdi Wang and Xiaowei Li and Yinhe Han and Ying Wang},
  doi          = {10.1109/TC.2025.3587617},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3210-3222},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DNA: A general dynamic neural network accelerator},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic generation of system-level test for un-core logic of large automotive SoC. <em>TC</em>, <em>74</em>(9), 3195-3209. (<a href='https://doi.org/10.1109/TC.2025.3587515'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional structural tests are powerful automatic approaches for capturing faulty behavior in integrated circuits. Besides the ease of generating test patterns, structural methods are known to be able to cover a vast but incomplete spectrum of all possible faults in a System-on-Chip (SoC). A new step in the manufacturing test flow has been added to fill the leftover gaps of structural tests, called the System-Level Test (SLT), which resembles the final workload, and environment. This work illustrates how to build up an automated generation engine to synthesize SLT programs that effectively attack structural test weaknesses from both a holistic and an analytical perspective. The methodology targets the crossbar module, as one of the most critical areas in the SoC, and it simultaneously creates a ripple effect across the un-core logic. Experimental results are conducted on an automotive SoC manufactured by STMicroelectronics.},
  archive      = {J_TC},
  author       = {Francesco Angione and Paolo Bernardi and Giusy Iaria and Claudia Bertani and Vincenzo Tancorre},
  doi          = {10.1109/TC.2025.3587515},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3195-3209},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Automatic generation of system-level test for un-core logic of large automotive SoC},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PolyLUT: Ultra-low latency polynomial inference with hardware-aware structured pruning. <em>TC</em>, <em>74</em>(9), 3181-3194. (<a href='https://doi.org/10.1109/TC.2025.3586311'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded these operations inside FPGA lookup tables (LUTs). However, FPGA LUTs can implement a much greater variety of functions. In this paper, we propose a novel approach to training DNNs for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with minimal overhead. By using polynomial building blocks, we achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area improvements. LUT-based implementations also face a significant challenge: the LUT size grows exponentially with the number of inputs. Prior work relies on a priori fixed sparsity, with results heavily dependent on seed selection. To address this, we propose a structured pruning strategy using a bespoke hardware-aware group regularizer that encourages a particular sparsity pattern that leads to a small number of inputs per neuron. We demonstrate the effectiveness of PolyLUT on three tasks: network intrusion detection, jet identification at the CERN Large Hadron Collider, and MNIST.},
  archive      = {J_TC},
  author       = {Marta Andronic and Jiawen Li and George A. Constantinides},
  doi          = {10.1109/TC.2025.3586311},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3181-3194},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PolyLUT: Ultra-low latency polynomial inference with hardware-aware structured pruning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic memory optimisations: Precision tuning in heterogeneous memory hierarchies. <em>TC</em>, <em>74</em>(9), 3168-3180. (<a href='https://doi.org/10.1109/TC.2025.3586025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Balancing energy efficiency and high performance in embedded systems requires fine-tuning hardware and software components to co-optimize their interaction. In this work, we address the automated optimization of memory usage through a compiler toolchain that leverages DMA-aware precision tuning and mathematical function memorization. The proposed solution extends the llvm infrastructure, employing the taffo plugins for precision tuning, with the SeTHet extension for DMA-aware precision tuning and luTHet for automated, DMA-aware mathematical function memorization. We performed an experimental assessment on hero, a heterogeneous platform employing risc-v cores as a parallel accelerator. Our solution enables speedups ranging from $1.5\boldsymbol{\times}$ to $51.1\boldsymbol{\times}$ on AxBench benchmarks that employ trigonometrical functions and $4.23-48.4\boldsymbol{\times}$ on Polybench benchmarks over the baseline hero platform.},
  archive      = {J_TC},
  author       = {Gabriele Magnani and Daniele Cattaneo and Lev Denisov and Giuseppe Tagliavini and Giovanni Agosta and Stefano Cherubin},
  doi          = {10.1109/TC.2025.3586025},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3168-3180},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Synergistic memory optimisations: Precision tuning in heterogeneous memory hierarchies},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task optimization allocation in vehicle based edge computing systems with deep reinforcement learning. <em>TC</em>, <em>74</em>(9), 3156-3167. (<a href='https://doi.org/10.1109/TC.2025.3585630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the recent advancement in network technologies, the vehicle based medical networks extend medical services to mobile vehicles, thereby offering flexible and efficient healthcare services for vehicle users in need. The integration of vehicle based medical network and edge computing enables computation intensive medical service tasks to be offloaded on edge servers, to provide fast service response for vehicle users. An efficient task offloading and resource allocation strategy is critical for Vehicle based Medical Edge Computing System (VMECS) to satisfy real-time and reliability requirements while ensuring service performance. To this end, in this paper, we investigate the problem of task computation allocation in VMECS networks. By introducing deep reinforcement learning, we first present a novel VMECS architecture to automatically achieve the optimal task offloading and resource allocation through the multi-agent collaboration, thereby improving service performance. Then, we formulate the problem of task offloading and resource allocation in VMECS networks as an optimization model with the aim of maximizing task success rate by jointly considering communication interferences, resource allocation and delay requirements. To solve it, we further devise a Distributed distributional deterministic policy gradients based Task offloading and Resource allocation (DTR) algorithm. Final simulation results demonstrate that compared with benchmark algorithms, DTR algorithm can obtain higher task success rate, smaller service time, and less task processing time.},
  archive      = {J_TC},
  author       = {Qiang He and Quanwei Li and Chuangchuang Zhang and Xingwei Wang and Yuanguo Bi and Liang Zhao and Ammar Hawbani and Keping Yu},
  doi          = {10.1109/TC.2025.3585630},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3156-3167},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Task optimization allocation in vehicle based edge computing systems with deep reinforcement learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconfigurable intelligent surface assisted UAV-MCS based on transformer enhanced deep reinforcement learning. <em>TC</em>, <em>74</em>(9), 3143-3155. (<a href='https://doi.org/10.1109/TC.2025.3585361'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowd sensing (MCS) is an emerging paradigm that enables participants to collaborate on various sensing tasks. UAVs are increasingly integrated into MCS systems to provide more reliable, accurate and cost-effective sensing services. However, optimizing UAV trajectories and communication efficiency, especially under non-line-of-sight (NLoS) channel conditions, remains a significant challenge. This paper proposes TRAIL, a Transformer-enhanced deep reinforcement Learning (DRL) algorithm. TRAIL aims to jointly optimize UAV trajectories and Reconfigurable Intelligent Surface (RIS) phase shifts to maximize data throughput while minimizing UAV energy consumption. The optimization problem is modeled as a Markov Decision Process (MDP), where the Transformer architecture captures long-term dependencies in UAV trajectories, and these features are input into a Double Deep Q-Network with Prioritized Experience Replay (PER-DDQN) to guide the agent in learning the optimal strategy. Simulation results demonstrate that TRAIL significantly outperforms state-of-the-art methods in both data throughput and energy efficiency.},
  archive      = {J_TC},
  author       = {Qianqian Wu and Qiang Liu and Ying He and Zefan Wu},
  doi          = {10.1109/TC.2025.3585361},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3143-3155},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reconfigurable intelligent surface assisted UAV-MCS based on transformer enhanced deep reinforcement learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A high-intensity solution of hardware accelerator for sparse and redundant computations in semantic segmentation models. <em>TC</em>, <em>74</em>(9), 3129-3142. (<a href='https://doi.org/10.1109/TC.2025.3585354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of artificial intelligence (AI) has met people's personalized needs. However, with the increase of data capacities and computing requirements, the imbalance between large-scale data transmission and limited network bandwidth has become increasingly prominent. To improve the speed of embedded system, real-time intelligent computing is gradually moving from the cloud to the edge. Traditional FPGA-based AI accelerators mainly utilize PE architecture, but the low computing throughput and resource utilization make it difficult to meet the power requirement of edge AI application scenarios such as image segmentation. In recent years, AI accelerators based on streaming architecture have become a trend, and it is necessary to customize high-performance streaming accelerators for specific segmentation algorithms. In this paper, we design a high-intensity pixel-level fully pipelined accelerator with customized strategies to eliminate the sparse and redundant computations in specific algorithms of semantic segmentation, which significantly improve the accelerator's computing throughput and hardware resources utilization. On Xilinx FPGA, our acceleration of two typical semantic segmentation networks-ESPNet and DeepLabV3, achieves optimized throughputs of 171.3 GOPS and 1324.8 GOPS, and computing efficiency of 9.26 and 9.01, respectively. It provides the possibility of hardware deployment in real-time application with high computing intensity.},
  archive      = {J_TC},
  author       = {Jiahui Huang and Zhan Li and Yuxian Jiang and Zhihan Zhang and Hao Wang and Sheng Chang},
  doi          = {10.1109/TC.2025.3585354},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3129-3142},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A high-intensity solution of hardware accelerator for sparse and redundant computations in semantic segmentation models},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RO(SE)${}^{2}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow><mml:mn> </mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>: Search-efficient robust searchable encryption with forward and backward security. <em>TC</em>, <em>74</em>(9), 3114-3128. (<a href='https://doi.org/10.1109/TC.2025.3585349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic searchable symmetric encryption (DSSE) enables clients to store encrypted data on untrusted servers while retaining the ability to search and update the data efficiently. However, most existing DSSE schemes are vulnerable to incorrect update queries, such as duplicated insertions or invalid deletions, which can compromise both security and availability. Although existing robust schemes have made progress in addressing these issues, they still suffer from significant search inefficiencies, particularly when handling large numbers of updates. To overcome these limitations, we propose RO(SE)${}^{2}$<mml:math><mml:msup><mml:mrow><mml:mn> </mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>, a novel robust DSSE scheme that simultaneously achieves robustness, forward-and-Type-III-backward security, and optimal search performance. RO(SE)${}^{2}$<mml:math><mml:msup><mml:mrow><mml:mn> </mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math> introduces a hierarchical binary tree structure combined with an oblivious map (OMAP) to handle incorrect updates during the update phase, eliminating the need for filtering during search queries and significantly improving search efficiency. Additionally, RO(SE)${}^{2}$<mml:math><mml:msup><mml:mrow><mml:mn> </mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math> employs a two-layer encryption mechanism to ensure forward security and supports efficient search result verification through its verifiable extension, RO(SE)${}^{2}$<mml:math><mml:msup><mml:mrow><mml:mn> </mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>-v. Rigorous security analysis proves that RO(SE)${}^{2}$<mml:math><mml:msup><mml:mrow><mml:mn> </mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math> can achieve not only robustness, forward and backward security but optimal search efficiency as well. Comparative analysis reveals that RO(SE)${}^{2}$<mml:math><mml:msup><mml:mrow><mml:mn> </mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math> outperforms existing robust schemes in terms of search performance, while RO(SE)${}^{2}$<mml:math><mml:msup><mml:mrow><mml:mn> </mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>-v outperforms the state-of-the-art verifiable robust schemes in verification performance.},
  archive      = {J_TC},
  author       = {Xu Yang and Qiuhao Wang and Saiyu Qi and Ke Li and Yong Qi},
  doi          = {10.1109/TC.2025.3585349},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3114-3128},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RO(SE)${}^{2}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow><mml:mn> </mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>: Search-efficient robust searchable encryption with forward and backward security},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COFFA: A co-design framework for fused-grained reconfigurable architecture towards efficient irregular loop handling. <em>TC</em>, <em>74</em>(9), 3099-3113. (<a href='https://doi.org/10.1109/TC.2025.3585345'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coarse-Grained Reconfigurable Architecture (CGRA) emerges as a competitive accelerator due to its high flexibility and energy efficiency. However, most CGRAs are effective for computation-intensive applications with regular loops but struggle with irregular loops containing control flows. These loops introduce fine-grained logic operations and are costly to execute by coarse-grained arithmetic units in CGRA. Efficiently handling such logic operations necessitates incorporating Boolean algebra optimization, which can improve logic density and reduce logic depth. Unfortunately, no previous research has incorporated it into the compilation flow to support irregular loops efficiently. We propose COFFA, an open-source framework for heterogeneous architecture with a RISC-V CPU and a fused-grained reconfigurable accelerator, which integrates coarse-grained arithmetic and fine-grained logic units, along with flexible IO units and distributed interconnects. As a software/hardware co-design framework, COFFA has a powerful compiler that extracts and optimizes fine-grained logic operations from irregular loops, performs coarse-grained arithmetic and memory optimizations, and offloads the loops to the accelerator. Across various challenging benchmarks with irregular loops, COFFA achieves significant performance and energy efficiency improvements over an in-order, an out-of-order RISC-V CPUs, and a recent FPGA, respectively. Moreover, compared with the state-of-the-art CGRA UE-CGRA and Hycube, COFFA can achieve 2.5$\times$ and 3.5$\times$ performance gains, respectively.},
  archive      = {J_TC},
  author       = {Yuan Dai and Xuchen Gao and Yunhui Qiu and Jingyuan Li and Yuhang Cao and Yiqing Mao and Sichao Chen and Wenbo Yin and Wai-Shing Luk and Lingli Wang},
  doi          = {10.1109/TC.2025.3585345},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3099-3113},
  shortjournal = {IEEE Trans. Comput.},
  title        = {COFFA: A co-design framework for fused-grained reconfigurable architecture towards efficient irregular loop handling},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating robustness of subnetworks for the split-star network. <em>TC</em>, <em>74</em>(9), 3087-3098. (<a href='https://doi.org/10.1109/TC.2025.3584558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The robustness of subnetworks for the interconnection network of a computer system is an important consideration for the system performance. It can be measured by the extent to which subnetworks can stay fault-free when faults are present in the system. In this paper, we evaluate the subnetwork robustness for the n-dimensional split-star network $S_{n}^{2}$. Let $S_{n-m}^{2}$, $1\leq m\leq n-3$, be a subnetwork of $S_{n}^{2}$, and let p be the node reliability, the probability that a single node remains fault-free. We determine two values that reflect how robust $S_{n-m}^{2}$ subnetworks are, from two perspectives. We first establish the upper/lower bounds for $\mathcal{F}_{m}(S_{n}^{2})$, the minimum number of faulty nodes to make all $S_{n-m}^{2}$ subnetworks faulty. Then, we determine the subnetwork reliability, denoted by $\mathcal{R}_{m}(S_{n}^{2},p)$, which is the probability that at least one fault-free $S_{n-m}^{2}$ subnetwork exists in $S_{n}^{2}$, given the node reliability p. The upper/lower bounds and an approximation expression for $\mathcal{R}_{m}(S_{n}^{2},p)$ are obtained. We also propose a simulation method to estimate $\mathcal{R}_{m}(S_{n}^{2},p)$. The experimental results show that a) when p is relatively low, $\mathcal{R}_{m}(S_{n}^{2},p)$ can be approximated by the mean value of its upper and lower bounds, or the estimation value by the approximation expression; b) when p is high, $\mathcal{R}_{m}(S_{n}^{2},p)$ can be more accurately estimated by our simulation method.},
  archive      = {J_TC},
  author       = {Kai Feng and Guodong Xie and Zhangjian Ji and Dajin Wang},
  doi          = {10.1109/TC.2025.3584558},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3087-3098},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Evaluating robustness of subnetworks for the split-star network},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AXI-REALM: Safe, modular and lightweight traffic monitoring and regulation for heterogeneous mixed-criticality systems. <em>TC</em>, <em>74</em>(9), 3072-3086. (<a href='https://doi.org/10.1109/TC.2025.3584530'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The automotive industry is transitioning from federated, homogeneous, interconnected devices to integrated, heterogeneous, mixed-criticality systems (MCS). This leads to challenges in achieving timing predictability techniques due to access contention on shared resources, which can be mitigated using hardware-based spatial and temporal isolation techniques. Focusing on the interconnect as the point of access for shared resources, we propose AXI-REALM, a lightweight, modular, technology-independent, and open-source real-time extension to AXI4 interconnects. AXI-REALM uses a budget-based mechanism enforced on periodic time windows and transfer fragmentation to provide fair arbitration, coupled with execution predictability on real-time workloads. AXI-REALM features a comprehensive bandwidth and latency monitor at both the ingress and egress of the interconnect system. Latency information is also used to detect and reset malfunctioning subordinates, preventing missed deadlines. We provide a detailed cost assessment in a $12\,$nm node and an end-to-end case study implementing AXI-REALM into an open-source MCS, incurring an area overhead of less than $2\,\%$. When running a mixed-criticality workload, with a time-critical application sharing the interconnect with non-critical applications, we demonstrate that the critical application can achieve up to $68.2\,\%$ of the isolated performance by enforcing fairness on the interconnect traffic through burst fragmentation, thus reducing the subordinate access latency by up to 24 times. Near-ideal performance, (above $95\,\%$ of the isolated performance) can be achieved by distributing the available bandwidth in favor of the critical application.},
  archive      = {J_TC},
  author       = {Thomas Benz and Alessandro Ottaviano and Chaoqun Liang and Robert Balas and Angelo Garofalo and Francesco Restuccia and Alessandro Biondi and Davide Rossi and Luca Benini},
  doi          = {10.1109/TC.2025.3584530},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3072-3086},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AXI-REALM: Safe, modular and lightweight traffic monitoring and regulation for heterogeneous mixed-criticality systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDC+: A cooperative approach to memory-efficient fork-based checkpointing for in-memory database systems. <em>TC</em>, <em>74</em>(9), 3059-3071. (<a href='https://doi.org/10.1109/TC.2025.3584520'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consistent checkpointing is a critical for in-memory databases (IMDBs) but its resource-intensive nature poses challenges for small- and medium-sized deployments in cloud environments, where memory utilization directly affects operational costs. Although traditional fork-based checkpointing offers merits in terms of performance and implementation simplicity, it incurs a considerable rise in memory footprint during checkpointing, particularly under update-intensive workloads. Memory provisioning emerges as a practical remedy to handle peak demands without compromising performance, albeit with potential concerns related to memory over-provisioning. In this article, we propose MDC+, a memory-efficient fork-based checkpointing scheme designed to maintain a reasonable memory footprint during checkpointing by leveraging collaboration among an IMDB, a user-level memory allocator, and the operating system. We explore two key techniques within the checkpointing scheme: (1) memory dump-based checkpointing, which enables early memory release, and (2) hint-based segregated memory allocation, which isolates immutable and updatable data to minimize page duplication. Our evaluation demonstrates that MDC+ significantly lowers peak memory footprint during checkpointing without affecting throughput or checkpointing time.},
  archive      = {J_TC},
  author       = {Cheolgi Min and Jiwoong Park and Heon Young Yeom and Hyungsoo Jung},
  doi          = {10.1109/TC.2025.3584520},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3059-3071},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MDC+: A cooperative approach to memory-efficient fork-based checkpointing for in-memory database systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic DPU offloading and computational resource management in heterogeneous systems. <em>TC</em>, <em>74</em>(9), 3046-3058. (<a href='https://doi.org/10.1109/TC.2025.3584501'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DPU offloading has emerged as a promising way to enhance data processing efficiency and free up host CPU resources. However, unsuitable offloading may overwhelm the hardware and hurt overall system performance. It is still unclear how to make full use of the shared hardware resources and select optimal execution units for each tenant application. In this paper, we propose DORM, a dynamic DPU offloading and resource management architecture for multi-tenant cloud environments with CPU-DPU heterogeneous platforms. The primary goal of DORM is to minimize host resource consumption and maximize request processing efficiency. By establishing a joint optimization model for offloading decision and resource allocation, we abstract the problem into a mixed integer programming mathematical model. To simplify the complexity of model-solving, we decompose the model into two subproblems: a 0-1 integer programming model for offloading decision-making and a convex optimization problem for fine-grained resource allocation. Besides, DORM presents an orchestrator agent to detect load changes and dynamically adjust the scheduling strategy. Experimental results demonstrate that DORM significantly improves resource efficiency, reducing host CPU core usage by up to 83.3%, increasing per-core throughput by up to 4.61x, and lowering the latency by up to 58.5% compared to baseline systems.},
  archive      = {J_TC},
  author       = {Zhaoyang Huang and Yanjie Tan and Yifu Zhu and Huailiang Tan and Keqin Li},
  doi          = {10.1109/TC.2025.3584501},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3046-3058},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dynamic DPU offloading and computational resource management in heterogeneous systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypercall-oriented abnormal VM status detection system: A non-intrusive solution for both hypervisor and guests. <em>TC</em>, <em>74</em>(9), 3032-3045. (<a href='https://doi.org/10.1109/TC.2025.3584274'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypervisor is a VMM (Virtual Machine Monitor) that creates and runs multiple VMs (Virtual Machines) through abstracting resources from a physical machine. Hypercall is a special and crucial call used in virtualized systems as it serves as a main communication channel between VMs and the hypervisor. However, hypercall attacks occur when an attacker manipulates the communication channel, and it could cause abnormal VM status, potentially leading to the abnormal resource allocation of the host OS (Operating System) and crash of VMs. Therefore, the virtualized system should execute abnormal VM status detection to identify potential abnormal behaviors to protect VMs and the host OS; however, existing works are either for reconstructing the hypervisor or hardware isolation, not for the VM status detection for abnormal hypercall. This study develops a hypercall-oriented abnormal VM status detection system called HypercallDetector based on the following three innovations: 1) we implement a hypercall tracing based on eBPF to obtain the hypercall-related running status (including CPU usage, memory usage, network traffic, etc.) of each VM; 2) we implement a window division technology to divide the VM status into multiple status windows of the same size, and appropriate window size with balanced detection precision (95.0%) and latency (within 8.8 ms) obtained by proposing the window regulator; and 3) we implement a CS-H algorithm (Compressing Sensing for Hypercall) to distinguish whether the VM status is abnormal. HypercallDetector shows higher precision and lower latency than its opponent and consumes only 8.6% CPU of single core and 0.3% memory usage when starting 240 VMs.},
  archive      = {J_TC},
  author       = {Fangqi Bi and Guoqi Xie and Yuan Wang and Hao Wen and Zhenli He and Shaowen Yao and Sirong Zhao and Chenglai Xiong and Xingyu Hu and Bo Wan and Yiwen Jiang},
  doi          = {10.1109/TC.2025.3584274},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3032-3045},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hypercall-oriented abnormal VM status detection system: A non-intrusive solution for both hypervisor and guests},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fed-OGD: Mitigating straggler effects in federated learning via orthogonal gradient descent. <em>TC</em>, <em>74</em>(9), 3018-3031. (<a href='https://doi.org/10.1109/TC.2025.3584272'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) faces challenges due to straggler clients that impede timely parameter uploads, potentially leading to suboptimal global model performance. Existing approaches using synchronous and asynchronous communication suffer from long waiting times or convergence issues. We propose Fed-OGD, a novel asynchronous FL method addressing the straggler problem through gradient orthogonalization. Our approach innovatively frames the straggler issue using catastrophic forgetting theory, viewing stragglers as instances of the global model “forgetting” to aggregate their parameters. Fed-OGD introduces an Orthogonal Gradient Descent (OGD) technique that caches straggler gradients and orthogonalizes the difference between these and current active client gradients. By projecting active gradients onto straggler orthogonal bases and subtracting the resulting components, we obtain orthogonalized gradients guiding the model towards optimality. We provide theoretical convergence guarantees and demonstrate Fed-OGD’s effectiveness through extensive experiments. Our method achieves state-of-the-art performance across multiple datasets among SOTA FL baselines, with notable improvements in non-IID (non-Independent and identically distributed) scenarios: there are few main categories with many samples while other categories hold few samples in a client. Fed-OGD achieves that 16.66% increase in accuracy on CIFAR-10, and significant gains on CIFAR-100 (5.37%), Tiny-ImageNet (38.51%), and AG_NEWS (16.30%).},
  archive      = {J_TC},
  author       = {Wei Li and Zicheng Shen and Xiulong Liu and Chuntao Ding and Jiaxing Shen},
  doi          = {10.1109/TC.2025.3584272},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3018-3031},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fed-OGD: Mitigating straggler effects in federated learning via orthogonal gradient descent},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intra- and inter-layer scheduling exploration and optimization for ReRAM-based DNN accelerators. <em>TC</em>, <em>74</em>(9), 3003-3017. (<a href='https://doi.org/10.1109/TC.2025.3584270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resistive Random Access Memory (ReRAM) based architectures have shown great potential for realizing energy-efficient Deep Neural Network (DNN) acceleration. When deploying a DNN, the ReRAM-based designs need a scheduling scheme to translate massive hardware resources into actual performance. Different scheduling schemes would lead to different levels of data reuse and computational parallelism, resulting in different energy efficiency and performance. However, the ReRAM-based scheduling scheme faces the following limitations. First, current studies mainly focus on intra-layer scheduling scheme optimizations by using the Weight Stationary (WS) data flow. These studies ignore the difference between layers and limit optimization opportunities. Second, there is no systematic definition and analysis for inter-layer scheduling schemes. Third, there is no co-optimization study on intra- and inter-layer scheduling schemes. Fourth, the complex network structure leads to intricate inter-layer data dependency, making the optimization of the scheduling scheme more challenging. These limitations restrict the comprehensive understanding of the scheduling schemes. Inspired by these observations, we identify the fundamental impact of intra-layer scheduling schemes on ReRAM-based designs, including the WS and Input Stationary (IS) data flows. We also systematically define and analyze inter-layer scheduling schemes according to different combinations of data flows, including the WS-WS, IS-IS, WS-IS, and IS-WS data flows. We analyze and explore different resource allocation strategies for these schemes. We also propose the intra- and inter-layer co-optimization to further improve performance and energy efficiency. Then, we propose a method for building a hybrid scheduling scheme by flexibly combining these inter-layer scheduling schemes for complex networks. Finally, we seek the potential to improve performance and energy efficiency for hybrid scheduling schemes. For deploying the MobileNet-V1, ResNet-18, VGG-16, and AlexNet, the hybrid scheduling scheme improves performance by $10.2\times$ $\sim$ $130.7\times$, $1.9\times$ $\sim$ $16.5\times$, $7.0\times$ $\sim$ $56\times$, and $1\times$ $\sim$ $153.1\times$ than the WS-WS, IS-IS, IS-WS, and WS-IS based scheduling schemes, respectively. Similarly, the power efficiency can also be increased by $15\times$ and $26\times$ than the WS-WS and IS-IS based scheduling schemes, respectively.},
  archive      = {J_TC},
  author       = {Yunping Zhao and Sheng Ma and Tiejun Li and Jianmin Zhang and Yuhua Tang},
  doi          = {10.1109/TC.2025.3584270},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {3003-3017},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Intra- and inter-layer scheduling exploration and optimization for ReRAM-based DNN accelerators},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STEMS: Spatial-temporal mapping for spiking neural networks. <em>TC</em>, <em>74</em>(9), 2991-3002. (<a href='https://doi.org/10.1109/TC.2025.3584201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neural Networks (SNNs) are event-driven bio-inspired neural networks. Recent research has trained SNN models with accuracy on par with Artificial Neural Networks (ANNs) on computer vision tasks. Due to their sparse, event-based computation, SNNs are particularly promising for energy-efficient processing, especially in event-based vision applications. However, neurons have internal states which evolve over time and keeping track of them can be costly. Hence, efficiently deploying them, especially on memory-constrained edge devices, requires careful mapping of their computation across both spatial and temporal dimensions. To address this issue, we introduce STEMS, Spatial-Temporal Mapping for SNNs. STEMS supports inter-layer mapping exploration, as well as loop tiling optimizations. By applying STEMS inter-layer exploration, we show up to 12× reduction in external memory traffic and up-to 5× reduction in energy consumption. Finally, we show that neuron states may not be needed in early SNN layers. By optimizing neuron states in one of our benchmarks, we reduced neuron states by 20x and improved energy performance by 1.4x saving without sacrificing accuracy.},
  archive      = {J_TC},
  author       = {Sherif Eissa and Sander Stuijk and Floran de Putter and Andrea Nardi-Dei and Federico Corradi and Henk Corporaal},
  doi          = {10.1109/TC.2025.3584201},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2991-3002},
  shortjournal = {IEEE Trans. Comput.},
  title        = {STEMS: Spatial-temporal mapping for spiking neural networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BHerd: Accelerating federated learning by selecting beneficial herd of local gradients. <em>TC</em>, <em>74</em>(9), 2977-2990. (<a href='https://doi.org/10.1109/TC.2025.3583827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of computer architecture, Federated Learning (FL) is a paradigm of distributed machine learning in edge systems. However, the systems’ Non-Independent and Identically Distributed (Non-IID) data negatively affect the convergence efficiency of the global model, since only a subset of these data samples is beneficial for accelerating model convergence. In pursuit of this subset, a reliable approach involves determining a measure of validity to rank the samples within the dataset. In this paper, we propose the BHerd strategy, which selects a beneficial herd of local gradients to accelerate the convergence of the FL model. Specifically, we map the distribution of the local dataset to the local gradients and use the Herding strategy to obtain a permutation of the set of gradients, where the more advanced gradients in the permutation are closer to the average of the set of gradients. These top portions of the gradients will be selected and sent to the server for global aggregation. We conduct experiments on different datasets, models, and scenarios by building a prototype system, and experimental results demonstrate that our BHerd strategy is effective in selecting beneficial local gradients to mitigate the effects brought by the Non-IID dataset.},
  archive      = {J_TC},
  author       = {Ping Luo and Xiaoge Deng and Ziqing Wen and Tao Sun and Dongsheng Li},
  doi          = {10.1109/TC.2025.3583827},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2977-2990},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BHerd: Accelerating federated learning by selecting beneficial herd of local gradients},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CROSC: Compilation-runtime joint optimization for fast smart contract execution. <em>TC</em>, <em>74</em>(9), 2962-2976. (<a href='https://doi.org/10.1109/TC.2025.3583734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State access is a critical part of smart contract execution which seriously affects the efficiency of smart contract execution in the mainstream Ethereum blockchain. To reduce state access latency, existing studies typically require manual source code modifications, which in practice may deliver limited performance gains and shift the burden to developers. In this paper, we propose CROSC to reduce state access latency and improve smart contract execution efficiency by a compilation-runtime joint optimization approach. CROSC consists of three key parts: 1) a runtime memory management mechanism named Fast State Memory (FastSM) to fully utilize the working memory and provide the context for the contract compiler; 2) a State Variable Address Relocation (SVAR) strategy to minimize costly persistent storage operations by precisely redirecting state variable access targets during compilation; 3) a one-shot unpacking design that eliminates frequent decoding overhead for low-bitwidth state variables. Preliminary experimental results highlight that, compared with the baseline compilation and runtime system of Ethereum, CROSC can achieve 2.5$\times$ and 7.5$\times$ speedups for single state load and store operations, respectively. CROSC reduces state access latency by up to 81.3%, and overall contract execution latency by 32.9% on average across 14 typical types of smart contracts. Extended evaluations on ERC20 and ERC721 token standard contracts show that CROSC delivers significant benefits in critical areas while remaining unobtrusive for less intensive state operations.},
  archive      = {J_TC},
  author       = {Surong Dai and Jinni Yang and Wenyang Cui and Yaozheng Fang and Ye Lu},
  doi          = {10.1109/TC.2025.3583734},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2962-2976},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CROSC: Compilation-runtime joint optimization for fast smart contract execution},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human-inspired computing for robust and efficient audio-visual speech recognition. <em>TC</em>, <em>74</em>(9), 2950-2961. (<a href='https://doi.org/10.1109/TC.2025.3582069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans excel at audiovisual speech recognition (AVSR), motivating the development of human-inspired computing for robust and efficient AVSR models. Spiking neural networks (SNNs), mimicking the brain’s information-processing mechanisms, offer a promising foundation. However, research on SNN-based AVSR remains limited, with most audio-visual methods focusing on object or digit recognition. These methods oversimplify multimodal fusion, neglecting modality-specific characteristics and interactions. Additionally, they often rely on future information, increasing recognition latency and limiting real-time applicability. Inspired by human speech perception, this paper proposes a novel human-inspired SNN named HI-AVSNN for AVSR, incorporating three computing characteristics: spike activity, cueing interaction, and causal processing. For cueing interaction, we introduce a Spike-Driven Visual-Cued Speech Processing (sVCSP) scheme, where visual features hierarchically guide speech processing to enhance critical features. For causal processing, we align the temporal dimension of SNN with that of audio-visual inputs and apply temporal masking to ensure only past and current information is used. For spike activity, in addition to SNNs, we incorporate event cameras to capture lip movements as spikes, efficiently encoding visual data like the human retina. Experiments on two event-based AVSR datasets demonstrate our method outperforms existing audio-visual SNN fusion techniques, showcasing the effectiveness, robustness, and efficiency achieved through our human-inspired computing.},
  archive      = {J_TC},
  author       = {Qianhui Liu and Jiadong Wang and Yang Wang and Xin Yang and Gang Pan and Haizhou Li},
  doi          = {10.1109/TC.2025.3582069},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2950-2961},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Human-inspired computing for robust and efficient audio-visual speech recognition},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An energy-efficient and privacy-aware MEC-enabled IoMT health monitoring system. <em>TC</em>, <em>74</em>(9), 2936-2949. (<a href='https://doi.org/10.1109/TC.2025.3576944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advancements in the Internet of Medical Things (IoMT) have made remote patient monitoring increasingly viable. However, challenges persist in safeguarding sensitive data, optimizing resources, and addressing the energy constraints of patient devices. This paper presents a health monitoring framework integrating Mobile Edge Computing (MEC) and sixth-generation (6G) technologies, structured into internal Medical Body Area Networks (int-MBANs) and external communications beyond MBANs (ext-MBANs). For int-MBANs, the proposed OptiBand algorithm optimizes energy consumption, extends device standby time, and considers message timeliness and medical criticality. A key innovation of OptiBand is its incorporation of patient’s device standby time into the resource allocation strategy to address real-world patient needs. For ext-MBANs, the DynaMEC algorithm dynamically balances energy efficiency, privacy protection, latency, and fairness, even under varying patient scales. A latency-aware scheduling mechanism also be introduced to guarantee timely completion of emergency tasks. Theoretical analysis and experimental results confirm the feasibility, convergence, and optimality of both algorithms. These characteristics and advantages of the proposed system make remote patient monitoring through IoMT more feasible and effective.},
  archive      = {J_TC},
  author       = {Xiaolu Cheng and Xiaoshuang Xing and Wei Li and Hong Xue and Tong Can},
  doi          = {10.1109/TC.2025.3576944},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2936-2949},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An energy-efficient and privacy-aware MEC-enabled IoMT health monitoring system},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-performance in-memory bayesian inference with multi-bit ferroelectric FET. <em>TC</em>, <em>74</em>(9), 2923-2935. (<a href='https://doi.org/10.1109/TC.2025.3576941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional neural network-based machine learning algorithms often encounter difficulties in data-limited scenarios or where interpretability is critical. Conversely, Bayesian inference-based models excel with reliable uncertainty estimates and explainable predictions. Recently, many in-memory computing (IMC) architectures achieve exceptional computing capacity and efficiency for neural network tasks leveraging emerging non-volatile memory (NVM) technologies. However, their application in Bayesian inference remains limited because the operations in Bayesian inference differ substantially from those in neural networks. In this article, we introduce a compact in-memory Bayesian inference engine with high efficiency and performance utilizing a multi-bit ferroelectric field-effect transistor (FeFET). This design encodes a Bayesian model within a compact FeFET-based crossbar by mapping quantized probabilities to discrete FeFET states. Consequently, the crossbar’s outputs naturally represent the output posteriors of the Bayesian model. Our design facilitates efficient Bayesian inference, accommodating various input types and probability precisions, without additional calculation circuitry. As the first FeFET-based in-memory Bayesian inference engine, our design demonstrates a notable storage density of 26.32 Mb/mm2 and a computing efficiency of 581.40 TOPS/W in a representative Bayesian classification task, indicating a 10.7×/43.4× compactness/efficiency improvement compared to the state-of-the-art alternative. Utilizing the proposed Bayesian inference engine, we develop a feature selection system that efficiently addresses a representative NP-hard optimization problem, showcasing our design’s capability and potential to enhance various Bayesian inference-based applications. Test results suggest that our design identifies the essential features, enhancing the model’s performance while reducing its complexity, surpassing the latest implementation in operation speed and algorithm efficiency by 2.9×/2.0×, respectively.},
  archive      = {J_TC},
  author       = {Chao Li and Xuchu Huang and Zhicheng Xu and Bo Wen and Ruibin Mao and Min Zhou and Thomas Kämpfe and Kai Ni and Can Li and Xunzhao Yin and Cheng Zhuo},
  doi          = {10.1109/TC.2025.3576941},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2923-2935},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-performance in-memory bayesian inference with multi-bit ferroelectric FET},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAL-PIM: A subarray-level processing-in-memory architecture with LUT-based linear interpolation for transformer-based text generation. <em>TC</em>, <em>74</em>(9), 2909-2922. (<a href='https://doi.org/10.1109/TC.2025.3576935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text generation is a compelling sub-field of natural language processing, aiming to generate human-readable text from input words. Although many deep learning models have been proposed, the recent emergence of transformer-based large language models advances its academic research and industry development, showing remarkable qualitative results in text generation. In particular, the decoder-only generative models, such as generative pre-trained transformer (GPT), are widely used for text generation, with two major computational stages: summarization and generation. Unlike the summarization stage, which can process the input tokens in parallel, the generation stage is difficult to accelerate due to its sequential generation of output tokens through iteration. Moreover, each iteration requires reading a whole model with little data reuse opportunity. Therefore, the workload of transformer-based text generation is severely memory-bound, making the external memory bandwidth system bottleneck. In this paper, we propose a subarray-level processing-in-memory (PIM) architecture named SAL-PIM, the first HBM-based PIM architecture for the end-to-end acceleration of transformer-based text generation. With optimized data mapping schemes for different operations, SAL-PIM utilizes higher internal bandwidth by integrating multiple subarray-level arithmetic logic units (S-ALUs) next to memory subarrays. To minimize the area overhead for S-ALU, it uses shared MACs leveraging slow clock frequency of commands for the same bank. In addition, a few subarrays in the bank are used as look-up tables (LUTs) to handle non-linear functions in PIM, supporting multiple addressing to select sections for linear interpolation. Lastly, the channel-level arithmetic logic unit (C-ALU) is added in the buffer die of HBM to perform the accumulation and reduce-sum operations of data across multiple banks, completing end-to-end inference on PIM. To validate the SAL-PIM architecture, we built a cycle-accurate simulator based on Ramulator. We also implemented the SAL-PIM’s logic units in 28-nm CMOS technology and scaled the results to DRAM technology to verify its feasibility. We measured the end-to-end latency of SAL-PIM when it runs various text generation workloads on the GPT-2 medium model (with 345 million parameters), in which the input and output token numbers vary from 32 to 128 and from 1 to 256, respectively. As a result, with 4.81% area overhead, SAL-PIM achieves up to 4.72× speedup (1.83× on average) over the Nvidia Titan RTX GPU running Faster Transformer Framework.},
  archive      = {J_TC},
  author       = {Wontak Han and Hyunjun Cho and Donghyuk Kim and Joo-Young Kim},
  doi          = {10.1109/TC.2025.3576935},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2909-2922},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SAL-PIM: A subarray-level processing-in-memory architecture with LUT-based linear interpolation for transformer-based text generation},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-performance hardware implementation of crystals-dilithium based on improved MDC-NTT. <em>TC</em>, <em>74</em>(9), 2896-2908. (<a href='https://doi.org/10.1109/TC.2025.3576934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growing threat of quantum computing to traditional cryptographic systems has necessitated the development of robust post-quantum algorithms. Crystal-Dilithium, recently standardized by NIST after a three-round competition, is a leading lattice-based digital signature algorithm designed to meet this need. However, conventional hardware implementations of Dilithium often suffer from inefficiencies and performance bottlenecks. To address these weaknesses, this work presents an optimized hardware design for Dilithium across all security levels. The proposed design features a parallel modular multiplication unit, and an enhanced scaling method to reduce bit width and minimize calibration. Additionally, an improved radix-2 Multipath Delay Commutator Number Theoretic Transform (MDC-NTT) and pipelined parallelization using FIFO and BRAM-based buffers are integrated to maximize operating frequency. Evaluated on the Xilinx Artix-7 platform, our implementation achieves a peak frequency of 191 MHz, delivering speedups of 26.3%, 32.5% and 29.6% for key generation, signature generation and signature verification respectively, compared with state-of-the-art works at the highest security level, along with superior hardware efficiency.},
  archive      = {J_TC},
  author       = {Yijun Cui and Junjie Zhong and Bei Wang and Tianyu Xu and Chenghua Wang and Weiqiang Liu},
  doi          = {10.1109/TC.2025.3576934},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2896-2908},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-performance hardware implementation of crystals-dilithium based on improved MDC-NTT},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient static schedules for fault-tolerant transmissions on shared media. <em>TC</em>, <em>74</em>(9), 2882-2895. (<a href='https://doi.org/10.1109/TC.2025.3576908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shared communication media are widely used in many applications including safety-critical applications. However, noise and transient errors can cause transmission failures. We consider the problem of designing and minimizing the length of fault-tolerant static schedules for transmitting messages in these media provided the number of errors fall below some upper bound. To transmit n messages in a medium while tolerating a maximum of f faults, prior work had shown how to construct schedules which had a fault tolerance overhead of $nf/2$. In this paper, we provide an efficient constructive algorithm for producing a schedule for n messages with total length $n\,+\,O(f^{2}\,\mathbf{\log}^{2}\,n)$ that can tolerate f medium errors. We also provide an algorithm for randomly generating fault-tolerant schedules with length $n\,+\,O(f\,\mathbf{\log}(f)\mathbf{\log}(n))$ as well as a technique for quickly verifying these on reasonably small inputs.},
  archive      = {J_TC},
  author       = {Scott Sirri and Zhe Wang and Netanel Raviv and Jeremy Fineman and Kunal Agrawal},
  doi          = {10.1109/TC.2025.3576908},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2882-2895},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient static schedules for fault-tolerant transmissions on shared media},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CBuild: Cluster-oriented collaborative image building for containers. <em>TC</em>, <em>74</em>(9), 2870-2881. (<a href='https://doi.org/10.1109/TC.2025.3575912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Starting a container needs to build a container image layer-by-layer if the required image is not available. However, the image building involves downloading a large amount of data, which significantly delays the development and deployment of containerized services. To reduce data downloads and accelerate image building, current methods typically focus on improving data sharing through reconstructing images. Unfortunately, these approaches show limited performance improvement in clusters as they only improve data sharing on a single node. In this paper, we find that there are significant duplicated remote file downloads between nodes in a cluster. Accordingly, we propose cBuild, a distributed file cache to minimize costly image data downloads in cluster environments. Specifically, to enable inter-node image data sharing, cBuild designs a non-intrusive interception mechanism based on network namespace, instead of directly detecting building instructions that dirty images. Based on the distribution characteristics of duplicated files in layers, cBuild places image files among nodes in a balanced manner to prevent transfer bottlenecks caused by hotspot nodes and employs a layer-aware searching strategy to quickly locate the desired files. We implement cBuild on the basis of Docker. Experiments show that cBuild improves building speed by up to 15.3$\times$ and reduces the data downloading by 80%.},
  archive      = {J_TC},
  author       = {Zhuo Huang and Hao Fan and Bin Tang and Song Wu and Chen Yu and Hai Jin},
  doi          = {10.1109/TC.2025.3575912},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2870-2881},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CBuild: Cluster-oriented collaborative image building for containers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). In-situ NAS: A plug-and-search neural architecture search framework across hardware platforms. <em>TC</em>, <em>74</em>(9), 2856-2869. (<a href='https://doi.org/10.1109/TC.2025.3569161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware-aware Neural Architecture Search (HW-NAS) has garnered significant research interest due to its ability to automate the design of neural networks for various hardware platforms. Prevalent HW-NAS frameworks often use fast predictors to estimate network performance, bypassing the time-consuming actual profiling step. However, the resource-intensive nature of building these predictors and their accuracy limitations hinder their practical use in diverse deployment scenarios. In response, we emphasize the indispensable role of actual profiling in HW-NAS and explore efficiency optimization possibilities within the HW-NAS framework. We provide a systematic analysis of profiling overhead in HW-NAS and identify many redundant and unnecessary operations during the search phase. We then optimize the workflow and present In-situ NAS, which leverages similarity features and exploration history to eliminate redundancy and improve runtime efficiency. In-situ NAS also offers simplified interfaces to ease the user’s effort in managing the complex device-dependent profiling flow, enabling plug-and-search functionality across diverse hardware platforms. Experimental results show that In-situ NAS achieves an average 10x speedup across different hardware platforms while reducing the search overhead by 8x compared to predictor-based approaches in various deployment scenarios. Additionally, In-situ NAS consistently discovers networks with better accuracy (about 1.5%) across diverse hardware platforms compared to predictor-based NAS.},
  archive      = {J_TC},
  author       = {Hao Lv and Lei Zhang and Ying Wang},
  doi          = {10.1109/TC.2025.3569161},
  journal      = {IEEE Transactions on Computers},
  month        = {9},
  number       = {9},
  pages        = {2856-2869},
  shortjournal = {IEEE Trans. Comput.},
  title        = {In-situ NAS: A plug-and-search neural architecture search framework across hardware platforms},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ls-stream: Lightening stragglers in join operators for skewed data stream processing. <em>TC</em>, <em>74</em>(8), 2841-2855. (<a href='https://doi.org/10.1109/TC.2025.3575917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load imbalance can lead to the emergence of stragglers, i.e., join instances that significantly lag behind others in processing data streams. Currently, state-of-the-art solutions are capable of balancing the load between join instances to mitigate stragglers by managing hot keys and random partitioning. However, these solutions rely on either complicated routing strategies or resource-inefficient processing structures, making them susceptible to frequent changes in load between instances. Therefore, we present Ls-Stream, a data stream scheduler that aims to support dynamic workload assignment for join instances to lighten stragglers. This paper outlines our solution from the following aspects: (1) The models for partitioning, communication, matrix, and resource are developed, formalizing problems like imbalanced load between join instances and state migration costs. (2) Ls-Stream employs a two-level routing strategy for workload allocation by combining hash-based and key-based data partitioning, specifying the destination join instances for data tuples. (3) Ls-Stream also constructs a fine-grained model for minimizing the state migration cost. This allows us to make trade-offs between data transfer overhead and migration benefits. (4) Experimental results demonstrate significant improvements made by Ls-Stream: reducing maximum system latency by 49.3% and increasing maximum throughput by more than 2x compared to existing state-of-the-art works.},
  archive      = {J_TC},
  author       = {Minghui Wu and Dawei Sun and Shang Gao and Keqin Li and Rajkumar Buyya},
  doi          = {10.1109/TC.2025.3575917},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2841-2855},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Ls-stream: Lightening stragglers in join operators for skewed data stream processing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast garbage collection in erasure-coded storage clusters. <em>TC</em>, <em>74</em>(8), 2827-2840. (<a href='https://doi.org/10.1109/TC.2025.3575914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure codes (EC) have been widely adopted to provide high data reliability with low storage costs in clusters. Due to the deletion and out-of-place update operations, some data blocks are invalid, which unfortunately arouses the tedious garbage collection (GC) problem. Several limitations still plague existing designs: substantial network traffic, unbalanced traffic load, and low read/write performance after GC. This paper proposes FastGC, a fast garbage collection method that merges the old stripes into a new stripe and reclaims invalid blocks. FastGC quickly generates an efficient merge solution by stripe grouping and bit sequences operations to minimize network traffic and maintains data block distributions of the same stripe to ensure read performance. It carefully allocates the storage space for new stripes during merging to eliminate the discontinuous free spaces that affect write performance. Furthermore, to accelerate the parity updates after merging, FastGC greedily schedules the transmission links for multi-stripe updates to balance the traffic load across nodes and adopts a maximum flow algorithm to saturate the bandwidth utilization. Comprehensive evaluation results show via simulations and Alibaba ECS experiments that FastGC can significantly reduce 10.36%-81.22% of the network traffic and 34.25%-72.36% of the GC time while maintaining read/write performance after GC.},
  archive      = {J_TC},
  author       = {Hai Zhou and Dan Feng and Yuchong Hu and Wei Wang and Huadong Huang},
  doi          = {10.1109/TC.2025.3575914},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2827-2840},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Fast garbage collection in erasure-coded storage clusters},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual fast-track cache: Organizing ring-shaped racetracks to work as l1 caches. <em>TC</em>, <em>74</em>(8), 2812-2826. (<a href='https://doi.org/10.1109/TC.2025.3575909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Static Random-Access Memory (SRAM) is the fastest memory technology and has been the common design choice for implementing first-level (L1) caches in the processor pipeline, where speed is a key design issue that must be fulfilled. On the contrary, this technology offers much lower density compared to other technologies like Dynamic RAM, limiting L1 cache sizes of modern processors to a few tens of KB. This paper explores the use of slower but denser Domain Wall Memory (DWM) technology for L1 caches. This technology provides slow access times since it arranges multiple bits sequentially in a magnetic racetrack. To access these bits, they need to be shifted in order to place them under a header. A 1-bit shift usually takes one processor cycle, which can significantly hurt the application performance, making this working behavior inappropriate for L1 caches. Based on the locality (temporal and spatial) principles exploited by caches, this work proposes the Dual Fast-Track Cache (Dual FTC) design, a new approach to organizing a set of racetracks to build set-associative caches. Compared to a conventional SRAM cache, Dual FTC enhances storage capacity by 5× while incurring minimal shifting overhead, thereby rendering it a practical and appealing solution for L1 cache implementations. Experimental results show that the devised cache organization is as fast as an SRAM cache for 78% and 86% of the L1 data cache hits and L1 instruction cache hits, respectively (i.e., no shift is required). Consequently, due to the larger L1 cache capacities, significant system performance gains (by 22% on average) are obtained under the same silicon area.},
  archive      = {J_TC},
  author       = {Alejandro Valero and Vicente Lorente and Salvador Petit and Julio Sahuquillo},
  doi          = {10.1109/TC.2025.3575909},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2812-2826},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dual fast-track cache: Organizing ring-shaped racetracks to work as l1 caches},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Serving MoE models on resource-constrained edge devices via dynamic expert swapping. <em>TC</em>, <em>74</em>(8), 2799-2811. (<a href='https://doi.org/10.1109/TC.2025.3575905'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixture of experts (MoE) is a popular technique in deep learning that improves model capacity with conditionally-activated parallel neural network modules (experts). However, serving MoE models in resource-constrained latency-critical edge scenarios is challenging due to the significantly increased model size and complexity. In this paper, we first analyze the behavior pattern of MoE models in continuous inference scenarios, which leads to three key observations about the expert activations, including temporal locality, exchangeability, and skippable computation. Based on these observations, we introduce PC-MoE, an inference framework for resource-constrained continuous MoE model serving. The core of PC-MoE is a new data structure, Parameter Committee, that intelligently maintains a subset of important experts in use to reduce resource consumption. To evaluate the effectiveness of PC-MoE, we conduct experiments using state-of-the-art MoE models on common computer vision and natural language processing tasks. The results demonstrate optimal trade-offs between resource consumption and model accuracy achieved by PC-MoE. For instance, on object detection tasks with the Swin-MoE model, our approach can reduce memory usage and latency by 42.34% and 18.63% with only 0.10% accuracy degradation.},
  archive      = {J_TC},
  author       = {Rui Kong and Yuanchun Li and Weijun Wang and Linghe Kong and Yunxin Liu},
  doi          = {10.1109/TC.2025.3575905},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2799-2811},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Serving MoE models on resource-constrained edge devices via dynamic expert swapping},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time scheduling and analysis of fixed-priority tasks on a basic heterogeneous architecture with multiple CPUs and many PEs. <em>TC</em>, <em>74</em>(8), 2785-2798. (<a href='https://doi.org/10.1109/TC.2025.3573602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While accelerator-based heterogeneous architectures have gained traction in accelerating AI tasks, effectively managing them with stringent timing constraints remains a challenge. Although many scheduling and response time analysis approaches are proposed for multi-core or heterogeneous multi-core (i.e., big.LITTLE cores) processors, direct application of them to accelerator-based heterogeneous architectures with multiple CPUs and numerous processing units (PEs) often results in significant pessimism. This paper introduces real-time scheduling and comprehensive response time analysis from unit-level micro view to job-level macro view, for general accelerator-based heterogeneous architectures, greatly enhancing schedulability and utilization rates. We begin by establishing a general task execution pattern on heterogeneous architectures that integrates multiple CPU cores and various PEs. Subsequently, we present a real-time scheduling strategy and corresponding response time analysis based on this task execution pattern from micro to macro views. Through extensive experiments conducted on GEMM and AI workloads, our proposed scheduling and response time analysis significantly outperforms state-of-the-art scheduling algorithms, improving schedulability by 10.3% to 52.9%. Furthermore, experiments on NVIDIA GPU systems indicate a potential pessimism reduction of up to 30.7%. As we target general heterogeneous architectures, our approach can be readily applied to off-the-shelf accelerator-based heterogeneous computing systems, ensuring adherence to deadlines and enhancing schedulability.},
  archive      = {J_TC},
  author       = {Yuankai Xu and Yinchen Ni and Tiancheng He and Ruiqi Sun and Yier Jin and An Zou},
  doi          = {10.1109/TC.2025.3573602},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2785-2798},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Real-time scheduling and analysis of fixed-priority tasks on a basic heterogeneous architecture with multiple CPUs and many PEs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pike: Two-phase BFT with linearity and flexible view change. <em>TC</em>, <em>74</em>(8), 2772-2784. (<a href='https://doi.org/10.1109/TC.2025.3573597'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the first Byzantine fault-tolerant (BFT) protocol with linear communication complexity, HotStuff (PODC 2019) has received significant attention. HotStuff has three round-trips for both normal case operations and view change protocols. Follow-up studies attempt to reduce the number of phases for HotStuff. However, most studies give up on one thing in return for another. This paper extends our previous work Marlin(DSN 2022) to Pike, another BFT protocol with two phases and linear communication complexity. Both Pikeand Marlinuse the same cryptographic tools as in HotStuff and introduce no additional assumptions. Marlinhas a more efficient view change (i.e., leader election) protocol but a more complicated data structure. Pikefurther simplifies the data structure at the cost of longer view changes in extreme cases. We implement the Pike, Marlin, HotStuff, and HotStuff-2, showing that both Pikeand Marlinoutperform HotStuff in normal case operations.},
  archive      = {J_TC},
  author       = {Xiao Sui and Qichang Liu and Sisi Duan and Haibin Zhang},
  doi          = {10.1109/TC.2025.3573597},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2772-2784},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Pike: Two-phase BFT with linearity and flexible view change},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A framework for carbon-aware real-time workload management in clouds using renewables-driven cores. <em>TC</em>, <em>74</em>(8), 2757-2771. (<a href='https://doi.org/10.1109/TC.2025.3571495'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud platforms commonly exploit workload temporal flexibility to reduce their carbon emissions. They suspend/resume workload execution for when and where the energy is greenest. However, increasingly prevalent delay-intolerant real-time workloads challenge this approach. To this end, we present a framework to harvest green renewable energy for real-time workloads in cloud systems. We use Renewables-driven cores in servers to dynamically switch CPU cores between real-time and low-power profiles, matching renewable energy availability. We then develop a VM Execution Model to guarantee that running VMs are allocated with cores in the real-time power profile. If such cores are insufficient, we conduct criticality-aware VM evictions as needed. Furthermore, we develop a VM Packing Algorithm to utilize available cores across the servers. We introduce the Green Cores concept in our algorithm to convert renewable energy usage into a server inventory attribute. Based on this, we jointly optimize for renewable energy utilization and reduction of VM eviction incidents. We implement a prototype of our framework in OpenStack as openstack-gc. Using an experimental openstack-gc cloud and a large-scale simulation testbed, we expose our framework to VMs running RTEval, a real-time evaluation program, and a 14-day Azure VM arrival trace. Our results show: i) a $ 6.52\times $ reduction in coefficient of variation of real-time latency over an existing workload temporal flexibility-based solution, and ii) a joint 79.64% reduction in eviction incidents with a 34.83% increase in energy harvest over the state-of-the-art packing algorithms. We open source openstack-gc at https://github.com/tharindu-b-hewage/openstack-gc.},
  archive      = {J_TC},
  author       = {Tharindu B. Hewage and Shashikant Ilager and Maria A. Rodriguez and Rajkumar Buyya},
  doi          = {10.1109/TC.2025.3571495},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2757-2771},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A framework for carbon-aware real-time workload management in clouds using renewables-driven cores},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asymmetrically decentralized federated learning. <em>TC</em>, <em>74</em>(8), 2745-2756. (<a href='https://doi.org/10.1109/TC.2025.3569185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the communication burden and privacy concerns associated with the centralized server in Federated Learning (FL), Decentralized Federated Learning (DFL) has emerged, which discards the server with a peer-to-peer (P2P) communication framework, significantly expanding the application scenarios of FL. However, most existing DFL algorithms are based on symmetric topologies, such as ring and grid topology, which can easily lead to deadlocks and are susceptible to the impact of network link quality in practice. To address these issues, we propose DFedSGPSM, a transitional framework that converts symmetric DFL optimizers into asymmetric variants. By adopting the Push-Sum protocol in asymmetric network topologies, our framework successfully circumvents the deadlock and link-quality issues prevalent in symmetric configurations. To further validate the effectiveness of our algorithm framework, we integrate the local momentum (in DFedAvgM) and SAM (in DFedSAM) from existing symmetric DFL optimizer into DFedSGPSM to accelerate training and pursue smooth local minimum, which enables existing symmetric DFL optimizers to be seamlessly integrated into asymmetric DFL. Theoretical analysis proves that DFedSGPSM achieves a linear speedup rate of $ \mathcal{O}\left(\frac{1}{\sqrt{nT}}\right)$ in the non-convex setting. This analysis also reveals crucial issues such as tighter upper bounds achieved with improved topological connectivity. Empirically, extensive experiments conducted on the MNIST, CIFAR10&100 datasets demonstrate the superior performance of our proposed algorithm compared to several existing SOTA optimizers in terms of generalization.},
  archive      = {J_TC},
  author       = {Qinglun Li and Miao Zhang and Nan Yin and Quanjun Yin and Li Shen and Xiaochun Cao},
  doi          = {10.1109/TC.2025.3569185},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2745-2756},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Asymmetrically decentralized federated learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling verifiable search and integrity auditing in encrypted decentralized storage using one proof. <em>TC</em>, <em>74</em>(8), 2731-2744. (<a href='https://doi.org/10.1109/TC.2025.3569182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the properties of autonomy and scalability, decentralized storage networks (DSNs) leveraging blockchain technology have attracted growing attention. Integrity auditing and verifiable searchable encryption are two essential functions for DSNs. The former ensures reliable and fair storage services, while the latter enables users to conduct keyword searches over encrypted data and guarantees the public verifiability of search results. However, all existing research in DSN has focused either on integrity auditing or on verifiable searchable encryption separately. In this paper, we propose a novel scheme for encrypted decentralized storage that simultaneously supports verifiable search and integrity auditing. It employs a unified proof and supports one-time proof verification to validate both the correctness of the returned file identifiers and the integrity of the files associated with these identifiers. As a result, compared to previous schemes supporting only integrity auditing, our scheme maintains a similar proof size and the support for search result verification does not significantly increase the on-chain storage overhead. Additionally, our scheme allows users to dynamically update their outsourced files while ensuring forward security during the file insertion process. We formally analyze the correctness and security of our scheme, and implement a system prototype to evaluate its performance. The experimental results demonstrate that it achieves verifiable searchable encryption and integrity auditing with practically affordable overhead.},
  archive      = {J_TC},
  author       = {Mingyang Song and Zhongyun Hua and Yifeng Zheng and Qing Liao and Xiaohua Jia},
  doi          = {10.1109/TC.2025.3569182},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2731-2744},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling verifiable search and integrity auditing in encrypted decentralized storage using one proof},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SVDE: Serverless framework for low-latency video analytic queries with hardware disaggregation. <em>TC</em>, <em>74</em>(8), 2717-2730. (<a href='https://doi.org/10.1109/TC.2025.3569173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video analytics applications are gaining popularity among serverless environments. One issue that appears in existing serverless platforms is that they do not fully exploit opportunities to efficiently handle video chunking as they assume that all video chunks display similar computation and communication overheads. Those overheads vary and benefit from fine-grained handling. Scheduling non-uniform chunks is even more challenging when the hardware resources are heterogeneous and the network between the resources is non-uniform. To address these challenges, we propose SVDE, a heterogeneous serverless cloud framework for massive video processing workloads. SVDE employs a trained decision tree regression to efficiently decide where to process each video chunk, by holistically considering the non-uniform chunk sizes, heterogeneity of the node hardware, queuing status of each node, and an unbalanced network. Furthermore, we develop an efficient operator backend that will be open‐sourced as part of SVDE Ċompared to prior works, SVDE achieves up to 3.2$ \times $ speedup on ten real-world video workloads due to its holistic scheduling decision-making, while our operator backend outperforms the popular Pytorch JIT backend by 5$ \times $.},
  archive      = {J_TC},
  author       = {Pingyi Huo and Theodore Michailidis and Yi Zheng and Prapti Panigrahi and Kiwan Maeng and Jishen Zhao and Vijaykrishnan Narayanan},
  doi          = {10.1109/TC.2025.3569173},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2717-2730},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SVDE: Serverless framework for low-latency video analytic queries with hardware disaggregation},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KPU: Kernel processing unit for in-memory analytical query processing. <em>TC</em>, <em>74</em>(8), 2702-2716. (<a href='https://doi.org/10.1109/TC.2025.3569163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain-specific architecture has greatly improved performance and energy efficiency in in-memory databases, especially for accelerating single-functional computing logic in analytic query processing, such as sort, join and aggregation. However, as data volumes surge exponentially, these dedicated accelerators are struggling to satisfy the burgeoning demand for handling intricate and multifaceted workloads. A major challenge lies in establishing a flexible framework that engages these ‘coarse-grained’ units without incurring extra overheads from hardware integration, programming, compilation, runtime and operating systems. In this paper, the kernel processing unit (KPU) is proposed to optimize CPU-accelerator heterogeneous systems for in-memory databases. KPU provides a unified interface to consolidate all database query operators. In terms of KPU hardware architecture, kernel customization and data transmission are two critical bottlenecks. To address the challenges, multiple independently designed homogeneous table cores are integrated to support flexible high-performance SQL queries, and a customized efficient data management system (DMS) works collaboratively to maximize the utilization of on-chip memory bandwidth. Additionally, a database application-specific KPU instruction set architecture (KISA) dedicated to parallel analytical query processing is proposed to enable parallel KPU programming. To trade off between accelerator computing capacity and data transfer latency, KPU designs an offloading mechanism to map SQL queries between the CPU and accelerator adaptively based on a performance model and a function simulator. The experiments demonstrate that KPU surpasses the general-purpose CPU and GPU by an average of 24.5x and 8.75x, respectively.},
  archive      = {J_TC},
  author       = {Jingya Wu and Wenyan Lu and Haishuang Fan and Hao Kong and Xiaowei Li and Guihai Yan},
  doi          = {10.1109/TC.2025.3569163},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2702-2716},
  shortjournal = {IEEE Trans. Comput.},
  title        = {KPU: Kernel processing unit for in-memory analytical query processing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DAHE: Parameter-adaptive and memory efficient FPGA acceleration of homomorphic encryption. <em>TC</em>, <em>74</em>(8), 2687-2701. (<a href='https://doi.org/10.1109/TC.2025.3569159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While homomorphic encryption (HE) has been well-recognized as a promising data privacy protection technique, there are many challenges to the real-world deployment of HE applications. In this work, we propose a design flow for parameter-adaptive and memory-efficient FPGA acceleration of homomorphic encryption. In the framework, we explore the correlations between HE parameter selection to meet various design objectives and the huge design space due to underlying FPGA hardware resource allocation. Particularly, we demonstrate that adaptive management of the FPGA memory hierarchy is crucial to supporting diverse cryptosystem parameter selection for application-level security, accuracy, and performance requirements. We propose a resource-efficient and flexible micro-architectural design for HE operations, where data access patterns in various pipeline execution stages are optimized for high memory bandwidth utilization. Furthermore, a memory-aware performance model is built for automatic design space exploration for cryptosystem parameter selection and hardware resource provisioning. Experimental results show 1.50X and 1.16X speedup for the NTT and Rotation operations w.r.t. the state-of-the-art FPGA implementation. Meanwhile, the proposed framework generates flexible and high-performance accelerator code for real HE application kernels with different cryptosystem parameters on a wide range of FPGA devices.},
  archive      = {J_TC},
  author       = {Yilan Zhu and Honghui You and Wei Zhang and Jiming Xu and Qian Lou and Shoumeng Yan and Lei Ju},
  doi          = {10.1109/TC.2025.3569159},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2687-2701},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DAHE: Parameter-adaptive and memory efficient FPGA acceleration of homomorphic encryption},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acceleration of timing-aware gate-level logic simulation through one-pass GPU parallelism. <em>TC</em>, <em>74</em>(8), 2675-2686. (<a href='https://doi.org/10.1109/TC.2025.3569135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Witnessing the advancements in the scale and complexity of chip design, along with the benefits from high-performance computing technologies, the simulation of Very Large Scale Integration (VLSI) circuits increasingly demands acceleration through parallel computing with GPU devices. However, conventional parallel strategies fail to fully leverage modern GPU capabilities, introducing new challenges in GPU-based parallelism for VLSI simulations despite previous demonstrations of significant acceleration. In this paper, we propose a novel approach for accelerating the simulation of 4-value logic timing-aware gate-level circuits through waveform-based GPU parallelism. Our approach introduces an innovative strategy that effectively manages task dependencies during the parallelism of combinational circuits, significantly reducing the synchronization requirement between CPU and GPU. The proposed approach achieves one-pass parallelism by requiring only a single round of data transfer. Moreover, to address the implementation challenges associated with our strategy on GPU devices, we have developed and optimized a series of data structures that dynamically allocate and store newly generated outputs of uncertain scale. Finally, we conduct experiments on industrial-scale open-source benchmarks to demonstrate our approach’s performance gains over several state-of-the-art baselines.},
  archive      = {J_TC},
  author       = {Weijie Fang and Yanggeng Fu and Jiaquan Gao and Longkun Guo and Gregory Gutin and Xiaoyan Zhang},
  doi          = {10.1109/TC.2025.3569135},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2675-2686},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Acceleration of timing-aware gate-level logic simulation through one-pass GPU parallelism},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cacomp: A cloud-assisted collaborative deep learning compiler framework for DNN tasks on edge. <em>TC</em>, <em>74</em>(8), 2663-2674. (<a href='https://doi.org/10.1109/TC.2025.3569132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of edge computing, DNN services have been widely deployed on edge devices. The deployment efficiency of deep learning models relies on the optimization of inference and scheduling policy. However, traditional optimization methods on edge devices still suffer from prohibitively long tuning time due to devices’ low computational power. Meanwhile, the widely used scheduling algorithm, the dominant resource fairness algorithm (DRF algorithm), struggles to maximize the efficiency of model execution on edge devices and inevitably increases average waiting time as it is not applicable in the real-time distributed computing environment. In this paper, we propose Cacomp, a distributed cloud-assisted deep learning compiler framework that features accelerating the optimization on edge devices with assistance from the cloud and a novel inference task scheduling algorithm. Our framework utilizes the tuning records from the cloud devices and proposes a two-step distillation strategy to obtain the best tuning record set for the edge device. For the scheduling process, we propose an RD-DRF algorithm to allocate inference tasks to edge devices based on dominant resource matching in real time. Extensive results show that our framework can achieve up to 2.19x improvement in the optimization time compared with other methods on edge devices. Our proposed scheduling algorithm significantly shortens the average waiting time of inference tasks by 30% and improves resource utilization by 20% on edge devices.},
  archive      = {J_TC},
  author       = {Weiwei Lin and Jinhui Lin and Haotong Zhang and Wentai Wu and Weizheng Wu and Zhetao Li and Keqin Li},
  doi          = {10.1109/TC.2025.3569132},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2663-2674},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Cacomp: A cloud-assisted collaborative deep learning compiler framework for DNN tasks on edge},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ImPACT: Importance-informed prefetching and caching for I/O-bound DNN training. <em>TC</em>, <em>74</em>(8), 2649-2662. (<a href='https://doi.org/10.1109/TC.2025.3569126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fetching large amounts of DNN training data from storage systems causes high I/O latency and GPU stalls. Importance sampling can reduce data processing on GPUs while maintaining model accuracy, but current frameworks lack a prefetching and caching layer to optimize data fetches and cache management based on sample importance. This leads to unnecessary fetches, poor cache hit ratios, and random I/Os. We present ImPACT, an importance-informed prefetching and caching system, to accelerate I/O-bound DNN training. First, we propose an importance-informed prefetching technique to reduce the prefetching of unimportant data. Then, we introduce an importance-aware caching layer, partitioned into two regions: H-cache and L-cache, which store samples of high importance and low importance respectively. Rather than using recency or frequency, we manage data items in H-cache according to their corresponding sample importance. When there is a cache miss in L-cache, we use sample substitutability and dynamic packaging to improve the cache hit ratio and reduce the number of random I/Os. Our experimental results show that ImPACT has a negligible impact on training accuracy while speeding up DNN training by up to 3.5$ \times $ compared to state-of-the-art prefetching and caching systems.},
  archive      = {J_TC},
  author       = {Weijian Chen and Shuibing He and Ruidong Zhang and Xuechen Zhang and Ping Chen and Siling Yang and Haoyang Qu and Xuan Zhan},
  doi          = {10.1109/TC.2025.3569126},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2649-2662},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ImPACT: Importance-informed prefetching and caching for I/O-bound DNN training},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UAV-assisted microservice mobile edge computing architecture: Addressing post-disaster emergency medical rescue. <em>TC</em>, <em>74</em>(8), 2635-2648. (<a href='https://doi.org/10.1109/TC.2025.3566913'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In post-disaster emergency medical rescue operations, rapidly establishing an adaptive and flexible edge computing (EC) network, balancing data offloading with energy consumption, and ensuring the stable operation of the network have become urgent priorities. To address these challenges, we proposed an unmanned aerial vehicle (UAV)-assisted microservice mobile edge computing (MEC) architecture. The architecture can be rapidly deployed to provide temporary network coverage and EC services in disaster-stricken areas. A transformer-based resource management (TBRM) approach is utilized to optimize data offloading efficiency and reduce energy consumption, thereby maximizing the service time of the architecture. To enhance the security and reliability of the architecture, four microservices are designed to manage the full UAV lifecycle, and UAV identity authentication is implemented through dual digital signature certificates. Large-scale simulation experiments have demonstrated the effectiveness of the architecture in complex rescue scenarios, providing strong technical support for post-disaster medical rescue efforts.},
  archive      = {J_TC},
  author       = {Ji Li and Qiang He and Xingwei Wang and Ammar Hawbani and Keping Yu and Yuanguo Bi and Liang Zhao},
  doi          = {10.1109/TC.2025.3566913},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2635-2648},
  shortjournal = {IEEE Trans. Comput.},
  title        = {UAV-assisted microservice mobile edge computing architecture: Addressing post-disaster emergency medical rescue},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight and holistic-scalable serverless secure container runtime for high-density deployment and high-concurrency startup. <em>TC</em>, <em>74</em>(8), 2621-2634. (<a href='https://doi.org/10.1109/TC.2025.3566912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The secure container that hosts a single container in a micro virtual machine (VM) is now used in serverless computing, as the containers are isolated through the microVMs. There are high demands on the high-density container deployment and high-concurrency container startup to improve both the resource utilization and user experience, as user functions are fine-grained in serverless platforms. Our investigation shows that the entire software stacks, containing the cgroups in the host operating system, the guest operating system, and the container rootfs for the function workload, together result in low deployment density and slow startup performance at high-concurrency. We propose a lightweight and holistic-scalable secure container runtime, named RunD-V, to resolve above problems in serverless computing. RunD-V proposes a guest-to-host runtime template for microVM scaling-out, and CR-bind feature in guest kernel for microVM scaling-up. Using guest-to-host runtime template, over 200 secure containers can be launched within 1s on a node equipped with 104 vCPUs. It also enables more than 2,500 secure containers to be deployed on a node with 384GB of memory. The vertical scaling mechanism CR-bind further enhances both startup concurrency and deployment density.},
  archive      = {J_TC},
  author       = {Zijun Li and Chenyang Wu and Chuhao Xu and Quan Chen and Shuo Quan and Bin Zha and Qiang Wang and Weidong Han and Jie Wu and Minyi Guo},
  doi          = {10.1109/TC.2025.3566912},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2621-2634},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lightweight and holistic-scalable serverless secure container runtime for high-density deployment and high-concurrency startup},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Timerlat: Real-time linux scheduling latency measurements, tracing, and analysis. <em>TC</em>, <em>74</em>(8), 2608-2620. (<a href='https://doi.org/10.1109/TC.2025.3566908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A trend in many embedded devices is the move from hardware-based to software-defined, such as software-defined networks and software-defined PLCs. This trend is motivated by multiple aspects, including the availability of complex software stacks and the consolidation of multiple devices into a single larger system. Due to its real-time capabilities and flexibility, Linux is the operating system of choice for many applications, including time-sensitive ones. However, assessing and debugging timing violations, especially those caused by scheduling latency, is challenging with the current state-of-the-art tools. This paper presents timerlat, a tool that integrates scheduling latency measurements, tracing, and analysis in an easy-to-use interface. Its output includes an auto-analysis, providing insightful details on the composition of the scheduling latency. Experimental results are reported, evaluating the effectiveness of timerlat in assessing the latencies, considering different setups and workloads.},
  archive      = {J_TC},
  author       = {Daniel Bristot De Oliveira and Daniel Casini and Juri Lelli and Tommaso Cucinotta},
  doi          = {10.1109/TC.2025.3566908},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2608-2620},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Timerlat: Real-time linux scheduling latency measurements, tracing, and analysis},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyte: A hotness-aware hybrid DRAM-PM native table storage engine. <em>TC</em>, <em>74</em>(8), 2593-2607. (<a href='https://doi.org/10.1109/TC.2025.3566906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging persistent memory (PM) technologies offer data persistence at close-to-DRAM latency, causing the legacy multi-layer storage stack to become a bottleneck. On the other hand, PM improves I/O efficiency while also introducing unique hardware features that require software modifications to exploit them. In this paper, we propose Hyte, a native table storage engine for hybrid memory, which abstracts the data service for the database’s table. Hyte performs a cross-layer design across the file system and storage engine to avoid the pitfalls of layered abstraction and provides SQL-compatible APIs. Meanwhile, it co-designs with PM’s properties to unlock the hardware’s performance potential. Furthermore, Hyte is hotness-aware and equipped with a suite of lightweight, customized fetching and eviction strategies. Our evaluation shows that Hyte can perform up to 1.4-7.2$ \times $ better than existing state-of-the-art PM-based storage systems in industry and academia.},
  archive      = {J_TC},
  author       = {Xiaopeng Fan and Xiaoshuang Peng and Kaixin Huang and Chuliang Weng},
  doi          = {10.1109/TC.2025.3566906},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2593-2607},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hyte: A hotness-aware hybrid DRAM-PM native table storage engine},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid adaptive filter for head tracking in augmented reality (AR)-based flight simulators. <em>TC</em>, <em>74</em>(8), 2581-2592. (<a href='https://doi.org/10.1109/TC.2025.3566901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach for head tracking in augmented reality (AR) flight simulators using an adaptive fusion of Kalman and particle filters. This fusion dynamically balances the strengths of both algorithms, leveraging Kalman filters for computational efficiency and particle filters for handling non-linearities based on real-time factors such as sensor noise and head movement patterns. Our method demonstrates superior tracking precision and reduced latency, making it particularly effective for immersive pilot training in AR-based flight simulations. While focused on flight simulation, our approach holds high potential for broader applications in other AR and virtual reality (VR) environments where precise, real-time head tracking is crucial. These results provide actionable design guidance for developers optimizing tracking systems in environments that require fast response times and high accuracy. Future extensions of this research could explore the generalization of our approach across diverse AR applications, including human-computer interaction (HCI), medical simulations, and gaming.},
  archive      = {J_TC},
  author       = {Onyeka Josephine Nwobodo and Godlove Suila Kuaban and Valery Nkemeni and Kamil Wereszczynski and Krzysztof Adam Cyran},
  doi          = {10.1109/TC.2025.3566901},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2581-2592},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A hybrid adaptive filter for head tracking in augmented reality (AR)-based flight simulators},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cryo-CACTI: Cryogenic-aware CACTI for cache modeling down to 10K in advanced 7nm FinFETs. <em>TC</em>, <em>74</em>(8), 2567-2580. (<a href='https://doi.org/10.1109/TC.2025.3566899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryogenic circuits are currently employed in fields such as quantum computing, particle detectors, magnetic resonance imaging, and space applications. While cryogenic circuits are being researched, there is limited work on designing cryogenic caches at temperatures below 77K. Moreover, there is no tool to estimate the delay, power, and area of cryogenic caches at advanced technology nodes. Our research focuses on the development of cryogenic caches tailored for the 7nm technology node, operating at 10K. However, a key challenge is the lack of cryogenic measurement data, especially in recent technologies. Consequently, through conducting our own FinFET transistor measurements, we calibrate cryogenic transistor models at 10K. With the 7nm cryogenic transistor data, we model Cryo-CACTI for cryogenic caches (due to cache’s vital role in improving performance and their considerable share in area and power of the processor). Using Cryo-CACTI, our evaluation reveals considerable improvements in the energy efficiency (up to 99%) of cryogenic caches of larger sizes compared to the caches at room temperature (300K). Additionally, we explore alternative cache configurations at circuit-level to optimize cryogenic operation. Furthermore, we use Cryo-CACTI to explore the performance/energy consumption of cryogenic caches while simulating workloads such as SPEC CPU2017 and machine learning via neural networks.},
  archive      = {J_TC},
  author       = {Divya Praneetha Ravipati and Victor M. van Santen and Shivendra Singh Parihar and Yogesh Singh Chauhan and Preeti Ranjan Panda and Hussam Amrouch},
  doi          = {10.1109/TC.2025.3566899},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2567-2580},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Cryo-CACTI: Cryogenic-aware CACTI for cache modeling down to 10K in advanced 7nm FinFETs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced maximal biclique enumeration on GPUs using bitmaps. <em>TC</em>, <em>74</em>(8), 2552-2566. (<a href='https://doi.org/10.1109/TC.2025.3566898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximal biclique enumeration (MBE) in bipartite graphs is an important problem in data mining with many real-world applications. Parallel MBE algorithms for GPUs are needed for MBE acceleration leveraging its many computing cores. However, enumerating maximal bicliques using GPUs has three main challenges including large memory requirement, thread divergence, and load imbalance. In this paper, we propose GMBE+, an advanced GPU solution for the MBE problem. To overcome the challenges, we design (1) a node-reuse approach to reduce GPU memory usage with advanced node pruning, (2) a bitmap-based set intersection approach to minimize thread divergence, and (3) a load-aware task scheduling framework to achieve load balance among threads within GPU warps, facilitated by a novel set union approach. Our experiments reveal that GMBE+ is 1.2$ \times $ faster than the latest GPU-based MBE algorithm GMBE on average when running on the same NVIDIA A100 GPU.},
  archive      = {J_TC},
  author       = {Zhe Pan and Shuibing He and Xu Li and Xuechen Zhang and Rui Wang and Yanlong Yin and Gang Chen},
  doi          = {10.1109/TC.2025.3566898},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2552-2566},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Advanced maximal biclique enumeration on GPUs using bitmaps},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel two-round two-party quantum private comparison protocol based on quantum walks. <em>TC</em>, <em>74</em>(8), 2542-2551. (<a href='https://doi.org/10.1109/TC.2025.3566897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to compare the size relationship and the equality of private integers from two users without any risk of leaking their secrets, we put forward a novel two-round two-party quantum private comparison (QPC) protocol with a third party (TP) based on one-direction quantum walks on a circle (ODQWC). Here, TP is allowed to launch all kinds of possible attacks except the collusion attack. This protocol adopts quantum walk (QW) states as the initial quantum resource, which are actually a kind of two-particle product states. This protocol doesn’t need any pre-shared secret key or quantum entanglement swapping operation, and only adopts unitary operations and d-dimensional single particle measurements. Security analysis validates the security of the proposed protocol against both the outside attacks and the participant attacks. Compared with previous QPC protocols based on QW, this protocol is more efficient under certain conditions. Moreover, we construct quantum circuits for this protocol and reveal its correctness and feasibility by IBM Qiskit simulator.},
  archive      = {J_TC},
  author       = {Jin-Tao Wang and Tian-Yu Ye},
  doi          = {10.1109/TC.2025.3566897},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2542-2551},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A novel two-round two-party quantum private comparison protocol based on quantum walks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prophet: SSD failure analysis and prediction guided by flash reliability characteristics in data centers. <em>TC</em>, <em>74</em>(8), 2529-2541. (<a href='https://doi.org/10.1109/TC.2025.3566871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solid-state drives (SSDs) are massively deployed in various fields, especially in data centers, for their excellent cost-effectiveness. However, SSDs may fail due to their imperfect manufacturing processes, resulting in system-level failures and even downtime in data centers. This makes SSD failure prediction critical. Current studies focus on dealing with data missing, numerical normalization, and other statistical issues in using machine learning methods, but the consideration of the reliability characteristics of the underlying flash media of SSDs and the timeliness (time duration between predicted failure and real failure) of SSD failure prediction result is missing. In this work, we study the failure characteristics of over 200,000 drives from industry data centers over a 4-year period, as well as daily data. The relationship between SSD attribute values and failures is first investigated. Then, we analyzed the SSD failure characteristics from several aspects (causes, differences between failures, and timeliness of prediction results) relying on flash reliability characteristics. Based on these, a novel SSD failure prediction method (Prophet) is proposed. Specifically, Prophet contains the following two components. First, to cope with the differences between failures, a diff-state method is proposed for differential machine learning modeling of SSDs in different “States”. We define the “State” of an SSD, which represents the range of values in which the SSD currently lies in terms of some key attributes. Through flash reliability characteristics, we distinguish between different failures before training the model to obtain accurate predictions of different failure behaviors. Second, a recovery period method is proposed to enhance the timeliness of SSD failure prediction result by designing the sample selection method. The enhanced timeliness can be utilized by operations personnel to handle failed SSDs, such as replacement and repair. The evaluation results of the real dataset show that the predictive ability of Prophet is improved amazingly, realizing a high recall and low false-positive rates while providing sufficient response time for the processing of failed SSDs.},
  archive      = {J_TC},
  author       = {Yunpeng Song and Yujiong Liang and Jialin Liu and Liang Shi},
  doi          = {10.1109/TC.2025.3566871},
  journal      = {IEEE Transactions on Computers},
  month        = {8},
  number       = {8},
  pages        = {2529-2541},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Prophet: SSD failure analysis and prediction guided by flash reliability characteristics in data centers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). THEMIS: Time, heterogeneity, and energy minded scheduling for fair multi-tenant use in FPGAs. <em>TC</em>, <em>74</em>(7), 2515-2528. (<a href='https://doi.org/10.1109/TC.2025.3566874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using correct design metrics and understanding the limitations of the underlying technology is critical to developing effective scheduling algorithms. Unfortunately, existing scheduling techniques used incorrect metrics and had unrealistic assumptions for fair scheduling of multi-tenant FPGAs where each tenant is aimed to share approximately the same number of resources both spatially (in space) and temporally (in time). This paper proposes an improved fair scheduling algorithm that fixes earlier issues with metrics and assumptions for ‘fair’ multi-tenant FPGA use. Specifically, we claim three improvements. First, we consider both spatial and temporal aspects to provide spatiotemporal fairness—this improves a recent prior work that assumed all tasks would have the same latency. Second, we add the energy dimension to fairness: by calibrating the scheduling decision intervals and including their energy overhead, our algorithm offers trading off energy efficiency for fairness. Third, we consider previously ignored facts about FPGA multi-tenancy, such as the existence of heterogeneous regions and the inflexibility of run-time merging/splitting of partially reconfigurable regions. We develop and evaluate our improved fair scheduling algorithm with these three enhancements. Inspired by the Greek goddess of law and personification of justice, we name our fair scheduling solution THEMIS: Time, Heterogeneity, and Energy Minded Scheduling. We implemented our solution on Xilinx Zedboard XC7Z020 with real hardware workloads and used real measurements on the FPGA to quantify the savings of our approach. Compared to previous algorithms, our improved scheduling algorithm enhances fairness between 24.2–98.4% and allows a trade-off between 55.3$\times$ in energy vs. 69.3$\times$ in fairness. The paper thus informs cloud providers about future scheduling optimizations for fairness with related challenges and opportunities.},
  archive      = {J_TC},
  author       = {Emre Karabulut and Arsalan Ali Malik and Amro Awad and Aydin Aysu},
  doi          = {10.1109/TC.2025.3566874},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2515-2528},
  shortjournal = {IEEE Trans. Comput.},
  title        = {THEMIS: Time, heterogeneity, and energy minded scheduling for fair multi-tenant use in FPGAs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anole: A pragmatic blend of classic and learning-based algorithms in congestion control. <em>TC</em>, <em>74</em>(7), 2501-2514. (<a href='https://doi.org/10.1109/TC.2025.3566872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, hybrid congestion control (CC) algorithms that combine rule-based CC and learning-based CC have gained significant attention. They incorporate the fast adaption ability of learning-based CC and the stability of rule-based CC, tending to select the better-performing rate based on the network feedback. However, the practical implementation of such algorithms has revealed primary issues. Specifically, they require both CCs to run alternately, which results in a poorly performing CC continuing to run in the network. Moreover, hybrid CCs cannot converge to the optimal rate when both CCs perform poorly. This paper proposes Anole to address these issues. Anole has three main algorithmic contributions: 1) Anole always selects the better-performing CC, 2) Anole temporarily deprecates the consistently underperforming CC, 3) when both CCs perform poorly, Anole infers the optimal sending rate based on the network feedback. We carry out comprehensive experiments in both emulated and real-world wired networks, as well as in real-world WiFi networks, to assess the performance of Anole. The experiment results demonstrate that Anole achieves approximately 6% higher throughput in real-world links and 34% lower delay in the 48Mbps link compared to the state-of-the-art CC. Anole also exhibits superior performance in adaptability and fair convergence.},
  archive      = {J_TC},
  author       = {Feixue Han and Yike Wang and Yunbo Zhang and Qing Li and Dayi Zhao and Yong Jiang},
  doi          = {10.1109/TC.2025.3566872},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2501-2514},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Anole: A pragmatic blend of classic and learning-based algorithms in congestion control},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GroPipe: A grouped pipeline hybrid parallel method for accelerating DCNNs training. <em>TC</em>, <em>74</em>(7), 2487-2500. (<a href='https://doi.org/10.1109/TC.2025.3566869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training large Deep Convolutional Neural Networks (DCNNs) with increasingly large datasets to improve model accuracy has become extremely time-consuming. Distributed training methods, such as data parallelism (DP) and pipeline model parallelism (PMP), offer potential solutions but face challenges like load imbalance and significant communication overhead. This paper introduces GroPipe, a novel architecture that synergistically integrates PMP and DP, markedly improving training speeds. GroPipe employs an automatic model partitioning algorithm based on a performance projection technique, ensuring load balance and facilitating quantitative performance evaluation in PMP. Additionally, it adopts a group-based delayed asynchronous communication strategy to efficiently reduce communication overhead in DP. Using the ResNet and VGG models with the ImageNet dataset, extensive experiments are performed on an 8-GPU server and demonstrate GroPipe’s effectiveness. GroPipe achieves substantial improvements in time to accuracy, showing an average improvement of 42.2% and 14.0% on the ResNet series, and 79.2% and 43.9% on the VGG series, without compromising Top-1 accuracy.},
  archive      = {J_TC},
  author       = {Bin Liu and Yongyao Ma and Zijian Hu and Zeyu Ji and Zhenli He and Keqin Li},
  doi          = {10.1109/TC.2025.3566869},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2487-2500},
  shortjournal = {IEEE Trans. Comput.},
  title        = {GroPipe: A grouped pipeline hybrid parallel method for accelerating DCNNs training},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional privacy-preserving transaction for the unspent transaction output-based multi-chain blockchain system. <em>TC</em>, <em>74</em>(7), 2473-2486. (<a href='https://doi.org/10.1109/TC.2025.3566868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The anonymity of blockchain may be exploited by criminals for illegal fund transfers, thus a conditional privacy-preserving scheme is important for blockchain regulation. Currently, sharding technology under a multi-chain architecture is used to improve blockchain scalability. However, current conditional privacy-preserving schemes cannot work on this architecture. To protect the privacy of the transaction, we present a conditional privacy-preserving transaction scheme (MC-CPPT) for multi-chain blockchain system. In this system, we proposed a zero-knowledge proof based anonymous transaction, in terms of the identities of transaction participants and amounts, which also enables the unlinkability of transactions and indistinguishability between cross-chain and intra-chain transactions in multi-chain blockchain system. In addition, a multi-node regulatory agency is introduced to control the transaction amount and frequency in the system without a single point of failure. Moreover, an ECC-based encryption scheme is proposed to achieve the traceability of suspicious transactions. A security model is defined and the security of MC-CPPT is demonstrated to meet the expected security goals. Evaluating the prototype revealed acceptable performance and additional security features.},
  archive      = {J_TC},
  author       = {Jie Cui and Wenting Zhuang and Hong Zhong and Qingyang Zhang and Fengqun Wang and Debiao He},
  doi          = {10.1109/TC.2025.3566868},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2473-2486},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Conditional privacy-preserving transaction for the unspent transaction output-based multi-chain blockchain system},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing multi-AAV cooperative tracking for real-time applications in network-challenged environments. <em>TC</em>, <em>74</em>(7), 2461-2472. (<a href='https://doi.org/10.1109/TC.2025.3566867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous aerial vehicles (AAVs) have found widespread utility in the field of multi-target tracking (MTT) due to their inherent advantages, such as ease of deployment, flexible maneuverability, and cooperative communication capabilities. AAVs can perform tasks ranging from regional surveillance to tracking and search-and-rescue operations in hazardous environments. Nonetheless, the issue of how to efficiently coordinate multiple AAVs to track diverse mobile targets remains a critical concern. This paper focuses on MTT with multi-AAV, aiming at optimizing system performance in scenarios where network availability is limited. In contrast to treating sensor perception as a monolithic process, we propose a scheme for cooperative sensing and data processing. This scheme is designed to reduce system response latency in environmental information sensing and multidimensional data processing for multiple AAVs. Furthermore, in contrast to assuming linear target movement and single-step target position prediction, we introduce a multi-agent deep reinforcement learning (MADRL) framework combined with multi-step prediction extended Kalman Filter (MP-EKF). This framework is tailored to enhance tracking precision, especially when targets’ trajectories are curved, which can reduce AAV flight displacement if the target’s position can be predicted multiple steps later. In addition, unlike using latency as a real-time application to measure the “freshness” of information, the Age of Information (AoI) is introduced for considering the waiting time of transmission and calculation between multiple AAVs. This assessment method is utilized to comprehensively evaluate and mitigate data latency within the MADRL algorithm. Finally, extensive simulation experiments demonstrate that the proposed scheme significantly outperforms both baseline methods and state-of-the-art approaches in terms of AoI, system latency, and energy consumption.},
  archive      = {J_TC},
  author       = {Na Lin and Zhijiang Wang and Liang Zhao and Ammar Hawbani and Zhi Liu and Mohsen Guizani},
  doi          = {10.1109/TC.2025.3566867},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2461-2472},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing multi-AAV cooperative tracking for real-time applications in network-challenged environments},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive incentivize for federated learning with cloud-edge collaboration under multi-level information sharing. <em>TC</em>, <em>74</em>(7), 2445-2460. (<a href='https://doi.org/10.1109/TC.2025.3566864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning with Cloud-Edge Collaboration (FL-CEC) has emerged as a cutting-edge paradigm in distributed learning. Efficient resource investment incentive mechanisms are crucial to encouraging clients in FL-CEC to contribute the necessary data and computational resources for training. However, existing studies are inadequate in meeting the incentive design requirements under multi-level information-sharing scenarios. Moreover, current works often rely on specific functional relationships between resource investment and global model accuracy. To bridge these gaps, this paper investigates the incentive problem for data and computational resource investment under multi-level information-sharing levels. We design a resource investment incentive mechanism based on a weighted potential game without depending on any specific functional relationship between data investment and model accuracy. Furthermore, we propose four algorithms to solve resource investment strategies for different levels of information sharing. The complexity and convergence rates of the proposed algorithms are thoroughly analyzed. Finally, we construct a simulation incentive platform on Aliyun. Extensive evaluations demonstrate that the proposed scheme effectively enhances social welfare, and improves collaborative training accuracy and efficiency.},
  archive      = {J_TC},
  author       = {Shijing Yuan and Beiyu Dong and Jie Li and Song Guo and Hongyang Chen and Chentao Wu and Jie Wu and Wei Zhao},
  doi          = {10.1109/TC.2025.3566864},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2445-2460},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive incentivize for federated learning with cloud-edge collaboration under multi-level information sharing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An effective iterative statistical fault injection methodology for deep neural networks. <em>TC</em>, <em>74</em>(7), 2431-2444. (<a href='https://doi.org/10.1109/TC.2025.3566863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of the state-of-the-art devices makes reliability assessments approaches extremely complex and, sometimes, out of the timing constraints and computational capabilities. Fault Injections (FIs) are one of the most used approaches for evaluating the dependability of safety-critical systems. With billion-transistor hardware devices running trillion-parameter deep neural networks, injecting the entire fault universe is unfeasible. A widespread solution consists in performing statistical fault injections (SFIs), injecting a subset of faults to estimate a characteristic with an error margin and a confidence level. This research work presents an iterative SFI approach to estimate failure rates in convolutional neural networks (CNNs), i.e., the percentage of wrong predictions caused by random hardware faults affecting synaptic weights. SFIs at different granularities have been performed with margin of errors equal to 1%, 0.1%, and 0.01%. Results for two CNNs (ResNet20 and MobileNetV2) are presented and experimentally and statistically demonstrate the effectiveness of the proposed approach. For instance, to estimate the network-wise failure rate with an error margin of 0.01%, the proposed approach reduces the total injected faults by about 66% and 90% compared to conservative methods, and by 1.94% and 1.65% compared to iterative SFI methods in the literature, for ResNet20 and MobileNetV2, respectively.},
  archive      = {J_TC},
  author       = {Annachiara Ruospo and Matteo Sonza Reorda and Riccardo Mariani and Ernesto Sanchez},
  doi          = {10.1109/TC.2025.3566863},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2431-2444},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An effective iterative statistical fault injection methodology for deep neural networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLCD: Machine learning-based code version and device selection for heterogeneous systems. <em>TC</em>, <em>74</em>(7), 2417-2430. (<a href='https://doi.org/10.1109/TC.2025.3558606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous systems with hardware accelerators are increasingly common, and various optimized implementations/algorithms exist for computation kernels. However, no single best combination of code version and device (C&D) can outperform others across all input cases, demanding a method to select the best C&D pair based on input. We present machine learning-based code version and device selection method, named MLCD, that uses input data characteristics to select the best C&D pair dynamically. We also apply active learning to reduce the number of samples needed to construct the model. Demonstrated on two different CPU-GPU systems, MLCD achieves near-optimal speed-up regardless of which systems tested. Concretely, reporting results from system one with mid-end hardwares, it achieves 99.9%, 95.6%, 99.9%, and 98.6% of the optimal acceleration attainable through the ideal choice of C&D pairs in General Matrix Multiply, PageRank, N-body Simulation, and K-Motif Counting, respectively. MLCD achieves a speed-up of 2.57$\boldsymbol{\times}$, 1.58$\boldsymbol{\times}$, 2.68$\boldsymbol{\times}$, and 1.09$\boldsymbol{\times}$ compared to baselines without MLCD. Additionally, MLCD handles end-to-end applications, achieving up to 10% and 46% speed-up over GPU-only and CPU-only solutions with Graph Neural Networks. Furthermore, it achieves 7.28$\boldsymbol{\times}$ average speed-up in execution latency over the state-of-the-art approach and determines suitable code versions for unseen input $10^{8}-10^{10}\boldsymbol{\times}$ faster.},
  archive      = {J_TC},
  author       = {Kaiwen Cao and Hanchen Ye and Yihan Pang and Deming Chen},
  doi          = {10.1109/TC.2025.3558606},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2417-2430},
  shortjournal = {IEEE Trans. Comput.},
  title        = {MLCD: Machine learning-based code version and device selection for heterogeneous systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BEAST-GNN: A united bit sparsity-aware accelerator for graph neural networks. <em>TC</em>, <em>74</em>(7), 2402-2416. (<a href='https://doi.org/10.1109/TC.2025.3558587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) excel in processing graph-structured data, making them attractive and promising for tasks such as recommender systems and traffic forecasting. However, GNNs’ irregular computational patterns limit their ability to achieve low latency and high energy efficiency, particularly in edge computing environments. Current GNN accelerators predominantly focus on value sparsity, underutilizing the potential performance gains from bit-level sparsity. However, applying existing bit-serial accelerators to GNNs presents several challenges. These challenges arise from GNNs’ more complex data flow compared to conventional neural networks, as well as difficulties in data localization and load balancing with irregular graph data. To address these challenges, we propose BEAST-GNN, a bit-serial GNN accelerator that fully exploits bit-level sparsity. BEAST-GNN introduces streamlined sparse-dense bit matrix multiplication for optimized data flow, a column-overlapped graph partitioning method to enhance data locality by reducing memory access inefficiencies, and a sparse bit-counting strategy to ensure balanced workload distribution across processing elements (PEs). Compared to state-of-the-art accelerators, including HyGCN, GCNAX, Laconic, GROW, I-GCN, SGCN, and MEGA, BEAST-GNN achieves speedups of 21.7$\boldsymbol{\times}$, 6.4$\boldsymbol{\times}$, 10.5$\boldsymbol{\times}$, 3.7$\boldsymbol{\times}$, 4.0$\boldsymbol{\times}$, 3.3$\boldsymbol{\times}$, and 1.4$\boldsymbol{\times}$ respectively, while also reducing DRAM access by 36.3$\boldsymbol{\times}$, 7.9$\boldsymbol{\times}$, 6.6$\boldsymbol{\times}$, 3.9$\boldsymbol{\times}$, 5.38$\boldsymbol{\times}$, 3.37$\boldsymbol{\times}$, and 1.44$\boldsymbol{\times}$. Additionally, BEAST-GNN consumes only 4.8%, 12.4%, 19.6%, 27.7%, 17.0%, 26.5%, and 82.8% of the energy required by these architectures.},
  archive      = {J_TC},
  author       = {Yunzhen Luo and Yan Ding and Zhuo Tang and Keqin Li and Kenli Li and Chubo Liu},
  doi          = {10.1109/TC.2025.3558587},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2402-2416},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BEAST-GNN: A united bit sparsity-aware accelerator for graph neural networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FVM: Practical feather-weight virtualization on commodity microcontrollers. <em>TC</em>, <em>74</em>(7), 2389-2401. (<a href='https://doi.org/10.1109/TC.2025.3558582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been an increasing drive to consolidate multiple microcontrollers into one physical entity, due to advantages in reducing overall costs, enhancing reliability, and simplifying hardware interconnections. To reduce consolidation engineering costs, minimizing system latency and memory footprint is important as well as maintaining compatibility with legacy software. In this paper, we propose a virtualization-based solution called Feather-weight Virtual Machine (FVM) that focuses on these goals. FVM enables low latency by specializing the virtualization model to Real-Time Operating Systems (RTOSes), achieves small footprint by adapting management policies to microcontroller memories, attains high compatibility by aligning with microcontroller ecosystem idiosyncrasies, finally allowing practical consolidation across a wide range of commodity microcontrollers. We implement and evaluate FVM on ARMv6-M, ARMv7-M, and RISC-V architectures with two toolchains and two RTOSes, and it can fit into 20 KiB of RAM with less than 5% latency bloat.},
  archive      = {J_TC},
  author       = {Junchao Li and Runsheng Hou and Guangyong Shang and Huanle Zhang and Xiuzhen Cheng and Runyu Pan},
  doi          = {10.1109/TC.2025.3558582},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2389-2401},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FVM: Practical feather-weight virtualization on commodity microcontrollers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BE-NPU: A bandwidth-efficient neural processing unit with adaptive processing schemes for reduced off-chip bandwidth demand. <em>TC</em>, <em>74</em>(7), 2376-2388. (<a href='https://doi.org/10.1109/TC.2025.3558579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing neural processing units (NPUs) mainly focus on the optimized multiply-accumulate (MAC) arrays for efficient inference of convolutional neural networks (CNNs). However, off-chip data transmission usually keeps NPUs waiting during CNN inference, causing up to 38.4GB/s off-chip bandwidth (OCB) demand for mobile AI devices. And none of the previous benchmarks quantitatively evaluate the bandwidth efficiency of different NPU architectures. In addition, CNNs exhibit distinct characteristics of off-chip data transmission when applied to different fields, and it has become a challenging task for NPUs to support different CNNs efficiently with reasonable OCB demand. To address the aforementioned issues, this paper proposes the Bandwidth-Peak Performance Ratio for n percentages of ideal frame rate (BPPR-n%) to demonstrate the normalized OCB demand of different NPU architectures. A bandwidth-efficient NPU (BE-NPU) is introduced with adaptive processing schemes to reduce the OCB demand during inference of different CNNs. The adaptive processing schemes include both instruction-level and thread-level schemes. For the instruction-level scheme, decoupled execute/access is introduced into depth-first (DF) and layer-first (LF) schemes to improve the concurrency between NPU calculation (CAL) and direct memory access (DMA) instructions. For the thread-level scheme, DF and LF threads are hybridly processed to further improve overall NPU efficiency. Compared with state-of-the-art works, BE-NPU achieves 48.1%∼80.6% reduction of BPPR-80% and 67.0%∼95.1% reduction of BPPR-95%. The proposed architecture is synthesized with TSMC 28nm technology node. BE-NPU utilizes 14.3% additional logic gates compared with baseline implementation.},
  archive      = {J_TC},
  author       = {Yichuan Bai and Xiaopeng Zhang and Qian Wang and Yaqing Li and Yuan Du and Li Du},
  doi          = {10.1109/TC.2025.3558579},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2376-2388},
  shortjournal = {IEEE Trans. Comput.},
  title        = {BE-NPU: A bandwidth-efficient neural processing unit with adaptive processing schemes for reduced off-chip bandwidth demand},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Schedulability analysis for self-suspending tasks under EDF-like scheduling. <em>TC</em>, <em>74</em>(7), 2364-2375. (<a href='https://doi.org/10.1109/TC.2025.3558079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time systems involve tasks that may voluntarily suspend their execution as they await specific events or resources. Such self-suspension can introduce further delays and unpredictability in scheduling, making the analysis more challenging. Most current schedulability analysis methods of self-suspending tasks focus on fixed-priority scheduling or tasks with constrained deadlines. This paper proposes two schedulability analysis methods for self-suspending tasks with arbitrary deadlines under earliest-deadline-first-like (EDF-like) scheduling. Both methods are designed for preemptive uniprocessor systems. We first present a jitter-based response time analysis (JRTA) method. JRTA is designed based on a self-suspending response time analysis (SS-RTA) method under earliest-deadline-first (EDF) scheduling. We first convert self-suspensions to release jitters and then present a response time analysis (RTA) method of tasks with release jitters under EDF-like scheduling. To address the complexity of JRTA, we propose an improved schedulability analysis (ISA), a sufficiency blocking-based method. Finally, we provide many simulation experiments under some EDF-like scheduling algorithms. The results verify the effectiveness and efficiency of both proposed methods.},
  archive      = {J_TC},
  author       = {Yan Wang and Bo Lv and Quan Zhou and Junfei Li and Tan Tan},
  doi          = {10.1109/TC.2025.3558079},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2364-2375},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Schedulability analysis for self-suspending tasks under EDF-like scheduling},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An area optimization approach for large-scale RM-TB dual logic circuits based on a multitasking optimization algorithm. <em>TC</em>, <em>74</em>(7), 2348-2363. (<a href='https://doi.org/10.1109/TC.2025.3558077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logic synthesis is a crucial step in integrated circuit design, and area optimization is an indispensable part of this process. However, the area optimization problem for large-scale Fixed Polarity Reed-Muller (FPRM) circuits is an NP-hard problem. To address this problem, we divide Boolean circuits into small-scale circuits based on the idea of divide-and-conquer using the proposed grouping decomposition mechanism. Each small-scale Boolean circuit is transformed into an FPRM circuit by a polarity transformation algorithm. To ensure the circuit's functionality remains unaffected, we integrate FPRM circuits into an FPRM and Boolean (RM-TB) dual logic circuit based on the proposed gate-level integration. However, the area optimization problem of RM-TB dual logic circuits is a multi-task, high-dimensional, and multi-extremal combinatorial optimization problem. Therefore, we propose a Multipopulation Multitasking Optimization Algorithm (MMuOA) that integrates self-evolution with a multitasking equilibrium optimizer and cross-task evolution through knowledge sharing and transfer. This forms a dynamic optimization framework for simultaneously searching for the optimal polarity corresponding to the minimal area of RM-TB dual logic circuits. Moreover, we propose an Area Optimization Approach (AOA) for an RM-TB dual logic circuit with the minimum area using the MMuOA. Experimental results based on the Microelectronics Center of North Carolina (MCNC) Benchmark test circuits demonstrate the effectiveness and superiority of the AOA compared to the state-of-the-art area optimization approach.},
  archive      = {J_TC},
  author       = {Xiaoqian Wu and Peng Wang and Shaoquan Li and Huaxiao Liu and Lei Liu},
  doi          = {10.1109/TC.2025.3558077},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2348-2363},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An area optimization approach for large-scale RM-TB dual logic circuits based on a multitasking optimization algorithm},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating RNA-seq quantification on a real processing-in-memory system. <em>TC</em>, <em>74</em>(7), 2334-2347. (<a href='https://doi.org/10.1109/TC.2025.3558075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, with the growth of the required data size for emerging applications (e.g., graph processing and machine learning), the von Neumann bottleneck has become a main problem for restricting the throughput of the applications. To address the problem, an acceleration technique called Processing in Memory (PIM) has garnered attention due to its potential to reduce off-chip data movement between the processing unit (e.g., CPU) and memory device (e.g., DRAM). In 2019, UPMEM introduced the commercially available processing-in-memory product, the DRAM Processing Unit (DPU) [8], showing a new chance for accelerating data-intensive applications. Among data-intensive applications, RNA sequence (RNA-seq) quantification is used to measure the abundance of RNA sequences, and it also plays a critical role in the field of bioinformatics. We aim to leverage UPMEM DPU to accelerate RNA-seq Quantification. However, due to the DPU usage limitations caused by DPU hardware, there are some challenges to realizing RNA-seq Quantification on the DPU system. To overcome these challenges, we propose UpPipe, which consists of the DPU-friendly transcriptome allocation, the DPU-aware pipeline management, and the WRAM prefetching scheme. The UpPipe considers the hardware limitations of DPUs, enabling efficient sequence alignment even within the resource-constrained DPUs. The experimental results demonstrate the feasibility and efficiency of our proposed design. We also provide an evaluation study on the impact of data granularity selection on pipeline management and the optimal size for the WRAM prefetching scheme.},
  archive      = {J_TC},
  author       = {Liang-Chi Chen and Chien-Chung Ho and Yuan-Hao Chang},
  doi          = {10.1109/TC.2025.3558075},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2334-2347},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating RNA-seq quantification on a real processing-in-memory system},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CIMUS: 3D-stacked computing-in-memory under image sensor architecture for efficient machine vision. <em>TC</em>, <em>74</em>(7), 2321-2333. (<a href='https://doi.org/10.1109/TC.2025.3558068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational image sensors with CNN processing capabilities are emerging to alleviate the energy-intensive and time-consuming data movement between sensors and external processors. However, deploying CNN models onto these computational image sensors faces challenges from the limited on-chip memory resources and insufficient image processing throughput. This work proposes a 3D-stacked NAND flash-based computing-in-memory under image sensor architecture (CIMUS) to facilitate the complete deployment of CNN model. To fully leverage the potential of high bandwidth from the 3D-stacked integration, we design a novel distributed CNN mapping and dataflow to process the full focal plane image in parallel, which senses and recognizes ImageNet tasks with >1000fps. To tackle the computational error of inputs “0” in 3D NAND flash-based CIM, we propose an input-independent offset compensation method, which reduces the average vector-matrix multiplication (VMM) error by 48%. Evaluation results indicate that CIMUS architecture achieves a 9.8× improvement in CNN inference speed and a 33× boost in energy efficiency compared to the state-of-the-art computational image sensor in the ImageNet recognition task.},
  archive      = {J_TC},
  author       = {Lixia Han and Yiyang Chen and Siyuan Chen and Haozhang Yang and Ao Shi and Guihai Yu and Jiaqi Li and Zheng Zhou and Yijiao Wang and Yanzhi Wang and Xiaoyan Liu and Jinfeng Kang and Peng Huang},
  doi          = {10.1109/TC.2025.3558068},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2321-2333},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CIMUS: 3D-stacked computing-in-memory under image sensor architecture for efficient machine vision},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient and unified RTL accelerator design for HQC-128, HQC-192, and HQC-256. <em>TC</em>, <em>74</em>(7), 2306-2320. (<a href='https://doi.org/10.1109/TC.2025.3558044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Post-Quantum Standardization (PQC) process held by the National Institute of Standards and Technology (NIST), the final round of evaluation of the asymmetric cryptographic schemes Classic McEliece, BIKE and HQC will elect the alternative Key Establishment Mechanism (KEM) to the FIPS $203$ standard CRYSTALS-Kyber. In this work we present two configurations of a RTL hardware design of the HQC candidate, either optimized for devices exclusively working with client-server style protocols, or a unified accelerator compatible with all KEM operations, i.e. Key Generation, Encapsulation, and Decapsulation. Our designs are compatible with all the parameter sets defined by the HQC specification, providing security margins equivalent to the ones of AES-128, AES-192, and AES-256 based on a selection made at runtime. We are providing an extensive comparison with the current state-of-the-art RTL hardware designs for Artix-$7$ FPGAs of the schemes in the PQC process, introducing a new metric to evaluate the area utilization, historically a challenging task for such devices made of heterogeneous resources, and determining that HQC has by far the best figures among the code-based candidates in terms of latency, area occupied and efficiency, and even comparable with the lattice-based CRYSTALS-Kyber when using the parameters with lowest security margin.},
  archive      = {J_TC},
  author       = {Francesco Antognazza and Alessandro Barenghi and Gerardo Pelosi},
  doi          = {10.1109/TC.2025.3558044},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2306-2320},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An efficient and unified RTL accelerator design for HQC-128, HQC-192, and HQC-256},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCGG: A dynamically adaptive and hardware-software coordinated runtime system for GNN acceleration on GPUs. <em>TC</em>, <em>74</em>(7), 2293-2305. (<a href='https://doi.org/10.1109/TC.2025.3558042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are a prominent trend in graph-based deep learning, known for their capacity to produce high-quality node embeddings. However, the existing GNN framework design is only implemented from the algorithm level, and the hardware architecture of the GPU is not fully utilized. To this end, we propose DCGG, a dynamic runtime adaptive framework, which can accelerate various GNN workloads on GPU platforms. DCGG has carried out deeper optimization work mainly in terms of load balancing and software and hardware matching. Accordingly, three optimization strategies are proposed. First, we propose dynamic 2D workload management methods and perform customized optimization based on it, effectively reducing additional memory operations. Second, a new slicing strategy is adopted, combined with hardware features, to effectively improve the efficiency of data reuse. Third, DCGG uses the Quantitative Dimension Parallel Strategy to optimize dimensions and parallel methods, greatly improving load balance and data locality. Extensive experiments demonstrate that DCGG outperforms the state-of-the-art GNN computing frameworks, such as Deep Graph Library (up to 3.10$\boldsymbol{\times}$ faster) and GNNAdvisor (up to 2.80$\boldsymbol{\times}$ faster), on mainstream GNN architectures across various datasets.},
  archive      = {J_TC},
  author       = {Guoqing Xiao and Li Xia and Yuedan Chen and Hongyang Chen and Wangdong Yang},
  doi          = {10.1109/TC.2025.3558042},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2293-2305},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DCGG: A dynamically adaptive and hardware-software coordinated runtime system for GNN acceleration on GPUs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $\mathtt{SFPoW}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi mathvariant="monospace">SFPoW</mml:mi></mml:mrow></mml:math>: Constructing secure and flexible proof-of-work sidechains for cross-chain interoperability with wrapped assets. <em>TC</em>, <em>74</em>(7), 2278-2292. (<a href='https://doi.org/10.1109/TC.2025.3558040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sidechain techniques enhance blockchain scalability and interoperability, enabling decentralized exchanges and cross-chain operations for wrapped digital assets. However, existing PoW sidechains face challenges, including centralization, high communication costs, and incomplete PoW-based security proofs. This paper introduces $\mathtt{SFPoW}$, a Secure and Flexible Proof-of-Work sidechains for cross-chain interoperability with wrapped assets. $\mathtt{SFPoW}$ facilitates decentralized asset transfers and token swaps across nearly all PoW-based cryptocurrencies without requiring soft or hard forks or fixed PoW targets. It establishes a decentralized, fair validation set within the sidechain, improving adaptability and reducing competition and confirmation periods. A pluggable cross-chain proof generation method is proposed, effectively filtering lazy nodes, incentivizing active participation, and minimizing on-chain verification overhead to a proof size of 200.5 bytes. Through mining behavior analysis and cryptographic reductions, $\mathtt{SFPoW}$ satisfies weak and strong atomicity. Experiments on the Ronin blockchain and an Ethereum testnet demonstrate a round-trip cost of $6.55 and latency of 372.1–382.6 seconds, confirming its practicality and efficiency.},
  archive      = {J_TC},
  author       = {Zhihong Deng and Chunming Tang and Taotao Li and Zhikang Zeng and Parhat Abla and Debiao He},
  doi          = {10.1109/TC.2025.3558040},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2278-2292},
  shortjournal = {IEEE Trans. Comput.},
  title        = {$\mathtt{SFPoW}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi mathvariant="monospace">SFPoW</mml:mi></mml:mrow></mml:math>: Constructing secure and flexible proof-of-work sidechains for cross-chain interoperability with wrapped assets},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential fault attack on HE-friendly stream ciphers: Masta, pasta, and elisabeth. <em>TC</em>, <em>74</em>(7), 2267-2277. (<a href='https://doi.org/10.1109/TC.2025.3558036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the Differential Fault Attack (DFA) on three Homomorphic Encryption (HE) friendly stream ciphers Masta, Pasta, and Elisabeth. Both Masta and Pasta are Rasta-like ciphers with publicly derived and pseudorandom affine layers. The design of Elisabeth is an extension of FLIP and FiLIP, following the group filter permutator paradigm. All these three ciphers operate on elements over $\mathbb{Z}_{p}$ or $\mathbb{Z}_{2^{n}}$, rather than $\mathbb{Z}_{2}$. We can recover the secret keys of all the targeted ciphers through DFA. In particular, for Elisabeth, we present a new method to determine the filtering path, which is vital to make the attack practical. Our attacks on various instances of Masta are practical and require only one block of keystream and a single word-based fault. By injecting three word-based faults, we can theoretically mount DFA on two instances of Pasta, Pasta-3 and Pasta-4. For Elisabeth-4, the only instance of the Elisabeth family, we present two DFAs in which we inject four bit-based faults or a single word-based fault. With 15000 normal and faulty keystream words, the DFA on Elisabeth-4 can be completed in just a few minutes.},
  archive      = {J_TC},
  author       = {Weizhe Wang and Deng Tang},
  doi          = {10.1109/TC.2025.3558036},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2267-2277},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Differential fault attack on HE-friendly stream ciphers: Masta, pasta, and elisabeth},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Virtual NVMe-based storage function framework with fast I/O request state management. <em>TC</em>, <em>74</em>(7), 2253-2266. (<a href='https://doi.org/10.1109/TC.2025.3558033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current cloud environments provide numerous storage functions to virtual machines such as disk encryption, snapshotting, compression and so on. These functions are implemented using software stacks inside the hypervisor's kernel, emulator, or as a userspace polling driver like SPDK. However, each stack brings its own limitations: Linux's kernel I/O stack cannot easily integrate proprietary technologies such as Intel SGX, while SPDK requires significant changes in software development and tooling yet lacks the rich feature set of existing solutions like Linux LVM. To remedy these limitations, we introduce NVMetro, a high-performance storage framework for virtual machines based on the NVMe protocol. NVMetro provides multiple I/O paths that can be dynamically combined to fit the needs of each storage function. It links these paths together with an eBPF-based I/O router/classifier framework, as well as a userspace software stack for out-of-kernel I/O processing. We implemented three different storage functions with NVMetro and evaluated them under various workloads. Our results show that NVMetro approaches the performance of kernel-bypass solutions like SPDK while maintaining the compatibility and ease of use of in-kernel storage stacks.},
  archive      = {J_TC},
  author       = {Tu Dinh Ngoc and Boris Teabe and Georges Da Costa and Daniel Hagimont},
  doi          = {10.1109/TC.2025.3558033},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2253-2266},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Virtual NVMe-based storage function framework with fast I/O request state management},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CINDA: Using cache-coherent interconnects for accelerating databases by enabling near-data processing of update transactions. <em>TC</em>, <em>74</em>(7), 2238-2252. (<a href='https://doi.org/10.1109/TC.2025.3558028'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Near-Data Processing (NDP) has been proven useful to accelerate Database Management Systems (DBMS) that handle infrequently accessed data stored in slow persistent storage. A key challenge for such an architecture is the synchronization of host-based and NDP operations, which require fine-grained interactions especially when the NDP device can also update (modify) the DBMS data autonomously. This paper introduces CINDA, the first full-stack computational storage capable of accelerating both read and update (write) database transactions using NDP. The proposed system relies on a hybrid host-device interface to enable the DBMS accessing persisted data, offloading computation to the storage device, and coordinating concurrent device-update operations with the host-update ones. A hybrid interface utilizes a cache-coherent interconnect such as CCIX or CXL for low-latency synchronization using a shared-lock table, and PCIe DMA for high-throughput bulk I/O. We evaluated the effectiveness of the proposed approach in a CCIX-based system by realizing an FPGA-based NDP-capable computational storage device and customizing an NDP-capable DBMS based on PostgreSQL to support update NDP operations. Our full-stack evaluation using the YCSB benchmark demonstrates that CINDA can deliver $\approx$4.2$\times$ end-to-end speedup when executing long-running update transactions directly on the storage device, while the host DBMS performs frequent short updates.},
  archive      = {J_TC},
  author       = {Sajjad Tamimi and Arthur Bernhardt and Florian Stock and Ilia Petrov and Andreas Koch},
  doi          = {10.1109/TC.2025.3558028},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2238-2252},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CINDA: Using cache-coherent interconnects for accelerating databases by enabling near-data processing of update transactions},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating loss recovery for content delivery network. <em>TC</em>, <em>74</em>(7), 2223-2237. (<a href='https://doi.org/10.1109/TC.2025.3558020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Packet losses significantly impact the user experience of content delivery network (CDN) services such as live streaming and data backup-and-archiving. However, our production network measurement studies show that the legacy loss recovery is far from satisfactory due to the wide-area loss characteristics (i.e., dynamics and burstiness) in the wild. In this paper, we propose a sender-side Adaptive ReTransmission scheme, ART, which minimizes the recovery time of lost packets with minimal redundancy cost. Distinguishing itself from forward-error-correction (FEC), which preemptively sends redundant data packets to prevent loss, ART functions as an automatic-repeat-request (ARQ) scheme. It applies redundancy specifically to lost packets instead of unlost packets, thereby addressing the characteristic patterns of wide-area losses in real-world scenarios. We implement ART upon QUIC protocol and evaluate it via both trace-driven emulation and real-world deployment. The results show that ART reduces up to 34% of flow completion time (FCT) for delay-sensitive transmissions, improves up to 26% of goodput for throughput-intensive transmissions, reduces 11.6% video playback rebuffering, and saves up to 90% of redundancy cost.},
  archive      = {J_TC},
  author       = {Tong Li and Wei Liu and Xinyu Ma and Shuaipeng Zhu and Jingkun Cao and Duling Xu and Zhaoqi Yang and Senzhen Liu and Taotao Zhang and Yinfeng Zhu and Bo Wu and Kezhi Wang and Ke Xu},
  doi          = {10.1109/TC.2025.3558020},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2223-2237},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accelerating loss recovery for content delivery network},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IOPS: A unified SpMM accelerator based on inner-outer-hybrid product. <em>TC</em>, <em>74</em>(7), 2210-2222. (<a href='https://doi.org/10.1109/TC.2025.3558013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix multiplication (SpMM) is widely applied to numerous domains, such as graph processing and machine learning. However, inner product (IP) induces redundant zero-element computing for mismatched nonzero operands, while outer product (OP) lacks input reuse across Process Elements (PEs). Besides, current accelerators only focus on sparse-sparse matrix multiplication (SSMM) or sparse-dense matrix multiplication (SDMM), rarely performing efficiently for both. To compensate for the shortcomings of IP and OP, we propose an inner-outer-hybrid product (IOHP) method, which reuses the input matrix among PEs with IP and removes zero-element calculations with OP in each PE. Based on IOHP, we co-design a accelerator with a unified computing flow, called IOPS, to efficiently process both SSMM and SDMM. It divides the SpMM into three stages: encoding, partial sum (psum) calculation, and address mapping, where the input matrices can be reused among PEs after encoding (IP) and the zero element can be skipped in the latter two stages (OP). Furthermore, an adaptive partition strategy is proposed to tile the input matrices based on their sparsity ratios, effectively utilizing the on-chip storage and reducing DRAM access. Compared with SpArch, we achieve $1.2\boldsymbol{\times}$~$4.3\boldsymbol{\times}$ performance and $1.3\boldsymbol{\times}$~$4.8\boldsymbol{\times}$ energy efficiency, with $1.4\boldsymbol{\times}$~$2.1\boldsymbol{\times}$ DRAM access saving.},
  archive      = {J_TC},
  author       = {Wenhao Sun and Wendi Sun and Song Chen and Yi Kang},
  doi          = {10.1109/TC.2025.3558013},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2210-2222},
  shortjournal = {IEEE Trans. Comput.},
  title        = {IOPS: A unified SpMM accelerator based on inner-outer-hybrid product},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TightLLM: Maximizing throughput for LLM inference via adaptive offloading policy. <em>TC</em>, <em>74</em>(7), 2195-2209. (<a href='https://doi.org/10.1109/TC.2025.3558009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, largely due to their substantial model size. However, this also results in significant GPU memory demands during inference. To address these challenges on hardware with limited GPU memory, existing approaches employ offloading techniques that offload unused tensors to CPU memory, thereby reducing GPU memory usage. Since offloading involves data transfer between GPU and CPU, it introduces transfer overhead. To mitigate this, prior works typically overlap data transfer with GPU computation using a fixed pipelining strategy applied uniformly across all inference iterations, referred to as static offloading. However, static offloading policies fail to maximize inference throughput because they cannot adapt to the dynamically changing transfer overhead during the inference process, leading to increasing GPU idleness and reduced inference throughput. We propose that offloading policies should be adaptive to the varying transfer overhead across inference iterations to maximize inference throughput. To this end, we design and implement an adaptive offloading-based inference system called TightLLM with two key innovations. First, its key-value (KV) distributor employs a trade-compute-for-transfer strategy to address growing transfer overhead by dynamically recomputing portions of the KV cache, effectively overlapping data transfer with computation and minimizing GPU idleness. Second, TightLLM's weight loader slices model weights and distributes the loading process across multiple batches, amortizing the excessive weight loading overhead and significantly improving throughput. Evaluation across various combinations of GPU hardware and LLM models shows that TightLLM achieves 1.3 to 23 times higher throughput during the decoding phase and 1.2 to 22 times higher throughput in the prefill phase compared to state-of-the-art offloading systems. Due to the higher throughput in prefill and decoding phases, TightLLM can reduce the completion time for large-scale tasks, which involve processing and generating a substantial number of tokens, by 59.6% to 94.9%.},
  archive      = {J_TC},
  author       = {Yitao Hu and Xiulong Liu and Guotao Yang and Linxuan Li and Kai Zeng and Zhixin Zhao and Sheng Chen and Laiping Zhao and Wenxin Li and Keqiu Li},
  doi          = {10.1109/TC.2025.3558009},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2195-2209},
  shortjournal = {IEEE Trans. Comput.},
  title        = {TightLLM: Maximizing throughput for LLM inference via adaptive offloading policy},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCAS-BMT: Dynamic construction and adjustment of skewed bonsai merkle tree for performance enhancement in secure non-volatile memory. <em>TC</em>, <em>74</em>(7), 2183-2194. (<a href='https://doi.org/10.1109/TC.2025.3558007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional DRAM-based memory solutions face challenges, including high energy consumption and limited scalability. Non-Volatile Memory (NVM) offers low energy consumption and high scalability. However, security challenges, particularly data remanence vulnerabilities, persist. Prevalent methods such as the Bonsai Merkle Tree (BMT) are employed to ensure data security. However, the consistency requirements for integrity tree updates have led to performance issues. It is observed that compared to a secure NVM system without persistent secure metadata, the average overhead for updating and persisting the BMT root with persistent secure metadata is as high as 2.48 times. Therefore, this paper aims to mitigate these inefficiencies by leveraging the principle of memory access locality. We propose the Dynamic Construction and Adjustment of Skewed Bonsai Merkle Tree (DCAS-BMT). The DCAS-BMT is dynamically built and continuously adjusted at runtime according to access weights, ensuring frequently accessed memory blocks reside on shorter paths to the root node. This reduces the verification steps for frequently accessed memory blocks, thereby lowering the overall cost of memory authentication and updates. Experimental results using the USIMM memory simulator demonstrate that compared to the widely used BMT approach, the DCAS-BMT scheme shows a performance improvement of 34.1%.},
  archive      = {J_TC},
  author       = {Yu Zhang and Renhai Chen and Hangyu Yan and Hongyue Wu and Zhiyong Feng},
  doi          = {10.1109/TC.2025.3558007},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2183-2194},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DCAS-BMT: Dynamic construction and adjustment of skewed bonsai merkle tree for performance enhancement in secure non-volatile memory},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HiCoCS: High concurrency cross-sharding on permissioned blockchains. <em>TC</em>, <em>74</em>(7), 2168-2182. (<a href='https://doi.org/10.1109/TC.2025.3558001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the foundation of the Web3 trust system, blockchain technology faces increasing demands for scalability. Sharding emerges as a promising solution, but it struggles to handle highly concurrent cross-shard transactions (CSTxs), primarily due to simultaneous ledger operations on the same account. Hyperledger Fabric, a permissioned blockchain, employs multi-version concurrency control for parallel processing. Existing solutions use channels and intermediaries to achieve cross-sharding in Hyperledger Fabric. However, the conflict problem caused by highly concurrent CSTxs has not been adequately resolved. To fill this gap, we propose HiCoCS, a high concurrency cross-shard scheme for permissioned blockchains. HiCoCS creates a unique virtual sub-broker for each CSTx by introducing a composite key structure, enabling conflict-free concurrent transaction processing while reducing resource overhead. The challenge lies in managing large numbers of composite keys and mitigating intermediary privacy risks. HiCoCS utilizes virtual sub-brokers to receive and process CSTxs concurrently while maintaining a transaction pool. Batch processing is employed to merge multiple CSTxs in the pool, improving efficiency. We explore composite key reuse to reduce the number of virtual sub-brokers and lower system overhead. Privacy preservation is enhanced using homomorphic encryption. Evaluations show that HiCoCS improves cross-shard transaction throughput by 3.5-20.2 times compared to the baselines.},
  archive      = {J_TC},
  author       = {Lingxiao Yang and Xuewen Dong and Zhiguo Wan and Di Lu and Yushu Zhang and Yulong Shen},
  doi          = {10.1109/TC.2025.3558001},
  journal      = {IEEE Transactions on Computers},
  month        = {7},
  number       = {7},
  pages        = {2168-2182},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HiCoCS: High concurrency cross-sharding on permissioned blockchains},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncover secrets through the cover: A deep learning-based side-channel attack against kyber implementations with anti-tampering covers. <em>TC</em>, <em>74</em>(6), 2159-2167. (<a href='https://doi.org/10.1109/TC.2025.3547610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The probe can directly contact the microcontroller in a typical EM side-channel attack (SCA) targeting cryptographic implementations. However, in a more practical setting such as security level 2 of FIPS 140-3 or ISO/IEC 19790 standards, the microcontroller is required to be safeguarded by an opaque anti-tampering cover. This raises an interesting problem: Can we still launch EM attacks against microcontrollers running cryptographic implementations even when equipped with the cover? This paper proposes an improved deep-learning-based profiled attack against NIST KEM standard Kyber. Our key observation is that the distance between the probe and the microcontroller results in attenuation of signal strength. Moreover, the cover restricts the proximity of the probe, thereby limiting the signal-to-noise ratio. We propose an Adaptive Slimmed Pyramid Network (ASPN) model to instantiate a distinguisher in a plaintext-checking oracle-based SCA, which is generic and easy to implement. The proposed ASPN approach significantly enhances the feature extraction process by employing a pyramid network structure, while simultaneously avoiding the inclusion of excessive parameters. Real-world experiments demonstrate that our proposed distinguishers achieve an accuracy above $99\%$ with an $18$ mm cover and higher than $89\%$ accuracy even with a $24$ mm cover.},
  archive      = {J_TC},
  author       = {Peng Chen and Jinnuo Li and Wei Cheng and Chi Cheng},
  doi          = {10.1109/TC.2025.3547610},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2159-2167},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Uncover secrets through the cover: A deep learning-based side-channel attack against kyber implementations with anti-tampering covers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SmartZone: Runtime support for secure and efficient on-device inference on ARM TrustZone. <em>TC</em>, <em>74</em>(6), 2144-2158. (<a href='https://doi.org/10.1109/TC.2025.3557971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {On-device inference is a burgeoning paradigm that performs model inference locally on end devices, allowing private data to remain local. ARM TrustZone as a widely supported trusted execution environment has been applied to provide confidentiality protection for on-device inference. However, with the rise of large-scale models like large language models (LLMs), TrustZone-based on-device inference faces challenges in migration difficulties and inefficient execution. The rudimentary TEE OS on TrustZone lacks both the inference runtime needed for building models and the parallel support necessary to accelerate inference. Moreover, the limited secure memory resources on end devices further constrain the model size and degrade performance. In this paper, we propose SmartZone to provide runtime support for secure and efficient on-device inference on TrustZone. SmartZone consists three main components: (1) a trusted inference-oriented operator set, providing the underlying mechanisms adapted to the TrustZone's execution mode for trusted inference of DNN models and LLMs. (2) the proactive multi-threading parallel support, which increases the number of CPU cores in the secure state via cross-world thread collaboration to achieve parallelism, and (3) the on-demand secure memory management method, which statically allocates the appropriate secure memory size based on pre-execution resource analysis. We implement a prototype of SmartZone on the Raspberry Pi 3B+ board and evaluate it on four well-known DNN models and llama2 LLM. Extensive experimental results show that SmartZone provides end-to-end protection for on-device inference while maintaining excellent performance. Compared to the origin trusted inference, SmartZone accelerates the inference speed by up to $4.26\boldsymbol{\times}$ and reduces energy consumption by $65.81\%$.},
  archive      = {J_TC},
  author       = {Zhaolong Jian and Xu Liu and Qiankun Dong and Longkai Cheng and Xueshuo Xie and Tao Li},
  doi          = {10.1109/TC.2025.3557971},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2144-2158},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SmartZone: Runtime support for secure and efficient on-device inference on ARM TrustZone},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient quantum secure vector dominance and its applications in computational geometry. <em>TC</em>, <em>74</em>(6), 2129-2143. (<a href='https://doi.org/10.1109/TC.2025.3557968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure vector dominance is a key cryptographic primitive in secure computational geometry (SCG), determining the dominance relationship of vectors between two participants without revealing their private information. However, the security of traditional SVD protocols is compromised by the formidable computational power of quantum computing, and their efficiency needs further improvement. To address these challenges, an efficient quantum secure vector dominance (QSVD) protocol is proposed. Specifically, we first introduce a quantum private permutation (QPP) subprotocol to shuffle the elements of each participant's private input vector. To further facilitate secure data comparison, we propose an enhanced quantum millionaire subprotocol with equality determination functionality, building upon Jia's original protocol. Based on the above two subprotocols, we propose a QSVD protocol with polynomial complexity, deriving vector dominance in a single interaction with a semi-honest third party. Performance analyses confirm that QSVD protocol is correct, resilient against malicious attacks, and retains polynomial computational complexity, ensuring both security and efficiency. To demonstrate the scalability of the QSVD protocol, we illustrate its applications in several geometric computation problems, such as point-line inclusion determination, line-line intersect determination, and point-in-polygon determination. Finally, we validate the feasibility of our protocol by conducting comprehensive simulations on IBM's Qiskit platform, demonstrating its practical applicability and effectiveness in real quantum computing environments.},
  archive      = {J_TC},
  author       = {Wenjie Liu and Bingmei Su and Feiyang Sun},
  doi          = {10.1109/TC.2025.3557968},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2129-2143},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient quantum secure vector dominance and its applications in computational geometry},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RSQC: Recursive sparse QUBO construction for quantum annealing machines. <em>TC</em>, <em>74</em>(6), 2114-2128. (<a href='https://doi.org/10.1109/TC.2025.3557965'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum annealing algorithms have shown commercial potential in solving some instances of combinatorial optimization problems. However, existing mapping for general optimization problems into a compatible format for quantum annealing yields dense topology and complicated weighting, which limits the size of solvable problems on practical quantum annealing machines. To address this issue, we propose a novel mapping framework with three new techniques. First, to address the issue from general constraints, we introduce a recursive methodology to map constraints into interconnected Boolean gates and small algebraic cliques, which yields sparse topology and hardware-friendly biases/interactions. Second, to better address frequently-used constraints, we introduce a specialized penalty set based on this methodology with detailed optimizations. Third, to address the issue from the objective, we reformulate the complicated objective into a single multi-bit variable and apply binary search to its range, which turns each search step into a constraint-only problem. Compared with the state-of-the-art, experimental results and analysis over an exhaustive scan for operand bit-widths from 1 to 64 show that: (1) the growth order of the number of physical qubits with regard to operand bit-widths is reduced from $O(w^{2})$ to $O(w)$, while the number is reduced by a factor of $10^{-1}$ in the best case; (2) the dynamic range of biases/interactions is reduced from $O(2^{2w})$ to $ \lt 32$; (3) the graph minor embedding run time is reduced by a factor of $10^{-2}$ in the best case. For the same optimization problem, our framework reduces the requirement of the number of physical qubits and machine precision, and shortens the time from problem to machine.},
  archive      = {J_TC},
  author       = {Jianwen Luo and Yuhao Shu and Yajun Ha},
  doi          = {10.1109/TC.2025.3557965},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2114-2128},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RSQC: Recursive sparse QUBO construction for quantum annealing machines},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning operators performance tuning for changeable sized input data on tensor accelerate hardware. <em>TC</em>, <em>74</em>(6), 2101-2113. (<a href='https://doi.org/10.1109/TC.2025.3551937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The operator library is the fundamental infrastructure of deep learning acceleration hardware. Automatically generating the library and tuning its performance is promising because the manual development by well-trained and skillful programmers is costly in terms of both time and money. Tensor hardware has the best computing efficiency for deep learning applications, but the operator library programs are hard to tune because the tensor hardware primitives have many limitations. Otherwise, the performance is difficult to be fully explored. The recent advancement in LLM exacerbates this problem because the size of input data is not fixed. Therefore, mapping the computing tasks of operators to tensor hardware units is a significant challenge when the shape of the input tensor is unknown before the runtime. We propose DSAT, a deep learning operator performance autotuning technique for changeable-sized input data on tensor hardware. To match the input tensor's undetermined shape, we choose a group of abstract computing units as the basic building blocks of operators for changeable-sized input tensor shapes. We design a group of programming tuning rules to construct a large exploration space of the variant implementation of the operator programs. Based on these rules, we construct an intermediate representation of computing and memory access to describe the computing process and use it to map the abstract computing units to tensor primitives. To speed up the tuning process, we narrow down the optimization space by predicting the actual hardware resource requirement and providing an optimized cost model for performance prediction. DSAT achieves performance comparable to the vendor's manually tuned operator libraries. Compared to state-of-the-art deep learning compilers, it improves the performance of inference by 13% on average and decreases the tuning time by an order of magnitude.},
  archive      = {J_TC},
  author       = {Pengyu Mu and Yi Liu and Rui Wang and Guoxiang Liu and Hangcheng An and Qianhe Zhao and Hailong Yang and Chenhao Xie and Zhongzhi Luan and Chunye Gong and Depei Qian},
  doi          = {10.1109/TC.2025.3551937},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2101-2113},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Deep learning operators performance tuning for changeable sized input data on tensor accelerate hardware},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An FPGA-based open-source hardware-software framework for side-channel security research. <em>TC</em>, <em>74</em>(6), 2087-2100. (<a href='https://doi.org/10.1109/TC.2025.3551936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attacks based on side-channel analysis (SCA) pose a severe security threat to modern computing platforms, further exacerbated on IoT devices by their pervasiveness and handling of private and critical data. Designing SCA-resistant computing platforms requires a significant additional effort in the early stages of the IoT devices’ life cycle, which is severely constrained by strict time-to-market deadlines and tight budgets. This manuscript introduces a hardware-software framework meant for SCA research on FPGA targets. It delivers an IoT-class system-on-chip (SoC) that includes a RISC-V CPU, provides observability and controllability through an ad-hoc debug infrastructure to facilitate SCA attacks and evaluate the platform's security, and streamlines the deployment of SCA countermeasures through dedicated hardware and software features such as a DFS actuator and FreeRTOS support. The open-source release of the framework includes the SoC, the scripts to configure the computing platform, compile a target application, and assess the SCA security, as well as a suite of state-of-the-art attacks and countermeasures. The goal is to foster its adoption and novel developments in the field, empowering designers and researchers to focus on studying SCA countermeasures and attacks while relying on a sound and stable hardware-software platform as the foundation for their research.},
  archive      = {J_TC},
  author       = {Davide Zoni and Andrea Galimberti and Davide Galli},
  doi          = {10.1109/TC.2025.3551936},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2087-2100},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An FPGA-based open-source hardware-software framework for side-channel security research},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AR-light: Enabling fast and lightweight multi-user augmented reality via semantic segmentation and collaborative view synchronization. <em>TC</em>, <em>74</em>(6), 2073-2086. (<a href='https://doi.org/10.1109/TC.2025.3549629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-user Augmented Reality (MuAR) allows multiple users to interact with shared virtual objects, facilitated by exchanging environment information. Current MuAR systems rely on 3D point clouds for real-world analysis, view synchronization, object rendering, and movement tracking. However, the complexity of 3D point clouds leads to significant processing delays, with approximately 80% of overhead in commercial frameworks. This hampers usability and degrades user experience. Our analysis reveals that maintaining the facing side of the real-world scene in a stable environment provides sufficient information for virtual object placement and rendering. To address this, we introduce a lightweight quadtree structure, representing 2D scenes through semantic segmentation and geometry, as an alternative to 3D point clouds. Additionally, we propose a novel correction method to handle potential shifts in virtual object placement during view synchronization among users. Combining all designs, we implement a fast and lightweight MuAR framework named AR-Light and test our framework on commercial AR devices. The evaluation results on real-world applications demonstrate that AR-Light can achieve high performance in various real-world scenes while maintaining a comparable virtual object placement accuracy.},
  archive      = {J_TC},
  author       = {Yu Wen and Aamir Bader Shah and Ruizhi Cao and Chen Zhang and Jiefu Chen and Xuqing Wu and Chenhao Xie and Xin Fu},
  doi          = {10.1109/TC.2025.3549629},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2073-2086},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AR-light: Enabling fast and lightweight multi-user augmented reality via semantic segmentation and collaborative view synchronization},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DESA: Dataflow efficient systolic array for acceleration of transformers. <em>TC</em>, <em>74</em>(6), 2058-2072. (<a href='https://doi.org/10.1109/TC.2025.3549621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have become prevalent in various Artificial Intelligence (AI) applications, spanning natural language processing to computer vision. Owing to their suboptimal performance on general-purpose platforms, various domain-specific accelerators that explore and utilize the model sparsity have been developed. Instead, we conduct a quantitative analysis of Transformers. (Transformers can be categorized into three types: Encoder-Only, Decoder-Only, and Encoder-Decoder. This paper focuses on Encoder-Only Transformers.) to identify key inefficiencies and adopt dataflow optimization to address them. These inefficiencies arise from 1) diverse matrix multiplication, 2) multi-phase non-linear operations and their dependencies, and 3) heavy memory requirements. We introduce a novel dataflow design to support decoupling with latency hiding, effectively reducing the dependencies and addressing the performance bottlenecks of nonlinear operations. To enable fully fused attention computation, we propose practical tiling and mapping strategies to sustain high throughput and notably decrease memory requirements from $O(N^{2}H)$ to $O(N)$. A hybrid buffer-level reuse strategy is also introduced to enhance utilization and diminish off-chip access. Based on these optimizations, we propose a novel systolic array design, named DESA, with three innovations: 1) A reconfigurable vector processing unit (VPU) and immediate processing units (IPUs) that can be seamlessly fused within the systolic array to support various normalization, post-processing, and transposition operations with efficient latency hiding. 2) A hybrid stationary systolic array that improves the compute and memory efficiency for matrix multiplications with diverse operational intensity and characteristics. 3) A novel tile fusion processing that efficiently addresses the low utilization issue in the conventional systolic array during the data setup and offloading. Across various benchmarks, extensive experiments demonstrate that DESA archives $5.0\boldsymbol{\times\thicksim}8.3\boldsymbol{\times}$ energy saving over 3090 GPU and $25.6\boldsymbol{\times\thicksim}88.4\boldsymbol{\times}$ than Intel 6226R CPU. Compared to the SOTA designs, DESA achieves $11.6\boldsymbol{\times\thicksim}15.0\boldsymbol{\times}$ speedup and up to $2.3\times$ energy saving over the SOTA accelerators.},
  archive      = {J_TC},
  author       = {Zhican Wang and Hongxiang Fan and Guanghui He},
  doi          = {10.1109/TC.2025.3549621},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2058-2072},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DESA: Dataflow efficient systolic array for acceleration of transformers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling consistent sensing data sharing among IoT edge servers via lightweight consensus. <em>TC</em>, <em>74</em>(6), 2045-2057. (<a href='https://doi.org/10.1109/TC.2025.3549616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain offers distinct advantages in terms of data credibility and provenance certification, and its fusion with Internet of Things (IoT) technology holds great promise. Nevertheless, IoT environments are marked by extensive node networks and intricate communication patterns, especially the sensing environment. The conventional blockchain consensus mechanism, hampered by its heavy reliance on computing resources and communication bandwidth, faces difficulties in ensuring seamless data exchange among IoT edge servers. The issues encountered by state-of-the-art Byzantine Fault Tolerance (BFT) consensus include: (i) high communication complexity between nodes; and (ii) the detrimental impact of Byzantine behavior on system performance. To overcome the above problems, we propose the lightweight blockchain consensus called AntB, firstly introducing the concept of sampling into the consensus and significantly reducing the number of participating consensus nodes from $N$ to $n$, which lowers the consensus complexity to $\mathbf{2\cdot O(n)+O(N)}$. We design a dynamic reputation mechanism so that Byzantine nodes cannot control the sampling set to affect the activity of the consensus in the long term. When implementing AntB, we address three significant technical challenges: (i) to determine the optimal sample size, we propose a sampling calculation method based on statistical confidence intervals, where the sample size is primarily determined by the chosen confidence level and margin of error; (ii) to prevent Byzantine behavior, we devise a weighted random sampling mechanism utilizing reputation coefficients based on edge servers’ behaviors; and (iii) to maintain consensus activity and consistency after sampling, we propose the consensus mechanism for partial sampling and global verification to avert potential issues. We implement AntB and conduct performance evaluations in a server with 32 cores and 64GB of memory. The evaluation results indicate that, the more nodes participating in the process of consensus, the better the performance of AntB will be. Especially, compared to HotStuff, AntB has a 24.94% higher success rate and Transactions Per Second (TPS) can improve by 102.10% when the number of nodes is 300.},
  archive      = {J_TC},
  author       = {Xiulong Liu and Zhiyuan Zheng and Hao Xu and Zhelin Liang and Gaowei Shi and Chenyu Zhang and Keqiu Li},
  doi          = {10.1109/TC.2025.3549616},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2045-2057},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling consistent sensing data sharing among IoT edge servers via lightweight consensus},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tangram: Enabling efficient and balanced dynamic storage extension on sharding blockchain systems. <em>TC</em>, <em>74</em>(6), 2031-2044. (<a href='https://doi.org/10.1109/TC.2025.3547622'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, sharding technology has been frequently applied in blockchain systems to increase scalability. However, when new shards are added, the system may result in significant overhead in terms of computing and networking since the data allocation approach is incompatible with dynamic changes in shards. Currently, S-Store, the state-of-the-art sharding solution built on the account model, has a high re-computing latency when growing shard numbers and an unbalanced sharded data distribution after growth. To address these issues, this paper presents Tangram, an efficient and balanced dynamic storage extension approach for sharding blockchain systems. Tangram reduces system extension overhead and latency while ensuring a balanced shard distribution. In implementing Tangram, we tackle three main technical challenges as follows. (1) Designing a novel state tree structure for the storage and maintenance of sharding state data. We introduce the Jump Merkle Tree (JMT) based on the Merkle Tree, which integrates node migration and orderliness. (2) Presenting a protocol to be compatible with dynamic shard scenarios. We devise a shard addition protocol to improve system extension availability and decrease shard extension delay. (3) Proposing an approach to guarantee system longevity after extension. We first devise algorithms for the state tree to eradicate invalid states after system expansion. Furthermore, we introduce a shard reduction protocol to enhance system storage extension support in complex scenarios, such as cleaning up inactive states to avoid bloating the state tree. We conduct extensive experiments to evaluate the performance of Tangram. Experiment results demonstrate that Tangram outperforms existing solutions, showing reduced latency and superior data balance. When compared to the state-of-the-art sharding storage solution, Tangram decreases the transaction execute time by up to 87.84%, the state data migration by more than approximately 74%, and achieves up to 7.63x improvement in the standard deviation of sharding data balance.},
  archive      = {J_TC},
  author       = {Hao Xu and Jiaqi Zhang and Xiulong Liu and Zhimin Yu and Tingyu Fan and Baochao Chen and Keqiu Li},
  doi          = {10.1109/TC.2025.3547622},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2031-2044},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Tangram: Enabling efficient and balanced dynamic storage extension on sharding blockchain systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eliminate data divergence in SpMV via processor and memory co-computing framework. <em>TC</em>, <em>74</em>(6), 2017-2030. (<a href='https://doi.org/10.1109/TC.2025.3547162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix-vector multiplication (SpMV) is a performance-critical kernel in various application domains, including high-performance computing, artificial intelligence, and big data. However, the performance of SpMV on SIMD devices is greatly affected by data divergences. To address this issue, we propose an In-SRAM Computing-based Processor Memory Co-Compute SpMV optimization framework that divides the SpMV kernel into two stages: a compute-intensive stage and a control-intensive stage. For optimizing the first stage, we leverage the parallel random access feature of multi-bank SRAM to eliminate overheads caused by memory divergences and use the Aggregate Table (AT) to reduce bank conflicts. For optimizing the second stage, we convert control divergences into memory divergences and utilize the Accumulate ScratchPad Memory (AccSPM) for executing reduction operations while eliminating overheads caused by memory divergences. Experimental results demonstrate that our solution achieves significant throughput increase over highly optimized vector SpMV kernels under CSR, CSR5, and CVR compression formats with performance speedups up to 4.74x, 5.58x, and 4.83x (3.11x, 3.04x, and 3.07x on average), respectively.},
  archive      = {J_TC},
  author       = {Zhang Dunbo and Shen Li and Lu Kai},
  doi          = {10.1109/TC.2025.3547162},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2017-2030},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Eliminate data divergence in SpMV via processor and memory co-computing framework},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable communication scheme based on completely independent spanning trees in data center networks. <em>TC</em>, <em>74</em>(6), 2003-2016. (<a href='https://doi.org/10.1109/TC.2025.3547161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With technological advancements, real-time applications have permeated various aspects of human life, relying on fast, reliable, and low-latency data transmission for seamless user experiences. The development of data center networks (DCNs) has greatly advanced real-time applications, with network reliability being a key factor in ensuring high-quality network services. As a switch-centric DCN, DPCell has good scalability and the ability to achieve load balancing at different traffic levels. With the increasing demand for high availability, fault tolerance, and efficient data transmission, highly reliable communication for DPCell is essential. Completely independent spanning trees (CISTs) play a significant role in enhancing reliable communication performance in networks. This paper proposes an algorithm for constructing CISTs in DPCell, which has relatively low time and space consumption compared to other CISTs construction algorithms in DCNs, offering an efficiency advantage. Communication simulations validate the effectiveness of using paths provided by CISTs in DPCell for data transmission. Furthermore, experimental results show that a multi-protection routing scheme configured with multiple CISTs significantly enhances fault tolerance in DPCell.},
  archive      = {J_TC},
  author       = {Hui Dong and Huaqun Wang and Mengjie Lv and Weibei Fan},
  doi          = {10.1109/TC.2025.3547161},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {2003-2016},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reliable communication scheme based on completely independent spanning trees in data center networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing serverless performance through game theory and efficient resource scheduling. <em>TC</em>, <em>74</em>(6), 1990-2002. (<a href='https://doi.org/10.1109/TC.2025.3547158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scaler and scheduler of serverless system are the two cornerstones that ensure service quality and efficiency. However, existing scalers and schedulers are constrained by static thresholds, scaling latency, and single-dimensional optimization, making them difficult to agilely respond to dynamic workloads of functions with different characteristics. This paper proposes a game theory-based scaler and a dual-layer optimization scheduler to enhance the resource management and task allocation capabilities of serverless systems. In the scaler, we introduce the Hawkes process to quantify the “temperature” of function as an indicator of their instantaneous invocation rate. By combining dynamic thresholds and continuous monitoring, this scaler enables that scaling operations no longer lag behind changes of function instances and can even warm up beforehand. For scheduler, we refer to bin-packing strategies to optimize the distribution of containers and reduce resource fragmentation. A new concept of “CPU starvation degree” is introduced to denote the degree of CPU contention during function execution, ensuring that function requests are efficiently scheduled. Experimental analysis on ServerlessBench and Alibaba clusterdata indicates that compared to classical and state-of-the-art scalers and schedulers, the proposed scaler and scheduler achieve at least a 149% improvement in the Quality-Price Ratio, which represents the trade-off between performance and cost.},
  archive      = {J_TC},
  author       = {Pengwei Wang and Yi Li and Chao Fang and Yichen Zhong and Zhijun Ding},
  doi          = {10.1109/TC.2025.3547158},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1990-2002},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing serverless performance through game theory and efficient resource scheduling},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based cloud security: Innovative attack detection and privacy focused key management. <em>TC</em>, <em>74</em>(6), 1978-1989. (<a href='https://doi.org/10.1109/TC.2025.3547150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud Computing (CC) is widely adopted in sectors like education, healthcare, and banking due to its scalability and cost-effectiveness. However, its internet-based nature exposes it to cyber threats, necessitating advanced security frameworks. Traditional models suffer from high false positives and limited adaptability. To address these challenges, VECGLSTM, an attack detection model integrating Variable Long Short-Term Memory (VLSTM), capsule networks, and the Enhanced Gannet Optimization Algorithm (EGOA), is introduced. This hybrid approach enhances accuracy, reduces false positives, and dynamically adapts to evolving threats. EGOA is employed for its superior optimization capability, ensuring faster convergence and resilience. Additionally, Chaotic Cryptographic Pelican Tunicate Swarm Optimization (CCPTSO) is proposed for privacy-preserving key management. This model combines chaotic cryptographic techniques with the Pelican Tunicate Swarm Optimization Algorithm (PTSOA), leveraging the pelican algorithm’s exploration strength and the tunicate swarm’s exploitation ability for optimal encryption security. Performance evaluation demonstrates 99.675% accuracy, 99.5175% recall, 99.7075% precision, and 99.615% F1-score, along with reduced training (1.79s), encryption (0.986s), and decryption (1.029s) times. This research significantly enhances CC security by providing a scalable, adaptive framework that effectively counters evolving cyber threats while ensuring efficient key management.},
  archive      = {J_TC},
  author       = {Shahnawaz Ahmad and Mohd Arif and Shabana Mehfuz and Javed Ahmad and Mohd Nazim},
  doi          = {10.1109/TC.2025.3547150},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1978-1989},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Deep learning-based cloud security: Innovative attack detection and privacy focused key management},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring hyperdimensional computing robustness against hardware errors. <em>TC</em>, <em>74</em>(6), 1963-1977. (<a href='https://doi.org/10.1109/TC.2025.3547142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-inspired hyperdimensional computing (HDC) is an emerging machine learning paradigm leveraging high-dimensional spaces for efficient tasks like pattern recognition and medical diagnostics. As a lightweight alternative to deep neural networks, HDC offers smaller model sizes, reduced computation, and memory-centric processing. However, deploying HDC in safety-critical applications, such as healthcare and robotics, is challenged by hardware-induced errors. This paper investigates HDC's robustness to memory errors via extensive bit-flip injection experiments on item and associative memories. Results reveal that certain bit-flips severely degrade accuracy. To address this, we introduce the Hyperdimensional Bit-Flip Search (HD-BFS), a similarity-guided method for identifying vulnerabilities and crafting efficient attacks, where flipping just 6 critical bits—3.9% of random bit-flips—reduces accuracy to chance levels. We further propose Hyperdimensional Accelerated Bit-Flip Search (HD-ABFS), which narrows the search space by targeting critical dimensions and most significant bits (MSBs), achieving up to 282$\times$ speedup over HD-BFS. Finally, we develop an effective protection mechanism to enhance model safety. These insights highlight HDC's resilience to random errors, offer robust defenses against targeted attacks, and advance the security and reliability of HDC systems.},
  archive      = {J_TC},
  author       = {Sizhe Zhang and Kyle Juretus and Xun Jiao},
  doi          = {10.1109/TC.2025.3547142},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1963-1977},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploring hyperdimensional computing robustness against hardware errors},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RTSA: A run-through sparse attention framework for video transformer. <em>TC</em>, <em>74</em>(6), 1949-1962. (<a href='https://doi.org/10.1109/TC.2025.3547139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of video understanding tasks, Video Transformer models (VidT) have recently exhibited impressive accuracy improvements in numerous edge devices. However, their deployment poses significant computational challenges for hardware. To address this, pruning has emerged as a promising approach to reduce computation and memory requirements by eliminating unimportant elements from the attention matrix. Unfortunately, existing pruning algorithms face a limitation in that they only optimize one of the two key modules on VidT's critical path: linear projection or self-attention. Regrettably, due to the variation in battery power in edge devices, the video resolution they generate will also change, which causes both linear projection and self-attention stages to potentially become bottlenecks, the existing approaches lack generality. Accordingly, we establish a Run-Through Sparse Attention (RTSA) framework that simultaneously sparsifies and accelerates two stages. On the algorithm side, unlike current methodologies conducting sparse linear projection by exploring redundancy within each frame, we extract extra redundancy naturally existing between frames. Moreover, for sparse self-attention, as existing pruning algorithms often provide either too coarse-grained or fine-grained sparsity patterns, these algorithms face limitations in simultaneously achieving high sparsity, low accuracy loss, and high speedup, resulting in either compromised accuracy or reduced efficiency. Thus, we prune the attention matrix at a medium granularity—sub-vector. The sub-vectors are generated by isolating each column of the attention matrix. On the hardware side, we observe that the use of distinct computational units for sparse linear projection and self-attention results in pipeline imbalances because of the bottleneck transformation between the two stages. To effectively eliminate pipeline stall, we design a RTSA architecture that supports sequential execution of both sparse linear projection and self-attention. To achieve this, we devised an atomic vector-scalar product computation underpinning all calculations in parse linear projection and self-attention, as well as evolving a spatial array architecture with augmented processing elements (PEs) tailored for the vector-scalar product. Experiments on VidT models show that RTSA can save 2.71$\boldsymbol{\times}$ to 5.32$\boldsymbol{\times}$ ideal computation with $ \lt 1\%$ accuracy loss, achieving 105$\boldsymbol{\times}$, 56.8$\boldsymbol{\times}$, 3.59$\boldsymbol{\times}$, and 3.31$\boldsymbol{\times}$ speedup compared to CPU, GPU, as well as the state-of-the-art ViT accelerators ViTCoD and HeatViT.},
  archive      = {J_TC},
  author       = {Xuhang Wang and Zhuoran Song and Chunyu Qi and Fangxin Liu and Naifeng Jing and Li Jiang and Xiaoyao Liang},
  doi          = {10.1109/TC.2025.3547139},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1949-1962},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RTSA: A run-through sparse attention framework for video transformer},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PFed-NS: An adaptive personalized federated learning scheme through neural network segmentation. <em>TC</em>, <em>74</em>(6), 1936-1948. (<a href='https://doi.org/10.1109/TC.2025.3547138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is typically deployed in a client-server architecture, which makes the Edge-Cloud architecture an ideal backbone for FL. A significant challenge in this setup arises from the diverse data feature distributions across different edge locations (i.e., non-IID data). In response, Personalized Federated Learning (PFL) approaches have been developed. Network segmentation-based PFL is an important approach to achieving PFL, in which the training network is divided into a global segment for server aggregation and a local segment maintained client-side. Existing methods determine the segmentation before the training, and the segmentation remains fixed throughout the PFL training. However, our investigation reveals that model representations vary as PFL progresses and the fixed segmentation may not deliver best performance across various training settings. To address this, we propose PFed-NS, a PFL framework based on adaptive network segmentation. This adaptive segmentation technique is composed of two elements: a mechanism for assessing divergence of clients’ probability density functions constructed from network layers’ outputs, and a model for dynamically establishing divergence thresholds, beyond which server aggregation is deemed detrimental. Further optimization strategies are proposed to reduce the computation and communication costs incurred by divergence modeling. Moreover, we propose a divergence-based BN strategy to optimize BN performance for network segmentation-based PFL. Extensive experiments have been conducted to compare PFed-NS against recent PFL models. The results demonstrate its superiority in enhancing model accuracy and accelerating convergence.},
  archive      = {J_TC},
  author       = {Yuchen Liu and Ligang He and Zhigao Zhang and Shenyuan Ren},
  doi          = {10.1109/TC.2025.3547138},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1936-1948},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PFed-NS: An adaptive personalized federated learning scheme through neural network segmentation},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoSpMV: Towards agile software and hardware co-design for SpMV computation. <em>TC</em>, <em>74</em>(6), 1921-1935. (<a href='https://doi.org/10.1109/TC.2025.3547136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Matrix-Vector multiplication (SpMV) is a widely used kernel in scientific or engineering applications and it is commonly implemented in FPGAs for acceleration. Existing works on FPGA usually pre-process the sparse matrix for data compression from the software perspective, and then design a unified architecture from the hardware perspective. However, as different SpMV kernels expose different levels of data parallelism after software processing, a unified architecture may not efficiently tap the underlying parallelism exposed in a specific kernel, leading to poor bandwidth utilization (BU) or poor resource utilization. To this end, this paper proposes an agile software and hardware co-design framework, CoSpMV, that employs design space exploration on both software and hardware for a specific kernel. Specifically, by providing a scalable compressed data format and a highly pipelined hardware template, CoSpMV can select the most suitable software and hardware configurations for different kernels and generate the accelerator instantly. The experimental results show that CoSpMV can achieve 3.91$\times$ speedup on GFLOPs, and 1.31$\times$ speedup on BU compared to the state-of-the-art work.},
  archive      = {J_TC},
  author       = {Minghao Tian and Yue Liang and Bowen Liu and Dajiang Liu},
  doi          = {10.1109/TC.2025.3547136},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1921-1935},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CoSpMV: Towards agile software and hardware co-design for SpMV computation},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Redactable blockchain from decentralized chameleon hash functions, revisited. <em>TC</em>, <em>74</em>(6), 1911-1920. (<a href='https://doi.org/10.1109/TC.2025.3544878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, redactable blockchains have attracted attention owing to enabling the contents of blocks to be re-written. The existing redactable blockchain solutions can be classified as two categories, the centralized one and decentralized one. In centralized solutions, a single blockchain node possessing the trapdoor conducts redaction operations. However, they suffer from the issue of single point of failure. In decentralized solutions, redaction operations are performed by numerous blockchain nodes cooperatively. But there also exists the issue of inefficiency or requiring a trusted party in these solutions. Subsequently, Jia et al. proposed a redactable blockchain solution from a decentralized chameleon hash function (DCH) they designed, which supports the threshold redaction, traceability and consistency check. Nevertheless, after carefully analyzing their solution, we find that it fails to achieve the security they claimed by presenting a concrete attack. To resolve this security issue, we propose a novel chameleon hash function scheme that achieves strong collision-resistant security while maintaining simple and efficient as the building block. Based on it, we then present an improved DCH scheme with sufficient security, so that the redactable blockchain from it can resist the presented attack. Theoretical and experimental analyses demonstrate that improved DCH achieves efficiency comparable to DCH.},
  archive      = {J_TC},
  author       = {Cong Li and Qingni Shen and Zhonghai Wu},
  doi          = {10.1109/TC.2025.3544878},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1911-1920},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Redactable blockchain from decentralized chameleon hash functions, revisited},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards effective local search for qubit mapping. <em>TC</em>, <em>74</em>(6), 1897-1910. (<a href='https://doi.org/10.1109/TC.2025.3544869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of noisy intermediate-scale quantum (NISQ), a quantum logical circuit must undergo certain compilation before it can be used on a NISQ device, subject to connectivity constraints posed by NISQ devices. During compilation, numerous auxiliary quantum gates are inserted, but a circuit with too many is unreliable, necessitating gate minimization. This requirement gives rise to the qubit mapping problem (QMP), an NP-hard optimization problem that is critical in quantum computing. This work proposes a novel and effective local search algorithm dubbed EffectiveQM. First, EffectiveQM proposes a new mode-aware search strategy to alleviate the challenge of being trapped in local optima, where local search typically suffers. Moreover, EffectiveQM introduces a novel potential-guided scoring function, which can thoroughly quantify the actual benefit brought by an operation of inserting auxiliary gates. By incorporating the potential-guided scoring function, EffectiveQM can effectively determine the appropriate operation to be performed. Extensive experiments on a diverse collection of logical circuits and 6 NISQ devices demonstrate that EffectiveQM can generate physical circuits with significantly fewer inserted auxiliary gates than current state-of-the-art QMP algorithms, indicating that EffectiveQM greatly advances the state of the art in QMP solving.},
  archive      = {J_TC},
  author       = {Chuan Luo and Shenghua Cao and Shanyu Guo and Chunming Hu},
  doi          = {10.1109/TC.2025.3544869},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1897-1910},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards effective local search for qubit mapping},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A path-based topology-agnostic fault diagnosis strategy for multiprocessor systems. <em>TC</em>, <em>74</em>(6), 1886-1896. (<a href='https://doi.org/10.1109/TC.2025.3543701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis technology is a method for locating faulty processors in multiprocessor systems, and it plays a crucial role in ensuring system stability, security and reliability. A widely used approach in this technology is the system-level strategy, which determines processor status by interpreting the set of test results between adjacent processors. Among them, the PMC and MM models are two commonly employed methods for generating these results. The diversity and complexity of network topologies in systems constrain existing algorithms to specific topologies, while the limitations of fault diagnosis strategies lead to reduced fault tolerance. In this paper, we present a novel path-based method to tackle the fault diagnosis problems in various networks according to the PMC and MM models. Firstly, we introduce the algorithm for partitioning the path into subpaths based on these models. To ensure that at least one subpath is diagnosed as fault-free, we derive the relationship between the fault bound $T$ and the path length $N$. Then, building on methods for recognizing the subpath states, we have developed fault diagnosis algorithms for both the PMC and MM models. The simulation results show that our proposed algorithms can quickly and accurately diagnose faults in multiprocessor systems.},
  archive      = {J_TC},
  author       = {Lin Chen and Hao Feng and Jiong Wu},
  doi          = {10.1109/TC.2025.3543701},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1886-1896},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A path-based topology-agnostic fault diagnosis strategy for multiprocessor systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HashScape: Leveraging virtual address dynamics for efficient hashed page tables. <em>TC</em>, <em>74</em>(6), 1872-1885. (<a href='https://doi.org/10.1109/TC.2025.3543698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolving memory landscape for larger capacity prompts alternative approaches due to scalability challenges in multi-level page tables, which require multiple serial memory accesses for address translation. Hashed Page Tables (HPTs) have gained attention for ideally facilitating a single memory access per translation. However, current HPTs increase minor page fault latency, thereby impeding its superiority over conventional multi-level page table design. This paper provides a comprehensive analysis of HPTs regarding minor page fault latency concerning memory management subsystems. In particular, we demonstrate how feasibility issues in memory management with HPTs can escalate minor page fault latency. We observe that different page types in HPTs (anon pages and page caches) exhibit distinct behaviors on the occurrence of minor page faults, indicating a significant correlation between page types and minor page faults. To address these challenges, we propose HashScape, a scheme that harmonizes with memory management using tailored HPTs per segment and size-tailored allocation via Virtual Memory Areas. Our evaluation demonstrates that HashScape significantly improves the insertion latency, with average, 95th, and 99th percentiles improving by 1.8$\boldsymbol{\times}$, 1.9$\boldsymbol{\times}$, and 2.2$\boldsymbol{\times}$, respectively, resulting in an overall 10% reduction in minor page fault latency compared to a state-of-the-art HPT design.},
  archive      = {J_TC},
  author       = {Won Hur and Jiwon Lee and Jaewon Kwon and Minjae Kim and Won Woo Ro},
  doi          = {10.1109/TC.2025.3543698},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1872-1885},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HashScape: Leveraging virtual address dynamics for efficient hashed page tables},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ETBench: Characterizing hybrid vision transformer workloads across edge devices. <em>TC</em>, <em>74</em>(6), 1857-1871. (<a href='https://doi.org/10.1109/TC.2025.3543697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight Convolution and Vision Transformer hybrid models have increasingly dominated the frontiers of deep learning (DL) on edge devices; however, to the best of our knowledge, no prior work has provided comprehensive evaluation on hybrid models’ performance and analyzed their characteristics by diving deep into the edge ecosystem with diversified modern DL inference engines and heterogeneous hardware. This paper proposes a comprehensive open-source benchmark suite, ETBench, to allow power-efficiency, performance and accuracy assessment for state-of-the-art (SOTA) hybrid models across 11 most widely-used DL engines deployed on diverse edge devices. After building ETBench that satisfies 6 design requirements proposed in our work, we conduct extensive experiments on 14 devices including 19 CPUs, 11 GPUs and 5 NPUs, and obtain benchmark results from all deployment scenarios (combinations of models, quantization formats, software engines, and hardware platforms). Valuable observations and insightful implications are finally summarized. For example, within current DL engines, the INT8 quantization is significantly underperformed in terms of accuracy and speed against FP16 for hybrid models. Overall, ETBench serves as a collaborative platform that assists model architects in better evaluating their models and makes it possible for future co-optimizations of DL engines and hardware accelerators.},
  archive      = {J_TC},
  author       = {Yingkun Zhou and Zhengshuyuan Tian and Wenhao Yang and Tingting Zhang and Jinpeng Ye and Chenji Han and Tianyi Liu and Fuxin Zhang},
  doi          = {10.1109/TC.2025.3543697},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1857-1871},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ETBench: Characterizing hybrid vision transformer workloads across edge devices},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAFA: Handling sparse and scarce data in federated learning with accumulative learning. <em>TC</em>, <em>74</em>(6), 1844-1856. (<a href='https://doi.org/10.1109/TC.2025.3543682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as an effective paradigm allowing multiple parties to collaboratively train a global model while protecting their private data. However, it is observed that the performance of FL approaches tends to degrade significantly when data are sparsely distributed across clients with small datasets. This is referred to as the sparse-and-scarce challenge, where data held by each client is both sparse (does not contain examples to all classes) and scarce (small dataset). Sparse-and-scarce data diminishes the generalizability of clients’ data, leading to intensive over-fitting and massive domain shifts in the local models and, ultimately, decreasing the aggregated model's performance. Interestingly, while this scenario is a specific manifestation of the well-known non-IID11This refers to the generic situation where local data distributions are not identical and independently distributed. challenge in FL, it has not been distinctly addressed. Our empirical investigation highlights that generic approaches to the non-IID challenge often prove inadequate in mitigating the sparse-and-scarce issue. To bridge this gap, we develop SAFA, a novel FL algorithm that specifically addresses the sparse-and-scarce challenge via a novel continual model iteration procedure. SAFA maximally exposes local models to the inter-client diversity of data with minimal effects of catastrophic forgetting. Our experiments show that SAFA outperforms existing FL solutions, up to 17.86%, compared to the prominent baseline. The code is accessible via https://github.com/HungNguyen20/SAFA.},
  archive      = {J_TC},
  author       = {Nang Hung Nguyen and Truong Thao Nguyen and Trong Nghia Hoang and Hieu H. Pham and Thanh Hung Nguyen and Phi Le Nguyen},
  doi          = {10.1109/TC.2025.3543682},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1844-1856},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SAFA: Handling sparse and scarce data in federated learning with accumulative learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NDirect2: A high-performance library for direct convolutions on multicore CPUs. <em>TC</em>, <em>74</em>(6), 1829-1843. (<a href='https://doi.org/10.1109/TC.2025.3543677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution kernels are widely seen in high-performance computing (HPC) and deep learning (DL) workloads and are often responsible for performance bottlenecks. Prior works have demonstrated that the direct convolution approach can outperform the conventional convolution implementation. Although well-studied, the existing approaches for direct convolution are either incompatible with the mainstream DL data layouts or lead to suboptimal performance. We design nDirect2, a novel direct convolution approach that targets multi-core CPUs commonly found in smartphones and HPC systems. nDirect2 is compatible with the data layout formats used by mainstream DL frameworks and offers new optimizations for the computational kernel, data packing, advanced operator fusion, and parallelization. We evaluate nDirect2 by applying it to representative convolution kernels and demonstrating how well it performs on four distinct ARM-based CPUs and an X86-based CPU. Experimental results show that nDirect2 outperforms four state-of-the-art convolution approaches across most evaluation cases and hardware architectures.},
  archive      = {J_TC},
  author       = {Weiling Yang and Pengyu Wang and Jianbin Fang and Dezun Dong and Zhengbin Pang and Runxi He and Peng Zhang and Tao Tang and Chun Huang and Yonggang Che and Jie Ren},
  doi          = {10.1109/TC.2025.3543677},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1829-1843},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NDirect2: A high-performance library for direct convolutions on multicore CPUs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DataFly: A confidentiality-preserving data migration across heterogeneous blockchains. <em>TC</em>, <em>74</em>(6), 1814-1828. (<a href='https://doi.org/10.1109/TC.2025.3535830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Permissioned blockchains play a significant role in various application scenarios. Applications built on heterogeneous permissioned blockchains need to migrate data from one chain to another, aiming to keep their competitiveness and security. Thus, data migration across heterogeneous chains is a building block of permissioned blockchains. However, existing data migration protocols across heterogeneous chains are rarely used in practice since data migration technologies are insecure. To this end, we propose a data migration protocol across permissioned blockchains, named DataFly. We design a peg consensus mechanism, which provides consistent data-migration functionality between any two permissioned blockchains. To preserve the confidentiality of data, we invoke two classical cryptographic methods, i.e., i) ECDSA feature and ii) the integrated signature and public key encryption scheme. Through combining those two methods, data can be securely migrated from one permissioned blockchain to another without exposing the migrated data to anyone except associated parties. To demonstrate the practicality of DataFly, we implement a prototype of DataFly using existing popular permissioned blockchains, i.e., Hyperledger Fabric and private enterprise Ethereum. Measurement results demonstrate that DataFly outperforms related works in terms of transaction latency and gas costs.},
  archive      = {J_TC},
  author       = {Taotao Li and Huawei Huang and Parhat Abla and Zhihong Deng and Qinglin Yang and Anke Xie and Debiao He and Zibin Zheng},
  doi          = {10.1109/TC.2025.3535830},
  journal      = {IEEE Transactions on Computers},
  month        = {6},
  number       = {6},
  pages        = {1814-1828},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DataFly: A confidentiality-preserving data migration across heterogeneous blockchains},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient methodology for binary logarithmic computations of floating-point numbers with normalized output within one ulp of accuracy. <em>TC</em>, <em>74</em>(5), 1800-1813. (<a href='https://doi.org/10.1109/TC.2025.3543676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies have focused on the hardware implementation of binary logarithmic computation with fixed-point output. Although their outputs are accurate within 1 ulp (unit in the last place) in fixed-point format, they are far from meeting the accuracy requirement of 1 ulp in floating-point format when the output is close to 0. However, normalized floating-point output that is accurate to within 1-3 ulp is needed in many math libraries (for example, OpenCL, NVIDIA CUDA, and AMD AOCL). To the best of our knowledge, this is the first study to propose a hardware implementation of binary logarithmic computation for floating-point numbers with a normalized output that is accurate to within 1 ulp. Instead of calculating $\textrm{log}_{2}(1+fi)$ (where $\boldsymbol{fi}$ is the fractional part of the floating-point number) directly, the proposed methodology uses two novel objective functions for the polynomial approximation method. The novel objective functions make the significant bits of the outputs move forward to eliminate the necessity for high precision near zero. Compared with the designs of fixed-point binary logarithmic converters, the proposed hardware implementation achieves greater accuracy to meet the requirement of 1 ulp of floating-point format with a 21% extra area consumption.},
  archive      = {J_TC},
  author       = {Fei Lyu and Yuanyong Luo and Weiqiang Liu},
  doi          = {10.1109/TC.2025.3543676},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1800-1813},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An efficient methodology for binary logarithmic computations of floating-point numbers with normalized output within one ulp of accuracy},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Localizing multiple bugs in RTL designs by classifying hit-statements using neural networks. <em>TC</em>, <em>74</em>(5), 1786-1799. (<a href='https://doi.org/10.1109/TC.2025.3543609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays the advanced applications required in our lives have led to a significant increase in the complexity of circuits, which enhances the possibility of occurring design errors. Hence an automated, powerful, and scalable debugging approach is needed. Therefore, this paper proposes a scalable approach for localizing multiple bugs in Register-Transfer level (RTL) designs by using neural networks. The main idea is that hit-statements which are covered by failed test-vectors are more suspicious than those covered by passed test-vectors. We use coverage data as samples of our data set, label these samples, and tune the neural network model. Then we encode hit-statements and give them to the tuned model as new samples. The model classifies hit-statements. Hit-statements that take the failed labels, labels related to the failed test-vectors, are more suspicious of containing bugs. The results demonstrate that the proposed methodology outperforms recent approaches Tarsel and CirFix by localizing 80% of bugs at Top-1. The results also imply that our methodology increases the F1-score metric by 1.13× in comparison with existing RTL debugging techniques, which are prediction-based.},
  archive      = {J_TC},
  author       = {Mahsa Heidari and Bijan Alizadeh},
  doi          = {10.1109/TC.2025.3543609},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1786-1799},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Localizing multiple bugs in RTL designs by classifying hit-statements using neural networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic graph publication with differential privacy guarantees for decentralized applications. <em>TC</em>, <em>74</em>(5), 1771-1785. (<a href='https://doi.org/10.1109/TC.2025.3543605'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized Applications (DApps) have garnered significant attention due to their decentralization, anonymity, and data autonomy. However, these systems face potential privacy challenge. The privacy challenge arises from the necessity for external service providers to collect and process user interaction data. The untrustworthiness of these providers may lead to privacy breaches, compromising the overall security of such DApp environments. To address this challenge, we model the interaction data in the DApp environments as dynamic graphs and propose a dynamic graph publication method named HMG (Hidden Markov Model for Dynamic Graphs). HMG estimates the interaction probabilities between users by extracting the temporal information from historically collected data and constructs an optimized model to generate synthetic graphs. The synthetic graphs can preserve the dynamic topological characteristics of the interaction processes within DApp environments while effectively protecting user privacy, thus assisting external service providers in performing effective analyses. Finally, we evaluate the performance of HMG using real-world datasets and benchmark it against commonly used graph metrics. The results demonstrate that the synthetic graphs preserve essential features, making them suitable for analysis by service providers.},
  archive      = {J_TC},
  author       = {Zhetao Li and Yong Xiao and Haolin Liu and Xiaofei Liao and Ye Yuan and Junzhao Du},
  doi          = {10.1109/TC.2025.3543605},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1771-1785},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dynamic graph publication with differential privacy guarantees for decentralized applications},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A low-rate DoS attack mitigation scheme based on port and traffic state in SDN. <em>TC</em>, <em>74</em>(5), 1758-1770. (<a href='https://doi.org/10.1109/TC.2025.3541143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rate Denial of Service (DoS) attacks can significantly compromise network availability and are difficult to detect and mitigate due to their stealthy exploitation of flaws in congestion control mechanisms. Software-Defined Networking (SDN) is a revolutionary architecture that decouples network control from packet forwarding, emerging as a promising solution for defending against low-rate DoS attacks. In this paper, we propose Trident, a low-rate DoS attack mitigation scheme based on port and traffic state in SDN. Specifically, we design a multi-step strategy to monitor switch states. First, Trident identifies switches suspected of suffering from low-rate DoS attacks through port state detection. Then, it monitors the traffic state of switches with abnormal port states. Once a switch is identified as suffering from an attack, Trident analyzes the flow information to pinpoint the malicious flow. Finally, Trident issues rules to the switch's flow table to block the malicious flow, effectively mitigating the attack. We prototype Trident on the Mininet platform and conduct experiments using a real-world topology to evaluate its performance. The experiments show that Trident can accurately and robustly detect low-rate DoS attacks, respond quickly to mitigate them, and maintain low overhead.},
  archive      = {J_TC},
  author       = {Dan Tang and Rui Dai and Chenguang Zuo and Jingwen Chen and Keqin Li and Zheng Qin},
  doi          = {10.1109/TC.2025.3541143},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1758-1770},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A low-rate DoS attack mitigation scheme based on port and traffic state in SDN},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KDN-based adaptive computation offloading and resource allocation strategy optimization: Maximizing user satisfaction. <em>TC</em>, <em>74</em>(5), 1743-1757. (<a href='https://doi.org/10.1109/TC.2025.3541142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale dynamic network environments, optimizing the computation offloading and resource allocation strategy is key to improving resource utilization and meeting the diverse demands of User Equipment (UE). However, traditional strategies for providing personalized computing services face several challenges: dynamic changes in the environment and UE demands, along with the inefficiency and high costs of real-time data collection; the unpredictability of resource status leads to an inability to ensure long-term UE satisfaction. To address these challenges, we propose a Knowledge-Defined Networking (KDN)-based Adaptive Edge Resource Allocation Optimization (KARO) architecture, facilitating real-time data collection and analysis of environmental conditions. Additionally, we implement an environmental resource change perception module in the KARO to assess current and future resource utilization trends. Based on the real-time state and resource urgency, we develop a deep reinforcement learning-based Adaptive Long-term Computation Offloading and Resource Allocation (AL-CORA) strategy optimization algorithm. This algorithm adapts to the environmental resource urgency, autonomously balancing UE satisfaction and task execution cost. Experimental results indicate that AL-CORA effectively improves long-term UE satisfaction and task execution success rates, under the limited computation resource constraints.},
  archive      = {J_TC},
  author       = {Kaiqi Yang and Qiang He and Xingwei Wang and Zhi Liu and Yufei Liu and Min Huang and Liang Zhao},
  doi          = {10.1109/TC.2025.3541142},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1743-1757},
  shortjournal = {IEEE Trans. Comput.},
  title        = {KDN-based adaptive computation offloading and resource allocation strategy optimization: Maximizing user satisfaction},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RelHDx: Hyperdimensional computing for learning on graphs with FeFET acceleration. <em>TC</em>, <em>74</em>(5), 1730-1742. (<a href='https://doi.org/10.1109/TC.2025.3541141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are a powerful machine learning (ML) method to analyze graph data. The training of GNN has compute and memory-intensive phases along with irregular data movements, which makes in-memory acceleration challenging. We present a hyperdimensional computing (HDC)-based graph ML framework called RelHDx that aggregates node features and graph structure, along with representing node and edge information in high-dimensional space. RelHDx enables single-pass training and inference with simple arithmetic operations, resulting in the efficient design of graph-based ML tasks: node classification and link prediction. We accelerate RelHDx using scalable processing in-memory (PIM) architecture based on emerging ferroelectric FET (FeFET) technology. Our accelerator uses a data allocation optimization and operation scheduler to address the irregularity of the graph and maximize the performance. Evaluation results show that RelHDx offers comparable accuracy to popular GNN-based algorithms while achieving up to $63.8\boldsymbol{\times}$ faster speed on GPU. Our FeFET-based accelerator, RelHDx-PIM, is $32\boldsymbol{\times}$ faster for node classification, while for link prediction it is $65.4\boldsymbol{\times}$ faster than when running on GPU. Furthermore, RelHDx-PIM improves energy efficiency by four orders of magnitude over GPU. Compared to the state-of-the-art in-memory processing-based GNN accelerator, PIM-GCN [1], RelHDx-PIM is $10\boldsymbol{\times}$ faster and $986\boldsymbol{\times}$ more energy-efficient on average.},
  archive      = {J_TC},
  author       = {Jaeyoung Kang and Minxuan Zhou and Weihong Xu and Tajana Rosing},
  doi          = {10.1109/TC.2025.3541141},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1730-1742},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RelHDx: Hyperdimensional computing for learning on graphs with FeFET acceleration},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-based privacy-preserving deduplication and integrity auditing in cloud storage. <em>TC</em>, <em>74</em>(5), 1717-1729. (<a href='https://doi.org/10.1109/TC.2025.3540670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring cloud data security and reducing cloud storage costs have become particularly important. Many schemes expose user file ownership privacy when deduplicating authentication tags and during integrity auditing. Moreover, key management becomes more difficult as the number of files increases. Also, many audit schemes rely on third-party auditors (TPAs), but finding a fully trustworthy TPA is challenging. Therefore, we propose a blockchain-based integrity audit scheme supporting data deduplication. It protects file tag privacy during deduplication of ciphertexts and authentication tags, safeguards audit proof privacy, and effectively protects user file ownership privacy. To reduce key management costs, we introduce identity-based broadcast encryption (IBBE) that does not require interaction with key servers, eliminating additional communication costs. Additionally, we use smart contracts for integrity auditing, eliminating the need for a fully trusted TPA. We evaluate the proposed scheme through security and theoretical analyses and a series of experiments, demonstrating its efficiency and practicality.},
  archive      = {J_TC},
  author       = {Qingyang Zhang and Shuai Qian and Jie Cui and Hong Zhong and Fengqun Wang and Debiao He},
  doi          = {10.1109/TC.2025.3540670},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1717-1729},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Blockchain-based privacy-preserving deduplication and integrity auditing in cloud storage},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and secure storage verification in cloud-assisted industrial IoT networks. <em>TC</em>, <em>74</em>(5), 1702-1716. (<a href='https://doi.org/10.1109/TC.2025.3540661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Industrial IoT (IIoT) has caused the explosion of industrial data, which opens up promising possibilities for data analysis in IIoT networks. Due to the limitation of computation and storage capacity, IIoT devices choose to outsource the collected data to remote cloud servers. Unfortunately, the cloud storage service is not as reliable as it claims, whilst the loss of physical control over the cloud data makes it a significant challenge in ensuring the integrity of the data. Existing schemes are designed to check the data integrity in the cloud. However, it is still an open problem since IIoT devices have to devote lots of computation resources in existing schemes, which are especially not friendly to resource-constrained IIoT devices. In this paper, we propose an efficient storage verification approach for cloud-assisted industrial IoT platform by adopting a homomorphic hash function combined with polynomial commitment. The proposed approach can efficiently generate verification tags and verify the integrity of data in the industrial cloud platform for IIoT devices. Moreover, the proposed scheme can be extended to support privacy-enhanced verification and dynamic updates. We prove the security of the proposed approach under the random oracle model. Extensive experiments demonstrate the superior performance of our approach for resource-constrained devices in comparison with the state-of-the-art.},
  archive      = {J_TC},
  author       = {Haiyang Yu and Hui Zhang and Zhen Yang and Yuwen Chen and Huan Liu},
  doi          = {10.1109/TC.2025.3540661},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1702-1716},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient and secure storage verification in cloud-assisted industrial IoT networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $\mathsf{Aurora}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi mathvariant="sans-serif">Aurora</mml:mi></mml:mrow></mml:math>: Leaderless state-machine replication with high throughput. <em>TC</em>, <em>74</em>(5), 1690-1701. (<a href='https://doi.org/10.1109/TC.2025.3540656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-machine replication (SMR) allows a deterministic state machine to be replicated across a set of replicas and handle clients’ requests as a single machine. Most existing SMR protocols are leader-based requiring a leader to order requests and coordinate the protocol. This design places a disproportionately high load on the leader, inevitably impairing the scalability. If the leader fails, a complex and bug-prone fail-over protocol is needed to switch to a new leader. An adversary can also exploit the fail-over protocol to slow down the protocol. In this paper, we propose a crash-fault tolerant SMR named $\mathsf{Aurora}$<mml:math><mml:mrow><mml:mi mathvariant="sans-serif">Aurora</mml:mi></mml:mrow></mml:math>, with the following properties: • Leaderless: it does not require a leader, hence completely get rid of the fail-over protocol. • Scalable: it can scale up to $11$<mml:math><mml:mn>11</mml:mn></mml:math> replicas. • Robust: it behaves well even under a poor network connection. We provide a full-fledged implementation of $\mathsf{Aurora}$<mml:math><mml:mrow><mml:mi mathvariant="sans-serif">Aurora</mml:mi></mml:mrow></mml:math> and systematically evaluate its performance. Our benchmark results show that $\mathsf{Aurora}$<mml:math><mml:mrow><mml:mi mathvariant="sans-serif">Aurora</mml:mi></mml:mrow></mml:math> achieves a throughput of around two million Transactions Per Second (TPS), up to 8.7$\boldsymbol{\times}$<mml:math><mml:mo mathvariant="bold">×</mml:mo></mml:math> higher than the state-of-the-art leaderless SMR.},
  archive      = {J_TC},
  author       = {Hao Lu and Jian Liu and Kui Ren},
  doi          = {10.1109/TC.2025.3540656},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1690-1701},
  shortjournal = {IEEE Trans. Comput.},
  title        = {$\mathsf{Aurora}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi mathvariant="sans-serif">Aurora</mml:mi></mml:mrow></mml:math>: Leaderless state-machine replication with high throughput},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defying multi-model forgetting in one-shot neural architecture search using orthogonal gradient learning. <em>TC</em>, <em>74</em>(5), 1678-1689. (<a href='https://doi.org/10.1109/TC.2025.3540650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-shot neural architecture search (NAS) trains an over-parameterized network (termed as supernet) that assembles all the architectures as its subnets by using weight sharing for computational budget reduction. However, there is an issue of multi-model forgetting during supernet training that some weights of the previously well-trained architecture will be overwritten by that of the newly sampled architecture which has overlapped structures with the old one. To overcome the issue, we propose an orthogonal gradient learning (OGL) guided supernet training paradigm, where the novelty lies in the fact that the weights of the overlapped structures of current architecture are updated in the orthogonal direction to the gradient space of these overlapped structures of all previously trained architectures. Moreover, a new approach of calculating the projection is designed to effectively find the base vectors of the gradient space to acquire the orthogonal direction. We have theoretically and experimentally proved the effectiveness of the proposed paradigm in overcoming the multi-model forgetting. Besides, we apply the proposed paradigm to two one-shot NAS baselines, and experimental results demonstrate that our approach is able to mitigate the multi-model forgetting and enhance the predictive ability of the supernet with remarkable efficiency on popular test datasets.},
  archive      = {J_TC},
  author       = {Lianbo Ma and Yuee Zhou and Ye Ma and Guo Yu and Qing Li and Qiang He and Yan Pei},
  doi          = {10.1109/TC.2025.3540650},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1678-1689},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Defying multi-model forgetting in one-shot neural architecture search using orthogonal gradient learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lattice-based forward secure multi-user authenticated searchable encryption for cloud storage systems. <em>TC</em>, <em>74</em>(5), 1663-1677. (<a href='https://doi.org/10.1109/TC.2025.3540649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public key authenticated encryption with keyword search (PAEKS) has been widely studied in cloud storage systems, which allows the cloud server to search encrypted data while safeguarding against insider keyword guessing attacks (IKGAs). Most PAEKS schemes are based on the discrete logarithm (DL) hardness. However, this assumption becomes insecure when it comes to quantum attacks. To address this concern, there have been studies on post-quantum PAEKS based on lattice. But to our best knowledge, current lattice-based PAEKS exhibit limited applicability and security, such as only supporting single user scenarios, or encountering secret key leakage problem. In this paper, we propose FS-MUAEKS, the forward-secure multi-user authenticated searchable encryption, mitigating the secret key exposure problem and further supporting multi-user scenarios in a quantum setting. Additionally, we formalize the security models of FS-MUAEKS and prove its security in the random oracle model (ROM). Ultimately, the comprehensive performance evaluation indicates that our scheme is computationally efficient and surpasses other state-of-the-art PAEKS schemes. The ciphertext generation overhead of our scheme is only 0.27 times of others in the best case. The communication overhead of our FS-MUAEKS algorithm is constant at 1.75MB under different security parameter settings.},
  archive      = {J_TC},
  author       = {Shiyuan Xu and Xue Chen and Yu Guo and Yuer Yang and Shengling Wang and Siu-Ming Yiu and Xiuzhen Cheng},
  doi          = {10.1109/TC.2025.3540649},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1663-1677},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lattice-based forward secure multi-user authenticated searchable encryption for cloud storage systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PQNTRU: Acceleration of NTRU-based schemes via customized post-quantum processor. <em>TC</em>, <em>74</em>(5), 1649-1662. (<a href='https://doi.org/10.1109/TC.2025.3540647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-quantum cryptography (PQC) has rapidly evolved in response to the emergence of quantum computers, with the US National Institute of Standards and Technology (NIST) selecting four finalist algorithms for PQC standardization in 2022, including the Falcon digital signature scheme. Hawk is currently the only lattice-based candidate in NIST Round 2 additional signatures. Falcon and Hawk are based on the NTRU lattice, offering compact signatures, fast generation, and verification suitable for deployment on resource-constrained Internet-of-Things (IoT) devices. Despite the popularity of ML-DSA and ML-KEM, research on NTRU-based schemes has been limited due to their complex algorithms and operations. Falcon and Hawk's performance remains constrained by the lack of parallel execution in crucial operations like the Number Theoretic Transform (NTT) and Fast Fourier Transform (FFT), with data dependency being a significant bottleneck. This paper enhances NTRU-based schemes Falcon and Hawk through hardware/software co-design on a customized Single-Instruction-Multiple-Data (SIMD) processor, proposing new SIMD hardware units and instructions to expedite these schemes along with software optimizations to boost performance. Our NTT optimization includes a novel layer merging technique for SIMD architecture to reduce memory accesses, and the use of modular algorithms (Signed Montgomery and Improved Plantard) targets various modulus data widths to enhance performance. We explore applying layer merging to accelerate fixed-point FFT at the SIMD instruction level and devise a dual-issue parser to streamline assembly code organization to maximize dual-issue utilization. A System-on-chip (SoC) architecture is devised to improve the practical application of the processor in real-world scenarios. Evaluation on 28 $nm$ technology and field programmable gate array (FPGA) platform shows that our design and optimizations can increase the performance of Hawk signature generation and verification by over 7$\times$.},
  archive      = {J_TC},
  author       = {Zewen Ye and Junhao Huang and Tianshun Huang and Yudan Bai and Jinze Li and Hao Zhang and Guangyan Li and Donglong Chen and Ray C. C. Cheung and Kejie Huang},
  doi          = {10.1109/TC.2025.3540647},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1649-1662},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PQNTRU: Acceleration of NTRU-based schemes via customized post-quantum processor},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 29-billion atoms molecular dynamics simulation with ab initio accuracy on 35 million cores of new sunway supercomputer. <em>TC</em>, <em>74</em>(5), 1634-1648. (<a href='https://doi.org/10.1109/TC.2025.3540646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical phenomena such as bond breaking and phase transitions require molecular dynamics (MD) with ab initio accuracy, involving up to billions of atoms and over nanosecond timescales. Previous state-of-the-art work has demonstrated that neural network molecular dynamics (NNMD) like deep potential molecular dynamics (DeePMD), can successfully extend the temporal and spatial scales of MD with ab initio accuracy on both ARM and GPU platforms. However, the DeePMD-kit package is currently unable to fully exploit the computational potential of the new Sunway supercomputer due to its unique many-core architecture, memory hierarchy, and low precision capability. In this paper, we re-design the DeePMD-kit to harness the massive computing power of the new Sunway, enabling the MD with over ten billion atoms. We first design a large-scale parallelization scheme to exploit the massive parallelism of the new Sunway. Then we devise specialized optimizations for the time-consuming operators. Finally, we design a novel mixed precision method for DeePMD-kit customized operators to leverage the low precision computing power of the new Sunway. The optimized DeePMD-kit achieves 67.6 / 56.5 $\boldsymbol{\times}$ speedup for water / copper systems on the new Sunway. Meanwhile, it can perform 29 billion atoms simulation for the water system on 35 million cores (i.e., 90,000 computing nodes, around 84% of the whole supercomputer) with a peak performance of 57.1 PFLOPs, which is 7.9$\boldsymbol{\times}$ bigger and 1.2$\boldsymbol{\times}$ faster than state-of-the-art results. This paves the way for investigating more realistic scenarios, such as studying the mechanical properties of metals, semiconductor devices, batteries, and other materials and physical systems.},
  archive      = {J_TC},
  author       = {Xun Wang and Xiangyu Meng and Zhuoqiang Guo and Mingzhen Li and Lijun Liu and Mingfan Li and Qian Xiao and Tong Zhao and Ninghui Sun and Guangming Tan and Weile Jia},
  doi          = {10.1109/TC.2025.3540646},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1634-1648},
  shortjournal = {IEEE Trans. Comput.},
  title        = {29-billion atoms molecular dynamics simulation with ab initio accuracy on 35 million cores of new sunway supercomputer},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AToM: Adaptive token merging for efficient acceleration of vision transformer. <em>TC</em>, <em>74</em>(5), 1620-1633. (<a href='https://doi.org/10.1109/TC.2025.3540638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Vision Transformers (ViTs) have set a new standard in computer vision (CV), showing unparalleled image processing performance. However, their substantial computational requirements hinder practical deployment, especially on resource-limited devices common in CV applications. Token merging has emerged as a solution, condensing tokens with similar features to cut computational and memory demands. Yet, existing applications on ViTs often miss the mark in token compression, with rigid merging strategies and a lack of in-depth analysis of ViT merging characteristics. To overcome these issues, this paper introduces Adaptive Token Merging (AToM), a comprehensive algorithm-architecture co-design for accelerating ViTs. The AToM algorithm employs an image-adaptive, fine-grained merging strategy, significantly boosting computational efficiency. We also optimize the merging and unmerging processes to minimize overhead, employing techniques like First-Come-First-Merge mapping and Linear Distance Calculation. On the hardware side, the AToM architecture is tailor-made to exploit the AToM algorithm's benefits, with specialized engines for efficient merge and unmerge operations. Our pipeline architecture ensures end-to-end ViT processing, minimizing latency and memory overhead from the AToM algorithm. Across various hardware platforms including CPU, EdgeGPU, and GPU, AToM achieves average end-to-end speedups of 10.9$\boldsymbol{\times}$, 7.7$\boldsymbol{\times}$, and 5.4$\boldsymbol{\times}$, alongside energy savings of 24.9$\boldsymbol{\times}$, 1.8$\boldsymbol{\times}$, and 16.7$\boldsymbol{\times}$. Moreover, AToM offers 1.2$\boldsymbol{\times}$ 1.9$\boldsymbol{\times}$ higher effective throughput compared to existing transformer accelerators.},
  archive      = {J_TC},
  author       = {Jaekang Shin and Myeonggu Kang and Yunki Han and Junyoung Park and Lee-Sup Kim},
  doi          = {10.1109/TC.2025.3540638},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1620-1633},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AToM: Adaptive token merging for efficient acceleration of vision transformer},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RGKV: A GPGPU-empowered compaction framework for LSM-tree-based KV stores with optimized data transfer and parallel processing. <em>TC</em>, <em>74</em>(5), 1605-1619. (<a href='https://doi.org/10.1109/TC.2025.3535832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Log-structured merge-tree (LSM-tree), widely adopted in key-value stores (KV stores), is esteemed for its efficient write performance and superb scalability amid large-scale data processing. The compaction process of LSM-trees consumes significant computational resources, thereby becoming a bottleneck for system performance. Traditionally, compaction is handled by CPUs, but CPU processing capacity often falls short of increasing demands with the surge in data volumes. To address this challenge, existing solutions attempt to accelerate compaction using GPGPUs. Due to low GPGPU parallelism and data transfer delay in prior studies, the anticipated performance improvements have not yet been fully realized. In this paper, we bring forth RGKV – a comprehensive optimization approach to overcoming the limitations of current GPGPU-empowered KV stores. RGKV features the GPGPU-adapted contiguous memory allocation and GPGPU-optimized key-value block architecture to furnish high-efficient GPGPU parallel encoding and decoding catering to the needs of KV stores. To enhance the computational efficiency and overall performance of KV stores, RGKV employs a parallel merge-sorting algorithm to maximize the parallel processing capabilities of the GPGPU. Moreover, RGKV incorporates a data transfer module anchored on the GPUDirect storage technology – designed for KV stores – and designs an efficient data structure to substantially curtail data transfer latency between an SSD and a GPGPU, boosting data transfer speed and alleviating CPU load. The experimental results demonstrate that RGKV achieves a remarkable 4$\times$ improvement in overall throughput and a 7$\times$ improvement in compaction throughput compared to the state-of-the-art KV stores, while also reducing average write latency by 70.6%.},
  archive      = {J_TC},
  author       = {Hui Sun and Xiangxiang Jiang and Yinliang Yue and Xiao Qin},
  doi          = {10.1109/TC.2025.3535832},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1605-1619},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RGKV: A GPGPU-empowered compaction framework for LSM-tree-based KV stores with optimized data transfer and parallel processing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-delay-aware joint microservice deployment and request routing with DVFS in edge: A reinforcement learning approach. <em>TC</em>, <em>74</em>(5), 1589-1604. (<a href='https://doi.org/10.1109/TC.2025.3535826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emerging microservice architecture offers opportunities for accommodating delay-sensitive applications in edge. However, such applications are computation-intensive and energy-consuming, imposing great difficulties to edge servers with limited computing resources, energy supply, and cooling capabilities. To reduce delay and energy consumption in edge, efficient microservice orchestration is necessary, but significantly challenging. Due to frequent communications among multiple microservices, service deployment and request routing are tightly-coupled, which motivates a complex joint optimization problem. When considering multi-instance modeling and fine-grained orchestration for massive microservices, the difficulty is extremely enlarged. Nevertheless, previous work failed to address the above difficulties. Also, they neglected to balance delay and energy, especially lacking dynamic energy-saving abilities. Therefore, this paper minimizes energy and delay by jointly optimizing microservice deployment and request routing via multi-instance modeling, fine-grained orchestration, and dynamic adaptation. Our queuing network model enables accurate end-to-end time analysis covering queuing, computing, and communicating delays. We then propose a delay-aware reinforcement learning algorithm, which derives the static service deployment and routing decisions. Moreover, we design an energy-aware dynamic frequency scaling algorithm, which saves energy with fluctuating request patterns. Experiment results demonstrate that our approaches significantly outperform baseline algorithms in both delay and energy consumption.},
  archive      = {J_TC},
  author       = {Liangyuan Wang and Xudong Liu and Haonan Ding and Yi Hu and Kai Peng and Menglan Hu},
  doi          = {10.1109/TC.2025.3535826},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1589-1604},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy-delay-aware joint microservice deployment and request routing with DVFS in edge: A reinforcement learning approach},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Slack time management for imprecise mixed-criticality systems with reliability constraints. <em>TC</em>, <em>74</em>(5), 1577-1588. (<a href='https://doi.org/10.1109/TC.2025.3533100'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Mixed-Criticality System (MCS) integrates multiple applications with different criticality levels on the same hardware platform. For power and energy-constrained systems such as Unmanned Aerial Vehicles, it is important to minimize energy consumption of the computing system while meeting reliability constraints. In this paper, we first determine the number of tolerated faults according to the given reliability target. Second, we propose a schedulability test for MCS with semi-clairvoyance and checkpointing. Third, we propose the Energy-Aware Scheduling with Reliability Constraint (EASRC) scheduling algorithm for MCS with semi-clairvoyance and checkpointing. It consists of an offline phase and an online phase. In the offline phase, we determine the offline processor speed by reclaiming static slack time. In the online phase, we adjust the processor speed by reclaiming dynamic slack time to further save energy. Finally, we show the performance of our proposed algorithm through experimental evaluations. The results show that the proposed algorithm can save an average of 9.67% of energy consumption compared with existing methods.},
  archive      = {J_TC},
  author       = {Yi-Wen Zhang and Hui Zheng},
  doi          = {10.1109/TC.2025.3533100},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1577-1588},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Slack time management for imprecise mixed-criticality systems with reliability constraints},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computing tasks saving schemes through early exit in edge intelligence-assisted systems. <em>TC</em>, <em>74</em>(5), 1565-1576. (<a href='https://doi.org/10.1109/TC.2025.3533098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge intelligence (EI) is a promising paradigm where end devices collaborate with edge servers to provide artificial intelligence services to users. In most realistic scenarios, end devices often move unconsciously, resulting in frequent computing migrations. Moreover, a surge in computing tasks offloaded to edge servers significantly prolongs queuing latency. These two issues obstruct the timely completion of computing tasks in EI-assisted systems. In this paper, we formulate an optimization problem aiming to maximize computing task completion under latency constraints. To address this issue, we first categorize computing tasks into new computing tasks (NCTs) and partially completed computing tasks (PCTs). Subsequently, based on model partitioning, we design a new computing task saving scheme (NSS) to optimize early exit points for NCTs and computing tasks in the queuing queue. Furthermore, we propose a partially completed computing task saving scheme (PSS) to set early exit points for PCTs during computing migrations. Numerous experiments show that computing saving schemes can achieve at least 90% computing task completion rate and up to 61.81% latency reduction compared to other methods.},
  archive      = {J_TC},
  author       = {Xin Niu and Xianwei Lv and Wang Chen and Chen Yu and Hai Jin},
  doi          = {10.1109/TC.2025.3533098},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1565-1576},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Computing tasks saving schemes through early exit in edge intelligence-assisted systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Small hazard-free transducers. <em>TC</em>, <em>74</em>(5), 1549-1564. (<a href='https://doi.org/10.1109/TC.2025.3533096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In digital circuits, hazardous input signals are a result of spurious operation of bistable elements. For example, the problem occurs in circuits with asynchronous inputs or clock domain crossings. Marino (TC’81) showed that hazards in bistable elements are inevitable. Hazard-free circuits compute the “most stable” output possible on hazardous inputs, under the constraint that it returns the same output as the circuit on stable inputs. Ikenmeyer et al. (JACM’19) proved an unconditional exponential separation between the hazard-free complexity and (standard) circuit complexity of explicit functions. Despite that, asymptotically optimal hazard-free sorting circuit are possible (Bund et al., TC’19). This raises the question: Which classes of functions permit efficient hazard-free circuits? We prove that circuit implementations of transducers with small state space are such a class. A transducer is a finite state machine that transcribes, symbol by symbol, an input string of length n into an output string of length n. We present a construction that transforms any function arising from a transducer into an efficient circuit that computes the hazard-free extension of the function. For transducers with constant state space, the circuit has asymptotically optimal size, with small constants if the state space is small.},
  archive      = {J_TC},
  author       = {Johannes Bund and Christoph Lenzen and Moti Medina},
  doi          = {10.1109/TC.2025.3533096},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1549-1564},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Small hazard-free transducers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pruning-based adaptive federated learning at the edge. <em>TC</em>, <em>74</em>(5), 1538-1548. (<a href='https://doi.org/10.1109/TC.2025.3533095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a new learning framework in which $s$ clients collaboratively train a model under the guidance of a central server. Meanwhile, with the advent of the era of large models, the parameters of models are facing explosive growth. Therefore, it is important to design federated learning algorithms for edge environment. However, the edge environment is severely limited in computing, storage, and network bandwidth resources. Concurrently, adaptive gradient methods show better performance than constant learning rate in non-distributed settings. In this paper, we propose a pruning-based distributed Adam (PD-Adam) algorithm, which combines model pruning and adaptive learning steps to achieve asymptotically optimal convergence rate of $O(1/\sqrt[4]{K})$. At the same time, the algorithm can achieve convergence consistent with the centralized model. Finally, extensive experiments have confirmed the convergence of our algorithm, demonstrating its reliability and effectiveness across various scenarios. Specially, our proposed algorithm is $2$% and $18$% more accurate than the current state-of-the-art FedAvg algorithm on the ResNet and CIFAR datasets.},
  archive      = {J_TC},
  author       = {Dongxiao Yu and Yuan Yuan and Yifei Zou and Xiao Zhang and Yu Liu and Lizhen Cui and Xiuzhen Cheng},
  doi          = {10.1109/TC.2025.3533095},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1538-1548},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Pruning-based adaptive federated learning at the edge},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The graph structure of baker's maps implemented on a computer. <em>TC</em>, <em>74</em>(5), 1524-1537. (<a href='https://doi.org/10.1109/TC.2025.3533094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complex dynamics of the baker's map and its variants in infinite-precision mathematical domains and quantum settings have been extensively studied over the past five decades. However, their behavior in finite-precision digital computing remains largely unknown. This paper addresses this gap by investigating the graph structure of the generalized two-dimensional baker's map and its higher-dimensional extension, referred to as HDBM, as implemented on the discrete setting in a digital computer. We provide a rigorous analysis of how the map parameters shape the in-degree bounds and distribution within the functional graph, revealing fractal-like structures intensify as parameters approach each other and arithmetic precision increases. Furthermore, we demonstrate that recursive tree structures can characterize the functional graph structure of HDBM in a fixed-point arithmetic domain. Similar to the 2-D case, the degree of any non-leaf node in the functional graph, when implemented in the floating-point arithmetic domain, is determined solely by its last component. We also reveal the relationship between the functional graphs of HDBM across the two arithmetic domains. These findings lay the groundwork for dynamic analysis, effective control, and broader application of the baker's map and its variants in diverse domains.},
  archive      = {J_TC},
  author       = {Chengqing Li and Kai Tan},
  doi          = {10.1109/TC.2025.3533094},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1524-1537},
  shortjournal = {IEEE Trans. Comput.},
  title        = {The graph structure of baker's maps implemented on a computer},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic caching dependency-aware task offloading in mobile edge computing. <em>TC</em>, <em>74</em>(5), 1510-1523. (<a href='https://doi.org/10.1109/TC.2025.3533091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) is a distributed computing paradigm that provides computing capabilities at the periphery of mobile cellular networks. This architecture empowers Mobile Users (MUs) to offload computation-intensive applications to large-scale computing nodes near the edge side, reducing application latency for MUs. The resource allocation and task offloading in MEC has been widely studied. However, the burgeoning complexity inherent to modern applications, often represented as Directed Acyclic Graphs (DAGs) comprising a multitude of subtasks with interdependencies, poses huge challenges for application offloading and resource allocation. Meanwhile, previous work has neglected the impact of edge caching on the offloading execution of dependent tasks. Therefore, this paper introduces a novel dynamic caching dependency-aware task offloading (CachOf) scheme. First, to effectively enhance the rationality of cache and computing resource allocation, we develop a subtask priority computation scheme based on DAG dependencies. This scheme includes the execution sequence priority of subtasks on a single MU and the offloading sequence priority of subtasks from multiple MUs. Second, a dynamic caching scheme, designed to cater to dependent tasks, is proposed. This caching approach can not only assist offloading decisions, but also contribute to load balancing by harmonizing caching resources among edge servers. Finally, based on the task prioritization results and caching results, this paper presents a Deep Reinforcement Learning (DRL)-based offloading scheme to judiciously allocate resources and improve the execution efficiency of applications. Extensive simulation experiments demonstrate that CachOf outperforms other baseline schemes, achieving improved execution efficiency for applications.},
  archive      = {J_TC},
  author       = {Liang Zhao and Zijia Zhao and Ammar Hawbani and Zhi Liu and Zhiyuan Tan and Keping Yu},
  doi          = {10.1109/TC.2025.3533091},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1510-1523},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dynamic caching dependency-aware task offloading in mobile edge computing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DC-ORAM: An ORAM scheme based on dynamic compression of data blocks and position map. <em>TC</em>, <em>74</em>(5), 1495-1509. (<a href='https://doi.org/10.1109/TC.2025.3533089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oblivious RAM (ORAM) is an efficient cryptographic primitive that prevents leakage of memory access patterns. It has been referenced by modern secure processors and plays an important role in memory security protection. Although the most advanced ORAM has made great progress in performance optimization, the access overhead (i.e., data blocks) and on-chip (i.e., PosMap) storage overhead is still too high, which will lead to problems such as low system performance. To overcome the above challenges, in this paper, we propose a DC-ORAM system, which reduces the data access overhead and on-chip PosMap storage overhead by using dynamic compression technology. Specifically, we use byte stream redundancy compression technology to compress data blocks on the ORAM tree. And in PosMap, a high-bit multiplexing strategy is used to achieve data compression for binary high-bit repeated data of leaf labels (or path labels). By introducing the above compression technology, in this work, compared with conventional Path ORAM, the compression rate of the ORAM tree is $52.9\%$, and the compression rate of PosMap is $40.0\%$. In terms of performance, compared to conventional Path ORAM, our proposed DC-ORAM system reduces the average latency by $33.6\%$. In addition, we apply the compression technology proposed in this work to the Ring ORAM system. By comparison, it is found that with the same compression ratio as Path ORAM, our design can still reduce latency by an average of $21.5\%$.},
  archive      = {J_TC},
  author       = {Chuang Li and Changyao Tan and Gang Liu and Yanhua Wen and Yan Wang and Kenli Li},
  doi          = {10.1109/TC.2025.3533089},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1495-1509},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DC-ORAM: An ORAM scheme based on dynamic compression of data blocks and position map},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A complexity-effective local delta prefetcher. <em>TC</em>, <em>74</em>(5), 1482-1494. (<a href='https://doi.org/10.1109/TC.2025.3533086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data prefetching is crucial for performance in modern processors by effectively masking long-latency memory accesses. Over the past decades, numerous data prefetching mechanisms have been proposed, which have continuously reduced the access latency to the memory hierarchy. Several state-of-the-art prefetchers, namely Instruction Pointer Classifier Prefetcher (IPCP) and Berti, target the first-level data cache, and thus, they are able to completely hide the miss latency for timely prefetched cache lines. Berti exploits timely local deltas to achieve high accuracy and performance. This paper extends Berti with a larger evaluation and with extra optimizations on top of the previous conference paper. The result is a complexity-effective version of Berti that outperforms it for a large amount of workloads and simplifies its control logic. The key for those advancements is a simple mechanism for learning timely deltas without the need to track the fetch latency of each cache miss. Our experiments conducted with a wide range of workloads (CVP traces by Qualcomm, SPEC CPU2017, and GAP) show performance improvements by 4.0% over a mainstream stride prefetcher, and by a non-negligible 1.4% over the previously published version of Berti requiring similar storage.},
  archive      = {J_TC},
  author       = {Agustín Navarro-Torres and Biswabandan Panda and Jesús Alastruey-Benedé and Pablo Ibáñez and Víctor Viñals-Yúfera and Alberto Ros},
  doi          = {10.1109/TC.2025.3533086},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1482-1494},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A complexity-effective local delta prefetcher},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware trojan detection methods for gate-level netlists based on graph neural networks. <em>TC</em>, <em>74</em>(5), 1470-1481. (<a href='https://doi.org/10.1109/TC.2025.3533085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, untrusted third-party entities are increasingly involved in various stages of IC design and manufacturing, posing a significant threat to the reliability and security of SoCs due to the presence of hardware Trojans (HTs). In this paper, gate-level HT detection methods based on graph neural networks (GNNs) are established to overcome the defects of existing machine learning, which makes it difficult to characterize circuit connection relationships. We introduce harmonic centrality in the feature engineering of gate-level HT detection, which reflects the positional information of nodes and their adjacent nodes in the graph, thereby enhancing the quality of feature engineering. We use the golden section weight optimization algorithm to configure penalty weights to alleviate the problem of extreme data imbalance. In the SAED database, GraphSAGE-LSTM model obtained a TPR of 88.06% and an average F1 score of 90.95%. In the combined HT netlist of LEDA datasets, GraphSAGE-POOL model obtains a TPR of 88.50% and the best F1 score of 92.17%. In sequential HT netlist, GraphSAGE-LSTM model performs optimally, with a TPR of 98.25% and an average F1 score of 98.59%. Compared to existing detection models, the F1 score is enhanced by 8.86% and 2.48% on combined and sequential HT datasets, respectively.},
  archive      = {J_TC},
  author       = {Peijun Ma and Jie Li and Hongjin Liu and Jiangyi Shi and Shaolin Zhang and Weitao Pan and Yue Hao},
  doi          = {10.1109/TC.2025.3533085},
  journal      = {IEEE Transactions on Computers},
  month        = {5},
  number       = {5},
  pages        = {1470-1481},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hardware trojan detection methods for gate-level netlists based on graph neural networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling printed multilayer perceptrons realization via area-aware neural minimization. <em>TC</em>, <em>74</em>(4), 1461-1469. (<a href='https://doi.org/10.1109/TC.2024.3524076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Printed Electronics (PE) set up a new path for the realization of ultra low-cost circuits that can be deployed in every-day consumer goods and disposables. In addition, PE satisfy requirements such as porosity, flexibility, and conformity. However, the large feature sizes in PE and limited device counts incur high restrictions and increased area and power overheads, prohibiting the realization of complex circuits. As a result, although printed Machine Learning (ML) circuits could open new horizons and bring “intelligence” in such domains, the implementation of complex classifiers, as required in target applications, is hardly feasible. In this paper, we aim to address this and focus on the design of battery-powered printed Multilayer Perceptrons (MLPs). To that end, we exploit fully-customized circuit (bespoke) implementations, enabled in PE, and propose a hardware-aware neural minimization framework dedicated for such customized MLP circuits. Our evaluation demonstrates that, for up to 3% accuracy loss, our co-design methodology enables, for the first time, battery-powered operation of complex printed MLPs.},
  archive      = {J_TC},
  author       = {Argyris Kokkinis and Georgios Zervakis and Kostas Siozios and Mehdi Baradaran Tahoori and Jörg Henkel},
  doi          = {10.1109/TC.2024.3524076},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1461-1469},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling printed multilayer perceptrons realization via area-aware neural minimization},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing structured-sparse matrix multiplication in RISC-V vector processors. <em>TC</em>, <em>74</em>(4), 1446-1460. (<a href='https://doi.org/10.1109/TC.2025.3533083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structured sparsity has been proposed as an efficient way to prune the complexity of Machine Learning (ML) applications and to simplify the handling of sparse data in hardware. Accelerating ML models, whether for training, or inference, heavily relies on matrix multiplications that can be efficiently executed on vector processors, or custom matrix engines. This work aims to integrate the simplicity of structured sparsity into vector execution to speed up the corresponding matrix multiplications. Initially, the implementation of structured-sparse matrix multiplication using the current RISC-V instruction set vector extension is comprehensively explored. Critical parameters that affect performance, such as the impact of data distribution across the scalar and vector register files, data locality, and the effectiveness of loop unrolling are analyzed both qualitatively and quantitatively. Furthermore, it is demonstrated that the addition of a single new instruction would reap even higher performance. The newly proposed instruction is called vindexmac, i.e., vector index-multiply-accumulate. It allows for indirect reads from the vector register file and it reduces the number of instructions executed per matrix multiplication iteration, without introducing additional dependencies that would limit loop unrolling. The proposed new instruction was integrated in a decoupled RISC-V vector processor with negligible hardware cost. Experimental results demonstrate the runtime efficiency and the scalability offered by the introduced optimizations and the new instruction for the execution of state-of-the-art Convolutional Neural Networks. More particularly, the addition of a custom instruction improves runtime by 25% and 33%, when compared with highly-optimized vectorized kernels that use only the currently defined RISC-V instructions.},
  archive      = {J_TC},
  author       = {Vasileios Titopoulos and Kosmas Alexandridis and Christodoulos Peltekis and Chrysostomos Nicopoulos and Giorgos Dimitrakopoulos},
  doi          = {10.1109/TC.2025.3533083},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1446-1460},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing structured-sparse matrix multiplication in RISC-V vector processors},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLOpt: Serving real-time inference pipeline with strict latency constraint. <em>TC</em>, <em>74</em>(4), 1431-1445. (<a href='https://doi.org/10.1109/TC.2025.3528125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of machine learning as a service (MLaaS) has driven the demand for complex and customized real-time inference tasks, often requiring cascading multiple deep neural network (DNN) models into inference pipelines. However, these pipelines pose significant challenges due to scheduling complexity, particularly in maintaining strict latency service level objectives (SLOs). Existing systems serve pipelines with model-independent scheduling policies, which ignore the unique workload characteristics introduced by model cascading in the inference pipeline, leading to SLO violations and resource inefficiencies. In this paper, we propose that the serving system should exploit the model-cascading nature and intermodel workload dependency of the inference pipeline to ensure strict latency SLO cost-effectively. Based on this, we design and implement SLOpt, a serving system optimized for real-time inference pipelines with a three-stage codesign of workload estimation, resource provisioning, and request execution. SLOpt proposes cascade workload estimation and ahead-of-time tuning, which together address the challenge of cascade blocking and head-of-line blocking in workload estimation and resource provisioning. SLOpt further implements an adaptive batch drop policy to mitigate latency amplification issues within the pipeline. These innovations enable SLOpt to reduce the 99th percentile latency (P99 latency) by $1.4$ to $2.5$ times compared to the state of the arts while lowering serving costs by up to $29\%$. Moreover, to achieve comparable P99 latency, SLOpt requires up to $70\%$ less cost than existing systems. Extensive evaluations on a 64-GPU cluster demonstrate SLOpt's effectiveness in meeting strict P99 latency SLOs under diverse real-world workloads.},
  archive      = {J_TC},
  author       = {Zhixin Zhao and Yitao Hu and Guotao Yang and Ziqi Gong and Chen Shen and Laiping Zhao and Wenxin Li and Xiulong Liu and Wenyu Qu},
  doi          = {10.1109/TC.2025.3528125},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1431-1445},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SLOpt: Serving real-time inference pipeline with strict latency constraint},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NetCRC-NR: In-network 5G NR CRC accelerator. <em>TC</em>, <em>74</em>(4), 1418-1430. (<a href='https://doi.org/10.1109/TC.2025.3526326'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 5G Radio Access Networks (RAN), Cyclic Redundancy Check (CRC) algorithms play a vital role in detecting accidental changes to digital data during transmission. However, due to the massive bandwidth demands in 5G networks, CRC computation is a resource-intensive process. To address this challenge, we propose performing CRC computation and verification directly in the network path. Specifically, we introduce NetCRC-NR, a 5G New Radio (NR) standard-compliant in-network CRC accelerator. NetCRC-NR implements the 5G NR CRC algorithms specified in 3GPP TS 38.212, including CRC24A, CRC24B, CRC24C, CRC16, CRC11, and CRC6. It leverages programmable switches to perform in-network CRC generation and validation for the Transport Blocks (TBs) and Code Blocks (CBs), aiming at providing high CRC computation throughput and alleviating the computational burden on General-Purpose Processors (GPPs). We design and implement NetCRC-NR on Intel Tofino programmable switch and commodity servers running the Data Plane Development Kit (DPDK). Extensive experiments demonstrate that NetCRC-NR performs CRC generation and verification at the switch line rate of up to 4+Tbps CRC throughput, showcasing its efficiency and potential in accelerating the 5G RAN error detection process.},
  archive      = {J_TC},
  author       = {Abdulbary Naji and Xingfu Wang and Ping Liu and Ammar Hawbani and Liang Zhao and Xiaohua Xu and Fuyou Miao},
  doi          = {10.1109/TC.2025.3526326},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1418-1430},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NetCRC-NR: In-network 5G NR CRC accelerator},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure and efficient cross-modal retrieval over encrypted multimodal data. <em>TC</em>, <em>74</em>(4), 1405-1417. (<a href='https://doi.org/10.1109/TC.2025.3525614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of social media, mobile devices and the Internet, a large amount of multimodal data (e.g, text, image, audio, video, etc.) is increasingly being outsourced to cloud to save local computing and storage costs. To search through encrypted multimodal data in the cloud, privacy-preserving cross-modal retrieval (PPCMR) techniques have attracted extensive attention. However, most of the existing PPCMR schemes lack the ability to resist quantum attacks and have low search efficiency on large-scale datasets. To solve above problems, we first propose a basic PPCMR scheme FECMR using the enhanced Single-key Function-hiding Inner Product Functional Encryption for Binary strings (SFB-IPFE) and cross-modal hashing technology, which achieves the measurement of similarity over encrypted multimodal data while resisting quantum attacks. Then, we design an efficient index KM-tree utilizing the K-modes clustering algorithm. On this basis, we propose an improved scheme FECMR+, which achieves sub-linear search complexity. Finally, formal security analysis proves that our schemes are secure against quantum attacks, and extensive experiments prove that our schemes are efficient and feasible for practical application.},
  archive      = {J_TC},
  author       = {Li Yang and Wei Zhang and Yinbin Miao and Yanrong Liang and Xinghua Li and Kim-Kwang Raymond Choo and Robert H. Deng},
  doi          = {10.1109/TC.2025.3525614},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1405-1417},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Secure and efficient cross-modal retrieval over encrypted multimodal data},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-precision error bit prediction for 3D QLC NAND flash memory: Observations, analysis, and modeling. <em>TC</em>, <em>74</em>(4), 1392-1404. (<a href='https://doi.org/10.1109/TC.2025.3525610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of artificial intelligence, large language models (LLM) require rapid development along with massive volumes of training data and parameter storage. Over the past decade, 3D NAND flash memory has emerged as the dominant non-volatile memory technology due to its high bit density and large capacity. However, because of its 3D vertical stacking technique and array designs, 3D NAND flash memory has more complicated data loss mechanisms compared to 2D NAND flash memory. As bit densities rise to Quad-level-cells (QLC), the small read margins will further complicate and make the situation more unpredictable. In this work, we propose an error-bit prediction model in this paper for 3D QLC NAND flash memory with the charge-trap (CT) cell structure based on a thorough analysis of multiple parameters that affect the error-bit distributions, including read disturb (RD) and degradation from program/erase (PE) cycles. Specifically, we develop the whole-block prediction (WBP) and the dynamic-worst-page prediction (DWPM) models. It is shown that the proposed models can be used for high-precision error-bit prediction to guarantee data reliability in commonly used NAND-based storage systems based on the characterization results of raw NAND chips.},
  archive      = {J_TC},
  author       = {Guangkuo Yang and Meng Zhang and Peng Guo and Xuepeng Zhan and Shaoqi Yang and Xiaohuan Zhao and Xinyi Guo and Pengpeng Sang and Jixuan Wu and Fei Wu and Jiezhi Chen},
  doi          = {10.1109/TC.2025.3525610},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1392-1404},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-precision error bit prediction for 3D QLC NAND flash memory: Observations, analysis, and modeling},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Karatsuba matrix multiplication and its efficient custom hardware implementations. <em>TC</em>, <em>74</em>(4), 1377-1391. (<a href='https://doi.org/10.1109/TC.2025.3525606'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the Karatsuba algorithm reduces the complexity of large integer multiplication, the extra additions required minimize its benefits for smaller integers of more commonly-used bitwidths. In this work, we propose the extension of the scalar Karatsuba multiplication algorithm to matrix multiplication, showing how this maintains the reduction in multiplication complexity of the original Karatsuba algorithm while reducing the complexity of the extra additions. Furthermore, we propose new matrix multiplication hardware architectures for efficiently exploiting this extension of the Karatsuba algorithm in custom hardware. We show that the proposed algorithm and hardware architectures can provide real area or execution time improvements for integer matrix multiplication compared to scalar Karatsuba or conventional matrix multiplication algorithms, while also supporting implementation through proven systolic array and conventional multiplier architectures at the core. We provide a complexity analysis of the algorithm and architectures and evaluate the proposed designs both in isolation and in an end-to-end deep learning accelerator system compared to baseline designs and prior state-of-the-art works implemented on the same type of compute platform, demonstrating their ability to increase the performance-per-area of matrix multiplication hardware.},
  archive      = {J_TC},
  author       = {Trevor E. Pogue and Nicola Nicolici},
  doi          = {10.1109/TC.2025.3525606},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1377-1391},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Karatsuba matrix multiplication and its efficient custom hardware implementations},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Asynchronous control based aggregation transport protocol for distributed deep learning. <em>TC</em>, <em>74</em>(4), 1362-1376. (<a href='https://doi.org/10.1109/TC.2025.3525604'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth scale of dataset and model, the training of deep neural networks (DNN) tends to be deployed in a distributed manner. In the large-scale distributed training, the bottlenecks have gradually moved from computational resources to communication process. Recent researches adopt in-network aggregation (INA) that offloads the gradient aggregation process to programmable switches, thereby reducing network traffic amount and transmission latency. Unfortunately, due to the bandwidth competition in shared training clusters, the straggler will slow down the training efficiency of INA. To address this issue, we propose an Asynchronous Control based Aggregation Transport Protocol (AC-ATP), which makes full use uncongested links to transmit gradients and the switch memory to cache gradients from the fast workers to accelerate the gradient aggregation. Meanwhile, AC-ATP performs congestion control according to the transmission progress of worker and the remaining completion time of the job. The evaluation results of real testbed and large-scale simulations show that AC-ATP reduces the aggregate time by up to 68% and speeds up training in real-world benchmark models.},
  archive      = {J_TC},
  author       = {Jin Ye and Yajun Peng and Yijun Li and Zhaoyi Li and Jiawei Huang},
  doi          = {10.1109/TC.2025.3525604},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1362-1376},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Asynchronous control based aggregation transport protocol for distributed deep learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LUNA-CiM: A programmable compute-in-memory fabric for neural network acceleration. <em>TC</em>, <em>74</em>(4), 1348-1361. (<a href='https://doi.org/10.1109/TC.2025.3525601'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compute-in-memory (CiM) has emerged as a promising approach for improving energy efficiency for diverse data-intensive applications. In this paper, we present LUNA-CiM, a lookup table (LUT)-based programmable fabric for flexible and efficient mapping of artificial neural network (ANN) in memory. Its objective is to tackle scalability challenges in LUT-based computation by minimizing hardware, storage elements, and energy consumption. The proposed method utilizes the divide and conquer (D&C) strategy to enhance the scalability of LUT-based computation. For example, in a 4b $\boldsymbol{\times}$ 4b lookup table-based multiplier, as one of the main components in ANN, decomposing high-precision operations into lower-precision counterparts leads to a substantial reduction in area overheads, approximately 73% less compared to conventional LUT-based approaches. Importantly, this efficiency gain is achieved without compromising accuracy. Extensive simulations were conducted to validate the performance of the proposed method. The analysis presented in this paper reveals a noteworthy advancement in energy efficiency, indicating a 58% reduction in energy consumption per computation compared to the conventional lookup table approach. Additionally, the introduced approach demonstrates a 36% improvement in speed over the traditional lookup table approach. These findings highlight notable advancements in performance, showcasing the potential of this inventive method to achieve low power, low-area overhead, and fast computations through the utilization of LUTs within an SRAM array.},
  archive      = {J_TC},
  author       = {Peyman Dehghanzadeh and Ovishake Sen and Baibhab Chatterjee and Swarup Bhunia},
  doi          = {10.1109/TC.2025.3525601},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1348-1361},
  shortjournal = {IEEE Trans. Comput.},
  title        = {LUNA-CiM: A programmable compute-in-memory fabric for neural network acceleration},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AVL function table for LeafHooks insertion with obfuscated control flow integrity. <em>TC</em>, <em>74</em>(4), 1334-1347. (<a href='https://doi.org/10.1109/TC.2024.3524080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control flow is the execution order of individual statements, instructions, or function calls within an imperative program. Malicious operation of control flow (e.g., tampering with normal function addresses) leads to severe consequences such as data leakage and system crash. Control Flow Integrity (CFI) is a defense restricting the execution order of program within Control Flow Graph (CFG). IndexHooks is an existing CFI solution designed against forward function calls tampering (including direct and indirect jump). This solution constructs a read-only linear function table that stores function addresses during compilation. Then, IndexHooks checks the table to make program jump to the correct target address during runtime. However, IndexHooks faces limitations in backtracking CFG construction, which can lead to excessive memory usage; the linear structure of the function table is vulnerable to brute force tampering. Addressing the limitations of IndexHooks, this study develops an obfuscated CFI solution called LeafHooks. LeafHooks is implemented during compilation by the LLVM compiler, which performs static analysis and instrumentation on the LLVM Intermediate Representation (IR) code of a program. We make the following three innovations: 1) we propose a speculation-free identification method for indirect function calls by linear traversing and analyzing codes to obtain legal function information (function address); 2) we save this information into a function table in the form of a Balanced Binary Tree (also known as AVL), enhancing the fuzzification of function addresses to defend against brute force; 3) we design a method to simulate control tamper attacks on ARM64 architecture to verify the ability of LeafHooks to protection. LeafHooks shows less overhead than state-of-the-art solutions and reduces 2.9% and 0.55% overhead on average using UnixBench and Phoronix, respectively.},
  archive      = {J_TC},
  author       = {Sirong Zhao and Guoqi Xie and Chenglai Xiong and Kenli Li and Xuejun Yu and Bo Wan and Yiwen Jiang},
  doi          = {10.1109/TC.2024.3524080},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1334-1347},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AVL function table for LeafHooks insertion with obfuscated control flow integrity},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Flexible job scheduling with spatial-temporal compatibility for in-network aggregation. <em>TC</em>, <em>74</em>(4), 1322-1333. (<a href='https://doi.org/10.1109/TC.2024.3523420'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-Network Aggregation (INA) solutions represent the forefront in advancing All-Reduce, utilizing limited switch memory for efficient gradient aggregation. However, existing INA solutions primarily focus on enhancing aggregation efficiency, often overlooking the efficient utilization of memory. Isolation solutions typically pre-allocate resources for each job, leading to memory wastage due to the uncontrolled use of resources. In contrast, the sharing solutions encounter significant memory contention, resulting in performance degradation within a multi-tenant environment. In this paper, we propose DynaINA, a flexible job scheduler to support multi-tenant training. The core idea of DynaINA is to provide spatial and temporal compatibility between jobs. For spatial compatibility, DynaINA utilizes multiple dynamic memory pools to provide job isolation. For temporal compatibility, DynaINA employs contention-aware job scheduling to facilitate memory sharing. Furthermore, DynaINA prioritizes communication-intensive jobs, leveraging the benefits of INA to enhance overall performance in training clusters. Extensive experiments with popular vision and language models demonstrate that DynaINA reduces training time by up to 65.16% and improves switch memory utilization by up to 85.02% compared to state-of-the-art solutions in a 100Gbps network.},
  archive      = {J_TC},
  author       = {Yulong Li and Wenxin Li and Yuxuan Du and Yinan Yao and Song Zhang and Linxuan Zhong and Keqiu Li},
  doi          = {10.1109/TC.2024.3523420},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1322-1333},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Flexible job scheduling with spatial-temporal compatibility for in-network aggregation},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Qu-trefoil: Large-scale quantum circuit simulator working on FPGA with SATA storages. <em>TC</em>, <em>74</em>(4), 1306-1321. (<a href='https://doi.org/10.1109/TC.2024.3521546'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum circuits are fundamental components of quantum computing, and state-vector-based quantum circuit simulation is a widely used technique for tracking qubit behavior throughout circuit evolution. However, simulating a circuit with $n$ qubits requires $2^{n+4}$ bytes of memory, making simulations of more than 40 qubits feasible only on supercomputers. To address this limitation, we propose the Qu-Trefoil, a system designed for large-scale quantum circuit simulations on an FPGA-based platform called Trefoil. Trefoil is a multi-FPGA system connected to eight storage subsystems, each equipped with 32 SATA disks. Qu-Trefoil integrates a suite of HLS-based universal quantum gates, including Clifford gates (Hadamard (H), Pauli-Z (Z), Phase (S), Controlled-NOT (CNOT)), the T gate, and unitary matrix computation, along with HDL-designed modules for system-wide integration. Our extensive evaluation demonstrates the system's robustness and flexibility, covering quantum gate performance, chunk size, disk extensibility, and efficiency across different SATA generations. We successfully simulated quantum circuits with over 43 qubits, which required more than 128 TB of memory, in approximately 3.72 to 13.06 hours on a single storage subsystem equipped with one FPGA. This achievement represents a significant milestone in the advancement of quantum computing simulations. Furthermore, thanks to its unique architecture, Qu-Trefoil is more accessible, flexible, and cost-efficient than other existing simulators for large-scale quantum circuit simulations, making it a viable option for researchers with limited access to supercomputers.},
  archive      = {J_TC},
  author       = {Kaijie Wei and Hideharu Amano and Ryohei Niwase and Yoshiki Yamaguchi and Takefumi Miyoshi},
  doi          = {10.1109/TC.2024.3521546},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1306-1321},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Qu-trefoil: Large-scale quantum circuit simulator working on FPGA with SATA storages},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new ECC configuration method for DRAM system considering metadata. <em>TC</em>, <em>74</em>(4), 1293-1305. (<a href='https://doi.org/10.1109/TC.2024.3521545'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new ECC (error correcting code) solution for DRAM (dynamic random access memory) in computing systems is proposed. Existing papers on ECC for DRAM systems do not consider storage space for metadata. The methodology proposed in this paper considers storing metadata attached to a cacheline data in DRAM. We infer the maximum number of single-chip error correction cases that a linear code can support while considering metadata storage space. This can be said to be the maximum theoretical correction probability for a single chip error. A methodology to construct a code with maximum single-chip error correction is presented. A decoding methodology for the code is proposed. The proposed ECC solution can correct not only single chip failure but also additional small bit errors. We calculate the correction capability of the proposed methodology and verified it through simulation. The encoder and decoder hardware were synthesized and compared with existing methodologies.},
  archive      = {J_TC},
  author       = {Jaeil Lim and Jaewon Chung and Donghun Jeong and Daegeun Jee and Euicheol Lim},
  doi          = {10.1109/TC.2024.3521545},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1293-1305},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A new ECC configuration method for DRAM system considering metadata},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive scan test cost model to optimize the production of very large SoCs. <em>TC</em>, <em>74</em>(4), 1278-1292. (<a href='https://doi.org/10.1109/TC.2024.3521246'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the trade-offs of reducing scan test patterns during Wafer Sort, accepting additional packaging costs, and screening more chips during Package Tests. Previous works proposed ways of selecting or reordering patterns to bring the most efficient to the left. Unlike such studies, this work quantifies the benefit of removing patterns directly from the tail of any pattern set. The paper elaborates on novel formulas to propose a comprehensive cost model that combines yield, Wafer Sort, packaging, and Package Test costs. The model evolves from known concepts by assuming that mass production defectivity is non-uniformly distributed over the die population and accounts for sacrificial lots to extract guiding information. It is shown that reducing patterns at Wafer Sort is beneficial under certain conditions of yield, fault coverage, and considering equipment and production costs. The model accurately estimates the number of patterns to remove for maximum gain in these cases. As a further by-product, the paper shows that a significant cost advantage can be achieved if pattern generation is guided based on the basics of the non-uniform failure distribution. This approach is validated with an academic benchmark and by observing six months of production for a real-world microcontroller by STMicroelectronics.},
  archive      = {J_TC},
  author       = {Giusy Iaria and Paolo Bernardi and Claudia Bertani and Lorenzo Cardone and Giuseppe Garozzo and Vincenzo Tancorre},
  doi          = {10.1109/TC.2024.3521246},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1278-1292},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A comprehensive scan test cost model to optimize the production of very large SoCs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of a universal decoder model based on DNA winner-takes-all neural networks. <em>TC</em>, <em>74</em>(4), 1267-1277. (<a href='https://doi.org/10.1109/TC.2024.3521230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {DNA computing has proven to possess strong parallel processing capabilities, offering notable advantages for multi-objective computation. Traditional, complex nonlinear DNA molecular logic circuits require the pre-construction of basic logic gates, followed by their cascading to achieve logic functions. However, as the number of cascade levels increases, more DNA strands are required to amplify and recover signals, causing the system's reaction time to grow exponentially. This paper introduces a novel approach for building complex nonlinear digital logic circuits using DNA winner-take-all neural networks. The logic circuit comprises four computational modules: weight multiplication, summation, competitive annihilation, and reporting. First, an annihilation strand is designed to control the reaction rate between two competing signals, resolving the interference between weight multiplication and competitive annihilation. Second, new grouping strategies—complementary annihilation, equal annihilation, and denoise annihilation—are introduced. These strategies exponentially reduce the number of competitive strands and significantly decrease system reaction time. The effect becomes more pronounced as the number of input patterns increases. Finally, 2-4, 3-8, and 4-16 decoder circuits are built using the winner-take-all neural networks, and an $\boldsymbol{n}\boldsymbol{-}\boldsymbol{2}^{\boldsymbol{n}}$ universal decoder model is further developed. This study presents an effective method for implementing complex nonlinear logic circuits through DNA strand displacement reactions.},
  archive      = {J_TC},
  author       = {Chun Huang and Jiaying Shao and Baolei Peng and Qingshuang Guo and Panlong Li and Junwei Sun and Yanfeng Wang},
  doi          = {10.1109/TC.2024.3521230},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1267-1277},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Design of a universal decoder model based on DNA winner-takes-all neural networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Microarchitectural attacks and mitigations on retire resources in modern processors. <em>TC</em>, <em>74</em>(4), 1253-1266. (<a href='https://doi.org/10.1109/TC.2024.3521225'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern processors, the Retire Control Unit (RCU) is responsible for receiving the µops decoded from the frontend and retiring the completed µops in order through the retirement. Consequently, the retirement may stall differently depending on the execution time of the first instruction in the RCU, causing varying stalling in the RCU reception. Moreover, We find that the RCU reception in AMD processors and retirement in Intel processors are shared between two logical cores of the same physical core, allowing an attacker to infer the instructions executed by another logical core based on its retire resources efficiency. Based on these findings, we introduce the retirement covert channel on Intel processors and the RCU covert channel on AMD processors. Furthermore, we explores additional applications of retire resources. On the one hand, we combined the misprediction penalty mechanism to apply our covert channels to the Spectre attacks. On the other hand, based on the principle that different programs result in varied usage patterns of retire resources, we propose an attack method that leverages the retire resources to infer the program run by the victim. Finally, we design the corresponding mitigations and extend our mitigation to fetch unit to reduce the performance overhead.},
  archive      = {J_TC},
  author       = {Ke Xu and Ming Tang and Quancheng Wang and Han Wang},
  doi          = {10.1109/TC.2024.3521225},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1253-1266},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Microarchitectural attacks and mitigations on retire resources in modern processors},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Big-computing and little-storing STT-MRAM PIM architecture with charge domain based MAC operation. <em>TC</em>, <em>74</em>(4), 1239-1252. (<a href='https://doi.org/10.1109/TC.2024.3517754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spin transfer torque magnetic random access memory (STT-MRAM) is a promising memory technology for processing in memory (PIM) thanks to its high endurance and relatively low device-to-device and cycle-to-cycle variations. However, the low OFF/ON ratio of STT device limits the number of active row-lines during multiply-accumulate (MAC) operations, degrading energy efficiency and computation speed. In this paper, we present an energy efficient and high speed Big-computing and Little-storing STT-MRAM PIM (BCLS-SP) architecture, which can increase the number of active row-lines with almost no area overhead. In the BCLS-SP architecture, a charge domain-based STT-MRAM PIM (CD-SP) structure is employed to concurrently activate many row-lines by improving MAC operation reliability. Filter-wise weight compression (FWC) and weight sharing (WS) are also devised to compress the weights stored in CD-SP, thus reducing area cost. In addition, the proposed architecture performs MAC operations with skipping zero-valued input (SZI) and zero-conversion scheme (ZCS) for better energy efficiency and performance. The simulations using 28nm CMOS process show that the BCLS-SP architecture shows energy reduction of 29% and performance improvement of 3.6 compared to the recent memristive device-based PIM using weight compression and input skipping.},
  archive      = {J_TC},
  author       = {Yunho Jang and Dongsu Kim and Yeseul Kim and Jongsun Park},
  doi          = {10.1109/TC.2024.3517754},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1239-1252},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Big-computing and little-storing STT-MRAM PIM architecture with charge domain based MAC operation},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware accelerated vision transformer via heterogeneous architecture design and adaptive dataflow mapping. <em>TC</em>, <em>74</em>(4), 1224-1238. (<a href='https://doi.org/10.1109/TC.2024.3517751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision transformer (ViT) models have demonstrated remarkable advantages in visual tasks. However, the ViT model contains various types of operators, and its sophisticated model structure imposes substantial computational complexity and storage burden. Existing hardware solutions still fail to fully unleash the ViT acceleration potential due to the mismatch between operators and hardware architectures, suffering from inefficient dataflow mapping. This work proposes HDViT, a full-fledged heterogeneous hardware accelerator on FPGA, to enhance the ViT acceleration by comprehensively analyzing and addressing the challenges of heterogeneous architecture design. Specifically, HDViT first develops a heterogeneous architecture design that is composed of multiple processing engines (PEs) to accelerate various operators in the ViT model. Then, HDViT devises a hybrid-oriented dataflow mapping strategy to reduce data transmission granularity and alleviate storage resource pressure. Lastly, to achieve the latency balancing among multiple PEs, we formulate the HDViT architecture and implement an automated exploration process to identify optimized parallelism parameters that satisfy computation and storage demands while enhancing the heterogeneous architectural performance. Experimental results indicate that HDViT achieves significant performance speedups of 2.16$\times$ and 3.51$\times$ compared to previous heterogeneous and unified accelerators, respectively. HDViT also achieves a maximum of 98.46% hardware utilization.},
  archive      = {J_TC},
  author       = {Yingxue Gao and Teng Wang and Lei Gong and Chao Wang and Dong Dai and Yang Yang and Xianglan Chen and Xi Li and Xuehai Zhou},
  doi          = {10.1109/TC.2024.3517751},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1224-1238},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hardware accelerated vision transformer via heterogeneous architecture design and adaptive dataflow mapping},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed sketch deployment for software switches. <em>TC</em>, <em>74</em>(4), 1210-1223. (<a href='https://doi.org/10.1109/TC.2024.3517749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network measurement is critical for various network applications, but scaling measurement techniques to the network-wide level is challenging for existing sketch-based solutions. In software switches, centralized deployment provides low resource usage but suffers from poor load balancing. In contrast, collaborative measurement achieves load balancing through flow distribution across software switches but requires high resource usage. This paper presents a novel distributed deployment framework that overcomes the limitations above. First, our framework is lightweight such that it splits sketches into segments and allocates them across forwarding paths to minimize resource usage and achieve load balancing. This also enables per-packet load balancing by distributing computations across software switches. Second, through a novel collaborative strategy, our framework achieves finer-grained flow distribution and further optimizes load balancing. Third, we further optimize load balancing by eliminating the mutual influence among forwarding paths. We evaluate the proposed framework on various network topologies and different sketches. Results indicate our solution matches the load balancing of collaborative measurement while approaching the low resource usage of centralized deployment. Moreover, it achieves superior performance in per-packet load balancing, which is not considered in previous deployment solutions.},
  archive      = {J_TC},
  author       = {Kejun Guo and Fuliang Li and Jiaxing Shen and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2024.3517749},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1210-1223},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Distributed sketch deployment for software switches},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AutoPipe-H: A heterogeneity-aware data-paralleled pipeline approach on commodity GPU servers. <em>TC</em>, <em>74</em>(4), 1196-1209. (<a href='https://doi.org/10.1109/TC.2024.3517748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the data-parallel pipeline approach has been widely used in training DNN models on commodity GPU servers. However, there are still three challenges for hybrid parallelism on commodity GPU servers: i) a balanced model partition is crucial for efficiency, whereas prior works lack a sound solution to generate a balanced partition automatically; ii) an orchestrated device mapping is essential to reduce communication contention, however, prior works ignore server heterogeneity, exacerbating communication contention; iii) the startup overhead is inevitable and especially significant for deep pipelines, which is an essential source of pipeline bubbles and severely affects pipeline scalability. We propose AutoPipe-H to solve these three problems, which contains i) a pipeline partitioner component for automatically and quickly generating a balanced sub-block partition scheme; ii) a device mapping component that assigns pipeline stages to devices, considering server heterogeneity, to reduce communication contention; and iii) a distributed training runtime component that reduces pipeline startup overhead by splitting the micro-batch evenly. The experimental results show that AutoPipe-H can accelerate training by up to 1.26x over the hybrid parallelism framework DAPPLE and Piper, with a 2.73x-12.7x improvement in the partition balance and an order-of-magnitude time reduction in partition scheme searching.},
  archive      = {J_TC},
  author       = {Weijie Liu and Kai Lu and Zhiquan Lai and Shengwei Li and Keshi Ge and Dongsheng Li and Xicheng Lu},
  doi          = {10.1109/TC.2024.3517748},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1196-1209},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AutoPipe-H: A heterogeneity-aware data-paralleled pipeline approach on commodity GPU servers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A context-awareness and hardware-friendly sparse matrix multiplication kernel for CNN inference acceleration. <em>TC</em>, <em>74</em>(4), 1182-1195. (<a href='https://doi.org/10.1109/TC.2024.3517745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparsification technology is crucial for deploying convolutional neural networks in resource-constrained environments. However, the efficiency of sparse models is hampered by irregular memory access patterns in sparse matrix multiplication kernels. Hardware-level support for 2:4 granularity in sparse tensor cores presents an opportunity for designing efficient sparse matrix multiplication kernels. Existing approaches often involve adjusting sparse structures or secondary sparsification, introducing additional computational errors. To tackle this challenge, we introduce a flexible 2:4 structured adaptive sparse matrix multiplication (FS-AMM) method, a hardware-friendly sparse matrix multiplication kernel that leverages model context to accelerate convolutional neural networks. First, we propose a model context-aware matrix pre-processing method that employs heuristic algorithms to estimate a loss of accuracy due to weight sparsity at each layer. Second, we design a hardware-friendly sparse storage format that combines 2:4 sparse and dense storage formats, enabling more versatile sparsity ratio selection. Third, we implement efficient matrix multiplication kernels to optimize GPU utilization. Finally, experimental results on A100 GPUs show that our method effectively utilizes the sparse tensor kernel and obtains an average 3.09 times speedup ratio compared to other sparse methods while maintaining a high accuracy.},
  archive      = {J_TC},
  author       = {Haotian Wang and Yan Ding and Yumeng Liu and Weichen Liu and Chubo Liu and Wangdong Yang and Kenli Li},
  doi          = {10.1109/TC.2024.3517745},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1182-1195},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A context-awareness and hardware-friendly sparse matrix multiplication kernel for CNN inference acceleration},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A GPU-enabled framework for light field efficient compression and real-time rendering. <em>TC</em>, <em>74</em>(4), 1168-1181. (<a href='https://doi.org/10.1109/TC.2024.3517743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time rendering offers instantaneous visual feedback, making it crucial for mixed-reality applications. The light field captures both light intensity and direction in a 3D environment, serving as a data-rich medium to enhance mixed-reality experiences. However, two major challenges remain: 1) current light field rendering techniques are unsuitable for real-time computation, and 2) existing real-time methods cannot efficiently process high-dimensional light field data on GPU platforms. To overcome these challenges, we propose an framework utilizing a compact neural representation of light field data, implemented on a GPU platform for real-time rendering. This framework provides both compact storage and high-fidelity real-time computation. Specifically, we introduce a ray global alignment strategy to simplify the framework and improve practicality. This strategy enables the learning of an optimal embedding for all local rays in a globally consistent way, removing the need for camera pose calculations. To achieve effective compression, the neural light field is employed to map each embedded ray to its corresponding color. To enable real-time rendering, we design a novel super-resolution network to enhance rendering speed. Extensive experiments demonstrate that our framework significantly enhances compression efficiency and real-time rendering performance, achieving nearly 50$\mathbf{\times}$ compression ratio and 100 FPS rendering.},
  archive      = {J_TC},
  author       = {Mingyuan Zhao and Hao Sheng and Rongshan Chen and Ruixuan Cong and Tun Wang and Zhenglong Cui and Da Yang and Shuai Wang and Wei Ke},
  doi          = {10.1109/TC.2024.3517743},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1168-1181},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A GPU-enabled framework for light field efficient compression and real-time rendering},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Trident: The acceleration architecture for high-performance private set intersection. <em>TC</em>, <em>74</em>(4), 1152-1167. (<a href='https://doi.org/10.1109/TC.2024.3517738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Private Set Intersection (PSI) is imperative in discovering the properties of the same data owned by two competitive parties, without revealing anything else of their respective data asset. Existing PSI solutions such as APSI and ORI-PSI suffer from severe communication and computation overhead due to inefficient communication and FHE polynomial evaluation, which hinders their deployment in practice. This issue is evident in both the upper-level protocol and the lower-level hardware platform. In this paper, we propose a novel software/hardware co-design acceleration architecture for PSI, termed as “Trident”, which includes two tightly coupled segments: from the protocol perspective, we investigate existing bottlenecks and propose a new PSI protocol with significantly less communication and computation under the security guarantee; besides, we re-architect the hardware platform by designing a PSI-specific accelerator, implemented with both FPGA and ASIC, targeting the key operations in the proposed protocol. We build a real-world experimental environment with two instantiated parties to verify the acceleration architecture, and highlight the following results: (1) up to 130$\boldsymbol{\times}$/145$\boldsymbol{\times}$ speedup for the computation of receiver and sender parties; (2) up to 37$\boldsymbol{\times}$ reduction of communication overhead. (3) up to 93,651$\boldsymbol{\times}$ and 74,326$\boldsymbol{\times}$ higher energy efficiency over the CPU-based ORI-PSI and APSI, respectively.},
  archive      = {J_TC},
  author       = {Jinkai Zhang and Yinghao Yang and Zhe Zhou and Zhicheng Hu and Xin Zhao and Liang Chang and Hang Lu and Xiaowei Li},
  doi          = {10.1109/TC.2024.3517738},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1152-1167},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Trident: The acceleration architecture for high-performance private set intersection},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical hashing: A dynamic hashing method with low write amplification and high performance for non-volatile memory. <em>TC</em>, <em>74</em>(4), 1138-1151. (<a href='https://doi.org/10.1109/TC.2024.3517737'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hashing method is widely used as the index structure, which can be stored in NVM to improve the application performance. However, existing hashing methods may cause high extra write amplification to NVM and bring high additional storage overhead on NVM while providing low request performance. To solve these problems, we have proposed a dynamic hashing method called Hierarchical Hashing, whose basic idea is to leverage a novel hash collision resolution mechanism that can dynamically expand the size of the hash table. Hierarchical Hashing can incur no extra write amplification to NVM when resolving hash collisions. Additionally, it can directly address all cells when resizing the hash table, thereby avoiding the additional storage overhead caused by non-addressable linked lists. Furthermore, the request performance can be improved as all cells of the hash table are addressable when resizing to resolve hash collisions. The experimental results demonstrate that Hierarchical Hashing brings no extra write amplification to NVM and achieves nearly 90% space utilization and high request performance while providing 99% memory utilization, compared with existing representative hashing methods.},
  archive      = {J_TC},
  author       = {Jinquan Wang and Zhisheng Huo and Limin Xiao and Jinqian Yang and Jiantong Huo and Minyi Guo},
  doi          = {10.1109/TC.2024.3517737},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1138-1151},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hierarchical hashing: A dynamic hashing method with low write amplification and high performance for non-volatile memory},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards optimal customized architecture for heterogeneous federated learning with contrastive cloud-edge model decoupling. <em>TC</em>, <em>74</em>(4), 1123-1137. (<a href='https://doi.org/10.1109/TC.2024.3514302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning, as a promising distributed learning paradigm, enables collaborative training of a global model across multiple network edge clients without the need for central data collecting. However, the heterogeneity of edge data distribution drags the model towards the local minima, which can be distant from the global optimum. Such heterogeneity often leads to slow convergence and substantial communication overhead. To address these issues, we propose a novel federated learning framework called FedCMD, a model decoupling tailored to the Cloud-edge supported federated learning that separates deep neural networks into a body for capturing shared representations in Cloud and a personalized head for migrating data heterogeneity. Our motivation is that, by the deep investigation of the performance of selecting different neural network layers as the personalized head, we found rigidly assigning the last layer as the personalized head in current studies is not always optimal. Instead, it is necessary to dynamically select the personalized layer that maximizes the training performance by taking the representation difference between neighbor layers into account. To find the optimal personalized layer, we utilize the low-dimensional representation of each layer to contrast feature distribution transfer and introduce a Wasserstein-based layer selection method, aimed at identifying the best-match layer for personalization. Additionally, a weighted global aggregation algorithm is proposed based on the selected personalized layer for the practical application of FedCMD. Extensive experiments on ten benchmarks demonstrate the efficiency and superior performance of our solution compared with nine state-of-the-art solutions. All code and results are available at https://github.com/elegy112138/FedCMD.},
  archive      = {J_TC},
  author       = {Xingyan Chen and Tian Du and Mu Wang and Tiancheng Gu and Yu Zhao and Gang Kou and Changqiao Xu and Dapeng Oliver Wu},
  doi          = {10.1109/TC.2024.3514302},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1123-1137},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards optimal customized architecture for heterogeneous federated learning with contrastive cloud-edge model decoupling},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A data-centric software-hardware co-designed architecture for large-scale graph processing. <em>TC</em>, <em>74</em>(4), 1109-1122. (<a href='https://doi.org/10.1109/TC.2024.3514292'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph processing plays an important role in many practical applications. However, the inherent characteristics of graph processing, including random memory access and the low computation-to-communication ratio, make it difficult to efficiently execute on traditional computing architectures, such as CPUs and GPUs. Near-memory computing has the characteristics of low latency and high bandwidth. It is widely regarded as a promising direction for designing graph processing accelerators. However, the storage space of a single device cannot meet the demand of large-scale graph processing. Using multiple devices will bring lots of inter-device data transmission, which may counteract the benefits of near-memory computing. To fundamentally reduce the data transmission overhead, we propose a data-centric graph processing framework for systems with multiple near-memory computing devices. The framework uses a data-centric programming model as the software hardware interface. For software, we propose an optimized data flow and a heuristic multi-step weighted maximum matching algorithm to achieve efficient inter-device communication and ensure load balancing. For hardware, we design a data reuse driven task controller and a data type-aware on-chip memory, which can effectively improve the utilization of the on-chip memory. Compared with the two most recent near-memory graph accelerators, our framework significantly reduces energy consumption and inter-device communication.},
  archive      = {J_TC},
  author       = {Zerun Li and Xiaoming Chen and Yuxin Yang and Feng Min and Xiaoyu Zhang and Yinhe Han},
  doi          = {10.1109/TC.2024.3514292},
  journal      = {IEEE Transactions on Computers},
  month        = {4},
  number       = {4},
  pages        = {1109-1122},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A data-centric software-hardware co-designed architecture for large-scale graph processing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WaWoT: Towards flexible and efficient web of things services via WebAssembly on resource-constrained IoT devices. <em>TC</em>, <em>74</em>(3), 1094-1108. (<a href='https://doi.org/10.1109/TC.2024.3500385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web of Things (WoT) is an emerging concept to connect IoT devices to the web using standard interfaces. This provides interoperability between different IoT platforms and enables seamless integration with web and cloud services. However, running sophisticated web services directly on resource-constrained IoT devices is challenging due to limitations in memory, computation, and energy. This paper proposes WaWoT, a Wasm-based framework for flexible and efficient Web of Things services. WaWoT allows flexible WoT service development using annotations and automatic partitioning. It also enables dynamic service migration using WebAssembly modules to adapt placement between IoT devices and web clients. We also introduce an ahead-of-time compiler optimized for low memory usage through techniques like streamed compilation and trimming. For energy efficiency, we use optimizations like bulk instruction writing and direct I/O accessing. Safety is ensured through compile-time and run-time analyses to guarantee sandboxed execution. Evaluations demonstrate WaWoT exhibits better flexibility than existing WoT development approaches. Furthermore, WaWoT can also reduce RAM usage by 84.9x and energy consumption by 1.9x-4.9x over existing WebAssembly runtimes. Overall, it enables efficient, safe, and flexible WoT services on constrained IoT devices.},
  archive      = {J_TC},
  author       = {Borui Li and Hongchang Fan and Yi Gao and Wei Dong},
  doi          = {10.1109/TC.2024.3500385},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {1094-1108},
  shortjournal = {IEEE Trans. Comput.},
  title        = {WaWoT: Towards flexible and efficient web of things services via WebAssembly on resource-constrained IoT devices},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReViT: Vision transformer accelerator with reconfigurable semantic-aware differential attention. <em>TC</em>, <em>74</em>(3), 1079-1093. (<a href='https://doi.org/10.1109/TC.2024.3504263'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While vision transformers (ViTs) have continued to achieve new milestones in computer vision, their complicated network architectures with high computation and memory costs have hindered their deployment on resource-limited edge devices. Some customized accelerators have been proposed to accelerate the execution of ViTs, achieving improved performance with reduced energy consumption. However, these approaches utilize flattened attention mechanisms and ignore the inherent hierarchical visual semantics in images. In this work, we conduct a thorough analysis of hierarchical visual semantics in real-world images, revealing opportunities and challenges of leveraging visual semantics to accelerate ViTs. We propose ReViT, a systematic algorithm and architecture co-design approach, which aims to exploit the visual semantics to accelerate ViTs. Our proposed algorithm can leverage the same semantic class with strong feature similarity to reduce computation and communication in a differential attention mechanism, and support the semantic-aware attention efficiently. A novel dedicated architecture is designed to support the proposed algorithm and translate it into performance improvements. Moreover, we propose an efficient execution dataflow to alleviate workload imbalance and maximize hardware utilization. ReViT opens new directions for accelerating ViTs by exploring the underlying visual semantics of images. ReViT gains an average of 2.3$\boldsymbol{\times}$ speedup and 3.6$\boldsymbol{\times}$ energy efficiency over state-of-the-art ViT accelerators.},
  archive      = {J_TC},
  author       = {Xiaofeng Zou and Cen Chen and Hongen Shao and Qinyu Wang and Xiaobin Zhuang and Yangfan Li and Keqin Li},
  doi          = {10.1109/TC.2024.3504263},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {1079-1093},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ReViT: Vision transformer accelerator with reconfigurable semantic-aware differential attention},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Access-pattern hiding search over encrypted databases by using distributed point functions. <em>TC</em>, <em>74</em>(3), 1066-1078. (<a href='https://doi.org/10.1109/TC.2024.3504288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encrypted databases have been extensively studied with the increasing concern of data privacy in cloud services. For practical efficiency, most encrypted database systems are built under Dynamic Searchable Symmetric Encryption (DSSE) schemes to support fast query and update over encrypted data. However, DSSE schemes allow leakages in their security frameworks, especially access-pattern leakages (i.e., the search results corresponding to queried keywords), which lead to various attacks to infer sensitive information of queries and databases. Existing oblivious-access techniques, such as Oblivious RAM and differential privacy, suffer from excessive communication overhead and loss of query accuracy. In this paper, we propose a new DSSE scheme that enables access-pattern hiding keyword search and update operations. Servers can obliviously query and update databases within only a single communication round. Our building block is based on the Distributed Point Function (DPF), an advanced secret sharing technique that provides provable security guarantees against adversaries with arbitrary background knowledge. Moreover, we devise a novel update protocol that integrates DPF and Somewhat Homomorphic Encryption (SHE) such that servers can obliviously update their local data. We formally analyze the security and implement the prototype. The comprehensive experimental results demonstrate the security and efficiency of our scheme.},
  archive      = {J_TC},
  author       = {Hongcheng Xie and Yu Guo and Yinbin Miao and Xiaohua Jia},
  doi          = {10.1109/TC.2024.3504288},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {1066-1078},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Access-pattern hiding search over encrypted databases by using distributed point functions},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feynman meets turing: The uncomputability of quantum gate-circuit emulation and concatenation. <em>TC</em>, <em>74</em>(3), 1053-1065. (<a href='https://doi.org/10.1109/TC.2024.3506861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the feasibility of computing quantum gate-circuit emulation (QGCE) and quantum gate-circuit concatenation (QGCC) on digital hardware. QGCE serves the purpose of rewriting gate circuits comprised of gates from a varying input gate set to gate circuits formed of gates from a fixed target gate set. Analogously, QGCC serves the purpose of finding an approximation to the concatenation of two arbitrary elements of a varying list of input gate circuits in terms of another element from the same list. Problems of this kind occur regularly in quantum computing and are often assumed an easy task for the digital computers controlling the quantum hardware. Arguably, this belief is due to analogical reasoning: The classical Boolean equivalents of QGCE and QGCC are natively computable on digital hardware. In the present paper, we present two insights in this regard: Upon applying a rigorous theory of computability, QGCE and QGCC turn out to be uncomputable on digital hardware. The results remain valid when we restrict the set of feasible inputs for the relevant functions to one parameter families of fixed gate sets. Our results underline the possibility that several ideas from quantum-computing theory may require a rethinking to become feasible for practical implementation.},
  archive      = {J_TC},
  author       = {Holger Boche and Yannik N. Böck and Zoe Garcia del Toro and Frank H. P. Fitzek},
  doi          = {10.1109/TC.2024.3506861},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {1053-1065},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Feynman meets turing: The uncomputability of quantum gate-circuit emulation and concatenation},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lock-free triangle counting on GPU. <em>TC</em>, <em>74</em>(3), 1040-1052. (<a href='https://doi.org/10.1109/TC.2024.3504295'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the triangles of large scale graphs is a fundamental graph mining task in many applications, such as motif detection, microscopic evolution, and link prediction. The recent works on triangle counting can be classified into merge-based or binary search-based paradigms. The merge-based triangle counting paradigm locates the triangles using the set intersection operation, which suffers from the random memory access problem. The binary search-based triangle counting paradigm sets the neighbors of the source vertex of an edge as the lookup array and searches the neighbors of the destination vertex. There are lots of expensive lock operations needed in the binary search-based paradigm, which leads to low thread efficiency. In this paper, we aim to improve the triangle counting efficiency on GPU by designing a lock-free policy named Skiff to implement a hash-based triangle counting algorithm. In Skiff, we first design a hash trie data layout to meet the coalesced memory access model and then propose a lock-free policy to reduce the conflicts of the hash trie. In addition, we use a level array to manage the index of the hash trie to make sure the nodes of the hash trie can be quickly located. Furthermore, we implement a CTA thread organization model to reduce the load imbalance of the real-world graphs. We conducted extensive experiments on NVIDIA GPUs to show the performance of Skiff. The results show that Skiff can achieve a good system performance improvement than the state-of-the-art (SOTA) works.},
  archive      = {J_TC},
  author       = {Zhigao Zheng and Guojia Wan and Jiawei Jiang and Chuang Hu and Hao Liu and Shahid Mumtaz and Bo Du},
  doi          = {10.1109/TC.2024.3504295},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {1040-1052},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lock-free triangle counting on GPU},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NPC: A non-conflicting processing-in-memory controller in DDR memory systems. <em>TC</em>, <em>74</em>(3), 1025-1039. (<a href='https://doi.org/10.1109/TC.2024.3477981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing-in-Memory (PIM) has emerged as a promising solution to address the memory wall problem. Existing memory interfaces must support new PIM commands to utilize PIM, making the definition of PIM commands according to memory modes a major issue in the development of practical PIM products. For performance and OS-transparency, the memory controller is responsible for changing the memory mode, which requires modifying the controller and resolving conflicts with existing functionalities. Additionally, it must operate to minimize mode transition overhead, which can cause significant performance degradation. In this study, we present NPC, a memory controller designed for mode transition PIM that delivers PIM commands via the DDR interface. NPC issues PIM commands while transparently changing the memory mode with a dedicated scheduling policy that reduces the number of mode transitions with aggregative issuing. Moreover, existing functions, such as refresh, are optimized for PIM operation. We implement NPC in hardware and develop a PIM emulation system to validate it on FPGA platforms. Experimental results reveal that NPC is compatible with existing interfaces and functionality, and the proposed scheduling policy improves performance by 2.2$\boldsymbol{\times}$ with balanced fairness, achieving up to 97% of the ideal performance. These findings have the potential to aid the application of PIM in real systems and contribute to the commercialization of mode transition PIM.},
  archive      = {J_TC},
  author       = {Seungyong Lee and Sanghyun Lee and Minseok Seo and Chunmyung Park and Woojae Shin and Hyuk-Jae Lee and Hyun Kim},
  doi          = {10.1109/TC.2024.3477981},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {1025-1039},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NPC: A non-conflicting processing-in-memory controller in DDR memory systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accurate and reliable energy measurement and modelling of data transfer between CPU and GPU in parallel applications on heterogeneous hybrid platforms. <em>TC</em>, <em>74</em>(3), 1011-1024. (<a href='https://doi.org/10.1109/TC.2024.3504262'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing energy-efficient software that leverages application-level energy optimization techniques is essential to tackle the pressing technological challenge of energy efficiency on modern heterogeneous computing platforms. While energy modelling and optimization of computations have received considerable attention in energy research, there remains a significant gap in the energy modelling of data transfer between computing devices on heterogeneous hybrid platforms. Our study aims to fill this crucial gap. In this work, we comprehensively study the energy consumption of data transfer between a host CPU and a GPU accelerator on heterogeneous hybrid platforms using the three mainstream energy measurement methods: (a) System-level physical measurements based on external power meters (ground-truth), (b) Measurements using on-chip power sensors, and (c) Energy predictive models. The ground-truth method is accurate but prohibitively time-consuming. While the on-chip sensors in Intel multicore CPU processors are inaccurate, the Nvidia GPU sensors do not capture data transfer activity. Therefore, we focus on the third approach and propose a novel methodology to select a small subset of performance events that effectively capture all the energy consumption activities during a data transfer and develop accurate linear energy predictive models employing the shortlisted performance events. Finally, we develop independent and accurate runtime pluggable software energy sensors based on our proposed energy predictive models that employ disjoint sets of performance events to estimate the dynamic energy of computations and data transfers. We employ the sensors to predict the energy consumption of computations and data transfer between a host CPU and two A40 Nvidia GPUs in three parallel scientific applications, and the high accuracy (average prediction error of 5%) of our sensors’ predictions further underscores their practical relevance.},
  archive      = {J_TC},
  author       = {Hafiz Adnan Niaz and Ravi Reddy Manumachu and Alexey Lastovetsky},
  doi          = {10.1109/TC.2024.3504262},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {1011-1024},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Accurate and reliable energy measurement and modelling of data transfer between CPU and GPU in parallel applications on heterogeneous hybrid platforms},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A parallel computing scheme utilizing memristor crossbars for fast corner detection and rotation invariance in the ORB algorithm. <em>TC</em>, <em>74</em>(3), 996-1010. (<a href='https://doi.org/10.1109/TC.2024.3504817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Oriented FAST and Rotated BRIEF (ORB) algorithm plays a crucial role in rapidly extracting image keypoints. However, in the domain of high-frame-rate real-time applications, the algorithm faces challenges of the speed and computational efficiency with the increase in both the size and quantity of images. To address this issue, an ORB algorithm accelerator based on a computing-in-memory (CIM) circuit is firstly proposed in this paper, which replaces the iterative calculations in traditional methods with one-step parallel analog computation. The proposed accelerator improves algorithm computational efficiency through CIM technology and enhances algorithm speed through parallel computation. Simulation demonstrate that the proposed method exhibits an average processing speed 22 $\boldsymbol{\times}$ faster than traditional methods and obtains more uniform corners distribution in large-scale images.},
  archive      = {J_TC},
  author       = {Qinghui Hong and Haoyou Jiang and Pingdan Xiao and Sichun Du and Tao Li},
  doi          = {10.1109/TC.2024.3504817},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {996-1010},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A parallel computing scheme utilizing memristor crossbars for fast corner detection and rotation invariance in the ORB algorithm},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-radix generalized hyperbolic CORDIC and its hardware implementation. <em>TC</em>, <em>74</em>(3), 983-995. (<a href='https://doi.org/10.1109/TC.2024.3512183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a high-radix generalized hyperbolic coordinate rotation digital computer (HGH-CORDIC). This algorithm not only computes logarithmic and exponential functions with any fixed base but also significantly reduces the number of iterations required compared to traditional CORDIC methods. Initially, we present the general iteration formulas for HGH-CORDIC. Subsequently, we discuss its pivotal convergence properties and selection criteria, exemplifying these with commonly used cases. Through extensive software simulations, we validate the theoretical foundations of our approach. Finally, we explore efficient hardware implementation strategies. Our analysis indicates that, relative to state-of-the-art radix-2 GH-CORDIC, the proposed HGH-CORDIC can decrease the number of iterations by more than $50\%$ while maintaining comparable accuracy. Synthesized under the 28nm CMOS technology, the reports show that the reference circuit can save about $40\%$ area and power consumption averagely for $2^{x}$ and $log_{2}x$ calculations compared with the latest CORDIC method.},
  archive      = {J_TC},
  author       = {Hui Chen and Lianghua Quan and Ke Chen and Weiqiang Liu},
  doi          = {10.1109/TC.2024.3512183},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {983-995},
  shortjournal = {IEEE Trans. Comput.},
  title        = {High-radix generalized hyperbolic CORDIC and its hardware implementation},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Humas: A heterogeneity- and upgrade-aware microservice auto-scaling framework in large-scale data centers. <em>TC</em>, <em>74</em>(3), 968-982. (<a href='https://doi.org/10.1109/TC.2024.3506862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An effective auto-scaling framework is essential for microservices to ensure performance stability and resource efficiency under dynamic workloads. As revealed by many prior studies, the key to efficient auto-scaling lies in accurately learning performance patterns, i.e., the relationship between performance metrics and workloads in data-driven schemes. However, we notice that there are two significant challenges in characterizing performance patterns for large-scale microservices. Firstly, diverse microservices demonstrate varying sensitivities to heterogeneous machines, causing difficulty in quantifying the performance difference in a fixed manner. Secondly, frequent version upgrades of microservices result in uncertain changes in performance patterns, known as pattern drifts, leading to imprecise resource capacity estimation issues. To address these challenges, we propose Humas, a heterogeneity- and upgrade-aware auto-scaling framework for large-scale microservices. Firstly, Humas quantifies the difference in resource efficiency among heterogeneous machines for various microservices online and normalizes their resources in standard units. Additionally, Humas develops a least-squares density-difference (LSDD) based algorithm to identify pattern drifts caused by upgrades. Lastly, Humas generates capacity adjustment plans for microservices based on the latest performance patterns and predicted workloads. The experiment results conducted on 50 real microservices with over 11,000 containers demonstrate that Humas improves resource efficiency and performance stability by approximately 30.4% and 48.0%, respectively, compared to state-of-the-art approaches.},
  archive      = {J_TC},
  author       = {Qin Hua and Dingyu Yang and Shiyou Qian and Jian Cao and Guangtao Xue and Minglu Li},
  doi          = {10.1109/TC.2024.3506862},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {968-982},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Humas: A heterogeneity- and upgrade-aware microservice auto-scaling framework in large-scale data centers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RuYi: Optimizing burst buffer through automated, fine-grained process-to-BB mapping. <em>TC</em>, <em>74</em>(3), 955-967. (<a href='https://doi.org/10.1109/TC.2024.3510624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current supercomputers use an SSD-based storage layer called Burst Buffer (BB) to provide I/O-intensive applications with accelerated storage access. However, efficiently utilizing this limited and expensive storage remains a critical issue, creating an urgent need for implementing Quality of Service (QoS) in BB. To address this, we propose RuYi, a QoS-aware method to provide applications with bandwidth guarantees in the BB file system. RuYi tackles two main issues. First, it quantitatively profiles available bandwidth resources in BB to ensure reliable QoS, a crucial aspect seldom studied in the literature. Second, RuYi offers fine-grained process-level QoS via an innovative process-to-BB mapping, maximizing resource utilization—something not achievable with conventional coarse-grained compute-to-BB mapping. We evaluated RuYi on a subsystem of the leading exascale supercomputer Sunway, consisting of 4,000 compute nodes and 200 BB nodes. The experimental results demonstrate that RuYi achieves an impressive end-to-end bandwidth control accuracy of 97%, while improving BB utilization by up to 116% compared to conventional coarse-grained compute-to-BB mapping.},
  archive      = {J_TC},
  author       = {Yusheng Hua and Xuanhua Shi and Ligang He and Kang He and Teng Zhang and Hai Jin and Yong Chen},
  doi          = {10.1109/TC.2024.3510624},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {955-967},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RuYi: Optimizing burst buffer through automated, fine-grained process-to-BB mapping},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protecting the CCSDS 123.0-B-2 compression algorithm against single-event upsets for space applications. <em>TC</em>, <em>74</em>(3), 944-954. (<a href='https://doi.org/10.1109/TC.2024.3512203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral imaging is an excellent tool to remotely analyze the Earth from in-orbit devices. Satellites capture these images containing vast information about the ground pixels. To optimize storage and transmission speeds, compression is often performed onboard the satellite. To that end, algorithms such as the CCSDS 123.0-B-2 are implemented on FPGAs, enabling this process in an efficient and fast manner. Single-Event Upsets (SEU) are commonplace in this scenario, e.g. bit flips in the FPGA’s configuration memory which can catastrophically alter the algorithm’s output. In this paper, we propose a fault tolerance technique for this specific case. The compression core is checked periodically by running a golden model designed to excite the full internal datapath based on a synthetic image. A failure in this check will trigger a reconfiguration of the compression core. Results show better detection rates than Dual Modular Redundancy (DMR) at a fraction of the resource cost, proving this technique as a viable alternative. Furthermore, other algorithms with similar processing flows might benefit as well from this technique.},
  archive      = {J_TC},
  author       = {Daniel Báscones and Francisco García-Herrero and Óscar Ruano and Carlos González and Daniel Mozos and Juan Antonio Maestro},
  doi          = {10.1109/TC.2024.3512203},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {944-954},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Protecting the CCSDS 123.0-B-2 compression algorithm against single-event upsets for space applications},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NStore: A high-performance NUMA-aware key-value store for hybrid memory. <em>TC</em>, <em>74</em>(3), 929-943. (<a href='https://doi.org/10.1109/TC.2024.3504269'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging persistent memory (PM) promises near-DRAM performance, larger capacity, and data persistence, attracting researchers to design PM-based key-value stores. However, existing PM-based key-value stores lack awareness of the Non-Uniform Memory Access (NUMA) architecture on PM, where accessing PM on remote NUMA sockets is considerably slower than accessing local PM. This NUMA-unawareness results in sub-optimal performance when scaling on NUMA. Although DRAM caching alleviates this issue, existing cache policies ignore the performance disparity between remote and local PM accesses, keeping remote PM access as a performance bottleneck when scaling PM stores on NUMA. Furthermore, creating hot data views in each socket's PM fails to eliminate remote PM writes and, worse, induces additional local PM writes. This paper presents NStore, a high-performance NUMA-aware key-value store for the PM-DRAM hybrid memory. NStore introduces a NUMA-aware cache replacement strategy, called Remote Access First (RAF) cache in DRAM, to minimize remote PM accesses. In addition, NStore deploys Nlog, a write-optimized log-structured persistent storage, purposed to eliminate remote PM writes. NStore further mitigates the NUMA impacts through localized scan operations, efficient garbage collection, and multi-thread recovery for Nlog. Evaluations show that NStore outperforms state-of-the-art PM-based key-value stores, achieving up to 13.9$\times$ and 11.2$\times$ higher write and read throughput, respectively.},
  archive      = {J_TC},
  author       = {Zhonghua Wang and Kai Lu and Jiguang Wan and Hong Jiang and Zeyang Zhao and Peng Xu and Biliang Lai and Guokuan Li and Changsheng Xie},
  doi          = {10.1109/TC.2024.3504269},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {929-943},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NStore: A high-performance NUMA-aware key-value store for hybrid memory},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nicaea: A byzantine fault tolerant consensus under unpredictable message delivery failures for parallel and distributed computing. <em>TC</em>, <em>74</em>(3), 915-928. (<a href='https://doi.org/10.1109/TC.2024.3506856'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Byzantine fault-tolerant (BFT) consensus is a critical problem in parallel and distributed computing systems, particularly with potential adversaries. Most prior work on BFT consensus assumes reliable message delivery and tolerates arbitrary failures of up to $\frac{n}{3}$ nodes out of $n$ total nodes. However, many systems face unpredictable message delivery failures. This paper investigates the impact of unpredictable message delivery failures on the BFT consensus problem. We propose Nicaea, a novel protocol enabling consensus among loyal nodes when the number of Byzantine nodes is below a new threshold, given by: $\frac{\left(2-\rho\right)\left(1-\rho\right)^{2n-2}-1}{\left(2-\rho\right) \left(1-\rho\right)^{2n-2}+1}n$, where $\rho$ denotes the message failure rate. Theoretical proofs and experimental results validate Nicaea's Byzantine resilience. Our findings reveal a fundamental trade-off: as message delivery instability increases, a system's tolerance to Byzantine failures decreases. The well-known $\frac{n}{3}$ threshold under reliable message delivery is a special case of our generalized threshold when $\rho=0$. To the best of our knowledge, this work presents the first quantitative characterization of unpredictable message delivery failures’ impact on Byzantine fault tolerance in parallel and distributed computing.},
  archive      = {J_TC},
  author       = {Guanlin Jing and Yifei Zou and Minghui Xu and Yanqiang Zhang and Dongxiao Yu and Zhiguang Shan and Xiuzhen Cheng and Rajiv Ranjan},
  doi          = {10.1109/TC.2024.3506856},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {915-928},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Nicaea: A byzantine fault tolerant consensus under unpredictable message delivery failures for parallel and distributed computing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data sharing in the metaverse with key abuse resistance based on decentralized CP-ABE. <em>TC</em>, <em>74</em>(3), 901-914. (<a href='https://doi.org/10.1109/TC.2024.3512177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data sharing is ubiquitous in the metaverse, which adopts blockchain as its foundation. Blockchain is employed because it enables data transparency, achieves tamper resistance, and supports smart contracts. However, securely sharing data based on blockchain necessitates further consideration. Ciphertext-policy attribute-based encryption (CP-ABE) is a promising primitive to provide confidentiality and fine-grained access control. Nonetheless, authority accountability and key abuse are critical issues that practical applications must address. Few studies have considered CP-ABE key confidentiality and authority accountability simultaneously. To our knowledge, we are the first to fill this gap by integrating non-interactive zero-knowledge (NIZK) proofs into CP-ABE keys and outsourcing the verification process to a smart contract. To meet the decentralization requirement, we incorporate a decentralized CP-ABE scheme into the proposed data sharing system. Additionally, we provide an implementation based on smart contract to determine whether an access control policy is satisfied by a set of CP-ABE keys. We also introduce an open incentive mechanism to encourage honest participation in data sharing. Hence, the key abuse issue is resolved through the NIZK proof and the incentive mechanism. We provide a theoretical analysis and conduct comprehensive experiments to demonstrate the feasibility and efficiency of the data sharing system. Based on the proposed accountable approach, we further illustrate an application in GameFi, where players can play to earn or contribute to an accountable DAO, fostering a thriving metaverse ecosystem.},
  archive      = {J_TC},
  author       = {Liang Zhang and Zhanrong Ou and Changhui Hu and Haibin Kan and Jiheng Zhang},
  doi          = {10.1109/TC.2024.3512177},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {901-914},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Data sharing in the metaverse with key abuse resistance based on decentralized CP-ABE},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pako: Multi-valued byzantine agreement comparable to partially-synchronous BFT. <em>TC</em>, <em>74</em>(3), 887-900. (<a href='https://doi.org/10.1109/TC.2024.3510620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous Byzantine Fault Tolerance (BFT) consensus protocols are gaining attention for their resilience against network attacks. Among them, Multi-valued Byzantine Agreement (MVBA) protocols play a critical role, which accepts input values from each replica and returns a consistent output. The state-of-the-art MVBA protocol, sMVBA, has a good-case latency of $6\delta$ and an expected bad-case latency of $12\delta$, with $\delta$ representing the network delay. Additionally, sMVBA exhibits a communication of $O(n^{2})$ in both good and bad cases. Although it outperforms other MVBA protocols, sMVBA still lags behind partially-synchronous counterparts. For instance, PBFT achieves a good-case latency of $3\delta$, and HotStuff boasts a good-case communication of $O(n)$. This paper introduces a novel MVBA protocol, Pako, aiming for performance comparable to partially-synchronous protocols. Pako leverages an existing MVBA protocol as a black box and introduces an additional view with an optimistic path to commit values efficiently. Two Pako variants, Pako1 and Pako2, provide a trade-off between latency and communication. To be more specific, Pako1 achieves a good-case latency of $3\delta$ with $O(n^{2})$ communication, while Pako2 reduces the communication to $O(n)$ with a slightly higher good-case latency of $5\delta$. A series of experiments demonstrate Pako's significant outperformance of counterparts.},
  archive      = {J_TC},
  author       = {Xiaohai Dai and Zhengxuan Guo and Jiang Xiao and Guanxiong Wang and Yifei Liang and Chen Yu and Hai Jin},
  doi          = {10.1109/TC.2024.3510620},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {887-900},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Pako: Multi-valued byzantine agreement comparable to partially-synchronous BFT},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Being patient and persistent: Optimizing an early stopping strategy for deep learning in profiled attacks. <em>TC</em>, <em>74</em>(3), 875-886. (<a href='https://doi.org/10.1109/TC.2023.3234205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The absence of an algorithm that effectively monitors the deep learning models used in side-channel attacks increases the difficulty of a security evaluation. If an attack is unsuccessful, that could be due to multiple reasons. It can be that we are indeed dealing with a resistant implementation, but it is possible that the deep learning model used is faulty. In this contribution, we formalize two conditions, persistence and patience, for a deep learning model to be optimal and we propose an early stopping algorithm that reliably recognizes the model's optimal state during training. The novelty of our solution is in an efficient implementation of guessing entropy estimation as a success metric used to measure the strength of a side-channel adversary. As a result, the model which uses our strategy for learning converges with fewer traces than other known methods.},
  archive      = {J_TC},
  author       = {Servio Paguada and Lejla Batina and Ileana Buhan and Igor Armendariz},
  doi          = {10.1109/TC.2023.3234205},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {875-886},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Being patient and persistent: Optimizing an early stopping strategy for deep learning in profiled attacks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI trojan attack for evading machine learning-based detection of hardware trojans. <em>TC</em>, <em>74</em>(3), 860-874. (<a href='https://doi.org/10.1109/TC.2023.3251864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The globalized semiconductor supply chain significantly increases the risk of exposing System-on-Chip (SoC) designs to hardware Trojans. While machine learning (ML) based Trojan detection approaches are promising due to their scalability as well as detection accuracy, ML-based methods themselves are vulnerable from Trojan attacks. In this paper, we propose a robust backdoor attack on ML-based Trojan detection algorithms to demonstrate this serious vulnerability. The proposed framework is able to design an AI Trojan and implant it inside the ML model that can be triggered by specific inputs. Experimental results demonstrate that the proposed AI Trojans can bypass state-of-the-art defense algorithms. Moreover, our approach provides a fast and cost-effective solution in achieving 100% attack success rate that outperforms state-of-the art methods based on adversarial attacks.},
  archive      = {J_TC},
  author       = {Zhixin Pan and Prabhat Mishra},
  doi          = {10.1109/TC.2023.3251864},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {860-874},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AI trojan attack for evading machine learning-based detection of hardware trojans},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure remote attestation with strong key insulation guarantees. <em>TC</em>, <em>74</em>(3), 848-859. (<a href='https://doi.org/10.1109/TC.2023.3290870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure processors with hardware-enforced isolation are crucial for secure cloud computation. However, commercial secure processors have underestimated the capabilities of attackers and failed to provide secure execution environments capable of protecting sensitive information against side-channel attacks. Remote Attestation protocols based on traditional signature schemes are not secure under side-channel attacks anymore since their secret keys can be leaked. Previously, Key-Insulated Schemes (KIS) have been introduced to mitigate the damage caused by secret key exposure in cryptosystems by breaking the lifetime of secret keys into independent sessions. KIS protect the security of all other sessions if any session keys are compromised, however, provide no security guarantees for a compromised session. We introduce a new cryptographic primitive called One-Time Signature with Secret Key Exposure (OTS-SKE), which ensures no one can forge a valid signature of a new message or nonce even if all secret session keys are leaked. OTS-SKE enables us to sign attestation reports securely under a powerful adversary who can observe all digital states in secure enclaves through side-channel attacks. We also minimize the trusted computing base by introducing a secure co-processor that is only responsible for key generation into the system. Our experiments show that the signing of OTS-SKE is faster than KIS as well as Elliptic Curve Digital Signature Algorithm (ECDSA) used in Intel SGX.},
  archive      = {J_TC},
  author       = {Deniz Gurevin and Chenglu Jin and Phuong Ha Nguyen and Omer Khan and Marten van Dijk},
  doi          = {10.1109/TC.2023.3290870},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {848-859},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Secure remote attestation with strong key insulation guarantees},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How to launch a powerful side-channel collision attack?. <em>TC</em>, <em>74</em>(3), 835-847. (<a href='https://doi.org/10.1109/TC.2023.3259319'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A cryptographic implementation produces very similar power leakages when fed with the same input. Side-channel collision attacks exploit these similarities to establish the relationship between sub-keys and improve the efficiency of key recovery. Benefiting from independence of leakage model, they play an important role in non-profiled setting. However, performance of existing approaches against single collision value is still sub-optimal and optimization is promising. Motivated by this, we first theoretically analyze the mathematical dependency between the number of collisions and the number of encryptions, and propose an efficient side-channel attack named Collision-Paired Correlation Attack (CPCA) to guarantee that the side with fewer samples in a collision is completely paired in low noise scenario. This allows overcoming the inefficient utilization of information in existing works. Moreover, to further employ underlying informativeness, we maximize collision pairs as many as possible. This optimization significantly improves performance of CPCA and thereby extends it to large noise scenarios. Finally, to achieve moderate computational complexity, two equivalent variants of CPCA are investigated to address the potential problem of limited computing resources. Our further theoretical study illustrates that CPCA provides the upper security bound of Correlation-Enhanced Collision Attack (CECA), and experimental results fully verify its superiority.},
  archive      = {J_TC},
  author       = {Jiangshan Long and Changhai Ou and Yajun Ma and Yifan Fan and Hua Chen and Shihui Zheng},
  doi          = {10.1109/TC.2023.3259319},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {835-847},
  shortjournal = {IEEE Trans. Comput.},
  title        = {How to launch a powerful side-channel collision attack?},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). $\mathtt{PARLE}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="monospace">PARLE</mml:mi></mml:math>-$\mathtt{G}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="monospace">G</mml:mi></mml:math>: Provable automated representation and analysis framework for learnability evaluation of generic PUF compositions. <em>TC</em>, <em>74</em>(3), 820-834. (<a href='https://doi.org/10.1109/TC.2023.3259327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Besides enormous research efforts in the design of Physically Unclonable Functions (PUFs), its vulnerabilities are still being exploited using machine learning (ML) based model-building attacks. Due to inherent complicacy in exploring and manually converging to a strong PUF composition, the challenge of building ML-attack resistant PUFs continues. Hence, it becomes imperative to develop an automated framework that can formally assess the learnability of different PUF constructions and compositions to guide the designer to explore resilient PUFs. In this work, we present an automated analysis framework ($\mathtt{PARLE}$-$\mathtt{G}$), to formally represent and evaluate the Probably Approximately Correct (PAC) learnability of PUF constructions and their compositions. A high-level specification language $\mathtt{PUF}$-$\mathtt{G}$ has been developed to structurally represent any PUF composition comprising a specified set of primitive components and composition operations. The tool takes a PUF design represented in $\mathtt{PUF}$-$\mathtt{G}$ language as input and returns its PAC learnability result, identifying a suitable PAC learning algorithm and the PAC model parameters based on the input PUF design. PUF designs proven to be learnable by $\mathtt{PARLE}$-$\mathtt{G}$ are segregated into different classes based on the asymptotic complexity of their learnability bounds. Such automated analysis helps a designer to make informed design choices, thereby strengthening a PUF construction from the architectural level.},
  archive      = {J_TC},
  author       = {Durba Chatterjee and Aritra Hazra and Debdeep Mukhopadhyay},
  doi          = {10.1109/TC.2023.3259327},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {820-834},
  shortjournal = {IEEE Trans. Comput.},
  title        = {$\mathtt{PARLE}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="monospace">PARLE</mml:mi></mml:math>-$\mathtt{G}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="monospace">G</mml:mi></mml:math>: Provable automated representation and analysis framework for learnability evaluation of generic PUF compositions},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WhistleBlower: A system-level empirical study on RowHammer. <em>TC</em>, <em>74</em>(3), 805-819. (<a href='https://doi.org/10.1109/TC.2023.3235973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With frequent software-induced activations on DRAM rows, bit flips can occur on their physically adjacent rows (i.e., RowHammer). Existing studies leverage FPGA platforms to characterize RowHammer, which have identified key factors that contribute to RowHammer bit flips, e.g., data pattern. As the FPGA-based studies have removed the interference of the OS and the memory controller, their findings on the identified contributing factors do not always work as reported in a real-world computing system, resulting in negative effects on system-level RowHammer attacks and defenses. In this paper, we carry out a system-level empirical study on factors from both the software side and the DRAM side that contribute to RowHammer. We conduct the study on 33 DRAM modules including both DDR4 and DDR3, with 292 DRAM chips from various vendors. Our experimental results from the software side show that some prior findings about existing factors are inconsistent with our observations, thus not applicable to a real-world system. Also, we contribute to identifying one new factor that effectively affects RowHammer bit flips. Our DRAM-side results identify three types of new contributing factors and indicate that DRAM modules are more vulnerable if they achieve better performance and lower power consumption. Particularly, Intel XMP, intended for improving DRAM performance, might be abused for RowHammer attacks.},
  archive      = {J_TC},
  author       = {Wei He and Zhi Zhang and Yueqiang Cheng and Wenhao Wang and Wei Song and Yansong Gao and Qifei Zhang and Kang Li and Dongxi Liu and Surya Nepal},
  doi          = {10.1109/TC.2023.3235973},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {805-819},
  shortjournal = {IEEE Trans. Comput.},
  title        = {WhistleBlower: A system-level empirical study on RowHammer},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariate TVLA—Efficient side-channel evaluation using confidence intervals. <em>TC</em>, <em>74</em>(3), 790-804. (<a href='https://doi.org/10.1109/TC.2023.3299045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Securing cryptographic hardware designs and software implementations against side-channel attacks that leverage the power consumption or electromagnetic emanations of a device is an active topic of research. Different countermeasures against these attacks have been published, many of which rely on masking where sensitive information is split into multiple shares. Here, the information is hidden in higher statistical moments of the leakage if processed at the same time (univariate) or in combinations of side-channel information from different points in time (multivariate) if processed sequentially. Test Vector Leakage Assessment (TVLA) is a common evaluation technique to address the growing number of specific attacks. However, the assessment of multivariate leakage requires the evaluation of all possible combinations of sample points, massively slowing down the evaluation and in turn the development of countermeasures due to computational complexity. In this work, we develop and compare techniques to determine clock cycle combinations that leak information in a multivariate setting. We develop an efficient multivariate assessment framework and show how this approach can be used to generate evaluation results that satisfy a desired confidence level. Eventually, we demonstrate the practical relevance of our approach by applying it to two masked implementations of block ciphers.},
  archive      = {J_TC},
  author       = {Florian Bache and Jonas Wloka and Pascal Sasdrich and Tim Güneysu},
  doi          = {10.1109/TC.2023.3299045},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {790-804},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Multivariate TVLA—Efficient side-channel evaluation using confidence intervals},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reducing paging and exit overheads in intel SGX for oblivious conjunctive keyword search. <em>TC</em>, <em>74</em>(3), 776-789. (<a href='https://doi.org/10.1109/TC.2023.3281857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paging and exit overheads have been proven to be the performance bottlenecks when adopting Searchable Symmetric Encryption (SSE) with trusted hardware such as Intel SGX for keyword search. This problem becomes more serious when incorporating ORAM and SGX to design oblivious SSE schemes such as POSUP (Hoang et al., 2019) and Oblidb (Eskandarian and Zaharia, 2019) which can defend against inference attacks. The main reason comes from high round communication complexity of ORAM and constrained trusted memory created by SGX. To overcome this performance bottleneck, we propose a set of novel SSE constructions with realistic security/performance trade-offs. Our core idea is to encode the keyword-identifier pairs into a bloom filter to reduce the number of ORAM operations during the search procedure. Specifically, Construction 1 loads the bloom filter into the enclave sequentially, which outperforms about $1.7\times$ when the dataset is large compared with the performance of the baseline that directly combines ORAM and SGX. To further improve the performance of Construction 1, Construction 2 classifies keywords into groups and stores these groups in different bloom filters. By additionally leaking the keywords in search token belonging to which groups, Construction 2 outperforms Construction 1 by $16.5\sim 36.8\times$ and provides an improvement of at least one order over state-of-the-art oblivious protocols.},
  archive      = {J_TC},
  author       = {Qin Jiang and Saiyu Qi and Xu Yang and Yong Qi and Jianfeng Wang and Youshui Lu and Bochao An and Ee-Chien Chang},
  doi          = {10.1109/TC.2023.3281857},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {776-789},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reducing paging and exit overheads in intel SGX for oblivious conjunctive keyword search},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware design of an advanced-feature cryptographic tile within the european processor initiative. <em>TC</em>, <em>74</em>(3), 762-775. (<a href='https://doi.org/10.1109/TC.2023.3278536'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work describes the hardware implementation of a cryptographic accelerators suite, named Crypto-Tile, in the framework of the European Processor Initiative (EPI) project. The EPI project traced the roadmap to develop the first family of low-power processors with the design fully made in Europe, for Big Data, supercomputers and automotive. Each of the coprocessors of Crypto-Tile is dedicated to a specific family of cryptographic algorithms, offering functions for symmetric and public-key cryptography, computation of digests, generation of random numbers, and Post-Quantum cryptography. The performances of each coprocessor outperform other available solutions, offering innovative hardware-native services, such as key management, clock randomisation and access privilege mechanisms. The system has been synthesised on a 7 nm standard-cell technology, being the first Cryptoprocessor to be characterised in such an advanced silicon technology. The post-synthesis netlist has been employed to assess the resistance of Crypto-Tile to power analysis side-channel attacks. Finally, a demoboard has been implemented, integrating a RISC-V softcore processor and the Crypto-Tile module, and drivers for hardware abstraction layer, bare-metal applications and drivers for Linux kernel in C language have been developed. Finally, we exploited them to compare in terms of execution speed the hardware-accelerated algorithms against software-only solutions.},
  archive      = {J_TC},
  author       = {Pietro Nannipieri and Luca Crocetti and Stefano Di Matteo and Luca Fanucci and Sergio Saponara},
  doi          = {10.1109/TC.2023.3278536},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {762-775},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Hardware design of an advanced-feature cryptographic tile within the european processor initiative},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Node-wise hardware trojan detection based on graph learning. <em>TC</em>, <em>74</em>(3), 749-761. (<a href='https://doi.org/10.1109/TC.2023.3280134'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the fourth industrial revolution, securing the protection of supply chains has become an ever-growing concern. One such cyber threat is a hardware Trojan (HT), a malicious modification to an IC. HTs are often identified during the hardware manufacturing process but should be removed earlier in the design process. Machine learning-based HT detection in gate-level netlists is an efficient approach to identifying HTs at the early stage. However, feature-based modeling has limitations in terms of discovering an appropriate set of HT features. We thus propose NHTD-GL in this paper, a novel node-wise HT detection method based on graph learning (GL). Given the formal analysis of the HT features obtained from domain knowledge, NHTD-GL bridges the gap between graph representation learning and feature-based HT detection. The experimental results demonstrate that NHTD-GL achieves 0.998 detection accuracy and 0.921 F1-score and outperforms state-of-the-art node-wise HT detection methods. NHTD-GL extracts HT features without heuristic feature engineering.},
  archive      = {J_TC},
  author       = {Kento Hasegawa and Kazuki Yamashita and Seira Hidano and Kazuhide Fukushima and Kazuo Hashimoto and Nozomu Togawa},
  doi          = {10.1109/TC.2023.3280134},
  journal      = {IEEE Transactions on Computers},
  month        = {3},
  number       = {3},
  pages        = {749-761},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Node-wise hardware trojan detection based on graph learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shared recurrence floating-point Divide/Sqrt and integer Divide/Remainder with early termination. <em>TC</em>, <em>74</em>(2), 740-748. (<a href='https://doi.org/10.1109/TC.2024.3500380'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Division, square root, and remainder are fundamental operations required by most computer systems. Floating-point and integer operations are commonly performed on separate datapaths. This paper presents the first detailed implementation of a shared recurrence unit that supports floating-point division/square root and integer division/remainder. It supports early termination and shares the normalization shifter needed for integer and subnormal inputs. Synthesis results show that shared double-precision dividers producing at least 4 bits per cycle are 9 - 18% smaller and 3 - 16% faster than separate integer and floating-point units.},
  archive      = {J_TC},
  author       = {Kevin Kim and Katherine Parry and David Harris and Cedar Turek and Alessandro Maiuolo and Rose Thompson and James Stine},
  doi          = {10.1109/TC.2024.3500380},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {740-748},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Shared recurrence floating-point Divide/Sqrt and integer Divide/Remainder with early termination},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A system-level test methodology for communication peripherals in system-on-chips. <em>TC</em>, <em>74</em>(2), 731-739. (<a href='https://doi.org/10.1109/TC.2024.3500375'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with functional System-Level Test (SLT) for System-on-Chips (SoCs) communication peripherals. The proposed methodology is based on analyzing the potential weaknesses of applied structural tests such as Scan-based. Then, the paper illustrates how to develop a functional SLT programs software suite to address such issues. In case the communication peripheral provides detection/correction features, the methodology proposes the design of a hardware companion module to be added to the Automatic Test Equipment (ATE) to interact with the SoC communication module by purposely corrupting data frames. Experimental results are obtained on an industrial, automotive SoC produced by STMicroelectronics focusing on the Controller Area Network (CAN) communication peripheral and showing the effectiveness of the SLT suite to complement structural tests.},
  archive      = {J_TC},
  author       = {Francesco Angione and Paolo Bernardi and Nicola di Gruttola Giardino and Gabriele Filipponi and Claudia Bertani and Vincenzo Tancorre},
  doi          = {10.1109/TC.2024.3500375},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {731-739},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A system-level test methodology for communication peripherals in system-on-chips},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedQClip: Accelerating federated learning via quantized clipped SGD. <em>TC</em>, <em>74</em>(2), 717-730. (<a href='https://doi.org/10.1109/TC.2024.3477972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as a promising technique for collaboratively training machine learning models among multiple participants while preserving privacy-sensitive data. However, the conventional parameter server architecture presents challenges in terms of communication overhead when employing iterative optimization methods such as Stochastic Gradient Descent (SGD). Although communication compression techniques can reduce the traffic cost of FL during each training round, they often lead to degraded convergence rates, mainly due to compression errors and data heterogeneity. To address these issues, this paper presents FedQClip, an innovative approach that combines quantization and Clipped SGD. FedQClip leverages an adaptive step size inversely proportional to the $\ell_{2}$ norm of the gradient, effectively mitigating the negative impacts of quantized errors. Additionally, clipped operations can be applied locally and globally to further expedite training. Theoretical analyses provide evidence that, even under the settings of Non-IID (non-independent and identically distributed) data, FedQClip achieves a convergence rate of $\mathcal{O}(\frac{1}{\sqrt{T}})$, effectively addressing the convergence degradation caused by compression errors. Furthermore, our theoretical analysis highlights the importance of selecting an appropriate number of local updates to enhance the convergence of FL training. Through extensive experiments, we demonstrate that FedQClip outperforms state-of-the-art methods in terms of communication efficiency and convergence rate.},
  archive      = {J_TC},
  author       = {Zhihao Qu and Ninghui Jia and Baoliu Ye and Shihong Hu and Song Guo},
  doi          = {10.1109/TC.2024.3477972},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {717-730},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FedQClip: Accelerating federated learning via quantized clipped SGD},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed differentially private matrix factorization for implicit data via secure aggregation. <em>TC</em>, <em>74</em>(2), 705-716. (<a href='https://doi.org/10.1109/TC.2024.3500383'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Implicit feedback data has become the primary choice for building recommendation models due to its abundance and ease for collection in the real world. The strong generalization capability and high computational efficiency of matrix factorization make it one of the principal models for constructing recommender systems. Recommenders have to collect vast amounts of user data for model training, which poses a significant threat to user privacy. Most of the current privacy enhancing recommendation systems mainly focus on explicit feedback data, and there are limited studies dedicated to the privacy protection of implicit recommender. To bridge the existing research gap, this paper designs a distributed differentially private matrix factorization for implicit feedback data in scenarios where the recommender is not trusted. Our mechanism not only eliminates the assumption of a trusted recommender, but also achieves the same accuracy as CDP-based privacy-preserving MF model. We prove that our mechanism satisfies $(\epsilon,\delta)$-CDP. The experimental results on three public datasets confirm that the proposed mechanism can achieve high recommendation quality.},
  archive      = {J_TC},
  author       = {Chenhong Luo and Yong Wang and Yanjun Zhang and Leo Yu Zhang},
  doi          = {10.1109/TC.2024.3500383},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {705-716},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Distributed differentially private matrix factorization for implicit data via secure aggregation},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving efficiency in multi-modal autonomous embedded systems through adaptive gating. <em>TC</em>, <em>74</em>(2), 691-704. (<a href='https://doi.org/10.1109/TC.2024.3500382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The parallel advancement of AI and IoT technologies has recently boosted the development of multi-modal computing ($M^{2}C$) on pervasive autonomous embedded systems (AES). $M^{2}C$ takes advantage of data from different modalities such as images, audio, and text and is able to achieve notable improvements in accuracy. However, achieving these accuracy gains often comes at the cost of increased computational complexity and energy consumption. Furthermore, the presence of numerous advanced sensors in these systems significantly contributes to power consumption, exacerbating the issue of limited power resources. Collectively, these challenges pose difficulties in deploying $M^{2}C$ on small embedded devices with scarce energy resources. In this article, we propose an Adaptive Modality Gating technique called AMG for in-situ $M^{2}C$ applications. The primary objective of AMG is to conserve energy while preserving the accuracy advantages of $M^{2}C$. To achieve this goal, AMG incorporates two first-of-its-kind designs. Firstly, it introduces a novel semi-gating architecture that enables partial modality sensor power gating. Specifically, we devise the de-centralized AMG (D-AMG) and centralized AMG (C-AMG) architecture. The former buffers raw data on sensors while the latter buffers raw data on the computing board, which are suitable for different edge scenarios respectively. Secondly, it facilitates a self-initialization/tuning process on the AES, which is supported by carefully-built analytical model. Extensive evaluations demonstrate the effectiveness of AMG. It achieves a 1.6x to 3.8x throughput higher than other power management methods and improves the lifespan of AES by 10% to 280% longer within the same energy budget, while satisfying all performance and latency requirements across various scenarios.},
  archive      = {J_TC},
  author       = {Xiaofeng Hou and Cheng Xu and Chao Li and Jiacheng Liu and Xuehan Tang and Kwang-Ting Cheng and Minyi Guo},
  doi          = {10.1109/TC.2024.3500382},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {691-704},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Improving efficiency in multi-modal autonomous embedded systems through adaptive gating},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalpel: High performance contention-aware task co-scheduling for shared cache hierarchy. <em>TC</em>, <em>74</em>(2), 678-690. (<a href='https://doi.org/10.1109/TC.2024.3500381'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For scientific computing applications that consist of many loosely coupled tasks, efficient scheduling is critical to achieve high performance and good quality of service (QoS). One of the challenges for co-running tasks is the frequent contention for shared cache hierarchy of multi-core processors. Such contention significantly increases cache miss rate and therefore, results in performance deterioration for computational tasks. This paper presents Scalpel, a contention-aware task grouping and co-scheduling approach for efficient task scheduling on shared cache hierarchy. Scalpel utilizes the shared cache access features of tasks to group them in a heuristic way, which reduces the contention within groups by achieving equal shared cache locality, while maintaining load balancing between groups. Based thereon, it proposes a two-level scheduling strategy to schedule groups to processors and assign tasks to available cores in a timely manner, while considering the impact of task scheduling on shared cache locality to minimize task execution time. Experiments show that Scalpel reduces the shared cache miss rate by up to 2.14× and optimizes the execution time by up to 1.53× for scientific computing benchmarks, compared to several baseline approaches.},
  archive      = {J_TC},
  author       = {Song Liu and Jie Ma and Zengyuan Zhang and Xinhe Wan and Bo Zhao and Weiguo Wu},
  doi          = {10.1109/TC.2024.3500381},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {678-690},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scalpel: High performance contention-aware task co-scheduling for shared cache hierarchy},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NetMod: Toward accelerating cloud RAN distributed unit modulation within programmable switches. <em>TC</em>, <em>74</em>(2), 665-677. (<a href='https://doi.org/10.1109/TC.2024.3500379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radio Access Networks (RAN) are anticipated to gradually transition towards Cloud RAN (C-RAN), leveraging the full advantages of the cloud-native computing model. While this paradigm shift offers a promising architectural evolution to improve scalability, efficiency, and performance, significant challenges remain in managing the massive computing requirements of physical layer (PHY) processing. To address these challenges and meet the stringent Service Level Objectives (SLOs) in 5G networks, hardware acceleration technologies are essential. In this paper, we aim to mitigate this challenge by offloading 5G modulation mapping, a critical yet demanding function to encode bits into IQ symbols, directly onto the switch ASICs. Specifically, we introduce NetMod, a 5G New Radio (NR) standard-compliant in-network modulation mapper accelerator. NetMod leverages the capabilities of new-generation programmable switches within the C-RAN infrastructure to offload and accelerate PHY modulation functions. We implemented a NetMod prototype on a real-world platform using the Intel Tofino programmable switch and commodity servers running the Data Plane Development Kit (DPDK). Through extensive experiments, we demonstrate that NetMod achieves modulation mapping at switch line rate using minimal switch resources, thereby preserving ample space for traditional switching tasks. Furthermore, comparisons with a GPU-based 5G modulation mapper show that NetMod is 2.2$\boldsymbol{\times}$ to 3.3$\boldsymbol{\times}$ faster using only a single switch port. These results highlight the potential of in-network acceleration to enhance 5G network performance and efficiency.},
  archive      = {J_TC},
  author       = {Abdulbary Naji and Xingfu Wang and Ammar Hawbani and Aiman Ghannami and Liang Zhao and XiaoHua Xu and Wei Zhao},
  doi          = {10.1109/TC.2024.3500379},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {665-677},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NetMod: Toward accelerating cloud RAN distributed unit modulation within programmable switches},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COSMO: COmpressed sensing for models and logging optimization in MCU performance screening. <em>TC</em>, <em>74</em>(2), 652-664. (<a href='https://doi.org/10.1109/TC.2024.3500378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In safety-critical applications, microcontrollers must meet stringent quality and performance standards, including the maximum operating frequency $F_{\max}$. Machine learning models have proven effective in estimating $F_{\max}$ by utilizing data from on-chip ring oscillators. Previous research has shown that increasing the number of ring oscillators on board can enable the deployment of simple linear regression models to predict $F_{\max}$. However, the scarcity of labeled data that characterize this context poses a challenge in managing high-dimensional feature spaces; moreover, a very high number of ring oscillators is not desirable due to technological reasons. By modeling $F_{\max}$ as a linear combination of the ring oscillators’ values, this paper employs Compressed Sensing theory to build the model and perform feature selection, enhancing model efficiency and interpretability. We explore regularized linear methods with convex/non-convex penalties in microcontroller performance screening, focusing on selecting informative ring oscillators. This permits reducing models’ footprint while retaining high prediction accuracy. Our experiments on two real-world microcontroller products compare Compressed Sensing with two alternative feature selection approaches: filter and wrapped methods. In our experiments, regularized linear models effectively identify relevant ring oscillators, achieving compression rates of up to 32:1, with no substantial loss in prediction metrics.},
  archive      = {J_TC},
  author       = {Nicolò Bellarmino and Riccardo Cantoro and Sophie M. Fosson and Martin Huch and Tobias Kilian and Ulf Schlichtmann and Giovanni Squillero},
  doi          = {10.1109/TC.2024.3500378},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {652-664},
  shortjournal = {IEEE Trans. Comput.},
  title        = {COSMO: COmpressed sensing for models and logging optimization in MCU performance screening},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncovering the intricacies and synergies of processor microarchitecture mechanisms using explainable AI. <em>TC</em>, <em>74</em>(2), 637-651. (<a href='https://doi.org/10.1109/TC.2024.3500377'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper defines a data-driven methodology seamlessly combining machine learning (ML) and eXplainable Artificial Intelligence (XAI) techniques to address the challenge of understanding the intricate relationships between microarchitecture mechanisms with respect to system performance. By applying the SHapley Additive exPlanations (SHAP) XAI method, it analyzes the synergies of cache replacement, branch prediction, and hardware prefetching on instructions per cycle (IPC) scores. We validate our methodology by using the SPEC CPU 2006 and 2017 benchmark suites with the ChampSim simulator. We illustrate the benefits of the proposed methodology and discuss the major insights and limitations obtained from this study.},
  archive      = {J_TC},
  author       = {Abdoulaye Gamatié and Yuyang Wang and Diego Valdez Duran},
  doi          = {10.1109/TC.2024.3500377},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {637-651},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Uncovering the intricacies and synergies of processor microarchitecture mechanisms using explainable AI},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigation of phase transitions in self-organizing NoC for stable queueing dynamics. <em>TC</em>, <em>74</em>(2), 623-636. (<a href='https://doi.org/10.1109/TC.2024.3500373'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most complex cooperative systems, such as networks on chip (NoCs), possess self-organizing properties and exhibit fluctuations in data traffic with similar statistical characteristics across multiple timescales, a.k.a., scaling behavior. Abrupt transitions in the scaling behavior of these fluctuations, caused by spikes in data traffic, network congestion, etc., indicate instability in the queueing dynamics of NoC routers. This instability hampers the predictability of real-time flow control mechanisms, leading to unpredictable delays and communication failures. Detecting and mitigating these instabilities or phase transitions is crucial in domains requiring stability and real-time control, such as aviation and healthcare. In this paper, we propose a real-time monitoring and characterization strategy for data traffic from influential routers to identify and mitigate impending instabilities before their onset. Leveraging the self-organization characteristic of NoCs, we propose to implement targeted mitigation on influential nodes to achieve network-wide effects. We demonstrate the effectiveness of our strategy on various benchmarks by comparing traffic analysis plots before and after mitigation. Our results show that the proposed phase transition mitigation improves the network performance by an average of 39.6% and buffer utilization by an average of 4.62%.},
  archive      = {J_TC},
  author       = {Sneha Agarwal and Keshav Goel and Mitali Sinha and Sujay Deb},
  doi          = {10.1109/TC.2024.3500373},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {623-636},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Mitigation of phase transitions in self-organizing NoC for stable queueing dynamics},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance characteristics and guidelines of offloading middleboxes onto BlueField-2 DPU. <em>TC</em>, <em>74</em>(2), 609-622. (<a href='https://doi.org/10.1109/TC.2024.3500372'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth in data center network bandwidth far outpacing improvements in CPU performance, traditional software middleboxes running on servers have become inefficient. The emerging data processing units aim to address this by offloading network functions from the CPU. However, as DPUs are still a new technology, there lacks comprehensive evaluation of their capabilities for accelerating middleboxes. This paper benchmarks and analyzes the performance of offloading middleboxes onto the NVIDIA BlueField-2 DPU. Three key DPU capabilities are explored: flow tables offloading, ARM subsystem packet processing, and connection tracking hardware offload. By applying these to implement representative middleboxes for firewall, packet scheduling, and load balancing, their performance is characterized and compared to conventional CPU-based versions. Results reveal the high throughput of flow tables offloading for stateless firewalls, but limitations as pipeline depth increases. Packet scheduling using ARM cores is shown to currently reduce performance versus CPU-based scheduling. Finally, while connection tracking hardware offload boosts load balancer bandwidth, it also weakens connection creation abilities. Key lessons on efficient middleboxes offloading strategies with DPUs are provided to guide further research and development. Overall, this paper offers useful benchmarking and analysis of emerging DPUs for accelerating middleboxes in modern data centers.},
  archive      = {J_TC},
  author       = {Fuliang Li and Qin Chen and Jiaxing Shen and Xingwei Wang and Jiannong Cao},
  doi          = {10.1109/TC.2024.3500372},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {609-622},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Performance characteristics and guidelines of offloading middleboxes onto BlueField-2 DPU},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-delay efficient segmented approximate adder with smart chaining. <em>TC</em>, <em>74</em>(2), 597-608. (<a href='https://doi.org/10.1109/TC.2024.3500371'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing is a promising approach for high-performance, and low-energy computation in inherently error-tolerant applications. This paper proposes an approximate adder comprising a constant-truncation block in the least significant part and several non-overlapping summation blocks in the more significant parts of the adder. The carry-in of each block is supplied using the most significant bit of one of the input operands from the earlier block. In the most significant block, two more-precise approaches are used to generate candidate values for the carry-in. The final value of the carry-in for this block is selected based on the values of the input operands. In fact, the proposed approximate adder is input-aware, and dynamically adjusts its operation in one or two cycles to improve accuracy while limiting the average delay. The experimental results indicate that the proposed adder has a better quality-effort tradeoff than state-of-the-art approximate adders. Different configurations of the proposed adder improve delay, energy, and the energy-delay product (EDP) by 78%, 72%, and 87%, respectively, when compared to state-of-the-art approximate adders, all without any loss in accuracy. Additionally, the efficiency of the proposed adder is confirmed in both image dithering and stock price prediction through regression.},
  archive      = {J_TC},
  author       = {Tayebeh Karimi and Arezoo Kamran},
  doi          = {10.1109/TC.2024.3500371},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {597-608},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy-delay efficient segmented approximate adder with smart chaining},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mix-GEMM: Extending RISC-V CPUs for energy-efficient mixed-precision DNN inference using binary segmentation. <em>TC</em>, <em>74</em>(2), 582-596. (<a href='https://doi.org/10.1109/TC.2024.3500369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently computing Deep Neural Networks (DNNs) has become a primary challenge in today's computers, especially on devices targeting mobile or edge applications. Recent progress on Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) has shown that the key to high energy efficiency lies in executing deep learning models with low- (8- to 5-bit) or ultra-low-precision (4- to 2-bit). Unfortunately, current Central Processing Unit (CPU) architectures and Instruction Set Architectures (ISAs) present severe limitations on the range of data sizes supported to compute DNN kernels. In this work, we present Mix-GEMM, a hardware-software co-designed architecture that enables RISC-V processors to efficiently compute arbitrary mixed-precision DNN kernels, supporting all data size combinations from 8- to 2-bit. By applying binary segmentation, our architecture can scale its throughput by decreasing the data size of the operands, resulting in a flexible approach capable of leveraging state-of-the-art QAT and PTQ to achieve high energy efficiency at a very low cost. Evaluating our Mix-GEMM architecture in a dual-issue in-order RISC-V processor shows that we are able to boost its performance and energy efficiency by up to $44\times$ and $11\times$ with respect to the baseline processor, with an area overhead of only 2%. This allows our extended processor to execute state-of-the-art DNNs with significantly higher performance and energy efficiency than the standard FP32 precision, while retaining almost the same model accuracy.},
  archive      = {J_TC},
  author       = {Jordi Fornt and Enrico Reggiani and Pau Fontova-Musté and Narcís Rodas and Alessandro Pappalardo and Osman Sabri Unsal and Adrián Cristal Kestelman and Josep Altet and Francesc Moll and Jaume Abella},
  doi          = {10.1109/TC.2024.3500369},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {582-596},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Mix-GEMM: Extending RISC-V CPUs for energy-efficient mixed-precision DNN inference using binary segmentation},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-efficient, delay-constrained edge computing of a network of DNNs. <em>TC</em>, <em>74</em>(2), 569-581. (<a href='https://doi.org/10.1109/TC.2024.3500368'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach for executing the inference of a network of pre-trained deep neural networks (DNNs) on commercial-off-the-shelf devices that are deployed at the edge. The problem is to partition the computation of the DNNs between an energy-constrained and performance-limited edge device $\boldsymbol{\mathcal{E}}$, and an energy-unconstrained, higher performance device $\boldsymbol{\mathcal{C}}$, referred to as the cloudlet, with the objective of minimizing the energy consumption of $\boldsymbol{\mathcal{E}}$ subject to a deadline constraint. The proposed partitioning algorithm takes into account the performance profiles of executing DNNs on the devices, the power consumption profiles, and the variability in the delay of the wireless channel. The algorithm is demonstrated on a platform that consists of an NVIDIA Jetson Nano as the edge device $\boldsymbol{\mathcal{E}}$ and a Dell workstation with a Titan Xp GPU as the cloudlet. Experimental results show significant improvements both in terms of energy consumption of $\boldsymbol{\mathcal{E}}$ and processing delay of the application. Additionally, it is shown how the energy-optimal solution is changed when the deadline constraint is altered. Moreover, the overhead of decision-making for our proposed method is significantly lower than the state-of-the-art Integer Linear Programming (ILP) solutions.},
  archive      = {J_TC},
  author       = {Mehdi Ghasemi and Soroush Heidari and Young Geun Kim and Carole-Jean Wu and Sarma Vrudhula},
  doi          = {10.1109/TC.2024.3500368},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {569-581},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Energy-efficient, delay-constrained edge computing of a network of DNNs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating GPU's instruction-level error characteristics under low supply voltages. <em>TC</em>, <em>74</em>(2), 555-568. (<a href='https://doi.org/10.1109/TC.2024.3500366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supply voltage underscaling has been an effective approach to improve the energy-efficiency of modern high-performance processors, such as GPUs. However, energy efficiency and reliability are two sides of a trade-off. Undervolting will inevitably undermine reliability, since it reduces chip manufacturers’ voltage guardbands that is designed to ensure correct operations under worst-case scenarios. To achieve optimal energy efficiency while maintaining enough reliability, it is necessary to deeply understand the error characteristics caused by undervolting. Unlike previous works which focus mostly on program level, we perform the first comprehensive instruction-level voltage margin and error characteristics evaluation for GPU architectures. We systematically measure the error probability and patterns of GPU instructions during undervolting. Then, we also analyze the impact of locations (SMs, threads, and bits) and operand data values on the error characteristics. Based on our observations, we reduce the voltage to the minimum safe limit for different instructions which achieves 18.37% energy saving, and we further propose an error detection strategy which reduces the performance and energy overhead by 14.8% with negligible 0.01% degradation for error detection rate.},
  archive      = {J_TC},
  author       = {Jingweijia Tan and Jiashuo Wang and Kaige Yan and Xiaohui Wei and Xin Fu},
  doi          = {10.1109/TC.2024.3500366},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {555-568},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Evaluating GPU's instruction-level error characteristics under low supply voltages},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SADIMM: Accelerating $\underline{\text{S}}$<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:munder><mml:mtext>S</mml:mtext><mml:mo accent="true">―</mml:mo></mml:munder></mml:math>parse $\underline{\text{A}}$<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:munder><mml:mtext>A</mml:mtext><mml:mo accent="true">―</mml:mo></mml:munder></mml:math>ttention using $\underline{\text{DIMM}}$<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:munder><mml:mtext>DIMM</mml:mtext><mml:mo accent="true">―</mml:mo></mml:munder></mml:math>-based near-memory processing. <em>TC</em>, <em>74</em>(2), 542-554. (<a href='https://doi.org/10.1109/TC.2024.3500362'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-attention mechanism is the performance bottleneck of Transformer based language models. In response, researchers have proposed sparse attention to expedite Transformer execution. However, sparse attention involves massive random access, rendering it as a memory-intensive kernel. Memory-based architectures, such as near-memory processing (NMP), demonstrate notable performance enhancements in memory-intensive applications. Nonetheless, existing NMP-based sparse attention accelerators face suboptimal performance due to hardware and software challenges. On the hardware front, current solutions employ homogeneous logic integration, struggling to support the diverse operations in sparse attention. On the software side, token-based dataflow is commonly adopted, leading to load imbalance after the pruning of weakly connected tokens. To address these challenges, this paper introduces SADIMM, a hardware-software co-designed NMP-based sparse attention accelerator. In hardware, we propose a heterogeneous integration approach to efficiently support various operations within the attention mechanism. This involves employing different logic units for different operations, thereby improving hardware efficiency. In software, we implement a dimension-based dataflow, dividing input sequences by model dimensions. This approach achieves load balancing after the pruning of weakly connected tokens. Compared to NVIDIA RTX A6000 GPU, the experimental results on BERT, BART, and GPT-2 models demonstrate that SADIMM achieves 48$\boldsymbol{\times}$, 35$\boldsymbol{\times}$, 37$\boldsymbol{\times}$ speedups and 194$\boldsymbol{\times}$, 202$\boldsymbol{\times}$, 191$\boldsymbol{\times}$ energy efficiency improvement, respectively.},
  archive      = {J_TC},
  author       = {Huize Li and Dan Chen and Tulika Mitra},
  doi          = {10.1109/TC.2024.3500362},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {542-554},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SADIMM: Accelerating $\underline{\text{S}}$<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:munder><mml:mtext>S</mml:mtext><mml:mo accent="true">―</mml:mo></mml:munder></mml:math>parse $\underline{\text{A}}$<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:munder><mml:mtext>A</mml:mtext><mml:mo accent="true">―</mml:mo></mml:munder></mml:math>ttention using $\underline{\text{DIMM}}$<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:munder><mml:mtext>DIMM</mml:mtext><mml:mo accent="true">―</mml:mo></mml:munder></mml:math>-based near-memory processing},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing the deployment of tiny transformers on low-power MCUs. <em>TC</em>, <em>74</em>(2), 526-541. (<a href='https://doi.org/10.1109/TC.2024.3500360'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer networks are rapidly becoming State of the Art (SotA) in many fields, such as Natural Language Processing (NLP) and Computer Vision (CV). Similarly to Convolutional Neural Networks (CNNs), there is a strong push for deploying Transformer models at the extreme edge, ultimately fitting the tiny power budget and memory footprint of Micro-Controller Units (MCUs). However, the early approaches in this direction are mostly ad-hoc, platform, and model-specific. This work aims to enable and optimize the flexible, multi-platform deployment of encoder Tiny Transformers on commercial MCUs. We propose a complete framework to perform end-to-end deployment of Transformer models onto single and multi-core MCUs. Our framework provides an optimized library of kernels to maximize data reuse and avoid unnecessary data marshaling operations into the crucial attention block. A novel Multi-Head Self-Attention (MHSA) inference schedule, named Fused-Weight Self-Attention (FWSA), is introduced, fusing the linear projection weights offline to further reduce the number of operations and parameters. Furthermore, to mitigate the memory peak reached by the computation of the attention map, we present a Depth-First Tiling (DFT) scheme for MHSA tailored for cache-less MCU devices that allows splitting the computation of the attention map into successive steps, never materializing the whole matrix in memory. We evaluate our framework on three different MCU classes exploiting ARM and RISC-V Instruction Set Architecture (ISA), namely the STM32H7 (ARM Cortex M7), the STM32L4 (ARM Cortex M4), and GAP9 (RV32IMC-XpulpV2). We reach an average of 4.79 $\times$ and 2.0 $\times$ lower latency compared to SotA libraries CMSIS-NN (ARM) and PULP-NN (RISC-V), respectively. Moreover, we show that our MHSA depth-first tiling scheme reduces the memory peak by up to 6.19 $\times$, while the fused-weight attention can reduce the runtime by 1.53 $\times$, and number of parameters by 25%. Leveraging the optimizations proposed in this work, we run end-to-end inference of three SotA Tiny Transformers for three applications characterized by different input dimensions and network hyperparameters. We report significant improvements across the networks: for instance, when executing a transformer block for the task of radar-based hand-gesture recognition on GAP9, we achieve a latency of $0.14 \textrm{ms}$ and energy consumption of $4.92 \boldsymbol{\mu}\textrm{J}$, 2.32 $\times$ lower than the SotA PULP-NN library on the same platform.},
  archive      = {J_TC},
  author       = {Victor Jean-Baptiste Jung and Alessio Burrello and Moritz Scherer and Francesco Conti and Luca Benini},
  doi          = {10.1109/TC.2024.3500360},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {526-541},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Optimizing the deployment of tiny transformers on low-power MCUs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On task mapping in multi-chiplet based many-core systems to optimize inter- and intra-chiplet communications. <em>TC</em>, <em>74</em>(2), 510-525. (<a href='https://doi.org/10.1109/TC.2024.3500354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-chiplet system design, by integrating multiple chiplets/dielets within a single package, has emerged as a promising paradigm in the post-Moore era. This paper introduces a novel task mapping algorithm for multi-chiplet based many-core systems, addressing the unique challenges posed by intra- and inter-chiplet communications under power and thermal constraints. Traditional task mapping algorithms fail to account for the latency and bandwidth differences between these communications, leading to sub-optimal performance in multi-chiplet systems. Our proposed algorithm employs a two-step process: (1) task assignment to chiplets using binary linear programming, leveraging a totally unimodular constraint matrix, and (2) intra-chiplet mapping that minimizes communication latency while considering both thermal and power constraints. This method strategically positions tasks with extensive inter-chiplet communication near interface nodes and centralizes those with predominant intra-chiplet communication. Experimental results demonstrate that the proposed algorithm outperforms existing methods (DAR and IOA) with a 37.5% and 24.7% reduction in execution time, respectively. Communication latency is also reduced by up to 43.2% and 32.9%, compared to DAR and IOA. These findings affirm that the proposed task mapping algorithm aligns well with the characteristics of multi-chiplet based many-core systems, and thus improves optimal performance.},
  archive      = {J_TC},
  author       = {Xiaohang Wang and Yifan Wang and Yingtao Jiang and Amit Kumar Singh and Mei Yang},
  doi          = {10.1109/TC.2024.3500354},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {510-525},
  shortjournal = {IEEE Trans. Comput.},
  title        = {On task mapping in multi-chiplet based many-core systems to optimize inter- and intra-chiplet communications},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scaling persistent in-memory key-value stores over modern tiered, heterogeneous memory hierarchies. <em>TC</em>, <em>74</em>(2), 495-509. (<a href='https://doi.org/10.1109/TC.2024.3500352'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in ultra-fast non-volatile memories (e.g., 3D XPoint) and high-speed interconnect fabrics (e.g., RDMA) enable a high-performance tiered, heterogeneous memory system, effectively overcoming the cost, scaling, and capacity limitations in DRAM-based key-value stores. To fully unleash the performance potential of such memory systems, this paper presents BonsaiKV+, a key-value store that makes the best use of different components in a modern RDMA-enabled heterogeneous memory system. The core of BonsaiKV+ is a tri-layer architecture that achieves efficient, elastic scaling up/out using a set of novel mechanisms and techniques—pipelined tiered indexing, NVM congestion control mechanisms, fine-grained data striping, and NUMA-aware data management—to leverage hardware strengths and tackle device deficiencies. We compare BonsaiKV+ with state-of-the-art key-value stores using a variety of YCSB workloads. Evaluation results demonstrate that BonsaiKV+ outperforms others by up to 7.30$\times$, 18.89$\times$, and 13.67$\times$ in read-, write-, and scan-intensive scenarios, respectively.},
  archive      = {J_TC},
  author       = {Miao Cai and Junru Shen and Yifan Yuan and Zhihao Qu and Baoliu Ye},
  doi          = {10.1109/TC.2024.3500352},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {495-509},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Scaling persistent in-memory key-value stores over modern tiered, heterogeneous memory hierarchies},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CPI: A collaborative partial indexing design for large-scale deduplication systems. <em>TC</em>, <em>74</em>(2), 483-494. (<a href='https://doi.org/10.1109/TC.2024.3485238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data deduplication relies on a chunk index to identify the redundancy of incoming chunks. As backup data scales, it is impractical to maintain the entire chunk index in memory. Consequently, an index lookup needs to search the portion of the on-storage index, causing a dramatic regression of index lookup throughput. Existing studies propose to search a subset of the whole index (partial index) to limit the storage I/Os and guarantee a high index lookup throughput. However, several core factors of designing partial indexing are not fully exploited. In this paper, we first comprehensively investigate the trade-offs of using different meta-groups, sampling methods, and meta-group selection policies for a partial index. We then propose a Collaborative Partial Index (CPI) which takes advantage of two meta-groups including recipe-segment and container-catalog to achieve more efficient and effective unique chunk identification. CPI further introduces a hook-entry sharing technology and a two-stage eviction policy to reduce memory usage without hurting the deduplication ratio. According to evaluation, with the same constraints of memory usage and storage I/O, CPI achieves a 1.21x-2.17x higher deduplication ratio than the state-of-the-art partial indexing schemes. Alternatively, CPI achieves 1.8X-4.98x higher index lookup throughput than others when the same deduplication ratio is achieved. Compared with full indexing, CPI's maximum deduplication ratio is only 4.07% lower but its throughput is 37.1x - 122.2x of that of full indexing depending on different storage I/O constraints in our evaluation cases.},
  archive      = {J_TC},
  author       = {Yixun Wei and Zhichao Cao and David H. C. Du},
  doi          = {10.1109/TC.2024.3485238},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {483-494},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CPI: A collaborative partial indexing design for large-scale deduplication systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FasDL: An efficient serverless-based training architecture with communication optimization and resource configuration. <em>TC</em>, <em>74</em>(2), 468-482. (<a href='https://doi.org/10.1109/TC.2024.3485202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deploying distributed training workloads of deep learning models atop serverless architecture alleviates the burden of managing servers from deep learning practitioners. However, when supporting deep model training, the current serverless architecture faces the challenges of inefficient communication patterns and rigid resource configuration that incur subpar and unpredictable training performance. In this paper, we propose FasDL, an efficient serverless-based deep learning training architecture to solve these two challenges. FasDL adopts a novel training framework K-REDUCE to release the communication overhead and accelerate the training. Additionally, FasDL builds a lightweight mathematical model for K-REDUCE training, offering predictable performance and supporting subsequent resource configuration. It achieves the optimal resource configuration by formulating an optimization problem related to system-level and application-level parameters and solving it with a pruning-based heuristic search algorithm. Extensive experiments on AWS Lambda verify a prediction accuracy over 94% and demonstrate performance and cost advantages over the state-of-art architecture LambdaML by up to 16.8% and 28.3% respectively.},
  archive      = {J_TC},
  author       = {Xinglei Chen and Zinuo Cai and Hanwen Zhang and Ruhui Ma and Rajkumar Buyya},
  doi          = {10.1109/TC.2024.3485202},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {468-482},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FasDL: An efficient serverless-based training architecture with communication optimization and resource configuration},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Response-hiding and volume-hiding verifiable searchable encryption with conjunctive keyword search. <em>TC</em>, <em>74</em>(2), 455-467. (<a href='https://doi.org/10.1109/TC.2024.3485172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Verifiable searchable encryption (VSE) not only allows the client to search encrypted data, but also allows the client to verify whether the server honestly executes search operations. Currently, VSE scheme has been widely studied in cloud storage. However, most existing VSE schemes did not hide the access pattern and volume pattern, which respectively refer to the document identifiers and the number of documents matching the queried keywords. Recent studies have exploited these two patterns to launch attacks on searchable encryption schemes, resulting in compromising the confidentiality of encrypted data and queried keywords. In order to solve above issues, we utilize additively symmetric homomorphic encryption scheme and private set intersection protocol to construct a VSE scheme that supports conjunctive keyword search and hides the access pattern and volume pattern (i.e., response-hiding and volume-hiding). Our security model assumes that the server is malicious in the sense that it might deliberately carry out incorrect search operations. Formal security analysis demonstrates that our scheme achieves the desired security properties under our leakage function. Compared to previous schemes, our scheme has advantages in terms of performance and functionality. In an experimental setup with a security parameter of 128 bits and $2^{23}$ keyword/document pairs, the search time is approximately only 7.18 seconds.},
  archive      = {J_TC},
  author       = {Jiguo Li and Licheng Ji and Yicheng Zhang and Yang Lu and Jianting Ning},
  doi          = {10.1109/TC.2024.3485172},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {455-467},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Response-hiding and volume-hiding verifiable searchable encryption with conjunctive keyword search},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-UAV cooperative task scheduling in dynamic environments: Throughput maximization. <em>TC</em>, <em>74</em>(2), 442-454. (<a href='https://doi.org/10.1109/TC.2024.3483636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicle (UAV) has been considered a promising technology for advancing terrestrial mobile computing in the dynamic environment. In this research field, throughput, the number of completed tasks and latency are critical evaluation indicators used to measure the efficiency of UAVs in existing studies. In this paper, we transform these metrics to a single optimization objective, i.e., throughput maximization. To maximize the throughput, we consider realizing this goal in two respects. The first is to adapt the formation of the UAVs to provide cooperative computing service in a dynamic environment, we integrate a policy-based gradient algorithm and the task factorization network as a new reinforcement learning algorithm to improve the cooperation of UAVs. The second is to optimize the association process between UAVs and users, where the heterogeneity of tasks is considered. This algorithm is modified from the Gale-Shapley stability concept to optimize the appropriate association between tasks and UAVs in a dynamic time-varying condition to get the near-optimal association with few iterations. The scheduling of dependent tasks and independent tasks jointly also has to be considered. Finally, simulation results demonstrate the improvement of cooperation performance and the practicability of the association process.},
  archive      = {J_TC},
  author       = {Liang Zhao and Shuo Li and Zhiyuan Tan and Ammar Hawbani and Stelios Timotheou and Keping Yu},
  doi          = {10.1109/TC.2024.3483636},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {442-454},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A multi-UAV cooperative task scheduling in dynamic environments: Throughput maximization},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Allspark: Workload orchestration for visual transformers on processing in-memory systems. <em>TC</em>, <em>74</em>(2), 427-441. (<a href='https://doi.org/10.1109/TC.2024.3483633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of Transformers has revolutionized computer vision, offering a powerful alternative to convolutional neural networks (CNNs), especially with the local attention mechanism that excels at capturing local structures within the input and achieve state-of-the-art performance. Processing in-memory (PIM) architecture offers extensive parallelism, low data movement costs, and scalable memory bandwidth, making it a promising solution to accelerate Transformer with memory-intensive operations. However, the crucial issue lies in efficiently deploying an entire model onto resource-limited PIM system while parallelizing each transformer block with potentially many computational branches based on local-attention mechanisms. We present Allspark, which focuses on workload orchestration for visual Transformers on PIM systems, aiming at minimizing inference latency. Firstly, to fully utilize the massive parallelism of PIM, Allspark employs a fine-grained partitioning scheme for computational branches, and formats a systematic layout and interleaved dataflow with maximized data locality and reduced data movement. Secondly, Allspark formulates the scheduling of the complete model on a resource-limited distributed PIM system as an integer linear programming (ILP) problem. Thirdly, as local-global data interactions exhibit complex yet regular dependencies, Allspark provides a two-stage placement method, which simplifies the challenging placement of computational branches on the PIM system into the structured layout and greedy-based binding, to minimize NoC communication costs. Extensive experiments on 3D-stacked DRAM-based PIM systems show that Allspark brings $1.2\times$ $\sim$ $24.0\times$ inference speedup for various visual Transformers over baselines. Compared to Nvidia V100 GPU, Allspark-enriched PIM system yields average speedups of $2.3\times$ and energy savings of $20\times$ $\sim$ $55\times$.},
  archive      = {J_TC},
  author       = {Mengke Ge and Junpeng Wang and Binhan Chen and Yingjian Zhong and Haitao Du and Song Chen and Yi Kang},
  doi          = {10.1109/TC.2024.3483633},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {427-441},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Allspark: Workload orchestration for visual transformers on processing in-memory systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bit-sparsity aware acceleration with compact CSD code on generic matrix multiplication. <em>TC</em>, <em>74</em>(2), 414-426. (<a href='https://doi.org/10.1109/TC.2024.3483632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ever-increasing demand for matrix multiplication in artificial intelligence (AI) and generic computing emphasizes the necessity of efficient computing power accommodating both floating-point (FP) and quantized integer (QINT). While state-of-the-art bit-sparsity-aware acceleration techniques have demonstrated impressive performance and efficiency in neural networks through software-driven methods such as pruning and quantization, these approaches are not always feasible in typical generic computing scenarios. In this paper, we propose Bit-Cigma, a hardware-centric architecture that leverages bit-sparsity to accelerate generic matrix multiplication. Bit-Cigma features (1) CCSD encoding, an optimized on-chip sparsification technique based on canonical signed digit (CSD) representation; (2) segmented dot product, a multi-stage exponent matching technique for long FP vectors; and (3) the versatility to efficiently process both FP and QINT data types. CCSD encoding halves the cost of CSD encoding while achieving optimal bit-sparsity, and segmented dot product improves both accuracy and throughput. Bit-Cigma cores are implemented using 65 nm technology at 1 GHz, demonstrating substantial gains in performance and efficiency for both FP and QINT configurations. Compared to state-of-the-art Bitlet, Bit-Cigma achieves 3.2$\boldsymbol{\times}$ performance, 6.1$\boldsymbol{\times}$ area efficiency, and 15.3$\boldsymbol{\times}$ energy efficiency when processing FP32 data while ensuring zero computing error.},
  archive      = {J_TC},
  author       = {Zixuan Zhu and Xiaolong Zhou and Chundong Wang and Li Tian and Zunkai Huang and Yongxin Zhu},
  doi          = {10.1109/TC.2024.3483632},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {414-426},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Bit-sparsity aware acceleration with compact CSD code on generic matrix multiplication},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ECO-CRYSTALS: Efficient cryptography CRYSTALS on standard RISC-V ISA. <em>TC</em>, <em>74</em>(2), 401-413. (<a href='https://doi.org/10.1109/TC.2024.3483631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of post-quantum cryptography (PQC) is continuously evolving. Many researchers are exploring efficient PQC implementation on various platforms, including x86, ARM, FPGA, GPU, etc. In this paper, we present an Efficient CryptOgraphy CRYSTALS (ECO-CRYSTALS) implementation on standard 64-bit RISC-V Instruction Set Architecture (ISA). The target schemes are two winners of the National Institute of Standards and Technology (NIST) PQC competition: CRYSTALS-Kyber and CRYSTALS-Dilithium, where the two most time-consuming operations are Keccak and polynomial multiplication. Notably, this paper is the first highly-optimized assembly software implementation to deploy Kyber and Dilithium on the 64-bit RISC-V ISA. Firstly, we propose a better scheduling strategy for Keccak, which is specifically tailored for the 64-bit dual-issue RISC-V architecture. Our 24-round Keccak permutation (Keccak-$p$[1600,24]) achieves a 59.18% speed-up compared to the reference implementation. Secondly, we apply two modular arithmetic (Montgomery arithmetic and Plantard arithmetic) in the polynomial multiplication of Kyber and Dilithium to get a better lazy reduction. Then, we propose a flexible dual-instruction-issue scheme of Number Theoretic Transform (NTT). As for the matrix-vector multiplication, we introduce a row-to-column processing methodology to minimize the expensive memory access operations. Compared to the reference implementation, we obtain a speedup of 53.85%$\thicksim$85.57% for NTT, matrix-vector multiplication, and INTT in our ECO-CRYSTALS. Finally, the ECO-CRYSTALS implementation for key generation, encapsulation, and decapsulation in Kyber achieves 399k, 448k, and 479k cycles respectively, achieving speedups of 60.82%, 63.93%, and 65.56% compared to the NIST reference implementation. Similarly, the ECO-CRYSTALS implementation for key generation, sign, and verify in Dilithium reaches 1 364k, 3 191k, and 1 369k cycles, showcasing speedups of 54.84%, 64.98%, and 57.20%, respectively.},
  archive      = {J_TC},
  author       = {Xinyi Ji and Jiankuo Dong and Junhao Huang and Zhijian Yuan and Wangchen Dai and Fu Xiao and Jingqiang Lin},
  doi          = {10.1109/TC.2024.3483631},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {401-413},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ECO-CRYSTALS: Efficient cryptography CRYSTALS on standard RISC-V ISA},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive kernel fusion for improving the GPU utilization while ensuring QoS. <em>TC</em>, <em>74</em>(2), 386-400. (<a href='https://doi.org/10.1109/TC.2024.3477995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prosperity of machine learning applications has promoted the rapid development of GPU architecture. It continues to integrate more CUDA Cores, larger L2 cache and memory bandwidth within SM. Moreover, the GPU integrates Tensor Core dedicated to matrix multiplication. Although studies have shown that task co-location could effectively improve system throughput, existing works only focus on resource scheduling at the SM level and cannot improve resource utilization within the SM. In this paper, we propose Aker, a static kernel fusion and scheduling approach to improve resource utilization inside the SM while ensuring the QoS (Quality-of-Service) of co-located tasks. Aker consists of a static kernel fuser, a duration predictor for fused kernels, an adaptive fused kernel selector, and an enhanced QoS-aware kernel manager. The kernel fuser enables the static and flexible fusion for a kernel pair. The kernel pair could be Tensor Core kernel and CUDA Core kernel, or computing-prefer CUDA Core kernel and memory-prefer CUDA Core kernel. After the kernel fuser provides multiple fused kernel versions for a kernel pair, the duration predictor precisely predicts the duration of the fused kernels and the adaptive fused kernel selector locates the optimal fused kernel version. Finally, the kernel manager invokes the fused kernel or the original kernel based on the QoS headroom of latency-critical tasks to improve the system throughput. Our experimental results show that Aker improves the throughput of best-effort applications compared with state-of-the-art solutions by 50.1% on average, while ensuring the QoS of latency-critical tasks.},
  archive      = {J_TC},
  author       = {Han Zhao and Junxiao Deng and Weihao Cui and Quan Chen and Youtao Zhang and Deze Zeng and Minyi Guo},
  doi          = {10.1109/TC.2024.3477995},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {386-400},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Adaptive kernel fusion for improving the GPU utilization while ensuring QoS},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling high performance and resource utilization in clustered cache via hotness identification, data copying, and instance merging. <em>TC</em>, <em>74</em>(2), 371-385. (<a href='https://doi.org/10.1109/TC.2024.3477994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory cache systems such as Redis provide low-latency and high-performance data access for modern internet services. However, in large-scale Redis systems, the workloads show strong skewness and varied locality, which degrades system performance and incurs low CPU utilization. Though there are many approaches toward load imbalance, the two-layered architecture of Redis makes its workload skewness show special characteristics. Redis first maps data into data groups, which is called Group Mapping. Then the data groups are distributed to instances by Instance Mapping. Under Redis's layered architecture, it gives rise to a small number of hot-spot instances with very limited hot data groups, as well as a large number of remaining cold instances. To improve Redis's performance and CPU utilization, it entails the accurate identification of instance and data group hotness, and handling hot data groups and cold instances. We propose HPUCache+ to address the hot-spot problem via hotness identification, hot data copying, and cold instance merging. HPUCache+ accurately and dynamically detects instance and data group hotness based on multiple resources and workload characteristics at low cost. It enables access to multiple data copies by dynamically updating the cached mapping in Redis client, achieving high user access performance with Redis client compatibility, while providing highly self-definable service level agreement. It also proposes an asynchronous instance merging strategy based on disk snapshots and temporal caches, which separates the massive data movement from the critical user access path to achieve high-performance instance merging. We implement HPUCache+ into Redis. Experiments show that, compared to the native Redis design, HPUCache+ achieves up to 2.3$\times$ and 3.5$\times$ throughput gains, 11.3$\times$ and 14.3$\times$ CPU utilization gains, respectively. It also achieves up to 50% less CPU and 75% less memory consumption compared to the state-of-the-art approach Anna.},
  archive      = {J_TC},
  author       = {Hongmin Li and Si Wu and Zhipeng Li and Qianli Wang and Yongkun Li and Yinlong Xu},
  doi          = {10.1109/TC.2024.3477994},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {371-385},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Enabling high performance and resource utilization in clustered cache via hotness identification, data copying, and instance merging},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AsyncGBP${}^{+}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:math>: Bridging SSL/TLS and heterogeneous computing power with GPU-based providers. <em>TC</em>, <em>74</em>(2), 356-370. (<a href='https://doi.org/10.1109/TC.2024.3477987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of GPUs has emerged as a promising solution for accelerating the worldwide used SSL/TLS, which faces performance bottlenecks due to its underlying heavy cryptographic computations. Nevertheless, substantial structural adjustments from the parallel mode of GPUs to the serial mode of the SSL/TLS stack are imperative, potentially constraining the practical deployment of GPUs. In this paper, we propose AsyncGBP${}^{+}$, a three-level framework that facilitates the seamless conversion of cryptographic requests from synchronous to asynchronous mode. We conduct an in-depth analysis of the OpenSSL provider and cryptographic primitive features relevant to GPU implementations, aiming to fully exploit the potential of GPUs. Notably, AsyncGBP${}^{+}$ supports three working settings (offline/online/hybrid), finely tailored for various public key cryptographic primitives, including traditional ones like X25519, Ed25519, ECDSA, and the quantum-safe CRYSTALS-Kyber. A comprehensive evaluation demonstrates that AsyncGBP${}^{+}$ can efficiently achieve an improvement of up to 137.8$\times$ compared to the default OpenSSL provider (for X25519, Ed25519, ECDSA) and 113.30$\times$ compared to OpenSSL-compatible liboqs (for CRYSTALS-Kyber) in a single-process setting. Furthermore, AsyncGBP${}^{+}$ surpasses the current fastest commercial-off-the-shelf OpenSSL-compatible TLS accelerator with a 5.3$\times$ to 7.0$\times$ performance improvement.},
  archive      = {J_TC},
  author       = {Yi Bian and Fangyu Zheng and Yuewu Wang and Lingguang Lei and Yuan Ma and Tian Zhou and Jiankuo Dong and Guang Fan and Jiwu Jing},
  doi          = {10.1109/TC.2024.3477987},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {356-370},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AsyncGBP${}^{+}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow/><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:math>: Bridging SSL/TLS and heterogeneous computing power with GPU-based providers},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSTC: Dual-side sparse tensor core for DNNs acceleration on modern GPU architectures. <em>TC</em>, <em>74</em>(2), 341-355. (<a href='https://doi.org/10.1109/TC.2024.3475814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging sparsity in deep neural network (DNN) models holds significant promise for accelerating model inference. However, current GPUs can only harness sparsity in model weights, leaving activations unutilized due to their dynamic and unpredictable nature, which poses a considerable challenge for exploitation. In our research, we introduce a novel architectural approach aimed at effectively leveraging dual-side sparsity, encompassing both weight and activation sparsity. Our methodology involves a systematic examination of previous sparsity-related architectures, and culminating in the proposal of an uncharted paradigm that combines outer-product computation primitive and bitmap-based encoding format. Our approach showcases feasibility through minimal modifications to existing production-scale inner-product-based Tensor Cores. We introduce a set of innovative ISA extensions and carefully co-design matrix-matrix multiplication and convolution algorithms, the two predominant computation patterns in contemporary DNN models, to exploit our novel dual-side sparse Tensor Core. Our evaluation demonstrates the efficacy of our design, unlocking the full potential of dual-side DNN sparsity and delivering performance enhancements of up to an order of magnitude while incurring only modest hardware overhead.},
  archive      = {J_TC},
  author       = {Chen Zhang and Yang Wang and Zhiqiang Xie and Cong Guo and Yunxin Liu and Jingwen Leng and Zhigang Ji and Yuan Xie and Ru Huang},
  doi          = {10.1109/TC.2024.3475814},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  number       = {2},
  pages        = {341-355},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DSTC: Dual-side sparse tensor core for DNNs acceleration on modern GPU architectures},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opara: Exploiting operator parallelism for expediting DNN inference on GPUs. <em>TC</em>, <em>74</em>(1), 325-333. (<a href='https://doi.org/10.1109/TC.2024.3475589'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {GPUs have become the defacto hardware devices for accelerating Deep Neural Network (DNN) inference workloads. However, the conventional sequential execution mode of DNN operators in mainstream deep learning frameworks cannot fully utilize GPU resources, even with the operator fusion enabled, due to the increasing complexity of model structures and a greater diversity of operators. Moreover, the inadequate operator launch order in parallelized execution scenarios can lead to GPU resource wastage and unexpected performance interference among operators. In this paper, we propose Opara, a resource- and interference-aware DNN Operator parallel scheduling framework to accelerate DNN inference on GPUs. Specifically, Opara first employs CUDA Streams and CUDA Graph to parallelize the execution of multiple operators automatically. To further expedite DNN inference, Opara leverages the resource demands of operators to judiciously adjust the operator launch order on GPUs, overlapping the execution of compute-intensive and memory-intensive operators. We implement and open source a prototype of Opara based on PyTorch in a non-intrusive manner. Extensive prototype experiments with representative DNN and Transformer-based models demonstrate that Opara outperforms the default sequential CUDA Graph in PyTorch and the state-of-the-art operator parallelism systems by up to $1.68\boldsymbol{\times}$ and $1.29\boldsymbol{\times}$, respectively, yet with acceptable runtime overhead.},
  archive      = {J_TC},
  author       = {Aodong Chen and Fei Xu and Li Han and Yuan Dong and Li Chen and Zhi Zhou and Fangming Liu},
  doi          = {10.1109/TC.2024.3475589},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {325-333},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Opara: Exploiting operator parallelism for expediting DNN inference on GPUs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balanced modular addition for the moduli set $ \{2^{q},2^{q}\mp 1,2^{2q}+1\}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>{</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>∓</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:math> via moduli-($ 2^{q}\mp \sqrt{-1}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>∓</mml:mo><mml:msqrt><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:msqrt></mml:math>) adders. <em>TC</em>, <em>74</em>(1), 316-324. (<a href='https://doi.org/10.1109/TC.2024.3461235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moduli-set $ \mathbf{\tau}=\{2^{\boldsymbol{q}},2^{\boldsymbol{q}}\pm 1\}$ is often the base of choice for realization of digital computations via residue number systems. The optimum arithmetic performance in parallel residue channels, is generally achieved via equal bit-width residues (e.g., $ \boldsymbol{q}~ \mathbf{i}\mathbf{n}~ \mathbf{\tau}$) that usually leads to equal computation speed within all the residue channels. However, the commonly difficult and costly task of reverse conversion (RC) is often eased in the existence of conjugate moduli. For example, $ 2^{\boldsymbol{q}}\mp 1\in \mathbf{\tau}$, lead to the efficient modulo-($ 2^{2\boldsymbol{q}}-1$) addition, as the bulk of $ \mathbf{\tau}$-RC, via the New-CRT reverse conversion method. Nevertheless, for additional dynamic range, $ \mathbf{\tau}$ is augmented with other moduli. In particular, $ \mathbf{\phi}=\mathbf{\tau}\cup \{2^{2\boldsymbol{q}}+1\}$, leads to efficient RC, where the added modulo is conjugate with the product $ 2^{2\boldsymbol{q}}-1$ of $ 2^{\boldsymbol{q}}\mp 1\in \mathbf{\tau}$. Therefore, the final step of $ \mathbf{\phi}$-RC would be fast and low cost/power modulo-($ 2^{4\boldsymbol{q}}-1$) addition. However, the $ 2\boldsymbol{q}$-bit channel-width jeopardizes the existing delay-balance in $ \mathbf{\tau}$. As a remedial solution, given that $ 2^{2\boldsymbol{q}}+1=\left(2^{\boldsymbol{q}}-\boldsymbol{j}\right)\left(2^{\boldsymbol{q}}+\boldsymbol{j}\right)$, with $ \boldsymbol{j}=\sqrt{-1}$, we design and implement modulo-($ 2^{2\boldsymbol{q}}+1$) adders via two parallel $ \boldsymbol{q}$-bit moduli-($ 2^{\boldsymbol{q}}\mp \boldsymbol{j}$) adders. The analytical and synthesis based evaluations of the proposed modulo-($ 2^{\boldsymbol{q}}\mp \boldsymbol{j}$) adders show that the delay-balance of $ \mathbf{\tau}$ is preserved with no cost overhead vs. $ \mathbf{\phi}$. In particular, the binary-to-complex and complex-to-binary convertors are merely cost-free and immediate.},
  archive      = {J_TC},
  author       = {Ghassem Jaberipur and Elham Rahman and Jeong-A Lee},
  doi          = {10.1109/TC.2024.3461235},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {316-324},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Balanced modular addition for the moduli set $ \{2^{q},2^{q}\mp 1,2^{2q}+1\}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>{</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>∓</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:math> via moduli-($ 2^{q}\mp \sqrt{-1}$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msup><mml:mo>∓</mml:mo><mml:msqrt><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:msqrt></mml:math>) adders},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compressed test pattern generation for deep neural networks. <em>TC</em>, <em>74</em>(1), 307-315. (<a href='https://doi.org/10.1109/TC.2024.3457738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have emerged as an effective approach in many artificial intelligence tasks. Several specialized accelerators are often used to enhance DNN's performance and lower their energy costs. However, the presence of faults can drastically impair the performance and accuracy of these accelerators. Usually, many test patterns are required for certain types of faults to reach a target fault coverage, which in turn hence increases the testing overhead and storage cost, particularly for in-field testing. For this reason, compression is typically done after test generation step to reduce the storage cost for the generated test patterns. However, compression is more efficient when considered in an earlier stage. This paper generates the test pattern in a compressed form to require less storage. This is done by generating all test patterns as a linear combination of a set of jointly used test patterns (basis), for which only the coefficients need to be stored. The fault coverage achieved by the generated test patterns is compared to that of the adversarial and randomly generated test images. The experimental results showed that our proposed test pattern outperformed and achieved high fault coverage (up to 99.99%) and a high compression ratio (up to 307.2$\times$).},
  archive      = {J_TC},
  author       = {Dina A. Moussa and Michael Hefenbrock and Mehdi Tahoori},
  doi          = {10.1109/TC.2024.3457738},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {307-315},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Compressed test pattern generation for deep neural networks},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep learning-assisted template attack against dynamic frequency scaling countermeasures. <em>TC</em>, <em>74</em>(1), 293-306. (<a href='https://doi.org/10.1109/TC.2024.3477997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decades, machine learning techniques have been extensively used in place of classical template attacks to implement profiled side-channel analysis. This manuscript focuses on the application of machine learning to counteract Dynamic Frequency Scaling defenses. While state-of-the-art attacks have shown promising results against desynchronization countermeasures, a robust attack strategy has yet to be realized. Motivated by the simplicity and effectiveness of template attacks for devices lacking desynchronization countermeasures, this work presents a Deep Learning-assisted Template Attack (DLaTA) methodology specifically designed to target highly desynchronized traces through Dynamic Frequency Scaling. A deep learning-based pre-processing step recovers information obscured by desynchronization, followed by a template attack for key extraction. Specifically, we developed a three-stage deep learning pipeline to resynchronize traces to a uniform reference clock frequency. The experimental results on the AES cryptosystem executed on a RISC-V System-on-Chip reported a Guessing Entropy equal to 1 and a Guessing Distance greater than 0.25. Results demonstrate the method's ability to successfully retrieve secret keys even in the presence of high desynchronization. As an additional contribution, we publicly release our DFS_DESYNCH database11https://github.com/hardware-fab/DLaTA containing the first set of real-world highly desynchronized power traces from the execution of a software AES cryptosystem.},
  archive      = {J_TC},
  author       = {Davide Galli and Francesco Lattari and Matteo Matteucci and Davide Zoni},
  doi          = {10.1109/TC.2024.3477997},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {293-306},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A deep learning-assisted template attack against dynamic frequency scaling countermeasures},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing privacy and accuracy using significant gradient protection in federated learning. <em>TC</em>, <em>74</em>(1), 278-292. (<a href='https://doi.org/10.1109/TC.2024.3477971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous state-of-the-art studies have demonstrated that adversaries can access sensitive user data by membership inference attacks (MIAs) in Federated Learning (FL). Introducing differential privacy (DP) into the FL framework is an effective way to enhance the privacy of FL. Nevertheless, in differentially private federated learning (DP-FL), local gradients become excessively sparse in certain training rounds. Especially when training with low privacy budgets, there is a risk of introducing excessive noise into clients’ gradients. This issue can lead to a significant degradation in the accuracy of the global model. Thus, how to balance the user's privacy and global model accuracy becomes a challenge in DP-FL. To this end, we propose an approach, known as differential privacy federated aggregation, based on significant gradient protection (DP-FedASGP). DP-FedASGP can mitigate excessive noises by protecting significant gradients and accelerate the convergence of the global model by calculating dynamic aggregation weights for gradients. Experimental results show that DP-FedASGP achieves comparable privacy protection effects to DP-FedAvg and cpSGD (communication-private SGD based on gradient quantization) but outperforms DP-FedSNLC (sparse noise based on clipping losses and privacy budget costs) and FedSMP (sparsified model perturbation). Furthermore, the average global test accuracy of DP-FedASGP across four datasets and three models is about $2.62$%, $4.71$%, $0.45$%, and $0.19$% higher than the above methods, respectively. These improvements indicate that DP-FedASGP is a promising approach for balancing the privacy and accuracy of DP-FL.},
  archive      = {J_TC},
  author       = {Benteng Zhang and Yingchi Mao and Xiaoming He and Huawei Huang and Jie Wu},
  doi          = {10.1109/TC.2024.3477971},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {278-292},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Balancing privacy and accuracy using significant gradient protection in federated learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A heterogeneous and adaptive architecture for decision-tree-based ACL engine on FPGA. <em>TC</em>, <em>74</em>(1), 263-277. (<a href='https://doi.org/10.1109/TC.2024.3477955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Access Control Lists (ACLs) are crucial for ensuring the security and integrity of modern cloud and carrier networks by regulating access to sensitive information and resources. However, previous software and hardware implementations no longer meet the requirements of modern datacenters. The emergence of FPGA-based SmartNICs presents an opportunity to offload ACL functions from the host CPU, leading to improved network performance in datacenter applications. However, previous FPGA-based ACL designs lacked the necessary flexibility to support different rulesets without hardware reconfiguration while maintaining high performance. In this paper, we propose HACL, a heterogeneous and adaptive architecture for decision-tree-based ACL engine on FPGA. By employing techniques such as tree decomposition and recirculated pipeline scheduling, HACL can accommodate various rulesets without reconfiguring the underlying architecture. To facilitate the efficient mapping of different decision trees to memory and optimize the throughput of a ruleset, we also introduce a heterogeneous framework with a compiler in CPU platform for HACL. We implement HACL on a typical SmartNIC and evaluate its performance. The results demonstrate that HACL achieves a throughput exceeding 260 Mpps when processing 100K-scale ACL rulesets, with low hardware resource utilization. By integrating more engines, HACL can achieve even higher throughput and support larger rulesets.},
  archive      = {J_TC},
  author       = {Yao Xin and Chengjun Jia and Wenjun Li and Ori Rottenstreich and Yang Xu and Gaogang Xie and Zhihong Tian and Jun Li},
  doi          = {10.1109/TC.2024.3477955},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {263-277},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A heterogeneous and adaptive architecture for decision-tree-based ACL engine on FPGA},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative neural architecture search for personalized federated learning. <em>TC</em>, <em>74</em>(1), 250-262. (<a href='https://doi.org/10.1109/TC.2024.3477945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalized federated learning (pFL) is a promising approach to train customized models for multiple clients over heterogeneous data distributions. However, existing works on pFL often rely on the optimization of model parameters and ignore the personalization demand on neural network architecture, which can greatly affect the model performance in practice. Therefore, generating personalized models with different neural architectures for different clients is a key issue in implementing pFL in a heterogeneous environment. Motivated by Neural Architecture Search (NAS), a model architecture searching methodology, this paper aims to automate the model design in a collaborative manner while achieving good training performance for each client. Specifically, we reconstruct the centralized searching of NAS into the distributed scheme called Personalized Architecture Search (PAS), where differentiable architecture fine-tuning is achieved via gradient-descent optimization, thus making each client obtain the most appropriate model. Furthermore, to aggregate knowledge from heterogeneous neural architectures, a knowledge distillation-based training framework is proposed to achieve a good trade-off between generalization and personalization in federated learning. Extensive experiments demonstrate that our architecture-level personalization method achieves higher accuracy under the non-iid settings, while not aggravating model complexity over state-of-the-art benchmarks.},
  archive      = {J_TC},
  author       = {Yi Liu and Song Guo and Jie Zhang and Zicong Hong and Yufeng Zhan and Qihua Zhou},
  doi          = {10.1109/TC.2024.3477945},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {250-262},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Collaborative neural architecture search for personalized federated learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stream: Design space exploration of layer-fused DNNs on heterogeneous dataflow accelerators. <em>TC</em>, <em>74</em>(1), 237-249. (<a href='https://doi.org/10.1109/TC.2024.3477938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the landscape of deep neural networks evolves, heterogeneous dataflow accelerators, in the form of multi-core architectures or chiplet-based designs, promise more flexibility and higher inference performance through scalability. So far, these systems exploit the increased parallelism by coarsely mapping a single layer at a time across cores, which incurs frequent costly off-chip memory accesses, or by pipelining batches of inputs, which falls short in meeting the demands of latency-critical applications. To alleviate these bottlenecks, this work explores a new fine-grain mapping paradigm, referred to as layer fusion, on heterogeneous dataflow accelerators through a novel design space exploration framework called Stream. Stream captures a wide variety of heterogeneous dataflow architectures and mapping granularities, and implements a memory and communication-aware latency and energy analysis validated with three distinct state-of-the-art hardware implementations. As such, it facilitates a holistic exploration of architecture and mapping, by strategically allocating the workload through constraint optimization. The findings demonstrate that the integration of layer fusion with heterogeneous dataflow accelerators yields up to $2.2\times$ lower energy-delay product in inference efficiency, addressing both energy consumption and latency concerns. The framework is available open-source at: github.com/kuleuven-micas/stream.},
  archive      = {J_TC},
  author       = {Arne Symons and Linyan Mei and Steven Colleman and Pouya Houshmand and Sebastian Karl and Marian Verhelst},
  doi          = {10.1109/TC.2024.3477938},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {237-249},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Stream: Design space exploration of layer-fused DNNs on heterogeneous dataflow accelerators},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient service function chain placement over heterogeneous devices in deviceless edge computing environments. <em>TC</em>, <em>74</em>(1), 222-236. (<a href='https://doi.org/10.1109/TC.2024.3475590'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous devices in edge computing bring challenges as well as opportunities for edge computing to utilize powerful and heterogeneous hardware for a variety of complex tasks. In this paper, we propose a service function chain placement strategy considering the heterogeneity of devices in deviceless edge computing environments. The service function chain system utilizes lightweight virtualization technologies to manage resources, considering the heterogeneity of devices to support various complex tasks, and offer low latency services to user requests. We propose an optimal service function chain placement problem minimizing the service delay and formulate it into a quasi-convex problem. We implement different edge applications that can be served by function chains and conduct extensive experiments over real heterogeneous edge devices. Results from the experiments and simulations show that our proposed service function chain scheme is applicable in edge environments, and perform well over services latency, resource utilization as well as the power consumption of edge devices.},
  archive      = {J_TC},
  author       = {Yaodong Huang and Tingting Yao and Zelin Lin and Xiaojun Shang and Yukun Yuan and Laizhong Cui and Yuanyuan Yang},
  doi          = {10.1109/TC.2024.3475590},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {222-236},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient service function chain placement over heterogeneous devices in deviceless edge computing environments},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dependability of the k minimum values sketch: Protection and comparative analysis. <em>TC</em>, <em>74</em>(1), 210-221. (<a href='https://doi.org/10.1109/TC.2024.3475588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A basic operation in big data analysis is to find the cardinality estimate; to estimate the cardinality at high speed and with a low memory requirement, data sketches that provide approximate estimates, are usually used. The K Minimum Value (KMV) sketch is one of the most popular options; however, soft errors on memories in KMV may substantially degrade performance. This paper is the first to consider the impact of soft errors on the KMV sketch and to compare it with HyperLogLog (HLL), another widely used sketch for cardinality estimate. Initially, the operation of KMV in the presence of soft errors (so its dependability) in the memory is studied by a theoretical analysis and simulation by error injection. The evaluation results show that errors during the construction phase of KMV may cause large deviations in the estimate results. Subsequently, based on the algorithmic features of the KMV sketch, two protection schemes are proposed. The first scheme is based on using a single parity check (SPC) to detect errors and reduce their impact on the cardinality estimate; the second scheme is based on the incremental property of the memory list in KMV. The presented evaluation shows that both schemes can dramatically improve the performance of KMV, and the SPC scheme performs better even though it requires more memory footprint and overheads in the checking operation. Finally, it is shown that soft errors on the unprotected KMV produce larger worst-case errors than in HLL, but the average impact of errors is lower; also, the protected KMV using the proposed schemes are more dependable than HLL with existing protection techniques.},
  archive      = {J_TC},
  author       = {Jinhua Zhu and Zhen Gao and Pedro Reviriego and Shanshan Liu and Fabrizio Lombardi},
  doi          = {10.1109/TC.2024.3475588},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {210-221},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dependability of the k minimum values sketch: Protection and comparative analysis},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A numerical variability approach to results stability tests and its application to neuroimaging. <em>TC</em>, <em>74</em>(1), 200-209. (<a href='https://doi.org/10.1109/TC.2024.3475586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring the long-term reproducibility of data analyses requires results stability tests to verify that analysis results remain within acceptable variation bounds despite inevitable software updates and hardware evolutions. This paper introduces a numerical variability approach for results stability tests, which determines acceptable variation bounds using random rounding of floating-point calculations. By applying the resulting stability test to fMRIPrep, a widely-used neuroimaging tool, we show that the test is sensitive enough to detect subtle updates in image processing methods while remaining specific enough to accept numerical variations within a reference version of the application. This result contributes to enhancing the reliability and reproducibility of data analyses by providing a robust and flexible method for stability testing.},
  archive      = {J_TC},
  author       = {Yohan Chatelain and Loïc Tetrel and Christopher J. Markiewicz and Mathias Goncalves and Gregory Kiar and Oscar Esteban and Pierre Bellec and Tristan Glatard},
  doi          = {10.1109/TC.2024.3475586},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {200-209},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A numerical variability approach to results stability tests and its application to neuroimaging},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking control flow in spatial architectures: Insights into control flow plane design. <em>TC</em>, <em>74</em>(1), 185-199. (<a href='https://doi.org/10.1109/TC.2024.3475582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial architecture is a high-performance paradigm that employs control flow graphs and data flow graphs as computation model, and producer/consumer models as execution model. However, existing spatial architectures struggle with control flow handling challenges. Upon thoroughly characterizing their PE execution models, we observe that they lack autonomous, peer-to-peer, and temporally loosely-coupled control flow handling capability. This degrades its performance in intensive control programs. To tackle the existing control flow handling challenges, Marionette, a spatial architecture with an explicit-designed control flow plane, is proposed. We elaborately develop a full stack of Marionette architecture, from ISA, compiler, simulator to RTL. Marionette's flexible Control Flow Plane enables autonomous, peer-to-peer, and temporally loosely-coupled control flow management. Its Proactive PE Configuration ensures computation-overlapped and timely configuration to promote Branch Divergence handling capability. Besides, Marionette's Agile PE Assignment improves pipeline performance of imperfect loops. Compared to state-of-the-art spatial architectures, the experimental results demonstrate that Marionette outperforms Softbrain, TIA, REVEL, and RipTide by geomean 2.88$\mathbf{\times}$, 3.38$\mathbf{\times}$, 1.55$\mathbf{\times}$, and 2.66$\mathbf{\times}$ in a variety of challenging intensive control programs.},
  archive      = {J_TC},
  author       = {Jinyi Deng and Xinru Tang and Jiahao Zhang and Yuxuan Li and Linyun Zhang and Fengbin Tu and Shaojun Wei and Yang Hu and Shouyi Yin},
  doi          = {10.1109/TC.2024.3475582},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {185-199},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Rethinking control flow in spatial architectures: Insights into control flow plane design},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sketch-based adaptive communication optimization in federated learning. <em>TC</em>, <em>74</em>(1), 170-184. (<a href='https://doi.org/10.1109/TC.2024.3475578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, cross-device federated learning (FL), particularly in the context of Internet of Things (IoT) applications, has demonstrated its remarkable potential. Despite significant efforts, empirical evidence suggests that FL algorithms have yet to gain widespread practical adoption. The primary obstacle stems from the inherent bandwidth overhead associated with gradient exchanges between clients and the server, resulting in substantial delays, especially within communication networks. To deal with the problem, various solutions are proposed with the hope of finding a better balance between efficiency and accuracy. Following this goal, we focus on investigating how to design a lightweight FL algorithm that requires less communication cost while maintaining comparable accuracy. Specifically, we propose a Sketch-based FL algorithm that combines the incremental singular value decomposition (ISVD) method in a way that does not negatively affect accuracy much in the training process. Moreover, we also provide adaptive gradient error accumulation and error compensation mechanisms to mitigate accumulated gradient errors caused by sketch compression and improve the model accuracy. Our extensive experimentation with various datasets demonstrates the efficacy of our proposed approach. Specifically, our scheme achieves nearly a 93% reduction in communication cost during the training of multi-layer perceptron models (MLP) using the MNIST dataset.},
  archive      = {J_TC},
  author       = {Pan Zhang and Lei Xu and Lin Mei and Chungen Xu},
  doi          = {10.1109/TC.2024.3475578},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {170-184},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Sketch-based adaptive communication optimization in federated learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and fast high-performance library generation for deep learning accelerators. <em>TC</em>, <em>74</em>(1), 155-169. (<a href='https://doi.org/10.1109/TC.2024.3475575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of deep learning accelerators (DLAs) underscores their pivotal role in improving the performance and energy efficiency of neural networks. To fully leverage the capabilities of these accelerators, exploration-based library generation approaches have been widely used to substantially reduce software development overhead. However, these approaches have been challenged by issues related to sub-optimal optimization results and excessive optimization overheads. In this paper, we propose Heron to generate high-performance libraries of DLAs in an efficient and fast way. The key is automatically enforcing massive constraints through the entire program generation process and guiding the exploration with an accurate pre-trained cost model. Heron represents the search space as a constrained satisfaction problem (CSP) and explores the space via evolving the CSPs. Thus, the sophisticated constraints of the search space are strictly preserved during the entire exploration process. The exploration algorithm has the flexibility to engage in space exploration using either online-trained models or pre-trained models. Experimental results demonstrate that Heron averagely achieves 2.71$\times$ speedup over three state-of-the-art automatic generation approaches. Also, compared to vendor-provided hand-tuned libraries, Heron achieves a 2.00$\times$ speedup on average. When employing a pre-trained model, Heron achieves 11.6$\times$ compilation time speedup, incurring a minor impact on execution time.},
  archive      = {J_TC},
  author       = {Jun Bi and Yuanbo Wen and Xiaqing Li and Yongwei Zhao and Yuxuan Guo and Enshuai Zhou and Xing Hu and Zidong Du and Ling Li and Huaping Chen and Tianshi Chen and Qi Guo},
  doi          = {10.1109/TC.2024.3475575},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {155-169},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient and fast high-performance library generation for deep learning accelerators},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel modular multiplication using variable length algorithms. <em>TC</em>, <em>74</em>(1), 143-154. (<a href='https://doi.org/10.1109/TC.2024.3475574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents two improved modular multiplication algorithms: variable length Interleaved modular multiplication (VLIM) algorithm and parallel modular multiplication (P_MM) method using variable length algorithms to achieve high throughput rates. The new Interleaved modular multiplication algorithm applies the zero counting and partitioning algorithm to a multiplier’s non-adjacent form (NAF). It divides this input into sections with variable-radix. The sections include a digit of zero sequences and a non-zero digit (-1 or 1) in the most valuable place. Therefore, in addition to reducing the number of required clock pulses, high-radix partial multiplication $\mathbf{X}^{\left(\mathbf{i}\right)}\cdot \mathbf{Y}$ is simplified and performed as a binary addition or subtraction operation, and multiplication operations for consecutive zero bits are executed in one clock cycle instead of several clock cycles. The proposed parallel modular multiplication algorithm divides the multiplier into two parts. It utilizes (VLIM) and variable length Montgomery modular multiplication (VLM3) methods to compute the modular multiplication for the upper and lower portions in parallel, according to the proximity of their multiplication time. The implementation results on a Xilinx Virtex-7 FPGA show that the parallel modular multiplication computes a 2048-bit modular multiplication in 0.903 µs, with a maximum clock frequency of 387 MHz and area × time per bit value equal to 9.14.},
  archive      = {J_TC},
  author       = {Shahab Mirzaei-Teshnizi and Parviz Keshavarzi},
  doi          = {10.1109/TC.2024.3475574},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {143-154},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Parallel modular multiplication using variable length algorithms},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance and environment-aware advanced driving assistance systems. <em>TC</em>, <em>74</em>(1), 131-142. (<a href='https://doi.org/10.1109/TC.2024.3475572'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In autonomous and self-driving vehicles, visual perception of the driving environment plays a key role. Vehicles rely on machine learning (ML) techniques such as deep neural networks (DNNs), which are extensively trained on manually annotated databases to achieve this goal. However, the availability of training data that can represent different environmental conditions can be limited. Furthermore, as different driving terrains require different decisions by the driver, it is tedious and impractical to design a database with all possible scenarios. This work proposes a semi-parametric approach that bypasses the manual annotation required to train vehicle perception systems in autonomous and self-driving vehicles. We present a novel “Performance and Environment-aware Advanced Driving Assistance Systems” which employs one-shot learning for efficient data generation using user action and response in addition to the synthetic traffic data generated as Pareto optimal solutions from one-shot objects using a set of generalization functions. Adapting to the driving environments through such optimization adds more robustness and safety features to autonomous driving. We evaluate the proposed framework on environment perception challenges encountered in autonomous driving assistance systems. To accelerate the learning and adapt in real-time to perceived data, a novel deep learning-based Alternating Direction Method of Multipliers (dlADMM) algorithm is introduced to improve the convergence capabilities of regular machine learning models. This methodology optimizes the training process and makes applying the machine learning model to real-world problems more feasible. We evaluated the proposed technique on AlexNet and MobileNetv2 networks and achieved more than 18$\times$ speedup. By making the proposed technique behavior-aware we observed performance of upto 99% while detecting traffic signals.},
  archive      = {J_TC},
  author       = {Sreenitha Kasarapu and Sai Manoj Pudukotai Dinakarrao},
  doi          = {10.1109/TC.2024.3475572},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {131-142},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Performance and environment-aware advanced driving assistance systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ClusPar: A game-theoretic approach for efficient and scalable streaming edge partitioning. <em>TC</em>, <em>74</em>(1), 116-130. (<a href='https://doi.org/10.1109/TC.2024.3475568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Streaming edge partitioning plays a crucial role in the distributed processing of large-scale web graphs, such as pagerank. The quality of partitioning is of utmost importance and directly affects the runtime cost of distributed graph processing. However, streaming graph clustering, a key component of mainstream streaming edge partitioning, is vertex-centric. This incurs a mismatch with the edge-centric partitioning strategy, necessitating additional post-processing and several graph traversals to transition from vertex-centric clusters to edge-centric partitions. This transition not only adds extra runtime overhead but also risks a decline in partitioning quality. In this paper, we propose a novel algorithm, called ClusPar, to address the problem of streaming edge partitioning. The ClusPar framework consists of two steps, streaming edge clustering and edge cluster partitioning. Different from prior studies, the first step traverses the input graph in a single pass to generate edge-centric clusters, while the second step applies game theory over these edge-centric clusters to produce partitions. Extensive experiments show that ClusPar outperforms the state-of-the-art streaming edge partitioning methods in terms of the partitioning quality, efficiency, and scalability.},
  archive      = {J_TC},
  author       = {Zezhong Ding and Deyu Kong and Zhuoxu Zhang and Xike Xie and Jianliang Xu},
  doi          = {10.1109/TC.2024.3475568},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {116-130},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ClusPar: A game-theoretic approach for efficient and scalable streaming edge partitioning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning based DDoS attacks detection in large scale software-defined network. <em>TC</em>, <em>74</em>(1), 101-115. (<a href='https://doi.org/10.1109/TC.2024.3474180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Networking (SDN) is an innovative concept that segments the network into three planes: a control plane comprising of one or multiple controllers; a data plane responsible for data transmission; and an application plane which enables the reconfiguration of network functionalities. Nevertheless, this approach has exposed the controller as a prime target for malicious elements to attack it, such as Distributed Denial of Service (DDoS) attacks. Current DDoS defense schemes often increased the controller load and resource consumption. These schemes are typically tailored for single-controller architectures, a significant limitation when considering the scalability requirements of large-scale SDN. To address these limitations, we introduce an efficient Federated Learning approach, named “FedLAD,” designed to counter DDoS attacks in SDN-based large-scale networks, particularly in multi-controller architectures. Federated learning is a decentralized approach to machine learning where models are trained across multiple devices as controllers store local data samples, without exchanging them. The evaluation of the proposed scheme's performance, using InSDN, CICDDoS2019, and CICDoS2017 datasets, shows an accuracy exceeding 98%, a significant improvement compared to related works. Furthermore, the evaluation of the FedLAD protocol with real-time traffic in an SDN context demonstrates its ability to detect DDoS attacks with high accuracy and minimal resource consumption. To the best of our knowledge, this work introduces a new technique in applying FL for DDoS attack detection in large-scale SDN.},
  archive      = {J_TC},
  author       = {Yannis Steve Nsuloun Fotse and Vianney Kengne Tchendji and Mthulisi Velempini},
  doi          = {10.1109/TC.2024.3474180},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {101-115},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Federated learning based DDoS attacks detection in large scale software-defined network},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Olive-like networking: A uniformity driven robust topology generation scheme for IoT system. <em>TC</em>, <em>74</em>(1), 86-100. (<a href='https://doi.org/10.1109/TC.2024.3465934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the scale of the Internet of Things (IoT) system growing constantly, node failures frequently occur due to device malfunctions or cyberattacks. Existing robust network generation methods utilize heuristic algorithms or neural network approaches to optimize the initial topology. These methods do not explore the core of topology robustness, namely how edges are allocated to each node in the topology. As a result, these methods use massive iterative processes to optimize the initial topology, leading to substantial time overhead when the scale of the topology is large. We examine various robust networks and observe that uniform degree distribution is the core of topology robustness. Consequently, we propose a novel UNIformity driven robusT topologY generation scheme (UNITY) for IoT systems to prevent the node degree from becoming excessively high or low, thereby balancing node degrees. Comprehensive experimental results demonstrate that networks generated with UNITY have an “olive-like” topology consisting of a substantial number of medium-degree nodes and possess strong robustness against both random node failures and targeted attacks. This promising result indicates that the UNITY makes a significant advancement in designing robust IoT systems.},
  archive      = {J_TC},
  author       = {Tie Qiu and Jingchen Sun and Ning Chen and Songwei Zhang and Weisheng Si and Xingwei Wang},
  doi          = {10.1109/TC.2024.3465934},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {86-100},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Olive-like networking: A uniformity driven robust topology generation scheme for IoT system},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FiDRL: Flexible invocation-based deep reinforcement learning for DVFS scheduling in embedded systems. <em>TC</em>, <em>74</em>(1), 71-85. (<a href='https://doi.org/10.1109/TC.2024.3465933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Reinforcement Learning (DRL)-based Dynamic Voltage Frequency Scaling (DVFS) has shown great promise for energy conservation in embedded systems. While many works were devoted to validating its efficacy or improving its performance, few discuss the feasibility of the DRL agent deployment for embedded computing. State-of-the-art approaches focus on the miniaturization of agents’ inferential networks, such as pruning and quantization, to minimize their energy and resource consumption. However, this spatial-based paradigm still proves inadequate for resource-stringent systems. In this paper, we address the feasibility from a temporal perspective, where FiDRL, a flexible invocation-based DRL model is proposed to judiciously invoke itself to minimize the overall system energy consumption, given that the DRL agent incurs non-negligible energy overhead during invocations. Our approach is three-fold: (1) FiDRL that extends DRL by incorporating the agent's invocation interval into the action space to achieve invocation flexibility; (2) a FiDRL-based DVFS approach for both inter- and intra-task scheduling that minimizes the overall execution energy consumption; and (3) a FiDRL-based DVFS platform design and an on/off-chip hybrid algorithm specialized for training the DRL agent for embedded systems. Experiment results show that FiDRL achieves 55.1% agent invocation cost reduction, under 23.3% overall energy reduction, compared to state-of-the-art approaches.},
  archive      = {J_TC},
  author       = {Jingjin Li and Weixiong Jiang and Yuting He and Qingyu Yang and Anqi Gao and Yajun Ha and Ender Özcan and Ruibin Bai and Tianxiang Cui and Heng Yu},
  doi          = {10.1109/TC.2024.3465933},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {71-85},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FiDRL: Flexible invocation-based deep reinforcement learning for DVFS scheduling in embedded systems},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Remora: A low-latency DAG-based BFT through optimistic paths. <em>TC</em>, <em>74</em>(1), 57-70. (<a href='https://doi.org/10.1109/TC.2024.3461309'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standing as a foundational element within blockchain systems, the Byzantine Fault Tolerant (BFT) consensus has garnered significant attention over the past decade. The introduction of a Directed Acyclic Directed (DAG) structure into BFT consensus design, termed DAG-based BFT, has emerged to bolster throughput. However, prevalent DAG-based protocols grapple with substantial latency issues, suffering from a latency gap compared to non-DAG protocols. For instance, leading-edge DAG-based protocols named GradedDAG and BullShark exhibit a good-case latency of $4$ and $6$ communication rounds, respectively. In contrast, the non-DAG protocol, exemplified by PBFT, attains a latency of $3$ rounds in favorable conditions. To bridge this latency gap, we propose Remora, a novel DAG-based BFT protocol. Remora achieves a reduced latency of $3$ rounds by incorporating optimistic paths. At its core, Remora endeavors to commit blocks through the optimistic path initially, facilitating low latency in favorable situations. Conversely, in unfavorable scenarios, Remora seamlessly transitions to a pessimistic path to ensure liveness. Various experiments validate Remora's feasibility and efficiency, highlighting its potential as a robust solution in the realm of BFT consensus protocols.},
  archive      = {J_TC},
  author       = {Xiaohai Dai and Wei Li and Guanxiong Wang and Jiang Xiao and Haoyang Chen and Shufei Li and Albert Y. Zomaya and Hai Jin},
  doi          = {10.1109/TC.2024.3461309},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {57-70},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Remora: A low-latency DAG-based BFT through optimistic paths},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chiplet-gym: Optimizing chiplet-based AI accelerator design with reinforcement learning. <em>TC</em>, <em>74</em>(1), 43-56. (<a href='https://doi.org/10.1109/TC.2024.3457740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern Artificial Intelligence (AI) workloads demand computing systems with large silicon area to sustain throughput and competitive performance. However, prohibitive manufacturing costs and yield limitations at advanced tech nodes and die-size reaching the reticle limit restrain us from achieving this. With the recent innovations in advanced packaging technologies, chiplet-based architectures have gained significant attention in the AI hardware domain. However, the vast design space of chiplet-based AI accelerator design and the absence of system and package-level co-design methodology make it difficult for the designer to find the optimum design point regarding Power, Performance, Area, and manufacturing Cost (PPAC). This paper presents Chiplet-Gym, a Reinforcement Learning (RL)-based optimization framework to explore the vast design space of chiplet-based AI accelerators, encompassing the resource allocation, placement, and packaging architecture. We analytically model the PPAC of the chiplet-based AI accelerator and integrate it into an OpenAI gym environment to evaluate the design points. We also explore non-RL-based optimization approaches and combine these two approaches to ensure the robustness of the optimizer. The optimizer-suggested design point achieves $1.52\boldsymbol{\times}$ throughput, $0.27\boldsymbol{\times}$ energy, and $0.89\boldsymbol{\times}$ cost of its monolithic counterpart at iso-area.},
  archive      = {J_TC},
  author       = {Kaniz Mishty and Mehdi Sadi},
  doi          = {10.1109/TC.2024.3457740},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {43-56},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Chiplet-gym: Optimizing chiplet-based AI accelerator design with reinforcement learning},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLALM: A flexible low area-latency montgomery modular multiplication on FPGA. <em>TC</em>, <em>74</em>(1), 29-42. (<a href='https://doi.org/10.1109/TC.2024.3457739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Montgomery Modular Multiplication (MMM) is widely used in many public key cryptography systems. This paper presents a Flexible Low Area-Latency MMM (FLALM) implementation, which supports Generic Montgomery Modular Multiplication (GMM) and Square Montgomery Modular Multiplication (SMM) operations. A new SMM schedule for the Finely Integrated Product Scanning (FIPS) GMM algorithm is proposed to accelerate SMM with tiny additional design. Furthermore, a new FIPS dual-schedule is proposed to solve the data hazards of this algorithm. Finally, we explore the trade-off between area and latency, and present the FLALM to accelerate GMM and SMM. The FLALM is implemented on FPGA (Virtex-7 platform). The results show that the area*latency (AL) value of FLALM (wordsize $w$=128) is 38.1% and 44.7% better than the previous state-of-art scalable references when performing 1024-bit and 2048-bit GMM, respectively. Moreover, when computing SMM, the advantage of AL value is raised to 73.7% and 86.3% respectively.},
  archive      = {J_TC},
  author       = {Yujun Xie and Yuan Liu and Xin Zheng and Bohan Lan and Dengyun Lei and Dehao Xiang and Shuting Cai and Xiaoming Xiong},
  doi          = {10.1109/TC.2024.3457739},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {29-42},
  shortjournal = {IEEE Trans. Comput.},
  title        = {FLALM: A flexible low area-latency montgomery modular multiplication on FPGA},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CUSPX: Efficient GPU implementations of post-quantum signature SPHINCS+. <em>TC</em>, <em>74</em>(1), 15-28. (<a href='https://doi.org/10.1109/TC.2024.3457736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computers pose a serious threat to existing cryptographic systems. While Post-Quantum Cryptography (PQC) offers resilience against quantum attacks, its performance limitations often hinder widespread adoption. Among the three National Institute of Standards and Technology (NIST)-selected general-purpose PQC schemes, SPHINCS${}^{+}$ is particularly susceptible to these limitations. We introduce CUSPX (CUDA SPHINCS ${}^{+}$), the first large-scale parallel implementation of SPHINCS${}^{+}$ capable of running across 10,000 cores. CUSPX leverages a novel three-level parallelism framework, applying it to algorithmic parallelism, data parallelism, and hybrid parallelism. Notably, CUSPX introduces parallel Merkle tree construction algorithms for arbitrary parallel scales and several load-balancing solutions, further enhancing performance. By treating tasks parallelism as the top level of parallelism, CUSPX provides a four-level parallel scheme that can run with any number of tasks. Evaluated on a single GeForce RTX 3090 using the SPHINCS${}^{+}$-SHA-256-128s-simple parameter set, CUSPX achieves a single task's signature generation latency of 0.67 ms, demonstrating a 5,105$\times$ speedup over a single-thread version and an 18.50$\times$ speedup over the previous fastest implementation.},
  archive      = {J_TC},
  author       = {Ziheng Wang and Xiaoshe Dong and Heng Chen and Yan Kang and Qiang Wang},
  doi          = {10.1109/TC.2024.3457736},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {15-28},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CUSPX: Efficient GPU implementations of post-quantum signature SPHINCS+},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acceleration of fast sample entropy for FPGAs. <em>TC</em>, <em>74</em>(1), 1-14. (<a href='https://doi.org/10.1109/TC.2024.3457735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complexity measurement, essential in diverse fields like finance, biomedicine, climate science, and network traffic, demands real-time computation to mitigate risks and losses. Sample Entropy (SampEn) is an efficacious metric which quantifies the complexity by assessing the similarities among microscale patterns within the time-series data. Unfortunately, the conventional implementation of SampEn is computationally demanding, posing challenges for its application in real-time analysis, particularly for long time series. Field Programmable Gate Arrays (FPGAs) offer a promising solution due to their fast processing and energy efficiency, which can be customized to perform specific signal processing tasks directly in hardware. The presented work focuses on accelerating SampEn analysis on FPGAs for efficient time-series complexity analysis. A refined, fast, Lightweight SampEn architecture (LW SampEn) on FPGA, which is optimized to use sorted sequences to reduce computational complexity, is accelerated for FPGAs. Various sorting algorithms on FPGAs are assessed, and novel dynamic loop strategies and micro-architectures are proposed to tackle SampEn's undetermined search boundaries. Multi-source biomedical signals are used to profile the above design and select a proper architecture, underscoring the importance of customizing FPGA design for specific applications. Our optimized architecture achieves a 7x to 560x speedup over standard baseline architecture, enabling real-time processing of time-sensitive data.},
  archive      = {J_TC},
  author       = {Chao Chen and Chengyu Liu and Jianqing Li and Bruno da Silva},
  doi          = {10.1109/TC.2024.3457735},
  journal      = {IEEE Transactions on Computers},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Acceleration of fast sample entropy for FPGAs},
  volume       = {74},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
