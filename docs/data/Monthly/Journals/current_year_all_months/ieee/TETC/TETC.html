<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TETC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tetc">TETC - 98</h2>
<ul>
<li><details>
<summary>
(2025). Breakout local search solution to the offloading decision problem in a multi-access edge computing cloud-enabled network. <em>TETC</em>, <em>13</em>(3), 1328-1338. (<a href='https://doi.org/10.1109/TETC.2025.3598369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud offloading is an important technique for Internet of Things systems, as it allows devices with limited capabilities to access the powerful resources in the cloud when executing their applications. However, relying solely on the remote cloud is problematic, as the long access time from the far distance to the server makes real-time applications impossible to be executed. Multi-access edge computing addresses this by deploying cloud servers near the devices. The issue then becomes how to allocate devices between either remote cloud and multi-access edge computing, based on the device requirements. In this paper, we propose a Breakout Local Search-based solution that, given our designed binary integer linear programming model of the offloading problem, finds a near-optimal configuration for allocating devices between the two cloud types. The proposal is based on iterating between exploiting the local optimum found so far and perturbation of the current solution to explore more the search space. A comparison study shows that our proposal is better than baseline and conventional algorithms, speeding up the total service delay of tasks by at least 30 ms.},
  archive      = {J_TETC},
  author       = {Mina Kato and Tiago Koketsu Rodrigues and Nei Kato},
  doi          = {10.1109/TETC.2025.3598369},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1328-1338},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Breakout local search solution to the offloading decision problem in a multi-access edge computing cloud-enabled network},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incentive mechanism design for hierarchical federated learning with selfishness queue stability. <em>TETC</em>, <em>13</em>(3), 1316-1327. (<a href='https://doi.org/10.1109/TETC.2025.3562336'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The potential privacy breaches in centralized artificial intelligence model training have raised significant public concern. Hierarchical federated learning, as a technology addressing privacy and network efficiency issues, coordinates local devices using edge servers for model training and parameter updates, thereby reducing communication with central cloud servers and diminishing the risk of privacy leaks. However, in this context, the rise of node selfishness presents a significant challenge, undermining training efficiency and the quality of local models, thereby impacting the overall system’s performance. This paper addresses the issue by introducing a virtual node selfish queue to characterize dynamic selfishness, considering both training costs and rewards, and formulating the problem of maximizing model quality within the bounds of controlled node selfishness. Utilizing Lyapunov optimization, this issue is divided into two subproblems: controlling the quantity of node data and optimizing node associations. To solve these, we propose the Data Quantity Control and Client Association (DCCA) algorithm, based on the Hungarian method. This algorithm is shown to ensure boundedness, stability, and optimality in the system. Experimental results demonstrate that the DCCA algorithm enhances model quality by 8.43% and 13.83% compared to the Fmore and FedAvg algorithms, respectively.},
  archive      = {J_TETC},
  author       = {Zhuo Li and Fangxing Geng},
  doi          = {10.1109/TETC.2025.3562336},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1316-1327},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Incentive mechanism design for hierarchical federated learning with selfishness queue stability},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GLAMP: Generative learning for adversarially-robust malware prediction. <em>TETC</em>, <em>13</em>(3), 1299-1315. (<a href='https://doi.org/10.1109/TETC.2025.3583872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel Generative Malware Defense strategy. When an antivirus company detects a malware sample $m$, they should: (i) generate a set ${Var}(m)$ of several variants of $m$ and then (ii) train their malware classifiers on their usual training set augmented with ${Var}(m)$. We believe this leads to a more proactive defense by making the classifiers more robust to future malware developed by the attacker. We formally define the malware generation problem as a non-traditional optimization problem. Our novel GLAMP (Generative Learning for Adversarially-robust Malware Prediction) framework analyzes the complexity of the malware generation problem and includes novel malware variant generation algorithms for (i) that leverage the complexity results. Our experiments show that a sufficiently large percentage of samples generated by GLAMP are able to evade both commercial anti-virus and machine learning classifiers with evasion rates up to 83.81% and 50.54%, respectively. GLAMP then proposes an adversarial training model as well. Our experiments show that GLAMP generates running malware that can evade 11 white boxclassifiers and 4 commercial (i.e., black box) detectors. Our experiments show GLAMP’s best adversarial training engine improves the recall by 16.1% and the F1 score by 2.4%-5.4% depending on the test set used.},
  archive      = {J_TETC},
  author       = {Saurabh Kumar and Cristian Molinaro and Lirika Sola and V. S. Subrahmanian},
  doi          = {10.1109/TETC.2025.3583872},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1299-1315},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {GLAMP: Generative learning for adversarially-robust malware prediction},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving publicly verifiable outsourced distributed computation scheme for matrix multiplication. <em>TETC</em>, <em>13</em>(3), 1285-1298. (<a href='https://doi.org/10.1109/TETC.2025.3584354'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Publicly verifiable outsourced computation (PVC) facilitates the data owner to outsource some computation-intensive tasks to the powerful but untrusted cloud server, while enabling any client to check the integrity of results with little cost. Matrix multiplication is a fundamental operation in mathematics, which is widely used in many real-world applications. In this paper, we focus on PVC for matrix multiplication (PVC2M) and propose a new primitive called privacy-preserving publicly verifiable outsourced distributed computation scheme (PPVDC) for matrix multiplication. Different from the existing PVC2M solutions, our proposed scheme offers higher efficiency and reliability, where the computation is jointly calculated by multiple workers. In such a distributed setting, the computation result can be recovered if the number of workers who perform the computation honestly is no less than threshold. Besides, another technical highlight is to enhance privacy. Even though all workers are corrupted and may collude, they are unable to obtain any knowledge about the matrix $M$ outsourced by the data owner and the vector $x$ issued by the client at the end of the protocol. Security analysis demonstrates that our proposed PPVDC scheme can meet the desired security requirements under the computational Diffie-Hellman assumption. The detailed performance analysis and experimental evaluation further validate the efficiency of our scheme.},
  archive      = {J_TETC},
  author       = {Qiang Wang and Yiheng Chen and Fucai Zhou and Jian Xu},
  doi          = {10.1109/TETC.2025.3584354},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1285-1298},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Privacy-preserving publicly verifiable outsourced distributed computation scheme for matrix multiplication},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Path integral quantum annealing optimizations validated on 0-1 multidimensional knapsack problem. <em>TETC</em>, <em>13</em>(3), 1272-1284. (<a href='https://doi.org/10.1109/TETC.2025.3583224'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum Annealing (QA) is a metaheuristic designed to enhance Simulated Annealing by leveraging concepts from quantum mechanics, improving parallelization on classical computers. Studies have shown promising results for this technique in the field of NP-hard problems and constrained optimization. In this article, we examine Path Integral Quantum Annealing (PIQA), a well-known technique for simulating QA on conventional computers. We then propose optimizations to the algorithm, offering hardware software developers a suite of parallelization techniques evaluated for their effectiveness in enhancing quality and speed. The proposed approach encompasses four distinct degrees of optimization, leveraging techniques based on multiple-trial parallelism and a novel pre-optimization method. The article further proposes a methodology for handling multiple instances within the search space, whereby problem data is replicated into slices and allocated to concurrent processes during the simulation. Through empirical trials, we evaluate the impact of our optimization techniques on the convergence speed of the algorithm compared to unoptimized PIQA, using the Multidimensional Knapsack Problem as a benchmark. Our findings show that these optimizations, applied individually or collectively, enable the algorithm to achieve equal or superior results with fewer simulation steps. Overall, the results highlight the potential for future implementations of optimized PIQA on dedicated hardware.},
  archive      = {J_TETC},
  author       = {Evelina Forno and Riccardo Pignari and Vittorio Fra and Enrico Macii and Gianvito Urgese},
  doi          = {10.1109/TETC.2025.3583224},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1272-1284},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Path integral quantum annealing optimizations validated on 0-1 multidimensional knapsack problem},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved modular multiplication algorithms using solely IEEE 754 binary floating-point operations. <em>TETC</em>, <em>13</em>(3), 1259-1271. (<a href='https://doi.org/10.1109/TETC.2025.3582551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose three modular multiplication algorithms that use only the IEEE 754 binary floating-point operations. Several previous studies have used floating-point operations to perform modular multiplication. However, they considered only positive integers and did not utilize the dedicated sign bit in the floating-point representation. Our first algorithm is an extension of these studies, which are based on Shoup multiplication. By allowing operands to be negative, we increased the maximum supported modulus size by approximately 1.21 times. Our remaining two algorithms are based on Montgomery multiplication for positive and signed integers, respectively. Although these algorithms require more round-to-integral operations, they support a modulus size of up to twice as large as that for Shoup multiplication for positive integers. For processors with relatively low round-to-integral performance, we propose versions of the three algorithms without the round-to-integral operation. Evaluations on four CPUs with different levels of instruction performance show that floating-point-based algorithms, including the proposed algorithms, can be regarded as alternatives to integer-based algorithms for mid-sized moduli, especially when floating-point operations are faster on the processors.},
  archive      = {J_TETC},
  author       = {Yukimasa Sugizaki and Daisuke Takahashi},
  doi          = {10.1109/TETC.2025.3582551},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1259-1271},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Improved modular multiplication algorithms using solely IEEE 754 binary floating-point operations},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LSTable: A new white-box cipher for embedded devices in IoT against side-channel attacks. <em>TETC</em>, <em>13</em>(3), 1242-1258. (<a href='https://doi.org/10.1109/TETC.2025.3575787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embedded devices such as sensors and surveillance cameras play a critical role in the Internet of Things (IoT). However, their unattended and wireless features expose them to a high risk of side-channel attacks. These attacks exploit information leakage through side channels to deduce secret keys or even extract implementations of cryptographic algorithms. The possession of such knowledge empowers attackers to decrypt sensitive information transmitted among IoT devices, posing a significant threat to data confidentiality. To address this issue, we propose LSTable, a new white-box cipher enlightened by LS-Design. Instead of directly using secret keys for encryption and decryption, LSTable transforms secret keys into key-dependent lookup tables to mitigate side-channel attacks, and the size of these tables is designed to fit the hardware constraints of embedded devices. The security analysis of LSTable shows its security in both the black-box and white-box models. Furthermore, experimental evaluations on different devices exhibit that even the efficiency of the slowest instances of LSTable is 2.2 to 14.8 times that of existing space-hard white-box ciphers with IoT-friendly table sizes, while the energy consumption is only around 1/13 to 1/3.},
  archive      = {J_TETC},
  author       = {Yang Shi and Yimin Li and Qiaoliang Ouyang and Jiayao Gao and Shengjie Zhao},
  doi          = {10.1109/TETC.2025.3575787},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1242-1258},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {LSTable: A new white-box cipher for embedded devices in IoT against side-channel attacks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel RFET-based FPGA architecture based on delay-aware packing algorithm. <em>TETC</em>, <em>13</em>(3), 1230-1241. (<a href='https://doi.org/10.1109/TETC.2025.3572712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable devices are attracting growing interest as both a potential alternative and complement to traditional CMOS technology. This paper develops a novel field-programmable gate array (FPGA) architecture based on MClusters, which is made of fast and area-efficient 2-input look-up tables (LUTs) through reconfigurable field-effect transistors (RFETs). To fully utilize the MClusters, we propose an SAT-based delay-aware packing algorithm for the technology mapping. In addition, we integrate a partitioning algorithm to divide the circuit into several sub-circuits to further reduce the global routing resources and their associated switching energy of the system. Finally, we develop an efficient technology/circuit/system co-design framework for optimizing the overall performance of FPGAs. Based on comprehensive benchmarking, results demonstrate that optimal design yields significant reductions of up to 39% area, 36% wire length, and 40% switching energy compared to traditional CMOS 6-input LUT FPGAs.},
  archive      = {J_TETC},
  author       = {Sheng Lu and Liuting Shang and Sungyong Jung and Qilian Liang and Chenyun Pan},
  doi          = {10.1109/TETC.2025.3572712},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1230-1241},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A novel RFET-based FPGA architecture based on delay-aware packing algorithm},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). What, when, where to compute-in-memory for efficient matrix multiplication during machine learning inference. <em>TETC</em>, <em>13</em>(3), 1215-1229. (<a href='https://doi.org/10.1109/TETC.2025.3574508'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix multiplication is the dominant computation during Machine Learning (ML) inference. To efficiently perform such multiplication operations, Compute-in-memory (CiM) paradigms have emerged as a highly energy efficient solution. However, integrating compute in memory poses key questions, such as 1) What type of CiM to use: Given a multitude of CiM design characteristics, determining their suitability from architecture perspective is needed. 2) When to use CiM: ML inference includes workloads with a variety of memory and compute requirements, making it difficult to identify when CiM is more beneficial than standard processing cores. 3) Where to integrate CiM: Each memory level has different bandwidth and capacity, creating different data reuse opportunities for CiM integration. To answer such questions regarding on-chip CiM integration for accelerating ML workloads, we use an analytical architecture-evaluation methodology with tailored mapping algorithm. The mapping algorithm aims to achieve highest weight reuse and reduced data movements for a given CiM prototype and workload. Our analysis considers the integration of CiM prototypes into the cache levels of a tensor-core-like architecture, and shows that CiM integrated memory improves energy efficiency by up to $3.4 \times$ and throughput by up to $15.6 \times$ compared to established baseline with INT-8 precision. We believe the proposed work provides insights into what type of CiM to use, and when and where to optimally integrate it in the cache hierarchy for efficient matrix multiplication.},
  archive      = {J_TETC},
  author       = {Tanvi Sharma and Mustafa Ali and Indranil Chakraborty and Kaushik Roy},
  doi          = {10.1109/TETC.2025.3574508},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1215-1229},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {What, when, where to compute-in-memory for efficient matrix multiplication during machine learning inference},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The cancelable multimodal template protection algorithm based on random index. <em>TETC</em>, <em>13</em>(3), 1200-1214. (<a href='https://doi.org/10.1109/TETC.2025.3574359'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current multimodal template protection methods typically require encryption or transformation of the original biometric features. However, these operations carry certain risks, as attackers may reverse-engineer or decrypt the protected multimodal templates to retrieve partial or complete information about the original templates, leading to the leakage of the original biometric features. To address this issue, we propose a cancelable multimodal template protection method based on random indexing. First, hash functions are used to generate integer sequences as index values, which are then employed to create single-modal cancelable templates using random binary vectors. Second, the single-modal cancelable templates are used as indices for random binary sequences, which locate the corresponding template information and are filled into the fusion cancelable template at the respective positions, achieving template fusion. The resulting template is unrelated to the original biometric features. Finally, without directly storing the binary factor sequences, an XOR operation is performed on the extended biometric feature vectors and random binary sequences to generate the encoded key. Experimental results demonstrate that the proposed method significantly enhances performance on the FVC2002DB1 fingerprint, MMCBNU_6000 finger-vein, and NUPT_FPV databases, while also satisfying the standards for cancelable biometric feature design. We also analyze four privacy and security attacks against this scheme.},
  archive      = {J_TETC},
  author       = {Huabin Wang and Mingzhao Wang and Xinxin Liu and Yingfan Cheng and Fei Liu and Jian Zhou and Liang Tao},
  doi          = {10.1109/TETC.2025.3574359},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1200-1214},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {The cancelable multimodal template protection algorithm based on random index},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DT-net: Point cloud completion network with neighboring adaptive denoiser and splitting-based upsampling transformer. <em>TETC</em>, <em>13</em>(3), 1185-1199. (<a href='https://doi.org/10.1109/TETC.2025.3573505'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion, which involves inferring missing regions of 3D objects from partial observations, remains a challenging problem in 3D vision and robotics. Existing learning-based frameworks typically leverage an encoder-decoder architecture to predict the complete point cloud based on the global shape representation extracted from the incomplete input, or further introduce a refinement network to optimize the obtained complete point cloud in a coarse-to-fine manner, which is unable to capture fine-grained local geometric details and filled with noisy points in the thin or complex structure. In this article, we propose a novel coarse-to-fine point cloud completion framework called DT-Net, by focusing on coarse point cloud denoising and multi-level upsampling. Specifically, we propose a Neighboring Adaptive Denoiser (NAD) to effectively denoise the coarse point cloud generated by an autoencoder, and reduce noise around the slender structures, making them clear and well represented. Moreover, a novel Splitting-based Upsampling Transformer (SUT), which effectively incorporates spatial and semantic relationships between local neighborhoods in the point cloud, is also proposed for multi-level upsampling. Extensive qualitative and quantitative experiments demonstrate that our method outperforms state-of-the-art methods under widely used benchmarks.},
  archive      = {J_TETC},
  author       = {Aihua Mao and Qing Liu and Yuxuan Tang and Sheng Ye and Ran Yi and Minjing Yu and Yong-Jin Liu},
  doi          = {10.1109/TETC.2025.3573505},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1185-1199},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DT-net: Point cloud completion network with neighboring adaptive denoiser and splitting-based upsampling transformer},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PipeDAP: An efficient communication framework for scheduling decoupled all-reduce primitives in distributed DNN training. <em>TETC</em>, <em>13</em>(3), 1170-1184. (<a href='https://doi.org/10.1109/TETC.2025.3573522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Communication scheduling effectively improves the scalability of distributed deep learning by overlapping computation and communication tasks during training. However, existing communication scheduling frameworks based on tensor partitioning suffer from two fundamental issues: (1) partitioning schemes at the data volume level introduce extensive startup overheads leading to higher energy consumption, and (2) partitioning schemes at the communication primitive level do not provide optimal scheduling resulting in longer training time. In this article, we propose an efficient communication mechanism, namely PipeDAP, which schedules decoupled all-reduce operations in a near-optimal order to minimize the time and energy consumption of training DNN models. We build the mathematical model for PipeDAP and derive the near-optimal scheduling order of the reduce-scatter and all-gather operations. Meanwhile, we leverage simultaneous communication of reduce-scatter and all-gather operations to further reduce the startup overheads. We implement the PipeDAP architecture on PyTorch framework, and apply it for distributed training of benchmark DNN models. Experimental results on two GPU clusters demonstrate that PipeDAP achieves up to 1.82x speedup and saves up to 45.4% of energy consumption compared to the state-of-the-art communication scheduling frameworks.},
  archive      = {J_TETC},
  author       = {Yunqi Gao and Bing Hu and Mahdi Boloursaz Mashhadi and Wei Wang and Rahim Tafazolli and Mérouane Debbah},
  doi          = {10.1109/TETC.2025.3573522},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1170-1184},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {PipeDAP: An efficient communication framework for scheduling decoupled all-reduce primitives in distributed DNN training},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware-software co-design low-complexity fuzzy algorithm for high-dimensional feature space with in-situ learning. <em>TETC</em>, <em>13</em>(3), 1156-1169. (<a href='https://doi.org/10.1109/TETC.2025.3573013'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exploitation of fuzzy techniques in machine learning for modelling uncertainty is highly versatile and widespread. In addition to providing a suitable fuzzy algorithm, its hardware implementation targeting proper performance in real-time applications is another challenge. In this paper, we propose a technique to deal with high-dimensional systems. Despite considering the uncertainty in the data, the proposed algorithm (HCLFA) is low-complex, thanks to the small number of calculations required. In the proposed algorithm, a D-dimensional system is transformed into a D one-dimensional system, and a Gaussian function is distributed over each training data. The main contribution is to provide a complete fuzzy system with tuning one parameter and simple operation without any complex mathematical relationships. The paper also presents an efficient memristor-crossbar hardware structure for the algorithm. The circuit is designed, and the ArC One hardware platform is used to demonstrate the results experimentally. It can learn and adapt to the application without the need for a host system and can be utilized as a basis for providing evolutionary systems. The results and comparisons indicate the significant superiority of the proposed algorithm, and achieve an accuracy of $99.67 \pm 0.25$ on the MNIST dataset.},
  archive      = {J_TETC},
  author       = {Mohamad Momtaz Sheykh Ahmad and Sajad Haghzad Klidbary},
  doi          = {10.1109/TETC.2025.3573013},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1156-1169},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Hardware-software co-design low-complexity fuzzy algorithm for high-dimensional feature space with in-situ learning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Q-point: A numeric format for quantum circuit simulation using polar form complex numbers. <em>TETC</em>, <em>13</em>(3), 1142-1155. (<a href='https://doi.org/10.1109/TETC.2025.3572935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum circuit simulation is playing a critical role in the current era of quantum computing. However, quantum circuit simulation suffers from huge memory requirements that scale exponentially according to the number of qubits. Our observation reveals that the conventional complex number representation using real and imaginary values adds to the memory overhead beyond the intrinsic cost of simulating quantum states. Instead, using the radius and phase value of a complex number better reflects the properties of the complex values used in the quantum circuit simulation providing better memory efficiency. This paper proposes q-Point, a compact numeric format for quantum circuit simulation that utilizes polar form representation instead of rectangular form representation to store complex numbers. The proposed q-Point format consists of three fields: i) exponent bits for radius value ii) mantissa bits for radius value iii) mantissa bits for phase value. However, a naive application of the q-Point format has the potential to cause issues with both simulation accuracy and simulation speed. To preserve simulation accuracy with fewer bits, we use a multi-level encoding scheme that employs different mantissa bits depending on the exponent range. Additionally, to prevent possible slowdown due to the add operation in polar form complex numbers, we use a technique that adaptively applies both polar and rectangular forms. Equipped with these optimizations, the proposed q-Point format demonstrates reasonable simulation accuracy while using only half of the memory requirement using the baseline format. Additionally, the q-Point format enables an average of 1.37× and 1.16× faster simulation for QAOA and VQE benchmark circuits.},
  archive      = {J_TETC},
  author       = {Seungwoo Choi and Enhyeok Jang and Youngmin Kim and Sungwoo Ahn and Won Woo Ro},
  doi          = {10.1109/TETC.2025.3572935},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1142-1155},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Q-point: A numeric format for quantum circuit simulation using polar form complex numbers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time access control for background and co-occurrence image privacy protection. <em>TETC</em>, <em>13</em>(3), 1130-1141. (<a href='https://doi.org/10.1109/TETC.2025.3572396'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s digital age, the proliferation of social networks and advanced camera technology has led to countless images being shared on online social platforms daily, potentially resulting in significant breaches of personal privacy. In recent years, many methods have been proposed to protect image privacy, allowing users to be notified of potential privacy leaks before publishing their photos. However, most existing research primarily addresses the privacy protection of image owners or co-owners, while neglecting the privacy of people who appear in the background of others’ images or who are co-occurring with others in the same image. In this paper, we propose a system capable of conducting real-time access control for protecting privacy of every individual appearing in a photo, as well as the privacy of people who co-occur in the same image. Specifically, we first detect all the faces in the image, then use a facial recognition algorithm to identify the corresponding users’ privacy policies, and finally determine whether the image violates any user’s privacy policy. In order to provide real-time access control, we have designed a facial attribute index tree to speed up the process of user identification. The experimental results show that compared with the method without our proposed index tree, our approach improves the time efficiency by almost two orders of magnitude while maintaining the accuracy of more than 97%.},
  archive      = {J_TETC},
  author       = {Chaoquan Cai and Dan Lin and Kannappan Palaniappan and Chris Clifton},
  doi          = {10.1109/TETC.2025.3572396},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1130-1141},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Real-time access control for background and co-occurrence image privacy protection},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic task replication with imperfect fault detection in multicore cyber-physical systems. <em>TETC</em>, <em>13</em>(3), 1113-1129. (<a href='https://doi.org/10.1109/TETC.2025.3572277'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task replication is a common technique for achieving fault tolerance. However, its effectiveness is limited by the accuracy of the fault detection mechanism; imperfect detection imposes a ceiling on achievable reliability. While perfect fault detection mechanisms offer higher reliability, they introduce significant overhead. To address this, we introduce Dynamic Task Replication, a fault tolerance technique that dynamically determines the number of replicas at runtime to overcome the limitations of imperfect fault detection. Our primary contribution, Reliability-Aware Replica-Efficient Dynamic Task Replication, optimizes this approach by minimizing the expected number of replicas while achieving the desired reliability target. We incorporate actual execution times into the reliability assessment. Additionally, we propose the Energy-Aware Reliability-Guaranteeing scheduling technique, which integrates our optimized replication method into hard real-time systems and leverages Dynamic Voltage and Frequency Scaling to minimize energy consumption while ensuring reliability and system schedulability. Experimental results demonstrate that our method requires 24% fewer replicas on average than the N-Modular Redundancy technique, with the advantage increasing to 58% for tasks with low base reliabilities. Furthermore, our scheduling technique significantly conserves energy and enhances feasibility compared to existing methods across diverse system workloads.},
  archive      = {J_TETC},
  author       = {Hossein Hosseini and Mohsen Ansari and Jörg Henkel},
  doi          = {10.1109/TETC.2025.3572277},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1113-1129},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Dynamic task replication with imperfect fault detection in multicore cyber-physical systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MALITE: Lightweight malware detection and classification for constrained devices. <em>TETC</em>, <em>13</em>(3), 1099-1112. (<a href='https://doi.org/10.1109/TETC.2025.3566370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, malware is one of the primary cyber threats to organizations, pervading all types of computing devices, including resource constrained devices such as mobile phones, tablets and embedded devices like Internet-of-Things (IoT) devices. In recent years, researchers have leveraged machine learning based strategies for malware detection and classification. However, malware analysis approaches can only be employed in resource constrained environments if the methods are lightweight in nature. In this paper, we present MALITE, a lightweight malware analysis system, that can distinguish between benign and malicious binaries and classify various malware families. MALITE converts a binary into a grayscale or an RGB image requiring low memory and battery power consumption and uses computationally inexpensive malware analysis strategies. We have designed MALITE-MN, a lightweight neural network based architecture and MALITE-HRF, an ultra lightweight random forest based method that uses histogram features extracted by a sliding window. An extensive empirical evaluation is conducted on seven publicly available datasets (Malimg, Microsoft BIG, Dumpware10, MOTIF, Drebin, CICAndMal2017 and MalNet), and performance is compared to four state-of-the-art baselines. The results show that MALITE-MN and MALITE-HRF not only accurately identify and classify malware but also respectively consume several orders of magnitude lower resources (in terms of both memory as well as computation capabilities), making them much more suitable for resource constrained environments.},
  archive      = {J_TETC},
  author       = {Sidharth Anand and Barsha Mitra and Soumyadeep Dey and Abhinav Rao and Rupsha Dhar and Jaideep Vaidya},
  doi          = {10.1109/TETC.2025.3566370},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1099-1112},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {MALITE: Lightweight malware detection and classification for constrained devices},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid quantum ResNet for time series classification. <em>TETC</em>, <em>13</em>(3), 1083-1098. (<a href='https://doi.org/10.1109/TETC.2025.3563944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Residual networks (ResNet) are known to be effective for image classification. However, challenges such as computational time remain because of the significant number of parameters. Quantum computing using quantum entanglement and quantum parallelism is an emerging computing paradigm that addresses this issue. Although quantum advantage is still studied in many research fields, quantum machine learning is a research area that leverages the strengths of quantum computing and machine learning. In this study, we investigated the quantum speedup with respect to the number of parameters in each model for a time-series classification task. This paper proposes a novel hybrid quantum residual network (HQResNet) inspired by the classical ResNet for time-series classification. HQResNet introduces a classical layer before a quantum convolutional neural network (QCNN), where the QCNN is used as a residual block. These structures enable shortcut connections and are particularly effective in achieving classification tasks without a data re-uploading scheme. We used ultra-wide-band (UWB) channel impulse response data to demonstrate the performance of the proposed algorithm and compared the state-of-the-art benchmarks with HQResNet using evaluation metrics. The results show that HQResNet achieved high performance with a small number of trainable parameters.},
  archive      = {J_TETC},
  author       = {Dae-Il Noh and Seon-Geun Jeong and Won-Joo Hwang},
  doi          = {10.1109/TETC.2025.3563944},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1083-1098},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Hybrid quantum ResNet for time series classification},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing graph processing workloads in heterogeneous CPU-PIM systems. <em>TETC</em>, <em>13</em>(3), 1068-1082. (<a href='https://doi.org/10.1109/TETC.2025.3563249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Processing-in-Memory (PIM) offers a promising architecture to alleviate the memory wall challenge in graph processing applications. The key aspect of PIM is to incorporate logic within the memory, thereby leveraging the near-data advantages. State-of-the-art PIM-based graph processing accelerators tend to offload more to the memory in order to maximize near-data benefits, causing significant load imbalance in PIM systems. In this paper, we demonstrate that this intention is not true and that host processors still play a vital role in heterogeneous CPU-PIM systems. For this purpose, we propose CAPLBS, an online contention-aware Processing-in-Memory load-balance scheduler for graph processing applications in CPU-PIM systems. The core concept of CAPLBS is to steal workload candidates back to host processors with minimal off-chip data synchronization overhead when some host processors are idle. To model data contentions among workloads and determine the stealing decision, a measurement structure called Locality Cohesive Subgraph is proposed by deeply exploring the connectivity of the input graph and the memory access patterns of deployed graph applications. Experimental results show that CAPLBS achieved an average speed-up of 4.8× and 1.3× (up to 9.1× and 1.9×) compared with CPU-only and the upper bound of locality-aware fine-grained in-memory atomics. Moreover, CAPLBS adds no hardware overhead and works well with existing CPU-PIM graph processing accelerators.},
  archive      = {J_TETC},
  author       = {Sheng Xu and Chun Li and Le Luo and Ming Zheng and Liang Yan and Xingqi Zou and Xiaoming Chen},
  doi          = {10.1109/TETC.2025.3563249},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1068-1082},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Balancing graph processing workloads in heterogeneous CPU-PIM systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and implementation of cost-effective end-to-end authentication protocol for PUF-enabled IoT devices. <em>TETC</em>, <em>13</em>(3), 1055-1067. (<a href='https://doi.org/10.1109/TETC.2025.3563064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ubiquitous presence of Internet of Things (IoT) prospers in every aspect of human life. The low-powered sensors, actuators, and mobile devices in IoT transfer a high volume of security-sensitive data. Unmonitored IoT devices are highly susceptible to security vulnerabilities. Their operating environment, with minimal or no safeguards, allows physical invasion. The conventional end-to-end authentications protocols are inadequate because of the limited resources and ambient working environment of IoT. In this direction, a lightweight and secure end-to-end authentication protocol is proposed for the Physically Unclonability Function (PUF) embedded IoT devices by processing them in pairs. PUF promises to be a unique hardware-based security solution for resource-constrained devices. The proposed protocol exploits the coherent conduct of public and private key-based cryptosystems with PUF. The protocol integrates the concept of ECC with ECDH and the cryptographic hash function. Security of the proposed protocol is validated using authentication validation, BAN logic, Scyther tool, and against different adversarial attacks. The performance evaluation and extensive comparative study of the proposed protocol highlight its lightweight feature. The practical feasibility of the proposed protocol is verified by an empirical evaluation using an Arbiter PUF implemented on Xilinx Spartan-3E FPGA and Raspberry Pi as an IoT device.},
  archive      = {J_TETC},
  author       = {Sourav Roy and Mahabub Hasan Mahalat and Bibhash Sen},
  doi          = {10.1109/TETC.2025.3563064},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1055-1067},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Design and implementation of cost-effective end-to-end authentication protocol for PUF-enabled IoT devices},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLOG-CD: Curriculum learning based on oscillating granularity of class decomposed medical image classification. <em>TETC</em>, <em>13</em>(3), 1043-1054. (<a href='https://doi.org/10.1109/TETC.2025.3562620'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Curriculum learning strategies have been proven to be effective in various applications and have gained significant interest in the field of machine learning. It has the ability to improve the final model’s performance and accelerate the training process. However, in the medical imaging domain, data irregularities can make the recognition task more challenging and usually result in misclassification between the different classes in the dataset. Class-decomposition approaches have shown promising results in solving such a problem by learning the boundaries within the classes of the data set. In this paper, we present a novel convolutional neural network (CNN) training method based on the curriculum learning strategy and the class decomposition approach, which we call CLOG-CD, to improve the performance of medical image classification. We evaluated our method on four different imbalanced medical image datasets, such as Chest X-ray (CXR), brain tumour, digital knee x-ray, and histopathology colorectal cancer (CRC). CLOG-CD utilises the learnt weights from the decomposition granularity of the classes, and the training is accomplished from descending to ascending order (i.e. anti-curriculum technique). We also investigated the classification performance of our proposed method based on different acceleration factors and pace function curricula. We used two pre-trained networks, ResNet-50 and DenseNet-121, as the backbone for CLOG-CD. The results with ResNet-50 show that CLOG-CD has the ability to improve classification performance with an accuracy of 96.08% for the CXR dataset, 96.91% for the brain tumour dataset, 79.76% for the digital knee x-ray, and 99.17% for the CRC dataset, compared to other training strategies. In addition, with DenseNet-121, CLOG-CD has achieved 94.86%, 94.63%, 76.19%, and 99.45% for CXR, brain tumour, digital knee x-ray, and CRC datasets, respectively.},
  archive      = {J_TETC},
  author       = {Asmaa Abbas and Mohamed Medhat Gaber and Mohammed M. Abdelsamea},
  doi          = {10.1109/TETC.2025.3562620},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1043-1054},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CLOG-CD: Curriculum learning based on oscillating granularity of class decomposed medical image classification},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Approximated coded computing: Towards fast, private and secure distributed machine learning. <em>TETC</em>, <em>13</em>(3), 1030-1042. (<a href='https://doi.org/10.1109/TETC.2025.3562192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a large-scale distributed machine learning system, coded computing has attracted wide-spread attention since it can effectively alleviate the impact of stragglers. However, several emerging problems greatly limit the performance of coded distributed systems. First, an existence of colluding workers who collude results with each other leads to serious privacy leakage issues. Second, there are few existing works considering security issues in data transmission of distributed computing systems/or coded distributed machine learning systems. Third, the number of required results for which need to wait increases with the degree of decoding functions. In this article, we design a secure and private approximated coded distributed computing (SPACDC) scheme that deals with the above-mentioned problems simultaneously. Our SPACDC scheme guarantees data security during the transmission process using a new encryption algorithm based on elliptic curve cryptography. Especially, the SPACDC scheme does not impose strict constraints on the minimum number of results required to be waited for. An extensive performance analysis is conducted to demonstrate the effectiveness of our SPACDC scheme. Furthermore, we present a secure and private distributed learning algorithm based on the SPACDC scheme, which can provide information-theoretic privacy protection for training data. Our experiments show that the SPACDC-based deep learning algorithm achieves a significant speedup over the baseline approaches.},
  archive      = {J_TETC},
  author       = {Houming Qiu and Kun Zhu and Nguyen Cong Luong and Dusit Niyato},
  doi          = {10.1109/TETC.2025.3562192},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1030-1042},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Approximated coded computing: Towards fast, private and secure distributed machine learning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Certificateless sanitizable signature with designated verifier. <em>TETC</em>, <em>13</em>(3), 1019-1029. (<a href='https://doi.org/10.1109/TETC.2025.3562050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new type of digital signature, sanitizable signature enables a semi-trusted entity to alter a signed document and re-create a signature of the altered document in the name of original signer. This approach offers an effective solution to sanitize sensitive information in signed documents while ensuring the authenticity of sanitized documents. Most of current sanitizable signature schemes have the complex certificate management issue or the key escrow limitation. Recently, two certificateless sanitizable signature schemes have been proposed to address the above issues. However, they both rely on costly bilinear pairings, which incur high computation costs to create signature, make sanitization and perform verification. In the work, we design a pairing-free certificateless sanitizable signature scheme with a designated verifier. The proposed scheme achieves signature verification through a designated verifier, thereby preventing malicious propagation and illegal abuse of signatures. By eliminating the need for pairing operations, the scheme offers substantial improvements in computational efficiency. Security proofs demonstrate that it satisfies existential unforgeability and immutability against adaptive chosen message attacks. In addition, simulation experiments indicate that our approach reduces the computation costs of signature generation, sanitization, and verification by approximately 88.15%/88.48%, 99.98%/99.01%, and 71.22%/78.64%, respectively, when compared to the most recent two certificateless sanitizable signature schemes.},
  archive      = {J_TETC},
  author       = {Qi Sun and Yang Lu and Yinxia Sun and Jiguo Li},
  doi          = {10.1109/TETC.2025.3562050},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1019-1029},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Certificateless sanitizable signature with designated verifier},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable and RISC-V programmable near-memory computing architectures for edge nodes. <em>TETC</em>, <em>13</em>(3), 1003-1018. (<a href='https://doi.org/10.1109/TETC.2025.3555869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread adoption of data-centric algorithms, particularly artificial intelligence (AI) and machine learning (ML), has exposed the limitations of centralized processing infrastructures, driving a shift towards edge computing. This necessitates stringent constraints on energy efficiency, which traditional von Neumann architectures struggle to meet. The compute-in-memory (CIM) paradigm has emerged as a better candidate due to its efficient exploitation of the available memory bandwidth. However, existing CIM solutions require a high implementation effort and lack flexibility from a software integration standpoint. This work proposes a novel, software-friendly, general-purpose, and low-integration-effort near-memory computing (NMC) approach, paving the way for the adoption of CIM-based systems in the next generation of edge computing nodes. Two architectural variants, NM-Caesar and NM-Carus, are proposed and characterized to target different trade-offs in area efficiency, performance, and flexibility, covering a wide range of embedded microcontrollers. Post-layout simulations show up to 28.0 × and 53.9 × lower execution time and 25.0 × and 35.6 × higher energy efficiency at system level, respectively, compared to the execution of the same tasks on a state-of-the-art RISC-V CPU (RV32IMC). NM-Carus achieves a peak energy efficiency of 306.7 GOPS/W in 8-bit matrix multiplications, surpassing recent state-of-the-art in- and near-memory circuits.},
  archive      = {J_TETC},
  author       = {Michele Caon and Clément Choné and Pasquale Davide Schiavone and Alexandre Levisse and Guido Masera and Maurizio Martina and David Atienza},
  doi          = {10.1109/TETC.2025.3555869},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1003-1018},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Scalable and RISC-V programmable near-memory computing architectures for edge nodes},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Virtual reinforcement learning for defect prediction in smart manufacturing. <em>TETC</em>, <em>13</em>(3), 990-1002. (<a href='https://doi.org/10.1109/TETC.2025.3546244'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has focused on the integration of smart manufacturing and deep learning owing to the widespread application of neural computation. For deep learning, how to construct the architecture of a neural network is a critical issue. Especially on defect prediction or detection, a proper neural architecture could effectively extract features from the given manufacturing data to accomplish the targeted task. In this paper, we introduce a Virtual Space concept to effectively shrink the search space of potential neural network structures, with the aim of downgrading the computation complexity for learning and accuracy derivation. In addition, a novel reinforcement learning model, namely, Virtual Proximal Policy Optimization (Virtu-PPO), is developed to efficiently and effectively discover the optimal neural network structure. We also propose an optimization strategy to enhance the searching process of neural architecture for defect prediction. In addition, the proposed model is applied on several real-world manufacturing datasets to show the performance and practicability of defect prediction.},
  archive      = {J_TETC},
  author       = {Yi-Cheng Chen and Mu-Ping Chang and Wang-Chien Lee},
  doi          = {10.1109/TETC.2025.3546244},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {990-1002},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Virtual reinforcement learning for defect prediction in smart manufacturing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two low-cost and security-enhanced implementations against side-channel attacks of NTT for lattice-based cryptography. <em>TETC</em>, <em>13</em>(3), 977-989. (<a href='https://doi.org/10.1109/TETC.2025.3552941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lattice-based cryptography is considered secure against quantum computing attacks. However, naive implementations on embedded devices are vulnerable to side-channel attacks (SCAs) with full key recovery possible through power and electromagnetic leakage analysis. This article presents two protection schemes, masking and shuffling, for the baseline Radix-2 multi-path delay commutator (R2MDC) number theoretic transform (NTT) architecture. The proposed masking NTT scheme introduces a random number to protect the secret key during the decryption phase and leverages the linear property of arithmetic transform in NTT polynomial multiplication. By adjusting the comparing decoding threshold, the masking method greatly reduces the ratio of $t$-$test$ value exceeding the threshold of unprotected NTT scheme from 77.38% to 3.91%. An ingenious shuffling transform process is also proposed to disturb the calculation sequence of butterfly transformation, adapting to the high-throughput architecture of R2MDC-NTT. This shuffling NTT scheme does not require operations to remove shuffle or additional operation cycles, reducing the leakage ratio to 13.49% with minimal extra hardware resources and wide applicability. The proposed masking and shuffling techniques effectively suppress side-channel leakage, improving the security of hardware architecture while maintaining a balance between overall performance and additional hardware resources.},
  archive      = {J_TETC},
  author       = {Yijun Cui and Jiatong Tian and Chuanchao Lu and Yang Li and Ziying Ni and Chenghua Wang and Weiqiang Liu},
  doi          = {10.1109/TETC.2025.3552941},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {977-989},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Two low-cost and security-enhanced implementations against side-channel attacks of NTT for lattice-based cryptography},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel porcelain fingerprinting technique. <em>TETC</em>, <em>13</em>(3), 964-976. (<a href='https://doi.org/10.1109/TETC.2025.3546602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Porcelain, as a significant cultural heritage, embodies the wisdom of human civilization. However, existing anti-counterfeiting and authentication technologies for porcelain are often unreliable and costly. This paper proposes a physical unclonable functions (PUF) design based on crack physical feature extraction for the anti-counterfeiting and authentication of Gold-Wire porcelain. The proposed method generates PUF information by extracting inherent physical deviations in the surface cracks of Gold-Wire porcelain. First, a standard crack extraction process is established using digital image processing to obtain crack information from the porcelain surface. Then, a physical feature extraction model based on the chain code encoding technique and the Delaunay triangulation technique is used to derive the physical feature values from the cracks. Subsequently, a PUF encoding algorithm is designed to convert these physical feature values into a PUF response. Finally, the security and reliability of the designed PUF are evaluated, and a PUF-based porcelain authentication protocol is developed. Experimental results show that the proposed PUF exhibits 50.16% uniqueness and 98.85% reliability, and the PUF data successfully passed the NIST randomness test, demonstrating that the proposed technology can effectively achieve low-cost, high-reliability anti-counterfeiting for commercial porcelain.},
  archive      = {J_TETC},
  author       = {Chengjie Wang and Yuejun Zhang and Ziyu Zhou},
  doi          = {10.1109/TETC.2025.3546602},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {964-976},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A novel porcelain fingerprinting technique},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SoSTA: Skill-oriented stable task assignment with bidirectional preferences in crowdsourcing. <em>TETC</em>, <em>13</em>(3), 947-963. (<a href='https://doi.org/10.1109/TETC.2025.3548672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional task assignment approaches in crowdsourcing platforms have focused on optimizing utility for workers or tasks, often neglecting the general utility of the platform and the influence of mutual preference considering skill availability and budget restrictions. This oversight can destabilize task allocation outcomes, diminishing user experience, and, ultimately, the platform’s long-term utility and gives rise to the Worker Task Stable Matching (WTSM) problem. To solve WTSM, we propose the Skill-oriented Stable Task Assignment with a Bi-directional Preference (SoSTA) method based on deferred acceptance strategy. SoSTA aims to generate stable allocations between tasks and workers considering mutually their preferences, optimizing overall utility while following skill and budget constraints. Our study redefines the general utility of the platform as an amalgamation of utilities on both the workers’ and tasks’ sides, incorporating the preference lists of each worker or task based on their respective utility scores for the other party. SoSTA incorporates Multi Skill-oriented Stable Worker Task Mapping (Multi-SoS-WTM) algorithm for contributions with multiple skills per worker. SoSTA is rational, non-wasteful, fair, and hence stable. SoSTA outperformed other approaches in the simulations of the MeetUp dataset. SoSTA improves execution speed by 80%, task completion rate by 60%, and user happiness by 8%.},
  archive      = {J_TETC},
  author       = {Riya Samanta and Soumya K. Ghosh and Sajal K. Das},
  doi          = {10.1109/TETC.2025.3548672},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {947-963},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SoSTA: Skill-oriented stable task assignment with bidirectional preferences in crowdsourcing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLCL: Feature-level contrastive learning for few-shot image classification. <em>TETC</em>, <em>13</em>(3), 935-946. (<a href='https://doi.org/10.1109/TETC.2025.3546366'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot classification is the task of recognizing unseen classes using a limited number of samples. In this paper, we propose a new contrastive learning method called Feature-Level Contrastive Learning (FLCL). FLCL conducts contrastive learning at the feature level and leverages the subtle relationships between positive and negative samples to achieve more effective classification. Additionally, we address the challenges of requiring a large number of negative samples and the difficulty of selecting high-quality negative samples in traditional contrastive learning methods. For feature learning, we design a Feature Enhancement Coding (FEC) module to analyze the interactions and correlations between nonlinear features, enhancing the quality of feature representations. In the metric stage, we propose a centered hypersphere projection metric to map feature vectors onto the hypersphere, improving the comparison between the support and query sets. Experimental results on four few-shot classification benchmark datasets demonstrate that our method, while simple in design, outperforms previous methods and achieves state-of-the-art performance. A detailed ablation study further confirms the effectiveness of each component of our model.},
  archive      = {J_TETC},
  author       = {Wenming Cao and Jiewen Zeng and Qifan Liu},
  doi          = {10.1109/TETC.2025.3546366},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {935-946},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {FLCL: Feature-level contrastive learning for few-shot image classification},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum implementation and analysis of SHA-2 and SHA-3. <em>TETC</em>, <em>13</em>(3), 919-934. (<a href='https://doi.org/10.1109/TETC.2025.3546648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computers have the potential to solve a number of hard problems that are believed to be almost impossible to solve by classical computers. This observation has sparked a surge of research to apply quantum algorithms against the cryptographic systems to evaluate its quantum resistance. In assessing the security strength of the cryptographic algorithms against the upcoming quantum threats, it is crucial to precisely estimate the quantum resource requirement (generally in terms of circuit depth and quantum bit count). The National Institute of Standards and Technology by the US government specified five quantum security levels so that the relative quantum strength of a given cipher can be compared to the standard ones. There have been some progress in the NIST-specified quantum security levels for the odd levels (i.e., 1, 3 and 5), following the work of Jaques et al. (Eurocrypt’20). However, levels 2 and 4, which correspond to the quantum collision finding attacks for the SHA-2 and SHA-3 hash functions, quantum attack complexities are arguably not well-studied. This is where our article fits in. In this article, we present novel techniques for optimizing the quantum circuit implementations for SHA-2 and SHA-3 algorithms in all the categories specified by NIST. After that, we evaluate the quantum circuits of target cryptographic hash functions for quantum collision search. Finally, we define the quantum attack complexity for levels 2 and 4, and comment on the security strength of the extended level. We present new concepts to optimize the quantum circuits at the component level and the architecture level.},
  archive      = {J_TETC},
  author       = {Kyungbae Jang and Sejin Lim and Yujin Oh and Hyunjun Kim and Anubhab Baksi and Sumanta Chakraborty and Hwajeong Seo},
  doi          = {10.1109/TETC.2025.3546648},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {919-934},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Quantum implementation and analysis of SHA-2 and SHA-3},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing wet-neuromorphic computing using bacterial gene regulatory neural networks. <em>TETC</em>, <em>13</em>(3), 902-918. (<a href='https://doi.org/10.1109/TETC.2025.3546119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biocomputing envisions the development computing paradigms using biological systems, ranging from micron-level components to collections of cells, including organoids. This paradigm shift exploits hidden natural computing properties, to develop miniaturized wet-computing devices that can be deployed in harsh environments, and to explore designs of novel energy-efficient systems. In parallel, we witness the emergence of AI hardware, including neuromorphic processors with the aim of improving computational capacity. This study brings together the concept of biocomputing and neuromorphic systems by focusing on the bacterial gene regulatory networks and their transformation into Gene Regulatory Neural Networks (GRNNs). We explore the intrinsic properties of gene regulations, map this to a gene-perceptron function, and propose an application-specific sub-GRNN search algorithm that maps the network structure to match a computing problem. Focusing on the model organism Escherichia coli, the base-GRNN is initially extracted and validated for accuracy. Subsequently, a comprehensive feasibility analysis of the derived GRNN confirms its computational prowess in classification and regression tasks. Furthermore, we discuss the possibility of performing a well-known digit classification task as a use case. Our analysis and simulation experiments show promising results in the offloading of computation tasks to GRNN in bacterial cells, advancing wet-neuromorphic computing using natural cells.},
  archive      = {J_TETC},
  author       = {Samitha Somathilaka and Sasitharan Balasubramaniam and Daniel P. Martins},
  doi          = {10.1109/TETC.2025.3546119},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {902-918},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Analyzing wet-neuromorphic computing using bacterial gene regulatory neural networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting entity information for robust prediction over event knowledge graphs. <em>TETC</em>, <em>13</em>(3), 890-901. (<a href='https://doi.org/10.1109/TETC.2025.3534243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Script event prediction is the task of predicting the subsequent event given a sequence of events that already took place. It benefits task planning and process scheduling for event-centric systems including enterprise systems, IoT systems, etc. Sequence-based and graph-based learning models have been applied to this task. However, when learning data is limited, especially in a multiple-participant-involved enterprise environment, the performance of such models falls short of expectations as they heavily rely on large-scale training data. To take full advantage of given data, in this article we propose a new type of knowledge graph (KG) that models not just events but also entities participating in the events, and we design a collaborative event prediction model exploiting such KGs. Our model identifies semantically similar vertices as collaborators to resolve unknown events, applies gated graph neural networks to extract event-wise sequential features, and exploits a heterogeneous attention network to cope with entity-wise influence in event sequences. To verify the effectiveness of our approach, we designed multiple-choice narrative cloze tasks with inadequate knowledge. Our experimental evaluation with three datasets generated from well-known corpora shows our method can successfully defend against such incompleteness of data and outperforms the state-of-the-art approaches for event prediction.},
  archive      = {J_TETC},
  author       = {Han Yu and Hongming Cai and Shengtung Tsai and Mengyao Li and Pan Hu and Jiaoyan Chen and Bingqing Shen},
  doi          = {10.1109/TETC.2025.3534243},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {890-901},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Exploiting entity information for robust prediction over event knowledge graphs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pervasive edge computing model for proactive intelligent data migration. <em>TETC</em>, <em>13</em>(3), 878-889. (<a href='https://doi.org/10.1109/TETC.2025.3528994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, there is a great attention of the research community for the intelligent management of data in a context-aware manner at the intersection of the Internet of Things (IoT) and Edge Computing (EC). In this article, we propose a strategy to be adopted by autonomous edge nodes related to their decision on what data should be migrated to specific locations of the infrastructure and support the desired requests for processing. Our intention is to arm nodes with the ability of learning the access patterns of offloaded data-driven tasks and predict which data should be migrated to the original ‘owners’ of tasks. Naturally, these tasks are linked to the processing of data that are absent at the original hosting nodes indicating the required data assets that need to be accessed directly. To identify these data intervals, we employ an ensemble scheme that combines a statistically oriented model and a machine learning scheme. Hence, we are able not only to detect the density of the requests but also to learn and infer the ‘strong’ data assets. The proposed approach is analyzed in detail by presenting the corresponding formulations being also evaluated and compared against baselines and models found in the respective literature.},
  archive      = {J_TETC},
  author       = {Georgios Boulougaris and Kostas Kolomvatsos},
  doi          = {10.1109/TETC.2025.3528994},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {878-889},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A pervasive edge computing model for proactive intelligent data migration},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual test-time adaptation with weighted contrastive learning and pseudo-label correction. <em>TETC</em>, <em>13</em>(3), 866-877. (<a href='https://doi.org/10.1109/TETC.2025.3528985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time adaptability is often required to maintain system accuracy in scenarios involving domain shifts caused by constantly changing environments. While continual test-time adaptation has been proposed to handle such scenarios, existing methods rely on high-accuracy pseudo-labels. Moreover, contrastive learning methods for continuous test-time adaptation consider the aggregation of features from the same class while neglecting the problem of aggregating similar features within the same class. Therefore, we propose “Weighted Contrastive Learning” and apply it to both pre-training and continual test-time adaptation. To address the issue of catastrophic forgetting caused by continual adaptation, previous studies have employed source-domain knowledge to stochastically recover the target-domain model. However, significant domain shifts may cause the source-domain knowledge to behave as noise, thus impacting the model's adaptability. Therefore, we propose “Domain-aware Pseudo-label Correction” to mitigate catastrophic forgetting and error accumulation without accessing the original source-domain data while minimizing the impact on model adaptability. The thorough evaluations in our experiments have demonstrated the effectiveness of our proposed approach.},
  archive      = {J_TETC},
  author       = {Shih-Chieh Chuang and Ching-Hu Lu},
  doi          = {10.1109/TETC.2025.3528985},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {866-877},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Continual test-time adaptation with weighted contrastive learning and pseudo-label correction},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software-defined number formats for high-speed belief propagation. <em>TETC</em>, <em>13</em>(3), 853-865. (<a href='https://doi.org/10.1109/TETC.2025.3528972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents the design and implementation of Software-Defined Floating-Point (SDF) number formats for high-speed implementation of the Belief Propagation (BP) algorithm. SDF formats are designed specifically to meet the numeric needs of the computation and are more compact representations of the data. They reduce memory footprint and memory bandwidth requirements without sacrificing accuracy, given that BP for loopy graphs inherently involves algorithmic errors. This article designs several SDF formats for sum-product BP applications by careful analysis of the computation. Our theoretical analysis leads to the design of 16-bit (half-precision) and 8-bit (mini-precision) widths. We moreover present highly efficient software implementation of the proposed SDF formats which is centered around conversion to hardware-supported single-precision arithmetic hardware. Our solution demonstrates negligible conversion overhead on commercially available CPUs. For Ising grids with sizes from 100 × 100 to 500 × 500, the 16- and 8-bit SDF formats along with our conversion module produce equivalent accuracy to double-precision floating-point format but with 2.86× speedups on average on an Intel Xeon processor. Particularly, increasing the grid size results in higher speed-up. For example, the proposed half-precision format with 3-bit exponent and 13-bit mantissa achieved the minimum and maximum speedups of 1.30× and 1.39× over single-precision, and 2.55× and 3.40× over double-precision, by increasing grid size from 100 × 100 to 500 × 500.},
  archive      = {J_TETC},
  author       = {Amir Sabbagh Molahosseini and JunKyu Lee and Hans Vandierendonck},
  doi          = {10.1109/TETC.2025.3528972},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {853-865},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Software-defined number formats for high-speed belief propagation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scatter-gather DMA performance analysis within an SoC-based control system for trapped-ion quantum computing. <em>TETC</em>, <em>13</em>(3), 841-852. (<a href='https://doi.org/10.1109/TETC.2025.3528899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatter-gather dynamic-memory-access (SG-DMA) is utilized in applications that require high bandwidth and low latency data transfers between memory and peripherals, where data blocks, described using buffer descriptors (BDs), are distributed throughout the memory system. The data transfer organization and requirements of a Trapped-Ion Quantum Computer (TIQC) possess characteristics similar to those targeted by SG-DMA. In particular, the ion qubits in a TIQC are manipulated by applying control sequences consisting primarily of modulated laser pulses. These optical pulses are defined by parameters that are (re)configured by the electrical control system. Variations in the operating environment and equipment make it necessary to create and run a wide range of control sequence permutations, which can be well represented as BD regions distributed across the main memory. In this article, we experimentally evaluate the latency and throughput of SG-DMA on Xilinx radiofrequency SoC (RFSoC) devices under a variety of BD and payload sizes as a means of determining the benefits and limitations of an RFSoC system architecture for TIQC applications.},
  archive      = {J_TETC},
  author       = {Tiamike Dudley and Jim Plusquellic and Eirini Eleni Tsiropoulou and Joshua Goldberg and Daniel Stick and Daniel Lobser},
  doi          = {10.1109/TETC.2025.3528899},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {841-852},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Scatter-gather DMA performance analysis within an SoC-based control system for trapped-ion quantum computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving deep neural network reliability via transient-fault-aware design and training. <em>TETC</em>, <em>13</em>(3), 829-840. (<a href='https://doi.org/10.1109/TETC.2024.3520672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have revolutionized several fields, including safety- and mission-critical applications, such as autonomous driving and space exploration. However, recent studies have highlighted that transient hardware faults can corrupt the model's output, leading to high misprediction probabilities. Since traditional reliability strategies, based on modular hardware, software replications, or matrix multiplication checksum impose a high overhead, there is a pressing need for efficient and effective hardening solutions tailored for DNNs. In this article we present several network design choices and a training procedure that increase the robustness of standard deep models and thoroughly evaluate these strategies with experimental analyses on vision classification tasks. We name DieHardNet the specialized DNN obtained by applying all our hardening techniques that combine knowledge from experimental hardware faults characterization and machine learning studies. We conduct extensive ablation studies to quantify the reliability gain of each hardening component in DieHardNet. We perform over 10,000 instruction-level fault injections to validate our approach and expose DieHardNet executed on GPUs to an accelerated neutron beam equivalent to more than 570,000 years of natural radiation. Our evaluation demonstrates that DieHardNet can reduce the critical error rate (i.e., errors that modify the inference) up to 100 times compared to the unprotected baseline model, without causing any increase in inference time.},
  archive      = {J_TETC},
  author       = {Fernando Fernandes dos Santos and Niccolò Cavagnero and Marco Ciccone and Giuseppe Averta and Angeliki Kritikakou and Olivier Sentieys and Paolo Rech and Tatiana Tommasi},
  doi          = {10.1109/TETC.2024.3520672},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {829-840},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Improving deep neural network reliability via transient-fault-aware design and training},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy efficient approximate computing framework for DNN acceleration using a probabilistic-oriented method. <em>TETC</em>, <em>13</em>(3), 816-828. (<a href='https://doi.org/10.1109/TETC.2024.3522307'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing (AxC) has recently emerged as a successful approach for optimizing energy consumption in error-tolerant applications, such as deep neural networks (DNNs). The enormous model size and high computation cost of DNNs present significant challenges for deployment in energy-efficient and resource-constrained computing systems. Emerging DNN hardware accelerators based on AxC designs selectively approximate the non-critical segments of computation to address these challenges. However, a systematic and principled approach that incorporates domain knowledge and approximate hardware for optimal approximation is still lacking. In this paper, we propose a probabilistic-oriented AxC (PAxC) framework that provides high energy savings with acceptable quality by considering the overall probability effect of approximation. To achieve aggressive approximate designs, we utilize the minimum likelihood error to determine the AxC synergy profile at both application and circuit levels. This enables effective coordination of the trade-off between energy and accuracy. Compared with a baseline design, the power-delay product (PDP) is significantly reduced by up to 83.66% with an acceptable accuracy reduction. Simulation and a case study of the image process validate the effectiveness of the proposed framework.},
  archive      = {J_TETC},
  author       = {Pengfei Huang and Ke Chen and Chenghua Wang and Weiqiang Liu},
  doi          = {10.1109/TETC.2024.3522307},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {816-828},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Energy efficient approximate computing framework for DNN acceleration using a probabilistic-oriented method},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D invisible cloak: A robust person stealth attack against object detector in complex 3D physical scenarios. <em>TETC</em>, <em>13</em>(3), 799-815. (<a href='https://doi.org/10.1109/TETC.2024.3513392'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a novel physical stealth attack against the person detectors in real world. For the first time, we consider the impacts of those complex and challenging 3D physical constraints (e.g., radian, wrinkle, occlusion, angle, etc.) on person stealth attacks, and propose 3D transformations to generate robust 3D invisible cloak. We launch the person stealth attacks in 3D physical space instead of 2D plane by printing the adversarial patches on real clothes. Anyone wearing the cloak can evade the detection of person detectors and achieve stealth under challenging and complex 3D physical scenarios. Experimental results in various indoor and outdoor physical scenarios show that, the proposed person stealth attack method is robust and effective even under those complex and challenging physical conditions, such as the cloak is wrinkled, obscured, curved, and from different/large angles. The attack success rate of the generated adversarial patch in digital domain (Inria dataset) is 86.56% against YOLO v2 and 80.32% against YOLO v5, while the static and dynamic stealth attack success rates of the generated 3D invisible cloak in physical world are 100%, 77% against YOLO v2 and 100%, 83.95% against YOLO v5, respectively, which are significantly better than state-of-the-art works.},
  archive      = {J_TETC},
  author       = {Mingfu Xue and Can He and Yushu Zhang and Zhe Liu and Weiqiang Liu},
  doi          = {10.1109/TETC.2024.3513392},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {799-815},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {3D invisible cloak: A robust person stealth attack against object detector in complex 3D physical scenarios},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spiker+: A framework for the generation of efficient spiking neural networks FPGA accelerators for inference at the edge. <em>TETC</em>, <em>13</em>(3), 784-798. (<a href='https://doi.org/10.1109/TETC.2024.3511676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Including Artificial Neural Networks in embedded systems at the edge allows applications to exploit Artificial Intelligence capabilities directly within devices operating at the network periphery, containing sensitive data within the boundaries of the edge device. This facilitates real-time decision-making, reduces latency and power consumption, and enhances privacy and security. Spiking Neural Networks (SNNs) offer a promising computing paradigm in these environments. However, deploying efficient SNNs in resource-constrained edge devices requires highly parallel and reconfigurable hardware implementations. We introduce Spiker+, a comprehensive framework for generating efficient, low-power, and low-area SNN accelerators on Field Programmable Gate Arrays for inference at the edge. Spiker+ presents a configurable multi-layer SNN hardware architecture, a library of highly efficient neuron architectures, and a design framework to enable easy, Python-based customization of accelerators. Spiker+ is tested on three benchmark datasets: MNIST, Spiking Heidelberg Dataset (SHD), and AudioMNIST. On MNIST, it outperforms state-of-the-art SNN accelerators in terms of resource allocation, with a requirement of 7,612 logic cells and 18 Block RAMS (BRAMs), and power consumption, draining only 180 mW, with comparable latency (780 $\mu$s/img) and accuracy (97%). On SHD and AudioMNIST, Spiker+ requires 18,268 and 10,124 logic cells, respectively, requiring 51 and 16 BRAMs, consuming 430 mW and 290 mW, with an accuracy of 75% and 95%. These results underscore the significance of Spiker+ in the hardware-accelerated SNN landscape, making it an excellent solution for deploying configurable and tunable SNN architectures in resource and power-constrained edge applications.},
  archive      = {J_TETC},
  author       = {Alessio Carpegna and Alessandro Savino and Stefano Di Carlo},
  doi          = {10.1109/TETC.2024.3511676},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {784-798},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Spiker+: A framework for the generation of efficient spiking neural networks FPGA accelerators for inference at the edge},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A crowdsourcing-driven AI model design framework to public health policy-adherence assessment. <em>TETC</em>, <em>13</em>(3), 768-783. (<a href='https://doi.org/10.1109/TETC.2024.3496835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on a public health policy-adherence assessment (PHPA) application that aims to automatically assess people's public health policy adherence during emergent global health crisis events (e.g., COVID-19, MonkeyPox) by leveraging massive public health policy adherence imagery data from the social media. In particular, we study an optimal AI model design problem in the PHPA application, where the goal is to leverage the crowdsourced human intelligence to accurately identify the optimal AI model design (i.e., network architecture and hyperparameter configuration combination) without the need of AI experts. However, two critical challenges exist in our problem: 1) it is challenging to effectively optimize the AI model design given the interdependence between network architecture and hyperparameter configuration; 2) it is non-trivial to leverage the human intelligence queried from ordinary crowd workers to identify the optimal AI model design in the PHPA application. To address these challenges, we develop CrowdDesign, a subjective logic-driven human-AI collaborative learning framework that explores the complementary strength of AI and human intelligence to jointly identify the optimal network architecture and hyperparameter configuration of an AI model in the PHPA application. The experimental results from two real-world PHPA applications demonstrate that CrowdDesign consistently outperforms the state-of-the-art baseline methods by achieving the best PHPA performance.},
  archive      = {J_TETC},
  author       = {Yang Zhang and Ruohan Zong and Lanyu Shang and Dong Wang},
  doi          = {10.1109/TETC.2024.3496835},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {768-783},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A crowdsourcing-driven AI model design framework to public health policy-adherence assessment},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoaT: Compiler-assisted two-stage offloading approach for data-intensive applications under NMP framework. <em>TETC</em>, <em>13</em>(3), 753-767. (<a href='https://doi.org/10.1109/TETC.2024.3495218'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As we head toward a data-centric era, conventional computing systems become inadequate to meet the evolving demands of the applications. As a result, the near-memory processing (NMP) computing paradigm emerges as a potential alternative framework where regions of an application are offloaded for execution near the memory. Although some interesting research works have been proposed in recent times, none of them have considered placing processing cores jointly on the primary memories and cache memory. Further, they did not consider the data locality offered by the last level cache (LLC) and the estimated execution time of an application region together while designing the offloading strategy. This paper presents a novel hybrid NMP computation framework comprising a traditional multicore processor, NMP-enabled 3D memories and NMP-enabled LLC. The application source code is processed through a compilation framework to identify potential offloadable regions. The paper further proposes a two-stage offloading strategy, CoaT, which determines the execution location of the application regions based on the region’s overall execution time and the data locality offered by the LLC. A comprehensive series of experiments conducted using well-established simulators for large data-intensive applications, provides strong evidence of the efficacy of our approach. The results demonstrate significant reductions in execution time (averaging 60% with a maximum reduction of 64%), un-core energy consumption (averaging 34% with a maximum reduction of 44%), and off-chip data block transfer count (averaging 61% with a maximum reduction of 80%) compared to the state-of-the-art policies. The proposed policy achieves a speedup of 2.6x (on average) and 3.1x (maximum) w.r.t. the conventional execution.},
  archive      = {J_TETC},
  author       = {Satanu Maity and Mayank Goel and Manojit Ghose},
  doi          = {10.1109/TETC.2024.3495218},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {753-767},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CoaT: Compiler-assisted two-stage offloading approach for data-intensive applications under NMP framework},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Darwin: A DRAM-based multi-level processing-in-memory architecture for column-oriented database. <em>TETC</em>, <em>13</em>(3), 739-752. (<a href='https://doi.org/10.1109/TETC.2024.3493132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Darwin, a practical LRDIMM-based multi-level Processing-in-memory (PIM) architecture for data analytics, which exploits the internal bandwidth of DRAM using the bank-, bank group-, chip-, and rank-level parallelisms. Considering the properties of data analytics operators and DRAM's area constraints, Darwin maximizes the internal bandwidth by placing the PIM processing units, buffers, and control circuits across the hierarchy of DRAM. Darwin supports a novel PIM instruction architecture that concatenates instructions for multiple thread executions on bank group processing entities, addressing the command bottleneck by enabling separate control of up to 512 different in-memory processing units simultaneously. We build a cycle-accurate simulation framework to evaluate Darwin with various DRAM configurations, optimization schemes and workloads. Darwin achieves up to 14.7× speedup over the non-optimized version, leveraging many optimization schemes. Darwin architecture achieves 4.0 × −43.9× higher throughput and reduces energy consumption by 85.7% than the baseline CPU system (Intel Xeon Gold 6226 + 4 channels of DDR4-2933) for essential data analytics operators. Compared to the state-of-the-art PIM, Darwin achieves up to 7.5× and 7.1× in the basic query operators and TPC-H queries, respectively. Darwin in GDDR6 configuration requires only 5.6% area overhead, suggesting a promising PIM solution for the future main memory system.},
  archive      = {J_TETC},
  author       = {Donghyuk Kim and Jae-Young Kim and Wontak Han and Jongsoon Won and Haerang Choi and Yongkee Kwon and Joo-Young Kim},
  doi          = {10.1109/TETC.2024.3493132},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {739-752},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Darwin: A DRAM-based multi-level processing-in-memory architecture for column-oriented database},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Area-time efficient hardware implementation for binary ring-LWE based post-quantum cryptography. <em>TETC</em>, <em>13</em>(3), 724-738. (<a href='https://doi.org/10.1109/TETC.2024.3482324'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-quantum cryptography (PQC) has recently gained intensive attention as the existing public-key cryptosystems are vulnerable to quantum attacks. The ring-learning-with-errors (RLWE)-based PQC is one promising type of the lattice-based schemes. A light variant, called binary RLWE (BRLWE), was developed with applications to Internet-of-Things (IoT) and edge computing. However, deploying the number theoretic transform (NTT) is not beneficial to the parameter settings of the BRLWE-based scheme. This article presents three high-speed architectures of decryption for the BRLWE-based scheme with low area-time complexity. The first one is modified and corrected from the low-latency design of the previous work. The second and third ones utilize the multiplexer-based design for multiplication and innovatively exploit the property of the skew-circulant matrix to reduce the computational latency. Moreover, the third one applies the Karatsuba algorithm to reduce the number of multiplications. However, the results demonstrate that it is not in favor of the design since the multiplication is involved in an integer and a binary number, not both integers. Let the lengths of the secret and public keys be $n$ and $n\log _{2}q$ bits. The synthesized results reveal that the second and third architectures are superior to the lookup table (LUT)-based and linear-feedback shift register (LFSR)-based designs in the previous works in terms of area-time complexity. The FPGA implementation results indicate the second design outperforms the Karatsuba and Toeplitz matrix vector product (TMVP)-initiated accelerators in the literatures by reductions of 62.4% and 51.7% in area-time complexity for the case of $(n, q) = (256, 256)$. As $(n,q)=(512,256)$, the improvements are 44.3% and 28.3%. The third architecture is also superior to these high-speed designs. The proposed implementations are efficient in area-time complexity and are suitable for high-performance applications.},
  archive      = {J_TETC},
  author       = {Shao-I Chu and Syuan-An Ke},
  doi          = {10.1109/TETC.2024.3482324},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {724-738},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Area-time efficient hardware implementation for binary ring-LWE based post-quantum cryptography},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fault tolerance in triplet network training: Analysis, evaluation and protection methods. <em>TETC</em>, <em>13</em>(3), 714-723. (<a href='https://doi.org/10.1109/TETC.2024.3481962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the tolerance of Triplet Networks (TNs) with a focus on faults in the training process. For compatibility with the existing literature. So-called stuck-at faults of a functional nature are considered for the operation of the neurons and activation function. While TNs are shown to be generally robust against such faults in the anchor and positive subnetworks, the presented analysis reveals a significant vulnerability in the negative subnetwork, in which stuck-at faults can lead to false convergence and training failures. An in-depth treatment is provided to show the incorrect convergence of training in the presence of stuck-at faults, highlighting the behavior of the network with faulty neurons. Extensive simulations are presented to evaluate the impact of these faults and propose two innovative fault-tolerant methods: the regularization of the anchor outputs and the modified margin. Simulation shows that false convergence can be very efficiently avoided by utilizing the proposed techniques, and thus the overall accuracy loss of the TN is negligible. These findings contribute to the understanding of fault tolerance in emerging neural networks such as TNs and offer practical solutions for enhancing their robustness against faults.},
  archive      = {J_TETC},
  author       = {Ziheng Wang and Farzad Niknia and Shanshan Liu and Pedro Reviriego and Ahmed Louri and Fabrizio Lombardi},
  doi          = {10.1109/TETC.2024.3481962},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {714-723},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fault tolerance in triplet network training: Analysis, evaluation and protection methods},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGSEI: Adaptive graph structure estimation with long-tail distributed implicit graphs. <em>TETC</em>, <em>13</em>(3), 698-713. (<a href='https://doi.org/10.1109/TETC.2024.3480132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empowered by their remarkable advantages, graph neural networks (GNN) serve as potent tools for embedding graph-structured data and finding applications across various domains. Particularly, a prevalent assumption in most GNNs is the reliability of the underlying graph structure. This assumption, often implicit, can inadvertently lead to the propagation of misleading information through structures like false links. In response to this challenge, numerous methods for graph structure learning (GSL) have been developed. Among these methods, one popular approach is to construct a simple and intuitive K-nearest neighbor (KNN) graph as a sample to infer true graph structure. However, KNN graphs that follow the single-point distribution can easily mislead the true graph structure estimation. The primary reason is that, from a statistical perspective, the KNN graph, as a sample, follows a single-point distribution, whereas the true graph structure, as the population, as a whole mostly follows a long-tail distribution. In theory, the sample and the population should share the same distribution; otherwise, accurately inferring the true graph structure becomes challenging. To address this problem, this paper proposes an Adaptive Graph Structure Estimation with Long-Tail Distributed Implicit Graph, referred to as AGSEI. AGSEI comprises three main components: long-tail implicit graph construction, explicit graph structure estimation, and joint optimization. The first component relies on a multi-layer graph convolutional network to learn low-order to high-order node representations, compute node similarity, and construct several corresponding long-tail implicit graphs. Since the original imperfect graph structure can mislead GNNs into propagating false information, it reduces the reliability of the long-tail implicit graphs. AGSEI attempts to limit the aggregation of irrelevant information by introducing the Hilbert-Schmidt independence criterion. That is, maximizing the dependence between the predicted label and ground truth. With this strategy, AGSEI can learn node features dependent on labels to facilitate the construction of reliable long-tail implicit graphs, and then provide adaptive multi-view graph structure information to support subsequent GSL. In the second component, the graph structure is estimated using the stochastic block model (SBM) with the Expectation-Maximization algorithm. Considering that it is difficult for a single GSL to approach the true graph structure, the third part considers the joint optimization of the long-tail implicit graph construction and the explicit graph structure estimation. This involves optimizing the two parts alternately until the model converges. We conducted multiple experiments on five public datasets, including tasks such as classification and clustering. These experiments not only demonstrated the performance of AGSEI but also confirmed that the graph structures it estimates align with the long-tail distribution.},
  archive      = {J_TETC},
  author       = {Yunfei He and Yang Wu and Lishan Huang and Zhenwan Peng and Fei Yang and Yiwen Zhang and Victor S Sheng},
  doi          = {10.1109/TETC.2024.3480132},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {698-713},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {AGSEI: Adaptive graph structure estimation with long-tail distributed implicit graphs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TReX- reusing vision transformer’s attention for efficient xbar-based computing. <em>TETC</em>, <em>13</em>(3), 686-697. (<a href='https://doi.org/10.1109/TETC.2024.3480524'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the high computation overhead of Vision Transformers (ViTs), In-memory Computing architectures are being researched towards energy-efficient deployment in edge-computing scenarios. Prior works have proposed efficient algorithm-hardware co-design and IMC-architectural improvements to improve the energy-efficiency of IMC-implemented ViTs. However, all prior works have neglected the overhead and co-depencence of attention blocks on the accuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose TReX- an attention-reuse-driven ViT optimization framework that effectively performs attention reuse in ViT models to achieve optimal accuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer encoders for attention reuse to achieve near-iso-accuracy performance while meeting the user-specified delay requirement. Based on our analysis on the Imagenet-1k dataset, we find that TReX achieves 2.3× (2.19×) EDAP reduction and 1.86× (1.79×) TOPS/mm$^{2}$ improvement with $\sim$1% accuracy drop in case of DeiT-S (LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP reduction compared to state-of-the-art token pruning and weight sharing approaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal accuracy compared to baseline at 1.6× lower EDAP.},
  archive      = {J_TETC},
  author       = {Abhishek Moitra and Abhiroop Bhattacharjee and Youngeun Kim and Priyadarshini Panda},
  doi          = {10.1109/TETC.2024.3480524},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {686-697},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {TReX- reusing vision transformer’s attention for efficient xbar-based computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fully parallel, one-cycle random shuffling for efficient countermeasure against side channel attack and its complexity verification. <em>TETC</em>, <em>13</em>(3), 669-685. (<a href='https://doi.org/10.1109/TETC.2024.3478228'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hiding countermeasures are the most widely utilized techniques for thwarting side-channel attacks. Commonly, the Fisher-Yates algorithm is adopted in hiding countermeasures with permuted operation for its security and efficiency in implementation, yet the inherently sequential nature of the algorithm imposes limitations on hardware acceleration. In this work, we propose a novel method named Addition Round Rotation ($\mathsf {ARR}$), which can introduce a time-area trade-off with block-based permutation. Our findings indicate that this approach can achieve a permutation brute force complexity level ranging from $2^{128}$, with the modified version achieving up to $2^{288}$ in a single clock cycle, while maintaining substantial resistance against second-order analysis. To substantiate the security of our proposed method, we introduce a new validation technique – Identity Verification. This technique allows theoretical validation of the proposed algorithm’s security and is consistent with the experimental results. Finally, we introduce an actual hardware design and provide the implementation results on Application-Specific Integrated Circuit (ASIC). The measured performance demonstrates that our proposal fully supports the practical applicability.},
  archive      = {J_TETC},
  author       = {Jong-Yeon Park and Dongsoo Lee and Seonggyeom Kim and Wonil Lee and Bo Gyeong Kang and Kouichi Sakurai},
  doi          = {10.1109/TETC.2024.3478228},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {669-685},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fully parallel, one-cycle random shuffling for efficient countermeasure against side channel attack and its complexity verification},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep reinforcement learning with curriculum design for quantum state classification. <em>TETC</em>, <em>13</em>(3), 654-668. (<a href='https://doi.org/10.1109/TETC.2024.3479202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In quantum information science, one of the ambitious goals is to look for an efficient technique for classifying multiple quantum states. To solve the binary classification problem for multiple quantum states characterized by parameters, we propose a deep reinforcement learning with curriculum design (DRL-CD) method. In DRL-CD, a series of tasks are created, using state parameter intervals and fidelity thresholds, to form a curriculum. Then, a quantum state binary classifier can be obtained by utilizing deep reinforcement learning (DRL) to solve each task in the designed curriculum. In particular, we construct a training set by sampling the state parameter interval corresponding to each task, and each task is accomplished by learning the control strategies capable of steering the sampled quantum states to the target state. In addition, a knowledge review method is proposed to prevent DRL from forgetting the learned classification knowledge. Some state classification problems of the spin-1/2 quantum system and $\Lambda$-type atomic system are solved by the proposed DRL-CD method, and comparison experiments with deep Q-network (DQN) and stochastic gradient descent (SGD) show the better classification performance of DRL-CD.},
  archive      = {J_TETC},
  author       = {Haixu Yu and Xudong Zhao},
  doi          = {10.1109/TETC.2024.3479202},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {654-668},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Deep reinforcement learning with curriculum design for quantum state classification},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QuripfeNet: Quantum-resistant IPFE-based neural network. <em>TETC</em>, <em>13</em>(3), 640-653. (<a href='https://doi.org/10.1109/TETC.2024.3479193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to protect the sensitive information in many applications involving neural networks, several privacy-preserving neural networks that operate on encrypted data have been developed. Unfortunately, existing encryption-based privacy-preserving neural networks are mainly built on classical cryptography primitives, which are not secure from the threat of quantum computing. In this paper, we propose the first quantum-resistant solution to protect neural network inferences based on an inner-product functional encryption scheme. The selected state-of-the-art functional encryption scheme based on lattice-based cryptography works with integer-type inputs, which is not directly compatible with neural network computations that operate in the floating point domain. We propose a polynomial-based secure convolution layer to allow a neural network to resolve this problem, along with a technique that reduces memory consumption. The proposed solution, named QuripfeNet, was applied in LeNet-5 and evaluated using the MNIST dataset. In a single-threaded implementation (CPU), QuripfeNet took 107.4 seconds for an inference to classify one image, achieving accuracy of 97.85%, which is very close to the unencrypted version. Additionally, the GPU-optimized QuripfeNet took 25.9 seconds to complete the same task, which is improved by 4.15× compared to the CPU version.},
  archive      = {J_TETC},
  author       = {KyungHyun Han and Wai-Kong Lee and Angshuman Karmakar and Myung-Kyu Yi and Seong Oun Hwang},
  doi          = {10.1109/TETC.2024.3479193},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {640-653},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {QuripfeNet: Quantum-resistant IPFE-based neural network},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pip-SW: Pipeline architectures for accelerating smith-waterman algorithm on FPGA platforms. <em>TETC</em>, <em>13</em>(3), 628-639. (<a href='https://doi.org/10.1109/TETC.2024.3472649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Smith-Waterman algorithm, which is founded on a dynamic programming approach, serves as a precise tool for aligning biological sequences. Despite its utility, the algorithm grapples with computational complexity and resource demands. Various implementations across multi-core, GPU, and FPGA platforms have sought to expedite the algorithm, yet frequently encounter issues such as suboptimal speedup, heightened reliance on external memory resources, and an exclusive focus on the forward step of the algorithm. To tackle these challenges, this study introduces an architecture aimed at accelerating the Smith-Waterman algorithm on FPGA platforms. Our architecture capitalizes on a pipeline structure that integrates optimized circuitry for parallel computations and employs memory allocation techniques, thus delivering an efficient, low power and cost-effective implementation for biological sequence alignment. Our assessments, coupled with comparisons against alternative FPGA implementations supporting protein sequence alignment, reveal a 17% increase in operating frequency and a 17% enhancement in Giga cell updates per second. Moreover, our approach competes with GPU-based solutions, showcasing comparable performance metrics alongside superior energy efficiency, with a 35% improvement. We substantiate the utility and performance of our pipeline architecture on FPGA platforms using four benchmark datasets. The validation results demonstrate a speedup ranging from 10 to 45 times for alignment score computation compared to the CPU platform.},
  archive      = {J_TETC},
  author       = {Mahmood Kalemati and Ali Dehghan Nayeri and Somayyeh Koohi},
  doi          = {10.1109/TETC.2024.3472649},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {628-639},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Pip-SW: Pipeline architectures for accelerating smith-waterman algorithm on FPGA platforms},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). (In)security of stream ciphers against quantum annealing attacks on the example of the grain 128 and grain 128a ciphers. <em>TETC</em>, <em>13</em>(3), 614-627. (<a href='https://doi.org/10.1109/TETC.2024.3474856'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The security level of a cipher is a key parameter. While general-purpose quantum computers significantly threaten modern symmetric ciphers, other quantum approaches like quantum annealing have been less concerning. However, this paper argues that a quantum annealer specifically designed to attack Grain 128 and Grain 128a ciphers could soon be technologically feasible. Such an annealer would require 5,751 (6,761) qubits and 77,496 (94,865) couplers, with a qubit connectivity of 225 (245). This work also shows that modern stream ciphers like Grain 128 and Grain 128a may be vulnerable to quantum annealing attacks. Although the exact complexity of quantum annealing is unknown, heuristic estimates suggest that for many problems with $N$ variables, a $\sqrt{N}$ exponential advantage over simulated annealing may hold. We detail how to transform algebraic attacks on Grain ciphers into the QUBO problem, making our attack potentially more efficient than classical brute-force methods. We demonstrate that applying our attack to rescaled Grain cipher versions, Grain $l$ and Grain $la$, overtakes brute-force and Grover’s attacks for sufficiently large $l$, assuming quantum annealing’s exponential benefit over simulated annealing.},
  archive      = {J_TETC},
  author       = {Michał Wroński and Elżbieta Burek and Mateusz Leśniak},
  doi          = {10.1109/TETC.2024.3474856},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {614-627},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {(In)security of stream ciphers against quantum annealing attacks on the example of the grain 128 and grain 128a ciphers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A virtual reality perceptual study of multi-technique redirected walking method. <em>TETC</em>, <em>13</em>(3), 604-613. (<a href='https://doi.org/10.1109/TETC.2024.3471249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within virtual reality experiences, locomotion methods manage the user’s movement within the virtual environment. The use of natural locomotion, common in virtual reality, can be limited in video games with large scenarios. Thus, video games with gamepad or teleport-based locomotion methods are gaining importance. Redirected walking methods focus on maximizing the exploitation of the real workspace. As the user moves in the real environment, subtle modifications are applied to that movement within the virtual environment. Although the results of the Multi-Technique Redirected Walking (MTRW) method that combines the application of four gain algorithms are promising, a perceptual evaluation with users is needed to determine its suitability. This article presents the perceptual evaluation of the presence and cybersickness factors for the MTRW method, comparing it with a Fully Natural Walking (FNW) method. The presence factor was measured with the Igroup Presence Questionnaire (IPQ), and no significant differences in the overall presence score were detected between the FNW and the MTRW methods. The cybersickness factor was measured using the Simulator Sickness Questionnaire (SSQ) and, this time, significant differences in cybersickness between the two locomotion methods were obtained. The potential increase in cybersickness should be weighed against the benefit of maximizing workspace utilization.},
  archive      = {J_TETC},
  author       = {Jesus Mayor and Laura Raya and Sofia Bayona and Alberto Sanchez},
  doi          = {10.1109/TETC.2024.3471249},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {604-613},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A virtual reality perceptual study of multi-technique redirected walking method},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NGQR: A novel generalized quantum image representation. <em>TETC</em>, <em>13</em>(3), 591-603. (<a href='https://doi.org/10.1109/TETC.2024.3471086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the size limitations of existing quantum image models in terms of accurate image representation, as well as inaccurate image operation and retrieval, we propose a Novel Generalized Quantum Image Representation (NGQR) for images of arbitrary size and type. For generalizing the size model, we first propose the Perception-Aided Encoding (PE) method to perceive the target qubits in the quantum information. Based on PE, we propose the quantum image representation PE-NGQR, which accurately ignores redundant information thereby targeting valid pixels for operations and retrieval. Then, to accurately represent the needed pixel information without redundancy, we propose the Coherent-Size Encoding (CE) method. The CE can encode an arbitrary number of quantum states. Based on CE, we propose CE-NGQR, a quantum image model capable of accurate image representation, processing and retrieval. Specifically, we describe in detail the concept, representation and quantum circuits of NGQR. We provide detailed quantum circuits and simulations of NGQR-based operations and geometric transformations. Moreover, NGQR enables flexible quantum image scaling. We illustrate the complementarity of the proposed PE-NGQR and CE-NGQR through complexity simulations and clarify the respective applicability scenarios. Finally, comparisons and analyses with existing quantum image models demonstrate the versatility and flexibility advantages of NGQR.},
  archive      = {J_TETC},
  author       = {Zheng Xing and Xiaochen Yuan and Chan-Tong Lam and Penousal Machado},
  doi          = {10.1109/TETC.2024.3471086},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {591-603},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {NGQR: A novel generalized quantum image representation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DALTON - Deep local learning in SNNs via local weights and surrogate-derivative transfer. <em>TETC</em>, <em>13</em>(3), 578-590. (<a href='https://doi.org/10.1109/TETC.2024.3440932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct training of Spiking Neural Networks (SNNs) is a challenging task because of their inherent temporality. Added to it, the vanilla Back-propagation based methods are not applicable either, due to the non-differentiability of the spikes in SNNs. Surrogate-Derivative based methods with Back-propagation Through Time (BPTT) address these direct training challenges quite well; however, such methods are not neuromorphic-hardware friendly for the On-chip training of SNNs. Recently formalized Three-Factor based Rules (TFR) for direct local-training of SNNs are neuromorphic-hardware friendly; however, they do not effectively leverage the depth of the SNN architectures (we show it empirically here), thus, are limited. In this work, we present an improved version of a conventional three-factor rule, for local learning in SNNs which effectively leverages depth – in the context of learning features hierarchically. Taking inspiration from the Back-propagation algorithm, we theoretically derive our improved, local, three-factor based learning method, named DALTON (Deep LocAl Learning via local WeighTs and SurrOgate-Derivative TraNsfer), which employs weights and surrogate-derivative transfer from the local layers. Along the lines of TFR, our proposed method DALTON is also amenable to the neuromorphic-hardware implementation. Through extensive experiments on static (MNIST, FMNIST, & CIFAR10) and event-based (N-MNIST, DVS128-Gesture, & DVS-CIFAR10) datasets, we show that our proposed local-learning method DALTON makes effective use of the depth in Convolutional SNNs, compared to the vanilla TFR implementation.},
  archive      = {J_TETC},
  author       = {Ramashish Gaurav and Duy Anh Do and Thinh T. Doan and Yang Yi},
  doi          = {10.1109/TETC.2024.3440932},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {578-590},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DALTON - Deep local learning in SNNs via local weights and surrogate-derivative transfer},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DP-PartFIM: Frequent itemset mining using differential privacy and partition. <em>TETC</em>, <em>13</em>(3), 567-577. (<a href='https://doi.org/10.1109/TETC.2024.3443060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Itemset mining is a popular data mining technique for extracting interesting and valuable information from large datasets. However, since datasets contain sensitive private data, it is not permitted to directly mine the data or share the mining results. Previous privacy-preserving frequent itemset mining research was not efficient because of the use of privacy budgets or long transaction truncation strategies, which are impractical for large datasets. In this article, we propose a more efficient partition mining technology, DP-PartFIM, based on differential privacy, which protects privacy while mining data. DP-PartFIM uses partition mining to mine frequent itemsets and constructs vertical data storage formats for each partition, which makes the algorithm equally efficient for large datasets. To protect data privacy, DP-PartFIM adds Laplace noise to support candidate itemsets. The experimental results show that, compared with the classical privacy-preserving itemset mining methods, DP-PartFIM better guarantees data utility and privacy.},
  archive      = {J_TETC},
  author       = {Xinyu Liu and Wensheng Gan and Lele Yu and Yining Liu},
  doi          = {10.1109/TETC.2024.3443060},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {7-9},
  number       = {3},
  pages        = {567-577},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DP-PartFIM: Frequent itemset mining using differential privacy and partition},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TampML: Tampering attack detection and malicious nodes localization in NoC-based MPSoC. <em>TETC</em>, <em>13</em>(2), 551-562. (<a href='https://doi.org/10.1109/TETC.2024.3434663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The relentless growth in demand for computing resources has spurred the development of large-scale, high-performance chips with diverse, innovative architectures. The Network-on-Chip (NoC) paradigm has become a predominant system for on-chip communication within Multi-Processor System-on-Chip (MPSoC) designs. However, the increasing complexity and the reliance on outsourced Third-Party Intellectual Properties (3PIPs) introduce non-negligible risks of Hardware Trojan (HT) insertions by untrusted IP vendors. One of the most critical threats posed by HTs is the tampering with communication data packets. In this article, we introduce a comprehensive framework for the detection of tampering attacks and localization of HTs within NoCs. This framework is incorporated into a novel distributed monitoring architecture that leverages the NoC structure. Utilizing a machine learning model for malicious flit detection and a high-precision algorithm for HT node localization, the framework's efficacy has been substantiated through tests with real PARSEC benchmark workloads. Achieving an impressive detection accuracy and precision of 99.8% and 99.5% respectively, the framework can localize HT nodes with up to 100% precision and recall in most cases. Furthermore, the data cost of localization is on average only 3.7% of tampered flits, which is significantly more efficient—up to 11 times faster—than our initial methods. As a comprehensive and cutting-edge security solution for combating communication data tampering attacks, it accomplishes the expected performance while maintaining minimal power and hardware overhead.},
  archive      = {J_TETC},
  author       = {Haoyu Wang and Basel Halak},
  doi          = {10.1109/TETC.2024.3434663},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {551-562},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {TampML: Tampering attack detection and malicious nodes localization in NoC-based MPSoC},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel adaptive $360^{\circ }$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>360</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math> livestreaming with graph representation learning based FoV prediction. <em>TETC</em>, <em>13</em>(2), 537-550. (<a href='https://doi.org/10.1109/TETC.2024.3435002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exceptionally high bandwidth requirements associated with the delivery of live $360^{\circ }$ video content pose significant challenges in the current network context. An avenue for addressing this bandwidth challenge is to use the limited network resources for sending the user's Field-of-View (FoV) tiles at a high resolution, instead of transmitting all frame components at high quality. However, precisely forecasting the FoV for $360^{\circ }$ live video content distribution remains a complex endeavor due to the lack of pre-knowledge on user viewing behaviors. In this paper, we present GL360, a novel $360^{\circ }$ transmission framework, which employs Graph Representation Learning for FoV prediction. First, we analyze the interaction between users and tiles in panoramic videos utilizing a dynamic heterogeneous Relational Graph Convolutional Network (RGCN), which facilitates efficient user and tile embedding representation learning. Second, we propose an online dynamic heterogeneous graph learning (DHGL)-based algorithm to dynamically capture the time-varying features of the user's viewing behaviors with limited prior knowledge. Further, we design a FoV-aware content delivery algorithm that allows the edge servers to determine the video tiles’ resolution for each accessed user. Experimental results based on real traces demonstrate how our solution outperforms four other solutions in terms of FoV prediction and network performance.},
  archive      = {J_TETC},
  author       = {Xingyan Chen and Huaming Du and Mu Wang and Yu Zhao and Xiaoyang Shu and Changqiao Xu and Gabriel-Miro Muntean},
  doi          = {10.1109/TETC.2024.3435002},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {537-550},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A novel adaptive $360^{\circ }$<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>360</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math> livestreaming with graph representation learning based FoV prediction},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Area efficient skyrmion logic based approximate adder architecture design methodology. <em>TETC</em>, <em>13</em>(2), 525-536. (<a href='https://doi.org/10.1109/TETC.2024.3434723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, the first of its kind skyrmion logic based area efficient approximate nanomagnetic (APN) adder architecture design methodology is introduced along with its implementation using theoretical modelling and micromagnetic simulations. We propose here for the first time, skyrmion based APN adder architecture design using only one majority gate reconfigured runtime (RR) using single layout. This low complex device structure is modelled using three inputs with the bilayer ferromagnet/heavy metal utilizing the exploitation of output reversal mechanism using magnetic tunnel junctions (MTJs) for read and write of skyrmions. The implementation is performed using this same device where current is passed through a metallic gate for control mechanism to achieve various logic functionalities. We also introduce here the boolean optimzation followed by mapping logic for the demonstration of skyrmion RRAPN adder alongside the majority logic gate. This proposed RRAPN adder architecture design possess low complexity in terms of utilization of resources aiding towards the reduction of number of majority logic gates ($ \sim$$60 \%$ device footprint reduction) and evaluated against standard error metrics. RRAPN adder architecture design proposed has its advantages with miniaturisation aided by enhanced lithographic process nodes, creating a new potential for nanomagnetic logic devices.},
  archive      = {J_TETC},
  author       = {Santhosh Sivasubramani and Bibekananda Paikaray and Mahathi Kuchibhotla and Arabinda Haldar and Chandrasekhar Murapaka and Amit Acharyya},
  doi          = {10.1109/TETC.2024.3434723},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {525-536},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Area efficient skyrmion logic based approximate adder architecture design methodology},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAT SNN: Conversion aware training for high accuracy and hardware friendly spiking neural networks. <em>TETC</em>, <em>13</em>(2), 512-524. (<a href='https://doi.org/10.1109/TETC.2024.3435135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the various training algorithms for spiking neural network (SNN), ANN-to-SNN conversion gained popularity due to high accuracy and scalability to deep networks. By converting artificial neural network (ANN) to SNN and employing conversion loss reduction techniques, previous ANN-to-SNN conversion approaches achieved good accuracies. However, previous works do not consider the overheads to implement conversion loss reductions in hardware, thereby limiting its feasibility of hardware implementation. In this paper, we present conversion aware training (CAT), where SNN is simulated as closely as possible during ANN training for obtaining SNN-like ANN. So, our approach does not need any conversion loss reduction techniques after conversion, thus reducing hardware overhead while achieving state-of-the-art accuracies for SNNs using various neural coding methods. In addition, as an application of CAT for obtaining a hardware friendly SNN, we demonstrate a lightweight time-to-first-spike (TTFS) coding that adopts logarithmic computations enabled by CAT. An SNN processor that supports the logarithmic TTFS is implemented in 28nm CMOS process, achieving 91.7/67.9/57.4% accuracy and 486.7/503.6/1426uJ inference energy on CIFAR-10/100/Tiny-ImageNet, when running 5-bit logarithmic weight VGG-16. The key contributions are 1) proposing CAT as an ANN-to-SNN conversion guideline 2) applying CAT on various neural codings 3) presenting co-designed TTFS coding and processor.},
  archive      = {J_TETC},
  author       = {Dongwoo Lew and Jongsun Park},
  doi          = {10.1109/TETC.2024.3435135},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {512-524},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {CAT SNN: Conversion aware training for high accuracy and hardware friendly spiking neural networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acceleration of the bootstrapping in TFHE by FPGA. <em>TETC</em>, <em>13</em>(2), 496-511. (<a href='https://doi.org/10.1109/TETC.2024.3433473'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving computing is playing an ever-increasingly important role in various fields. A leading example of privacy-preserving computing is Fully Homomorphic Encryption (FHE). FHE enables arbitrary computations directly on the ciphertext. This guarantees that the original data will not be disclosed while processing the data. However, FHE brings in the high computation cost which, in turn, limits the application of FHE. Among all steps of FHE, bootstrapping is a critical operation yet a bottleneck for the FHE efficiency. Torus FHE (TFHE) was presented as a method which can compute arbitrary Boolean functions on ciphertext with fast gate bootstrapping. In this paper, we show an implementation of TFHE gate bootstrapping on ZYNQ ZCU102 FPGA board. The memory operation is specially organized to facilitate the implementation of the adopted Number Theoretic Transform (NTT) of external product. Each function involved in the TFHE gate bootstrapping is implemented at the register-transfer level (RTL), and each operation is carefully scheduled to maximize the parallelism. Experimental results show that with ZCU102 working at the frequency of 300MHz, the proposed scheme can bootstrap one bit within 1.9ms on average. Compared with the accelerated TFHE using the mainstream CPU, the proposed scheme shows a 5.0X speedup. If under the similar clock frequency, it presents 1.23X faster than cuFHE which is accelerated by GPU. The proposed scheme also shows other advantages such as high efficiency and better tradeoff than existing FPGA-based acceleration schemes.},
  archive      = {J_TETC},
  author       = {Jian Zhang and Aijiao Cui and Yier Jin},
  doi          = {10.1109/TETC.2024.3433473},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {496-511},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Acceleration of the bootstrapping in TFHE by FPGA},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantum-inspired differential evolution with decoding using hashing for efficient user allocation in edge computing environment. <em>TETC</em>, <em>13</em>(2), 481-495. (<a href='https://doi.org/10.1109/TETC.2024.3433570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern apps require high computing resources for real-time data processing, allowing app users (AUs) to access real-time information. Edge computing (EC) provides dynamic computing resources to AUs for real-time data processing. However, due to resources and coverage constraints, edge servers (ESs) in specific areas can only serve a limited number of AUs. Hence, the app user allocation problem (AUAP) becomes challenging in the EC environment. This paper proposes a quantum-inspired differential evolution algorithm (QDE-UA) for efficient user allocation in the EC environment. The quantum vector is designed to provide a complete solution to the AUAP. The fitness function considers the minimum use of ES, user allocation rate (UAR), energy consumption, and load balance. Extensive simulations and hypotheses-based statistical analyses (ANOVA, Friedman test) are performed to show the significance of the proposed QDE-UA. The results indicate that QDE-UA outperforms the majority of the existing strategies with an average UAR improvement of 112.42%, and 140.62% enhancement in load balance while utilizing 13.98% fewer ESs. Due to the higher UAR, QDE-UA shows 59.28% higher total energy consumption on average. However, the lower energy consumption per AU is evidence of its energy efficiency.},
  archive      = {J_TETC},
  author       = {Marlom Bey and Pratyay Kuila and Banavath Balaji Naik},
  doi          = {10.1109/TETC.2024.3433570},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {481-495},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Quantum-inspired differential evolution with decoding using hashing for efficient user allocation in edge computing environment},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WASMBOX: A lightweight wasm-based runtime for trustworthy multi-tenant embedded systems. <em>TETC</em>, <em>13</em>(2), 467-480. (<a href='https://doi.org/10.1109/TETC.2024.3409817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Enabling multi-tenancy on edge devices is crucial for maximizing resource utilization, enhancing scalability, and reducing costs. However, it introduces the challenge of maintaining tenant isolation, preventing adverse inter-tenant effects and unauthorized resource access. Traditional multi-tenant solutions often struggle in embedded systems due to resource constraints, and current lightweight approaches suffer from performance, portability, and tenant density issues. We propose WASMBOX, a novel solution for sandboxing applications in multi-tenant embedded systems. It leverages WebAssembly to offer strong isolation, small attack surface, high portability, efficient resource usage, and near-native performance. Our system ensures both attack prevention and detection, using a patched WebAssembly System Interface for safe system call execution, and a monitoring layer for anomaly detection. Additionally, WASMBOX uses a Trusted Execution Environment for further isolating applications against escaping tenants and attesting to the integrity of WebAssembly applications. We validated our solution in a real-world case study with the SpaceApplications company, aiming to adopt a multi-tenant model for its ISS-based micro-gravity research facility. The experimental evaluation compared WASMBOX with approaches relying on VMs, containers, and microkernel-based VMs. The obtained results show that WASMBOX has the lowest resource usage, the highest tenant density, the second lowest startup (preceded by microkernels), and execution time (preceded by containers).},
  archive      = {J_TETC},
  author       = {Luigi Coppolino and Salvatore D'Antonio and Giovanni Mazzeo and Roberto Nardone and Luigi Romano and Mathieu Schmitt},
  doi          = {10.1109/TETC.2024.3409817},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {467-480},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {WASMBOX: A lightweight wasm-based runtime for trustworthy multi-tenant embedded systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-layer personalized federated learning for mitigating biases in student predictive analytics. <em>TETC</em>, <em>13</em>(2), 451-466. (<a href='https://doi.org/10.1109/TETC.2024.3407716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional methods for student modeling, which involve predicting grades based on measured activities, struggle to provide accurate results for minority/ underrepresented student groups due to data availability biases. In this paper, we propose a Multi-Layer Personalized Federated Learning (MLPFL) methodology that optimizes inference accuracy over different layers of student grouping criteria, such as by course and by demographic subgroups within each course. In our approach, personalized models for individual student subgroups are derived from a global model, which is trained in a distributed fashion via meta-gradient updates that account for subgroup heterogeneity while preserving modeling commonalities that exist across the full dataset. The evaluation of the proposed methodology considers case studies of two popular downstream student modeling tasks, knowledge tracing and outcome prediction, which leverage multiple modalities of student behavior (e.g., visits to lecture videos and participation on forums) in model training. Experiments on three real-world online course datasets show significant improvements achieved by our approach over existing student modeling benchmarks, as evidenced by an increased average prediction quality and decreased variance across different student subgroups. Visual analysis of the resulting students’ knowledge state embeddings confirm that our personalization methodology extracts activity patterns clustered into different student subgroups, consistent with the performance enhancements we obtain over the baselines.},
  archive      = {J_TETC},
  author       = {Yun-Wei Chu and Seyyedali Hosseinalipour and Elizabeth Tenorio and Laura Cruz and Kerrie Douglas and Andrew S. Lan and Christopher G. Brinton},
  doi          = {10.1109/TETC.2024.3407716},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {451-466},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Multi-layer personalized federated learning for mitigating biases in student predictive analytics},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DECC: Delay-aware edge-cloud collaboration for accelerating DNN inference. <em>TETC</em>, <em>13</em>(2), 438-450. (<a href='https://doi.org/10.1109/TETC.2024.3404551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN)-enabled edge intelligence has been widely adopted to support a variety of smart applications because of its ability to preserve privacy and conserve communication efficiency. The dilemma is that DNN models can be too large to be deployed on computationally constrained edge devices, and the volume of raw data can be too large to be efficiently transmitted to a centralized server. Thus, it is of utter importance that edge devices and cloud servers collaborate with each other to achieve fast and dependable model inference. Current collaborative solutions separate the DNN into two parts, which are placed and executed at the edge and in the cloud, respectively. However, these separated parts are executed consecutively, and all subsequent layers have to wait for the output of the previous layer even if they are not directly connected, causing significant inference latency. We propose a delay-aware edge-cloud collaboration (DECC) algorithm to reorganize the execution of DNN layers. By dividing DNN into several independent branches and selecting the optimal partition points, we apply a pipeline approach to parallelize the execution of these branches to minimize the inference delay. Extensive experiments show that the DECC outperforms existing methods by significantly reducing inference latency and improving throughput.},
  archive      = {J_TETC},
  author       = {Zirui Zhuang and Jianan Chen and Wenchao Xu and Qi Qi and Song Guo and Jingyu Wang and Lu Lu and Hongwei Yang and Jianxin Liao},
  doi          = {10.1109/TETC.2024.3404551},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {438-450},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {DECC: Delay-aware edge-cloud collaboration for accelerating DNN inference},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Area and power efficient FFT/IFFT processor for FALCON post-quantum cryptography. <em>TETC</em>, <em>13</em>(2), 423-437. (<a href='https://doi.org/10.1109/TETC.2024.3407124'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum computing is an emerging technology on the verge of reshaping industries, while simultaneously challenging existing cryptographic algorithms. FALCON, a recent standard quantum-resistant digital signature, presents a challenging hardware implementation due to its extensive non-integer polynomial operations, necessitating FFT over the ring $\mathbb {Q}[x]/(x^{n}+1)$. This paper introduces an ultra-low-power and compact processor tailored for FFT/IFFT operations over the ring for efficient FALCON implementation. The proposed processor incorporates various optimization techniques, including twiddle factor compression and conflict-free scheduling. In an ASIC implementation using a 22 nm GF process, the proposed processor demonstrates an area occupancy of 0.15 mm$^{2}$ and a power consumption of 12.6 mW/28.1 mW at an operating frequency of 167 MHz/500 MHz for the non-pipelined/pipelined version of the processor. Since a hardware implementation of FFT/IFFT over the ring is currently non-existent, the execution time achieved by this processor is compared to the reference software implementation of FFT/IFFT of FALCON on a Raspberry Pi 4 with Cortex-A72, where the proposed pipelined processor achieves a speedup up to 3.8×. Furthermore, in comparison to dedicated state-of-the-art hardware accelerators for classic FFT, the pipelined architecture occupies 42% less area and consumes 64% less power, on average. The quantified speedup in the context of FALCON suggests that the proposed hardware design offers a promising solution for the efficient implementation of FALCON.},
  archive      = {J_TETC},
  author       = {Ghada Alsuhli and Hani Saleh and Mahmoud Al-Qutayri and Baker Mohammad and Thanos Stouraitis},
  doi          = {10.1109/TETC.2024.3407124},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {423-437},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Area and power efficient FFT/IFFT processor for FALCON post-quantum cryptography},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Balancing throughput and fair execution of multi-DNN workloads on heterogeneous embedded devices. <em>TETC</em>, <em>13</em>(2), 409-422. (<a href='https://doi.org/10.1109/TETC.2024.3407055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of Deep Neural Networks (DNNs) has resulted in complex workloads employing multiple DNNs concurrently. This trend introduces unique challenges related to workload distribution, particularly in heterogeneous embedded systems. Current run-time managers struggle to efficiently utilize all computing components on these platforms, resulting in two major problems. First, the system throughput deteriorates due to contention on the computing resources. Second, not all DNNs are affected equally, leading to inconsistent performance levels across different models. To address these challenges, we introduce FairBoost, a framework for efficient and fair multi-DNN inference on heterogeneous embedded systems. FairBoost employs Reinforcement Learning (RL) to efficiently manage multi-DNN workloads. Additionally, it incorporates a novel numerical representation of DNN layers via a Vector Quantized Variational Auto-Encoder (VQ-VAE). Finally, it enables knowledge transfer to similar heterogeneous embedded systems without retraining and/or fine-tuning. Experimental evaluation of FairBoost over 18 DNNs and various multi-DNN scenarios shows an average throughput/fairness improvement of $\times 3.24$. Additionally, FairBoost facilitates knowledge transfer from the initial platform, Orange Pi 5, to a new system, Odroid N2+, without any retraining or fine-tuning achieving similar gains.},
  archive      = {J_TETC},
  author       = {Andreas Karatzas and Iraklis Anagnostopoulos},
  doi          = {10.1109/TETC.2024.3407055},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {409-422},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Balancing throughput and fair execution of multi-DNN workloads on heterogeneous embedded devices},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BCIM: Efficient implementation of binary neural network based on computation in memory. <em>TETC</em>, <em>13</em>(2), 395-408. (<a href='https://doi.org/10.1109/TETC.2024.3406628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications of Binary Neural Networks (BNNs) are promising for embedded systems with hard constraints on energy and computing power. Contrary to conventional neural networks using floating-point datatypes, BNNs use binarized weights and activations to reduce memory and computation requirements. Memristors, emerging non-volatile memory devices, show great potential as a target implementation platform for BNNs by integrating storage and compute units. However, the efficiency of this hardware highly depends on how the network is mapped and executed on these devices. In this paper, we propose an efficient implementation of XNOR-based BNN to maximize parallelization. In this implementation, costly analog-to-digital converters are replaced with sense amplifiers with custom reference(s) to generate activation values. Besides, a novel mapping is introduced to minimize the overhead of data communication between convolution layers mapped to different memristor crossbars. This comes with extensive analytical and simulation-based analysis to evaluate the implication of different design choices considering the accuracy of the network. The results show that our approach achieves up to $5\times$ energy-saving and $100\times$ improvement in latency compared to baselines.},
  archive      = {J_TETC},
  author       = {Mahdi Zahedi and Taha Shahroodi and Carlos Escuin and Georgi Gaydadjiev and Stephan Wong and Said Hamdioui},
  doi          = {10.1109/TETC.2024.3406628},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {395-408},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {BCIM: Efficient implementation of binary neural network based on computation in memory},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating the resilience source of classification systems for approximate computing techniques. <em>TETC</em>, <em>13</em>(2), 382-394. (<a href='https://doi.org/10.1109/TETC.2024.3403757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {During the last decade, classification systems (CSs) received significant research attention, with new learning algorithms achieving high accuracy in various applications. However, their resource-intensive nature, in terms of hardware and computation time, poses new design challenges. CSs exhibit inherent error resilience, due to redundancy of training sets, and self-healing properties, making them suitable for Approximate Computing (AxC). AxC enables efficient computation by using reduced precision or approximate values, leading to energy, time, and silicon area savings. Exploiting AxC involves estimating the introduced error for each approximate variant found during a Design-Space Exploration (DSE). This estimation has to be both rapid and meaningful, considering a substantial number of test samples, which are utterly conflicting demands. In this article, we investigate on sources of error resiliency of CSs, and we propose a technique to haste the DSE that reduces the computational time for error estimation by systematically reducing the test set. In particular, we cherry-pick samples that are likely to be more sensitive to approximation and perform accuracy-loss estimation just by exploiting such a sample subset. In order to demonstrate its efficacy, we integrate our technique into two different approaches for generating approximate CSs, showing an average speed-up up to $\approx$18.},
  archive      = {J_TETC},
  author       = {Mario Barbareschi and Salvatore Barone},
  doi          = {10.1109/TETC.2024.3403757},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {382-394},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Investigating the resilience source of classification systems for approximate computing techniques},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative analysis of software aging in relational database system environments. <em>TETC</em>, <em>13</em>(2), 370-381. (<a href='https://doi.org/10.1109/TETC.2024.3471684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer systems that operate continuously over extended periods of time can be susceptible to a phenomenon known as software aging. This phenomenon can result in the gradual depletion of computational resources and has the potential to cause performance degradation in these systems. Among the systems affected, Database Management Systems (DBMSs) are particularly crucial. The consequences of software aging in DBMSs can result in data loss, compromised database integrity, transaction failures, and negative effects on system availability. This work analyzes and compares the effects of software aging in systems using SQL Server and MySQL DBMSs. The presence of this phenomenon is confirmed through statistical analysis of memory consumption and response time degradation. Process-level analysis identified database and server processes contributing most to memory consumption. Additionally, we developed machine learning models to predict memory exhaustion in both SQL Server and MySQL environments across diverse workloads.},
  archive      = {J_TETC},
  author       = {Herderson Couto and Fumio Machida and Gustavo Callou and Ermeson Andrade},
  doi          = {10.1109/TETC.2024.3471684},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {370-381},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A comparative analysis of software aging in relational database system environments},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-effective software rejuvenation combining time-based and inspection-based policies. <em>TETC</em>, <em>13</em>(2), 354-369. (<a href='https://doi.org/10.1109/TETC.2024.3475214'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software rejuvenation is a proactive maintenance technique that counteracts software aging by restarting a system, making selection of rejuvenation times critical to improve reliability without incurring excessive downtime costs. Various stochastic models of Software Aging and Rejuvenation (SAR) have been developed, mostly having an underlying stochastic process in the class of Continuous Time Markov Chains (CTMCs), Semi-Markov Processes (SMPs), and Markov Regenerative Processes (MRGPs) under the enabling restriction, requiring that at most one general (GEN), i.e., non-Exponential, timer be enabled in each state. We present a SAR model with an underlying MRGP under the bounded regeneration restriction, allowing for multiple GEN timers to be concurrently enabled in each state. This expressivity gain not only supports more accurate fitting of duration distributions from observed statistics, but also enables the definition of mixed rejuvenation strategies combining time-based and inspection-based policies, where the time to the next inspection or rejuvenation depends on the outcomes of diagnostic tests. Experimental results show that replacing GEN timers with Exponential timers with the same mean (to satisfy the enabling restriction) yields inaccurate rejuvenation policies, and that mixed rejuvenation outperforms time-based rejuvenation in maximizing reliability, though at the cost of an acceptable decrease in availability.},
  archive      = {J_TETC},
  author       = {Laura Carnevali and Marco Paolieri and Riccardo Reali and Leonardo Scommegna and Enrico Vicario},
  doi          = {10.1109/TETC.2024.3475214},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {354-369},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Cost-effective software rejuvenation combining time-based and inspection-based policies},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performability of service chains with rejuvenation: A multidimensional universal generating function approach. <em>TETC</em>, <em>13</em>(2), 341-353. (<a href='https://doi.org/10.1109/TETC.2024.3496195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network Function Virtualization (NFV) converts legacy telecommunication systems into modular software appliances, known as service chains, running on the cloud. To address potential software aging-related issues, rejuvenation is often employed to clean up their state and maximize performance and availability. In this work, we propose a framework to model the performability of service chains with rejuvenation. Performance modeling uses queueing theory, specifically adopting an $M/G/m$ model with the Allen-Cunneen approximation, to capture real-world aspects related to service times. Availability modeling is addressed through the Multidimensional Universal Generating Function (MUGF), a recent technique that achieves computational efficiency when dealing with systems with many sub-elements, particularly useful for multi-provider service chains. Additionally, we deploy an experimental testbed based on the Open5GS service chain, to estimate key performance and availability parameters. Supported by experimental results, we evaluate the impact of rejuvenation on the performability of the Open5GS service chain. The numerical analysis shows that i) the configuration of replicas across nodes is important to meet availability goals; ii) rejuvenation can bring one additional “nine” of availability, depending on the time to recovery; and iii) MUGF can significantly reduce computational complexity through straightforward algebraic manipulations.},
  archive      = {J_TETC},
  author       = {Luigi De Simone and Mario Di Mauro and Roberto Natella and Fabio Postiglione},
  doi          = {10.1109/TETC.2024.3496195},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {341-353},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Performability of service chains with rejuvenation: A multidimensional universal generating function approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Open and closed-loop predictive control strategies for software rejuvenation. <em>TETC</em>, <em>13</em>(2), 330-340. (<a href='https://doi.org/10.1109/TETC.2024.3481997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software rejuvenation is a cyberdefense mechanism that periodically resets the control software of a system to limit the impact of cyberattacks. We propose open and closed-loop tree-based model predictive controllers to explicitly account for the software refresh events and the cyberattacks. The benefits of the proposed methods are illustrated using a simulated microgrid as a case study and randomized tests with different types of attacks.},
  archive      = {J_TETC},
  author       = {Teresa Arauz and José M. Maestre and Paula Chanfreut and Daniel E. Quevedo and Eduardo F. Camacho},
  doi          = {10.1109/TETC.2024.3481997},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {330-340},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Open and closed-loop predictive control strategies for software rejuvenation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards label-efficient deep learning-based aging-related bug prediction with spiking convolutional neural networks. <em>TETC</em>, <em>13</em>(2), 314-329. (<a href='https://doi.org/10.1109/TETC.2025.3531051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in Deep Learning (DL) have enhanced Aging-Related Bug (ARB) prediction for mitigating software aging. However, DL-based ARB prediction models face a dual challenge: overcoming overfitting to enhance generalization and managing the high labeling costs associated with extensive data requirements. To address the first issue, we utilize the sparse and binary nature of spiking communication in Spiking Neural Networks (SNNs), which inherently provides brain-inspired regularization to effectively alleviate overfitting. Therefore, we propose a Spiking Convolutional Neural Network (SCNN)-based ARB prediction model along with a training framework that handles the model’s spatial-temporal dynamics and non-differentiable nature. To reduce labeling costs, we introduce a Bio-inspired and Diversity-aware Active Learning framework (BiDAL), which prioritizes highly informative and diverse samples, enabling more efficient usage of the limited labeling budget. This framework incorporates bio-inspired uncertainty to enhance informativeness measurement along with using a diversity-aware selection strategy based on clustering to prevent redundant labeling. Experiments on three ARB datasets show that ARB-SCNN effectively reduces overfitting, improving generalization performance by 6.65% over other DL-based classifiers. Additionally, BiDAL boosts label efficiency for ARB-SCNN training, outperforming four state-of-the-art active learning methods by 4.77% within limited labeling budgets.},
  archive      = {J_TETC},
  author       = {Yunzhe Tian and Yike Li and Kang Chen and Zhenguo Zhang and Endong Tong and Jiqiang Liu and Fangyun Qin and Zheng Zheng and Wenjia Niu},
  doi          = {10.1109/TETC.2025.3531051},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {314-329},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Towards label-efficient deep learning-based aging-related bug prediction with spiking convolutional neural networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software aging detection and rejuvenation assessment in heterogeneous virtual networks. <em>TETC</em>, <em>13</em>(2), 299-313. (<a href='https://doi.org/10.1109/TETC.2025.3547612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we report on the application of resiliency enforcement strategies that were applied to a microservices system running on a real-world deployment of a large cluster of heterogeneous Virtual Machines (VMs). We present the evaluation results obtained from measurement and modeling implementations. The measurement infrastructure was composed of 15 large and 15 extra-large VMs. The modeling approach used Markov Decision Processes (MDP). On the measurement testbed, we implemented three different levels of software rejuvenation granularity to achieve software resiliency. We have discovered two threats to resiliency in this environment. The first threat to resiliency was a memory leak that was part of the underlying open-source infrastructure in each VM. The second threat to resiliency was the result of the contention for resources in the physical host, which is dependent on the number and size of VMs deployed to the physical host. In the MDP modeling approach, we evaluated four strategies for assigning tasks to VMs with different configurations and different levels of parallelism. Using the large cluster under study, we compared our approach of using software aging and rejuvenation with the state-of-the-art approach of using a network of VMs deployed to a private cloud without software aging detection and rejuvenation. In summary, we show that in a private cloud with non-elastic resource allocation in the physical hosts, careful performance engineering needs to be performed to optimize the trade-offs between the number of VMs allocated and the total memory allocated to each VM.},
  archive      = {J_TETC},
  author       = {Alberto Avritzer and Andrea Janes and Andrea Marin and Catia Trubiani and Andre van Hoorn and Matteo Camilli and Daniel S. Menasché and André B. Bondi},
  doi          = {10.1109/TETC.2025.3547612},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {299-313},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Software aging detection and rejuvenation assessment in heterogeneous virtual networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NegCPARBP: Enhancing privacy protection for cross-project aging-related bug prediction based on negative database. <em>TETC</em>, <em>13</em>(2), 283-298. (<a href='https://doi.org/10.1109/TETC.2025.3546549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emergence of Aging-Related Bugs (ARBs) poses a significant challenge to software systems, resulting in performance degradation and increased error rates in resource-intensive systems. Consequently, numerous ARB prediction methods have been developed to mitigate these issues. However, in scenarios where training data is limited, the effectiveness of ARB prediction is often suboptimal. To address this problem, Cross-Project Aging-Related Bug Prediction (CPARBP) is proposed, which utilizes data from other projects (i.e., source projects) to train a model aimed at predicting potential ARBs in a target project. However, the use of source-project data raises privacy concerns and discourages companies from sharing their data. Therefore, we propose a method called Cross-Project Aging-Related Bug Prediction based on Negative Database (NegCPARBP) for privacy protection. NegCPARBP first converts the feature vector of a software file into a binary string. Second, the corresponding Negative DataBase (NDB) is generated based on this binary string, containing data that is significantly more expressive from the original feature vector. Furthermore, to ensure more accurate prediction of ARB-prone and ARB-free files based on privacy-protected data (i.e., maintain the data utility), we propose a novel negative database generation algorithm that captures more information about important features, using information gain as a measure. Finally, NegCPARBP extracts a new feature vector from the NDB to represent the original feature vector, facilitating data sharing and ARB prediction objectives. Experimental results on Linux, MySQL, and NetBSD datasets demonstrate that NegCPARBP achieves a high defense against attacks (privacy protection performance reaching 0.97) and better data utility compared to existing privacy protection methods.},
  archive      = {J_TETC},
  author       = {Dongdong Zhao and Zhihui Liu and Fengji Zhang and Lei Liu and Jacky Wai Keung and Xiao Yu},
  doi          = {10.1109/TETC.2025.3546549},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {283-298},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {NegCPARBP: Enhancing privacy protection for cross-project aging-related bug prediction based on negative database},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Guest editorial: Special section on applied software aging and rejuvenation. <em>TETC</em>, <em>13</em>(2), 281-282. (<a href='https://doi.org/10.1109/TETC.2025.3579813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_TETC},
  author       = {Raffaele Romagnoli and Jianwen Xiang},
  doi          = {10.1109/TETC.2025.3579813},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {4-6},
  number       = {2},
  pages        = {281-282},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Guest editorial: Special section on applied software aging and rejuvenation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair influence maximization in social networks: A community-based evolutionary algorithm. <em>TETC</em>, <em>13</em>(1), 262-275. (<a href='https://doi.org/10.1109/TETC.2024.3403891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization (IM) has been extensively studied in network science, which attempts to find a subset of users to maximize the influence spread. A new variant of IM, fair IM (FIM), which primarily enhances the fair propagation of information, has attracted increasing attention in academia. However, existing algorithms for FIM suffer from a trade-off between fairness and running time, as it is difficult to ensure that users are fairly influenced in terms of sensitive attributes, such as race or gender, while maintaining a high influence spread. To tackle this problem, herein, we propose an effective and efficient community-based evolutionary algorithm for FIM (named CEA-FIM). In CEA-FIM, a community-based node selection strategy is proposed to identify potential nodes, which not only considers the size of the community but also the attributes of the nodes in the community. Subsequently, we designed an evolutionary algorithm based on the proposed node selection strategy to hasten the solution search, including the novel initialization, crossover, and mutation strategies. We validated the proposed algorithm by performing experiments on real-world and synthetic networks. The experimental results show that the proposed CEA-FIM achieves a better balance between effectiveness and efficiency than state-of-the-art methods do.},
  archive      = {J_TETC},
  author       = {Kaicong Ma and Xinxiang Xu and Haipeng Yang and Renzhi Cao and Lei Zhang},
  doi          = {10.1109/TETC.2024.3403891},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {262-275},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Fair influence maximization in social networks: A community-based evolutionary algorithm},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linearizing binary optimization problems using variable posets for ising machines. <em>TETC</em>, <em>13</em>(1), 250-261. (<a href='https://doi.org/10.1109/TETC.2024.3403871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ising machines are next-generation computers expected to efficiently sample near-optimal solutions of combinatorial optimization problems. Combinatorial optimization problems are modeled as quadratic unconstrained binary optimization (QUBO) problems to apply an Ising machine. However, current state-of-the-art Ising machines still often fail to output near-optimal solutions due to the complicated energy landscape of QUBO problems. Furthermore, the physical implementation of Ising machines severely restricts the size of QUBO problems to be input as a result of limited hardware graph structures. In this study, we take a new approach to these challenges by injecting auxiliary penalties preserving the optimum, which reduces quadratic terms in QUBO objective functions. The process simultaneously simplifies the energy landscape of QUBO problems, allowing the search for near-optimal solutions, and makes QUBO problems sparser, facilitating encoding into Ising machines with restriction on the hardware graph structure. We propose linearization of QUBO problems using variable posets as an outcome of the approach. By applying the proposed method to synthetic QUBO instances and to multi-dimensional knapsack problems, we empirically validate the effects on enhancing minor-embedding of QUBO problems and the performance of Ising machines.},
  archive      = {J_TETC},
  author       = {Kentaro Ohno and Nozomu Togawa},
  doi          = {10.1109/TETC.2024.3403871},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {250-261},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Linearizing binary optimization problems using variable posets for ising machines},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Online resource provisioning and batch scheduling for AIoT inference serving in an XPU edge cloud. <em>TETC</em>, <em>13</em>(1), 234-249. (<a href='https://doi.org/10.1109/TETC.2024.3403874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the accelerated convergence of artificial intelligence (AI) and the Internet of Things (IoT), the recent years have witnessed the booming of Artificial Intelligence of Things (AIoT). Edge clouds place computing and service capabilities at the network edges to reduce network transmission overhead, which has been widely recognized as the critical infrastructure for AIoT applications. Meanwhile, to accelerate computation-intensive edge cloud AI operations, specialized AI accelerators such as GPU, NPU, and TPU have been increasingly integrated into edge clouds. For such emerging XPU edge clouds, utilizing costly XPUs more efficiently has become a significant challenge. In this paper, we present an online optimization framework for joint resource provisioning and batch scheduling for more cost-efficient AIoT inference serving in an XPU edge cloud. The essential optimization process for the online framework is to first adaptively batch inference tasks to increase the system throughput without compromising the service level agreement (SLA). Next, heterogeneous XPU resources are provisioned for the batches. Finally, the resource instance is consolidated to a minimum of physical servers. Via extensive trace-driven simulations, we verify the performance of the presented online optimization framework.},
  archive      = {J_TETC},
  author       = {Rongkai Liu and Yuting Wu and Kongyange Zhao and Zhi Zhou and Xiang Gao and Xianchen Lin and Xiaoxi Zhang and Xu Chen and Gang Lu},
  doi          = {10.1109/TETC.2024.3403874},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {234-249},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Online resource provisioning and batch scheduling for AIoT inference serving in an XPU edge cloud},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamically activated de-glaring and detail- recovery for low-light image enhancement directly on smart cameras. <em>TETC</em>, <em>13</em>(1), 222-233. (<a href='https://doi.org/10.1109/TETC.2024.3403935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light conditions often significantly affect the stability of a computer-vision system. Existing studies of unpaired-learning-based low-light image enhancement do not consider glare that occurs during the night, which can lead to significant degradation of image quality. To improve image quality, our study proposes an additional enhancement module that can be applied to existing methods. That is, our proposed “lightweight low-light image de-glaring network” can remove glare from low-light images. We also propose a “low-light image-detail-recovery network” to enhance the boundary details of low-light images after removing glare to further improve image quality. The experimental results show that our proposed approaches can effectively improve low-light image quality. In addition, we propose “dynamically activated de-glaring” to assess the quality of input images first to determine whether de-glaring should be undertaken in order to effectively utilize the computational resources of a smart camera and avoid unnecessary image enhancement. The experimental results show that running time and frames per second can be greatly improved when applied to real-world scenarios.},
  archive      = {J_TETC},
  author       = {Shao-Wei Dong and Ching-Hu Lu},
  doi          = {10.1109/TETC.2024.3403935},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {222-233},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Dynamically activated de-glaring and detail- recovery for low-light image enhancement directly on smart cameras},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing knowledge reusability: A distributed multitask machine learning approach. <em>TETC</em>, <em>13</em>(1), 207-221. (<a href='https://doi.org/10.1109/TETC.2024.3390811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of the Internet of Things, the unprecedented growth of data surpasses current predictive analytics and processing capabilities. Due to the potential redundancy of similar data and analytics tasks, it is imperative to extract patterns from distributed data and predictive models so that existing schemes can be efficiently reused in distributed computing environments. This is expected to avoid building and maintaining reduplicative predictive models. The fundamental challenge, however, is the detection of reusable tasks and tuning models in order to improve predictive capacity while being reused. We introduce a two-phase Distributed Multi-task Machine Learning (DMtL) framework coping with this challenge. In the first phase, similar tasks are identified and efficiently grouped together according to locally trained models’ performance meta-features, using Partial Learning Curves (PLC). In the subsequent phase, we leverage the PLC-driven DMtL paradigm to boost the performance of candidate reusable models per group of tasks in distributed computing environments. We provide a thorough analysis of our framework along with a comparative assessment against relevant approaches and prior work found in the respective literature. Our experimental results showcase the feasibility of the PLC-driven DMtL method in terms of adaptability and reusability of existing knowledge in distributed computing systems.},
  archive      = {J_TETC},
  author       = {Qianyu Long and Christos Anagnostopoulos and Kostas Kolomvatsos},
  doi          = {10.1109/TETC.2024.3390811},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {207-221},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Enhancing knowledge reusability: A distributed multitask machine learning approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SelfVis: Self-supervised learning for human activity recognition based on area charts. <em>TETC</em>, <em>13</em>(1), 196-206. (<a href='https://doi.org/10.1109/TETC.2024.3392850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) has long been an active research topic as it enables us to infer human behaviors and daily routines from sensor data collected on wearables or on sensors embedded in a pervasive sensing environment. In recent years, deep learning has been widely used in HAR for feature extraction and multimodal fusion, and has achieved promising performance on activity recognition. However, they often require a large number of labeled data for training. To directly tackle this challenge, this paper proposes SelfVis, a novel visualization-based self-supervised learning technique, which aims to extract effective features without the need of labeled data. To achieve this goal, it encodes time-series IMU sensor readings into images and then employs ResNet, a pre-trained, state-of-the-art convolutional neural network (CNN) as the backbone feature extractor. It leverages the fact that there exist multiple sensors often being used and uses sensor identifications that are generated automatically as a prediction target during the self-supervised learning process. With these two, SelfVis has achieved high activity recognition accuracy even when only a small number of labeled data are available; that is, with only 1% training data, SelfVis has demonstrated the ability to achieve higher performance than state-of-the-art techniques by up to 0.46 in macro F1-scores.},
  archive      = {J_TETC},
  author       = {Ai Jiang and Juan Ye},
  doi          = {10.1109/TETC.2024.3392850},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {196-206},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SelfVis: Self-supervised learning for human activity recognition based on area charts},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Janus: A trusted execution environment approach for attack detection in industrial robot controllers. <em>TETC</em>, <em>13</em>(1), 185-195. (<a href='https://doi.org/10.1109/TETC.2024.3390435'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few decades, technological progress has led to a spike in the adoption of robots by the manufacturing industry. With the new “Industry 4.0” paradigm, companies strive to automate their production processes by interconnecting and integrating different industrial systems. The resulting increase in complexity contributes to a larger attack surface and paves the way for novel attacks. In the context of cyber-physical systems, consequences include economic and physical damage, as well as harm to human workers. In this article, we present Janus, a novel monitoring mechanism for industrial robot controllers that exploits the trusted execution environment (TEE) to guarantee the integrity of the attack detection algorithm even in case the controller's software is compromised, while not requiring external hardware for its detection process. In particular, we use the state observers strategy for detecting low-level controller (LLC) attacks. We assess our approach by testing it against various attacks, identifying those that are simpler to detect and pinpointing the more elusive ones, which are mostly detected nonetheless. Finally, we demonstrate that our approach does not add significant computation overheads.},
  archive      = {J_TETC},
  author       = {Stefano Longari and Jacopo Jannone and Mario Polino and Michele Carminati and Andrea Zanchettin and Mara Tanelli and Stefano Zanero},
  doi          = {10.1109/TETC.2024.3390435},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {185-195},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Janus: A trusted execution environment approach for attack detection in industrial robot controllers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HARPOCRATES: An approach towards efficient encryption of data-at-rest. <em>TETC</em>, <em>13</em>(1), 173-184. (<a href='https://doi.org/10.1109/TETC.2024.3387558'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new block cipher called HARPOCRATES, which is different from traditional SPN, Feistel, or ARX designs. The new design structure that we use is called the substitution convolution network. The novelty of the approach lies in that the substitution function does not use fixed S-boxes. Instead, it uses a key-driven lookup table storing a permutation of all 8-bit values. If the lookup table is sufficiently randomly shuffled, the round sub-operations achieve good confusion and diffusion to the cipher. While designing the cipher, the security, cost, and performances are balanced, keeping the requirements of encryption of data-at-rest in mind. The round sub-operations are massively parallelizable and designed such that a single active bit may make the entire state (an $8 \times 16$ binary matrix) active in one round. We analyze the security of the cipher against linear, differential, and impossible differential cryptanalysis. The cipher's resistance against many other attacks like algebraic attacks, structural attacks, and weak keys are also shown. We implemented the cipher in software and hardware; found that the software implementation of the cipher results in better throughput than many well-known ciphers. Although HARPOCRATES is appropriate for the encryption of data-at-rest, it is also well-suited in data-in-transit environments.},
  archive      = {J_TETC},
  author       = {Md Rasid Ali and Debranjan Pal and Abhijit Das and Dipanwita Roy Chowdhury},
  doi          = {10.1109/TETC.2024.3387558},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {173-184},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {HARPOCRATES: An approach towards efficient encryption of data-at-rest},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-spike SNN: Single-spike phase coding with base manipulation for ANN-to-SNN conversion loss minimization. <em>TETC</em>, <em>13</em>(1), 162-172. (<a href='https://doi.org/10.1109/TETC.2024.3386893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As spiking neural networks (SNNs) are event-driven, energy efficiency is higher than conventional artificial neural networks (ANNs). Since SNN delivers data through discrete spikes, it is difficult to use gradient methods for training, limiting its accuracy. To keep the accuracy of SNNs similar to ANN counterparts, pre-trained ANNs are converted to SNNs (ANN-to-SNN conversion). During the conversion, encoding activations of ANNs to a set of spikes in SNNs is crucial for minimizing the conversion loss. In this work, we propose a single-spike phase coding as an encoding scheme that minimizes the number of spikes to transfer data between SNN layers. To minimize the encoding error due to single-spike approximation in phase coding, threshold shift and base manipulation are proposed. Without any additional retraining or architectural constraints on ANNs, the proposed conversion method does not lose inference accuracy (0.58% on average) verified on three convolutional neural networks (CNNs) with CIFAR and ImageNet datasets. In addition, graph convolutional networks (GCNs) are converted to SNNs successfully with an average accuracy loss of 0.90%. Most importantly, the energy efficiency of our SNN improves by 4.6$\sim\!\! 17.3\times$ compared to the ANN baseline.},
  archive      = {J_TETC},
  author       = {Sangwoo Hwang and Jaeha Kung},
  doi          = {10.1109/TETC.2024.3386893},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {162-172},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {One-spike SNN: Single-spike phase coding with base manipulation for ANN-to-SNN conversion loss minimization},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LP-star: Embedding longest paths into star networks with large-scale missing edges under an emerging assessment model. <em>TETC</em>, <em>13</em>(1), 147-161. (<a href='https://doi.org/10.1109/TETC.2024.3387119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Star networks play an essential role in designing parallel and distributed systems. With the massive growth of faulty edges and the widespread applications of the longest paths and cycles, it is crucial to embed the longest fault-free paths and cycles in edge-faulty networks. However, the traditional fault model allows a concentrated distribution of faulty edges and thus can only tolerate faults that depend on the minimum degree of the network vertices. This article introduces an improved fault model called the partitioned fault model, which is an emerging assessment model for fault tolerance. Based on this model, we first explore the longest fault-free paths and cycles by proving the edge fault-tolerant Hamiltonian laceability, edge fault-tolerant strongly Hamiltonian laceability, and edge fault-tolerant Hamiltonicity in the $n$-dimensional star network $S_{n}$. Furthermore, based on the theoretical proof, we give an $O(nN)$ algorithm to construct the longest fault-free paths in star networks based on the partitioned fault model, where $N$ is the number of vertices in $S_{n}$. We also make comparisons to show that our result of edge fault tolerance has exponentially improved other known results.},
  archive      = {J_TETC},
  author       = {Xiao-Yan Li and Jou-Ming Chang},
  doi          = {10.1109/TETC.2024.3387119},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {147-161},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {LP-star: Embedding longest paths into star networks with large-scale missing edges under an emerging assessment model},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FakeTracer: Catching face-swap DeepFakes via implanting traces in training. <em>TETC</em>, <em>13</em>(1), 134-146. (<a href='https://doi.org/10.1109/TETC.2024.3386960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face-swap DeepFake is an emerging AI-based face forgery technique that can replace the original face in a video with a generated face of the target identity while retaining consistent facial attributes such as expression and orientation. Due to the high privacy of faces, the misuse of this technique can raise severe social concerns, drawing tremendous attention to defend against DeepFakes recently. In this article, we describe a new proactive defense method called FakeTracer to expose face-swap DeepFakes via implanting traces in training. Compared to general face-synthesis DeepFake, the face-swap DeepFake is more complex as it involves identity change, is subjected to the encoding-decoding process, and is trained unsupervised, increasing the difficulty of implanting traces into the training phase. To effectively defend against face-swap DeepFake, we design two types of traces, sustainable trace (STrace) and erasable trace (ETrace), to be added to training faces. During the training, these manipulated faces affect the learning of the face-swap DeepFake model, enabling it to generate faces that only contain sustainable traces. In light of these two traces, our method can effectively expose DeepFakes by identifying them. Extensive experiments corroborate the efficacy of our method on defending against face-swap DeepFake.},
  archive      = {J_TETC},
  author       = {Pu Sun and Honggang Qi and Yuezun Li and Siwei Lyu},
  doi          = {10.1109/TETC.2024.3386960},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {134-146},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {FakeTracer: Catching face-swap DeepFakes via implanting traces in training},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A bio-inspired implementation of a sparse-learning spike-based hippocampus memory model. <em>TETC</em>, <em>13</em>(1), 119-133. (<a href='https://doi.org/10.1109/TETC.2024.3387026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The brain is capable of solving complex problems simply and efficiently, far surpassing modern computers. In this regard, neuromorphic engineering focuses on mimicking the basic principles that govern the brain in order to develop systems that achieve such computational capabilities. Within this field, bio-inspired learning and memory systems are still a challenge to be solved, and this is where the hippocampus is involved. It is the region of the brain that acts as a short-term memory, allowing the learning and storage of information from all the sensory nuclei of the cerebral cortex and its subsequent recall. In this work, we propose a novel bio-inspired hippocampal memory model with the ability to learn memories, recall them from a fragment of itself (cue) and even forget memories when trying to learn others with the same cue. This model has been implemented on SpiNNaker using Spiking Neural Networks, and a set of experiments were performed to demonstrate its correct operation. This work presents the first simulation implemented on a special-purpose hardware platform for Spiking Neural Networks of a fully functional bio-inspired spike-based hippocampus memory model, paving the road for the development of future more complex neuromorphic systems.},
  archive      = {J_TETC},
  author       = {Daniel Casanueva-Morato and Alvaro Ayuso-Martinez and Juan P. Dominguez-Morales and Angel Jimenez-Fernandez and Gabriel Jimenez-Moreno},
  doi          = {10.1109/TETC.2024.3387026},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {119-133},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A bio-inspired implementation of a sparse-learning spike-based hippocampus memory model},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel privacy-preserving range query scheme with permissioned blockchain for smart grid. <em>TETC</em>, <em>13</em>(1), 105-118. (<a href='https://doi.org/10.1109/TETC.2024.3386803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-enhanced Smart Grid is being deeply studied by many scholars because of its unique advantages in system and security of distributed accounting and traceability. However, the problems of data privacy disclosure and low efficiency are still worthy of the attention of most researchers. In this paper, we design a general three-tier architecture of Smart Grid based on blockchain, including edge layer, permissioned blockchain layer, and application layer. Furthermore, for the three-tier architecture, a novel privacy-preserving range query scheme without a trusted authority is proposed by adopting fog computing, permissioned blockchain, Paillier homomorphic encryption system, and Goldwasser-Micali cryptosystems. This scheme can realize range query in batch, while it can also protect privacy and resist collusion attack. Performance evaluations and experiment comparisons show that our scheme has good advantages: higher efficiency and lower storage, and thus it can meet increasing data service requirements.},
  archive      = {J_TETC},
  author       = {Kun-Chang Li and Peng-Bo Wang and Run-Hua Shi},
  doi          = {10.1109/TETC.2024.3386803},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {105-118},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A novel privacy-preserving range query scheme with permissioned blockchain for smart grid},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning based intelligent tumor analytics framework for quantitative grading and analyzing cancer metastasis: Case of lymph node breast cancer. <em>TETC</em>, <em>13</em>(1), 90-104. (<a href='https://doi.org/10.1109/TETC.2024.3487258'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {False-positive or false-negative detection, and the resulting inappropriate treatments in cancer metastasis cases, have led to numerous fatal instances due to human errors. Traditional cancer diagnoses are often subjectively interpreted through naked-eye observation, which can vary among different medical practitioners. In this research, we propose a novel deep learning-based framework called Intelligent Tumor Analytics (ITA). ITA facilitates on-the-fly assessment of Whole Slide Imaging (WSI) at the histopathological level, primarily utilizing cellular appearance, spatial arrangement, and the relative proximities of various cell types (e.g., tumor cells, immune cells, and other objects of interest) observed within scanned WSI images of tumors. By automatically quantifying relevant indicators and estimating their scores, ITA establishes a standardized evaluation that aligns with widely recognized international tumor grading standards, including the TNM and Nottingham Grading Standards. The objective measurements and assessments offered by ITA provide informative and unbiased insights to users (i.e., pathologists) involved in determining prognosis and treatment plans. The quantified information regarding tumor risk and potential for further metastasis possibilities serves as crucial early knowledge during cancer development.},
  archive      = {J_TETC},
  author       = {Tengyue Li and Simon Fong and Yaoyang Wu and Xin Zhang and Qun Song and Huafeng Qin and Sabah Mohammed and Tian Feng and Juntao Gao and Andrea Sciarrone},
  doi          = {10.1109/TETC.2024.3487258},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {90-104},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Deep learning based intelligent tumor analytics framework for quantitative grading and analyzing cancer metastasis: Case of lymph node breast cancer},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Low-power real-time seizure monitoring using AI-assisted sonification of neonatal EEG. <em>TETC</em>, <em>13</em>(1), 80-89. (<a href='https://doi.org/10.1109/TETC.2024.3481035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting seizures in neonates requires continuous electroencephalography (EEG) monitoring, a costly process that demands trained experts. Although recent advancements in machine learning offer promising solutions for automated seizure detection, the opaque nature of these algorithms poses significant challenges to their adoption in healthcare settings. A prior study demonstrated that integrating machine learning with sonification—an interpretation method that converts bio-signals into sound—can mitigate the black-box problem while enhancing seizure detection performance. This AI-assisted sonification algorithm can provide a valuable complementary tool in seizure monitoring besides the traditional visualization method. A low-power and affordable implementation of the algorithm is presented in this study using a microcontroller. To improve its practicality, we also introduce a real-time design that allows the sonification algorithm to function in parallel with data acquisition. The system consumes 12 mW in average, making it suitable for a battery-powered device.},
  archive      = {J_TETC},
  author       = {Tien Nguyen and Aengus Daly and Sergi Gomez-Quintana and Feargal O'Sullivan and Andriy Temko and Emanuel Popovici},
  doi          = {10.1109/TETC.2024.3481035},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {80-89},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Low-power real-time seizure monitoring using AI-assisted sonification of neonatal EEG},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning approach for collaborative and secure smart healthcare applications. <em>TETC</em>, <em>13</em>(1), 68-79. (<a href='https://doi.org/10.1109/TETC.2024.3473911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Across all periods of human history, the importance attributed to health has remained a fundamental and significant facet. This statement holds greater validity within the present context. The pressing demand for healthcare solutions with real-time capabilities, affordability, and high precision is crucial in medical research and technology progress. In recent times, there has been a significant advancement in emerging technologies such as AI, IoT, blockchain, and edge computing. These breakthrough developments have led to the creation of various intelligent applications. Smart healthcare applications can be realized by combining robust AI detection and prediction capabilities with edge computing architecture, which offers low computing costs and latency. In this paper, we begin by conducting a literature review of AI-assisted EC-based smart healthcare applications from the past three years. Our goal is to identify gaps and barriers in this field. We propose a smart healthcare architecture model that integrates AI technology into the edge. Finally, we summarize the challenges and research directions associated with the proposed model.},
  archive      = {J_TETC},
  author       = {Quy Vu Khanh and Abdellah Chehri and Van Anh Dang and Quy Nguyen Minh},
  doi          = {10.1109/TETC.2024.3473911},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {68-79},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Federated learning approach for collaborative and secure smart healthcare applications},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FedRDF: A robust and dynamic aggregation function against poisoning attacks in federated learning. <em>TETC</em>, <em>13</em>(1), 48-67. (<a href='https://doi.org/10.1109/TETC.2024.3474484'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) represents a promising approach to typical privacy concerns associated with centralized Machine Learning (ML) deployments. Despite its well-known advantages, FL is vulnerable to security attacks such as Byzantine behaviors and poisoning attacks, which can significantly degrade model performance and hinder convergence. The effectiveness of existing approaches to mitigate complex attacks, such as median, trimmed mean, or Krum aggregation functions, has been only partially demonstrated in the case of specific attacks. Our study introduces a novel robust aggregation mechanism utilizing the Fourier Transform (FT), which is able to effectively handle sophisticated attacks without prior knowledge of the number of attackers. Employing this data technique, weights generated by FL clients are projected into the frequency domain to ascertain their density function, selecting the one exhibiting the highest frequency. Consequently, malicious clients’ weights are excluded. Our proposed approach was tested against various model poisoning attacks, demonstrating superior performance over state-of-the-art aggregation methods.},
  archive      = {J_TETC},
  author       = {Enrique Mármol Campos and Aurora Gonzalez-Vidal and José L. Hernández-Ramos and Antonio Skarmeta},
  doi          = {10.1109/TETC.2024.3474484},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {48-67},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {FedRDF: A robust and dynamic aggregation function against poisoning attacks in federated learning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge-based live learning for robot survival. <em>TETC</em>, <em>13</em>(1), 34-47. (<a href='https://doi.org/10.1109/TETC.2024.3479082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce survival-critical machine learning (SCML), in which a robot encounters dynamically evolving threats that it recognizes via machine learning (ML), and then neutralizes. We model survivability in SCML, and show the value of the recently developed approach of Live Learning. This edge-based ML technique embodies an iterative human-in-the-loop workflow that concurrently enlarges the training set, trains the next model in a sequence of “best-so-far” models, and performs inferencing for both threat detection and pseudo-labeling. We present experimental results using datasets from the domains of drone surveillance, planetary exploration, and underwater sensing to quantify the effectiveness of Live Learning as a mechanism for SCML.},
  archive      = {J_TETC},
  author       = {Eric Sturzinger and Jan Harkes and Padmanabhan Pillai and Mahadev Satyanarayanan},
  doi          = {10.1109/TETC.2024.3479082},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {34-47},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Edge-based live learning for robot survival},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). X-RAFT: Improve RAFT consensus to make blockchain better secure EdgeAI-human-IoT data. <em>TETC</em>, <em>13</em>(1), 22-33. (<a href='https://doi.org/10.1109/TETC.2024.3472059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of IoT devices, advancements in edge computing, and innovations in AI technology have created an ideal environment for the birth and growth of Edge AI. With the trend towards the Internet of Everything (IoE), the EdgeAI- Human-IoT architectural framework highlights the necessity for efficient data exchange interconnectivity. Ensuring secure data sharing and efficient data storage are pivotal challenges in achieving seamless data interconnection. Owing to its simplicity, ease of deployment, and consensus-reaching capabilities, the RAFT consensus algorithm, which is commonly used in distributed storage, faces limitations as the IoT scale expands. The computational, communication, and storage capabilities of nodes are constraints, and the security of data remains a concern. To address these complex challenges, we introduce the X-RAFT consensus algorithm, which is tailored for blockchain technology. This algorithm enhances system performance and robustness, mitigates the impact of system load, enhances system sustainability, and increases Byzantine fault tolerance. Through analysis and simulations, our proposed solution has been evidenced to provide reliable security and efficient performance.},
  archive      = {J_TETC},
  author       = {Fengqi Li and Jiaheng Wang and Weilin Xie and Ning Tong and Deguang Wang},
  doi          = {10.1109/TETC.2024.3472059},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {22-33},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {X-RAFT: Improve RAFT consensus to make blockchain better secure EdgeAI-human-IoT data},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel prediction technique for federated learning. <em>TETC</em>, <em>13</em>(1), 5-21. (<a href='https://doi.org/10.1109/TETC.2024.3471458'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers have studied how to improve Federated Learning (FL) in various areas, such as statistical and system heterogeneity, communication cost, and privacy. So far, most of the proposed solutions are either very tied to the application context or complex to be broadly reproduced in real-life applications involving humans. Developing modular solutions that can be leveraged by the vast majority of FL structures and are independent of the application people use is the new research direction opened by this paper. In this work, we propose a plugin (named FedPredict) to address three problems simultaneously: data heterogeneity, low performance of new/untrained and/or outdated clients, and communication cost. We do so mainly by combining global and local parameters (which brings generalization and personalization) in the inference step while adapting layer selection and matrix factorization techniques to reduce the downlink communication cost (server to client). Due to its simplicity, it can be applied to federated learning of different number of topologies. Results show that adding the proposed plugin to a given FL solution can significantly reduce the downlink communication cost by up to 83.3% and improve accuracy by up to 304% compared to the original solution.},
  archive      = {J_TETC},
  author       = {Cláudio G. S. Capanema and Allan M. de Souza and Joahannes B. D. da Costa and Fabrício A. Silva and Leandro A. Villas and Antonio A. F. Loureiro},
  doi          = {10.1109/TETC.2024.3471458},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {5-21},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A novel prediction technique for federated learning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Editorial special section on emerging edge AI for human-in-the-loop cyber physical systems. <em>TETC</em>, <em>13</em>(1), 3-4. (<a href='https://doi.org/10.1109/TETC.2024.3472428'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge Artificial Intelligence (AI) enables us to deploy distributed AI models, optimize computational and energy resources, minimize communication demands, and, most importantly, meet privacy requirements for Internet of Things (IoT) applications. Since data remains on the end-devices and only model parameters are shared with the server, it becomes possible to leverage the vast amount of data collected from smartphones and IoT devices without compromising the user's privacy. However, Federated Learning (FL) solutions also have well-known limitations. In particular, as systems that account for human behaviour become increasingly vital, future technologies need to become attuned to human behaviours. Indeed, we are already witnessing unparalleled advancements in technology that empower our tools and devices with intelligence, sensory abilities, and communication features. At the same time, continued advances in the miniaturization of computational capabilities can enable us to go far beyond the simple tagging and identification, towards integrating computational resources directly into these objects, thus making our tools “intelligent”. Yet, there is limited scientific work that considers humans as an integral part of these IoT-powered cyber-physical systems.},
  archive      = {J_TETC},
  author       = {Radu Marculescu and Jorge Sá Silva},
  doi          = {10.1109/TETC.2024.3472428},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1-3},
  number       = {1},
  pages        = {3-4},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Editorial special section on emerging edge AI for human-in-the-loop cyber physical systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
