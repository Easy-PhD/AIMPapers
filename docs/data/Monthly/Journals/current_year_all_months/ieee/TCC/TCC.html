<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcc">TCC - 75</h2>
<ul>
<li><details>
<summary>
(2025). Cloud load balancers need to stay off the data path. <em>TCC</em>, <em>13</em>(3), 1078-1090. (<a href='https://doi.org/10.1109/TCC.2025.3595172'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load balancers (LBs) are crucial in cloud environments, ensuring workload scalability. They route packets destined for a service (identified by a virtual IP address, or VIP) to a group of servers designated to deliver that service, each with its direct IP address (DIP). Consequently, LBs significantly impact the performance of cloud services and the experience of tenants. Many academic studies focus on specific issues such as designing new load balancing algorithms and developing hardware load balancing devices to enhance the LB’s performance, reliability, and scalability. However, we believe this approach is not ideal for cloud data centers for the following reasons: (i) the increasing demands of users and the variety of cloud service types turn the LB into a bottleneck; and (ii) continually adding machines or upgrading hardware devices can incur substantial costs. In this paper, we propose the Next Generation Load Balancer (NGLB), designed to bypass the TCP connection datapath from the LB, thereby eliminating latency overheads and scalability bottlenecks of traditional cloud LBs. The LB only participates in the TCP connection establishment phase. The three key features of our design are: (i) the introduction of an active address learning model to redirect traffic and bypass the LB, (ii) a multi-tenant isolation mechanism for deployment within multi-tenant Virtual Private Cloud networks, and (iii) a distributed flow control method, known as hierarchical connection cleaner, designed to ensure the availability of backend resources. The evaluation results demonstrate that NGLB reduces latency by 16% and increases nearly 3× throughput. With the same LB resources, NGLB improves 10× rate of new connection establishment. More importantly, five years of operational experience has proven NGLB’s stability for high-bandwidth services.},
  archive      = {J_TCC},
  author       = {Yuchen Zhang and Shuai Jin and Zhenyu Wen and Shibo He and Qingzheng Hou and Yang Song and Zhigang Zong and Xiaomin Wu and Bengbeng Xue and Chenghao Sun and Ku Li and Xing Li and Biao Lyu and Rong Wen and Jiming Chen and Shunmin Zhu},
  doi          = {10.1109/TCC.2025.3595172},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1078-1090},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud load balancers need to stay off the data path},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MKAC: Efficient and privacy-preserving multi- keyword ranked query with ciphertext access control in cloud environments. <em>TCC</em>, <em>13</em>(3), 1065-1077. (<a href='https://doi.org/10.1109/TCC.2025.3594575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosion of Big Data in cloud environments, data owners tend to delegate the storage and computation to cloud servers. Since cloud servers are generally untrustworthy, data owners often encrypt data before outsourcing it to the cloud. Numerous privacy-preserving schemes for the multi-keyword ranked query have been proposed, but most of these schemes do not support ciphertext access control, which can easily lead to malicious access by unauthorized users, causing serious damage to personal privacy and commercial secrets. To address the above challenges, we propose an efficient and privacy-preserving multi-keyword ranked query scheme (MKAC) that supports ciphertext access control. Specifically, in order to enhance the efficiency of the multi-keyword ranked query, we employ a vantage point (VP) tree to organize the keyword index. Additionally, we develop a VP tree-based multi-keyword ranked query algorithm, which utilizes the pruning strategy to minimize the number of nodes to search. Next, we propose a privacy-preserving multi-keyword ranked query scheme that combines asymmetric scalar-product-preserving encryption with the VP tree. Furthermore, attribute-based encryption mechanism is used to generate the decryption key based on the query user’s attributes, which is then employed to decrypt the query results and trace any malicious query user who may leak the secret key. Finally, a rigorous analysis of the security of MKAC is conducted. The extensive experimental evaluation shows that the proposed scheme is efficient and practical.},
  archive      = {J_TCC},
  author       = {Haiyong Bao and Lu Xing and Honglin Wu and Menghong Guan and Na Ruan and Cheng Huang and Hong-Ning Dai},
  doi          = {10.1109/TCC.2025.3594575},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1065-1077},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {MKAC: Efficient and privacy-preserving multi- keyword ranked query with ciphertext access control in cloud environments},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CSCR: A cross-view intelligent scheduling method implemented via cloud computing workflow reduction. <em>TCC</em>, <em>13</em>(3), 1050-1064. (<a href='https://doi.org/10.1109/TCC.2025.3591549'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in the development of artificial intelligence has led to increases in the complexity of computational tasks and the resource demands within cloud computing scenarios. Therefore, intelligent scheduling methods have formed a crucial research area. Solving complex scheduling problems requires many problem feature and long-sequence decision-making observations as possible. To address the workflow scheduling problem under the limited capabilities of models, workflow reduction and cross-view workflow scheduling problems are first proposed in this article, with the optimization objectives and constraints of each problem described. Second, a cross-view intelligent scheduling method implemented via cloud computing workflow reduction (CSCR), including a workflow reduction sorting algorithm (Task-priority ranker), an intelligent reduction algorithm (Workflow view-transformer), and a cross-view intelligent scheduling algorithm (Joint-scheduler), is proposed. We also propose an intelligent scheduling architecture under the workflow reduction paradigm. By reducing the workflow, we provide multiple views that support the decision-making processes of deep reinforcement learning-based scheduling models and coordinate workflow views before and after the reduction step to achieve cross-view joint scheduling. Experimental results show that CSCR achieves minimum advantages of 42.1%, 43.2%, and 33.3% in terms of three workflow reduction indicators over four other algorithms, significantly optimizing the effect of the employed scheduling model.},
  archive      = {J_TCC},
  author       = {Genxin Chen and Jin Qi and Xingjian Zhu and Jialin Hua and Zhenjiang Dong and Yanfei Sun},
  doi          = {10.1109/TCC.2025.3591549},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1050-1064},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {CSCR: A cross-view intelligent scheduling method implemented via cloud computing workflow reduction},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Layer redundancy aware DNN model repository planning for fast model download in edge cloud. <em>TCC</em>, <em>13</em>(3), 1038-1049. (<a href='https://doi.org/10.1109/TCC.2025.3591482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The booming development of artificial intelligence (AI) applications has greatly promoted edge intelligence technology. To support latency-sensitive Deep Neural Network (DNN) based applications, the integration of serverless inference paradigm into edge intelligence has become a widely recognized solution. However, the long DNN model downloading time from central clouds to edge servers hinders inference performance, and asks for establishing model repository within the edge cloud. This paper first identifies the inherent layer redundancy in DNN models, which is potentially beneficial to improve the storage efficiency of the model repository in the edge cloud. However, how to exploit the layer redundancy feature and allocate the DNN layers across different edge servers with capacitated storage resources to reduce the model downloading time remains challenging. To address this issue, we first formulate this problem in Quadratic Integer Programming (QIP) form, based on which a randomized rounding layer redundancy aware DNN model storage planning strategy is proposed. Our approach significantly reduces model downloading time by up to 63% compared to state-of-the-art methods, as demonstrated through extensive trace-driven experiments.},
  archive      = {J_TCC},
  author       = {Hongmin Geng and Yuepeng Li and Sheng Wang and Lin Gu and Deze Zeng},
  doi          = {10.1109/TCC.2025.3591482},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1038-1049},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Layer redundancy aware DNN model repository planning for fast model download in edge cloud},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Securing and sustaining IoT edge-computing architectures through nanoservice integration. <em>TCC</em>, <em>13</em>(3), 1026-1037. (<a href='https://doi.org/10.1109/TCC.2025.3588681'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of the Internet of Things (IoT) and edge computing devices calls for solutions that deliver low latency, energy efficiency, and robust security—often challenging goals to balance simultaneously. This paper introduces a novel nanoservice-based framework that dynamically adapts to changing demands while achieving sustainable and secure edge operations. By breaking down functionalities into specialized and narrowly scoped nanoservices that are requested only as needed and eliminated when idle, the approach significantly reduces latency and energy usage compared to conventional, more static methods. Moreover, integrating a Zero-Trust Architecture (ZTA) ensures that every component—computational or security-related—is continuously verified and restricted through strict access controls and micro-segmentation. This framework’s adaptability extends uniformly to all nanoservices, including those providing security features, thereby maintaining strong protective measures even as workloads and network conditions evolve. Experimental evaluations on IoT devices under varying workloads demonstrate that the proposed approach significantly reduces energy consumption and latency while maintaining security and scalability. These results underscore the potential for an integrated, flexible model that simultaneously addresses energy efficiency, performance, and security—an essential trifecta in future edge computing environments.},
  archive      = {J_TCC},
  author       = {Cinthya Celina Tamayo Gonzalez and Ijaz Ahmad and Simone Soderi and Erkki Harjula},
  doi          = {10.1109/TCC.2025.3588681},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1026-1037},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Securing and sustaining IoT edge-computing architectures through nanoservice integration},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Accelerating AI-generated content collaborative inference via transfer reinforcement learning in dynamic edge networks. <em>TCC</em>, <em>13</em>(3), 1011-1025. (<a href='https://doi.org/10.1109/TCC.2025.3586878'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While diffusion models have demonstrated remarkable success in computer vision tasks, their deployment in Internet of Things environments remains challenging. Edge devices face significant constraints in computational resources and must adapt to dynamic operating conditions. To address these limitations, we propose a novel system that accelerates AI-generated content (AIGC) collaborative inference in dynamic edge networks. The proposed system introduces a multi-exit vision transformer-based U-Net architecture that enables efficient processing through adaptive exit point selection during the diffusion process, optimizing the trade-off between inference accuracy and computational efficiency. To optimize device-level operations, we develop an innovative generative AI-assisted reinforcement learning framework that determines optimal exit selection and offloading strategies to maximize generation quality and inference speed. Furthermore, we design a fine-tuning approach with policy reuse mechanisms that facilitates rapid reinforcement learning algorithm deployment across diverse environments. Extensive experimental evaluations demonstrate that our system outperforms existing algorithms in terms of balancing inference latency and generation quality, while also exhibiting improved adaptability to environmental variations.},
  archive      = {J_TCC},
  author       = {Meng Tian and Zhicheng Liu and Chenxuan Hou and Chao Qiu and Xiaofei Wang and Dusit Niyato and Victor C. M. Leung},
  doi          = {10.1109/TCC.2025.3586878},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {1011-1025},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Accelerating AI-generated content collaborative inference via transfer reinforcement learning in dynamic edge networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fog-enhanced personalized privacy-preserving data analysis for smart homes. <em>TCC</em>, <em>13</em>(3), 995-1010. (<a href='https://doi.org/10.1109/TCC.2025.3586052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Internet of Things (IoT) devices has led to a surge in data generation within smart home environments. This data explosion has raised significant privacy concerns and highlighted a lack of user-friendly controls. Consequently, there is a pressing need for a robust privacy-enhancing mechanism tailored for smart homes, safeguarding sensitive data from a user-centric perspective. In this article, we introduce the Fog-enhanced Personalized Differential Privacy (FEPDP) model, which utilizes the distributed nature of fog computing to improve data processing efficiency and security in smart homes. Specifically, the personalization, as a key feature of FEPDP, is manifested through an array of user-driven policy specifications, enabling home users to specify secret and privacy specifications for their personal data. These specifications not only enhance control over personal data but also align with the heterogeneous nature of smart home environments. Subsequently, aligned with fog-based smart home architecture, we propose two policy-driven partitioning mechanisms that utilize threshold partitioning based on dynamic programming to effectively implement FEPDP. Finally, comprehensive theoretical analysis and experimental validation across various statistical analysis tasks and datasets confirm that FEPDP achieves a superior privacy-utility trade-off for smart home data by leveraging non-sensitive data and fog-based partitioning.},
  archive      = {J_TCC},
  author       = {Jiajun Chen and Chunqiang Hu and Weihong Sheng and Hui Xia and Pengfei Hu and Jiguo Yu},
  doi          = {10.1109/TCC.2025.3586052},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {995-1010},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Fog-enhanced personalized privacy-preserving data analysis for smart homes},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing container security through phase-based system call filtering. <em>TCC</em>, <em>13</em>(3), 983-994. (<a href='https://doi.org/10.1109/TCC.2025.3583414'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container technology in cloud computing has improved resource utilization and deployment efficiency, but it also introduces new security risks. Excessive privileges in containerized environments can allow attackers to exploit insufficiently restricted system calls, potentially leading to container escapes and other attacks. Some system calls are only necessary during the initialization phase of a container, and allowing them during runtime can increase the risk of exploitation. This paper proposes a Phase-based System Call Filtering (PSF) method to minimize system call permissions during the runtime of cloud containers. The PSF method builds a comprehensive whitelist of system calls during the initialization phase to cover all necessary calls for containerized applications. During runtime, a refined, phase-specific system call whitelist is enforced, dynamically adjusting privileges based on the functions encapsulated within the container. Additionally, we introduce a container phase recognition algorithm to distinguish between initialization and runtime phases, supporting the generation of phase-specific system call lists. Experimental results show that the proposed method enhances runtime system call restrictions, minimizes privileges, and improves the overall security of cloud containers.},
  archive      = {J_TCC},
  author       = {Ke Chen and Hui Lu and Yinnan Yao and Binxing Fang and Yuan Liu and Zhihong Tian},
  doi          = {10.1109/TCC.2025.3583414},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {983-994},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enhancing container security through phase-based system call filtering},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Refrain from inquiring about my scalable storage and boolean queries for secure cloud. <em>TCC</em>, <em>13</em>(3), 969-982. (<a href='https://doi.org/10.1109/TCC.2025.3582645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outsourcing personal data to a convenient and affordable cloud platform has become a popular practice. Considering the risk of privacy leakage, users usually encrypt their data before uploading it to the cloud server. Searchable encryption (SE) allows cloud servers to manage and search data in encrypted form based on user-specified requests. However, coercion attacks are rarely considered, where users may be forced to open search records and results. Therefore, deniable SE solutions against coercion attacks are presented, but they suffer from large storage overhead or fail to consider the dual coercion situation towards both sides of data owners and data users. In this paper, we roughly combine oblivious cross-tags protocol (OXT) and deniable encryption to propose a deniable SE (deniable cross-tag, DXT) scheme, which supports boolean queries and resists dual coercion attacks. Technically, we formalize a new primitive called updatable deniable encryption, and combine it with OXT in a non-trivial manner. In addition, we give formal system model, security model, and security proof of DXT. By employing the HUAWEI cloud platform, we conduct sufficient comparative experiments between DXT and state-of-the-art solutions based on a public dataset. The experimental results demonstrate that DXT outperforms higher search efficiency while achieving better features.},
  archive      = {J_TCC},
  author       = {Boli Hu and Kai Zhang and Junqing Gong and Haifeng Qian},
  doi          = {10.1109/TCC.2025.3582645},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {969-982},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Refrain from inquiring about my scalable storage and boolean queries for secure cloud},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REE-TM: Reliable and energy-efficient traffic management model for diverse cloud workloads. <em>TCC</em>, <em>13</em>(3), 953-968. (<a href='https://doi.org/10.1109/TCC.2025.3581697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diversity of workload demands lays a critical impact on efficient resource allocation and management of cloud services. The existing literature has either weakly considered or overlooked the heterogeneous feature of job requests received from wide range of internet services users. To address this context, the proposed approach named Reliable and Energy Efficient Traffic Management (REE-TM) has exploited the diversity of internet traffic in terms of variation in resource demands and expected complexity. Specifically, REE-TM incorporates categorization of heterogeneous job requests and executes them by selecting the most admissible virtual node (a software-defined instance such as a virtual machine or container) and physical node (an actual hardware server or compute host) within the cloud infrastructure. To deal with resource-contention-based resource failures and performance degradation, a novel workload estimator ‘Toffoli Gate-based Quantum Neural Network’ (TG-QNN) is proposed, wherein learning process or interconnection weights optimization is achieved using Quantum version of BlackHole (QBHO) algorithm. The proactively estimated workload is used to compute entropy of the upcoming internet traffic with various traffic states analysis for detection of probable resource-congestion. REE-TM is extensively evaluated through simulations using a benchmark dataset and compared with optimal and without REE-TM versions. The performance evaluation and comparison of REE-TM with measured significant metrics reveal its effectiveness in assuring higher reliability by up to 30.25% and energy-efficiency by up to 23% as compared without REE-TM.},
  archive      = {J_TCC},
  author       = {Ashutosh Kumar Singh and Deepika Saxena and Volker Lindenstruth},
  doi          = {10.1109/TCC.2025.3581697},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {953-968},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {REE-TM: Reliable and energy-efficient traffic management model for diverse cloud workloads},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A reference architecture for governance of cloud native applications. <em>TCC</em>, <em>13</em>(3), 935-952. (<a href='https://doi.org/10.1109/TCC.2025.3578557'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of cloud computing has given rise to Cloud Native Applications (CNAs), presenting new challenges in governance, particularly when faced with strict compliance requirements. This work explores the unique characteristics of CNAs and their impact on governance. We introduce a comprehensive reference architecture designed to streamline governance across CNAs, along with a sample implementation, offering insights for both single and multi-cloud environments. Our architecture seamlessly integrates governance within the CNA framework, adhering to a “battery-included” philosophy. Tailored for both expansive and compact CNA deployments across various industries, this design enables cloud practitioners to prioritize product development by alleviating the complexities associated with governance. In addition, it provides a building block for academic exploration of generic CNA frameworks, highlighting their relevance in the evolving cloud computing landscape.},
  archive      = {J_TCC},
  author       = {William Pourmajidi and Lei Zhang and John Steinbacher and Tony Erwin and Andriy Miranskyy},
  doi          = {10.1109/TCC.2025.3578557},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {935-952},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A reference architecture for governance of cloud native applications},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PHOENIX: Misconfiguration detection for AWS serverless computing. <em>TCC</em>, <em>13</em>(3), 922-934. (<a href='https://doi.org/10.1109/TCC.2025.3577211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless computing is a burgeoning cloud computing paradigm that allows developers to implement applications at the function level, known as serverless applications. Amazon Web Services (AWS), the leading provider in this field, offers Serverless Application Model (AWS SAM), a widely adopted configuration schema for configuring functions and managing resources. However, misconfigurations pose a major challenge during serverless application development, and existing methods are not applicable. To our knowledge, the configuration characteristics and misconfiguration detection for serverless applications have not been well explored. To address this gap, we collect and analyze 733 real-world serverless application configuration files using AWS SAM to understand their characteristics and challenges. Based on the insights, we design PHOENIX, a misconfiguration detection approach for serverless computing. PHOENIX learns configuration patterns from uniform representations of configurations and identifies potential misconfigurations that deviate from these patterns. To evaluate PHOENIX, we construct a dataset comprising 35 injected misconfigurations and 70 real-world misconfigurations with confirmed causes. Our results show that PHOENIX detects 100% of the injected misconfigurations and identifies 97.14% of real-world misconfigurations, significantly outperforming the state-of-the-art tool.},
  archive      = {J_TCC},
  author       = {Jinfeng Wen and Haodi Ping},
  doi          = {10.1109/TCC.2025.3577211},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {922-934},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PHOENIX: Misconfiguration detection for AWS serverless computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cross-workload power prediction method based on transfer gaussian process regression in cloud data centers. <em>TCC</em>, <em>13</em>(3), 910-921. (<a href='https://doi.org/10.1109/TCC.2025.3575790'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, machine learning (ML)-based power prediction models for servers have shown remarkable performance, leveraging large volumes of labeled data for training. However, collecting extensive labeled power data from servers in cloud data centers incurs substantial costs. Additionally, varying resource demands across different workloads (e.g., CPU-intensive, memory-intensive, and I/O-intensive) lead to significant differences in power consumption behaviors, known as domain shift. Consequently, power data collected from one type of workload cannot effectively train power prediction models for other workloads, limiting the exploration of the collected power data. To tackle these challenges, we propose TGCP, a cross-workload power prediction method based on multi-source transfer Gaussian process regression. TGCP transfers knowledge from abundant power data across multiple source workloads to a target workload with limited power data. Furthermore, Continuous normalizing flows adjust the posterior prediction distribution of Gaussian process, making it locally non-Gaussian, enhancing TGCP’s ability to handle real-world power data distribution. This method enhances prediction accuracy for the target workload while reducing the expense of acquiring power data for real cloud data centers. Experimental results on a realistic power consumption dataset demonstrate that TGCP surpasses four traditional ML methods and three transfer learning methods in cross-workload power prediction.},
  archive      = {J_TCC},
  author       = {Ruichao Mo and Weiwei Lin and Haocheng Zhong and Minxian Xu and Keqin Li},
  doi          = {10.1109/TCC.2025.3575790},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {910-921},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A cross-workload power prediction method based on transfer gaussian process regression in cloud data centers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robin: An efficient hierarchical federated learning framework via a learning-based synchronization scheme. <em>TCC</em>, <em>13</em>(3), 895-909. (<a href='https://doi.org/10.1109/TCC.2025.3574823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical federated learning (HFL) extends traditional federated learning by introducing a cloud-edge-device framework to enhance scalability. However, the challenge of determining when devices and edges should aggregate models remains unresolved, making the design of an effective synchronization scheme crucial. Additionally, the heterogeneity in computing and communication capabilities, coupled with non-independent and identically distributed (non-IID) data distributions, makes synchronization particularly complex. In this article, we propose Robin, a learning-based synchronization scheme for HFL systems. By collecting data such as models’ parameters, CPU usage, communication time, etc., we design a deep reinforcement learning-based approach to decide the frequencies of cloud aggregation and edge aggregation, respectively. The proposed scheme well considers device heterogeneity, non-IID data and device mobility, to maximize the training model accuracy while minimizing the energy overhead. Meanwhile, we prove the convergence of Robin’s synchronization scheme. And we build an HFL testbed and conduct the experiments with real data obtained from Raspberry Pi and Alibaba Cloud. Extensive experiments under various settings are conducted to confirm the effectiveness of Robin, which can improve 31.2% in model accuracy while reducing energy consumption by 36.4%.},
  archive      = {J_TCC},
  author       = {Tianyu Qi and Yufeng Zhan and Peng Li and Yuanqing Xia},
  doi          = {10.1109/TCC.2025.3574823},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {895-909},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Robin: An efficient hierarchical federated learning framework via a learning-based synchronization scheme},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leakage reduced searchable symmetric encryption for multi-keyword queries. <em>TCC</em>, <em>13</em>(3), 882-894. (<a href='https://doi.org/10.1109/TCC.2025.3573378'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conjunctive keyword queries on untrusted cloud servers represent one of the most common forms of search in encrypted environments. Extensive research has been devoted to developing efficient schemes that support multi-keyword queries. In particular, the Oblivious Cross-Tags (OXT) protocol has received significant attention and is widely regarded as a benchmark in this domain. However, existing schemes fail to simultaneously hide the Keyword-Pair Result Pattern (KPRP) and the conditional Intersection Pattern (IP), potentially leaking additional information to the server. In this work, we propose a novel searchable symmetric encryption (SSE) scheme, referred to as Result Hiding Search (RHS), which aims to minimize result pattern leakage and achieve query result hiding during the index retrieval phase by integrating Private Set Intersection (PSI) techniques. Our scheme enhances privacy by employing PSI for secure membership testing. To improve query efficiency, we shift the expensive complex computation to the offline phase, and utilize efficient pseudorandom functions and hash functions during the online phase. Moreover, we propose a variant of RHS, called vRHS, designed to reduce client-side storage overhead. A simulation-based security proof demonstrates that our scheme is robust against non-adaptive adversaries. Comprehensive experimental evaluation further shows that our approach achieves better security and efficiency trade-offs compared to existing SSE schemes.},
  archive      = {J_TCC},
  author       = {Qinghua Deng and Lanxiang Chen and Yizhao Zhu and Yi Mu},
  doi          = {10.1109/TCC.2025.3573378},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {882-894},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Leakage reduced searchable symmetric encryption for multi-keyword queries},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). T-COMS: A time-slot-aware and cost-effective data transfer method for geo-distributed data centers. <em>TCC</em>, <em>13</em>(3), 867-881. (<a href='https://doi.org/10.1109/TCC.2025.3572308'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demands placed on geographically distributed Data Centers (DCs), recent studies have focused on optimizing performance from the perspective of both cloud providers and customers. These studies address a variety of goals, such as minimizing transmission time, reducing resource usage, and optimizing network costs. However, many existing models for workload transfers operate using a uniform time-slot approach, which limits their flexibility in handling variable data transfer requests with different deadline requirements. This lack of adaptability can negatively impact the quality of service for users. Additionally, these models often overlook the potential benefits of incorporating multiple data sources, which can lead to sub-optimal transmission times. To overcome these limitations, this paper introduces T-COMS, a Time-slot-aware, COst-effective, and Multi-Source-aware method for file transfers tailored specifically for geo-distributed DCs, leveraging a multi-source and dynamic time-slot strategy to accelerate transmission and enhance service quality. The proposed model identifies the optimal sources, paths, and time slot lengths required to efficiently transmit workloads to their destinations while minimizing costs. Initially, we introduced a Mixed Integer Non-Linear Programming (MINLP) model and subsequently linearized it within our framework. Given the NP-hard nature of the proposed model, its applicability is limited in large-scale environments. To address this issue, we developed an efficient heuristic algorithm that can derive near-optimal solutions in polynomial time. The simulation results demonstrate the effectiveness of the proposed T-COMS model and the heuristic algorithm in terms of the reduction in cost and transmission time for file transfers between geographically distributed DCs.},
  archive      = {J_TCC},
  author       = {Bita Fatemipour and Zhe Zhang and Marc St-Hilaire},
  doi          = {10.1109/TCC.2025.3572308},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {867-881},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {T-COMS: A time-slot-aware and cost-effective data transfer method for geo-distributed data centers},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personalized cloud gaming: Multi-objective optimization for resource utilization and video encoding. <em>TCC</em>, <em>13</em>(3), 854-866. (<a href='https://doi.org/10.1109/TCC.2025.3571095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud gaming represents a major part of contemporary gaming. To boost the Quality-of-Experience (QoE) of cloud gaming, the integration of Dynamic Adaptive Video Encoding (DAVE) with Multi-access Edge Computing (MEC) has become the natural candidate owing to its flexibility and reliable transmission support for real-time interactions. However, as multiple gamers compete for limited resources to achieve personalized QoE, such as ultra-high video quality and ultra-low latency, how to support efficient edge resource optimization is a fundamental and important problem. Furthermore, determining the optimal game video encoding configuration in real-time poses significant challenges, especially when lacking the information on future video and edge network resources. To address these key issues, we jointly optimize the video encoding as well as computing and communication resource allocation by active mutual adaptation of video coding configurations and physical resources in a Software Defined Networking (SDN)-assisted edge network. This eliminates the performance bottleneck caused by decoupling optimization of coding parameter configuration and physical resource allocation. The SDN-assisted edge network architecture supports efficient on-demand resource management, provides global network information, and meets the stringent time-varying game requests. Due to the significant time scale difference between video chunk and physical resource block, we propose a novel Asynchronous Decision-Making Multi Agent Proximal Policy Optimization algorithm (AD-MAPPO), which can address the credit assignment problem with a single agent. It can also adapt to the highly dynamic cloud gaming environment without prior knowledge and a deterministic environmental model. Extensive experimentation based on real cloud gaming datasets convincingly demonstrates that our approach can significantly enhance the overall QoE of gamers.},
  archive      = {J_TCC},
  author       = {Jingjing Zhang and Xiaoheng Deng and Jinsong Gui and Xuechen Chen and Shaohua Wan and Geyong Min},
  doi          = {10.1109/TCC.2025.3571095},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {854-866},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Personalized cloud gaming: Multi-objective optimization for resource utilization and video encoding},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic QoS-driven framework for co-scheduling of distributed long-running applications on shared clusters. <em>TCC</em>, <em>13</em>(3), 837-853. (<a href='https://doi.org/10.1109/TCC.2025.3571098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud service providers typically co-locate various workloads within the same production cluster to improve resource utilization and reduce operational costs. These workloads primarily consist of batch analysis jobs composed of multiple parallel short-running tasks and long-running applications (LRAs) that continuously reside in the system. The adoption of microservice architecture has led to the emergence of distributed LRAs (DLRAs), which enhance deployment flexibility but pose challenges in detecting and investigating QoS violations due to workload variability and performance propagation across microservices. State-of-the-art resource managers are only responsible for resource allocation among applications/jobs and do not prioritize runtime QoS aspects, such as application-level latency. To address this, we introduce Prank, a QoS-driven resource management framework for co-located workloads. Prank incorporates a non-intrusive performance anomaly detection mechanism for DLRAs and proposes a root cause localization algorithm based on PageRank-weighted analysis of performance anomalies. Moreover, it dynamically balances resource allocation between DLRAs and co-located batch jobs on nodes hosting critical microservices, optimizing for both DLRA performance and overall cluster efficiency. Experimental results demonstrate that Prank outperforms state-of-the-art baselines, reducing DLRA tail latency by over 38% while increasing batch job completion time by no more than 21% on average.},
  archive      = {J_TCC},
  author       = {Jianyong Zhu and Hongtao Wang and Pan Su and Yang Wang and Weihua Pan},
  doi          = {10.1109/TCC.2025.3571098},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {837-853},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Dynamic QoS-driven framework for co-scheduling of distributed long-running applications on shared clusters},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reversible data hiding in encrypted images based on chinese remainder theorem. <em>TCC</em>, <em>13</em>(3), 821-836. (<a href='https://doi.org/10.1109/TCC.2025.3570327'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To deal with the development of the distributed server, this article proposes a new method for reversible data hiding in encrypted images based on the Chinese Remainder Theorem (CRT), encrypting and sharing one image to multiple data hiders through $(k,n)$-threshold secret sharing. First, an original image is divided into the most significant bit (MSB) compression area and the least significant bit (LSB) area by utilizing the spatial correlation. The $l$-MSB layers are predicted to obtain prediction errors, and these prediction errors are compressed by Huffman coding. Then according to the value of $k$, CRT and secret sharing scheme are performed on the $(8-l)$-LSB layers to generate the shared bitstream. Finally, $n$ encrypted images for sharing consist of MSB compression bitstreams and shared bitstreams, whose size is adjusted based on $k$ value. Each data hider can independently embed secret data after having one of the encrypted images, while the receiver can recover the original image only after receiving $k$ or more encrypted images. Experimental results show that the proposed algorithm not only provides a large embedding space for secret data, but is also able to complete the inverse operation of data hiding and realize the lossless recovery of the original image with $(k,n)$-threshold secret sharing.},
  archive      = {J_TCC},
  author       = {Jiani Chen and Dawen Xu},
  doi          = {10.1109/TCC.2025.3570327},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {821-836},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Reversible data hiding in encrypted images based on chinese remainder theorem},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lattice-based revocable IBEET scheme for mobile cloud computing. <em>TCC</em>, <em>13</em>(3), 807-820. (<a href='https://doi.org/10.1109/TCC.2025.3570332'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identity-based encryption with equality test (IBEET) is a special form of searchable encryption that has broad applications in cloud computing. It enables users to perform equality tests on encrypted data without decryption, thereby achieving secure data search while ensuring data privacy and confidentiality. However, in the context of mobile cloud computing, the susceptibility of mobile devices to loss significantly increases the risk of private key exposure. Existing IBEET schemes struggle to address this issue effectively, limiting their practical applicability. Moreover, with the rapid advancement of quantum computing, the security of traditional cryptographic hardness assumptions faces potential threats. To address these challenges and enhance system efficiency, we proposes the first lattice-based revocable IBEET (RIBEET) scheme, which supports user key revocation. We prove that our scheme satisfies adaptive CCA security under the assumption of DLWE hard problem. Additionally, performance evaluations comparing our scheme with existing ones demonstrate that our scheme offers significant efficiency advantages. Furthermore, we apply the proposed scheme to mobile health services, showcasing its practicality and reliability in mobile cloud computing environments.},
  archive      = {J_TCC},
  author       = {Hongwei Wang and Yongjian Liao and Zhishuo Zhang and Yingjie Dong and Shijie Zhou},
  doi          = {10.1109/TCC.2025.3570332},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {807-820},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Lattice-based revocable IBEET scheme for mobile cloud computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-related parameter selection for training deep learning models predicting application performance degradation in clouds. <em>TCC</em>, <em>13</em>(3), 794-806. (<a href='https://doi.org/10.1109/TCC.2025.3570093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications deployed in clouds are susceptible to performance degradation due to diverse underlying causes such as infrastructure faults. To maintain the expected availability of these applications, Machine Learning (ML) models can be used to predict the impending application performance degradations to take preventive measures. However, the prediction accuracy of these ML models, which is a key indicator of their performance, is influenced by several factors, including training data size, data sampling intervals, input window and prediction horizon. To optimize these data-related parameters, in this article, we propose a surrogate-assisted multi-objective optimization algorithm with the objective to maximize prediction model accuracy while minimizing the resources consumed for data collection and storage. We evaluated the proposed algorithm through two use cases focusing on the prediction of Key Performance Indicators (KPIs) for a 5G core network and a web application deployed in two Kubernetes-based cloud testbeds. It is demonstrated that the proposed algorithm can achieve a normalized hypervolume of 99.5% relative to the optimal Pareto front and reduce search time for the optimal solution by 0.6 hours compared to other surrogates and by 3.58 hours compared to using no surrogates.},
  archive      = {J_TCC},
  author       = {Behshid Shayesteh and Chunyan Fu and Amin Ebrahimzadeh and Roch H. Glitho},
  doi          = {10.1109/TCC.2025.3570093},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {794-806},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Data-related parameter selection for training deep learning models predicting application performance degradation in clouds},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demand-aware distributed scheduling with adaptive buffer control in reconfigurable data center networks. <em>TCC</em>, <em>13</em>(3), 783-793. (<a href='https://doi.org/10.1109/TCC.2025.3568369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable data center networks (RDCNs), integrating the electrical packet switch (EPS) with the optical circuit switch (OCS), improve network adaptability by enabling high-throughput connections between top-of-rack (ToR) pairs. However, existing RDCN scheduling schemes face challenges in responsiveness, particularly during traffic bursts. In this article, we propose a novel demand-aware distributed scheduling framework called P4-DADS, utilizing P4-based programmable ToR switches (P4ToR). To prevent conflicts arising from simultaneous OCS port allocations, P4-DADS employs a token-ring-based distributed reservation algorithm, enhanced with an adaptive buffer control (ABC) mechanism. By formulating a Markov decision process (MDP) problem, the optimal ABC policy is obtained through a value iteration algorithm, ensuring that packets are immediately ready for transmission during sudden demand surges. P4-DADS improves network responsiveness and scalability, as evidenced by a 145.95% increase in throughput and a 87.31% reduction in flow completion time. These improvements demonstrate the potential of P4-DADS as a scalable and efficient solution for resource management in RDCN.},
  archive      = {J_TCC},
  author       = {Subin Han and Eunsok Lee and Hyunkyung Yoo and Namseok Ko and Sangheon Pack},
  doi          = {10.1109/TCC.2025.3568369},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {783-793},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Demand-aware distributed scheduling with adaptive buffer control in reconfigurable data center networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Achieving enhanced bi-linear attention network for teaching manner analysis over edge cloud-assisted AIoT: Voice-body coordination perspective. <em>TCC</em>, <em>13</em>(3), 769-782. (<a href='https://doi.org/10.1109/TCC.2025.3568394'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing, an advanced extension of cloud computing, provides superior computational capabilities and low-latency processing at the network edge, facilitating its availability for real-time data analysis in resource-limited settings. When applied to the analysis of teaching methodologies, edge computing enables the seamless integration of vocal and physical cues, facilitating collaborative, dynamic, and real-time evaluations of teaching quality. However, the inherent complexity of human perception and multimodal interactions impose great challenges to the analysis of these aspects in Artificial Intelligence of Things (AIoT). This paper introduces an innovative mathematical model and a measurement index specifically designed to assess changes in voice-body coordination over time. To achieve this, we propose a cloud-enabled enhanced Bi-Linear Attention Network incorporating entropy and Fourier transforms (BAN-E-FT), which leverages both temporal and frequency-domain features. Specifically, by harnessing the computational and storage capabilities of edge computing, BAN-E-FT facilitates distributed training, expedites large-scale data processing, and enhances model scalability, where entropy measures and Fourier transforms capture modality dynamics, enhancing BAN's fusion capabilities. Moreover, a conditional domain adversarial network is embedded to address regional teaching variations, improving model generalizability. We also verify the robustness of BAN-E-FT with accuracy and convergence through convex optimization analysis. Experiments on the eNTERFACE’05 dataset demonstrate 81% accuracy in assessing teaching adaptability, while real-world test at Guizhou University confirms 78% accuracy when using BAN-E-FT, matching human expert assessments.},
  archive      = {J_TCC},
  author       = {Yu Zhou and Sai Zou and Bochun Wu and Wei Ni and Xiaojiang Du},
  doi          = {10.1109/TCC.2025.3568394},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {7-9},
  number       = {3},
  pages        = {769-782},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Achieving enhanced bi-linear attention network for teaching manner analysis over edge cloud-assisted AIoT: Voice-body coordination perspective},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing cloud computing performance through integration of a threshold-based load balancing algorithm with multiple service broker policies. <em>TCC</em>, <em>13</em>(2), 751-768. (<a href='https://doi.org/10.1109/TCC.2025.3563848'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The triumph of cloud computing hinges upon the adept instantiation of infrastructure and the judicious utilization of available resources. Load balancing, a pivotal facet, substantiates the fulfillment of these imperatives, thereby augmenting the performance of the cloud environment for its users. Our research introduces a load balancing algorithm grounded in threshold principles devised to ensure equitable distribution of workloads among nodes. The main objective of the algorithm is to preclude the overburdening of virtual machines (VMs) within the cloud with tasks or their idleness due to task allocation deficiencies in the presence of active tasks. The threshold values embedded in our algorithm ascertain the judicious deployment of VMs, forestalling both task overload and idle states arising from task allocation inadequacies. Simulation outcomes manifest that our threshold-based algorithm markedly enhances response time for tasks/requests and data processing duration within datacenters, outperforming extant algorithms such as First Come First Serve, Round Robin, and the Equally Spread Current Execution Load Balancing algorithm. Our threshold algorithm attains superior results to alternative load balancing algorithms when coupled with an optimized response time service broker policy.},
  archive      = {J_TCC},
  author       = {Shusmoy Chowdhury and Ajay Katangur},
  doi          = {10.1109/TCC.2025.3563848},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {751-768},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimizing cloud computing performance through integration of a threshold-based load balancing algorithm with multiple service broker policies},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling resource scheduling in optical switching DCNs under bursty and skewed traffic. <em>TCC</em>, <em>13</em>(2), 737-750. (<a href='https://doi.org/10.1109/TCC.2025.3561281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When optical switching is deployed in Data Center Networks (DCNs), the reconfiguration of the optical switching matrix leads to substantially longer overheads, posing a significant impact on the system performance. Despite the extensive studies on the scheduling algorithms based on demand matrix decomposition (DMD), the stateful and irregular nature of the scheduling processes hinders the development of quantitative models, thereby limiting our understanding of resource scheduling in optical switching DCNs based on DMD. In this article, we model the DMD based resource scheduling process under a bursty and skewed traffic pattern and derive closed-form equations for the burst completion time. Our study shows that an increased reconfiguration delay will lead to an approximate linear increase in the burst completion time. Our study also demonstrates that the size of the slot and the maximum allowed duration of one match are approximately inversely proportional to the burst completion time, with diminishing marginal returns.},
  archive      = {J_TCC},
  author       = {Shuai Zhang and Baojun Chen and Weiqiang Sun and Weisheng Hu},
  doi          = {10.1109/TCC.2025.3561281},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {737-750},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Modeling resource scheduling in optical switching DCNs under bursty and skewed traffic},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure kNN for distributed cloud environment using fully homomorphic encryption. <em>TCC</em>, <em>13</em>(2), 721-736. (<a href='https://doi.org/10.1109/TCC.2025.3561586'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-preserving k-nearest neighbor (PPkNN) classification for multiple clouds enables categorizing queried data into a class in keeping with data privacy, where the database and key servers jointly perform cryptographic operations. The existing solutions, unfortunately, take a long time and incur a large amount of traffic between the database and key servers. Therefore, in this article, we propose a fast and secure kNN classification protocol, namely FSkNN, over distributed databases deployed in multiple clouds under the semi-honest model. Particularly, we focus on optimizing the network-related operations during kNN classification. That is, the proposed cryptographic protocol reduces the number of interactions between the servers by using a fully homomorphic encryption scheme and eliminates unnecessary traffic by applying mathematical techniques. In addition, the indistinguishability-based security of FSkNN is proven. We implemented FSkNN with C++ and the testbed experiments demonstrate that the proposed scheme significantly facilitates the query response time and reduces the communication cost.},
  archive      = {J_TCC},
  author       = {Yuuya Fukuchi and Sota Hashimoto and Kazuya Sakai and Satoshi Fukumoto and Min-Te Sun and Wei-Shinn Ku},
  doi          = {10.1109/TCC.2025.3561586},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {721-736},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure kNN for distributed cloud environment using fully homomorphic encryption},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SOCT: Secure outsourcing computation toolkit using threshold ElGamal algorithm. <em>TCC</em>, <em>13</em>(2), 711-720. (<a href='https://doi.org/10.1109/TCC.2025.3561313'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing offers inexpensive and scalable solutions for data processing, however privacy concerns often hinder the outsourcing of sensitive information. Homomorphic encryption provides a promising approach for secure computations over encrypted data. However, existing models often rely on restrictive assumptions, such as semi-honest adversaries and inaccessible public data. To address these limitations, we introduce the Secure Outsourcing Computation Toolkit (SOCT), which is a novel framework based on the threshold ElGamal cryptosystem. The toolkit employs a dual-server decryption architecture using a (2,2) threshold additively homomorphic ElGamal (TAHEG) algorithm. This architecture ensures that ciphertexts can be decrypted only with the cooperation of both servers, mitigating the risk of data breaches. The TAHEG algorithm requires the input of a secret key for every decryption operation, preventing unauthorized access to plaintext data. Moreover, the key generation process does not burden users with generating or distributing partial secret keys. We provide rigorous security proofs for our threshold ElGamal cryptosystem and associated secure computation functions. Experimental results demonstrate that SOCT achieves significant efficiency gains compared to existing toolkits, making it a practical choice for privacy-preserving data outsourcing.},
  archive      = {J_TCC},
  author       = {Sen Hu and Shang Ci and Donghai Guan and Çetin Kaya Koç},
  doi          = {10.1109/TCC.2025.3561313},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {711-720},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SOCT: Secure outsourcing computation toolkit using threshold ElGamal algorithm},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Breaking the edge: Enabling efficient neural network inference on integrated edge devices. <em>TCC</em>, <em>13</em>(2), 694-710. (<a href='https://doi.org/10.1109/TCC.2025.3559346'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing has gained widespread attention in cloud computing due to the increasing demands of AIoT applications and the evolution of edge architectures. One prevalent application in this domain is neural network inference on edge for computing and processing. This article presents an in-depth exploration of inference on integrated edge devices and introduces EdgeNN, a groundbreaking solution for inference specifically designed for CPU-GPU integrated edge devices. EdgeNN offers three key innovations. First, EdgeNN adaptively employs zero-copy optimization by harnessing unified physical memory. Second, EdgeNN introduces an innovative approach to CPU-GPU hybrid execution tailored for inference tasks. This technique enables concurrent CPU and GPU operation, effectively leveraging edge platforms’ computational capabilities. Third, EdgeNN adopts a finely tuned adaptive inference tuning technique that analyzes complex inference structures. It divides computations into sub-tasks, intelligently assigning them to the two processors for better performance. Experimental results demonstrate EdgeNN's superiority across six popular neural network inference processing. EdgeNN delivers average speed improvements of 3.97×, 4.10×, 3.12×, and 8.80× when compared to inference on four distinct edge CPUs. Furthermore, EdgeNN achieves significant time advantages compared to the direct execution of original programs. This improvement is attributed to better unified memory utilization (44.37%) and the innovative CPU-GPU hybrid execution approach (17.91%). Additionally, EdgeNN exhibits superior energy efficiency, providing 29.14× higher energy efficiency than edge CPUs and 5.70× higher energy efficiency than discrete GPUs. EdgeNN is now open source at https://github.com/ChenyangZhang-cs/EdgeNN.},
  archive      = {J_TCC},
  author       = {Feng Zhang and Chenyang Zhang and Jiawei Guan and Qiangjun Zhou and Kuangyu Chen and Xiao Zhang and Bingsheng He and Jidong Zhai and Xiaoyong Du},
  doi          = {10.1109/TCC.2025.3559346},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {694-710},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Breaking the edge: Enabling efficient neural network inference on integrated edge devices},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PKEST: Public-key encryption with similarity test for medical consortia cloud computing. <em>TCC</em>, <em>13</em>(2), 680-693. (<a href='https://doi.org/10.1109/TCC.2025.3558858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing eliminates the limitations of local hardware architecture while also enabling rapid data sharing between healthcare institutions. Encryption of electronic medical records (EMRs) before uploading to cloud servers is necessary for privacy. However, encryption brings challenges for computation. Public Key Encryption with Equality Test (PKEET) allows cloud servers to test the underlying message equality without decryption. Therefore, it can be used to classify the encrypted EMRs corresponding to different medical symptoms. However, traditional PKEETs have limitations in testing the similarity between the ciphertexts. Undoubtedly, it can not handle EMR classification with similar medical symptoms efficiently. In this work, we propose a lightweight public key encryption with similarity test (PKEST) for the EMR classification shared in medical consortia. Our scheme can resist offline message recovery attacks, which may be launched by the insider manager, and the traditional paring computation is not necessary. Our experiment simulation shows that the similarity error between ciphertext and plaintext is tiny when the parameters are set properly. Compared to previous works, our scheme not only achieves the classification of similar encrypted EMRs but is also more efficient than traditional PKEETs since our construction does not need paring computation anymore.},
  archive      = {J_TCC},
  author       = {Junsong Chen and Shengke Zeng and Song Han and Jin Yin and Peng Chen},
  doi          = {10.1109/TCC.2025.3558858},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {680-693},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PKEST: Public-key encryption with similarity test for medical consortia cloud computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving and traceable functional encryption for inner product in cloud computing. <em>TCC</em>, <em>13</em>(2), 667-679. (<a href='https://doi.org/10.1109/TCC.2025.3556925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing is a distributed infrastructure that centralizes server resources on a platform in order to provide services over the internet. Traditional public-key encryption protects data confidentiality in cloud computing, while functional encryption provides a more fine-grained decryption method, which only reveals a function of the encrypted data. However, functional encryption in cloud computing faces the problem of key sharing. In order to trace malicious users who share keys with others, traceable FE-IP (TFE-IP) schemes were proposed where the key generation center (KGC) knows users’ identities and binds them with different secret keys. Nevertheless, existing schemes fail to protect the privacy of users’ identities. The fundamental challenge to construct a privacy-preserving TFE-IP scheme is that KGC needs to bind a key with a user's identity without knowing the identity. To balance privacy and accountability in cloud computing, we propose the concept of privacy-preserving traceable functional encryption for inner product (PPTFE-IP) and give a concrete construction which offers the features: (1) To prevent key sharing, both a user's identity and a vector are bound together in the key; (2) The KGC and a user execute a two-party secure computing protocol to generate a key without the former knowing anything about the latter's identity; (3) Each user can ensure the integrity and correctness of his/her key through verification; (4) The inner product of the two vectors embedded in a ciphertext and in his/her key can be calculated by an authorized user; (5) Only the tracer can trace the identity embedded in a key. We formally reduce the security of the proposed PPTFE-IP to well-known complexity assumptions, and conduct an implementation to evaluate its efficiency. The novelty of our scheme is to protect the user's privacy and provide traceability if required.},
  archive      = {J_TCC},
  author       = {Muyao Qiu and Jinguang Han and Feng Hao and Chao Sun and Ge Wu},
  doi          = {10.1109/TCC.2025.3556925},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {667-679},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Privacy-preserving and traceable functional encryption for inner product in cloud computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delegatable multi-authority attribute-based anonymous credentials. <em>TCC</em>, <em>13</em>(2), 655-666. (<a href='https://doi.org/10.1109/TCC.2025.3555519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing, users need to authenticate to access various resources. Attribute-based anonymous credentials (ABCs) provide a tool for privacy-preserving authentication, allowing users to prove possession of a set of attributes to cloud service providers anonymously. Most existing works on ABC deal with credentials on attributes issued by a single authority (issuer). In reality, it is more practical for users to obtain credentials on attributes from multiple authorities. There are a few works on multi-authority ABC, which do not support delegation needed in real deployments. In this article, we present the first delegatable multi-authority attribute-based anonymous credential system, which simultaneously achieves revocation and traceability. We also give the security analysis of our construction. Finally, we implement our system, and the experimental results show its efficiency.},
  archive      = {J_TCC},
  author       = {Meng Sun and Junzuo Lai and Xiaohan Mo and Chi Wu and Peng Li and Cheng-Kang Chu and Robert H. Deng},
  doi          = {10.1109/TCC.2025.3555519},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {655-666},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Delegatable multi-authority attribute-based anonymous credentials},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReflexPilot: Startup-aware dependent task scheduling based on deep reinforcement learning for edge-cloud collaborative computing. <em>TCC</em>, <em>13</em>(2), 641-654. (<a href='https://doi.org/10.1109/TCC.2025.3555231'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing number of devices, the demand for data computation is growing rapidly. In edge-cloud collaborative computing, tasks can be scheduled to servers as interdependent subtasks, enhancing performance through parallel computing. A task is executed in an executor, which must first initialize the runtime environment in a process called task startup. However, most existing research neglects the reuse of executors, leading to considerable delays during task startup. To address this issue, we model the edge-cloud collaborative task scheduling scenario considering executor reuse, task startup, and dependency relationships. We then formulate the dependent task scheduling problem with task startup. To meet real-time demands in edge-cloud collaborative computing, we propose ReflexPilot, an online task scheduling architecture featuring executor management. Building on this architecture, we introduce OTSA-PPO, a task scheduling algorithm based on Proximal Policy Optimization (PPO), and EMA, an advanced executor allocation algorithm. Under constraints of computational and communication resources, ReflexPilot leverages OTSA-PPO for online scheduling of dependent tasks based on current states, while EMA pre-creates and reuses executors to reduce the average task completion time. Extensive simulations demonstrate that ReflexPilot significantly reduces the average task completion time by 31% to 71% compared with existing baselines.},
  archive      = {J_TCC},
  author       = {Wenhao Zou and Zongshuai Zhang and Nina Wang and Yu Tian and Lin Tian},
  doi          = {10.1109/TCC.2025.3555231},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {641-654},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {ReflexPilot: Startup-aware dependent task scheduling based on deep reinforcement learning for edge-cloud collaborative computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BPFGuard: Multi-granularity container runtime mandatory access control. <em>TCC</em>, <em>13</em>(2), 629-640. (<a href='https://doi.org/10.1109/TCC.2025.3551838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adoption of container-based cloud computing services has been prevalent, especially with the introduction of Kubernetes, which enables the automated deployment, scaling, and administration of applications in containers, hence boosting the popularity of containers. As a result, researchers have placed greater emphasis on container runtime security, notably investigating the efficacy of traditional techniques such as Capabilities, Seccomp, and Linux security modules in guaranteeing container security. However, due to the limitations imposed by the container environment, the results have been unsatisfactory. In addition, eBPF-based solutions face the problem of being unable to quickly load policies and affect real-time operations when faced with newer kernel vulnerabilities. This paper investigates the limitations of existing container security mechanisms. Additionally, it examines the specific constraints of these mechanisms in Kubernetes environments. The paper classifies container monitoring and obligatory access control into three distinct categories: system call access control, LSM hook access control, and kernel function access control. Therefore, we propose a technique for regulating container access with a variety of granularity levels. This technique is executed using eBPF and is tightly integrated with Kubernetes to collect relevant meta-information. In addition, we suggest implementing a consolidated routing method and employing function tail call chaining to overcome the limitation of eBPF in enforcing mandatory access control for containers. Lastly, we conducted a series of experiment to verify the effectiveness of the system's security using CVE-2022-0492 and to benchmark the system that had BPFGuard enabled. The results indicate that the average performance loss increased merely by 2.16%, demonstrating that there are no adverse effects on the container services. This suggests that greater security can be achieved at a minimal cost.},
  archive      = {J_TCC},
  author       = {Hui Lu and Xiaojiang Du and Dawei Hu and Shen Su and Zhihong Tian},
  doi          = {10.1109/TCC.2025.3551838},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {629-640},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {BPFGuard: Multi-granularity container runtime mandatory access control},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bidirectional identity-based inner-product functional re-encryption in vaccine data sharing. <em>TCC</em>, <em>13</em>(2), 617-628. (<a href='https://doi.org/10.1109/TCC.2025.3552740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of cloud computing, more and more data is stored in cloud servers, which leads to an increasing degree of privacy of data stored in cloud servers. For example, in the critical domain of medical vaccine trials, where public health outcomes hinge on the analysis of sensitive patient data, the imperative to safeguard privacy has never been more pronounced. Traditional encryption methods, though effective at protecting data, often expose vulnerabilities during decryption and lack the ability to support granular data access and computation. One-way re-encryption schemes further impede the agility of data sharing, which is indispensable for the collaborative efforts of research institutions. To address these limitations, we propose a novel bidirectional re-encryption scheme for inner-product functional encryption (IPFE). Our scheme secures data while allowing computation and sharing in an encrypted state, preserving patient privacy without hindering research. By harnessing inner-product functional encryption, our approach allows authorized researchers to extract valuable insights from encrypted data, significantly enhancing privacy protections. Our scheme’s security is predicated on the $l$-ABDHE (augmented bilinear Diffie-Hellman exponent) assumption, ensuring robustness against chosen plaintext attacks within the standard model. This foundation not only secures the data but also yields compact ciphertext length, minimizing storage demands. We introduce a protocol specifically designed for medical vaccine trials, which leverages our bidirectional IB-IPFRE (Identity-Based Inner-Product Functional Re-Encryption) scheme. This protocol enhances data security, supports collaborative research, and maintains patient privacy. Its application in vaccine trials demonstrates the scheme’s effectiveness in protecting sensitive information while enabling critical research insights.},
  archive      = {J_TCC},
  author       = {Jing Wang and Yanwei Zhou and Yasi Zhu and Zhiquan Liu and Bo Yang and Mingwu Zhang},
  doi          = {10.1109/TCC.2025.3552740},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {617-628},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Bidirectional identity-based inner-product functional re-encryption in vaccine data sharing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consortium blockchain-based federated sensor-cloud for IoT services. <em>TCC</em>, <em>13</em>(2), 605-616. (<a href='https://doi.org/10.1109/TCC.2025.3543627'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work addresses the problem of ensuring service availability, trust, and profitability in sensor-cloud architecture designed to Sensors-as-a-Service (Se-aaS) using IoT generated data. Due to the requirement of geographically distributed wireless sensor networks for Se-aaS, it is not always possible for a single Sensor-cloud Service Provider (SCSP) to meet the end-users requirements. To address this problem, we propose a federated sensor-cloud architecture involving multiple SCSPs for provisioning high-quality Se-aaS. Moreover, for ensuring trust in such a distributed architecture, we propose the use of consortium blockchain to keep track of the activities of each SCSP and to automate several functionalities through Smart Contracts. Additionally, to ensure profitability and end-user satisfaction, we propose a composite scheme, named BRAIN, comprising of two parts. First, we define miner's score to select an optimal subset of SCSPs as miners periodically. Second, we propose a modified multiple-leaders-multiple-followers Stackelberg game-theoretic approach to decide the association of an optimal subset of SCSPs to each service. Thereafter, we evaluate the performance of BRAIN by comparing with three existing benchmark schemes through simulations. Simulation results depict that BRAIN outperforms existing schemes in terms of profits and resource consumption of SCSPs, and price charged from end-users.},
  archive      = {J_TCC},
  author       = {Sudip Misra and Aishwariya Chakraborty and Ayan Mondal and Dhanush Kamath},
  doi          = {10.1109/TCC.2025.3543627},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {605-616},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Consortium blockchain-based federated sensor-cloud for IoT services},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deadline-aware online job scheduling for distributed training in heterogeneous clusters. <em>TCC</em>, <em>13</em>(2), 590-604. (<a href='https://doi.org/10.1109/TCC.2025.3548604'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The explosive growth in training data and model sizes has spurred the adoption of distributed deep learning (DL) in heterogeneous computing clusters. Efficiently scheduling distributed training jobs in such heterogeneous environments while ensuring they meet user-specified deadlines remains a critical challenge. While most existing works focus on reducing job completion time in homogeneous clusters, they pay little attention to meeting job deadlines in heterogeneous clusters. To address this issue, we propose Dancer (Deadline-Aware dyNamiC GPU allocation approach for Efficient Resource utilization), a novel framework that dynamically adjusts not only the number but the type of GPUs assigned to each job throughout its training lifecycle. Dancer aims to maximize the number of jobs meeting their deadlines in heterogeneous GPU clusters. It decouples job placement from resource allocation and formulates the scheduling optimization problem for maximizing the number of deadline-meeting jobs as an Integer Linear Programming (ILP) problem. To solve this ILP problem in real-time, we propose an online algorithm with a competitive ratio guarantee, leveraging primal-dual and dynamic programming techniques. Extensive trace-driven simulations based on real-world DL workloads demonstrate that Dancer significantly outperforms state-of-the-art approaches, improving the deadline satisfactory ratio up to 58.9%–74.2%.},
  archive      = {J_TCC},
  author       = {Yuchen Zhang and Long Luo and Gang Sun and Hongfang Yu and Bo Li},
  doi          = {10.1109/TCC.2025.3548604},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {590-604},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Deadline-aware online job scheduling for distributed training in heterogeneous clusters},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure and efficient cloud-based multi-party private set intersection with union protocol. <em>TCC</em>, <em>13</em>(2), 578-589. (<a href='https://doi.org/10.1109/TCC.2025.3548570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure Multi-party Computation (MPC) is a highly active research field, with Private Set Intersection (PSI) being a classic subtopic within it. However, simple intersection computation is insufficient for many real-world scenarios, leading to the development of various PSI variant protocols. In this context, we propose a cloud-based multi-party private set intersection with union protocol, denoted as MPSI-U. This protocol securely computes the intersection of the designated party's set with the union of the sets of all other parties, which can be applied to scenarios such as contact tracing. MPSI-U leverages cloud servers to alleviate the computational burden placed on users, while guaranteeing privacy and security simultaneously for all involved parties with the threshold BGN cryptographic system. Furthermore, a comprehensive formal security analysis of the protocol was conducted under the semi-honest model to prove its resilience against potential security threats. Based on our performance analysis, MPSI-U exhibits favorable characteristics in terms of communication and computation overhead. This enhances the versatility of MPSI-U, rendering it a valuable solution that can be widely applied across various domains and scenarios.},
  archive      = {J_TCC},
  author       = {Qian Liu and Yu Zhan and Baocang Wang},
  doi          = {10.1109/TCC.2025.3548570},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {578-589},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Secure and efficient cloud-based multi-party private set intersection with union protocol},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Communication intensive task offloading with IDMZ for secure industrial edge computing. <em>TCC</em>, <em>13</em>(2), 560-577. (<a href='https://doi.org/10.1109/TCC.2025.3548043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Industrial Internet of Things provides an opportunity for flexible and collaborative manufacturing, but introduces more risk and more communication overhead from the Internet to the industrial field. To avoid attacks from unreliable service providers and requesters, Industrial Demilitarized Zone (IDMZ) is introduced in conjunction with firewalls to provide new communication modes between edge servers and industrial devices. As the number of tasks being offloaded to the edge side increases, optimal task offloading to balance the risk and the communication overhead with limited demilitarized buffer size becomes a challenge. Therefore, this paper establishes a mathematical model for secure task offloading in the Industrial Internet-of-Things considering dense communication with different communication modes. Then, a Parallel Gbest-centric differential evolution (P-G-DE) is designed to solve this task offloading problem with a heuristic-embedded initialization strategy, a modified Gbest-centric differential evolutionary operator and a circular-rotated parallelization scheme. The experimental results verify that the proposed method is capable of providing a high-quality solution with a lower risk and a shorter execution time in seconds, compared to six state-of-the-art evolutionary algorithms.},
  archive      = {J_TCC},
  author       = {Yuanjun Laili and Jiabei Gong and Yusheng Kong and Fei Wang and Lei Ren and Lin Zhang},
  doi          = {10.1109/TCC.2025.3548043},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {560-577},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Communication intensive task offloading with IDMZ for secure industrial edge computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PPSKSQ: Towards efficient and privacy-preserving spatial keyword similarity query in cloud. <em>TCC</em>, <em>13</em>(2), 544-559. (<a href='https://doi.org/10.1109/TCC.2025.3547563'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of cloud computing has led to the widespread use of location-based services, such as spatial keyword queries, which return spatial data points within a given range that have the highest similarity in keyword sets to the user’s. As the volume of spatial data increases, providers commonly outsource data to powerful cloud servers. Because cloud servers are untrustworthy, privacy-preserving keyword query schemes have been proposed. However, existing schemes consider only location queries or exact keyword matching. To address these issues, we propose the Privacy-Preserving Spatial Keyword Similarity Query Scheme (PPSKSQ), designed to search for spatial data points with the highest similarity while protecting the privacy of outsourced data, query requests, and results. First, we design two sub-protocols based on improved symmetric homomorphic encryption (iSHE): iSHE-SC for secure size comparison and iSHE-SIP for secure inner product computation. Then, we encode range information and integrate it with a quadtree to construct a novel index structure. Additionally, we use the Jaccard to measure similarity in conjunction with the iSHE-SC protocol, transforming similarity comparison into a matrix trace operation. Finally, rigorous security analysis and extensive simulation experiments confirm the flexibility, efficiency, and scalability of our scheme.},
  archive      = {J_TCC},
  author       = {Changrui Wang and Lei Wu and Lijuan Xu and Haojie Yuan and Hao Wang and Wenying Zhang and Weizhi Meng},
  doi          = {10.1109/TCC.2025.3547563},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {544-559},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PPSKSQ: Towards efficient and privacy-preserving spatial keyword similarity query in cloud},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RLDR: Reinforcement learning-based fast data recovery in cloud-of-clouds storage systems. <em>TCC</em>, <em>13</em>(2), 526-543. (<a href='https://doi.org/10.1109/TCC.2025.3546528'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-of-clouds storage systems are widely used in online applications, where user data are encrypted, encoded, and stored in multiple clouds. When some cloud nodes fail, the storage systems can reconstruct the lost data and store it in the substitute nodes. It is a challenge to reduce the latency of data recovery to ensure data reliability. In this paper, we adopt a Reinforcement Learning-based Data Recovery (RLDR) approach to reduce the regeneration time. By employing the Monte-Carlo method, our approach can construct the tree-topology-based regeneration process, a.k.a. regeneration tree, to effectively reduce the regeneration time. Through rigorous analysis, we apply the information flow graph to optimize the inter-cloud traffic for a given regeneration tree. To verify the merit of RLDR, We conduct extensive experiments on real-world traces. Experiments demonstrate that RLDR can significantly accelerate the regeneration process. Specifically, RLDR can reduce the regeneration time by up to 92% and increase the throughput by up to twelve-fold, compared to the prior art.},
  archive      = {J_TCC},
  author       = {Jiajie Shen and Bochun Wu and Maoyi Wang and Sai Zou and Laizhong Cui and Wei Ni},
  doi          = {10.1109/TCC.2025.3546528},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {526-543},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {RLDR: Reinforcement learning-based fast data recovery in cloud-of-clouds storage systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic and secure join query protocol for multi-user environment in cloud computing. <em>TCC</em>, <em>13</em>(2), 512-525. (<a href='https://doi.org/10.1109/TCC.2025.3544628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of cloud computing needs to continuously improve and perfect the privacy-preserving techniques for the user’s confidential data. Multi-user join query, as an important method of data sharing, allows multiple legitimate data users to perform join query over the data owner’s encrypted database. However, some existing join query protocols may face some challenges in the practical application, such as practicality, security, and efficiency. In this article, we put forward a dynamic and secure join query protocol in the multi-user environment. Compared with some existing protocols, the proposed protocol has the following advantages. On the one hand, we utilize the dynamic oblivious cross tags structure to realize an efficient join query with forward and backward security. On the other hand, we combine the randomizable distributed key-homomorphic pseudo-random functions with join query to support multiple data users, which can provide resilience against the single user’s key leakage and resist collusion attacks between the cloud server and a subset of data users. We formally define and prove the security of proposed protocol. In addition, we give a detailed analysis of computation and communication overheads to demonstrate the efficiency of proposed protocol. Finally, we carry out some experimental evaluations to further demonstrate the superiority of functionality and efficiency.},
  archive      = {J_TCC},
  author       = {Hongjun Li and Debiao He and Qi Feng and Xiaolin Yang and Qingcai Luo},
  doi          = {10.1109/TCC.2025.3544628},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {512-525},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A dynamic and secure join query protocol for multi-user environment in cloud computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HyperDrive: Direct network telemetry storage via programmable switches. <em>TCC</em>, <em>13</em>(2), 498-511. (<a href='https://doi.org/10.1109/TCC.2025.3543477'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud datacenter operations, telemetry and logs are indispensable, enabling essential services such as network diagnostics, auditing, and knowledge discovery. The escalating scale of data centers, coupled with increased bandwidth and finer-grained telemetry, results in an overwhelming volume of data. This proliferation poses significant storage challenges for telemetry systems. In this article, we introduce HyperDrive, an innovative system designed to efficiently store large volumes of telemetry and logs in data centers using programmable switches. This in-network approach effectively mitigates bandwidth bottlenecks commonly associated with traditional endpoint-based methods. To our knowledge, we are the first to use a programmable switch to directly control storage, bypassing the CPU to achieve the best performance. With merely 21% of a switch’s resources, our HyperDrive implementation showcases remarkable scalability and efficiency. Through rigorous evaluation, it has demonstrated linear scaling capabilities, efficiently managing 12 SSDs on a single server with minimal host overhead. In an eight-server testbed, HyperDrive achieved an impressive throughput of approximately 730 Gbps, underscoring its potential to transform data center telemetry and logging practices.},
  archive      = {J_TCC},
  author       = {Ziyuan Liu and Zhixiong Niu and Ran Shu and Wenxue Cheng and Lihua Yuan and Jacob Nelson and Dan R. K. Ports and Peng Cheng and Yongqiang Xiong},
  doi          = {10.1109/TCC.2025.3543477},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {498-511},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {HyperDrive: Direct network telemetry storage via programmable switches},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PPEC: A privacy-preserving, cost-effective incremental density peak clustering analysis on encrypted outsourced data. <em>TCC</em>, <em>13</em>(2), 485-497. (<a href='https://doi.org/10.1109/TCC.2025.3541749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Call detail records (CDRs) provide valuable insights into user behavior, which are instrumental for telecom companies in optimizing network coverage and service quality. However, while cloud computing facilitates clustering analysis on a vast scale of CDR data, it introduces privacy risks. The challenge lies in striking a balance between efficiency, security, and cost-effectiveness in privacy-preserving algorithms. To tackle this issue, we propose a privacy-preserving and cost-effective incremental density peak clustering scheme. Our approach leverages homomorphic encryption and order-preserving encryption to enable direct computations and clustering on encrypted data. Moreover, it employs reaching definition analysis to optimize the execution flow of static tasks, pinpointing the optimal junctures for transitioning between the two types of encryption to reduce communication overhead. Furthermore, our scheme utilizes a game theory-based verification strategy to ascertain the accuracy of the results. This methodology can be effectively deployed on the Ethereum blockchain via smart contracts. A comprehensive security analysis confirms that our scheme upholds both privacy and data integrity. Experimental evaluations substantiate the clustering accuracy, communication load, and computational efficiency of our scheme, thereby validating its viability in real-world applications.},
  archive      = {J_TCC},
  author       = {Haomiao Yang and ZiKang Ding and Ruiheng Lu and Kunlan Xiang and Hongwei Li and Dakui Wu},
  doi          = {10.1109/TCC.2025.3541749},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {485-497},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PPEC: A privacy-preserving, cost-effective incremental density peak clustering analysis on encrypted outsourced data},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GHPFL: Advancing personalized edge-based learning through optimized bandwidth utilization. <em>TCC</em>, <em>13</em>(2), 473-484. (<a href='https://doi.org/10.1109/TCC.2025.3540023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is increasingly adopted to combine knowledge from clients in training without revealing their private data. In order to improve the performance of different participants, personalized FL has recently been proposed. However, considering the non-independent and identically distributed (non-IID) data and limited bandwidth at clients, the model performance could be compromised. In reality, clients near each other often tend to have similar data distributions. In this work, we train the personalized edge-based model in the client-edge-server FL. While considering the differences in data distribution, we fully utilize the limited bandwidth resources. To make training efficient and accurate at the same time, An intuitive idea is to learn as much useful knowledge as possible from other edges and reduce the accuracy loss incurred by non-IID data. Therefore, we devise Grouping Hierarchical Personalized Federated Learning (GHPFL). In this framework, each edge establishes physical connections with multiple clients, while the server physically connects with edges. It clusters edges into groups and establishes client-edge logical connections for synchronization. This is based on data similarities that the nodes actively identify, as well as the underlying physical topology. We perform a large-scale evaluation to demonstrate GHPFL’s benefits over other schemes.},
  archive      = {J_TCC},
  author       = {Kaiwei Mo and Wei Lin and Jiaxun Lu and Chun Jason Xue and Yunfeng Shao and Hong Xu},
  doi          = {10.1109/TCC.2025.3540023},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {473-484},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {GHPFL: Advancing personalized edge-based learning through optimized bandwidth utilization},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cache allocation in multi-tenant edge computing: An online model-based reinforcement learning approach. <em>TCC</em>, <em>13</em>(2), 459-472. (<a href='https://doi.org/10.1109/TCC.2025.3538158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a Network Operator (NO) that owns Edge Computing (EC) resources, virtualizes them and lets third party Service Providers (SPs) run their services, using the allocated slice of resources. We focus on one specific resource, i.e., cache space, and on the problem of how to allocate it among several SPs in order to minimize the backhaul traffic. Due to confidentiality guarantees, the NO cannot observe the nature of the traffic of SPs, which is encrypted. Allocation decisions are thus challenging, since they must be taken solely based on observed monitoring information. Another challenge is that not all the traffic is cacheable. We propose a data-driven cache allocation strategy, based on Reinforcement Learning (RL). Unlike most RL applications, in which the decision policy is learned offline on a simulator, we assume no previous knowledge is available to build such a simulator. We thus apply RL in an online fashion, i.e., the model and the policy are learned by directly perturbing and monitoring the actual system. Since perturbations generate spurious traffic, we thus need to limit perturbations. This requires learning to be extremely efficient. To this aim, we devise a strategy that learns an approximation of the cost function, while interacting with the system. We then use such an approximation in a Model-Based RL (MB-RL) to speed up convergence. We prove analytically that our strategy brings cache allocation boundedly close to the optimum and stably remains in such an allocation. We show in simulations that such convergence is obtained within few minutes. We also study its fairness, its sensitivity to several scenario characteristics and compare it with a method from the state-of-the-art.},
  archive      = {J_TCC},
  author       = {Ayoub Ben-Ameur and Andrea Araldo and Tijani Chahed and György Dán},
  doi          = {10.1109/TCC.2025.3538158},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {4-6},
  number       = {2},
  pages        = {459-472},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cache allocation in multi-tenant edge computing: An online model-based reinforcement learning approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cost-aware operator migration approach for distributed stream processing system. <em>TCC</em>, <em>13</em>(1), 441-454. (<a href='https://doi.org/10.1109/TCC.2025.3538512'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing is integral to edge computing due to its low-latency attributes. Nevertheless, variability in user group sizes and disparate computing capabilities of edge devices necessitate frequent operator migrations within the stream. Moreover, intricate dependencies among stream operators often obscure the detection of potential bottleneck operators until an identified bottleneck is migrated in the stream. To address this, we propose a Cost-Aware Operator Migration (CAOM) scheme. The CAOM scheme incorporates a bottleneck operator detection mechanism that directly identifies all bottleneck operators based on task running metrics. This approach avoids multiple consecutive operator migrations in complex tasks, reducing the number of task interruptions caused by operator migration. Moreover, CAOM takes into account the temporal variance in operator migration costs. By factoring in the fluctuating data generation rate from data sources at different time intervals, CAOM selects the optimal start time for operator migration to minimize the amount of accumulated data during task interruptions. Finally, we implemented CAOM on Apache Flink and evaluated its performance using the WordCount and Nexmark applications. Our experiments show that CAOM effectively reduces the number of necessary operator migrations in tasks with complex topologies and decreases the latency overhead associated with operator migration compared to state-of-the-art schemes.},
  archive      = {J_TCC},
  author       = {Jiawei Tan and Zhuo Tang and Wentong Cai and Wen Jun Tan and Xiong Xiao and Jiapeng Zhang and Yi Gao and Kenli Li},
  doi          = {10.1109/TCC.2025.3538512},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {441-454},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A cost-aware operator migration approach for distributed stream processing system},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Developments on the “Machine learning as a service for high energy physics” framework and related cloud native solution. <em>TCC</em>, <em>13</em>(1), 429-440. (<a href='https://doi.org/10.1109/TCC.2025.3535793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) techniques have been successfully used in many areas of High Energy Physics (HEP) and will play a significant role in the success of upcoming High-Luminosity Large Hadron Collider (HL-LHC) program at CERN. An unprecedented amount of data at the exascale will be collected by LHC experiments in the next decade, and this effort will require novel approaches to train and use ML models. The work presented in this paper is focused on the developments of a ML as a Service (MLaaS) solution for HEP, aiming to provide a cloud service that allows HEP users to run ML pipelines via HTTPs calls. These pipelines are executed by using MLaaS4HEP framework, which allows reading data, processing data, and training ML models directly using ROOT files of arbitrary size from local or distributed data sources. In particular, new features implemented on the framework will be presented as well as updates on the architecture of an existing prototype of the MLaaS4HEP cloud service will be provided. This solution includes two OAuth2 proxy servers as authentication/authorization layer, a MLaaS4HEP server, an XRootD proxy server for enabling access to remote ROOT data, and the TensorFlow as a Service (TFaaS) service in charge of the inference phase.},
  archive      = {J_TCC},
  author       = {Luca Giommi and Daniele Spiga and Mattia Paladino and Valentin Kuznetsov and Daniele Bonacorsi},
  doi          = {10.1109/TCC.2025.3535793},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {429-440},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Developments on the “Machine learning as a service for high energy physics” framework and related cloud native solution},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint computation offloading and resource allocation in mobile-edge cloud computing: A two-layer game approach. <em>TCC</em>, <em>13</em>(1), 411-428. (<a href='https://doi.org/10.1109/TCC.2025.3538090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile-Edge Cloud Computing (MECC) plays a crucial role in balancing low-latency services at the edge with the computational capabilities of cloud data centers (DCs). However, many existing studies focus on single-provider settings or limit their analysis to interactions between mobile devices (MDs) and edge servers (ESs), often overlooking the competition that occurs among ESs from different providers. This article introduces an innovative two-layer game framework that captures independent self-interested competition among MDs and ESs, providing a more accurate reflection of multi-vendor environments. Additionally, the framework explores the influence of cloud-edge collaboration on ES competition, offering new insights into these dynamics. The proposed model extends previous research by developing algorithms that optimize task offloading and resource allocation strategies for both MDs and ESs, ensuring the convergence to Nash equilibrium in both layers. Simulation results demonstrate the potential of the framework to improve resource efficiency and system responsiveness in multi-provider MECC environments.},
  archive      = {J_TCC},
  author       = {Zhenli He and Ying Guo and Xiaolong Zhai and Mingxiong Zhao and Wei Zhou and Keqin Li},
  doi          = {10.1109/TCC.2025.3538090},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {411-428},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint computation offloading and resource allocation in mobile-edge cloud computing: A two-layer game approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Verifiable encrypted image retrieval with reversible data hiding in cloud environment. <em>TCC</em>, <em>13</em>(1), 397-410. (<a href='https://doi.org/10.1109/TCC.2025.3535937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With growing numbers of users outsourcing images to cloud servers, privacy-preserving content-based image retrieval (CBIR) is widely studied. However, existing privacy-preserving CBIR schemes have limitations in terms of low search accuracy and efficiency due to the use of unreasonable index structures or retrieval methods. Meanwhile, existing result verification schemes do not consider the privacy of verification information. To address these problems, we propose a new secure verification encrypted image retrieval scheme. Specifically, we design an additional homomorphic bitmap index structure by using a pre-trained CNN model with modified fully connected layers to extract image feature vectors and organize them into a bitmap. It makes the extracted features more representative and robust compared to manually designed features, and only performs vector addition during the search process, improving search efficiency and accuracy. Moreover, we design a reversible data hiding (RDH) technique with color images, which embeds the verification information into the least significant bits of the encrypted image pixels to improve the security of the verification information. Finally, we analyze the security of our scheme against chosen-plaintext attacks (CPA) in the security analysis and demonstrate the effectiveness of our scheme on two real-world datasets (i.e., COCO and Flickr-25 k) through experiments.},
  archive      = {J_TCC},
  author       = {Mingyue Li and Yuting Zhu and Ruizhong Du and Chunfu Jia},
  doi          = {10.1109/TCC.2025.3535937},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {397-410},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Verifiable encrypted image retrieval with reversible data hiding in cloud environment},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PiCoP: Service mesh for sharing microservices in multiple environments using protocol-independent context propagation. <em>TCC</em>, <em>13</em>(1), 383-396. (<a href='https://doi.org/10.1109/TCC.2025.3531954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous integration and continuous delivery require many production-like environments in a cluster for testing, staging, debugging, and previewing. In applications built on microservice architecture, sharing common microservices in multiple environments is an effective way to reduce resource consumption. Previous methods extend application layer protocols like HTTP and gRPC to propagate contexts including environment identifiers and to route requests. However, microservices also use other protocols such as MySQL, Redis, Memcached, and AMQP, and extending each protocol requires lots of effort to implement the extensions. This paper proposes PiCoP, a framework to share microservices in multiple environments by propagating contexts and routing requests independently of application layer protocols. PiCoP provides a protocol that propagates contexts by appending them to the front of each TCP byte stream and constructs a service mesh that uses the protocol to route requests. We design the protocol to make it easy to instrument into a system. We demonstrate that PiCoP can reduce resource usage and that it applies to a real-world application, enabling the sharing of microservices in multiple environments using any application layer protocol.},
  archive      = {J_TCC},
  author       = {Hiroya Onoe and Daisuke Kotani and Yasuo Okabe},
  doi          = {10.1109/TCC.2025.3531954},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {383-396},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PiCoP: Service mesh for sharing microservices in multiple environments using protocol-independent context propagation},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint adaptive aggregation and resource allocation for hierarchical federated learning systems based on edge-cloud collaboration. <em>TCC</em>, <em>13</em>(1), 369-382. (<a href='https://doi.org/10.1109/TCC.2025.3530681'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical federated learning shows excellent potential for communication-computation trade-offs and reliable data privacy protection by introducing edge-cloud collaboration. Considering non-independent and identically distributed data distribution among devices and edges, this article aims to minimize the final loss function under time and energy budget constraints by optimizing the aggregation frequency and resource allocation jointly. Although there is no closed-form expression relating the final loss function to optimization variables, we divide the hierarchical federated learning process into multiple cloud intervals and analyze the convergence bound for each cloud interval. Then, we transform the initial problem into one that can be adaptively optimized in each cloud interval. We propose an adaptive hierarchical federated learning process, termed as AHFLP, where we determine edge and cloud aggregation frequency for each cloud interval based on estimated parameters, and then the CPU frequency of devices and wireless channel bandwidth allocation can be optimized in each edge. Simulations are conducted under different models, datasets and data distributions, and the results demonstrate the superiority of our proposed AHFLP compared with existing schemes.},
  archive      = {J_TCC},
  author       = {Yi Su and Wenhao Fan and Qingcheng Meng and Penghui Chen and Yuan'an Liu},
  doi          = {10.1109/TCC.2025.3530681},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {369-382},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint adaptive aggregation and resource allocation for hierarchical federated learning systems based on edge-cloud collaboration},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid serverless platform for smart deployment of service function chains. <em>TCC</em>, <em>13</em>(1), 351-368. (<a href='https://doi.org/10.1109/TCC.2025.3528573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud Data Centres deal with dynamic changes all the time. Networks in particular, need to adapt their configurations to changing workloads. Given these expectations, Network Function Virtualization (NFV) using Software Defined Networks (SDNs) has realized the aspect of programmability in networks. NFVs allow network services to be programmed as software entities that can be deployed on commodity clusters in the Cloud. Being software, they inherently carry the ability to be customized to specific tenants’ requirements and thus support multi-tenant variations with ease. However, the ability to exploit scaling in alignment with changing demands with minimal loss of service, and improving resource usage efficiency still remains a challenge. Several recent works in literature have proposed platforms to realize Virtual Network functions (VNFs) on the Cloud using service offerings such as Infrastructure as a Service (IaaS) and serverless computing. These approaches are limited by deployment difficulties (configuration and sizing), adaptability to performance requirements (elastic scaling), and changing workload dynamics (scaling and customization). In the current work, we propose a Hybrid Serverless Platform (HSP) to address these identified lacunae. The HSP is implemented using a combination of persistent IaaS, and FaaS components. The IaaS components handle the steady state load, whereas the FaaS components activate during the dynamic change associated with scaling to minimize service loss. The HSP controller takes provisioning decisions based on Quality of Service (QoS) rules and flow statistics using an auto recommender, alleviating users of sizing decisions for function deployment. HSP controller design exploits data locality in SFC realization, reducing data-transfer times between VNFs. It also enables the usage of application characteristics to offer higher control over SFC deployment. A proof-of-concept realization of HSP is presented in the paper and is evaluated for a representative Service Function Chain (SFC) for a dynamic workload, which shows minimal loss in flowlet service, up to 35% resource savings as compared to a pure IaaS deployment and up to 55% lower end-to-end times as compared to a baseline FaaS implementation.},
  archive      = {J_TCC},
  author       = {Sheshadri K R and J. Lakshmi},
  doi          = {10.1109/TCC.2025.3528573},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {351-368},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Hybrid serverless platform for smart deployment of service function chains},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Energy-aware offloading of containerized tasks in cloud native V2X networks. <em>TCC</em>, <em>13</em>(1), 336-350. (<a href='https://doi.org/10.1109/TCC.2025.3529245'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud-native environments, executing vehicle-to-everything (V2X) tasks in edge nodes close to users significantly reduces service end-to-end latency. Containerization further reduces resource and time consumption, and, subsequently, application latency. Since edge nodes are typically resource and energy-constrained, optimizing offloading decisions and managing edge energy consumption is crucial. However, the offloading of containerized tasks has not been thoroughly explored from a practical implementation perspective. This paper proposes an optimization framework for energy-aware offloading of V2X tasks implemented as Kubernetes pods. A weighted utility function is derived based on cumulative pod response time, and an edge-to-cloud offloading decision algorithm (ECODA) is proposed. The system's energy cost model is derived, and a closed-loop repeated reward-based mechanism for CPU adjustment is presented. An energy-aware (EA)-ECODA is proposed to solve the offloading optimization problem while adjusting CPU usage according to energy considerations. Simulations show that ECODA and EA-ECODA outperform first-in, first-served (FIFS) and EA-FIFS in terms of utility, average pod response time, and resource usage, with low computational complexity. Additionally, a real testbed evaluation of a vulnerable road user application demonstrates that ECODA outperforms Kubernetes vertical scaling in terms of service-level delay. Moreover, EA-ECODA significantly improves energy usage utility.},
  archive      = {J_TCC},
  author       = {Estela Carmona-Cejudo and Francesco Iadanza},
  doi          = {10.1109/TCC.2025.3529245},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {336-350},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Energy-aware offloading of containerized tasks in cloud native V2X networks},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CARL: Cost-optimized online container placement on VMs using adversarial reinforcement learning. <em>TCC</em>, <em>13</em>(1), 321-335. (<a href='https://doi.org/10.1109/TCC.2025.3528446'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Containerization has become popular for the deployment of applications on public clouds. Large enterprises may host 100 s of applications on 1000 s containers that are placed onto Virtual Machines (VMs). Such placement decisions happen continuously as applications are updated by DevOps pipelines that deploy the containers. Managing the placement of container resource requests onto the available capacities of VMs needs to be cost-efficient. This is well-studied, and usually modelled as a multi-dimensional Vector Bin-packing Problem (VBP). Many heuristics, and recently machine learning approaches, have been developed to solve this NP-hard problem for real-time decisions. We propose CARL, a novel approach to solve VBP through Adversarial Reinforcement Learning (RL) for cost minimization. It mimics the placement behavior of an offline semi-optimal VBP solver (teacher), while automatically learning a reward function for reducing the VM costs which out-performs the teacher. It requires limited historical container workload traces to train, and is resilient to changes in the workload distribution during inferencing. We extensively evaluate CARL on workloads derived from realistic traces from Google and Alibaba for the placement of 5 k–10 k container requests onto 2 k–8 k VMs, and compare it with classic heuristics and state-of-the-art RL methods. (1) CARL is fast, e.g., making placement decisions at $\approx 1900$ requests/sec onto 8,900 candidate VMs. (2) It is efficient, achieving $\approx 16\%$ lower VM costs than classic and contemporary RL methods. (3) It is robust to changes in the workload, offering competitive results even when the resource needs or inter-arrival time of the container requests skew from the training workload.},
  archive      = {J_TCC},
  author       = {Prathamesh Saraf Vinayak and Saswat Subhajyoti Mallick and Lakshmi Jagarlamudi and Anirban Chakraborty and Yogesh Simmhan},
  doi          = {10.1109/TCC.2025.3528446},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {321-335},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {CARL: Cost-optimized online container placement on VMs using adversarial reinforcement learning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ByteTuning: Watermark tuning for RoCEv2. <em>TCC</em>, <em>13</em>(1), 303-320. (<a href='https://doi.org/10.1109/TCC.2025.3525496'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RDMA over Converged Ethernet v2 (RoCEv2) is one of the most popular high-speed datacenter networking solutions. Watermark is the general term for various trigger and release thresholds of RoCEv2 flow control protocols, and its reasonable configuration is an important factor affecting RoCEv2 performance. In this paper, we propose ByteTuning, a centralized watermark tuning system for RoCEv2. First, three real cases of network performance degradation caused by non-optimal or improper watermark configuration are reported, and the network performance results of different watermark configurations in three typical scenarios are traversed, indicating the necessity of watermark tuning. Then, based on the RDMA Fluid model, the influence of watermark on the RoCEv2 performance is modeled and evaluated. Next, the design of the ByteTuning is introduced, which includes three mechanisms. They are 1) using simulated annealing algorithm to make the real-time watermark converge to the near-optimal configuration, 2) using network telemetry to optimize the feedback overhead, 3) compressing the search space to improve the tuning efficiency. Finally, We validate the performance of ByteTuning in multiple real datacenter networking environments, and the results show that ByteTuning outperforms existing solutions.},
  archive      = {J_TCC},
  author       = {Lizhuang Tan and Zhuo Jiang and Kefei Liu and Haoran Wei and Pengfei Huo and Huiling Shi and Wei Zhang and Wei Su},
  doi          = {10.1109/TCC.2025.3525496},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {303-320},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {ByteTuning: Watermark tuning for RoCEv2},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-edge collaborative service architecture with large-tiny models based on deep reinforcement learning. <em>TCC</em>, <em>13</em>(1), 288-302. (<a href='https://doi.org/10.1109/TCC.2024.3525076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offshore drilling platforms (ODPs) are critical infrastructure for exploring and developing marine oil and gas resources. As these platforms’ capabilities expand, deploying intelligent surveillance services to ensure safe production has become increasingly important. However, the unique geographical locations and harsh environmental conditions of ODPs pose significant challenges for processing large volumes of video data, complicating the implementation of efficient surveillance systems. This study proposes a Cloud-Edge Large-Tiny Model Collaborative (CELTC) architecture grounded in deep reinforcement learning to optimize the processing and decision-making of surveillance data in offshore drilling platform scenarios. CELTC architecture leverages edge-cloud computing, deploying complex, high-precision large models on cloud servers and lightweight tiny models on edge devices. This dual deployment strategy capitalizes on tiny models’ rapid response and large cloud models’ high-precision capabilities. Additionally, the architecture integrates a deep reinforcement learning algorithm designed to optimize the scheduling and offloading of computational tasks between large and tiny models in the cloud-edge environment. The efficacy of the proposed architecture is validated using real-world surveillance data from ODPs through simulations and comparative experiments.},
  archive      = {J_TCC},
  author       = {Xiaofeng Ji and Faming Gong and Nuanlai Wang and Junjie Xu and Xing Yan},
  doi          = {10.1109/TCC.2024.3525076},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {288-302},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cloud-edge collaborative service architecture with large-tiny models based on deep reinforcement learning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient online computing offloading for budget- constrained cloud-edge collaborative video streaming systems. <em>TCC</em>, <em>13</em>(1), 273-287. (<a href='https://doi.org/10.1109/TCC.2024.3524310'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-Edge Collaborative Architecture (CEA) is a prominent framework that provides low-latency and energy-efficient solutions for video stream processing. In Cloud-Edge Collaborative Video Streaming Systems (CEAVS), efficient online offloading strategies for video tasks are crucial for enhancing user experience. However, most existing works overlook budget constraints, which limits their applicability in real-world scenarios constrained by finite resources. Moreover, they fail to adequately address the heterogeneity of video task redundancies, leading to suboptimal utilization of CEAVS's limited resources. To bridge these gaps, we propose an Efficient Online Computing framework for CEAVS (EOCA) that jointly optimizes accuracy, energy consumption, and latency performance through adaptive online offloading and redundancy compression, without requiring future task information. Technically, we formulate computing offloading and adaptive compression under budget constraints as a stochastic optimization problem that maximizes system satisfaction, defined as a weighted combination of accuracy, latency, and energy performance. We employ Lyapunov optimization to decouple the long-term budget constraint. We prove that the decoupled problem is a generalized ordinal potential game and propose algorithms based on generalized Benders decomposition (GBD) and the best response to obtain Nash equilibrium strategies for computing offloading and task compression. Finally, we analyze EOCA's performance bound, convergence rate, and worst-case performance guarantees. Evaluations demonstrate that EOCA effectively improves satisfaction while effectively balancing satisfaction and computational overhead.},
  archive      = {J_TCC},
  author       = {Shijing Yuan and Yuxin Liu and Song Guo and Jie Li and Hongyang Chen and Chentao Wu and Yang Yang},
  doi          = {10.1109/TCC.2024.3524310},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {273-287},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient online computing offloading for budget- constrained cloud-edge collaborative video streaming systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differentially private and truthful reverse auction with dynamic resource provisioning for VNFI procurement in NFV markets. <em>TCC</em>, <em>13</em>(1), 259-272. (<a href='https://doi.org/10.1109/TCC.2024.3522963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of network function virtualization (NFV), many users resort to network service provisioning through virtual network function instances (VNFIs) run on the standard physical server in clouds. Following this trend, NFV markets are emerging, which allow a user to procure VNFIs from cloud service providers (CSPs). In such procurement process, it is a significant challenge to ensure differential privacy and truthfulness while explicitly considering dynamic resource provisioning, location sensitiveness and budget of each VNFI. As such, we design a differentially private and truthful reverse auction with dynamic resource provisioning (PTRA-DRP) to resolve the VNFI procurement (VNFIP) problem. To allow dynamic resource provisioning, PTRA-DRP enables CSPs to submit a set of bids and accept as many as possible, and decides the provisioning VNFIs based on the auction outcomes. To be specific, we first devise a greedy heuristic approach to select the set of the winning bids in a differentially privacy-preserving manner. Next, we design a pricing strategy to compute the charges of CSPs, aiming to guarantee truthfulness. Strict theoretical analysis proves that PTRA-DRP can ensure differential privacy, truthfulness, individual rationality, computational efficiency and approximate social cost minimization. Extensive simulations also demonstrate the effectiveness and efficiency of PTRA-DRP.},
  archive      = {J_TCC},
  author       = {Xueyi Wang and Xingwei Wang and Zhitong Wang and Rongfei Zeng and Ruiyun Yu and Qiang He and Min Huang},
  doi          = {10.1109/TCC.2024.3522963},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {259-272},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Differentially private and truthful reverse auction with dynamic resource provisioning for VNFI procurement in NFV markets},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SROdcn: Scalable and reconfigurable optical DCN architecture for high-performance computing. <em>TCC</em>, <em>13</em>(1), 245-258. (<a href='https://doi.org/10.1109/TCC.2024.3523433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Center Network (DCN) flexibility is critical for providing adaptive and dynamic bandwidth while optimizing network resources to manage variable traffic patterns generated by heterogeneous applications. To provide flexible bandwidth, this work proposes a machine learning approach with a new Scalable and Reconfigurable Optical DCN (SROdcn) architecture that maintains dynamic and non-uniform network traffic according to the scale of the high-performance optical interconnected DCN. Our main device is the Fiber Optical Switch (FOS), which offers competitive wavelength resolution. We propose a new top-of-rack (ToR) switch that utilizes Wavelength Selective Switches (WSS) to investigate Software-Defined Networking (SDN) with machine learning-enabled flow prediction for reconfigurable optical Data Center Networks (DCNs). Our architecture provides highly scalable and flexible bandwidth allocation. Results from Mininet experimental simulations demonstrate that under the management of an SDN controller, machine learning traffic flow prediction and graph connectivity allow each optical bandwidth to be automatically reconfigured according to variable traffic patterns. The average server-to-server packet delay performance of the reconfigurable SROdcn improves by 42.33% compared to inflexible interconnects. Furthermore, the network performance of flexible SROdcn servers shows up to a 49.67% latency improvement over the Passive Optical Data Center Architecture (PODCA), a 16.87% latency improvement over the optical OPSquare DCN, and up to a 71.13% latency improvement over the fat-tree network. Additionally, our optimized Unsupervised Machine Learning (ML-UnS) method for SROdcn outperforms Supervised Machine Learning (ML-S) and Deep Learning (DL).},
  archive      = {J_TCC},
  author       = {Kassahun Geresu and Huaxi Gu and Xiaoshan Yu and Meaad Fadhel and Hui Tian and Wenting Wei},
  doi          = {10.1109/TCC.2024.3523433},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {245-258},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SROdcn: Scalable and reconfigurable optical DCN architecture for high-performance computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the availability and security of attestation scheme for multiparty-involved DLaaS: A circular approach. <em>TCC</em>, <em>13</em>(1), 227-244. (<a href='https://doi.org/10.1109/TCC.2024.3522993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a remote attestation approach based on multiple verifiers named CARE. CARE aims to enhance the practicality and efficiency of remote attestation while addressing trust issues within environments involving multiple stakeholders. Specifically, CARE adopts the concept of swarm verification, and employs a circular collaboration model with multiple verifiers to collect and validate evidence, thereby resolving trust issues and enhancing verification efficiency. Moreover, CARE introduces a meticulously designed filtering mechanism to address the issue of false positives in verification outcomes non-invasively. CARE utilizes a multiway tree structure to construct the baseline value library, which enhances the flexibility and fine-grained management capability of the system. Security analysis indicates that CARE can effectively resist collusion attacks. Further, detailed simulation experiments have validated its capability to convincingly attest to the trustworthiness of the dynamically constructed environment. Notably, CARE is also suitable for the remote attestation of large-scale virtual machines, achieving an efficiency 9 times greater than the classical practice approach. To the best of our knowledge, CARE is the first practical solution to address inaccuracies in remote attestation results caused by the activation of Integrity Measurement Architecture (IMA) at the application layer.},
  archive      = {J_TCC},
  author       = {Miaomiao Yang and Guosheng Huang and Honghai Chen and Yongyi Liao and Qixu Wang and Xingshu Chen},
  doi          = {10.1109/TCC.2024.3522993},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {227-244},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Enhancing the availability and security of attestation scheme for multiparty-involved DLaaS: A circular approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StreamSys: A lightweight executable delivery system for edge computing. <em>TCC</em>, <em>13</em>(1), 213-226. (<a href='https://doi.org/10.1109/TCC.2024.3521978'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing brings several challenges when it comes to data movement. First, moving large data from edge devices to the server is likely to waste bandwidth. Second, complex data patterns (e.g., traffic cameras) on devices require flexible handling. An ideal approach is to move code to data instead. However, since only a small portion of code is required, moving the executable as well as their libraries to the devices can be an overkill. While loading code on demand from remote such as NFS can be a stopgap, but on the other hand leads to low efficiency for irregular access patterns. This article presents StreamSys, a lightweight executable delivery system that loads code on demand by redirecting the local disk IO to the server through optimized network IO. We employ a Markov-based prefetch mechanism on the server side. It learns the access pattern of code and predicts the block sequence for the client to reduce the network round trip. Meanwhile, server-side StreamSys asynchronously prereads the block sequence from the disk to conceal disk IO latency beforehand. Evaluation shows that the latency of StreamSys is up to 71.4% lower than the native Linux file system based on SD card and up to 62% lower than NFS in wired environments.},
  archive      = {J_TCC},
  author       = {Jun Lu and Zhenya Ma and Yinggang Gao and Sheng Yue and Ju Ren and Yaoxue Zhang},
  doi          = {10.1109/TCC.2024.3521978},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {213-226},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {StreamSys: A lightweight executable delivery system for edge computing},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding serverless inference in mobile-edge networks: A benchmark approach. <em>TCC</em>, <em>13</em>(1), 198-212. (<a href='https://doi.org/10.1109/TCC.2024.3521657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although the emerging serverless paradigm has the potential to become a dominant way of deploying cloud-service tasks across millions of mobile and IoT devices, the overhead characteristics of executing these tasks on such a volume of mobile devices remain largely unclear. To address this issue, this paper conducts a deep analysis based on the OpenFaaS platform—a popular open-source serverless platform for mobile edge environments—to investigate the overhead of performing deep learning inference tasks on mobile devices. To thoroughly evaluate the inference overhead, we develop a performance benchmark, named ESBench, whereby a set of comprehensive experiments are conducted with respect to a bunch of simulated mobile devices associated with an edge cluster. Our investigation reveals that the performance of deep learning inference tasks is significantly influenced by the model size and resource contention in mobile devices, leading to up to $3\times$ degradation in performance. Moreover, we observe that the network environment can negatively impact the performance of mobile inference, increasing the CPU overhead under poor network conditions. Based on our findings, we further propose some recommendations for designing efficient serverless platforms and resource management strategies as well as for deploying serverless computing in the mobile edge environment.},
  archive      = {J_TCC},
  author       = {Junhong Chen and Yanying Lin and Shijie Peng and Shuaipeng Wu and Kenneth Kent and Hao Dai and Kejiang Ye and Yang Wang},
  doi          = {10.1109/TCC.2024.3521657},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {198-212},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Understanding serverless inference in mobile-edge networks: A benchmark approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing sustainability in data centers: Evaluation of hybrid Air/Liquid cooling schemes for IT payload using sea water. <em>TCC</em>, <em>13</em>(1), 184-197. (<a href='https://doi.org/10.1109/TCC.2024.3521666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth in cloud computing, Big Data, AI and high-performance computing (HPC) necessitate the deployment of additional data centers (DC’s) with high energy demands. The unprecedented increase in the Thermal Design Power (TDP) of the computing chips will require innovative cooling techniques. Furthermore, DC’s are increasingly limited in their ability to add powerful GPU servers by power capacity constraints. As cooling energy use accounts for up to 40% of DC energy consumption, creative cooling solutions are urgently needed to allow deployment of additional servers, enhance sustainability and increase energy efficiency of DC’s. The information in this study is provided from Start Campus’ Sines facility supported by Alfa Laval for the heat exchanger and CO2 emission calculations. The study evaluates the performance and sustainability impact of various data center cooling strategies including an air-only deployment and a subsequent hybrid air/water cooling solution all utilizing sea water as the cooling source. We evaluate scenarios from 3 MW to 15+1 MW of IT load in 3 MW increments which correspond to the size of heat exchangers used in the Start Campus’ modular system design. This study also evaluates the CO2 emissions compared to a conventional chiller system for all the presented scenarios. Results indicate that the effective use of the sea water cooled system combined with liquid cooled systems improve the efficiency of the DC, plays a role in decreasing the CO2 emissions and supports in achieving sustainability goals.},
  archive      = {J_TCC},
  author       = {Imran Latif and Muhammad Mubashar Ashraf and Umaima Haider and Gemma Reeves and Alexandrina Untaroiu and Fábio Coelho and Denis Browne},
  doi          = {10.1109/TCC.2024.3521666},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {184-197},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Advancing sustainability in data centers: Evaluation of hybrid Air/Liquid cooling schemes for IT payload using sea water},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI applications resource allocation in computing continuum: A stackelberg game approach. <em>TCC</em>, <em>13</em>(1), 166-183. (<a href='https://doi.org/10.1109/TCC.2024.3521213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth, development, and commercialization of artificial intelligence-based technologies such as self-driving cars, augmented-reality viewers, chatbots, and virtual assistants are driving the need for increased computing power. Most of these applications rely on Deep Neural Networks (DNNs), which demand substantial computing capacity to meet user demands. However, this capacity cannot be fully provided by users’ local devices due to their limited processing power, nor by cloud data centers due to high transmission latency from long distances. Edge cloud computing addresses this issue by processing user requests through 5G, which reduces transmission latency from local devices to computing resources and allows the offloading of some computations to cloud back-ends. This paper introduces a model for a Mobile Edge Cloud system designed for an application based on a DNN. The interaction among multiple mobile users and the edge platform is formulated as a one-leader multi-follower Stackelberg game, resulting in a challenging non-convex mixed integer nonlinear programming (MINLP) problem. To tackle this, we propose a heuristic approach based on Karush-Kuhn-Tucker conditions, which solves the MINLP problem significantly faster than the commercial state-of-the-art solvers (up to 50,000 times). Furthermore, we present an algorithm to estimate optimal platform profit when sensitive user parameters are unknown. Comparing this with the full-knowledge scenario, we observe a profit loss of approximately 1%. Lastly, we analyze the advantages for an edge provider to engage in a Stackelberg game rather than setting a fixed price for its users, showing potential profit increases ranging from 16% to 66%.},
  archive      = {J_TCC},
  author       = {Roberto Sala and Hamta Sedghani and Mauro Passacantando and Giacomo Verticale and Danilo Ardagna},
  doi          = {10.1109/TCC.2024.3521213},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {166-183},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {AI applications resource allocation in computing continuum: A stackelberg game approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage learning approach for semantic-aware task scheduling in container-based clouds. <em>TCC</em>, <em>13</em>(1), 148-165. (<a href='https://doi.org/10.1109/TCC.2024.3520101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Container-based task scheduling is critical for ensuring a reliable, flexible and cost-effective cloud computing mode. However, in different business cloud systems, state-of-the-art scheduling models are not as effective as those in the simulated world due to the sparsity issues associated with sample sizes and features. Herein, we propose a novel containerized task scheduling framework (SA2CTS) based on reinforcement learning (RL) that incorporates cross-modal contrastive learning (CL) loss. This framework optimizes the scheduler's understanding of the container-based cloud state in RL by adding a pretraining stage, promoting accurate scheduling action inference. Specifically, we design a two-stage learning pipeline. The initial stage involves pretraining the model on a large collection of aligned image-text pairs to extract fine-grained scheduling affinity features, and the high-level semantic representations of scheduling tasks are learned in the multimodal space. In the second stage, we fine-tune the pretrained model with multisource cluster feedback, i.e., build a mapping from state representations to scheduling actions through the RL paradigm, achieving task-oriented and semantic-aware scheduling. The experimental results obtained on three large-scale production cluster datasets substantiate that the proposed SA2CTS method can provide average convergence efficiency and resource utilization improvements of 17.57% and 10.42%, respectively, over the state-of-the-art RL scheduling methods.},
  archive      = {J_TCC},
  author       = {Lilu Zhu and Kai Huang and Yanfeng Hu and Yang Wang},
  doi          = {10.1109/TCC.2024.3520101},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {148-165},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Two-stage learning approach for semantic-aware task scheduling in container-based clouds},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SST-LOF: Container anomaly detection method based on singular spectrum transformation and local outlier factor. <em>TCC</em>, <em>13</em>(1), 130-147. (<a href='https://doi.org/10.1109/TCC.2024.3514297'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the use of container cloud platforms has experienced rapid growth. However, because containers are operating-system-level virtualization, their isolation is far less than that of virtual machines, posing considerable challenges for multi-tenant container cloud platforms. To address the issues associated with current container anomaly detection algorithms, such as the difficulty in mining periodic features and the high rate of false positives due to noisy data, we propose an anomaly detection method named SST-LOF, based on singular spectrum transformation and the local outlier factor. Our method enhances the traditional Singular Spectrum Transformation (SST) algorithm to meet the needs of streaming unsupervised detection. Furthermore, our method improves the calculation mode of the anomaly score of the Local Outlier Factor algorithm (LOF) and reduces false positives of noisy data with dynamic sliding windows. Additionally, we have designed and implemented a container cloud anomaly detection system that can perform real-time, unsupervised, streaming anomaly detection on containers quickly and accurately. The experimental results demonstrate the effectiveness and efficiency of our method in detecting anomalies in containers in both simulated and real cloud environments.},
  archive      = {J_TCC},
  author       = {Shilei Bu and Minpeng Jin and Jie Wang and Yulai Xie and Liangkang Zhang},
  doi          = {10.1109/TCC.2024.3514297},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {130-147},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {SST-LOF: Container anomaly detection method based on singular spectrum transformation and local outlier factor},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IBNR-RD: Intra-block neighborhood relationship-based resemblance detection for high-performance multi-node post-deduplication. <em>TCC</em>, <em>13</em>(1), 118-129. (<a href='https://doi.org/10.1109/TCC.2024.3514784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-deduplication in traditional cloud environments primarily focuses on single-node, where delta compression is performed on the same deduplication node located on server side. However, with data explosion, the multi-node post-deduplication, also called global deduplication, has become a hot issue in research communities, which aims to simultaneously execute delta compression on data distributed across all nodes. Simply setting up single-node deduplication systems on multi-node environments would significantly affect storage utilization and incur secondary overhead from file migration. Nevertheless, existing global deduplication solutions suffer from lower data compression ratios and high computational overhead due to their resemblance detection's inherent limitations and overly coarse granularities. Similar blocks typically have high correlations between sub-blocks; inspired by this observation, we propose IBNR (Intra-Block Neighborhood Relationship-Based Resemblance Detection for High-Performance Multi-Node Post-Deduplication), which introduces a novel resemblance detection based on relationships between sub-blocks and determines the ownership of blocks in entry stage to achieve efficient global deduplication. Furthermore, the by-products of IBNR have shown powerful scalability by replacing internal resemblance detection scheme with existing solutions on practical workloads. Experimental results indicate that IBNR outperforms state-of-the-art solutions, achieving an average 1.99× data reduction ratio and varying degrees of improvement across other key metrics.},
  archive      = {J_TCC},
  author       = {Dewen Zeng and Wenlong Tian and Tingting He and Ruixuan Li and Xuming Ye and Zhiyong Xu},
  doi          = {10.1109/TCC.2024.3514784},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {118-129},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {IBNR-RD: Intra-block neighborhood relationship-based resemblance detection for high-performance multi-node post-deduplication},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient dynamic resource management for spatial multitasking GPUs. <em>TCC</em>, <em>13</em>(1), 99-117. (<a href='https://doi.org/10.1109/TCC.2024.3511548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of microservice architecture enables complex cloud applications to be realized via a set of individually isolated components, increasing their flexibility and performance. As these applications require massive computing resources, graphics processing units (GPUs) are being widely used as high-speed parallel computing devices to meet the stringent demands. Although current GPUs allow application components to be executed concurrently via spatial multitasking, they face several challenges. The first challenge is allocating the computing resources to components dynamically to maximize efficiency. The second challenge is avoiding performance degradation caused by the data transfer overhead between the components. To address these challenges, we propose an efficient GPU resource management technique that dynamically allocates GPU resources to application components. The proposed method allocates resources based on component workloads and uses online performance monitoring to guarantee the application's performance. We also propose a GPU memory manager to reduce the data transfer overhead between components via shared memory. Our evaluation results indicate that the proposed dynamic resource allocation method improves application throughput by up to 134.12% compared to the state-of-the-art spatial multitasking techniques. We also show that using a shared memory results in 6x throughput improvement compared to the baseline User Datagram Protocol (UDP)-based technique.},
  archive      = {J_TCC},
  author       = {Hoda Sedighi and Daniel Gehberger and Amin Ebrahimzadeh and Fetahi Wuhib and Roch H. Glitho},
  doi          = {10.1109/TCC.2024.3511548},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {99-117},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Efficient dynamic resource management for spatial multitasking GPUs},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optical self-adjusting data center networks in the scalable matching model. <em>TCC</em>, <em>13</em>(1), 87-98. (<a href='https://doi.org/10.1109/TCC.2024.3510916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-Adjusting Networks (SAN) optimize their physical topology toward the demand in an online manner. Their application in data center networks is motivated by emerging hardware technologies, such as 3D MEMS Optical Circuit Switches (OCS). The Matching Model (MM) has been introduced to study the hybrid architecture of such networks. It abstracts from the electrical switches and focuses on the added (reconfigurable) optical ones. MM defines any SAN topology as a union of matchings over a set of top-of-rack (ToR) nodes, and assumes that rearranging the edges of a single matching comes at a fixed cost. In this work, we propose and study the Scalable Matching Model (SMM), a generalization of the MM, and present OpticNet, a framework that maps a set of ToRs to a set of OCSs to form a SAN topology. We prove that OpticNet uses the minimum number of switches to realize any bounded-degree topology and allows existing SAN algorithms to run on top of it, while preserving amortized performance guarantees. Our experimental results based on real workloads show that OpticNet is a flexible and efficient framework for the implementation and evaluation of SAN algorithms in reconfigurable data center environments.},
  archive      = {J_TCC},
  author       = {Caio Alves Caldeira and Otávio Augusto de Oliveira Souza and Olga Goussevskaia and Stefan Schmid},
  doi          = {10.1109/TCC.2024.3510916},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {87-98},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optical self-adjusting data center networks in the scalable matching model},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient delegatable order-revealing encryption scheme for multi-user range queries. <em>TCC</em>, <em>13</em>(1), 75-86. (<a href='https://doi.org/10.1109/TCC.2024.3506614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To balance data confidentiality and availability, order-revealing encryption (ORE) has emerged as a pivotal primitive facilitating range queries on encrypted data. However, challenges arise in diverse user domains where data is encrypted with different keys, giving rise to the development of delegatable order-revealing encryption (DORE) schemes. Regrettably, existing DORE schemes are susceptible to authorization token forgery attacks and rely on computationally intensive bilinear pairings. This work proposes a novel solution to address these challenges. We first introduce a delegatable equality-revealing encryption scheme, enabling the comparison of ciphertexts encrypted by distinct secret keys through authorization tokens. Building upon this, we present a delegatable order-revealing encryption that leverages bitwise encryption. DORE supports efficient multi-user ciphertext comparison while robustly resisting authorization token forgery attacks. Significantly, our approach distinguishes itself by minimizing bilinear pairings. Experimental results highlight the efficacy of DORE, showcasing a notable speedup of $2.8\times$ in encryption performance and $1.33\times$ in comparison performance compared to previous DORE schemes, respectively.},
  archive      = {J_TCC},
  author       = {Jingru Xu and Cong Peng and Rui Li and Jintao Fu and Min Luo},
  doi          = {10.1109/TCC.2024.3506614},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {75-86},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {An efficient delegatable order-revealing encryption scheme for multi-user range queries},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A run-time framework for ensuring zero-trust state of client’s machines in cloud environment. <em>TCC</em>, <em>13</em>(1), 61-74. (<a href='https://doi.org/10.1109/TCC.2024.3503358'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the unprecedented demand for cloud computing, ensuring trust in the underlying environment is challenging. Applications executing in the cloud are prone to attacks of different types including malware, network and data manipulation. These attacks may remain undetected for a significant length of time thus causing a lack of trust. Untrusted cloud services can also lead to business losses in many cases and therefore need urgent attention. In this paper, we present Trusted Public Cloud (TPC), a generic framework ensuring the Zero-trust security of client machine. It tracks the system state, alerting the user of unexpected changes in the machine’s state, thus increasing the run-time detection of security vulnerabilities. We validated TPC on Microsoft Azure with Local, Software Trusted Platform Module (SWTPM) and Software Guard Extension (SGX)-enabled SWTPM security providers. We also evaluated the scalability of TPC on Amazon Web Services (AWS) with a varying number of client machines executing in a concurrent environment. The execution results show the effectiveness of TPC as it takes a maximum of 35.6 seconds to recognise the system state when there are 128 client machines attached.},
  archive      = {J_TCC},
  author       = {Devki Nandan Jha and Graham Lenton and James Asker and David Blundell and Martin Higgins and David C. H. Wallom},
  doi          = {10.1109/TCC.2024.3503358},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {61-74},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A run-time framework for ensuring zero-trust state of client’s machines in cloud environment},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HyperPart: A hypergraph-based abstraction for deduplicated storage systems. <em>TCC</em>, <em>13</em>(1), 46-60. (<a href='https://doi.org/10.1109/TCC.2024.3502464'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, deduplication techniques are utilized to minimize the space overhead by deleting redundant data blocks across large-scale servers in data centers. However, such a process exacerbates the fragmentation of data blocks, causing more cross-server file retrievals with plummeting retrieval throughput. Some attempts prefer better file retrieval performance by confining all blocks of a file to one single server, resulting in non-trivial space consumption for more replicated blocks across servers. An ideal network storage system, in effect, should take both the deduplication and retrieval performance into account by implementing reasonable assignment of the detected unique blocks. Such a fine-grained assignment requires an accurate and comprehensive abstraction of the files, blocks, and the file-block affiliation relationships. To achieve this, we innovatively design the weighted hypergraph to profile the multivariate data correlations. With this delicate abstraction in place, we propose HyperPart, which elegantly transforms this complex block allocation problem into a hypergraph partition problem. For more general scenarios with dynamic file updates, we further propose a two-phase incremental hypergraph repartition scheme, which mitigates the performance degradation with minimal migration volume. We implement a prototype system of HyperPart, and the experiment results validate that it saves around 50% of the storage space and improves the retrieval throughput by approximately 30% of state-of-the-art methods under the balance constraints.},
  archive      = {J_TCC},
  author       = {Geyao Cheng and Junxu Xia and Lailong Luo and Haibo Mi and Deke Guo and Richard T. B. Ma},
  doi          = {10.1109/TCC.2024.3502464},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {46-60},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {HyperPart: A hypergraph-based abstraction for deduplicated storage systems},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method to compare scaling algorithms for cloud-based services. <em>TCC</em>, <em>13</em>(1), 34-45. (<a href='https://doi.org/10.1109/TCC.2024.3500139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, many services are offered via the cloud, i.e., they rely on interacting software components that can run on a set of connected Commercial Off-The-Shelf (COTS) servers sitting in data centers. As the demand for any particular service evolves over time, the computational resources associated with the service must be scaled accordingly while keeping the Key Performance Indicators (KPIs) associated with the service under control. Consequently, scaling always involves a delicate trade-off between using the cloud resources and complying with the KPIs. In this paper, we show that a (workload-dependent) Pareto front embodies this trade-off’s limits. We identify this Pareto front for various workloads and assess the ability of several scaling algorithms to approach that Pareto front.},
  archive      = {J_TCC},
  author       = {Danny De Vleeschauwer and Chia-Yu Chang and Paola Soto and Yorick De Bock and Miguel Camelo and Koen De Schepper},
  doi          = {10.1109/TCC.2024.3500139},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {34-45},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A method to compare scaling algorithms for cloud-based services},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-granularity federated learning by graph-partitioning. <em>TCC</em>, <em>13</em>(1), 18-33. (<a href='https://doi.org/10.1109/TCC.2024.3494765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge computing, energy-limited distributed edge clients present challenges such as heterogeneity, high energy consumption, and security risks. Traditional blockchain-based federated learning (BFL) struggles to address all three of these challenges simultaneously. This article proposes a Graph-Partitioning Multi-Granularity Federated Learning method on a consortium blockchain, namely GP-MGFL. To reduce the overall communication overhead, we adopt a balanced graph partitioning algorithm while introducing observer and consensus nodes. This method groups clients to minimize high-cost communications and focuses on the guidance effect within each group, thereby ensuring effective guidance with reduced overhead. To fully leverage heterogeneity, we introduce a cross-granularity guidance mechanism. This mechanism involves fine-granularity models guiding coarse-granularity models to enhance the accuracy of the latter models. We also introduce a credit model to adjust the contribution of models to the global model dynamically and to dynamically select leaders responsible for model aggregation. Finally, we implement a prototype system on real physical hardware and compare it with several baselines. Experimental results show that the accuracy of the GP-MGFL algorithm is 5.6% higher than that of ordinary BFL algorithms. In addition, compared to other grouping methods, such as greedy grouping, the accuracy of the proposed method improves by about 1.5%. In scenarios with malicious clients, the maximum accuracy improvement reaches 11.1%. We also analyze and summarize the impact of grouping and the number of clients on the model, as well as the impact of this method on the inherent security of the blockchain itself.},
  archive      = {J_TCC},
  author       = {Ziming Dai and Yunfeng Zhao and Chao Qiu and Xiaofei Wang and Haipeng Yao and Dusit Niyato},
  doi          = {10.1109/TCC.2024.3494765},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {18-33},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Multi-granularity federated learning by graph-partitioning},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing renewable energy utilization in cloud data centers through dynamic overbooking: An MDP-based approach. <em>TCC</em>, <em>13</em>(1), 1-17. (<a href='https://doi.org/10.1109/TCC.2024.3487954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shift towards renewable energy sources for powering data centers is increasingly important in the era of cloud computing. However, integrating renewable energy sources into cloud data centers presents a challenge due to their variable and intermittent nature. The unpredictable workload demands in cloud data centers further complicate this problem. In response to this pressing challenge, we propose a novel approach in this paper: adapting the workload to match the renewable energy supply. Our solution involves dynamic overbooking of resources, providing energy flexibility to data center operators. We propose a framework that stochastically models both workload and energy source information, leveraging Markov Decision Processes (MDP) to determine the optimal overbooking degree based on the workload flexibility of data center clients. We validate the proposed algorithm in realistic settings through extensive simulations. Results demonstrate the superiority of our proposed method over existing approaches, achieving better matching with the renewable energy supply by 55.6%, 34.65%, and 40.7% for workload traces from Nectar Cloud, Google, and Wikipedia, respectively.},
  archive      = {J_TCC},
  author       = {Tuhin Chakraborty and Carlo Kopp and Adel N. Toosi},
  doi          = {10.1109/TCC.2024.3487954},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {1-3},
  number       = {1},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Optimizing renewable energy utilization in cloud data centers through dynamic overbooking: An MDP-based approach},
  volume       = {13},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
