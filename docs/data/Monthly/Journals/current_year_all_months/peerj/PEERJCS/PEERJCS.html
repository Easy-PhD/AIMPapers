<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PEERJCS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="peerjcs">PEERJCS - 580</h2>
<ul>
<li><details>
<summary>
(2025). Enhancing drone autonomy through cloud integration: A comprehensive software architecture for navigation, visual servoing, and control. <em>PEERJCS</em>, <em>11</em>, e3238. (<a href='https://doi.org/10.7717/peerj-cs.3238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work proposes a comprehensive software framework for cloud-enabled autonomous drone navigation, featuring precise target tracking via image-based visual servoing (IBVS) coupled with a control scheme. In this study, a low-cost quadcopter running the ArduPilot firmware is evaluated within a simulation-in-the-loop (SITL) environment using a Gazebo-based simulation of a real-world mission. The tested software architecture can be seamlessly integrated with an onboard companion computer for real-time execution. The mission involves waypoint tracking, precise identification and descent onto visual markers using IBVS, along with real-time data visualization on a remote client connected via a cloud interface. Because the software architecture is versatile, it can accommodate any conventional or knowledge-based controller. To demonstrate the efficacy and robustness of the proposed architecture, the quadcopter was tested under challenging weather conditions, where it successfully completed the mission despite disturbances and sensor noise. Finally, the complete software architecture has been tested and implemented in the robot operating system (ROS).},
  archive      = {J_PEERJCS},
  author       = {Muhammad Bilal Kadri},
  doi          = {10.7717/peerj-cs.3238},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3238},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing drone autonomy through cloud integration: A comprehensive software architecture for navigation, visual servoing, and control},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of blockchain and machine learning integration for cybersecurity in microgrids. <em>PEERJCS</em>, <em>11</em>, e3237. (<a href='https://doi.org/10.7717/peerj-cs.3237'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The combination of blockchain technology with machine learning (ML) has emerged as a transformative approach to addressing cybersecurity challenges in microgrids (MG). As these systems become increasingly interconnected and dependent on real-time data transmission, they face growing risks from cyber threats such as false data injection attacks (FDIA), data tampering, denial of service (DoS), and adversarial attacks. This study provides a comprehensive analysis of how blockchain and ML can be integrated to mitigate these vulnerabilities. While ML offers advanced capabilities for anomaly detection, threat prediction, and adaptive response, blockchain’s decentralized, transparent, and secure architecture provides a reliable foundation for data and transaction processing. By combining these technologies, MGs can enhance operational efficiency, safeguard data integrity, and strengthen system resilience. This article reviews recent developments in blockchain and ML applications for MG cybersecurity and highlights key enabling technologies and implementation challenges. Future research directions include the design of hybrid models and improvements in scalability. The findings highlight the potential of blockchain and ML to transform cybersecurity in MGs and support the development of safer, more reliable, and sustainable energy systems.},
  archive      = {J_PEERJCS},
  author       = {Chou-Mo Yang and Chun-Lien Su and Mahmoud Elsisi},
  doi          = {10.7717/peerj-cs.3237},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3237},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A comprehensive review of blockchain and machine learning integration for cybersecurity in microgrids},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Chatbots integrated in the metaverse: A conceptual model based on interaction ritual chain theory. <em>PEERJCS</em>, <em>11</em>, e3235. (<a href='https://doi.org/10.7717/peerj-cs.3235'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chatbots are increasingly prevalent as conversational agents in the metaverse, enabled by commercial potential and artificial intelligence (AI) advancements that facilitate natural human-computer interaction (HCI). However, integrating chatbots as virtual community members remains challenging due to deficiencies in conversational and social intelligence. Current metaverse research prioritizes foundational infrastructure while neglecting commonalities between chatbots and community members. This gap is evident in the absence of standardized models for chatbots’ social capabilities. To address this, we propose a conceptual model of chatbots’ social capabilities based on the interaction ritual chain (IRC) theory, derived from a systematic analysis of 1,273 articles. Our qualitative methodology synthesizes 83 references, all of which dealt with the topic of chatbots and had direct or potential implications for the metaverse. The model addresses core challenges through four elements—virtual coexistence, dynamic constraint, shared intention, and positive incentive—providing solutions for chatbot integration. Finally, we examine social capability interrelationships and suggest future research directions.},
  archive      = {J_PEERJCS},
  author       = {Yan Yan and Mengjuan Fan},
  doi          = {10.7717/peerj-cs.3235},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3235},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Chatbots integrated in the metaverse: A conceptual model based on interaction ritual chain theory},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-scale convolutional and color-adaptive approach for sensory enhancement in cultural and creative product packaging. <em>PEERJCS</em>, <em>11</em>, e3230. (<a href='https://doi.org/10.7717/peerj-cs.3230'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study addresses the critical need for enhanced visual appeal in cultural product packaging by proposing a novel multi-scale convolutional neural network (MCCNN) with adaptive color enhancement. Unlike existing methods that struggle with uneven lighting and detail loss, our approach innovatively combines laser-based 3D feature fusion with illumination-aware enhancement to overcome these limitations. The method extracts multi-level visual features from packaging images through scale transformation and feature fusion, constructing a laser-based 3D multi-scale feature fusion model to achieve image preprocessing and noise reduction. Furthermore, by employing block matching and fuzziness detection techniques, a visual constraint model is established to effectively extract features from blurred regions and detect image block information. In terms of image enhancement, the integration of illumination compensation and adaptive dehazing techniques addresses issues such as image fogging and detail loss during brightness adjustment, thereby improving image quality and color richness. Experimental results demonstrate that the proposed method achieves a 90.62% completeness rate in 3D reconstruction of product packaging images, with an average design time of less than 5.3 s. Additionally, the color enhancement module shows outstanding performance, with a color enhancement effect of 94.99%, an image fitness value of 1.0148, and an information entropy of 78.96%, effectively enhancing image contrast and visual quality. This research offers new insights and technical support for the intelligent sensory design of cultural and creative product packaging.},
  archive      = {J_PEERJCS},
  author       = {Junyi Xu and Linian Liu},
  doi          = {10.7717/peerj-cs.3230},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3230},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A multi-scale convolutional and color-adaptive approach for sensory enhancement in cultural and creative product packaging},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A planning model for dedicated tourist bus routes based on an improved genetic-greedy algorithm and machine learning. <em>PEERJCS</em>, <em>11</em>, e3221. (<a href='https://doi.org/10.7717/peerj-cs.3221'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background This study addresses the challenges posed by the growing number of self-guided tourists and proposes an optimized tourist bus route planning model to enhance visitor satisfaction and support sustainable tourism. Methods Using machine learning algorithms—adaptive boosting (AdaBoost), support vector machine (SVM), naive Bayes, and K-Nearest Neighbor (KNN)—we analyze sentiment in tourist reviews, with SVM showing the best performance. A multi-criteria evaluation model combining analytic hierarchy process (AHP) and the entropy weight method (EWM) identifies key satisfaction factors, which are integrated into the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) model and the rank-sum ratio (RSR) method to recommend attractions. Results The optimized route is determined using a modified Genetic-Greedy Algorithm (GGA), which improves convergence speed by 94.489% compared to traditional genetic algorithms. Applied to a case study in Tibet, the model achieved a 94.6% satisfaction rate, demonstrating its effectiveness and adaptability for diverse tourism planning contexts.},
  archive      = {J_PEERJCS},
  author       = {Suping Cui and Xiang Zhang and Haiqiong Liang and Chang Liu and Sa Du and Boyu Hou and Xinyan Wang and Zhongfeng Wu},
  doi          = {10.7717/peerj-cs.3221},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3221},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A planning model for dedicated tourist bus routes based on an improved genetic-greedy algorithm and machine learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Refining medical large language models: Key insights from instruction tuning. <em>PEERJCS</em>, <em>11</em>, e3216. (<a href='https://doi.org/10.7717/peerj-cs.3216'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This literature review introduces a comprehensive summary of the most recent scholarly work on instruction-tuning strategies for medical large language models (Med-LLMs). It begins by reviewing three fundamental approaches to creating an instruction dataset: human-crafted datasets, synthesized datasets generated by LLMs, and datasets that incorporate Retrieval-Augmented Generation (RAG). This article explores the role of medical instruction datasets by reviewing thirteen different medical models, evaluating their effectiveness across multiple clinical tasks, and examining how their utilization can improve outcomes in the medical domain. This research discusses key insights for optimizing instruction-based fine-tuning of language models. It analyzes the effectiveness of the phased instruction method and the benefits of integrating mixed-prompt techniques. Additionally, it assesses the effect of choosing an appropriate backbone model before fine-tuning. Furthermore, it demonstrates how the selection of words when crafting instructions influences a model’s performance. The survey emphasizes that carefully curated instructional data, coupled with well-crafted strategies, can greatly enhance the potential of Med-LLMs in real-world healthcare applications. Nevertheless, several challenges must be addressed to ensure the safe, ethical, and effective deployment of Med-LLMs. This article outlines future research directions, including mitigating racial and gender biases, leveraging external knowledge sources, and reinforcing privacy through robust anonymization of patient information and regulatory adherence (e.g., Health Insurance Portability and Accountability Act (HIPAA)). Addressing these challenges will pave the way for reliable, safe, and ethical artificial intelligence (AI)-driven healthcare applications.},
  archive      = {J_PEERJCS},
  author       = {Muneerah Q. Alqahtani and Abdullah Albarakati and Fahd Alotaibi},
  doi          = {10.7717/peerj-cs.3216},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3216},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Refining medical large language models: Key insights from instruction tuning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating adversarial attacks in federated learning based network traffic classification applications using secure hierarchical remote attestation and adaptive aggregation framework. <em>PEERJCS</em>, <em>11</em>, e3215. (<a href='https://doi.org/10.7717/peerj-cs.3215'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Federated learning (FL) enhances network traffic classification (NTC) with significant benefits in privacy, performance, and efficiency. However, the distributed nature of FL exposes NTC models to critical adversarial attacks from byzantine clients. These attacks, such as label flipping (LF) and model poisoning, can severely degrade overall model performance, while backdoor and generative adversarial network (GAN) based attacks can force the model to misclassify specific traffic classes. Securing FL-based NTC is paramount, as these vulnerabilities pose substantial threats to its vital role in network management, quality of service, and threat identification. Methods While various defensive measures for FL exist, they are often ineffective against multiple types of adversarial attacks, and their effectiveness diminishes as the number of attackers increases. To address this gap, this study proposed SHeRAA-FL, a secure framework for FL-based NTC. The framework secures the training process by combining remote attestation scoring, hierarchical training, and adaptive aggregation mechanisms, reinforced with hardware-level security and encrypted communication. We developed and evaluated SHeRAA-FL on public datasets, such as ISCX-VPN 2016 and N-BaIoT, benchmarking it against existing approaches, including weighted averaging, median-mean, trim-mean, Krum, and Multi-Krum. Results The evaluation results show that SHeRAA-FL effectively mitigates the impact of multiple types of adversarial attacks, even in scenarios with multiple attackers. For example, in the LF attack, other approaches recorded a 99.6% accuracy reduction, while SHeRAA-FL only recorded a 5.33% reduction. Moreover, in a normal scenario, the framework produces a model with the highest accuracy of 0.9130, indicating minimal disruption to the FL process.},
  archive      = {J_PEERJCS},
  author       = {Azizi Ariffin and Faiz Zaki and Hazim Hanif and Nor Badrul Anuar},
  doi          = {10.7717/peerj-cs.3215},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3215},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Mitigating adversarial attacks in federated learning based network traffic classification applications using secure hierarchical remote attestation and adaptive aggregation framework},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced object detection of enterobius vermicularis eggs using cumulative transfer learning algorithm. <em>PEERJCS</em>, <em>11</em>, e3213. (<a href='https://doi.org/10.7717/peerj-cs.3213'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional diagnostic methods in medical parasitology rely heavily on manual microscopic examination, which is labor-intensive and prone to human error and subjectivity. This study introduced a novel approach for automating the detection of Enterobius vermicularis (pinworm) eggs using cumulative transfer learning algorithms. The proposed framework effectively captures subtle egg morphology by employing a sequential knowledge transfer paradigm, thereby enhancing diagnostic accuracy, efficiency, and reproducibility, even when data are limited. This study used E. vermicularis egg images from a publicly available dataset. The training image dataset comprised 1,000 images of artifacts and 1,000 images of pinworm eggs. Comparisons were made against established deep learning (DL) models, including conventional convolutional neural network (CNN), ResNet50, DenseNet121, Xception, and InceptionV3. Results demonstrated that the cumulative transfer learning strategy consistently outperformed both the conventional CNN method and DL baselines in terms of classification accuracy, F1-score, and computational efficiency, while also reducing computational overhead. Performance comparison with a conventional CNN model demonstrates that the proposed cumulative transfer learning CNN reduces training time from 2 h to 50 min. Moreover, it achieves optimal performance, with accuracy, precision, recall, and F1-score all reaching 1.0. The model’s detection accuracy was quantitatively assessed by comparing predicted bounding boxes to expert annotations across 103 microscopic images. The proposed cumulative transfer learning CNN achieved higher average precision (AP) @ intersection over union (IoU) 0.5 (0.530) and perfect sensitivity (1.00), but exhibited 97 false positives and lower mean average precision (mAP) @IoU0.5:0.05:0.95 (0.027). In contrast, the You Only Look Once version 8 (YOLOv8) model demonstrated lower sensitivity (0.72) but superior multi-threshold performance (mAP@IoU0.5:0.05:0.95 = 0.057). These results highlight a trade-off between detection sensitivity and generalization performance across varying IoU thresholds. These findings affirm the viability of cumulative transfer learning as a scalable, accurate, and efficient approach for automated parasitological diagnostics, particularly in resource-limited settings.},
  archive      = {J_PEERJCS},
  author       = {Pongphan Pongpanitanont and Naparat Suttidate and Hiroshi Yamasaki and Wanchai Maleewong and Penchom Janwan},
  doi          = {10.7717/peerj-cs.3213},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3213},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced object detection of enterobius vermicularis eggs using cumulative transfer learning algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of big data transfers among data center networks using deep reinforcement learning with graph neural networks. <em>PEERJCS</em>, <em>11</em>, e3212. (<a href='https://doi.org/10.7717/peerj-cs.3212'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The big data era is an emerging paradigm that has gained a lot of interest in the last few years from industry, academia, and governments around the world. Cloud computing infrastructure often operates over multiple distributed data centers around the globe, following a pay-as-you-go pricing model. Enabling fast data transfer across these data centers, with low monetary cost and without link congestion, is not a trivial task. Efficient protocols and tools are necessary to transfer a huge amount of data while taking into account the user’s quality of service (QoS) requirements. With the recent widespread use of artificial intelligence (AI) and its application in network optimization scenarios, deep reinforcement learning (DRL), which combines reinforcement learning with deep learning, has emerged as a prominent approach for big data transfers among data center networks. In this article, we introduce a novel approach that integrates DRL with graph neural networks (GNN) to come up with an efficient strategy for big data transfer. Our approach generates continuous control actions to optimize data transfer. It can learn from past actions and successfully generalize to different incoming scenarios. Results show that our method consistently optimizes big data centers among data centers.},
  archive      = {J_PEERJCS},
  author       = {Imen Filali and Ridha Ejbali and Sarah A. Alzakari and Amel Ali Alhussan},
  doi          = {10.7717/peerj-cs.3212},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3212},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimization of big data transfers among data center networks using deep reinforcement learning with graph neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-assisted genomic profiling to identify differences between bacillus calmette-guérin (BCG) vaccine strains and non-BCG wild-type mycobacterium bovis. <em>PEERJCS</em>, <em>11</em>, e3211. (<a href='https://doi.org/10.7717/peerj-cs.3211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Distinguishing Bacillus Calmette-Guérin (BCG) vaccines from pathogenic Mycobacterium bovis is critical in neonatal diagnostics, particularly where polymerase chain reaction (PCR) methods fail to detect key genomic variations in tuberculosis (TB)-endemic regions. Methods We developed a machine learning framework analyzing 72 clinical isolates (28 BCG, 44 non-BCG) using whole-genome sequencing. Two classifiers were implemented: a random forest optimized by out-of-bag error minimization and a one-dimensional convolutional neural network (1D CNN) with dropout regularization (0.3–0.5). Feature selection through permutation testing and gradient activation mapping enhanced interpretability. Results Cross-validation demonstrated robust performance for both models: the random forest achieved 96% accuracy using 47 BCG attenuation-related genes, while the convolutional neural network (CNN) maintained high generalizability with 95.8% (±3.4%) mean accuracy and perfect recall across stratified five-fold validation, supported by strong discriminative capacity (mean area under the curve (AUC): 0.964 ± 0.046). Key biomarkers included metabolic reprogramming (ko01100) and secondary metabolite biosynthesis (ko01110) pathways. Conclusion This genomic approach resolves BCG diagnostic ambiguities through conserved attenuation markers, with the 47-gene panel enabling rapid assays that reduce neonatal overtreatment risks. Future integration of transcriptomic data could optimize these biomarkers for clinical deployment in high-TB-burden settings.},
  archive      = {J_PEERJCS},
  author       = {Yunyun Shi and Jiang Yuan and Xiaobin Tang and Hai Luo and Genyun Tang and Zhiyong Shen},
  doi          = {10.7717/peerj-cs.3211},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3211},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Machine learning-assisted genomic profiling to identify differences between bacillus calmette-guérin (BCG) vaccine strains and non-BCG wild-type mycobacterium bovis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformers and capsule networks vs classical ML on clinical data for alzheimer classification. <em>PEERJCS</em>, <em>11</em>, e3208. (<a href='https://doi.org/10.7717/peerj-cs.3208'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a progressive neurodegenerative disorder and the leading cause of dementia worldwide. Although clinical examinations and neuroimaging are considered the diagnostic gold standard, their high cost, lengthy acquisition times, and limited accessibility underscore the need for alternative approaches. This study presents a rigorous comparative analysis of traditional machine learning (ML) algorithms and advanced deep learning (DL) architectures that that rely solely on structured clinical data, enabling early, scalable AD detection. We propose a novel hybrid model that integrates a convolutional neural networks (CNNs), DigitCapsule-Net, and a Transformer encoder to classify four disease stages—cognitively normal (CN), early mild cognitive impairment (EMCI), late mild cognitive impairment (LMCI), and AD. Feature selection was carried out on the ADNI cohort with the Boruta algorithm, Elastic Net regularization, and information-gain ranking. To address class imbalance, we applied three oversampling techniques: synthetic minority oversampling technique (SMOTE), oversample using adaptive synthetic (ADASYN), and SMOTE-Tomek. In the three-class setting, the CNN + DigitCapsule-Net hybrid attained 90.58% accuracy, outperforming state-of-the-art baselines that rely only on clinical variables. A tuned gradient boosting (GB) model achieved comparable performance with substantially lower computational requirements. Model interpretability was assessed with SHAP and gradient-weighted class activation map (Grad-CAM), which identified Clinical Dementia Rating-Sum of Boxes (CRD-SB), Logical Memory-Delayed Recall Total Number of Story Units Recalled (LDELTOTAL), and Modified Preclinical Alzheimer Cognitive Composite with Trails B (mPACC-TrailsB) as the most informative clinical features. This combination of predictive strength, computational efficiency, and transparent interpretation positions the proposed approach as a promising open-source tool for facilitating early AD diagnosis in clinical settings.},
  archive      = {J_PEERJCS},
  author       = {Mario Alejandro Bravo-Ortíz and Sergio Alejandro Holguin-Garcia and Ernesto Guevara-Navarro and Esteban Cerón-Cabrera and Alejandro Mora-Rubio and Harold Brayan Arteaga-Arteaga and Gonzalo A. Ruz and Reinel Tabares-Soto},
  doi          = {10.7717/peerj-cs.3208},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3208},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Transformers and capsule networks vs classical ML on clinical data for alzheimer classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tab transformer with meta-ensemble learning approaches for enhanced diabetes prediction. <em>PEERJCS</em>, <em>11</em>, e3206. (<a href='https://doi.org/10.7717/peerj-cs.3206'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes represents a significant metabolic disorder marked by elevated glucose levels due to suboptimal insulin production or function. Early diagnosis and effective diabetes management are crucial to reducing related health complications. This study introduces a robust approach for predicting diabetes through advanced machine learning methods. Utilizing the diabetes dataset from the University of California Irvine (UCI) machine learning repository, we performed extensive preprocessing to guarantee data quality and integrity. To counteract class imbalance, we employed the synthetic minority over-sampling technique, which improved the representation of minority classes. We explored several machine learning (ML) models, including Random Forest (RF), logistic regression (LR), and K-nearest neighbors (KNN), while optimizing hyperparameters through grid search and randomized search techniques. Additionally, we introduced a stacking ensemble method paired with a tab transformer model, effectively harnessing the advantages of both techniques for efficient handling of tabular data. The outcomes from the stacking and tab transformer models were later aggregated using a meta learner, specifically extreme gradient boosting (XGBoost), to create a robust ensemble model. Our comprehensive methodology yielded an impressive accuracy rate of 99%, significantly outperforming individual models. Unlike previous studies that rely solely on individual models, our approach fills the gap by combining deep learning with ensemble methods to enhance generalization and interpretability in diabetes prediction. We have validated the model’s performance using ablation studies and paired statistical significance tests. These results highlight the efficacy of integrating diverse ML strategies to enhance both the accuracy and reliability of diabetes prediction.},
  archive      = {J_PEERJCS},
  author       = {Sidra Khalid and Shabana Ramzan and Muhammad Munwar Iqbal and Ali Raza and Aseel Smerat and Mehdi Hosseinzadeh and Changgyun Kim and Muhammad Syafrudin and Norma Latif Fitriyani},
  doi          = {10.7717/peerj-cs.3206},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3206},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Tab transformer with meta-ensemble learning approaches for enhanced diabetes prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Invgamma: The inverse gamma distribution in r. <em>PEERJCS</em>, <em>11</em>, e3205. (<a href='https://doi.org/10.7717/peerj-cs.3205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {invgamma is a popular low dependency R package that implements the probability density function (PDF), cumulative distribution function (CDF), quantile function (QF) and random number generator (RNG) functions for the inverse gamma, inverse chi-squared, and inverse exponential distributions, which are missing from base R. The functions follow the standard R syntax and are efficient, leveraging the corresponding functions for the gamma distribution currently in R through straightforward mathematical relationships between the distributions. It is distributed through the Comprehensive R Archive Network (CRAN, https://cran.r-project.org) and GitHub (https://github.com/dkahle/invgamma), where it is version controlled.},
  archive      = {J_PEERJCS},
  author       = {David Kahle and James Stamey},
  doi          = {10.7717/peerj-cs.3205},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3205},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Invgamma: The inverse gamma distribution in r},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A literature review of research on question generation in education. <em>PEERJCS</em>, <em>11</em>, e3203. (<a href='https://doi.org/10.7717/peerj-cs.3203'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a key natural language processing (NLP) task, question generation (QG) is crucial for boosting educational quality and fostering personalized learning. This article offers an in-depth review of the research advancements and future directions in QG in education (QGEd). We start by tracing the evolution of QG and QGEd. Next, we explore the current state of QGEd research through three dimensions: its three core objectives, commonly used datasets, and question quality evaluation methods. This article also underscores its unique contributions to QGEd, including a systematic analysis of the research landscape and an identification of pivotal challenges and opportunities. Lastly, we highlight future research directions, emphasizing the need for deeper exploration in QGEd regarding multimodal data processing, controllability of fine-grained cognitive and difficulty levels, specialized educational dataset construction, automatic evaluation technology development, and system architecture design. Overall, this review aims to provide a comprehensive overview of the field, offering valuable insights for researchers and practitioners in educational technology.},
  archive      = {J_PEERJCS},
  author       = {Xiaohui Dong and Xinyu Zhang and Zhengluo Li and Quanxin Hou and Jixiang Xue and Xiaoyi Li},
  doi          = {10.7717/peerj-cs.3203},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3203},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A literature review of research on question generation in education},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for semantic textual similarity on long texts. <em>PEERJCS</em>, <em>11</em>, e3202. (<a href='https://doi.org/10.7717/peerj-cs.3202'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work introduces a method for the semantic similarity of long documents using sentence transformers and large language models. The method detects relevant information from a pair of long texts by exploiting sentence transformers and large language models. The degree of similarity is obtained with an analytical fuzzy strategy that enables selective iterative retrieval under noisy conditions. The method discards the least similar pairs of sentences and selects the most similar. The preprocessing consists of splitting texts into sentences. The analytical strategy classifies pairs of texts by a degree of similarity without prior training on a dataset of long documents. Instead, it uses pre-trained models with any token capacity, a set of fuzzy parameters is tuned based on a few assessment iterations, and the parameters are updated based on criteria to detect four classes of similarity: identical, same topic, concept related, and non-related. This method can be employed in both small sentence transformers and large language models to detect similarity between pairs of documents of random sizes and avoid truncation of texts by testing pairs of sentences. A dataset of long texts in English from Wikipedia and other public sources, jointly with its gold standard, is provided and reviewed to test the method’s performance. The method’s performance is tested with small-token-size sentence transformers, large language models (LLMs), and text pairs split into sentences. Results prove that smaller sentence transformers are reliable for obtaining the similarity on long texts and indicate this method is an economical alternative to the increasing need for larger language models to find the degree of similarity between two long texts and extract the relevant information. Code and datasets are available at: https://github.com/omarzatarain/long-texts-similarity. Results of the adjustment of parameters can be found at https://doi.org/10.6084/m9.figshare.29082791.},
  archive      = {J_PEERJCS},
  author       = {Omar Zatarain and Juan Carlos González-Castolo and Silvia Ramos-Cabral},
  doi          = {10.7717/peerj-cs.3202},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3202},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A method for semantic textual similarity on long texts},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autoregressive translation model design integrating syntactic encoding computation and reinforcement learning: A framework for enhancing translation optimization. <em>PEERJCS</em>, <em>11</em>, e3199. (<a href='https://doi.org/10.7717/peerj-cs.3199'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an autoregressive translation model that integrates syntactic encoding computation and reinforcement learning. The model enhances positional encoding by leveraging the strengths of linear transformer and adaptive Fourier transform (AFT), thereby achieving a self-attention mechanism with O(nlogn) computational complexity. To improve translation accuracy and grammatical correctness, the proposed approach incorporates grammatical information from input sentences into the encoder and introduces a component attention module (CAM). This syntactic-aware mechanism significantly improves the model’s capacity to capture hierarchical grammatical structures, yielding a 12.7% relative improvement in translation accuracy on complex syntactic constructions. Addressing the issue of reduced translation quality in noisy input texts, the study employs a gradient-based attack method within reinforcement learning to facilitate adversarial training. Evaluated on the WMT14 En-Fr and WMT17 En-De datasets, our model is compared against several baselines using bilingual evaluation understudy (BLEU) and TwoBLEU scores as evaluation metrics. Experimental results on the WMT14 En-Fr and WMT17 En-De datasets demonstrate the model’s superior performance, with BLEU scores and both twoBLEU scores (bbs) values of 27.31, 8.9, and 20.3, 6.7, respectively. Compared to existing translation models, the proposed model achieves a BLEU score improvement of 4.7% and has a better balance between translation quality and the text generation rate of the Transformer. In conclusion, the autoregressive translation model integrating syntactic encoding computation and reinforcement learning demonstrates significant improvements in optimizing the translation framework and enhancing translation accuracy and efficiency. This research not only introduces innovative methodologies for advancing machine translation technologies but also provides robust support for optimizing language education and translation training programs.},
  archive      = {J_PEERJCS},
  author       = {Huo Li and Liming Cui and Kemal Polat},
  doi          = {10.7717/peerj-cs.3199},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3199},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Autoregressive translation model design integrating syntactic encoding computation and reinforcement learning: A framework for enhancing translation optimization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing fruit freshness classification with adaptive knowledge distillation and global response normalization in convolutional networks. <em>PEERJCS</em>, <em>11</em>, e3198. (<a href='https://doi.org/10.7717/peerj-cs.3198'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The assessment of fruit freshness is crucial for ensuring food quality and reducing waste in agricultural production. In this study, we propose Global Response Normalization and Gaussian Error Linear Unit Enhanced Network (GGENet), a novel deep learning architecture that leverages adaptive knowledge distillation (AKD) and global response normalization (GRN) to classify fruits as fresh or rotten. Our model comprises two variants: GGENet-Teacher (GGENet-T), serving as the teacher model, and GGENet-Student (GGENet-S), functioning as the student model. By transferring attention maps from the teacher to the student model, we achieve efficient adaptive knowledge distillation, enhancing the performance of the lighter student model. Experimental results demonstrate that the GGENet with adaptive knowledge distillation (GGENet-AKD) achieves a competitive accuracy of 0.9818, an F1-score of 0.9818, and an area under the curve (AUC) score of 0.9891. The proposed method significantly contributes to reducing food waste and enhancing quality control in agriculture by facilitating early detection of rotting fruits.},
  archive      = {J_PEERJCS},
  author       = {Semih Demirel and Oktay Yıldız},
  doi          = {10.7717/peerj-cs.3198},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3198},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing fruit freshness classification with adaptive knowledge distillation and global response normalization in convolutional networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Financial trading decision model based on deep reinforcement learning for smart agricultural management. <em>PEERJCS</em>, <em>11</em>, e3196. (<a href='https://doi.org/10.7717/peerj-cs.3196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a decision-making model based on deep reinforcement learning (DRL) for agricultural financial transactions, addressing core challenges such as significant data noise, strong time-series dependence, and limited strategy adaptability. We developed a multifactor dynamic denoising framework by integrating the Grubbs test for outlier detection and the median absolute deviation (MAD) method for noise handling. This framework categorizes agricultural financial indicators into six feature types, significantly enhancing robustness against data noise and improving model reliability. Furthermore, an long short-term memory (LSTM)-enhanced DRL architecture is employed, incorporating a sliding window mechanism to capture market timing features. This framework constructs a transaction cost-based reward function. It establishes an intelligent trading decision model based on the LSTM algorithm and the data query language (DQL). Experimental results demonstrate an annualized return of 45.12% and a 35% reduction in maximum retracement for Deere & Company and BAYN.DE. The Sharpe ratio reaches 1.51, reflecting a 62% improvement over the benchmark model. The results validate the robustness of the proposed decision-making model in the face of price fluctuations and policy interventions. This model addresses critical bottlenecks in the application of DRL in agricultural finance, facilitating the transition of agricultural economic management from empirical judgment to data-driven approaches. Through three key innovations—data denoising, time-series modeling, and domain adaptation—it provides a vital decision-support tool for advancing smart agriculture.},
  archive      = {J_PEERJCS},
  author       = {Di Fan and Nazrul Hisyam Ab Razak and Wei Ni Soh},
  doi          = {10.7717/peerj-cs.3196},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3196},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Financial trading decision model based on deep reinforcement learning for smart agricultural management},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid ARIMA-LSTM for COVID-19 forecasting: A comparative AI modeling study. <em>PEERJCS</em>, <em>11</em>, e3195. (<a href='https://doi.org/10.7717/peerj-cs.3195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pandemics present critical challenges to global health systems, economies, and societal structures, necessitating the development of accurate forecasting models for effective intervention and resource allocation. Classical statistical models such as the autoregressive integrated moving average (ARIMA) have been widely employed in epidemiological forecasting; however, they struggle to capture the nonlinear trends and dynamic fluctuations inherent in pandemic data. Conversely, deep learning models such as long short-term memory (LSTM) networks demonstrate strong capabilities in modeling complex dependencies but often require substantial data and computational resources. To boost forecasting precision, hybrid models such as ARIMA-LSTM integrate the advantages of traditional and deep learning methods. This study evaluates and compares the performance of ARIMA, LSTM, and hybrid ARIMA-LSTM models in predicting pandemic trends, using COVID-19 data from the Malaysian Ministry of Health as a case study. The dataset covers the period from 4 January 2021 to 18 September 2021, and model performance is evaluated using key metrics, including mean squared error (MSE), mean absolute error (MAE), mean absolute percentage error (MAPE), root mean squared error (RMSE), relative root mean squared error (RRMSE), normalized root mean squared error (NRMSE), and the coefficient of determination (R2). The results demonstrate that ARIMA performs poorly in capturing pandemic trends, while LSTM improves forecasting accuracy. However, the hybrid ARIMA-LSTM model consistently achieves the lowest error rates, confirming the advantage of integrating statistical and deep learning methodologies. All findings support the adoption of hybrid modeling approaches for pandemic forecasting, contributing to more accurate and reliable predictive analytics in epidemiology. Future research should investigate the generalizability of hybrid models across various infectious diseases and integrate additional real-time external variables to improve forecasting reliability.},
  archive      = {J_PEERJCS},
  author       = {Al Mahmud and Syed Husni Noor Syed Hatim Noor and Kamarul Imran Musa and Firdaus Mohamad Hamzah and Zainab Mat Yudin and Noorshaida Kamaruddin and Ashwini M. Madawana and Mohamad Arif Awang Nawi},
  doi          = {10.7717/peerj-cs.3195},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3195},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hybrid ARIMA-LSTM for COVID-19 forecasting: A comparative AI modeling study},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CrossAlignNet: A self-supervised feature learning framework for 3D point cloud understanding. <em>PEERJCS</em>, <em>11</em>, e3194. (<a href='https://doi.org/10.7717/peerj-cs.3194'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a self-supervised point cloud representation learning framework CrossAlignNet based on cross-modal mask alignment strategy, to solve the problems of imbalance between global semantic and local geometric feature learning, as well as cross-modal information asymmetry in existing methods. A geometrically consistent mask region is established between the point cloud patches and the corresponding image patches through a synchronized mask alignment strategy to ensure cross-modal information symmetry. A dual-task learning framework is designed: the global semantic alignment task enhances the cross-modal semantic consistency through contrastive learning, and the local mask reconstruction task fuses the image cues using the cross-attention mechanism to recover the local geometric structure of the masked point cloud. In addition, the ShapeNet3D-CMA dataset is constructed to provide accurate point cloud-image spatial mapping relations to support cross-modal learning. Our framework shows superior or comparative results against existing methods on three point cloud understanding tasks including object classification, few-shot classification, and part segmentation.},
  archive      = {J_PEERJCS},
  author       = {Fei Wang and Xingzhen Dong and Jia Wu and Weishi Zhang and Tuo Zhou},
  doi          = {10.7717/peerj-cs.3194},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3194},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {CrossAlignNet: A self-supervised feature learning framework for 3D point cloud understanding},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A literature review on disease detection with automated machine learning. <em>PEERJCS</em>, <em>11</em>, e3193. (<a href='https://doi.org/10.7717/peerj-cs.3193'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study reviews disease detection with Automated Machine Learning (AutoML), aiming to identify gaps and evaluate AutoML’s impact in this field. In this study, seven review articles published in Q1- or Q2-quartile journals between 2020 and 2025 were analyzed. The reviews were assessed using ten academic criteria, covering AutoML performance, data strategies, feature techniques, noise reduction, model selection, training/testing methods, and frameworks for disease detection. Additionally, test reliability, patient selection, reference standards, and application processes were evaluated with the Quality Assessment of Diagnostic Accuracy Studies-2 (QUADAS-2) tool. A literature review was conducted using 11 different databases; however, due to limited functionality in four of them, the research primarily relied on seven digital databases, which initially yielded 552 studies. The study selection and screening processes were performed in accordance with the Preferred reporting items for systematic reviews and meta-analyses (PRISMA) guidelines. Next, 40 studies published outside the 2020–2025 period were removed, followed by the exclusion of 117 studies that were not journal articles. An additional 145 studies were eliminated because they were reviews, books, conference proceedings, posters, editorial notes, etc., and seven studies were excluded as they did not pertain to human diseases. After these elimination processes, 243 articles remained for full-text review. Out of these, 214 articles were read in full and assessed for relevance, leading to 29 articles deemed suitable for inclusion in this review on disease detection using AutoML. After removing five duplicate articles, a final total of 24 studies were included in the review. The research questions of the study include questions such as which disease detection models AutoML methods are preferred more, the input features and data sets used, the effects of feature extraction and selection methods on model performance, how often noise reduction methods are used in disease data, and what the AutoML model evaluation metrics are. The results show that AutoML methods are effectively used on disease detection and that different AutoML techniques, data sets, and model selection processes make significant contributions to success. This review provides an important resource for making AutoML applications for disease detection more efficient and for eliminating the deficiencies in the literature.},
  archive      = {J_PEERJCS},
  author       = {Ersin Enes Eryılmaz and Erdal Kılıç},
  doi          = {10.7717/peerj-cs.3193},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3193},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A literature review on disease detection with automated machine learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring mHealth interventions for medication management: A scoping review of digital tools, implementation barriers, and patient outcomes. <em>PEERJCS</em>, <em>11</em>, e3190. (<a href='https://doi.org/10.7717/peerj-cs.3190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Medication non-adherence remains a significant global healthcare challenge, resulting in inadequate disease management, increased hospitalisations, and higher healthcare costs. Mobile health (mHealth) applications have emerged as promising digital health tools for enhancing medication adherence through real-time monitoring, personalised reminders, artificial intelligence (AI)-driven interventions, and improved patient engagement. Objectives This scoping review examines the effectiveness, key features, and challenges of mHealth applications in promoting medication adherence across diverse patient populations and healthcare settings. It also seeks to identify research gaps and inform future development and implementation strategies for digital therapeutics. Eligibility Criteria Studies published between 2020 and 2024 were included if they investigated the use of mHealth applications to improve medication adherence and reported outcomes related to adherence rates, patient health indicators, or user engagement. Only studies with empirical data, including randomised controlled trials, observational studies, or mixed-methods research, were considered. Sources of Evidence A comprehensive search was conducted across Scopus, Web of Science, PubMed/MEDLINE, Google Scholar, and CINAHL databases. In total, 319 studies met the inclusion criteria following a systematic screening process based on Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR) guidelines. Charting Methods Data were extracted on study design, app functionalities, patient demographics, adherence outcomes, and barriers to adoption. The charted data were thematically synthesised to identify trends, success factors, and limitations. Results Among the included studies, 85% reported improved medication adherence associated with features such as personalised medication reminders, real-time health tracking, and AI-powered adherence prediction. Clinical outcomes were also frequently observed, including improved blood pressure, glucose control, and patient-reported quality of life. Key barriers to adoption included limited digital literacy, concerns about data privacy, socioeconomic disparities, and a lack of integration with electronic health records (EHRs). Conclusions mHealth applications show significant potential to improve medication adherence and health outcomes, particularly in the management of chronic diseases. However, inclusive design, robust data privacy frameworks, and evidence-based implementation strategies are essential for scalability and sustained impact. Future research should focus on long-term effectiveness, cost-efficiency, and integration of mHealth tools within broader healthcare systems.},
  archive      = {J_PEERJCS},
  author       = {Xuye Wang and Beibei Wang and Wan Yin Tew and Xiaoning Yang and Xiangyang Xu and Yifang Gao and Yongjia Chen and Mun Fei Yam},
  doi          = {10.7717/peerj-cs.3190},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3190},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Exploring mHealth interventions for medication management: A scoping review of digital tools, implementation barriers, and patient outcomes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A GAN-based approach to solar radiation prediction: Data augmentation and model optimization for saudi arabia. <em>PEERJCS</em>, <em>11</em>, e3189. (<a href='https://doi.org/10.7717/peerj-cs.3189'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Accurate solar radiation prediction is essential for optimizing renewable energy systems but remains challenging due to data scarcity and variability. This study addresses these challenges by employing generative adversarial networks (GANs) to generate high-quality synthetic solar radiation data. Methods A novel framework was developed that integrates GAN-generated synthetic data with machine learning and deep learning models, including CNN-LSTM architectures. These models were trained and evaluated using augmented datasets to improve predictive accuracy and adaptability across diverse climatic zones. Results Models trained on augmented datasets exhibited significant improvements, with root mean square error (RMSE) reduced by 15.2% and mean absolute error (MAE) decreased by 19.9%. The framework effectively bridged data gaps and enhanced model generalization, enabling applicability across various climatic regions in Saudi Arabia. Conclusions The proposed framework facilitates practical applications such as photovoltaic system optimization, grid stability enhancement, and resource planning. By aligning with Saudi Arabia’s Vision 2030 and global renewable energy objectives, this study presents a scalable and adaptable approach to advancing renewable energy systems. However, challenges such as computational complexity and hyperparameter sensitivity warrant further investigation, providing a robust pathway toward sustainable energy futures worldwide.},
  archive      = {J_PEERJCS},
  author       = {Abdalla Alameen and Sultan Mesfer Aldossary},
  doi          = {10.7717/peerj-cs.3189},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3189},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A GAN-based approach to solar radiation prediction: Data augmentation and model optimization for saudi arabia},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of tennis auxiliary teaching system based on reinforcement learning and multi-feature fusion. <em>PEERJCS</em>, <em>11</em>, e3188. (<a href='https://doi.org/10.7717/peerj-cs.3188'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To accurately identify and evaluate tennis movements, a tennis auxiliary teaching system based on reinforcement learning and multi-feature fusion was designed by combining deep learning methods with tennis-related knowledge to recognize and evaluate tennis movements accurately. The algorithm first extracts human skeletal joint points from a video sequence using a human pose-recognition algorithm. Reinforcement learning is then used to extract and optimize the keyframes. Second, genetic algorithms were used to fuse the different features. The results demonstrate that the proposed tennis action recognition method achieves a classification accuracy of 98.45% for four types of tennis subactions. Its generalization ability is greater than that of graph convolutional network-based techniques, such as AGCN and ST-GCN. Lastly, following action categorization, the suggested scoring method based on dynamic temporal warping may deliver accurate and real-time assessment ratings for corresponding actions, lowering the effort of tennis instructors and significantly raising the standard of tennis instruction.},
  archive      = {J_PEERJCS},
  author       = {Shiquan Zhang and Chaohong Gan},
  doi          = {10.7717/peerj-cs.3188},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3188},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of tennis auxiliary teaching system based on reinforcement learning and multi-feature fusion},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on the relationship and prediction model between nighttime lighting data, pm2.5 data, and urban GDP. <em>PEERJCS</em>, <em>11</em>, e3185. (<a href='https://doi.org/10.7717/peerj-cs.3185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the discovery of electricity and the widespread adoption of lighting technology, the extensive application of electricity has greatly increased productivity, making night-time factory production possible. At the same time, the rapid expansion of factories has led to a significant increase in particulate matter 2.5 (PM2.5) in the air. However, economic development heavily relies on lighting and factory production. To address this issue, researchers have focused on predicting urban gross domestic product (GDP) through night-time lights and PM2.5, but current studies often focus on the impact of a single factor on GDP, leaving room for improvement in model accuracy. In response to this problem, this article proposes the Relationship and Prediction Model between Night Light Data, PM2.5, and Urban GDP (R&P-NLPG model). Firstly, night light data, PM2.5 data, and GDP data are collected and preprocessed. Secondly, correlation analysis is conducted to analyze the correlation between data features. Then, data fusion methods are used to integrate features between night-time data and PM2.5 data, forming the third data features. Next, a neural network is constructed to establish a functional relationship between features and GDP. Finally, the trained neural network model is used to predict GDP. The experimental results demonstrate that the predictive capability of the R&P-NLPG model outperforms GDP prediction models constructed with single-feature input and existing multi-feature input.},
  archive      = {J_PEERJCS},
  author       = {Sen Chen and Junke Li},
  doi          = {10.7717/peerj-cs.3185},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3185},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on the relationship and prediction model between nighttime lighting data, pm2.5 data, and urban GDP},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A measurement framework to assess software maturity models. <em>PEERJCS</em>, <em>11</em>, e3183. (<a href='https://doi.org/10.7717/peerj-cs.3183'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software maturity models can be utilized by organizations to evaluate and enhance their development processes. Established and recognized models such as the Capability Maturity Model Integrated (CMMI) and ISO/IEC 15504 (Software Process Improvement and Capability Determination (SPICE)) have proven their value. However, many new software maturity models exist, and their quality and potential value remain questionable until they are properly assessed before adoption. Without such an assessment, organizations can implement poor or ineffective models, resulting in wasted resources and failed improvement initiatives. Our research aims to address this challenge by developing a measurement framework based on ISO/IEC 15504-3 standards to assess the quality of developed software maturity models. We derived our quality assessment criteria through literature analysis, analyzing four main categories: basic model information, structural design, assessment methods, and implementation support. After developing this framework, we validated it with expert reviews to assess its design and usability and through a series of case studies. Feedback from academics and industry practitioners confirmed the framework’s utility, especially recognizing its clear structure and comprehensiveness of evaluation criteria. Case studies also revealed the framework’s effectiveness in identifying strengths and areas of improvement, finding that evaluated models had quality scores ranging from 83.3% to 93.2%. Our study enhances software maturity models’ practical utility and adoption across different software contexts, providing professionals and academics with a structured way to evaluate and enhance maturity models.},
  archive      = {J_PEERJCS},
  author       = {Reem Alshareef and Mohammad Alshayeb and Mahmood Niazi and Sajjad Mahmood},
  doi          = {10.7717/peerj-cs.3183},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3183},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A measurement framework to assess software maturity models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble techniques for detecting profile cloning attacks in online social networks. <em>PEERJCS</em>, <em>11</em>, e3182. (<a href='https://doi.org/10.7717/peerj-cs.3182'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting cloned and impersonated profiles on online social networks (OSNs) has become an increasingly critical challenge, particularly with the proliferation of AI-generated content that closely emulates human communication patterns. Traditional identity deception detection methods are proving inadequate against adversaries who exploit large language models (LLMs) to craft syntactically accurate and semantically plausible fake profiles. This article focuses on the detection of profile cloning on LinkedIn by introducing a multi-stage, content-based detection framework that classifies profiles into four distinct categories: legitimate profiles, human-cloned profiles, LLM-generated legitimate profiles, and LLM-generated cloned profiles. The proposed framework integrates multiple analytical layers, including semantic representation learning through attention-based section embedding aggregation, linguistic style modeling using stylometric-perplexity features, anomaly scoring via cluster-based outlier detection, and ensemble classification through out-of-fold stacking. Experiments conducted on a publicly available dataset comprising 3,600 profiles demonstrate that the proposed meta-ensemble model consistently outperforms competitive baselines, achieving macro-averaged accuracy, precision, recall, and F1-scores above 96%. These results highlight the effectiveness of leveraging a combination of semantic, stylistic, and probabilistic signals to detect both human-crafted and artificial intelligence (AI)-generated impersonation attempts. Overall, this work presents a robust and scalable content-driven methodology for identity deception detection in contemporary OSNs.},
  archive      = {J_PEERJCS},
  author       = {Irfan Mohiuddin and Ahmad Almogren},
  doi          = {10.7717/peerj-cs.3182},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3182},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Ensemble techniques for detecting profile cloning attacks in online social networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HEMF: An adaptive hierarchical enhanced multi-attention feature fusion framework for cross-scale medical image classification. <em>PEERJCS</em>, <em>11</em>, e3181. (<a href='https://doi.org/10.7717/peerj-cs.3181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image classification is essential for contemporary clinical diagnosis and decision support systems. However, medical images generally have similar inter-class features and complex structure patterns, making it a challenging task. While both local and global features are critical for noise reduction and discriminative pattern extraction in medical images, conventional approaches exhibit limitations. Specifically, convolutional neural networks (CNNs) focus on local features extraction but lack a comprehensive understanding of global semantic. Conversely, vision transformers (ViTs) can model long-range feature dependencies but may cause disruption to local features. To address these limitations, we propose Hierarchical Enhanced Multi-attention Feature (HEMF), an adaptive hierarchical enhanced multi-attention feature fusion framework to synergistically extract and fuse multi-scale local and global features. It comprises two core components: (1) the enhanced local and global feature extraction modules to extract multi-scale local and global features in parallel; (2) the hierarchical enhanced feature fusion module integrating a novel attention mechanism named Mixed Attention (MA) and a novel inverted residual block named Squeezed Inverted Residual Multi-Layer Perceptron (SIRMLP) to effectively fuse multi-scale features. Experimental results demonstrate that with nearly minimal model parameters compared to other advanced models, HEMF achieves the accuracy and F1-score of 87.34% and 78.89% on the ISIC2018 dataset, 87.03% and 87.02% on the Kvasir dataset, and 82.26% and 82.20% on the COVID-19 CT dataset, which are the state-of-the-art performance. Our code is open source and available from https://github.com/Esgjgd/HEMF.},
  archive      = {J_PEERJCS},
  author       = {Jingdong He and Qiang Shi and Jun Ma and Dacheng Shi and Tie Min},
  doi          = {10.7717/peerj-cs.3181},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3181},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HEMF: An adaptive hierarchical enhanced multi-attention feature fusion framework for cross-scale medical image classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection for emotion recognition in speech: A comparative study of filter and wrapper methods. <em>PEERJCS</em>, <em>11</em>, e3180. (<a href='https://doi.org/10.7717/peerj-cs.3180'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is essential for enhancing the performance and reducing the complexity of speech emotion recognition models. This article evaluates various feature selection methods, including correlation-based (CB), mutual information (MI), and recursive feature elimination (RFE), against baseline approaches using three different feature sets: (1) all available features (Mel-frequency cepstral coefficients (MFCC), root mean square energy (RMS), zero crossing rate (ZCR), chromagram, spectral centroid frequency (SCF), Tonnetz, Mel spectrogram, and spectral bandwidth), totaling 170 features; (2) a five-feature subset (MFCC, RMS, ZCR, Chromagram, and Mel spectrogram), totaling 163 features; and (3) a six-feature subset (MFCC, RMS, ZCR, SCF, Tonnetz, and Mel spectrogram), totaling 157 features. Methods are compared based on precision, recall, F1-score, accuracy, and the number of features selected. Results show that using all features yields an accuracy of 61.42%, but often includes irrelevant data. MI with 120 features achieves the highest performance, with precision, recall, F1-score, and accuracy at 65%, 65%, 65%, and 64.71%, respectively. CB methods with moderate thresholds also perform well, balancing simplicity and accuracy. RFE methods improve consistently with more features, stabilizing around 120 features.},
  archive      = {J_PEERJCS},
  author       = {Alaa Altheneyan and Aseel Alhadlaq},
  doi          = {10.7717/peerj-cs.3180},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3180},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Feature selection for emotion recognition in speech: A comparative study of filter and wrapper methods},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning for detecting age and sex effects on corpus callosum volume. <em>PEERJCS</em>, <em>11</em>, e3179. (<a href='https://doi.org/10.7717/peerj-cs.3179'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aim This study aims to investigate whether machine learning (ML) could identify the volumetric changes in the corpus callosum (CC) and its sub-regions (genu, body, and splenium) due to aging and sex differences. Material and Methods Brain magnetic resonance imaging (MRI) images obtained from 301 healthy male and female subjects were used in the study. The volume measurements of the corpus callosum and its sub-regions were calculated from MRI images using MRICloud software. The classifications of age (young/adult) and sex (female/male) were performed using the Classification Learner Tool in MATLAB 2020b (MathWorks, Natick, MA, USA). Classifiers including k-Nearest Neighbor (KNN), support vector machine (SVM), decision tree, naïve Bayes, logistic regression, and Ensemble classifiers were evaluated using 10-fold cross-validation. Results Machine learning classification revealed moderate accuracy in distinguishing sex (best: Fine Gaussian SVM, 65.4% accuracy, AUC = 0.60) and higher accuracy in distinguishing age groups (young/adult; best: Fine Gaussian SVM, 83.7% accuracy, area under the curve (AUC) = 0.67) based on corpus callosum and its sub-region volumes derived from MRI. Conclusion This preliminary data suggests that ML can provide indications to assess the impact of age and sex on corpus callosum volume based on MRI data. The observed classification accuracies, particularly for sex, suggest that larger datasets are needed to enhance the accuracy of these models in future investigations.},
  archive      = {J_PEERJCS},
  author       = {Handan Soysal and Niyazi Acer and Meltem Özdemir and Kazim Gumus},
  doi          = {10.7717/peerj-cs.3179},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3179},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Machine learning for detecting age and sex effects on corpus callosum volume},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on furniture image classification based on MobileNetNAK. <em>PEERJCS</em>, <em>11</em>, e3178. (<a href='https://doi.org/10.7717/peerj-cs.3178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the furniture industry, automatic classification of furniture images has become an important research area. However, this task faces several challenges, including complex image backgrounds, diverse furniture types, and varying forms. To address these issues, we propose a novel furniture image classification method, MobileNetNAK, based on the MobileNetV3 network. First, the method integrates a non-local attention module to capture non-local dependencies within images, significantly enhancing the model’s ability to extract key information. Second, the Adamax optimizer is employed to train the model. By adaptively adjusting the learning rate, it accelerates convergence and reduces the risk of overfitting. Third, the Kolmogorov–Arnold networks method is incorporated to decompose complex convolution operations into multiple simpler ones, thereby improving computational efficiency and feature extraction capabilities. Experimental results demonstrate that MobileNetNAK significantly improves classification performance in furniture image tasks. On Dataset 1, the model achieves improvements of 6.7%, 6.6%, 6.6%, and 6.6% in accuracy, precision, recall, and F1-score, respectively, compared to the baseline. On Dataset 2, the corresponding improvements are 2.7%, 2.4%, 2.7%, and 2.9%. Additionally, the model maintains a high inference speed of 147.80 fps, balancing performance with computational efficiency. These results highlight the strong adaptability and deployment potential of MobileNetNAK in multi-category and fine-grained furniture image classification tasks, offering a novel and effective solution for this domain.},
  archive      = {J_PEERJCS},
  author       = {Danyang Zhang and Yi Zhai and Peiyuan Li and Fan Yang and Runpeng Du},
  doi          = {10.7717/peerj-cs.3178},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3178},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on furniture image classification based on MobileNetNAK},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cluster-assisted differential evolution-based hybrid oversampling method for imbalanced datasets. <em>PEERJCS</em>, <em>11</em>, e3177. (<a href='https://doi.org/10.7717/peerj-cs.3177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance remains a significant challenge in machine learning, leading to biased models that favor the majority class while failing to accurately classify minority instances. Traditional oversampling methods, such as Synthetic Minority Over-sampling Technique (SMOTE) and its variants, often struggle with class overlap, poor decision boundary representation, and noise accumulation. To address these limitations, this study introduces ClusterDEBO, a novel hybrid oversampling method that integrates K-Means clustering with differential evolution (DE) to generate synthetic samples in a more structured and adaptive manner. The proposed method first partitions the minority class into clusters using the silhouette score to determine the optimal number of clusters. Within each cluster, DE-based mutation and crossover operations are applied to generate diverse and well-distributed synthetic samples while preserving the underlying data distribution. Additionally, a selective sampling and noise reduction mechanism is employed to filter out low-impact synthetic samples based on their contribution to classification performance. The effectiveness of ClusterDEBO is evaluated on 44 benchmark datasets using k-Nearest Neighbors (kNN), decision tree (DT), and support vector machines (SVM) as classifiers. The results demonstrate that ClusterDEBO consistently outperforms existing oversampling techniques, leading to improved class separability and enhanced classifier robustness. Moreover, statistical validation using the Friedman test confirms the significance of the improvements, ensuring that the observed gains are not due to random variations. The findings highlight the potential of cluster-assisted differential evolution as a powerful strategy for handling imbalanced datasets.},
  archive      = {J_PEERJCS},
  author       = {Muhammed Abdulhamid Karabiyik and Bahaeddin Turkoglu and Tunc Asuroglu},
  doi          = {10.7717/peerj-cs.3177},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3177},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A cluster-assisted differential evolution-based hybrid oversampling method for imbalanced datasets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parametric art creation platform design based on visual delivery and multimedia data fusion. <em>PEERJCS</em>, <em>11</em>, e3175. (<a href='https://doi.org/10.7717/peerj-cs.3175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of informational ascendancy, the discourse of artistic communication has transcended the confines of conventional physical domains and geographical boundaries, extending its purview ubiquitously across the global expanse. Consequently, the predominant mode of artistic interaction has evolved towards swift and extensive engagement through virtual platforms. However, this paradigm shift has given rise to the imperative task of meticulous categorization and labeling of an extensive corpus of artistic works, demanding substantial temporal and human resources. This article introduces an innovative bimodal time series classification model (BTSCM) network for the purpose of categorizing and labeling artworks on virtual platforms. Rooted in the foundational principles of visual communication and leveraging multimedia fusion technology, the proposed model proves instrumental in discerning categories within the realm of video content. The BTSCM framework initiates the classification of video data into constituent image and sound elements, employing the conceptual framework of visual communication. Subsequently, feature extraction for both forms of information is achieved through the application of Inflated 3D ConvNet and Mel frequency cepstrum coefficient (MFCC). The synthesis of these extracted features is orchestrated through a fusion of fully convolutional network (FCN), deep Q-network (DQN), and long short-term memory (LSTM), collectively manifesting as the BTSCM network model. This amalgamated network, shaped by the union of fully convolutional network (FCN), DQN, and LSTM, adeptly conducts information processing, culminating in the realization of high-precision video classification. Experimental findings substantiate the efficacy of the BTSCM framework, as evidenced by outstanding classification results across diverse video classification datasets. The classification recognition rate on the self-established art platform exceeds 90%, surpassing benchmarks set by multiple multimodal fusion recognition networks. These commendable outcomes underscore the BTSCM framework’s potential significance, providing a theoretical and methodological foundation for the prospective scrutiny and annotation of content within art creation platforms.},
  archive      = {J_PEERJCS},
  author       = {Qing Yun},
  doi          = {10.7717/peerj-cs.3175},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3175},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Parametric art creation platform design based on visual delivery and multimedia data fusion},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep layered network model based on multi-scale feature extraction and deep feature optimization for acute lymphoblastic leukemia anomaly detection. <em>PEERJCS</em>, <em>11</em>, e3174. (<a href='https://doi.org/10.7717/peerj-cs.3174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acute lymphoblastic leukemia (ALL), one of the common diseases of our day, is one of the most common hematological malignant diseases in childhood. Early diagnosis of ALL, which plays a critical role in medical diagnosis processes, is of great importance especially for the effective management of the treatment process of cancer patients. Therefore, ALL cells must be detected and classified correctly. Traditional methods used today prolong the detection and classification processes of cells, cause hematologists to interpret them according to their expertise, and delay medical decision-making processes. In this study, the performance of the hybrid model developed with different deep learning models for ALL diagnosis was comparatively analyzed. In the proposed ALL detection architecture, blood cell images were processed using the center-based cropping strategy and irrelevant areas in the images were automatically removed. The dataset was divided into training, validation, and test sets, and then features were extracted with deep hyperparameters for convolution, pooling, and activation layers using a model based on Xception architecture. The obtained features were optimized to the advanced Extreme Gradient Boosting (XGBoost) classifier and model classification results were obtained. The results showed that the proposed model achieved 98.88% accuracy. This high accuracy rate was compared with different hybrid models and it was seen that the model was more successful in detecting ALL disease compared to existing studies.},
  archive      = {J_PEERJCS},
  author       = {Gökalp Çınarer},
  doi          = {10.7717/peerj-cs.3174},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3174},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hybrid deep layered network model based on multi-scale feature extraction and deep feature optimization for acute lymphoblastic leukemia anomaly detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling the capabilities of vision transformers in sperm morphology analysis: A comparative evaluation. <em>PEERJCS</em>, <em>11</em>, e3173. (<a href='https://doi.org/10.7717/peerj-cs.3173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional sperm morphology assessment relies on manual visual inspection or semi-automated computer-aided sperm analysis (CASA) systems, which often require labor-intensive pre-processing steps. While recent machine learning approaches, particularly convolutional neural networks (CNNs), have improved feature extraction from sperm images, achieving a fully automated and highly accurate system remains challenging due to the complexity of sperm morphology and the need for specialized image adjustments. This study presents a novel, end-to-end automated sperm morphology analysis framework based on vision transformers (ViTs), which processes raw sperm images from two benchmark datasets-Human Sperm Head Morphology (HuSHeM) and Sperm Morphology Image Data Set (SMIDS)-without manual pre-processing. We conducted an extensive hyperparameter optimization study across eight ViT variants, evaluating learning rates, optimization algorithms, and data augmentation scales. Our experiments demonstrated that data augmentation significantly enhances ViT performance by improving generalization, particularly in limited-data scenarios. A comparative analysis of CNNs, hybrid models, and pure ViTs revealed that transformer-based architectures consistently outperform traditional methods. The BEiT_Base model achieved state-of-the-art accuracies of 92.5% (SMIDS) and 93.52% (HuSHeM), surpassing prior CNN-based approaches by 1.63% and 1.42%, respectively. Statistical significance (p < 0.05, t-test) confirmed these improvements. Visualization techniques (Attention Maps, Grad-CAM) further validated ViTs’ superior ability to capture long-range spatial dependencies and discriminative morphological features, such as head shape and tail integrity. Our work bridges a critical gap in reproductive medicine by delivering a scalable, fully automated solution that eliminates manual intervention while improving diagnostic accuracy. These findings underscore the potential of transformer-based models in clinical andrology, with implications for broader applications in biomedical image analysis.},
  archive      = {J_PEERJCS},
  author       = {Abdulsamet Aktas and Gorkem Serbes and Hamza Osman Ilhan},
  doi          = {10.7717/peerj-cs.3173},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3173},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Unveiling the capabilities of vision transformers in sperm morphology analysis: A comparative evaluation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Skin cancer classification using novel fairness based federated learning algorithm. <em>PEERJCS</em>, <em>11</em>, e3171. (<a href='https://doi.org/10.7717/peerj-cs.3171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous skin conditions fall under the category of dermatological diseases, which make proper diagnosis and treatment planning difficult. Our research centres on tackling these obstacles within the framework of federated learning, a decentralized approach to machine learning. We provide a unique strategy that combines class-weighting strategies to reduce the negative effects of different data distributions among decentralized clients by leveraging the federated average algorithm. We assessed the effectiveness of our approach using the Fitzpatrick 17k dataset, an extensive collection encompasses a wide range of skin conditions. With its realistic representation of dermatological diagnosis scenarios, the dataset provides a solid foundation for training and testing federated learning models. One of the main issues driving our research is the ubiquitous problem of class imbalance within federated learning. When client data distributions are uneven, class imbalance can result in biased model predictions and subpar performance. To solve this issue and enhance model performance, we have incorporated class-weighting approaches into the federated average architecture. We show through thorough experimentation that our strategy is useful for improving federated learning models’ learning performance. Our methodology presents a possible solution to the class imbalance issue in federated learning situations by reducing bias and increasing prediction accuracy. Our study further emphasizes the significance of iterative refinement methods for optimizing federated average weights and fine-tuning model parameters. The results of our study show that the model performance has improved significantly, with an average accuracy of almost 92% across all categories. These results highlight our classification model’s potential usefulness for dermatological diagnosis and treatment planning in clinical settings. Furthermore, this study contributes valuable insights into the application of federated learning for dermatological disease classification, paving the way for future advancements in addressing key challenges such as data privacy, distribution heterogeneity, and model fairness in medical imaging.},
  archive      = {J_PEERJCS},
  author       = {Awais Karni and Qamar Abbas and Jamil Ahmad and Abdul Khader Jilani Saudagar},
  doi          = {10.7717/peerj-cs.3171},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3171},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Skin cancer classification using novel fairness based federated learning algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating inappropriate concepts in text-to-image generation with attention-guided image editing. <em>PEERJCS</em>, <em>11</em>, e3170. (<a href='https://doi.org/10.7717/peerj-cs.3170'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-image generative models have recently garnered a significant surge due to their ability to produce diverse images based on given text prompts. However, concerns regarding the occasional generation of inappropriate, offensive, or explicit content have arisen. To address this, we propose a simple yet effective method that leverages attention map to selectively suppress inappropriate concepts during image generation. Unlike existing approaches that often sacrifice original image context or demand substantial computational overhead, our method preserves image integrity without requiring additional model training or extensive engineering effort. To evaluate our method, we conducted comprehensive quantitative assessments on inappropriateness reduction, text fidelity, image consistency, and computational cost, alongside an online human perceptual study involving 20 participants. The results from our statistical analysis demonstrated that our method effectively removes inappropriate content while preserving the integrity of the original images with high computational efficiency.},
  archive      = {J_PEERJCS},
  author       = {Jiyeon Oh and Jae-Yeop Jeong and Yeong-Gi Hong and Jin-Woo Jeong},
  doi          = {10.7717/peerj-cs.3170},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3170},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Mitigating inappropriate concepts in text-to-image generation with attention-guided image editing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperparameter optimization of XGBoost and hybrid CnnSVM for cyber threat detection using modified harris hawks algorithm. <em>PEERJCS</em>, <em>11</em>, e3169. (<a href='https://doi.org/10.7717/peerj-cs.3169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating complexity of cyber threats in smart microgrids necessitates advanced detection frameworks to counter sophisticated attacks. Existing methods often underutilize optimization techniques like Harris hawks optimization (HHO) and struggle with class imbalance in cybersecurity datasets. This study proposes a novel framework integrating HHO with extreme gradient boosting (XGBoost) and a hybrid convolutional neural network with support vector machine (Cnn-SVM) to enhance cyber threat detection. Using the distributed denial of service (DDoS) botnet attack and KDD CUP99 datasets, the proposed models leverage HHO for hyperparameter optimization, achieving accuracies of 99.97% and 99.99%, respectively, alongside improved area under curve (AUC) metrics. These results highlight the framework’s ability to capture complex nonlinearities and address class imbalance through RandomOverSampler. The findings demonstrate the potential of HHO-optimized models to advance automated threat detection, offering robust and scalable solutions for securing critical infrastructures.},
  archive      = {J_PEERJCS},
  author       = {Haitham Elwahsh and Ali Bakhiet and Tarek Khalifa and Julian Hoxha and Maazen Alsabaan and Mohamed I. Ibrahem and Mahmoud Elwahsh and Engy El-shafeiy},
  doi          = {10.7717/peerj-cs.3169},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3169},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hyperparameter optimization of XGBoost and hybrid CnnSVM for cyber threat detection using modified harris hawks algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCGAN-based synthetic image generation of denim jeans defects. <em>PEERJCS</em>, <em>11</em>, e3167. (<a href='https://doi.org/10.7717/peerj-cs.3167'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Automated defect detection in denim jeans manufacturing is crucial for maintaining quality control efficiency. However, for automated defect detection of denim jeans, machine learning algorithms suffer due to limited data availability because manufacturing industry remains reluctant to share data due to privacy concerns. Moreover, remote manufacturing units make it more difficult to gather necessary defected images. Furthermore, trained personnel are required to capture standard images for training effective models. Traditional image augmentation approaches extend the datasets from seed images; however, there is a lack in image diversification and they do not expand data distribution and thus may lead to overfitting. Deep learning models, especially generative adversarial networks, have the potential to provide effective solutions for industrial problems, such as synthetic image generation for denim jeans defect detection. Methods This article proposes the use of a deep convolutional generative adversarial network (DCGAN) for generating diversified and realistic synthetic images of common denim jeans defects, including broken loops, broken stitches, skipped stitches and twisted legs. The DCGAN model was trained on an initial dataset of 3,930 defective images and subsequently augmented using techniques such as flipping, random zooming, and color space augmentation. Results The generated synthetic images were subjectively validated by domain experts, achieving an average accuracy of 81.5% Objective evaluation using the Fréchet inception distance metric also demonstrated the effectiveness of the proposed approach, with scores of 12.26, 6.75, 7.68 and 27.59 for broken loop, broken stitch, skipped stitch and twisted leg defects, respectively. This work not only contributes to addressing the challenge of data scarcity in defect detection but also paves the way for more accurate automated defect detection systems in denim jeans manufacturing.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Naeem and Qaisar Abbas and Haseeb Ahmad and Muhammad Salman Naeem and Mutlaq B. Aldajani and Hussain Dawood and Muhammad Awais Hussain},
  doi          = {10.7717/peerj-cs.3167},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3167},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DCGAN-based synthetic image generation of denim jeans defects},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SENSH: A blockchain-based searchable encrypted data sharing scheme in smart healthcare. <em>PEERJCS</em>, <em>11</em>, e3166. (<a href='https://doi.org/10.7717/peerj-cs.3166'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of the Internet of Things technology has led to a boom in the adoption of intelligent healthcare management systems in the healthcare industry. However, it has also highlighted key issues such as security, privacy, and efficient query of medical data. Traditional methods for querying medical data suffer from severe data leakage risks, low query performance, and excessive storage space. This article proposes a comprehensive Secure ENcrypted Search for Health Scheme (SENSH) solution based on consortium blockchain and searchable encryption to address these challenges. SENSH enables efficient authorization management through Bloom filters, ensuring fast querying of large datasets by authorized users while saving storage space. It uses off-chain Advanced Encryption Standard (AES) and on-chain storage management for data protection, significantly reducing the likelihood of data exposure. The system is also enhanced with event triggering and logging mechanisms to support real-time monitoring and data tracing to meet audit compliance requirements. It provides version control and timestamping to accommodate dynamic data updates, employs an obfuscationfactor to prevent tag-based original data content leakage, and supports dynamic updating of tags to accommodate different access requirements. Experimental results show that SENSH excels in authorization management, privacy protection, defense against tampering, and anti-replay and Distributed Denial of Service (DDoS). Compared with existing schemes, SENSH has significant advantages in terms of gas consumption, computation cost, and execution time. It is particularly suited for the protection and efficient query of medical and health data.},
  archive      = {J_PEERJCS},
  author       = {Song Luo and Lihuan Tan and Tan Hu and Maoshuang Hu},
  doi          = {10.7717/peerj-cs.3166},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3166},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SENSH: A blockchain-based searchable encrypted data sharing scheme in smart healthcare},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing privacy-preserving brain tumor classification with adaptive reputation-aware federated learning and homomorphic encryption. <em>PEERJCS</em>, <em>11</em>, e3165. (<a href='https://doi.org/10.7717/peerj-cs.3165'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumor diagnosis using magnetic resonance imaging (MRI) scans is critical for improving patient survival rates. However, automating the analysis of these scans faces significant challenges, including data privacy concerns and the scarcity of large, diverse datasets. A potential solution is federated learning (FL), which enables cooperative model training among multiple organizations without requiring the sharing of raw data; however, it faces various challenges. To address these, we propose Federated Adaptive Reputation-aware aggregation with CKKS (Cheon-Kim-Kim-Song) Homomorphic encryption (FedARCH), a novel FL framework designed for a cross-silo scenario, where client weights are aggregated based on reputation scores derived from performance evaluations. Our framework incorporates a weighted aggregation method using these reputation scores to enhance the robustness of the global model. To address sudden changes in client performance, a smoothing factor is introduced, while a decay factor ensures that recent updates have a greater influence on the global model. These factors work together for dynamic performance management. Additionally, we address potential privacy risks from model inversion attacks by implementing a simplified and computationally efficient CKKS homomorphic encryption, which allows secure operations on encrypted data. With FedARCH, encrypted model weights of each client are multiplied by a plaintext reputation score for weighted aggregation. Since we are multiplying ciphertexts by plaintexts, instead of ciphertexts, the need for relinearization is eliminated, efficiently reducing the computational overhead. FedARCH achieved an accuracy of 99.39%, highlighting its potential in distinguishing between brain tumor classes. Several experiments were conducted by adding noise to the clients’ data and varying the number of noisy clients. An accuracy of 94% was maintained even with 50% of noisy clients at a high noise level, while the standard FL approach accuracy dropped to 33%. Our results and the security analysis demonstrate the effectiveness of FedARCH in improving model accuracy, its robustness to noisy data, and its ability to ensure data privacy, making it a viable approach for medical image analysis in federated settings.},
  archive      = {J_PEERJCS},
  author       = {Swetha Ghanta and Prasanthi Boyapati and Sujit Biswas and Ashok K. Pradhan and Saraju P. Mohanty},
  doi          = {10.7717/peerj-cs.3165},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3165},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing privacy-preserving brain tumor classification with adaptive reputation-aware federated learning and homomorphic encryption},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning models for damage type detection in wind turbines. <em>PEERJCS</em>, <em>11</em>, e3163. (<a href='https://doi.org/10.7717/peerj-cs.3163'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents deep learning models that are frequently used in the literature for the detection and classification of damage types in wind turbines and a new deep learning model (SatNET) that offers computational efficiency and rapid inference. Wind turbines, which are critical components of renewable energy systems, are sensitive to various damages (paint damage, erosion, serration, vortex, and vortex damage) that may endanger their operational efficiency and lifespan. The dataset consists of 1,794 high-resolution images taken under different weather conditions and angles, including damage and types. The images were increased by four times to 7,176 images using data augmentation techniques. Damage and types were detected using the developed SatNET deep learning model, 11 deep learning models, and the Faster Region-based Convulational Neural Network (R-CNN) object detection algorithm. Each of the models was evaluated with average sensitivity. Accordingly, SatNET achieved avarage precision (AP) values of 55.7% for paint damage, 76.7% for erosion, 95.2% for serration, 66.1% for vortex, and 27.3% for vortex damage. It demonstrated superior performance when compared to deep learning models frequently used in the literature, such as ResNet50 and VGG19. In addition, it has been shown that the model requires less computational cost than other models, with a memory requirement of 192 MB. The results show that SatNET’s computational efficiency and accuracy are competitive with other models. The model is suitable for systems with limited memory and computational capacity, which require real-time operation, and for systems with resource constraints. The results obtained can contribute to sustainability in renewable energy production by providing low-cost monitoring of damage and types in wind turbines.},
  archive      = {J_PEERJCS},
  author       = {Ferdi Doğan and Saadin Oyucu and Emre Bicer and Ahmet Aksoz},
  doi          = {10.7717/peerj-cs.3163},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3163},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning models for damage type detection in wind turbines},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging deep learning and ensemble learning for air quality forecasting in smart urban environment. <em>PEERJCS</em>, <em>11</em>, e3162. (<a href='https://doi.org/10.7717/peerj-cs.3162'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban pollution has become a significant issue for the whole world, specifically for underdeveloped nations. This pollution poses significant challenges to public health, economic stability and environmental sustainability. The rapid growth of urbanization and industries, and inadequate regulatory frameworks has led to the deterioration of air, contamination of water and soil pollution. Major urban centers such as Lahore remain at the top among the most polluted cities, globally, with adverse effects such as rising respiratory diseases, contaminated water supplies and environmental degradation. The countries have proposed various policies and regulatory framework; however, these attempts do not reverse the trend of exacerbating urban pollution due to the lack of monitoring and measurable goals. This research proposes deep learning and ensemble learning approach to track pollution levels efficiently that could be utilized for policymaking and governance, supporting real time monitoring and data driven interventions. The findings indicate decision tree and random forest gave the most reliable and accurate air quality prediction, achieving an accuracy of 0.99 and 0.98, respectively, for particulate matter 2.5 (PM2.5) and particulate matter 10 (PM10), with high precision in classification across all categories. The smog-predict app has been made available via a user-friendly webserver at: https://smog-pred.streamlit.app.},
  archive      = {J_PEERJCS},
  author       = {Hafiz Muhammad Qadir and Muhammad Taseer Suleman and Rafaqat Alam Khan and Jianqiang Li and Tariq Mahmood and Tanzila Saba},
  doi          = {10.7717/peerj-cs.3162},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3162},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging deep learning and ensemble learning for air quality forecasting in smart urban environment},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quantification of left ventricular mass in multiple views of echocardiograms using model-agnostic meta learning in a few-shot setting. <em>PEERJCS</em>, <em>11</em>, e3161. (<a href='https://doi.org/10.7717/peerj-cs.3161'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Reliable measurement of left ventricular mass (LVM) in echocardiography is essential for early detection of left ventricular dysfunction, coronary artery disease, and arrhythmia risk, yet growing patient volumes have created critical shortage of experts in echocardiography. Recent deep learning approaches reduce inter‐operator variability but require large, fully labeled datasets for each standard view—an impractical demand in many clinical settings. Methods To overcome these limitations, we propose a heatmap-based point-estimation segmentation model trained via model-agnostic meta-learning (MAML) for few-shot LVM quantification across multiple echocardiographic views. Our framework adapts rapidly to new views by learning a shared representation and view-specific head performing K inner-loop updates, and then meta-updating in the outer loop. We used the EchoNet-LVH dataset for the PLAX view, the TMED-2 dataset for the PSAX view and the CAMUS dataset for both the apical 2-chamber and apical 4-chamber views under 1-, 5-, and 10-shot scenarios. Results As a result, the proposed MAML methods demonstrated comparable performance using mean distance error, mean angle error, successful distance error and spatial angular similarity in a few-shot setting compared to models trained with larger labeled datasets for each view of the echocardiogram.},
  archive      = {J_PEERJCS},
  author       = {Yeong Hyeon Kim and Donghoon Kim and Jin Young Youm and Jiyoon Won and Seola Kim and Woohyun Park and Yisak Kim and Dongheon Lee},
  doi          = {10.7717/peerj-cs.3161},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3161},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Quantification of left ventricular mass in multiple views of echocardiograms using model-agnostic meta learning in a few-shot setting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel deep learning based approach with hyperparameter selection using grey wolf optimization for leukemia classification and hematologic malignancy detection. <em>PEERJCS</em>, <em>11</em>, e3160. (<a href='https://doi.org/10.7717/peerj-cs.3160'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional diagnostic methods of leukemia, a blood cancer disease, are based on visual assessment of white cells in microscopic peripheral blood smears, and as a result, they are arbitrary, laborious, and susceptible to errors. This study proposes a new automated deep learning-based framework for accurately classifying leukemia cancer. A novel lightweight algorithm based on the hyperbolic sin function has been designed for contrast enhancement. In the next step, we proposed a customized convolutional neural network (CNN) model based on a parallel inverted dual self-attention network (PIDSAN4), and a tiny16 Vision Transformer (ViT) has been employed. The hyperparameters were tuned using the grey wolf optimization and then used to train the models. The experiment is carried out on a publicly available leukemia microscopic images dataset, and the proposed model achieved 0.913 accuracy, 0.892 sensitivity, 0.925 specificity, 0.883 precision, 0.894 F-measure, and 0.901 G-mean. The results were compared with state-of-the-art pre-trained models, showing that the proposed model improved accuracy.},
  archive      = {J_PEERJCS},
  author       = {Shams ur Rehman and Robertas Damaševicius and Hassan Al Sukhni and Abeer Aljohani and Ameer Hamza and Deema Mohammed Alsekait and Diaa Salama AbdElminaam},
  doi          = {10.7717/peerj-cs.3160},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3160},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel deep learning based approach with hyperparameter selection using grey wolf optimization for leukemia classification and hematologic malignancy detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A machine learning assistant for detecting fraudulent activities in synchronous online programming exams. <em>PEERJCS</em>, <em>11</em>, e3159. (<a href='https://doi.org/10.7717/peerj-cs.3159'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of online learning has made education more accessible but has also introduced significant challenges in maintaining academic integrity, particularly during online exams. For certain types of exams, students are prohibited from connecting to the Internet to prevent them from accessing unauthorized resources, utilizing generative artificial intelligence tools, or engaging in other forms of cheating. However, in online exams, students must remain connected to the Internet. Most existing online proctoring systems rely on various devices to monitor students’ actions and environments during the exam, focusing on tracking physical behavior, such as facial expressions, eye movements, and the presence of unauthorized materials, rather than analyzing the students’ work within their computers. This often requires human review to determine whether students are engaging in unauthorized actions. This article presents the development and evaluation of a machine-learning-based assistant designed to assist instructors in detecting fraudulent activities in real-time during online programming exams. Our system leverages a convolutional neural network (CNN) followed by a recurrent neural network (RNN) and a dense layer to analyze sequences of screenshot frames captured from students’ screens during exams. The system achieves an accuracy of 95.18% and an F2-score of 94.2%, prioritizing recall to emphasize detecting cheating instances, while minimizing false positives. Notably, data augmentation and class-weight adjustments during training significantly enhanced the model’s performance, while transfer learning and alternative loss functions did not provide additional improvements. In post-deployment feedback, instructors expressed high satisfaction with the system’s ability to assist in the rapid detection of cheating, reinforcing the potential of machine learning to support real-time monitoring in large-scale online exams.},
  archive      = {J_PEERJCS},
  author       = {Francisco Ortin and Alonso Gago and Jose Quiroga and Miguel Garcia},
  doi          = {10.7717/peerj-cs.3159},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3159},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A machine learning assistant for detecting fraudulent activities in synchronous online programming exams},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-stream transformer approach for pain assessment using visual-physiological data modeling. <em>PEERJCS</em>, <em>11</em>, e3158. (<a href='https://doi.org/10.7717/peerj-cs.3158'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic pain assessment involves accurately recognizing and quantifying pain, dependent on the data modality that may originate from various sources such as video and physiological signals. Traditional pain assessment methods rely on subjective self-reporting, which limits their objectivity, consistency, and overall effectiveness in clinical settings. While machine learning offers a promising alternative, many existing approaches rely on a single data modality, which may not adequately capture the multifaceted nature of pain-related responses. In contrast, multimodal approaches can provide a more comprehensive understanding by integrating diverse sources of information. To address this, we propose a dual-stream framework for classifying physiological and behavioral correlates of pain that leverages multimodal data to enhance robustness and adaptability across diverse clinical scenarios. Our framework begins with masked autoencoder pre-training for each modality: facial video and multivariate bio-psychological signals, to compress the raw temporal input into meaningful representations, enhancing their ability to capture complex patterns in high-dimensional data. In the second stage, the complete classifier consists of a dual hybrid positional encoding embedding and cross-attention fusion. The pain assessment evaluations reveal our model’s superior performance on the AI4Pain and BioVid datasets for electrode-based and heat-induced settings.},
  archive      = {J_PEERJCS},
  author       = {Minh-Duc Nguyen and Hyung-Jeong Yang and Duy-Phuong Dao and Soo-Hyung Kim and Seung-Won Kim and Ji-Eun Shin and Ngoc Anh Thi Nguyen and Trong-Nghia Nguyen},
  doi          = {10.7717/peerj-cs.3158},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3158},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dual-stream transformer approach for pain assessment using visual-physiological data modeling},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent educational systems based on adaptive learning algorithms and multimodal behavior modeling. <em>PEERJCS</em>, <em>11</em>, e3157. (<a href='https://doi.org/10.7717/peerj-cs.3157'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of artificial intelligence, the demand for personalized and adaptive learning has driven the development of intelligent educational systems. This article proposes a novel adaptive learning-driven architecture that combines multimodal behavioral modeling and personalized educational resource recommendation. Specifically, we introduce a multimodal fusion (MMF) algorithm to extract and integrate heterogeneous learning behavior data—including text, images, and interaction logs—via stacked denoising autoencoders and Restricted Boltzmann Machines. We further design an adaptive learning (AL) module that constructs a student-resource interaction graph and dynamically recommends learning materials using a graph-enhanced contrastive learning strategy and a dual-MLP-based enhancement mechanism. Extensive experiments on the Students’ Academic Performance Dataset demonstrate that our method significantly reduces prediction error (mean absolute error (MAE) = 0.01, mean squared error (MSE) = 0.0053) and achieves high precision (95.3%) and recall (96.7%). Ablation studies and benchmark comparisons validate the effectiveness and generalization ability of both MMF and AL. The system exhibits strong scalability, real-time responsiveness, and high user satisfaction, offering a robust technical foundation for next-generation AI-powered educational platforms.},
  archive      = {J_PEERJCS},
  author       = {Yuwei Li and Botao Lu},
  doi          = {10.7717/peerj-cs.3157},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3157},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Intelligent educational systems based on adaptive learning algorithms and multimodal behavior modeling},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive approach for waste management with GAN-augmented classification. <em>PEERJCS</em>, <em>11</em>, e3156. (<a href='https://doi.org/10.7717/peerj-cs.3156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image processing and computer vision highly rely on data augmentation in machine learning models to increase the diversity and variability within training datasets for better performance. One of the most promising and widely used applications of data augmentation is in classifying waste object images. This research focuses on augmenting waste object images with generative adversarial networks (GANS). Here deep convolutional GAN (DCGAN), an extension of GAN is utilized, which uses convolutional and convolutional-transpose layers for better image generation. This approach helps generate realism and variability in images. Furthermore, object detection and classification techniques are used. By utilizing ensemble learning techniques with DenseNet121, ConvNext, and Resnet101, the network can accurately identify and classify waste objects in images, thereby contributing to improved waste management practices and environmental sustainability. With ensemble learning, a notable accuracy of 99.80% was achieved. Thus, by investigating the effectiveness of these models in conjunction with data augmentation techniques, this novel approach of GAN-based augmentation cooperated with ensemble models aims to provide valuable insights into optimizing waste object identification processes for real-world applications. Future work will focus on better data augmentation methods with other types of GANS architectures and introducing multimodal sources of data to further increase the performance of the classification and detection models.},
  archive      = {J_PEERJCS},
  author       = {Yashashree Mahale and Nida Khan and Kunal Kulkarni and Shilpa Gite and Biswajeet Pradhan and Abdullah Alamri and Chang-Wook Lee and Nandhini K. and Mrinal Bachute},
  doi          = {10.7717/peerj-cs.3156},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3156},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A comprehensive approach for waste management with GAN-augmented classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of unsafe workplace behaviors: Sec-YOLO model with FEHA attention. <em>PEERJCS</em>, <em>11</em>, e3151. (<a href='https://doi.org/10.7717/peerj-cs.3151'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting unsafe human behaviors is crucial for enhancing safety in industrial production environments. Current models face limitations in multi-scale target detection within such settings. This study introduces a novel model, Sec-YOLO, which is specifically designed for detecting unsafe behaviors. Firstly, the model incorporates a receptive-field attention convolution (RFAConv) module to better focus on the key features of unsafe behaviors. Secondly, a deformable convolution network v2 (DCNv2) is integrated into the C2f module to enhance the model’s adaptability to the continually changing feature structures of unsafe behaviors. Additionally, inspired by the multi-branch auxiliary feature pyramid network (MAFPN) structure, the neck architecture of the model has been restructured. Importantly, to improve feature extraction and fusion, feature-enhanced hybrid attention (FEHA) is introduced and integrated with DCNv2 and MAFPN. Experimental results demonstrate that Sec-YOLO achieves a mean average precision (mAP) at 0.5 of 92.6% and mAP at 0.5:0.95 of 63.6% on a custom dataset comprising four common unsafe behaviors: falling, sleeping at the post, using mobile phones, and not wearing safety helmets. These results represent a 2.0% and 2.5% improvement over the YOLOv8n model. Sec-YOLO exhibits excellent performance in practical applications, focusing more precisely on feature handling and detection.},
  archive      = {J_PEERJCS},
  author       = {Yang Liu and Shuaixian Liu and Jie Gao and Tao Song and Wenyu Dong},
  doi          = {10.7717/peerj-cs.3151},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3151},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Detection of unsafe workplace behaviors: Sec-YOLO model with FEHA attention},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and evaluation of an e-governance ICT solution to empower the non-literate vegetable market consumers. <em>PEERJCS</em>, <em>11</em>, e3150. (<a href='https://doi.org/10.7717/peerj-cs.3150'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {E-governance services often remain inaccessible to non-literate populations worldwide. This study proposes and evaluates an e-governance solution for non-literate users. The solution is designed to identify price discrepancies between government-regulated rates and actual market prices for fruits and vegetables. The system features a user-friendly interface that enables non-literate consumers to lodge complaints and track their resolution, thereby enhancing empowerment. Usability was assessed using ISO Standard 9241-11, comparing the effectiveness and efficiency of non-literate participants with a literate benchmark, assuming optimal interface use by the latter. Although non-literate participants did not achieve the literate benchmarkís performance, the differences were minimal. These findings support the adoption of the proposed solution and its core design elements such as customized icons and local-language audio instructions in future e-governance initiatives targeting non-literate populations.},
  archive      = {J_PEERJCS},
  author       = {Syed Afzal Moshadi Shah and Anam Arif and Abdul Nasir Khan and Qazi Mudassar Ilyas and Abdullah Hamoud Ali Seraj and Iftikhar Ahmed Khan},
  doi          = {10.7717/peerj-cs.3150},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3150},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design and evaluation of an e-governance ICT solution to empower the non-literate vegetable market consumers},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on intelligent registration methods for multidimensional visual communication projection transform images. <em>PEERJCS</em>, <em>11</em>, e3147. (<a href='https://doi.org/10.7717/peerj-cs.3147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an intelligent image registration method tailored for images involved in multidimensional projection transformations in visual communication, which often suffer from blurred contours and incomplete shape alignments post-registration. The proposed approach comprises four integrated stages: (1) wavelet transform-based denoising for feature preservation, (2) edge detection using the Laplacian of Gaussian (LoG) operator, (3) contour matching via Fourier descriptors, and (4) projection and affine transformation for precise alignment. The method was evaluated on two high-resolution landscape datasets (Elephant Trunk Hill and Potala Palace, 1,920 × 1,080), achieving 99.1% accuracy and a root mean square error (RMSE) of 0.31. Compared to four state-of-the-art methods (DOI 10.1007/s10586-023-03974-3; DOI 10.1109/jsen.2023.3314608; DOI 10.1109/ACCESS.2023.3264968; DOI 10.1111/phor.12402), our approach demonstrated superior accuracy and execution speed. The technique is feasible for near real-time application on standard hardware (Intel i7 CPU, 16 GB RAM) and offers robustness without requiring supervised learning. This work contributes a clear, interpretable, and efficient framework for image registration.},
  archive      = {J_PEERJCS},
  author       = {Shupeng Liang},
  doi          = {10.7717/peerj-cs.3147},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3147},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on intelligent registration methods for multidimensional visual communication projection transform images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive gamification in collaborative virtual classroom: A systematic review. <em>PEERJCS</em>, <em>11</em>, e3146. (<a href='https://doi.org/10.7717/peerj-cs.3146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the potential of adaptive gamification in tackling challenges in collaborative virtual classrooms, including sustaining engagement, fostering motivation, and enhancing teamwork. The research identifies key theories and frameworks essential for designing gamified virtual learning environments by employing a systematic review guided by the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework. The methodology involved a four-phase process: identification, screening, eligibility assessment, and inclusion of relevant studies published between 2019 and 2024 in databases such as Scopus, Web of Science, IEEE, and ScienceDirect. A total of 43 articles were analyzed to derive themes and insights. The findings emphasize the integration of motivational frameworks like Self-Determination Theory (SDT) and learning models such as constructivism to enhance learner engagement and academic performance. These theories, centered on autonomy, competence, and relatedness, are effectively supported through adaptive gamification strategies. Frameworks like the Felder-Silverman learning style model (FSLSM) enable personalization by aligning gamified content with individual learning preferences, improving motivation and inclusivity. Furthermore, collaborative learning theories, such as online collaborative learning (OCL), provide a foundation for designing environments that promote peer interaction, mutual accountability, and teamwork. These frameworks balance individual and collective goals, transforming online education into an engaging and collaborative experience. This review concludes that adaptive gamification, underpinned by strong theoretical and systematic analysis, has significant potential to enhance virtual education by creating dynamic, personalized, and inclusive learning spaces that address diverse learner needs.},
  archive      = {J_PEERJCS},
  author       = {Intan Yusrina Zairon and Tengku Siti Meriam Tengku Wook and Syahanim M. Salleh and Hadi Affendy Dahlan},
  doi          = {10.7717/peerj-cs.3146},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3146},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Adaptive gamification in collaborative virtual classroom: A systematic review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated lung cancer diagnosis from chest X-ray images using convolutional neural networks. <em>PEERJCS</em>, <em>11</em>, e3145. (<a href='https://doi.org/10.7717/peerj-cs.3145'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background/Objectives Lung cancer is the leading cause of cancer-related deaths worldwide. While computed tomography (CT) scans provide more comprehensive medical information than chest X-rays (CXR), the high cost and limited availability of CT technology in rural areas pose significant challenges. CXR images, however, could serve as a potential preliminary diagnostic tool in diagnosing lung cancer, especially when combined with a computer-aided diagnosis (CAD) system. This study aims to enhance the accuracy and accessibility of lung cancer detection using a custom-designed convolutional neural network (CNN) trained on CXR images. Methods A custom-designed CNN was trained on an openly accessible CXR dataset from the Japanese Society for Radiological Technology (JSRT). Prior to training, the dataset underwent preprocessing, where each image was divided into overlapping patches. A t-test was applied to these patches to distinguish relevant from irrelevant ones. The relevant patches were retained for training the CNN model, while the irrelevant patches were excluded to enhance the model’s performance. Results The proposed model yielded a mean accuracy of 83.2 ± 2.91%, demonstrating its potential as a cost-effective and accessible preliminary diagnostic tool for lung cancer. Conclusions This approach could significantly improve the accuracy and accessibility of lung cancer detection, making it a viable option in resource-limited settings.},
  archive      = {J_PEERJCS},
  author       = {Aya Aboelghiet and Samaa M. Shohieb and Amira Rezk and Ahmed Abou Elfetouh and Ahmed Sharaf and Islam Abdelmaksoud},
  doi          = {10.7717/peerj-cs.3145},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3145},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automated lung cancer diagnosis from chest X-ray images using convolutional neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical decision support for dengue shock syndrome using a bipolar linear diophantine fuzzy hypersoft model with trigonometric similarity. <em>PEERJCS</em>, <em>11</em>, e3144. (<a href='https://doi.org/10.7717/peerj-cs.3144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach to trigonometric similarity measure in the framework of bipolar linear diophantine fuzzy hypersoft sets is introduced. It builds upon existing methodologies by integrating cosine and cotangent-based techniques to enhance decision-making in complex environments. The developed methodology incorporates both positive and negative membership degrees, control parameters, and diophantine information within hypersoft structures. The process involves the construction of bipolar linear diophantine fuzzy hypersoft matrices for multiple alternatives, computation of trigonometric similarity scores between these configurations, and ranking based on closeness to ideal criteria. Mathematical properties of the proposed measures are formally validated. The efficiency of the proposed similarity measures is demonstrated through a multi-attribute decision-making problem involving dengue shock syndrome. The model aids haematologists in identifying high-risk patients, improving diagnostic accuracy under uncertainty. This novel framework contributes to early detection strategies and supports clinical decision-making by providing a robust, mathematically grounded similarity evaluation. In the case study involving dengue shock syndrome, the proposed model achieved a similarity precision of 93.5% and ranked high-risk patients with 98% accuracy, outperforming existing cosine and cotangent-based similarity methods.},
  archive      = {J_PEERJCS},
  author       = {J. Vimala and S. Nithya Sri and Nasreen Kausar and Dragan Pamucar and Vladimir Simic and Jungeun Kim},
  doi          = {10.7717/peerj-cs.3144},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3144},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Medical decision support for dengue shock syndrome using a bipolar linear diophantine fuzzy hypersoft model with trigonometric similarity},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quality of experience-aware application deployment in fog computing environments using machine learning. <em>PEERJCS</em>, <em>11</em>, e3143. (<a href='https://doi.org/10.7717/peerj-cs.3143'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge intelligence is fast becoming indispensable as billions of sensors demand real-time inference without saturating backbone links or exposing sensitive data in remote data centres and emerging artificial intelligence (AI)-edge boards such as NVIDIA CPUs, 16 GB RAM, and microcontrollers with chip neural processing unit (NPU) (<1 W). This article introduces the Energy-Smart Component Placement (ESCP) algorithm of fog devices like fog cluster manager nodes (FCMNs) and fog nodes (FNs), allocates modules to fog devices, and saves energy by deactivating inactive devices framework transparently distributes compressed neural workloads across serverless. To optimize the deployment of AI workloads on fog edge devices as a service (FEdaaS), this project aims to provide a reliable and dynamic architecture that guarantees quality of service (QoS) and quality of experience (QoE). The cloud, fog, and extreme edge layers while upholding application-level QoS and QoE. Two machine learning (ML) methods that fuse eXtreme Gradient Boosting (XGB)-based instantaneous QoS scoring and long short term memory (LSTM) forecasting of node congestion, and a meta-heuristic scheduler that uses XGB for instantaneous QoS scoring and LSTM for short-horizon load forecasting. Compared with a cloud-only baseline, ESCP improved bandwidth utilization by 5.2%, scalability (requests per second) by 3.2%, energy consumption by 3.8% and response time by 2.1% while maintaining prediction accuracy within +0.4%. The results confirm that low-resource AI-edge devices, when orchestrated through our adaptive framework, can meet QoE targets such as 250 ms latency and 24 h of battery life. Future work will explore federated on-device learning to enhance data privacy, extend the scheduler to neuromorphic processors, and validate the architecture in real-time intensive care and smart city deployments.},
  archive      = {J_PEERJCS},
  author       = {P. Jenifer and J. Angela Jennifa Sujana},
  doi          = {10.7717/peerj-cs.3143},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3143},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Quality of experience-aware application deployment in fog computing environments using machine learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel nature-inspired feature selection algorithm for efficient moisture estimation in fruits using RF-sensed data. <em>PEERJCS</em>, <em>11</em>, e3142. (<a href='https://doi.org/10.7717/peerj-cs.3142'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel hybrid nature-inspired feature selection algorithm that unifies update mechanisms from Grey Wolf Optimizer (GWO), Artificial Bee Colony (ABC), and Bat Algorithm (BA). The resulting framework enables optimized machine learning models for precise grape moisture estimation from radio frequency (RF)-sensed data, addressing key challenges in smart agriculture. Performance is assessed in two phases: (1) pairing the feature selection method with a gated recurrent unit (GRU) model and comparing it against benchmark optimizers, and (2) integrating it with a customized convolutional neural network variant (CNN-R) designed for regression. The proposed feature selection technique demonstrates superior performance across all evaluation metrics. When combined with GRU, it achieves significantly lower root mean square error (RMSE) and mean absolute error (MAE) alongside a higher R2 (best-case: 0.999) compared to benchmark methods. With CNN-R, it maintains equally competitive results, validating its architecture-independent effectiveness. Crucially, the study shows that convolutional neural network (CNN), when adapted through CNN-R, can rival traditional regression models like GRU on numerical data.},
  archive      = {J_PEERJCS},
  author       = {Ali Roman and Youssef Altherwy and Syed Rameez Naqvi and Anas Alsuhaibani},
  doi          = {10.7717/peerj-cs.3142},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3142},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel nature-inspired feature selection algorithm for efficient moisture estimation in fruits using RF-sensed data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modular inverse visual cryptography for balancing security, quality, and efficiency in image transmission. <em>PEERJCS</em>, <em>11</em>, e3140. (<a href='https://doi.org/10.7717/peerj-cs.3140'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new Inverse Module Visual Cryptography (IMVC) system is proposed to transmit a small computed tomography (CT) scan image over the IoT network securely and efficiently. In particular, Internet of Things (IoT) devices are important for the healthcare industry as they transmit sensitive imaging data; thus, confidentiality and high image fidelity are critical. The IMVC framework stands as a blend between visual cryptography and the very efficient and light-weighted modular arithmetic, suitable for mostly computational-constrained IoT networks. In this way, CT scan images are divided into pixel-aligned shares, inserted into cover images, and securely sent to destinations of the receiver. Expected outcome shows that the IMVC reconstructs equally a complete and lossless image, which displays equal quality parameters such as an infinite peak signal to noise ratio (PSNR) and equal mean square error (MSE) of zero, signifying the reconstructed images were accurate. Furthermore, expected high immunity to brute-force, statistical, and collusion attacks makes the methodology more reliable for improving the security of the IoT-based medical application. As conceived, performance tests are expected to report an efficient computational building and time required to encrypt and decrypt the framework, which will amount to an average of 1.79 and 0.45 s, respectively, implying real-time suitability of the framework in clinical settings. In this work, the proposed IMVC allows for the development of a feasible and effective solution for secure transfer of important image data in an IoT healthcare context, while addressing the requirements of security, image quality, as well as efficient transmission.},
  archive      = {J_PEERJCS},
  author       = {Selva Mary and John Blesswin and Suresh Sankaranarayanan and Abdul Rahaman Wahab Sait},
  doi          = {10.7717/peerj-cs.3140},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3140},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Modular inverse visual cryptography for balancing security, quality, and efficiency in image transmission},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving english-chinese translation using the kolmogorov-arnold transformer. <em>PEERJCS</em>, <em>11</em>, e3139. (<a href='https://doi.org/10.7717/peerj-cs.3139'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine translation is an important part of natural language processing, helping people communicate across languages, localise content, and search for information in different languages. In this article, we introduce a new framework using the Kolmogorov-Arnold Transformer (KAT) to improve translation quality. We test KAT on the Bilingual MELD dataset and compare it with both traditional statistical models and modern neural translation models. Our results show that KAT performs better, achieving a BLEU-4 score of 42.8, a METEOR score of 45.3, and a TER of 40.5, all of which are improvements over standard transformer models. We also find that using a larger vocabulary and adding the Kolmogorov-Arnold network helps improve translation accuracy. These results suggest that Kolmogorov-Arnold-based methods can be a valuable addition to machine translation systems.},
  archive      = {J_PEERJCS},
  author       = {Yuzhe Nie},
  doi          = {10.7717/peerj-cs.3139},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3139},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving english-chinese translation using the kolmogorov-arnold transformer},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FEEL: Fast and effective emotion labeling, a dual ensemble approach for effective facial emotion recognition. <em>PEERJCS</em>, <em>11</em>, e3138. (<a href='https://doi.org/10.7717/peerj-cs.3138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expressions are a vital channel for communicating emotions and personality traits, making automatic emotion recognition from facial images a task of growing importance with wide-ranging applications. While deep learning models have shown considerable promise in this domain, most existing approaches are unimodal and limited to classifying only six basic emotions. This study introduces a dual-ensemble deep learning framework designed to recognize both basic and complex blended emotions with high accuracy. The first ensemble focuses on detecting primary emotions using DenseNet-169, VGG-16, and ResNet-50 as base models. The second ensemble focuses on identifying nuanced emotional blends, utilizing Xception and vision transformer (ViT) architectures. A squeeze-and-excitation (SE) block is incorporated to emphasize the most salient features, thereby enhancing overall model performance. The proposed framework is trained and evaluated on the widely used Facial Expression Recognition (FER)2013 and Indonesian Mixed Emotion Dataset (IMED) datasets. Experimental results demonstrate its effectiveness, achieving 95% accuracy for basic emotion recognition and 88% for blended emotions. These findings underscore the potential of the proposed approach to advance human-computer interaction by improving both the accuracy and depth of automated emotion recognition systems.},
  archive      = {J_PEERJCS},
  author       = {Shuiping Wang},
  doi          = {10.7717/peerj-cs.3138},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3138},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {FEEL: Fast and effective emotion labeling, a dual ensemble approach for effective facial emotion recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing human activity recognition with machine learning: Insights from smartphone accelerometer and magnetometer data. <em>PEERJCS</em>, <em>11</em>, e3137. (<a href='https://doi.org/10.7717/peerj-cs.3137'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The domain of Human Activity Recognition (HAR) has undergone a remarkable evolution, driven by advancements in sensor technology, artificial intelligence (AI), and machine learning algorithms. The aim of this article consists of taking as a basis the previously obtained results to implement other techniques to analyze the same dataset and improve the results previously obtained in the different studies, such as neural networks with different configurations, random forest, support vector machine, CN2 rule inducer, Naive Bayes, and AdaBoost. The methodology consists of data collection from smartphone sensors, data cleaning and normalization, feature extraction techniques, and the implementation of various machine learning models. The study analyzed machine learning models for recognizing human activities using data from smartphone sensors. The results showed that the neural network and random forest models were highly effective across multiple metrics. The models achieved an area under the curve (AUC) of 98.42%, a classification accuracy of 90.14%, an F1-score of 90.13%, a precision of 90.18%, and a recall of 90.14%. With significantly reduced computational cost, our approach outperforms earlier models using the same dataset and achieves results comparable to those of contemporary deep learning-based approaches. Unlike prior studies, our work utilizes non-normalized data and integrates magnetometer signals to enhance performance, all while employing lightweight models within a reproducible visual workflow. This approach is novel, efficient, and deployable on mobile devices in real-time. This approach makes it an ideal fit for real-time mobile applications.},
  archive      = {J_PEERJCS},
  author       = {Luis Augusto Silva Zendron and Paulo Jorge Coelho and Christophe Soares and Ivo Pereira and Ivan Miguel Pires},
  doi          = {10.7717/peerj-cs.3137},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3137},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing human activity recognition with machine learning: Insights from smartphone accelerometer and magnetometer data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Particle swarm optimization framework for parkinson’s disease prediction. <em>PEERJCS</em>, <em>11</em>, e3135. (<a href='https://doi.org/10.7717/peerj-cs.3135'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early diagnosis of Parkinson’s disease (PD) is challenging due to subtle initial symptoms. This study introduces an advanced machine learning framework that leverages particle swarm optimization (PSO) to improve PD detection through vocal biomarker analysis. Our novel approach unifies the optimization of both acoustic feature selection and classifier hyperparameter tuning within a single computational architecture. We systematically evaluated PSO-enhanced predictive models for PD detection using two comprehensive clinical datasets. Dataset 1 includes 1,195 patient records with 24 clinical features, and Dataset 2 comprises 2,105 patient records with 33 multidimensional features spanning demographic, lifestyle, medical history, and clinical assessment variables. For Dataset 1, the PSO model achieved 96.7% testing accuracy, an absolute improvement of 2.6% over the best-performing traditional classifier (Bagging classifier at 94.1%), while maintaining exceptional sensitivity (99.0%) and specificity (94.6%). Results were even more significant for Dataset 2, where the PSO model reached 98.9% final accuracy, a 3.9% improvement over the LGBM classifier (95.0%), with near-perfect discriminative capability (AUC = 0.999). These performance gains were achieved with reasonable computational overhead, averaging 250.93 s training time for Dataset 2, suggesting the practical viability of PSO optimization for clinical prediction tasks. Our findings underscore the potential of intelligent optimization techniques in developing practical decision support systems for early neurodegenerative disease detection, with significant implications for clinical practice.},
  archive      = {J_PEERJCS},
  author       = {Entesar Hamed I. Eliwa and Tarek Abd El-Hafeez},
  doi          = {10.7717/peerj-cs.3135},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3135},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Particle swarm optimization framework for parkinson’s disease prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Arabic hate speech detection using deep learning: A state-of-the-art survey of advances, challenges, and future directions (2020–2024). <em>PEERJCS</em>, <em>11</em>, e3133. (<a href='https://doi.org/10.7717/peerj-cs.3133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of social media has intensified concerns about the societal and psychological impacts of hate speech, particularly in Arabic-speaking communities, where dialectal diversity, morphological complexity, and sociopolitical factors complicate detection. Despite platform efforts, the automated detection of Arabic hate speech remains challenging owing to limited annotated datasets and linguistic nuances. This survey reviews the advances (2020–2024) in deep learning approaches, including convolutional neural networks (CNNs), recurrent neural networks (RNNs), transformer-based models (e.g., bidirectional encoder representations from transformers (BERT) and AraBERT), and hybrid architectures for Arabic hate speech detection. It further examines the dataset constraints involving dialectal variation, annotation inconsistencies, and scarcity. The analysis identified critical research gaps and proposed future directions: expanding multilingual datasets, enhancing contextual modeling, and developing ethically grounded frameworks. This review consolidates state-of-the-art methodologies to guide effective countermeasures against Arabic online hate speech.},
  archive      = {J_PEERJCS},
  author       = {Mariam Itriq and Mohd Halim Mohd Noor},
  doi          = {10.7717/peerj-cs.3133},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3133},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Arabic hate speech detection using deep learning: A state-of-the-art survey of advances, challenges, and future directions (2020–2024)},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Security and privacy for binance-based integrated blockchain for blood supply chain management systems. <em>PEERJCS</em>, <em>11</em>, e3123. (<a href='https://doi.org/10.7717/peerj-cs.3123'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The world faces a severe blood shortage, with a gap of 1.95 million units, highlighting the need for efficient blood allocation and management systems. Traditional cloud and blockchain approaches have been explored for blood bank management but faced implementation challenges. This study proposes designing and developing a decentralized Binance blockchain-based application framework to ensure transparency and security. It uses the AdaBoost algorithm to predict the availability of the nearest blood bank and blood donor. Supply chain management provides transparency without the intervention of third parties thereby preventing blood crimes. Metamask is incorporated for crypto transactions in the Binance Smart Chain test network (BSC). BSC stands out for its low transaction fees and high scalability, enabling swift transaction processing at a fraction of the cost compared to Ethereum. The smart contracts are deployed using hardhat configuration enabling BscScan as an Application Programming Interface (API) gateway to record transactions within the decentralized application (dApp). The proposed system achieved an accuracy of 99.5%, demonstrating the robustness of the AdaBoost model in predicting blood availability. The integration of blockchain technology ensures transparency, immutability, and secure traceability of blood transactions across the network.},
  archive      = {J_PEERJCS},
  author       = {Rupa Ch and Sai Varshitha G. and Divya D. and Thippa Reddy Gadekallu and Mustufa Haider Abidi and Fahad Alasim},
  doi          = {10.7717/peerj-cs.3123},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3123},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Security and privacy for binance-based integrated blockchain for blood supply chain management systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridges in social networks: Current status and challenges. <em>PEERJCS</em>, <em>11</em>, e3122. (<a href='https://doi.org/10.7717/peerj-cs.3122'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In social network analysis, bridges play a critical role in maintaining connectivity and facilitating the dissemination of information between communities. Despite increasing interest in bridge structures, a systematic classification of their roles across various network types remains unexplored. This study introduces a categorization of bridges into structural and functional types. Structural bridges maintain connectivity by preventing network fragmentation, whereas functional bridges facilitate the flow of information between communities. We conducted a comprehensive literature review and classified existing studies within this framework. The findings clarify the distinct roles of bridges and provide valuable insight for devising effective strategies for network design and analysis.},
  archive      = {J_PEERJCS},
  author       = {Jeongseon Kim and Soohwan Jeong and Jungeun Kim and Sungsu Lim},
  doi          = {10.7717/peerj-cs.3122},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3122},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Bridges in social networks: Current status and challenges},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid deep learning approach with progressive cyclical CNN and firebug swarm optimization for breast cancer detection. <em>PEERJCS</em>, <em>11</em>, e3119. (<a href='https://doi.org/10.7717/peerj-cs.3119'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The practice of diagnosing breast cancer retains its scope for improvement in medical imaging, where every correct and timely diagnosis enhances the survival rate of patients. This article presents an integrated approach utilizing patch-wise breast image segmentation, hybrid deep feature extraction, followed by progressive cyclical convolutional neural networks (P-CycCNN), and firebug swarm optimization (FSO) to enhance breast cancer detection. This method first incorporates image segmentation by patches to break down the mammography images into smaller patches, which are easier to focus on and allow for the extraction of more features to boost detection rates. Hybrid feature extraction combines convolutional neural network (CNN) features extracted from pre-trained models with handcrafted features that describe texture and shape, thereby enabling the model to grasp the nuances of both coarse and fine images comprehensively. The progressive cyclical CNN strategy incorporates cyclical, re-adjusted learning rates and a progressive training schedule to accelerate and enhance the model’s convergence. FSO is introduced to adjust the hyperparameters of the CNN topology, including the learning rate and regularisation parameters, thereby enhancing training and feature-fusion processes. Evaluated on the Curated Breast Imaging Subset of the Digital Database for Screening Mammography (CBIS-DDSM) dataset, the proposed model achieved 98% test accuracy, 95% precision, 97.2% recall, 96% F1-score, and an AUC of 0.95, outperforming baseline CNN models by 4%–6% across key metrics. This approach holds great potential for enhancing detection systems in clinics, allowing earlier and more accurate detection of malignant lesions.},
  archive      = {J_PEERJCS},
  author       = {Sudha Prathyusha Jakkaladiki and Filip Malý},
  doi          = {10.7717/peerj-cs.3119},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3119},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid deep learning approach with progressive cyclical CNN and firebug swarm optimization for breast cancer detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying the S-O-R model to explore impulsive buying behavior driven by influencers on social commerce websites. <em>PEERJCS</em>, <em>11</em>, e3113. (<a href='https://doi.org/10.7717/peerj-cs.3113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, influencer marketing has gained increasing popularity, with many influencers embedding product information into their content (e.g., videos and articles). When fans encounter these messages, they may make unplanned purchases, resulting in impulse buying behavior, a long-standing issue in marketing research. This study aims to explore the factors that lead to such behavior. Using the Stimulus–Organism–Response (S-O-R) model as a framework, the study investigates how interactions between individuals and influencer content (Stimuli) trigger psychological changes in consumers, namely positive affect, flow state, and emotional attachment (Organism), which in turn lead to impulse buying behavior (Response). The study surveyed fans who had previously purchased products recommended by influencers, collecting 404 valid responses. The findings reveal that: (1) Consumers’ psychological changes (positive affect, flow state, and emotional attachment) significantly and positively influence impulse buying behavior. (2) Scarcity, discounted price, review quality, and observational learning also have significant positive effects on impulse buying. (3) Social presence and sense of belonging significantly enhance flow state. (4) Entertainment and informativeness significantly enhance emotional attachment.},
  archive      = {J_PEERJCS},
  author       = {Tanaporn Hongsuchon and Shih-Chih Chen and Asif Khan},
  doi          = {10.7717/peerj-cs.3113},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3113},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Applying the S-O-R model to explore impulsive buying behavior driven by influencers on social commerce websites},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SignVLM: A pre-trained large video model for sign language recognition. <em>PEERJCS</em>, <em>11</em>, e3112. (<a href='https://doi.org/10.7717/peerj-cs.3112'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language recognition (SLR) plays a vital role in including people with hearing impairment in the community. It facilitates the recognition of sign gestures and converts them into spoken languages. One of the main challenges for developing SLR systems is the lack of annotated datasets. This issue is more noticeable with low–resourced sign languages. To address this issue, we propose a pretrained large vision model, SignVLM, for SLR. This work explores the capability of the contrastive language–image pre-training (CLIP) model for SLR. This model is used to extract spatial features from the sign video frames while a Transformer decoder is used for temporal learning. The proposed model has been evaluated on four different sign languages using the KArSL, WLASL, LSA64, and AUSTL datasets. Different evaluation settings have been followed in this work including zero-shot and few-shot learning. The proposed model outperformed other models on the KArSL, WLASL, and LSA64 datasets and achieved comparable performance on the AUTSL dataset. The obtained results demonstrate the generalization of the proposed model to new datasets with few samples. The code and data are available at https://github.com/Hamzah-Luqman/signVLM.},
  archive      = {J_PEERJCS},
  author       = {Hamzah Luqman},
  doi          = {10.7717/peerj-cs.3112},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3112},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SignVLM: A pre-trained large video model for sign language recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Periodontitis bone loss detection in panoramic radiographs using modified YOLOv7. <em>PEERJCS</em>, <em>11</em>, e3102. (<a href='https://doi.org/10.7717/peerj-cs.3102'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periodontitis is a common dental disease that results in tooth loss, if not diagnosed and treated in time. However, diagnosing bone loss due to periodontitis from panoramic radiographs is a time-consuming and error-prone process, requiring extensive training and expertise. This work addresses the research gap in automated periodontitis bone loss diagnosis using deep learning techniques. We have proposed a modified version of You Only Look Once (YOLO)v2, called YOLOv7-M, that includes a focus module and a feature fusion module for rapid inference and improved feature extraction ability. The proposed YOLOv7-M model was evaluated on a tooth detection dataset and demonstrated superior performance, achieving an F1-score, precision, recall, and mean average precision (mAP) of 92.5, 91.7, 87.1, and 91.0, respectively. Experimental results indicate that YOLOv7-M outperformed other state-of-the-art object detectors, including YOLOv5 and YOLOv7, in terms of both accuracy and speed. In addition, our comprehensive performance tests show that YOLOv7-M outperforms robust object detectors in terms of various statistical evaluation measures. The proposed method has potential applications in automated periodontitis diagnosis and can assist in the detection and treatment of the disease, eventually enhancing patient outcomes.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Gamal Ragab and Said Jadid Abdulkadir and Nadhem Qaid and Taimoor Muzaffar Gondal and Alawi Alqushaibi and Rizwan Qureshi and Furqan Shaukat},
  doi          = {10.7717/peerj-cs.3102},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3102},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Periodontitis bone loss detection in panoramic radiographs using modified YOLOv7},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GCL-ALG: Graph contrastive learning with adaptive learnable view generators. <em>PEERJCS</em>, <em>11</em>, e3101. (<a href='https://doi.org/10.7717/peerj-cs.3101'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data augmentation is a pivotal part of graph contrastive learning, which can mine implicit graph data information to improve the quality of representation learning. Research on graph data augmentation has achieved promising results in recent years. However, existing graph contrastive learning methods are trapped in inherent predefined augmentation schemes, which greatly limits the generalization of augmentation methods. To this end, we propose a new adaptive original topology learnable data augmentation algorithm, Graph Contrastive Learning with Adaptive Learnable View Generators (GCL-ALG), to optimize the augmentation process and feature learning in an end-to-end self-supervised learning approach. Specifically, GCL-ALG introduces graph neural networks (GNN), graph attention modules and edge probability distributions to build a dual-level feature extraction framework to generate highly reliable representations, while integrating network science theory to selectively modify the strength of augmentation probabilities from node-level and edge-level, and then train dynamically learnable augmentation instances. Moreover, GCL-ALG designs multiple loss functions to drive the representation optimization to ensure that the generated graph representations are highly discriminative across different tasks. Extensive experiments are conducted on unsupervised learning, semi-supervised learning and transfer learning application tasks. The experimental results demonstrate the superior performance of the proposed GCL-ALG method on 16 benchmark datasets.},
  archive      = {J_PEERJCS},
  author       = {Yafang Li and Jie Kang and Zhihua Chu and Baokai Zu},
  doi          = {10.7717/peerj-cs.3101},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3101},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GCL-ALG: Graph contrastive learning with adaptive learnable view generators},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on graph neural networks, machine learning and deep learning techniques for time series applications in industry. <em>PEERJCS</em>, <em>11</em>, e3097. (<a href='https://doi.org/10.7717/peerj-cs.3097'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive studies have been conducted to investigate Artificial Intelligence (AI) in the context of time series data. In this article, we investigate the complex domain of industrial time series, from the dimensions of classical machine learning (ML), deep neural networks (DNNs) and graph neural networks (GNNs). Current surveys often focus on a specific methodology or oversee the connection of diverse approaches; our article bridges this gap by providing an all-inclusive interpretation across numerous techniques. In addition, the aim of this article is to focus on the core areas of time series such as forecasting, classification, and anomaly detection. From traditional methodologies like Autoregressive Integrated Moving Average (ARIMA) and support vector machine (SVM) methods, the advancements of DNNs, for instance long-short-term memory (LSTMs), convolutional neural networks (CNNs), attention mechanisms, and transformers, describe how temporal information is used for forecasting, anomaly detection, and classification. Then the article discusses the advances and limitations in ML, DNN, and GNN in order to improve the different methods in either category. Lastly, we outline future directions and open research questions with the different methodologies used in time series.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Jamal Ahmed and Alberto Mozo and Amit Karamchandani},
  doi          = {10.7717/peerj-cs.3097},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3097},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A survey on graph neural networks, machine learning and deep learning techniques for time series applications in industry},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting academic performance for students’ university: Case study from saint cloud state university. <em>PEERJCS</em>, <em>11</em>, e3087. (<a href='https://doi.org/10.7717/peerj-cs.3087'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting students’ performance is one of the essential educational data mining approaches aimed at observing learning outcomes. Predicting grade point average (GPA) helps to monitor academic performance and assists advisors in identifying students at risk of failure, major changes, or dropout. To enhance prediction performance, this study employs a long short-term memory (LSTM) model using a rich set of academic and demographic features. The dataset, drawn from 29,455 students at Saint Cloud State University (SCSU) over eight years (2016–2024), was carefully preprocessed by eliminating irrelevant and missing data, encoding categorical variables, and normalizing numerical features. Feature importance was determined using a permutation-based method to identify the most impactful variables on term GPA prediction. Furthermore, model hyperparameters, including the number of LSTM layers, units per layer, batch size, learning rate, and activation functions, were fine-tuned using experimental validation with the Adam optimizer and learning rate scheduling. Two experiments were conducted at both the college and department levels. The proposed model outperformed traditional machine learning models such as linear regression (LR), K-nearest neighbor (KNN), decision tree (DT), random forest (RF), and support vector regressor (SVR), and it surpasses two deep learning models, recurrent neural network (RNN) and convolutional neural network (CNN), achieving 9.54 mean absolute percentage error (MAPE), 0.0059 mean absolute error (MAE), 0.0001 root mean square error (RMSE), and an R² score of 99%.},
  archive      = {J_PEERJCS},
  author       = {Bilal I. Al-Ahmad and Abdullah Alzaqebah and Rami Alkhawaldeh and Ala’ M. Al-Zoubi and Hsuehi Lo and Adel Ali},
  doi          = {10.7717/peerj-cs.3087},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3087},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting academic performance for students’ university: Case study from saint cloud state university},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning based cardiac disorder classification and user authentication for smart healthcare system using ECG signals. <em>PEERJCS</em>, <em>11</em>, e3082. (<a href='https://doi.org/10.7717/peerj-cs.3082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abnormal cardiac activity can lead to severe health complications, emphasizing the importance of timely diagnosis. It is essential to save lives if diseases are diagnosed in a reasonable timeframe. The intelligent telehealth system has the potential to transform the healthcare industry by continuously monitoring cardiac diseases remotely and non-invasively. A cloud-based telehealth system utilizing an Internet of Things (IoT)-enabled electrocardiogram (ECG) monitor gathers and analyzes ECG signals to predict cardiac complications and notify physicians in crises, facilitating prompt and precise diagnosis of cardiovascular disorders. Abnormal cardiac activity can lead to severe health complications, making early detection crucial for effective treatment. This study provides an efficient method based on deep learning convolutional neural network (CNN) and long short-term memory (LSTM) approaches to categorize and detect cardiovascular problems utilizing ECG data to increase classifications (referring to distinguishing between different ECG signal categories) and precision. Additionally, a threshold-based classifier is developed for the telehealth system’s security and privacy to enable user identification (for selecting the correct user from a group) using ECG data. A data preprocessing and augmentation technique was applied to improve the data quality and quantity. The proposed LSTM model attained 99.5% accuracy in the classification of cardiac diseases and 98.6% accuracy in user authentication utilizing ECG signals. These results exhibit enhanced performance compared to conventional machine learning and convolutional neural network models.},
  archive      = {J_PEERJCS},
  author       = {Tong Ding and Chenhe Liu and Jiasheng Zhang and Yibo Zhang and Cheng Ding},
  doi          = {10.7717/peerj-cs.3082},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3082},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning based cardiac disorder classification and user authentication for smart healthcare system using ECG signals},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust detect and describe framework for object recognition in early childhood education. <em>PEERJCS</em>, <em>11</em>, e3080. (<a href='https://doi.org/10.7717/peerj-cs.3080'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preschool education plays a vital role in the harmonious development of an individual. Understanding basic shapes, colors, and letters at an early age lays a strong foundation for academic excellence and emotional growth. At an early childhood stage, the skills of spatial reasoning and problem-solving can be developed by recognizing and comprehending the depicted objects. By exploring deep learning technology, this article presents a cognitive enhancement framework for recognizing nested objects. With cutting-edge models, such as You Only Look Once (YOLOv8) and Visual Geometry Group (VGG16), objects and intra-objects are detected. For semantic description, the neural network model, specifically long short-term memory (LSTM), is exploited, preceded by precise object recognition. The framework is implemented in Google Colab with the prominent packages of Ultralytics, PyTorch, and OpenCV. The models are trained and tested by a custom dataset: PreEduDS. The results of the systematic evaluation suggest that the framework has widespread applicability. A promising accuracy score of 94.4% is obtained for object recognition and 96.5% for predicting precise semantic textual description. The proposed system is well-suited for enhancing preschool education and training based on augmented reality (AR) applications.},
  archive      = {J_PEERJCS},
  author       = {Lan Lv and Suhui Yao},
  doi          = {10.7717/peerj-cs.3080},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3080},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A robust detect and describe framework for object recognition in early childhood education},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sales forecasting for retail stores using hybrid neural networks and sales-affecting variables. <em>PEERJCS</em>, <em>11</em>, e3058. (<a href='https://doi.org/10.7717/peerj-cs.3058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate sales forecasting is vital for balancing demand and supply and enhancing profitability in the retail sector. Deep learning (DL) models have shown promise in this area; however, most either handle temporal or spatial patterns in isolation. Moreover, many studies rely on synthetic datasets or omit critical contextual variables, reducing real-world accuracy. This study proposes a hybrid convolutional neural network (CNN)-long short-term memory (LSTM) model for retail sales forecasting using real-world data enhanced with environmental and demographic variables in term of holidays, salary days, protests, and weather conditions. CNNs capture spatial patterns, while LSTMs model temporal dependencies, making the hybrid architecture well-suited for multivariate forecasting tasks. Our model demonstrates a significant improvement in predictive performance, achieving a mean absolute percentage error (MAPE) of 4.16%, outperforming traditional and standalone neural models. By incorporating external factors, the proposed approach enables more reliable forecasting and supports informed decision-making in retail operations.},
  archive      = {J_PEERJCS},
  author       = {Saad Mansur and Kashif Sattar and Seyed Ebrahim Hosseini and Shahbaz Pervez and Iftikhar Ahmad and Kashif Saleem and Ahmed Zohier Elhendi},
  doi          = {10.7717/peerj-cs.3058},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3058},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Sales forecasting for retail stores using hybrid neural networks and sales-affecting variables},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). State-of-the-art artificial intelligence approaches for anomaly detection and remaining useful life prediction: A review. <em>PEERJCS</em>, <em>11</em>, e3056. (<a href='https://doi.org/10.7717/peerj-cs.3056'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Accurate prediction of the remaining useful life (RUL) of assets is fundamental to the development of effective maintenance strategies and overall asset management. Despite significant advancements, there remains a notable gap in integrating fault detection and diagnostics (FDD) with RUL prediction models to create more comprehensive and accurate maintenance systems. One of the key challenges in this field is the limited ability of current models to generalize effectively across different types of equipment and varying operating conditions. This gap emphasizes the need for further research and innovation in developing robust and adaptable RUL prediction methodologies that can be applied broadly across diverse industrial scenarios. Methodology This review systematically evaluates the machine learning (ML) and deep learning (DL) techniques used for anomaly detection and RUL prediction, focusing on their efficacy and practical application. By adhering to the Preferred Reporting Items for Systematic Review and Meta-Analyses (PRISMA) criteria, the review identifies and addresses the deficiencies in existing models. It explores a range of machine learning and deep learning methods, including probabilistic approaches, hybrid models that combine multiple machine learning techniques, and neural networks designed to handle large-scale time-series data. The review also examines the potential for synergy between machine learning models and FDD, aiming to enhance the precision of equipment monitoring and the early detection of defects. The challenges of data variability, the irregularity in equipment deterioration, and the interpretability of complex models are highlighted. Results The analysis reveals that while current machine learning and deep learning models have made considerable strides in predicting the RUL of assets, significant challenges remain, particularly in their ability to generalize across various equipment types and operational contexts. Hybrid models and neural networks have shown promise in improving the accuracy of RUL predictions, especially when managing large, complex datasets. However, the irregular nature of equipment wears and tear, coupled with data variability, continues to pose significant challenges. The review highlights the need for more robust and adaptable models that can not only predict RUL more accurately but also integrate seamlessly with FDD systems to provide a more holistic approach to maintenance. Conclusion This comprehensive review focusses on the need for continued research in developing more integrated, generalizable, and efficient predictive maintenance systems. By exploring the application of AI in virtual assistants, the review suggests promising avenues for extending asset longevity and optimizing maintenance schedules. While current models offer valuable insights, they must evolve to address the identified gaps in generalizability and model interpretability.},
  archive      = {J_PEERJCS},
  author       = {Mohd Khidir Gazali and Khairunnisa Hasikin and Khin Wee Lai and Aizat Hilmi Zamzam and Rafat Damseh},
  doi          = {10.7717/peerj-cs.3056},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3056},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {State-of-the-art artificial intelligence approaches for anomaly detection and remaining useful life prediction: A review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introducing the municipal digital offering index for evaluating online services and addressing the digital divide. <em>PEERJCS</em>, <em>11</em>, e3049. (<a href='https://doi.org/10.7717/peerj-cs.3049'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research introduces the Municipal Digital Offering Index (MDOI) to assess municipal online service development in Chile. The study utilizes content analysis of municipal websites, creating a systematic instrument to evaluate digital services. It evaluates all 344 Chilean municipalities based on 163 dichotomous variables. Through factor analysis and regression modeling, it investigates sociodemographic and economic factors influencing digital development at the municipal level, offering insights into the digital divide across municipalities. The findings highlight geographical disparities and indicate priority intervention areas. While education levels and financial resources influence digital technology adoption, many municipalities lack efficient online procedures, prompting focused digital transformation investments. This research emphasizes the importance of localized digital services in bridging the digital divide and promoting inclusive governance.},
  archive      = {J_PEERJCS},
  author       = {Carolina Busco and Felipe González and Paula Farina and Jonathan Vivas and Fernanda Saavedra and Lizbeth Avalos},
  doi          = {10.7717/peerj-cs.3049},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3049},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Introducing the municipal digital offering index for evaluating online services and addressing the digital divide},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Regularized multi-path XSENet ensembler for enhanced student performance prediction in higher education. <em>PEERJCS</em>, <em>11</em>, e3032. (<a href='https://doi.org/10.7717/peerj-cs.3032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of educational data, institutions face increasing pressure to adopt advanced predictive models that can enhance academic planning, resource allocation, and student support. This study presents a novel educational data mining approach designed to forecast student performance levels categorized as low, medium, and high by analyzing historical and behavioral trends. This work proposes XSEJNet, an innovative hybrid model that integrates ResNeXt architecture with squeeze-and-excitation (SE) attention mechanisms, and employs the Jaya optimization algorithm to refine hyperparameters and boost predictive accuracy and computational efficiency. The model works with structured and unstructured academic data, effectively capturing complex, high-dimensional features to support accurate classification. Through extensive simulations and comparative evaluations, XSEJNet consistently outperforms conventional machine learning models and recent existing techniques such as reinforcement learning co-evolutionary hybrid intelligence (RLCHI), Enhanced AEO-XGBoost, convolution-based deep learning (Conv-DL), and dual graph neural network (DualGNN). The model achieves a high prediction accuracy of 97.98% while also demonstrating faster convergence and reduced computational overhead, making it a scalable and practical solution for real-world educational settings. The findings underscore XSEJNet’s ability to support early intervention, strengthen e-learning platforms, and inform institutional decision-making. By advancing predictive capabilities in education, this work makes a meaningful contribution to developing inclusive, data-driven, and sustainable academic systems.},
  archive      = {J_PEERJCS},
  author       = {Eman Ali Aldhahri and Abdulwahab Ali Almazroi and Nasir Ayub},
  doi          = {10.7717/peerj-cs.3032},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3032},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Regularized multi-path XSENet ensembler for enhanced student performance prediction in higher education},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel framework for secure cryptocurrency transactions using quantum crypto guard. <em>PEERJCS</em>, <em>11</em>, e3030. (<a href='https://doi.org/10.7717/peerj-cs.3030'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In today’s digital world, cryptocurrencies like Bitcoin can secure transactions without banks. However, the rise of quantum computing poses significant threats to their security, as traditional cryptographic methods may be easily compromised. In addition, the existing algorithms face difficulties like slow transaction speeds, interoperability issues between different cryptocurrencies, and privacy concerns. Hence, Quantum Crypto Guard for Secure Transactions (QCG-ST), a novel blockchain framework, is introduced, offering enhanced security and efficiency for cryptocurrency transactions. The QCG-ST employs lattice-based cryptography to provide robust protection against quantum threats and incorporates a new consensus mechanism to increase the transaction speed and reduce energy consumption. The QCG-ST system uses lattice-based encryption that is based on the Ring Learning With Errors (Ring-LWE) issue to protect itself from quantum assaults. It uses sharding, a Proof-of-Stake (PoS) consensus method, and a threshold signature scheme (TSS) to make the system more scalable and use less energy. Zero-knowledge proofs (ZKPs) are used to check transactions without giving out private information. We offer a cross-chain atomic swap protocol that uses hashed time-lock contracts to make sure that it works on all platforms. Blockchain transaction data utilized in testing originated from the Bitcoin Historical Dataset available on Kaggle, and quantum resistance has been assessed using the Qiskit Aer simulator. It evaluated the framework’s performance to that of traditional methods like Payment Channel–Lightning Network (PC-LN), Variational Quantum Eigensolver (VQE), and Cross-Chain Transaction with Hyperledger (CCT-H). Results show that QCG-ST does far better than traditional systems in terms of transaction success rate (up to 98.5%), speed, energy efficiency, latency, and throughput, especially when tested in a quantum-simulated environment. This study completes in an essential vacuum in blockchain technology by suggesting a strong, quantum-resistant, privacy-protecting architecture that can handle the problems that could arise up in decentralized digital banking in the future.},
  archive      = {J_PEERJCS},
  author       = {Jamil Abedalrahim Jamil Alsayaydeh and Mohd Faizal Yusof and Nor Adnan Yahaya and Viacheslav Kovtun and Safarudin Gazali Herawan},
  doi          = {10.7717/peerj-cs.3030},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3030},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel framework for secure cryptocurrency transactions using quantum crypto guard},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing credit card fraud detection with a stacking-based hybrid machine learning approach. <em>PEERJCS</em>, <em>11</em>, e3007. (<a href='https://doi.org/10.7717/peerj-cs.3007'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The swift progression of technology has increased the complexity of cyber fraud, posing an escalating challenge for the banking sector to reliably and efficiently identify fraudulent credit card transactions. Conventional detection approaches fail to adapt to the advancing strategies of fraudsters, resulting in heightened false positives and inefficiency within fraud detection systems. This study overcomes these restrictions by creating an innovative stacking hybrid machine learning (ML) approach that combines decision trees (DT), random forests (RF), support vector machines (SVM), XGBoost, CatBoost, and logistic regression (LR) within a stacking ensemble framework. This method uses stacking to integrate diverse ML models, enhancing predictive performance, with a meta-model consolidating base model predictions, resulting in superior detection accuracy compared to any single model. The methodology utilizes sophisticated data preprocessing techniques, such as correlation-based feature selection and principal component analysis (PCA), to enhance computing efficiency while preserving essential information. Experimental assessments of a credit card transaction dataset reveal that the stacking ensemble model exhibits higher performance, achieving an F1-score of 88.14%, thereby efficiently balancing precision and recall. This outcome highlights the significance of ensemble methods such as stacking in attaining strong and dependable cyber fraud detection, emphasizing its capacity to markedly enhance the security of financial transactions.},
  archive      = {J_PEERJCS},
  author       = {Eyad Abdel Latif Marazqah Btoush and Xujuan Zhou and Raj Gururajan and Ka Ching Chan and Omar Alsodi},
  doi          = {10.7717/peerj-cs.3007},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e3007},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing credit card fraud detection with a stacking-based hybrid machine learning approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing emotion classification on the ISEAR dataset using fine-tuning and data augmentation with hybrid transformer models. <em>PEERJCS</em>, <em>11</em>, e2984. (<a href='https://doi.org/10.7717/peerj-cs.2984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion detection is a natural language processing task used in many applications, including customer support, human mental disorder identification, and analysis of social platforms. This study examines how data augmentation methods can be combined with convolutional neural networks (CNNs) and transformer models to improve emotion detection on the International Survey on Emotion Antecedents and Reactions (ISEAR) dataset affected by scarce labeled data and class imbalances. The study evaluates the performance outputs between five transformer models: Electra and XLNet with RoBERTa and T5 along with DeBERTa, while processing anger, joy, sadness, fear, disgust, guilt, and shame emotions. Combining DeBERTa-v3-large with CNN achieves an accuracy rate of 94.94%, the top result for identifying Shame and Disgust classification. The XLNet-base and Electra-base-discriminator produce results at 93% accuracy, but the T5-base displays notably lower results at 69%. Implementing CNN layers with transformer models like Electra and RoBERTa improves model performance, particularly in precision and recall measurements. The performance metrics of Electra Base + CNN surpass those measured in the baseline Electra Base model through better precision and recall values. Such models become stronger at identifying local dependencies when CNN techniques are integrated, thus boosting their capacity to detect emotional nuances. Synonym replacement methods extended the training data to overcome class imbalance problems while minimizing the occurrence of overfitting. This technique improved the model’s generalization, producing higher classification accuracy. The research shows that DeBERTa-v3-large + CNN performs optimally as an emotion classifier since it demonstrated superior precision and steadfast emotional identification among alternative models. The research adds knowledge to developing resistant emotion recognition algorithms, particularly in cases where training data availability is restricted. It also creates new pathways for improved augmentation practices involving back-translation and generative adversarial networks (GANs).},
  archive      = {J_PEERJCS},
  author       = {Uzair Muhammad and Khalil Ullah and Ibrar Hussain and Sulaiman Almutairi and Ikram Syed and Mohammed Abohashrh},
  doi          = {10.7717/peerj-cs.2984},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e2984},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing emotion classification on the ISEAR dataset using fine-tuning and data augmentation with hybrid transformer models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive congestion control algorithm for improving transmission control protocol performance over cellular-to-cloud networks. <em>PEERJCS</em>, <em>11</em>, e2956. (<a href='https://doi.org/10.7717/peerj-cs.2956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, significant advancements have been made in enhancing congestion control algorithms (CCAs) within the Transmission Control Protocol (TCP) for end-to-end communication in millimeter-wave (mmWave) cellular networks. However, TCP often struggles to effectively utilize available bandwidth in mmWave-based cellular-to-cloud networks due to round-trip time (RTT) constraints, resulting in suboptimal throughput and latency that negatively impact cellular-to-cloud applications. To address this limitation, we propose an adaptive CCA, MRVHS-based CCA, which integrates maximum segment size (MSS) and RTT variations into a comprehensive CCA framework. MRVHS is implemented and evaluated using the ns-3 network simulator, with its performance compared against well-established TCP variants, including NewReno, HighSpeed, CUBIC, bottleneck bandwidth and RTT (BBR), and fuzzy logic-based TCP (FB-TCP) in cellular-to-cloud networks. Simulation results indicate that MRVHS consistently achieves higher average throughput while maintaining low latency across varying packet error rate (PER) levels. Notably, MRVHS significantly outperforms HighSpeed and CUBIC at the highest PER, achieving a 4.75% improvement over BBR in high-PER scenarios. Moreover, MRVHS demonstrates substantial throughput gains in medium- and low-PER conditions, consistently surpassing the benchmark TCP protocols. Furthermore, MRVHS demonstrates substantial throughput gains under medium and low PER conditions, surpassing the benchmark TCP protocols.},
  archive      = {J_PEERJCS},
  author       = {Omar Imhemed Alramli and Zurina Mohd Hanapi and Mohamed Othman and Idawaty Ahmad and Normalia Samian},
  doi          = {10.7717/peerj-cs.2956},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e2956},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An adaptive congestion control algorithm for improving transmission control protocol performance over cellular-to-cloud networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interests of google users in information pollution terminologies: An infodemiology study. <em>PEERJCS</em>, <em>11</em>, e2912. (<a href='https://doi.org/10.7717/peerj-cs.2912'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aim This study aimed to identify the interests of Google users in the terms “fake news,” “misinformation,” “disinformation,” and “conspiracy theory,” particularly in specific health-related contexts. Methods This longitudinal and retrospective ecological study examined computational metadata concerning the interests of Google users from 25 countries regarding the search terms “fake news,” “misinformation,” “disinformation,” and “conspiracy theory.” Initially, relative search volume (RSV) data for these four topics were extracted from Google Trends, encompassing all categories for the period between January 2004 and March 2025. The data underwent seasonal decomposition to identify the trend, seasonal, and residual components of the collected time series, using Python 3 programming libraries within the Google Colaboratory interface. The Mann–Kendall test was subsequently applied to assess the significance of the observed trends. Additionally, search queries were qualitatively evaluated to identify health-related ones. Lastly, Spearman correlation analyses were conducted to examine the relationship between the proportion of health-related queries and both Internet penetration and mean schooling levels in the selected countries. Statistical significance was established at P < 0.05. Results Searches for “fake news” showed an increasing trend in all countries, while “misinformation” followed a similar pattern except in France and Japan. Interest in “disinformation” increased in most countries but decreased in Italy and showed no trend in France and the United States. For “conspiracy theory,” decreasing trends were observed in eight countries and increasing trends in 16. Furthermore, a total of 52 health-related queries were identified, with seven linked to “misinformation” in 10 countries, five to “disinformation” in seven countries, 16 to “fake news” in 15 countries, and 24 to “conspiracy theory” in 17 countries, primarily related to COVID-19 (n = 44; 84.6%). Finally, a significant positive correlation was found only between the percentage of health-related queries and internet penetration for “misinformation” (rs = 0.48, P = 0.017). Conclusions Overall, these findings support the increasing interest of Google users in terms related to information pollution over time, although the association of these terms with health-related queries appears limited. This underscores the importance of implementing media education—particularly in schools—as a long-term strategy to promote a better understanding of key concepts among the population and to encourage the pursuit of information from diverse and reliable sources. Moreover, international organizations should actively monitor and regulate the emergence and spread of new terminology related to information pollution, especially in the context of growing internet penetration worldwide.},
  archive      = {J_PEERJCS},
  author       = {Thiago Cruvinel and Olívia Santana Jorge and Matheus Lotto and Bruna Nogueira and Ana Maria Jucá and Agnes Cruvinel},
  doi          = {10.7717/peerj-cs.2912},
  journal      = {PeerJ Computer Science},
  month        = {9},
  pages        = {e2912},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Interests of google users in information pollution terminologies: An infodemiology study},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing hand-based and controller-based interactions in virtual reality learning: Effects on presence and interaction performance. <em>PEERJCS</em>, <em>11</em>, e3168. (<a href='https://doi.org/10.7717/peerj-cs.3168'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) holds significant promise for enhancing science education by providing immersive and interactive learning experiences. However, the optimal interaction modality within educational VR environments remains an open question. This study investigates the impact of hand-based vs. controller-based interaction on sixth-grade students’ sense of presence and interaction performance in a VR science laboratory simulation. Fifty-four sixth-grade students were randomly assigned to either a hand-based interaction group or a controller-based interaction group. Participants completed three interactive science experiments (solar system, electrical circuits, and force/energy) within a virtual laboratory environment designed to mimic their school’s physical lab. Presence was assessed using a validated Turkish adaptation of the Presence Questionnaire (PQ), while interaction performance was evaluated using a structured observation form completed by a school teacher. Independent samples t-tests and Mann-Whitney U tests were used to compare the presence and performance scores between the groups. Supplementary analyses explored the effects of gender and prior VR experience. Contrary to expectations, no significant differences were found in either presence (t(49.4) = −0.01, p = 0.992) or interaction performance (t(52) = −1.30, p = 0.199) between the hand-based and controller-based interaction groups. Both interaction modalities yielded comparable levels of self-reported presence and observed performance. However, an unexpected finding emerged regarding performance. A supplementary analysis revealed a significant main effect of gender on performance scores (F(1, 50) = 4.844, p = 0.032), independent of interaction type. Specifically, males demonstrated significantly higher performance than females. This study suggests that, for sixth-grade students engaging in these specific VR science simulations, hand-based and controller-based interactions are equally effective in terms of fostering presence and supporting interaction performance. These findings have practical implications for the design and implementation of VR learning environments, particularly in resource-constrained settings where the reduced maintenance and hygiene concerns associated with hand-based interaction may be advantageous.},
  archive      = {J_PEERJCS},
  author       = {Murat Saran},
  doi          = {10.7717/peerj-cs.3168},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3168},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparing hand-based and controller-based interactions in virtual reality learning: Effects on presence and interaction performance},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end autonomous driving model based on visual perception for temporary roads. <em>PEERJCS</em>, <em>11</em>, e3152. (<a href='https://doi.org/10.7717/peerj-cs.3152'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The research on autonomous driving using deep learning has made significant progress on structured roads, but there has been limited research on temporary roads. The End-to-End autonomous driving model is highly integrated, allowing for the direct translation of input data into desired driving actions. This method eliminates inter-module coupling, thereby enhancing the safety and stability of autonomous vehicles. Methods Therefore, we propose a novel End-to-End model for autonomous driving on temporary roads specifically designed for mobile robots. The model takes three road images as input, extracts image features using the Global Context Vision Transformer (GCViT) network, plans local paths through a Transformer network and a gated recurrent unit (GRU) network, and finally outputs the steering angle through a control model to manage the automatic tracking of unmanned ground vehicles. To verify the model performance, both simulation tests and field tests were conducted. Results The experimental results demonstrate that our End-to-End model accurately identifies temporary roads. The trajectory planning time for a single frame is approximately 100 ms, while the average trajectory deviation is 0.689 m. This performance meets the real-time processing requirements for low-speed vehicles, enabling unmanned vehicles to execute tracking tasks in temporary road environments.},
  archive      = {J_PEERJCS},
  author       = {Qinghua Su and Min Xie and Liyong Wang and Yue Song and Ao Cui and Zhihao Xie},
  doi          = {10.7717/peerj-cs.3152},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3152},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An end-to-end autonomous driving model based on visual perception for temporary roads},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mam-incept-net: A novel inception model for precise interpretation of mammography images. <em>PEERJCS</em>, <em>11</em>, e3149. (<a href='https://doi.org/10.7717/peerj-cs.3149'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early diagnosis of breast cancer through periodic screening is a vital ally in the fight for survival. Mammography, recognized as one of the most widely used and cost-effective tools for detecting early signs of asymmetry, calcification, masses, and architectural distortion in breast tissue, plays a significant role in nearly all screening scenarios. However, the interpretation and scoring of mammograms is a complex multi-parameter process that frequently leads to false-positive and false-negative results. This article introduces a new deep-learning-based model that classifies mammograms according to the Breast Imaging Reporting and Data System (BI-RADS) assessment categories. The model is trained on a private dataset, intentionally excluding no BI-RADS categories. A novel deep neural network architecture is employed to more accurately classify breasts, including their boundaries, as regions of interest (ROIs). The ConvNeXt architecture serves as a feature extractor for lower-level features, which are then combined with the layers of a randomly initialized naive inception module to capture higher-level features. Diagnosis is achieved through three experimental tests, yielding accuracy rates ranging from 82.08% to 86.27%. These promising accuracy levels, in comparison to previous studies, can be attributed to a more comprehensive approach to addressing BI-RADS scoring challenges. In addition to pursuing further enhancements in accuracy, future research should consider integrating prior radiology reports to create a more realistic end-to-end computer-aided detection system.},
  archive      = {J_PEERJCS},
  author       = {Amira Tandirovic Gursel and Yasin Kaya},
  doi          = {10.7717/peerj-cs.3149},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3149},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Mam-incept-net: A novel inception model for precise interpretation of mammography images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delegated multi-party private set intersections from extendable output functions. <em>PEERJCS</em>, <em>11</em>, e3141. (<a href='https://doi.org/10.7717/peerj-cs.3141'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operations on sensitive datasets from different parties are essential for various practical applications, such as verifying shopping lists or enforcing no-fly lists. Traditional methods often require one party to access both datasets, which poses privacy concerns. Private set operations provide a solution by enabling these functions without revealing the data involved. However, protocols involving three or more parties are generally much slower than unsecured methods. Outsourced private set operations, where computations are delegated to a non-colluding server, can significantly improve performance, though current protocols have not fully leveraged this assumption. We propose a new protocol that removes the need for public-key cryptography. Our non-interactive set intersection protocol relies solely on the security of an extendable output function, achieving high efficiency. Even in a ten-client setting with 16,384-element sets, the intersection can be computed in under 54 s without communication overhead. Our results indicate that substantial performance improvements can be made without sacrificing privacy, presenting a practical and efficient approach to private set operations.},
  archive      = {J_PEERJCS},
  author       = {Aslı Bay},
  doi          = {10.7717/peerj-cs.3141},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3141},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Delegated multi-party private set intersections from extendable output functions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight fabric defect detection with parallel dilated convolution and dual attention mechanism. <em>PEERJCS</em>, <em>11</em>, e3136. (<a href='https://doi.org/10.7717/peerj-cs.3136'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting defects in fabrics is essential to quality control in the manufacturing process of textile productions. To increase detection efficiency, a variety of automatic fabric defect detections have been developed. However, most of these methods rely on complex model with heavy parameters, leading to high computational costs that hinder their adaptation to real-time detection environments. To overcome these obstacles, we proposed a lightweight fabric defect detection (Light-FDD), building upon the You Only Look Once v8 Nano (YOLOv8n) framework with further optimizations. Specifically, the backbone employed an improved FasterNet architecture for feature extraction. In order to capture multi-scale contextual information, we designed a parallel dilated convolution downsampling (PDCD) block to replace the conventional downsampling block in the backbone. In addition, a novel dual attention mechanism, called the global context and receptive-filed (GCRF) attention, was presented to help the model focus on key regions. Furthermore, a lightweight cross-stage partial (CSP) layer was deployed by dual convolution for feature fusion, reducing redundant parameters to further lighten the model. Results from extensive experiments on public fabric defect datasets showed that Light-FDD outperforms existing state-of-the-art lightweight models in terms of detection accuracy while requiring low computational cost. The present study suggests that the performance and effectiveness of detection models can be balanced through the adoption of reasonable strategies.},
  archive      = {J_PEERJCS},
  author       = {Zheqing Zhang and Kezhong Lu and Gaoming Yang},
  doi          = {10.7717/peerj-cs.3136},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3136},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A lightweight fabric defect detection with parallel dilated convolution and dual attention mechanism},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MLPruner: Pruning convolutional neural networks with automatic mask learning. <em>PEERJCS</em>, <em>11</em>, e3132. (<a href='https://doi.org/10.7717/peerj-cs.3132'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, filter pruning has been recognized as an indispensable technique for mitigating the significant computational complexity and parameter burden associated with deep convolutional neural networks (CNNs). To date, existing methods are based on heuristically designed pruning metrics or implementing weight regulations to penalize filter parameters during the training process. Nevertheless, human-crafted pruning criteria tend not to identify the most critical filters, and the introduction of weight constraints can inadvertently interfere with weight training. To rectify these obstacles, this article introduces a novel mask learning method for autonomous filter pruning, negating requirements for weight penalties. Specifically, we attribute a learnable mask to each filter. During forward propagation, the mask is transformed to a binary value of 1 or 0, serving as indicators for the necessity of corresponding filter pruning. In contrast, throughout backward propagation, we use straight-through estimator (STE) to estimate the gradient of masks, accommodating the non-differentiable characteristic of the rounding function. We verify that these learned masks aptly reflect the significance of corresponding filters. Concurrently, throughout the mask learning process, the training of neural network parameters remains uninfluenced, therefore protecting the normal training process of weights. The efficacy of our proposed filter pruning method based on mask learning, termed MLPruner, is substantiated through its application to prevalent CNNs across numerous representative benchmarks.},
  archive      = {J_PEERJCS},
  author       = {Sihan Chen and Ying Zhao},
  doi          = {10.7717/peerj-cs.3132},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3132},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MLPruner: Pruning convolutional neural networks with automatic mask learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing a 3D convolutional neural network to detect alzheimer’s disease based on MRI. <em>PEERJCS</em>, <em>11</em>, e3129. (<a href='https://doi.org/10.7717/peerj-cs.3129'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a progressive neurological disorder that affects millions worldwide, leading to cognitive decline and memory impairment. Structural changes in the brain gradually impair cognitive functions, and by the time symptoms become evident, significant and often irreversible neuronal damage has already occurred. This makes early diagnosis critical, as timely intervention can help slow disease progression and improve patients’ quality of life. Recent advancements in machine learning and neuroimaging have enabled early detection of AD using imaging data and computer-aided diagnostic systems. Deep learning, particularly with magnetic resonance imaging (MRI), has gained widespread recognition for its ability to extract high-level features by leveraging localized connections, weight sharing, and three-dimensional invariance. In this study, we present a 3d convolutional neural network (3D-CNN) designed to enhance classification accuracy using data from the latest version of the OASIS database (OASIS-3). Unlike traditional 2D approaches, our model processes full 3D MRI scans to preserve spatial information and prevent information loss during dimensionality reduction. Additionally, we applied advanced preprocessing techniques, including intensity normalization and noise reduction, to enhance image quality and improve classification performance. Our proposed 3D-CNN achieved an impressive classification accuracy of 91%, outperforming several existing models. These results highlight the potential of deep learning in developing more reliable and efficient diagnostic tools for early Alzheimer’s detection, paving the way for improved clinical decision-making and patient outcomes.},
  archive      = {J_PEERJCS},
  author       = {Maitha Alarjani and Abdulmajeed Almuaibed},
  doi          = {10.7717/peerj-cs.3129},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3129},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimizing a 3D convolutional neural network to detect alzheimer’s disease based on MRI},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Plagiarism detection across languages: A comprehensive study of arabic and english-to-arabic long documents. <em>PEERJCS</em>, <em>11</em>, e3128. (<a href='https://doi.org/10.7717/peerj-cs.3128'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plagiarism detection in Arabic texts remains a significant challenge due to the complex morphological structure, rich linguistic diversity, and scarcity of high-quality labeled datasets. This study proposes a robust framework for Arabic plagiarism detection by integrating Siamese neural networks (SNN) with state-of-the-art transformer architectures, specifically AraT5 and Longformer. The system employs a hybrid workflow, combining transformer-based encoders and a classification objective to implicitly learn textual similarity. To address the inherent imbalance in Arabic plagiarism datasets, both weighted cross-entropy loss and Dice loss functions were utilized to optimize model training. Extensive experiments were conducted using the ExAraCorpusPAN2015 dataset, demonstrating the effectiveness of the proposed architecture. Results indicate that AraT5 with weighted cross-entropy loss outperformed other configurations, achieving an F1-score of 0.9058. Additionally, comparative analysis with existing methodologies highlights the superiority of our approach in handling nuanced semantic and structural variations within Arabic texts. This study underscores the importance of transformer-based architectures and class-specific loss functions in enhancing plagiarism detection accuracy in under-resourced languages like Arabic.},
  archive      = {J_PEERJCS},
  author       = {Ahmad Abdelaal and Abdallah Elsaadany and Abdelrhman Ahmed Medhat and Aysha Al Shamsi and Noha Gamal ElDin Saad Ali},
  doi          = {10.7717/peerj-cs.3128},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3128},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Plagiarism detection across languages: A comprehensive study of arabic and english-to-arabic long documents},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-based tokenization for IoT traffic classification across diverse network environments. <em>PEERJCS</em>, <em>11</em>, e3126. (<a href='https://doi.org/10.7717/peerj-cs.3126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid expansion of the Internet of Things (IoT) has significantly increased the volume and diversity of network traffic, making accurate IoT traffic classification crucial for maintaining network security and efficiency. However, existing traffic classification methods, including traditional machine learning and deep learning approaches, often exhibit critical limitations, such as insufficient generalization across diverse IoT environments, dependency on extensive labelled datasets, and susceptibility to overfitting in dynamic scenarios. While recent transformer-based models show promise in capturing contextual information, they typically rely on standard tokenization, which is ill-suited for the irregular nature of IoT traffic and often remains confined to single-purpose tasks. To address these challenges, this study introduces MIND-IoT, a novel and scalable framework for classifying generalized IoT traffic. MIND-IoT employs a hybrid architecture that combines Transformer-based models for capturing long-range dependencies and convolutional neural networks (CNNs) for efficient local feature extraction. A key innovation is IoT-Tokenize, a custom tokenization pipeline designed to preserve the structural semantics of network flows by converting statistical traffic features into semantically meaningful feature-value pairs. The framework operates in two phases: a pre-training phase utilizing masked language modeling (MLM) on large-scale IoT data (UNSW IoT Traces and MonIoTr) to learn robust representations and a fine-tuning phase that adapts the model to specific classification tasks, including binary IoT vs. non-IoT classification, IoT category classification, and device identification. Comprehensive evaluation across multiple diverse datasets (IoT Sentinel, YourThings, and IoT-FCSIT, in addition to the pre-training datasets) demonstrates MIND-IoT’s superior performance, robustness, and adaptability compared to traditional methods. The model achieves an accuracy of up to 98.14% and a 97.85% F1-score, demonstrating its ability to classify new datasets and adapt to emerging tasks with minimal fine-tuning and remarkable efficiency. This research positions MIND-IoT as a highly effective and scalable solution for real-world IoT traffic classification challenges.},
  archive      = {J_PEERJCS},
  author       = {Firdaus Afifi and Faiz Zaki and Hazim Hanif and Nik Aqil and Nor Badrul Anuar},
  doi          = {10.7717/peerj-cs.3126},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3126},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Transformer-based tokenization for IoT traffic classification across diverse network environments},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging PSO-MLP for intelligent assessment of student learning in remote environments: A multimodal approach. <em>PEERJCS</em>, <em>11</em>, e3121. (<a href='https://doi.org/10.7717/peerj-cs.3121'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid advancement of artificial intelligence (AI) has catalyzed transformative changes in education, particularly in mobile and online learning environments. While existing deep learning models struggle to efficiently integrate the complexity of remote education data and optimize model performance, this article proposes an intelligent evaluation method for students’ learning states based on multimodal data. First, the joint characteristics of the pre-class mental status survey information and the health big data of teachers and students in the online teaching process constitute input data. Then, the multilayer perceptron (MLP) is used to intelligently identify the students’ status and classify their enthusiasm for the class. Finally, the particle swarm optimization (PSO) model is used to optimize the model and improve the overall recognition rate. Compared to traditional methods, the PSO-MLP model with combined multimodal data performs well, achieving an accuracy of 0.891. It provides an operational, technical solution for the education system, provides a new AI foundation for personalized teaching and student health management by accurately assessing students’ learning status, and helps to improve the effectiveness and efficiency of remote education.},
  archive      = {J_PEERJCS},
  author       = {Jing Wang and Muhammad Asif},
  doi          = {10.7717/peerj-cs.3121},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3121},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging PSO-MLP for intelligent assessment of student learning in remote environments: A multimodal approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). International trade market forecasting and decision-making system: Multimodal data fusion under meta-learning. <em>PEERJCS</em>, <em>11</em>, e3120. (<a href='https://doi.org/10.7717/peerj-cs.3120'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional market analysis tools primarily rely on unidimensional data, such as historical trading records and price trends. However, these data are often insufficient to reflect the actual state of the market fully. This study introduces a meta-learning-based (MLB) multimodal data fusion approach to optimize feature extraction and fusion strategies, addressing the complexity and heterogeneity inherent in international trade market data. Initially, the mel-frequency cepstral coefficients (MFCC) method is employed to transform the original audio signal into more discriminative spectral features. For image data, the convolutional block attention module (CBAM) is incorporated to capture both channel-wise and spatial attention, thereby improving the model’s ability to focus on market-relevant information. In the feature fusion stage, a meta-learning bidirectional feature pyramid network (ML-BiFPN) is proposed to refine the interaction of multi-scale information via a bidirectional feature pyramid structure. An adaptive weighting mechanism is employed to adjust the feature fusion ratio dynamically. Experimental results demonstrate that the proposed multimodal data fusion model, ML-BiFPN under meta-learning, significantly outperforms existing methods in prediction performance. When tested on the publicly available Trade Map dataset, the average accuracy improves by 9.37%, and the F1-score increases by 0.0473 compare to multilayer perceptron (MLP), achieving a prediction accuracy of 94.55% and an F1-score of 0.912. Notably, under small sample conditions, the model’s advantage becomes even more pronounced, with an average precision (AP) improvement of 2.79%. These findings have significant implications for international trade market forecasting and decision-making, providing enterprises with a more comprehensive understanding of market dynamics, enhancing forecasting accuracy, and supporting scientifically informed decision-making to gain a competitive edge in the marketplace},
  archive      = {J_PEERJCS},
  author       = {Yiming Bai and Muhammad Asif},
  doi          = {10.7717/peerj-cs.3120},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3120},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {International trade market forecasting and decision-making system: Multimodal data fusion under meta-learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An advanced error state kalman filter (ESKF)-based terrain contour matching (TERCOM) method for tracking an aerial vehicle using a low-cost digital elevation map. <em>PEERJCS</em>, <em>11</em>, e3118. (<a href='https://doi.org/10.7717/peerj-cs.3118'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Terrain Aided Navigation (TAN) systems hold significant potential for delivering accurate navigation for Uncrewed Aerial Vehicles (UAVs). However, a major limitation of conventional TAN systems lies in the time-consuming correlation technique used to search the a priori map, specifically the Digital Elevation Maps (DEM). This article presents a fuzzy heuristic method for the mean absolute deviation (MAD) correlation scheme (FH-MAD), aimed at reducing the computational complexity and execution time of the TAN algorithm. The fuzzy logic system uses heading and roll angle data from onboard sensors to determine the aircraft’s matching area. The output membership functions are designed based on parameters that depend on terrain features. Additionally, the proposed method incorporates an error state Kalman Filter (ESKF) as the navigation algorithm to estimate the UAV’s position under various maneuvering conditions. To evaluate the effectiveness of the proposed system, tests were conducted using two distinct DEMs with varying topographical characteristics and dimensions. The results demonstrate improved position accuracy and a significant reduction in computation time compared to traditional TAN methods, making the approach suitable for real-time UAV navigation applications.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Bilal Kadri and Sofia Yousuf},
  doi          = {10.7717/peerj-cs.3118},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3118},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An advanced error state kalman filter (ESKF)-based terrain contour matching (TERCOM) method for tracking an aerial vehicle using a low-cost digital elevation map},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced information cross-attention fusion for drug–target binding affinity prediction. <em>PEERJCS</em>, <em>11</em>, e3117. (<a href='https://doi.org/10.7717/peerj-cs.3117'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The rapid development of artificial intelligence has permeated many fields, with its application in drug discovery becoming increasingly mature. Machine learning, particularly deep learning, has significantly improved the efficiency of drug discovery. In the core task of predicting drug–target affinity (DTA), deep learning enhances predictive performance by automatically extracting complex features from compounds and proteins. Methods Traditional approaches often rely heavily on sequence and two-dimensional structural information, overlooking critical three-dimensional and physicochemical properties. To address this, we propose a novel model—Cross Attention Fusion based on Information Enhancement for Drug–Target Affinity Prediction (CAFIE-DTA)—which incorporates protein 3D curvature and electrostatic potential information. The model approximates protein surface curvature using Delaunay triangulation, calculates total electrostatic potential via Adaptive Poisson-Boltzmann Solver (APBS) software, and employs cross multi-head attention to fuse physicochemical and sequence information of proteins. Simultaneously, it integrates graph-based and physicochemical features of compounds using the same attention mechanism. The resulting protein and compound vectors are concatenated for affinity prediction. Results Cross-validation and comparative evaluations on the benchmark Davis and KIBA datasets demonstrate that CAFIE-DTA outperforms existing methods. On the Davis dataset, it achieved improvements of 0.003 in confidence interval (CI) and 0.022 in R2. On the KIBA dataset, it improved MSE by 0.008, CI by 0.005, and R2 by 0.017. Compared to traditional models relying on 2D structures and sequence data, CAFIE-DTA shows superior performance in DTA prediction. The source code is available at: https://github.com/NTU-MedAI/CAFIE-DTA.},
  archive      = {J_PEERJCS},
  author       = {Ailu Fei and Yihan Wang and Tiantian Ruan and Yekang Zhang and Min Yao and Li Wang},
  doi          = {10.7717/peerj-cs.3117},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3117},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced information cross-attention fusion for drug–target binding affinity prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A literature survey of shapelet quality measures for time series classification. <em>PEERJCS</em>, <em>11</em>, e3115. (<a href='https://doi.org/10.7717/peerj-cs.3115'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the Internet of Things, time series classification (TSC) has gained significant attention from researchers due to its applications in various real-world fields, including electroencephalogram/electrocardiogram classification, emotion recognition, and error message detection. To improve classification performance, numerous TSC methods have been proposed in recent years. Among these, shapelet-based TSC methods are particularly notable for their intuitive interpretability. A critical task within these methods is evaluating the quality of candidate shapelets. This paper provides a comprehensive survey of the state-of-the-art measures for assessing shapelet quality. To present a structured overview, we begin by proposing a taxonomy of these measures, followed by a detailed description of each one. We then discuss these measures, highlighting the challenges faced by current research and offering suggestions for future directions. Finally, we summarize the findings of this survey. We hope that this work will serve as a valuable resource for researchers in the field.},
  archive      = {J_PEERJCS},
  author       = {Teng Li and Xiaodong Guo and Cun Ji},
  doi          = {10.7717/peerj-cs.3115},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3115},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A literature survey of shapelet quality measures for time series classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel deep learning approach for predicting stone-free rates post-ESWL on uncontrasted CT. <em>PEERJCS</em>, <em>11</em>, e3111. (<a href='https://doi.org/10.7717/peerj-cs.3111'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracorporeal shock wave lithotripsy (ESWL) is one of the most often employed therapy methods for managing kidney stones. In our work, we sought to assess the efficacy of the artificial intelligence model developed using non-contrast computed tomography (CT) images in predicting stone-free rates for ESWL. The main difference between this study and other studies is that it proposes an artificial intelligence-based model that predicts the success of ESWL treatment using artificial intelligence methods. Data from 910 patients who underwent ESWL between January 2016 and June 2021 were analyzed retrospectively. Since the local binary pattern (LBP) and histogram of oriented gradients (HOG) feature extraction methods gave more successful results than other methods, a new feature map was obtained using the neighborhood component analysis (NCA) dimension reduction method after combining the features obtained using these methods. Then, the reduced feature map was classified into classifiers. In conclusion, we analyzed the effect of ESWL treatment using different artificial intelligence methods and found that the prediction accuracy was 94% on average. Results were obtained from seven different convolutional neural networks (CNNs) and two textural-based models in the study. Since textural-based models achieved the highest success among these models, these models were used as the base in the proposed model. The proposed model achieved better results than nine different models used in the study. When the results obtained from the proposed hybrid model for ESWL prediction are examined, this model will guide experts in the treatment of the disease.},
  archive      = {J_PEERJCS},
  author       = {Ozgur Efiloglu and Muhammed Yildirim and Kadir Yildirim and Harun Bingol and Mustafa Kaan Akalin and Meftun Culpan and Bilal Alatas and Asif Yildirim},
  doi          = {10.7717/peerj-cs.3111},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3111},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel deep learning approach for predicting stone-free rates post-ESWL on uncontrasted CT},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving course evaluation processes in higher education institutions: A modular system approach. <em>PEERJCS</em>, <em>11</em>, e3110. (<a href='https://doi.org/10.7717/peerj-cs.3110'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Course and instructor evaluations (CIE) are essential tools for assessing educational quality in higher education. However, traditional CIE systems often suffer from inconsistencies between structured responses and open-ended feedback, leading to unreliable insights and increased administrative workload. This study suggests a modular system to address these challenges, leveraging sentiment analysis and inconsistency detection to enhance the reliability and efficiency of CIE processes. Background Improving the reliability of CIE data is crucial for informed decision-making in higher education. Existing methods fail to address discrepancies between numerical scores and textual feedback, resulting in misleading evaluations. This study proposes a system to identify and exclude inconsistent data, providing more reliable insights. Methods Using the Design Science Research Methodology (DSRM), a system architecture was developed with five modules: data collection, preprocessing, sentiment analysis, inconsistency detection, and reporting. A dataset of 13,651 anonymized Turkish CIE records was used to train and evaluate machine learning algorithms, including support vector machines, naive Bayes, random forest, decision trees, K-nearest neighbors, and OpenAI’s GPT-4 Turbo Preview model. Sentiment analysis results from open-ended responses were compared with structured responses to identify inconsistencies. Results The GPT-4 Turbo Preview model outperformed traditional algorithms, achieving 85% accuracy, 88% precision, and 95% recall. Analysis of a prototype system applied to 431 CIEs identified a 37% inconsistency rate. By excluding inconsistent data, the system generated reliable reports with actionable insights for course and instructor performance. The purpose of this study is to design and evaluate a new system using the Design Science Research (DSR) approach to enhance the accuracy and reliability of course evaluation processes employed in higher education institutions. The modular system effectively addresses inconsistencies in CIE processes, offering a scalable and adaptable solution for higher education institutions. By integrating advanced machine learning techniques, the system enhances the accuracy and reliability of evaluation reports, supporting data-driven decision-making. Future work will focus on refining sentiment analysis for neutral comments and broadening the system’s applicability to diverse educational contexts. This innovative approach represents a significant advancement in leveraging technology to improve educational quality.},
  archive      = {J_PEERJCS},
  author       = {İlker Kocaoğlu and Erinç Karataş},
  doi          = {10.7717/peerj-cs.3110},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3110},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving course evaluation processes in higher education institutions: A modular system approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Palmprint recognition based on principal line features. <em>PEERJCS</em>, <em>11</em>, e3109. (<a href='https://doi.org/10.7717/peerj-cs.3109'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing prevalence and diversity of imaging devices, palmprint recognition has emerged as a technology that better meets the demands of the modern era. However, traditional manual methods have limitations in effectively extracting palmprint principal line features. To address this, we introduce a novel data augmentation method. First, the wide line extraction (WLE) filter is utilized to specifically target and extract the prominent principal lines of palmprints by leveraging their direction and width characteristics. Then, a Gabor filter is applied to the WLE-extracted results to purify the features and remove fine lines, as fine lines can introduce noise and redundancy that interfere with the accurate extraction of significant principal line features crucial for palmprint recognition. Evaluating this data augmentation across four common Vision Transformer (ViT) classification models, experimental results show that it improves the recognition rates of all databases to varying degrees, with a remarkable 32.9% increase on the high-resolution XINHUA database. With the successful removal of fine lines by WLE, we propose a new Layer Visual Transformer (LViT) design paradigm. For its input, distinct blocking strategies are adopted, carefully designed to partition the data to capture different levels of spatial and feature information, using larger blocks for global structure and smaller ones for local details. The output results of these different blocking strategies are fused by “sum fusion” and “maximum fusion”, and the local and global features are effectively utilized by combining complementary information to improve the recognition performance and get state-of-the-art results on multiple databases. Moreover, LViT requires fewer training iterations due to the synergistic effects of the blocking strategies, optimizing the learning process. Finally, by simulating real-world noise conditions, we comprehensively evaluate LViT and find that, compared with traditional methods, our approach exhibits excellent noise-resistant generalization ability, maintaining stable performance across the PolyU II, IIT Delhi, XINHUA, and NTU-CP-V1 databases.},
  archive      = {J_PEERJCS},
  author       = {Hongxia Wang and Teng Lv},
  doi          = {10.7717/peerj-cs.3109},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3109},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Palmprint recognition based on principal line features},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BEPCD: An ensemble learning-based intrusion detection framework for in-vehicle CAN bus. <em>PEERJCS</em>, <em>11</em>, e3108. (<a href='https://doi.org/10.7717/peerj-cs.3108'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development and widespread adoption of intelligent vehicles and the Internet of Vehicles (IoV), vehicle security has become a growing concern. Modern vehicles manage key components via the controller area network (CAN) connected electronic control units (ECUs). CAN bus intrusion techniques are the primary methods of compromising the IoV, posing a significant threat to the normal operation of critical vehicle systems, such as the power systems. However, existing attack detection methods still have shortcomings in terms of feature extraction and the diversity of attack type detection. To address these challenges, we propose an intrusion detection framework named basic ensemble and pioneer class decision (BEPCD). The framework first constructs a 15-dimensional feature model to hierarchically characterize CAN bus messages. Subsequently, BEPCD incorporates multi-model ensemble learning enhanced by a Pioneer class selector and confidence-driven voting mechanisms, enabling precise classification of both conventional and emerging attack patterns. Additionally, we analyze the importance of different data features across four machine learning algorithms. Experimental results on public datasets demonstrate that the proposed detection framework effectively detects intrusions in-vehicle CAN bus. Compared to other intrusion detection frameworks, our framework improves the overall F1-score by 1% to 5%. Notably, it achieves an approximately 77.5% performance enhancement in detecting replay attacks.},
  archive      = {J_PEERJCS},
  author       = {Bocheng Xu and Fei Cao and Xilong Li and Song Tian and Wenbo Deng and Shudan Yue},
  doi          = {10.7717/peerj-cs.3108},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3108},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {BEPCD: An ensemble learning-based intrusion detection framework for in-vehicle CAN bus},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 2LE-BO-DeepTrade: An integrated deep learning framework for stock price prediction. <em>PEERJCS</em>, <em>11</em>, e3107. (<a href='https://doi.org/10.7717/peerj-cs.3107'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel, integrated deep-learning framework named 2LE-BO-DeepTrade for stock closing price prediction. This framework combines 2LE-ICEEMDAN denoising, deep learning models tuned with Bayesian optimization, and a piecewise linear representation (PLR)-based trading strategy. The framework utilizes the model that provides the highest accuracy among optimized long short-term memory (LSTM), long short term memory with batch normalization (LSTM-BN), and gated recurrent unit (GRU) models on data preprocessed with the 2LE-ICEEMDAN denoising method. The model’s performance is comprehensively evaluated using both statistical metrics and a PLR-based trading strategy specifically developed for this study. Experimental studies were conducted on AKBNK, MGROS, KCHOL, THYAO, and ULKER stocks, which are traded on Borsa Istanbul and represent different sectors. During the denoising phase, noise in the stock prices was successfully removed, and noiseless intrinsic mode functions (IMFs) were obtained. The optimal model and hyperparameters for each IMF component were determined using Bayesian optimization, significantly improving prediction accuracy. The model within this framework, characterized by its optimized yet simple structure, demonstrated superior predictive performance compared to the more complex ICE2DE-MDL model in the literature. When compared to ICE2DE-MDL, the 2LE-BO-DeepTrade model, across all tested stocks, reduced the average root mean square error (RMSE) value by 94.4%, the average mean absolute error (MAE) value by 93.6%, and the average mean absolute percentage error (MAPE) value by 37.4% while increasing the average R2 value by 1.1%. Furthermore, the PLR-based trading strategy, specifically developed for this study, generated “Buy” and “Sell” signals, exhibiting a remarkably superior financial performance to a passive investment strategy. Across all considered stocks, the PLR-based strategy yielded, on average, 66 times more profit than the passive approach. These findings substantiate that the proposed integrated deep learning-based stock forecasting framework can significantly enhance the accuracy of stock market predictions and the returns of trading strategies.},
  archive      = {J_PEERJCS},
  author       = {Zinnet Duygu Akşehir and Erdal Kılıç},
  doi          = {10.7717/peerj-cs.3107},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3107},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {2LE-BO-DeepTrade: An integrated deep learning framework for stock price prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data trace as the scientific foundation for trusted metrological data: A review for future metrology direction. <em>PEERJCS</em>, <em>11</em>, e3106. (<a href='https://doi.org/10.7717/peerj-cs.3106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of the digital transformation of metrology, ensuring the trustworthiness and integrity of measurement data during its generation, transmission, and storage—i.e., trustworthy detection of measurement data—has become a critical challenge. Data traces are residual marks left during the data processing, which help identify malicious activities targeting measurement data. These traces are especially important when the trust and integrity of potential data evidence are under threat. To this end, this article systematically reviews relevant core techniques and analyzes various detection methods across the different stages of the data lifecycle, evaluating their applicability and limitations in identifying data tampering, unauthorized access, and anomalous operations. The findings suggest that trace detection technologies can enhance the traceability and transparency of metrological data, thereby providing technical support for building a trustworthy digital metrology system. This review lays the theoretical foundation for future research on developing automated anomaly detection models, improving forensic techniques for data tampering in measurement devices, and constructing multi-modal, full-lifecycle traceability frameworks for measurement data. Subsequent studies should focus on aligning these technologies with metrological standards and verifying their deployment in real-world measurement instruments.},
  archive      = {J_PEERJCS},
  author       = {Zhanshuo Cao and Boyong Gao and Zilong Liu and Xingchuang Xiong and Bin Wang and Chenbo Pei},
  doi          = {10.7717/peerj-cs.3106},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3106},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data trace as the scientific foundation for trusted metrological data: A review for future metrology direction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of deep learning methods in aquatic animal husbandry. <em>PEERJCS</em>, <em>11</em>, e3105. (<a href='https://doi.org/10.7717/peerj-cs.3105'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aquatic animal husbandry is crucial for global food security and supports millions of livelihoods around the world. With the growing demand for seafood, this industry has become economically significant for many regions, contributing to local and global economies. However, as the industry grows, it faces various major challenges that are not encountered in small-scale setups. Traditional methods for classifying, detecting, and monitoring aquatic animals are often time-consuming, labor-intensive, and prone to inaccuracies. The labor-intensive nature of these operations has led many aquaculture operators to move towards automation systems. Yet, for an automation system to be effectively deployed, it needs an intelligent decision-making system, which is where deep learning techniques come into play. In this article, an extensive methodological review of machine learning methods, primarily the deep learning methods used in aquatic animal husbandry are concisely summarized. This article focuses on the use of deep learning in three key areas: classification, localization, and segmentation. Generally, classification techniques are vital in distinguishing between different species of aquatic organisms, while localization methods are used to identify the respective animal’s position within a video or an image. Segmentation techniques, on the other hand, enable the precise delineation of organism boundaries, which is essential information in accurate monitoring systems. Among these key areas, segmentation techniques, particularly through the U-Net model, have shown the best results, even achieving a high segmentation performance of 94.44%. This article also highlights the potential of deep learning to enhance the precision, productivity, and sustainability of automated operations in aquatic animal husbandry. Looking ahead, deep learning offers huge potential to transform the aquaculture industry in terms of cost and operations. Future research should focus on refining existing models to better address real-world challenges such as sensor input quality and multi-modal data across various environments for better automation in the aquaculture industry.},
  archive      = {J_PEERJCS},
  author       = {Marzuraikah Mohd Stofa and Fatimah Az Zahra Azizan and Mohd Asyraf Zulkifley},
  doi          = {10.7717/peerj-cs.3105},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3105},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A review of deep learning methods in aquatic animal husbandry},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced BERT model with improved local feature extraction and long-range dependency capture in promoter prediction for hearing loss. <em>PEERJCS</em>, <em>11</em>, e3104. (<a href='https://doi.org/10.7717/peerj-cs.3104'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Promoter prediction has a key role in helping to understand gene regulation and in developing gene therapies for complex diseases such as hearing loss (HL). While traditional Bidirectional Encoder Representations from Transformers (BERT) models excel in capturing contextual information, they often have limitations in simultaneously extracting local sequence features and long-range dependencies inherent in genomic data. To address this challenge, we propose DNABERT-CBL (DNABERT-2_CNN_BiLSTM), an enhanced BERT-based architecture that fuses a convolutional neural network (CNN) and a bidirectional long and short-term memory (BiLSTM) layer. The CNN module is able to capture local regulatory features, while the BiLSTM module can effectively model long-distance dependencies, enabling efficient integration of global and local features of promoter sequences. The models are optimized using three strategies: individual learning, cross-disease training and global training, and the performance of each module is verified by constructing comparison models with different combinations. The experimental results show that DNABERT-CBL outperforms the baseline DNABERT-2_BASE model in hearing loss promoter prediction, with a 20% reduction in loss, a 3.3% improvement in the area under the working characteristic curve (AUC) of the subjects, and a 5.8% improvement in accuracy at a sequence length of 600 base pairs. In addition, DNABERT-CBL consistently outperforms other state-of-the-art BERT-based genome models on several evaluation metrics, highlighting its superior generalization ability. Overall, DNABERT-CBL provides an effective framework for accurate promoter prediction, offers valuable insights into gene regulatory mechanisms, and supports the development of gene therapies for hearing loss and related diseases.},
  archive      = {J_PEERJCS},
  author       = {Jing Sun and Yangfan Huang and Jiale Fu and Li Teng and Xiao Liu and Xiaohua Luo},
  doi          = {10.7717/peerj-cs.3104},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3104},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An enhanced BERT model with improved local feature extraction and long-range dependency capture in promoter prediction for hearing loss},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software defined network intrusion system to detect malicious attacks in computer internet of things security using deep extractor supervised random forest technique. <em>PEERJCS</em>, <em>11</em>, e3103. (<a href='https://doi.org/10.7717/peerj-cs.3103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The architecture of software-defined networking (SDN) involves the separation of the network control plane from the routing plane. If this initiative turns out well, it has the potential to reduce operating expenses and the duration required to provide new services in comparison to traditional networks. However, this architecture has additional security concerns, including a single point of failure that could potentially provide any user with unrestricted access to the entire network. Nevertheless, it is essential to reduce the probability of security breaches. The development of immediate intrusion detection systems (IDSs) that can quickly spot and stop malicious activities like distributed denial of service (DDoS), DoS, web-attacks, and Bot-NET is an important part of SDN architecture. Several researchers are using cutting-edge methods, such as machine learning, to investigate and elucidate the causes behind the sudden rise in attacks and abnormal behavior, but the majority of these methods are deficient in terms of flexibility and accuracy. This study proposed a lightweight method for detecting different SDN attacks from intrusion-defined networks. The lightweight long short-term memory (LSTM) network has the capability to capture temporal patterns and sequential interactions in the SDN data. It also learned important context that is efficient for feature extraction and then developed supervised random forest (SRF) for the attack prediction. The dataset consists of 207,146 rows and 84 features that were preprocessed, including separate features and target attacks. The experiments show that the proposed method achieved 99.93% accuracy for attack detection and 0.0090 loss, confirming its efficacy. We also tested the proposed method on another SDN dataset and achieved 99.43% accuracy for multi-class attack detection. Furthermore, the use of supervised random forest reduces the model’s complexity, resulting in increased overall efficiency.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Mujahid and Abeer Rashad Mirdad and Faten S. Alamri and Anees Ara and Amjad Khan},
  doi          = {10.7717/peerj-cs.3103},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3103},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Software defined network intrusion system to detect malicious attacks in computer internet of things security using deep extractor supervised random forest technique},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pattern transition recognition based on transfer learning for exoskeleton across different terrains. <em>PEERJCS</em>, <em>11</em>, e3099. (<a href='https://doi.org/10.7717/peerj-cs.3099'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motion intention detection is a growing trend in wearable robots. In the study, a novel transfer learning method based on temporal convolutional network spatial attention (TCN-SA) is applied for pattern transition recognition under triple physical loads on different terrains. The proposed approach is used to recognize eight locomotion modes transition among five dynamic locomotion modes in sequence, such as level ground walking, stair ascending, stair descending, ramp ascending, and ramp descending. To address the problem of pattern transition recognition, transfer learning adapts a model from the source domain to the target domain. Temporal convolutional network (TCN) relies on local relationships in time sequence and gains steady gradient propagation. Furthermore, spatial attention (SA) provides insight into significant components in multi-dimensional feature selection. Pattern transition recognition based on a transfer learning method achieves higher accuracy and earlier prediction time (Pre-T). The accuracy of pattern transition detection reaches 97.46%, 97.62%, and 98.21% in M0, M20, and M40, respectively. In the process of pattern transition recognition, Pre-T of next locomotion mode in M0, M20, and M40 are 240–600 ms, 200–410 ms, and 120–420 ms before the step into that locomotion mode. The proportion of prediction time in a gait cycle (Pre-T/GC) in M0, M20, and M40 is 14.2–36%, 14.82–28.22%, and 7.4–29.1%, respectively. Ultimately, the results indicate that the proposed approach fulfills the expected performance in Pre-T and comparisons with TCN-attention, TCN, residual network (ResNet), and long short-term memory (LSTM) in assessment criteria. Our study early detects pattern transition, allowing the exoskeleton to traverse between adjacent terrains smoothly.},
  archive      = {J_PEERJCS},
  author       = {Yifan Gao and Jianbin Zheng and Yang Gao and Ziyao Chen and Jing Tang and Liping Huang},
  doi          = {10.7717/peerj-cs.3099},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3099},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Pattern transition recognition based on transfer learning for exoskeleton across different terrains},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing transformer-based prediction of human microbe–disease associations through integrated loss strategies. <em>PEERJCS</em>, <em>11</em>, e3098. (<a href='https://doi.org/10.7717/peerj-cs.3098'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microorganisms play an important role in many complex diseases, influencing their onset, progression, and potential treatment outcomes. Exploring the associations between microbes and human diseases can deepen our understanding of disease mechanisms and assist in improving diagnosis and therapy. However, traditional biological experiments used to uncover such relationships often demand substantial time and resources. In response to these limitations, computational methods have gained traction as more practical tools for predicting microbe-disease associations. Despite their growing use, many of these models still face challenges in terms of accuracy, stability, and adaptability to noisy or sparse data. To overcome the aforementioned limitations, we propose a novel predictive framework, HyperGraph Neural Network with Transformer for Microbe-Disease Associations (HGNNTMDA), designed to infer potential associations between human microbes and diseases. The framework begins by integrating microbe–disease association data with similarity-based features to construct node representations. Two graph construction strategies are employed: a K-nearest neighbor (KNN)-based adjacency matrix to build a standard graph, and a K-means clustering approach that groups similar nodes into clusters, which serve as hyperedges to define the incidence matrix of a hypergraph. Separate hypergraph neural networks (HGNNs) are then applied to microbe and disease graphs to extract structured node-level features. An attention mechanism (AM) is subsequently introduced to emphasize informative signals, followed by a Transformer module to capture contextual dependencies and enhance global feature representation. A fully connected layer then projects these features into a unified space, where association scores between microbes and diseases are computed. For model optimization, we propose a hybrid loss strategy combining contrastive loss and Huber loss. The contrastive loss aids in learning discriminative embeddings, while the Huber loss enhances robustness against outliers and improves predictive stability. The effectiveness of HGNNTMDA is validated on two benchmark datasets—HMDAD and Disbiome—using five-fold cross-validation (5CV). Our model achieves an AUC of 0.9976 on HMDAD and 0.9423 on Disbiome, outperforming six existing state-of-the-art methods. Further case studies confirm its practical value in discovering novel microbe–disease associations.},
  archive      = {J_PEERJCS},
  author       = {Rong Zhu and Yong Wang and Junliang Shang and Ling-Yun Dai and Feng Li},
  doi          = {10.7717/peerj-cs.3098},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3098},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimizing transformer-based prediction of human microbe–disease associations through integrated loss strategies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy evaluation and explainable machine learning for diagnosis of rheumatic and autoimmune diseases. <em>PEERJCS</em>, <em>11</em>, e3096. (<a href='https://doi.org/10.7717/peerj-cs.3096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, a new combination of an explainable machine learning approach with a fuzzy evaluation framework is proposed to improve the diagnostic performance and interpretation of rheumatic and autoimmune diseases. This work addresses three major challenges: (i) overlapping symptoms and complex clinical presentations, (ii) the lack of interpretability in traditional machine learning models, and (iii) the difficulty of selecting the best diagnosis model. To overcome these challenges, a new dataset was collected from Iraq’s hospitals and health centers between 2019 and 2024. The size of dataset is 12,085 patients and includes 14 features in seven classes (rheumatoid arthritis, reactive arthritis, ankylosing spondylitis, Sjogren syndrome, systemic lupus erythematosus, psoriatic arthritis, and normal). The dataset is subjected to extensive preprocessing with attribute imputation (mean and mode), encoding categorical features, and balancing the data to pass it to 12 different machine learning models. Performance is evaluated based on precision, recall, F-score, kappa, Hamming loss, Matthews correlation coefficient, and accuracy to identify the best model. To select the optimal model, we apply fuzzy decision by opinion score method (FDOSM). The FDOSM process involves assessments from three domain experts to ensure a robust and well-rounded evaluation. Furthermore, the explainable artificial intelligence (XAI) technique provides global and local explanations for model predictions. Local interpretable model explanations (LIME) were used as explanations and significantly increased the transparency and reliability of the clinical decision-making process. The results show that the FDOSM yields gradient boosting with a 0.1333 score and a rank of 1, is the best model with an accuracy of 86.89%, precision of 87.35%, and kappa of 84.51%. The best model using XAI to increase confidence and trustworthiness in clinical decision-making and healthcare applications.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Fadhil Mahdi and Arezoo Jahani and Dhafar Hamed Abd},
  doi          = {10.7717/peerj-cs.3096},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3096},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fuzzy evaluation and explainable machine learning for diagnosis of rheumatic and autoimmune diseases},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The DBCV index is more informative than DCSI, CDbw, and VIASCKDE indices for unsupervised clustering internal assessment of concave-shaped and density-based clusters. <em>PEERJCS</em>, <em>11</em>, e3095. (<a href='https://doi.org/10.7717/peerj-cs.3095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering methods are unsupervised machine learning techniques that aggregate data points into specific groups, called clusters, according to specific criteria defined by the clustering algorithm employed. Since clustering methods are unsupervised, no ground truth or gold standard information is available to assess its results, making it challenging to know the results obtained are good or not. In this context, several clustering internal rates are available, like Silhouette coefficient, Calinski-Harabasz index, Davies-Bouldin, Dunn index, Gap statistic, and Shannon entropy, just to mention a few. Even if popular, these clustering internal scores work well only when used to assess convex-shaped and well-separated clusters, but they fail when utilized to evaluate concave-shaped and nested clusters. In these concave-shaped and density-based cases, other coefficients can be informative: Density-Based Clustering Validation Index (DBCVI), Compose Density between and within clusters Index (CDbw), Density Cluster Separability Index (DCSI), Validity Index for Arbitrary-Shaped Clusters based on the kernel density estimation (VIASCKDE). In this study, we describe the DBCV index precisely, and compare its outcomes with the outcomes obtained by CDbw, DCSI, and VIASCKDE on several artificial datasets and on real-world medical datasets derived from electronic health records, produced by density-based clustering methods such as density-based spatial clustering of applications with noise (DBSCAN). To do so, we propose an innovative approach based on clustering result worsening or improving, rather than focusing on searching the “right” number of clusters like many studies do. Moreover, we also recommend open software packages in R and Python for its usage. Our results demonstrate the higher reliability of the DBCV index over CDbw, DCSI, and VIASCKDE when assessing concave-shaped, nested, clustering results.},
  archive      = {J_PEERJCS},
  author       = {Davide Chicco and Giuseppe Sabino and Luca Oneto and Giuseppe Jurman},
  doi          = {10.7717/peerj-cs.3095},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3095},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The DBCV index is more informative than DCSI, CDbw, and VIASCKDE indices for unsupervised clustering internal assessment of concave-shaped and density-based clusters},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing variable neighbourhood search algorithms for the direct aperture optimisation in radiotherapy. <em>PEERJCS</em>, <em>11</em>, e3094. (<a href='https://doi.org/10.7717/peerj-cs.3094'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intensity modulated radiation therapy (IMRT) is a prevalent approach for administering radiation therapy in cancer treatment. The primary objective of IMRT is to devise a treatment strategy that eradicates cancer cells from the tumour while minimising damage to the surrounding organs at risk. Conventional IMRT planning entails a sequential procedure: optimising beam intensity for a certain set of angles, followed by sequencing. Unfortunately, treatment plans obtained in the optimisation stage are severely impaired after the sequencing stage due to physical and delivery constraints that are not considered during the optimisation stage. One method that tackles the issues above is the direct aperture optimisation (DAO) technique. The DAO problem seeks to generate a set of deliverable aperture configurations and a corresponding set of radiation intensities. This method accounts for physical and delivery time limitations, facilitating the creation of clinically appropriate treatment programs. In this article, we propose and compare two variable neighbourhood search (VNS) based algorithms, called variable neighbourhood descent (VND) and reduced variable neighbourhood search (rVNS). The VND algorithm is a deterministic variant of VNS that systematically explores different neighbourhood structures. This approach allows for a more thorough solution for space exploration while maintaining computational efficiency. The rVNS, unlike traditional VNS algorithms, does not require any transition rule, as it integrates a set of predefined neighbourhood moves at each iteration. We apply our proposed algorithms to prostate cancer cases, achieving highly competitive results for both algorithms. In particular, the proposed rVNS requires 62.75% fewer apertures and achieved a 63.93% reduction in beam-on time compared to the sequential approach’s best case, which means treatment plans that can be delivered in less time. Additionally, we evaluate the clinical quality of the treatment plans using established dosimetric indicators, comparing our results against those produced by matRad’s tool for DAO to assess target coverage and organ-at-risk sparing.},
  archive      = {J_PEERJCS},
  author       = {Mauricio Moyano and Keiny Meza-Vasquez and Gonzalo Tello-Valenzuela and Nicolle Ojeda-Ortega and Carolina Lagos and Guillermo Cabrera-Guerrero},
  doi          = {10.7717/peerj-cs.3094},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3094},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparing variable neighbourhood search algorithms for the direct aperture optimisation in radiotherapy},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multicriteria scheduling of two-subassembly products with batch availability and precedence constraints. <em>PEERJCS</em>, <em>11</em>, e3093. (<a href='https://doi.org/10.7717/peerj-cs.3093'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the multicriteria problems of scheduling a set of n products on a fabrication facility, focusing on batch availability and precedence constraints. Each product is composed of two distinct subassemblies: a common subassembly, shared across all products, and a unique subassembly unique to each product. The common subassemblies are processed together in batches, with each batch requiring an initial setup, while unique subassemblies are handled individually. The availability of a common subassembly is contingent upon the completion of its entire batch (i.e., batch availability), whereas a unique subassembly becomes available immediately after its processing. The product completion time is determined by the availability of both subassemblies. Strict (weak) precedence means that if a product precedes another, then the latter can start only after the former is completed (the latter cannot start earlier than the former). We propose O(n4)-time algorithms to simultaneously optimize makespan and maximum cost, as well as to lexicographically optimize two maximum costs and makespan under strict or weak precedence constraints.},
  archive      = {J_PEERJCS},
  author       = {Zhenxin Wen and Shuguang Li},
  doi          = {10.7717/peerj-cs.3093},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3093},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multicriteria scheduling of two-subassembly products with batch availability and precedence constraints},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of artwork resource management system based on block classification coding and bit plane rearrangement. <em>PEERJCS</em>, <em>11</em>, e3092. (<a href='https://doi.org/10.7717/peerj-cs.3092'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the vigorous development of the art market, the management of art resources is confronted with increasingly difficult challenges, such as copyright protection, authenticity verification, and efficient storage. Currently, the digital watermarking and compression schemes applied to artworks struggle to achieve an effective balance among robustness, image quality preservation, and watermark capacity. Moreover, they lack sufficient scalability when dealing with large-scale datasets. To address these issues, this article proposes an innovative algorithm that integrates watermarking and compression for artwork images, namely the Block Classification Coding—Bit Plane Rearrangement—Integrated Compression and Watermark Embedding (BCC-BPR-ICWE) algorithm. By employing refined block classification coding (RS-BCC) and optimized bit plane rearrangement (BPR) techniques, this algorithm significantly enhances the watermark embedding capacity and robustness while ensuring image quality. Experimental results demonstrate that, compared to existing classical algorithms, the proposed method excels in terms of watermarked image quality (PSNR > 57 dB, SSIM = 0.9993), watermark capacity (0.5 bpp), and tampering recovery performance (PSNR = 41.17 dB, SSIM = 0.9993). The research in this article provides strong support for its practical application in large-scale art resource management systems. The proposed technique not only promotes the application of digital watermarking and compression technologies in the field of art management but also offers new ideas and directions for the future development of related technologies.},
  archive      = {J_PEERJCS},
  author       = {Xiaomeng Xia},
  doi          = {10.7717/peerj-cs.3092},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3092},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of artwork resource management system based on block classification coding and bit plane rearrangement},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDSUD: Dynamically detecting subsequence uncertainty and diversity for active learning in imbalanced chinese sentiment analysis. <em>PEERJCS</em>, <em>11</em>, e3091. (<a href='https://doi.org/10.7717/peerj-cs.3091'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment structure analysis in Chinese text typically relies on supervised deep-learning methods for sequence labeling. However, obtaining large-scale labeled datasets is both resource-intensive and time-consuming. To address these challenges, this study proposes Dynamically Detecting Subsequence Uncertainty and Diversity (DDSUD), a Bidirectional Encoder Representations from Transformers (BERT)-based active learning framework designed to tackle subsequence uncertainty and enhance the diversity of imbalanced datasets. DDSUD combines subsequence uncertainty detection, diversity-driven sample selection, and dynamic weighting, enabling an adaptive balance between these factors throughout the active learning iterations. Experimental results show that DDSUD achieves performance close to fully supervised training schemes with only 50% of the data labeled, and outperforms other state-of-the-art active learning methods with the same amount of labeled data. Moreover, by dynamically adjusting the trade-off between subsequence uncertainty and diversity, DDSUD demonstrates strong adaptability and generalization capability in low-resource environments, especially in handling imbalanced datasets, significantly improving the recognition of minority class samples.},
  archive      = {J_PEERJCS},
  author       = {Shufeng Xiong and Yibo Si and Guipei Zhang and Bingkun Wang and Guang Zheng and Haiping Si},
  doi          = {10.7717/peerj-cs.3091},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3091},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DDSUD: Dynamically detecting subsequence uncertainty and diversity for active learning in imbalanced chinese sentiment analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of performance evaluation method for higher education reform based on adaptive fuzzy algorithm. <em>PEERJCS</em>, <em>11</em>, e3090. (<a href='https://doi.org/10.7717/peerj-cs.3090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a performance evaluation framework for university teachers based on the adaptive neural fuzzy inference system (ANFIS), aiming to enhance teaching quality and institutional management through a scientific, objective, and comprehensive assessment mechanism. The proposed method begins by developing a robust evaluation index system that integrates key dimensions of academic activity, including teaching performance, research contributions, and fundamental faculty information. A total of 16 sub-indicators are incorporated into the evaluation framework. To optimize data processing and reduce redundancy, factor analysis is applied, simplifying the indicator set while maintaining the integrity and effectiveness of the evaluation process. The core of the system leverages the strengths of both fuzzy logic and neural networks, combining the capacity of fuzzy systems to handle imprecise and uncertain information with the adaptive learning capabilities of neural networks. This hybrid approach improves the accuracy, interpretability, and adaptability of the evaluation results. By continuously optimizing the model using training data, the system dynamically refines its rule base and parameters, eliminating the reliance on manually defined parameters common in traditional fuzzy systems. The effectiveness of the ANFIS-based evaluation model is validated through empirical experiments. The results demonstrate that the proposed model outperforms conventional methods, such as backpropagation (BP) neural networks and support vector machines (SVMs), in terms of accuracy, precision, and overall performance. This research offers a novel and practical approach for evaluating university teacher performance, enabling more accurate reflection of teaching and research outcomes, and providing valuable decision-making support for academic management.},
  archive      = {J_PEERJCS},
  author       = {Dakun Yang and Muhammad Sheraz Arshad Malik},
  doi          = {10.7717/peerj-cs.3090},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3090},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of performance evaluation method for higher education reform based on adaptive fuzzy algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HMCFormer (hierarchical multi-scale convolutional transformer): A hybrid CNN+Transformer network for intelligent VIA screening. <em>PEERJCS</em>, <em>11</em>, e3088. (<a href='https://doi.org/10.7717/peerj-cs.3088'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cervical cancer ranks first in incidence among malignant tumors of the female reproductive system, and 80% of women who die from cervical cancer worldwide are from developing countries. Visual inspection with acetic acid (VIA) screening based on artificial intelligence-assisted diagnosis can provide a cheap and rapid screening method. This will attract more low-income women to volunteer for regular cervical cancer screening. However, current AI-based VIA screening studies either have low accuracy or require expensive equipment assistance. In this article, we propose the Hierarchical Multi-Scale Convolutional Transformer network, which combines the hierarchical feature extraction capability of Convolutional Neural Network (CNNs) and the global dependency modeling capability of Transformers to address the challenges of realizing intelligent VIA screening. Hierarchical multi-scale convolutional transformer (HMCFormer) can be divided into a Transformer branch and a CNN branch. The Transformer branch receives unenhanced lesion sample images, and the CNN branch receives lesion sample images enhanced by the proposed dual-color space-based image enhancement algorithm. The authors design a hierarchical multi-scale pixel excitation module for adaptive multi-scale and multi-level local feature extraction. The authors apply the structure of the Swin Transformer network with minor modifications in the global perception modeling process. In addition, the authors propose two feature fusion concepts: adaptive preprocessing and superiority-inferiority fusion, and design a feature fusion module based on these concepts, which significantly improves the collaborative ability of the Transformer branch and the CNN branch. The authors collected and summarized 5,000 samples suitable for VIA screening methods from public datasets provided by companies such as Intel and Google, forming the PCC5000 dataset. On this dataset, the proposed algorithm achieves a screening accuracy of 97.4% and a grading accuracy of 94.8%.},
  archive      = {J_PEERJCS},
  author       = {Bo Feng and Chao Xu and Zhengping Li and Chuanyi Zhang},
  doi          = {10.7717/peerj-cs.3088},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3088},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HMCFormer (hierarchical multi-scale convolutional transformer): A hybrid CNN+Transformer network for intelligent VIA screening},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Noise injection into freeman chain codes. <em>PEERJCS</em>, <em>11</em>, e3084. (<a href='https://doi.org/10.7717/peerj-cs.3084'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a novel method for direct noise injection into geometric shapes described by eight-or four-directional Freeman chain codes. Noise is applied to randomly selected segments of a chain code sequence using a set of predefined actions. The design of alterations retains topological characteristics of shapes. The method is tested on various shapes, including open, self-intersecting, and simple shapes, among which the latest two may contain holes. Fractal dimension and mean distance from original are utilised in order to analyse the amount of injected noise in sequences of chain codes. The proposed method enables efficient noise injection directly into Freeman chain codes for use in data augmentation and regularization during neural network training.},
  archive      = {J_PEERJCS},
  author       = {Luka Lukač and Andrej Nerat and Damjan Strnad and Ivana Kolingerová and Borut Žalik},
  doi          = {10.7717/peerj-cs.3084},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3084},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Noise injection into freeman chain codes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A path aggregation network with deformable convolution for visual object detection. <em>PEERJCS</em>, <em>11</em>, e3083. (<a href='https://doi.org/10.7717/peerj-cs.3083'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the main challenges encountered in visual object detection is the multi-scale issue. Many approaches have been proposed to tackle this issue. In this article, we propose a novel neck that can perform effective fusion of multi-scale features for a single-stage object detector. This neck, named the deformable convolution and path aggregation network (DePAN), is an integration of a path aggregation network with a deformable convolution block added to the feature fusion branch to improve the flexibility of feature point sampling. The deformable convolution block is implemented by repeated stacking of a deformable convolution cell. The DePAN neck can be plugged in and easily applied to various models for object detection. We apply the proposed neck to the baseline models of Yolov6-N and YOLOV6-T, and test the improved models on COCO2017 and PASCAL VOC2012 datasets, as well as a medical image dataset. The experimental results verify the effectiveness and applicability in real-world object detection.},
  archive      = {J_PEERJCS},
  author       = {Chengming Rao and Zunhao Hu and QiMing Zhao and Min Shan and Li Mao},
  doi          = {10.7717/peerj-cs.3083},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3083},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A path aggregation network with deformable convolution for visual object detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comprehensive review of ball detection techniques in sports. <em>PEERJCS</em>, <em>11</em>, e3079. (<a href='https://doi.org/10.7717/peerj-cs.3079'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting balls in sports plays a pivotal role in enhancing game analysis, providing real-time data for spectators, and improving decision-making and strategic thinking for referees and coaches. This is a highly debated and researched topic, but most works focus on one sport. Effective generalization of a single method or algorithm to different sports is much harder to achieve. This article reviews methodologies and advancements in object detection tailored to ball detection across various sports. Traditional computer vision techniques and modern deep learning methods are visited, emphasizing their strengths, limitations, and adaptability to diverse game scenarios. The challenges of occlusion, dynamic backgrounds, varying ball sizes, and high-speed movements are identified and discussed. This review aims to consolidate existing knowledge, compare state-of-the-art detection models, highlight pivotal challenges and possible solutions, and propose future research directions. The article underscores the importance of optimizations for accurate and efficient ball detection, setting the foundation for next-generation sports analytics systems.},
  archive      = {J_PEERJCS},
  author       = {Cristiano Moreira and Lino Ferreira and Paulo Jorge Coelho},
  doi          = {10.7717/peerj-cs.3079},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3079},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A comprehensive review of ball detection techniques in sports},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hardware implementation of FPGA-based spiking attention neural network accelerator. <em>PEERJCS</em>, <em>11</em>, e3077. (<a href='https://doi.org/10.7717/peerj-cs.3077'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) are recognized as third-generation neural networks and have garnered significant attention due to their biological plausibility and energy efficiency. To address the resource constraints associated with using field programmable gate arrays (FPGAs) for numerical recognition in SNNs, we proposed a lightweight spiking efficient attention neural network (SeaSNN) accelerator. We designed a simple, four-layer structured network, achieving a recognition accuracy of 93.73% through software testing on the MNIST dataset. To further enhance the model’s accuracy, we developed a highly spiking efficient channel attention mechanism (SECA), resulting in a significant performance improvement and an increase in test accuracy to 94.28%. For higher recognition speed, we optimized circuit parallelism by applying techniques such as loop unrolling, loop pipelining, and array partitioning. Finally, SeaSNN was implemented and verified on an FPGA board, achieving an inference speed of 0.000401 seconds per frame and a power efficiency of 0.42 TOPS/W at a frequency of 200 MHz. These results demonstrate that the proposed low-power, high-precision, and fast handwritten digit recognition system is well-suited for handwritten digit recognition tasks.},
  archive      = {J_PEERJCS},
  author       = {Shiyong Geng and Zhida Wang and Zhipeng Liu and Mengzhao Zhang and Xuelong Zhu and Yongping Dan},
  doi          = {10.7717/peerj-cs.3077},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3077},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hardware implementation of FPGA-based spiking attention neural network accelerator},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-step partitioning combined with SOM neural network-based clustering technique effectively improves SAT solver performance. <em>PEERJCS</em>, <em>11</em>, e3076. (<a href='https://doi.org/10.7717/peerj-cs.3076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the core engine of electronic design automation (EDA) tools, the efficiency of Boolean Satisfiability Problem (SAT) solver largely determines the cycle of integrated circuit research and development. The effectiveness of SAT solvers has steadily turned into the key bottleneck of circuit design cycle due to the dramatically increased integrated circuit scale. The primary issue of SAT solver now is the divergence between SAT used in industry and research on pure solution algorithms. We propose a strategy for partitioning the SAT problem based on the structural information then solving it. By effectively extracting the structure information from the original SAT problem, the self-organizing map (SOM) neural network deployed in the division section can speed up the sub-thread solver’s processing while avoiding cumbersome parameter adjustments. The experimental results demonstrate the stability and scalability of our technique, which can drastically shorten the time required to solve industrial benchmarks from various sources.},
  archive      = {J_PEERJCS},
  author       = {Siyu Yun and Xinsheng Wang},
  doi          = {10.7717/peerj-cs.3076},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3076},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-step partitioning combined with SOM neural network-based clustering technique effectively improves SAT solver performance},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aligning to the teacher: Multilevel feature-aligned knowledge distillation. <em>PEERJCS</em>, <em>11</em>, e3075. (<a href='https://doi.org/10.7717/peerj-cs.3075'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation is a technique for transferring knowledge from a teacher’s (large) model to a student’s (small) model. Usually, the features of the teacher model contain richer information, while the features of the student model carry less information. This leads to a poor distillation effect in the process of knowledge transfer due to insufficient feature information and poor generalisation ability of the student model. In order to effectively reduce the feature differences between teacher–student models, we propose a Multilevel Feature Alignment Knowledge Distillation method (MFAKD), which includes a spatial dimension alignment module and a multibranch channel alignment (MBCA) module. The upsampling operation enables the student feature map to align with the teacher feature map in spatial dimensions, allowing students to learn more comprehensive teacher features. Moreover, MBCA achieves the alignment of the student feature maps and teacher feature maps on channels, which can transform the student features into a form more similar to the teacher features. Subsequently, the student model uses the discriminative classifiers from the pretrained teacher model to perform the student model inference. In summary, the student model can utilize their strengths and surpass the teacher model. The method was validated on the CIFAR-100 dataset and obtains state-of-the-art results. In particular, the classification accuracy of the student model WRN-40-2 exceeds that of the teacher model ResNet-8×4 by almost 2%. The method also demonstrated excellent performance on the Tiny ImageNet dataset.},
  archive      = {J_PEERJCS},
  author       = {Yang Zhang and Pan He and Chuanyun Xu and Jingyan Pang and Xiao Wang and Xinghai Yuan and Pengfei Lv and Gang Li},
  doi          = {10.7717/peerj-cs.3075},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3075},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Aligning to the teacher: Multilevel feature-aligned knowledge distillation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An image quality assessment algorithm based on ‘global + local’ feature fusion. <em>PEERJCS</em>, <em>11</em>, e3074. (<a href='https://doi.org/10.7717/peerj-cs.3074'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there has been increasing research on image quality assessment. Among the existing mainstream approaches, image feature extraction tends to be simplistic, leading to insufficient quality information extraction and underutilization of the extracted data. Additionally, the correlation between different regions of the image is often neglected. This study proposes an image quality assessment algorithm based on global-local feature fusion (IQA-GL). First, the global and local features of the image are extracted separately, and irrelevant information in the local features is filtered out. Then, a global-local feature fusion model is constructed to enhance the interaction of feature information and gather image quality data across all feature channels. Finally, the relationship between individual image patches and the global image is modeled, adjusting the weights of each image patch to aggregate a quality score for the global image. Experimental results show the IQA-GL performs excellently on public datasets. This study innovatively combines global and local features, offering a new perspective for image quality assessment.},
  archive      = {J_PEERJCS},
  author       = {Yang Yang and Norisma Binti Idris and Ainuddin Wahid Abdul Wahab and Dingguo Yu and Chang Liu},
  doi          = {10.7717/peerj-cs.3074},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3074},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An image quality assessment algorithm based on ‘global + local’ feature fusion},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Morphological and structural complexity analysis of low-resource english-turkish language pair using neural machine translation models. <em>PEERJCS</em>, <em>11</em>, e3072. (<a href='https://doi.org/10.7717/peerj-cs.3072'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural machine translation (NMT) has achieved remarkable success in high-resource language pairs; however, its effectiveness for morphologically rich and low-resource languages like Turkish remains underexplored. As a highly agglutinative and morphologically complex language with limited high-quality parallel data, Turkish serves as a representative case for evaluating NMT systems on low-resource and linguistically challenging settings. Its structural divergence from English makes it a critical testbed for assessing tokenization strategies, attention mechanisms, and model generalizability in neural translation. This study investigates the comparative performance of two prominent NMT paradigms—the Transformer architecture, and recurrent-based sequence-to-sequence (Seq2Seq) models with attention for both English-to-Turkish and Turkish-to-English translation. The models are evaluated under various configurations, including different tokenization strategies (Byte Pair Encoding (BPE) vs. Word Tokenization), attention mechanisms (Bahdanau and an exploratory hybrid mechanism combining Bahdanau and Scaled Dot-Product attention), and architectural depths (layer count and attention head number). Extensive experiments using automatic metrics such as BiLingual Evaluation Understudy (BLEU), Metric for Evaluation of Translation with Explicit ORdering (METEOR), and Translation Error Rate (TER) reveal that the Transformer model with three layers, eight attention heads, and BPE tokenization achieved the best performance, obtaining a BLEU score of 47.85 and METEOR score of 44.62 in the English-to-Turkish direction. Similar performance trends were observed in the reverse direction, indicating the model’s generalizability. These findings highlight the potential of carefully optimized Transformer-based NMT systems in handling the complexities of morphologically rich, low-resource languages like Turkish in both translation directions.},
  archive      = {J_PEERJCS},
  author       = {Mehmet Acı and Nisa Vuran Sarı and Çiğdem İnan Acı},
  doi          = {10.7717/peerj-cs.3072},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3072},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Morphological and structural complexity analysis of low-resource english-turkish language pair using neural machine translation models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Artificial intelligence-driven insights into arab media’s sustainable development goals coverage. <em>PEERJCS</em>, <em>11</em>, e3071. (<a href='https://doi.org/10.7717/peerj-cs.3071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines how Arab media have engaged with the United Nations Sustainable Development Goals (SDGs) over the past decade and evaluates the alignment between media coverage and official government priorities. The research addresses the lack of large-scale, Arabic-focused analyses in SDG discourse, which is often dominated by English-language studies. We collected and processed a unique dataset of over 1.2 million Arabic news articles from ten countries between 2010 and 2024. Using a combination of data augmentation, deep learning (specifically, Transformer-based models), and large language models (LLMs), we trained classifiers to detect references to the SDGs and categorize articles by specific SDGs. The results reveal regional patterns in SDG coverage, with North African countries focusing more on governance-related goals, while Gulf countries emphasize economic and environmental themes. Our findings reveal a general alignment between media discourse and official SDG priorities, with notable exceptions. This study is the first to combine artificial intelligence (AI) methods and Arabic media at this scale for SDG analysis, offering new tools and insights for policymakers, media professionals, and development stakeholders.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Alsuhaibani and Kamel Gaanoun and Ali Mustafa Qamar},
  doi          = {10.7717/peerj-cs.3071},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3071},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Artificial intelligence-driven insights into arab media’s sustainable development goals coverage},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multifeature fusion for claim scope-aware litigation risk prediction for patent drafts. <em>PEERJCS</em>, <em>11</em>, e3069. (<a href='https://doi.org/10.7717/peerj-cs.3069'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ‘claim scope’, or the ‘legal boundaries’ defined by patent claims, has been considered crucial for determining a patent’s value and its associated litigation risk. However, no direct claim semantics-based indicators currently exist to quantify patent claim scope, and existing scope measures are primarily indirect, which limits their ability to capture the semantic nuances of claim text. Additionally, the reliance on post-grant features restricts the applicability of existing litigation prediction models to patent drafts. These limitations complicate the patent drafting process, during which claims are formulated without feedback on scope and litigation risk. This often leads to suboptimal claim articulation, resulting in inadequate protection, increased legal vulnerabilities, or reduced patent grant probability. To address this gap, the hyponym tree score (HTS) is proposed as a novel indicator for quantifying claim scope by analysing hyponym counts, sentence structure, and dependency relations within patent claims. Building on this, early-stage litigation risk prediction has been achieved using a new deep learning model, the Multifeature BERT-Powered Fusion for Author-level Patent Litigation Risk Analysis (MAPRA). The MAPRA model restricts its input features to those available at early stages, such as indicators derived from claim text, inventor information, assignee details, and HTS, ensuring applicability to both draft-stage and granted patents. Despite excluding all post-grant or acquired data, MAPRA achieves a superior area under the receiver operating characteristic curve (AUC) of 0.878, outperforming the most comparable prior study, which reports an AUC of 0.822 using both early-stage and immediate post-grant features. By quantifying claim scope and enabling early-stage litigation risk prediction, this research offers a valuable screening tool for patent drafters, examiners, attorneys, and innovators. It supports informed decision-making during drafting and helps mitigate potential litigation risks. Furthermore, it lays a foundation for future research on claim scope modeling and the development of predictive tools for intellectual property litigation management.},
  archive      = {J_PEERJCS},
  author       = {Chitrakala Sakthivel and Jinesh Jose},
  doi          = {10.7717/peerj-cs.3069},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3069},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multifeature fusion for claim scope-aware litigation risk prediction for patent drafts},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identity-based linear homomorphic signature for a restricted combiners’ group for e-commerce. <em>PEERJCS</em>, <em>11</em>, e3068. (<a href='https://doi.org/10.7717/peerj-cs.3068'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the volume of electronic transaction data increases and the demand for real-time processing grows, network coding techniques have become popular for improving performance. However, early implementations often overlooked critical data security issues, such as forgery and data leakage. While existing homomorphic signature schemes effectively ensure data integrity, they can unintentionally allow malicious actors to exploit intermediate signatures. This misuse can lead to unnecessary bandwidth consumption and hinder the verification processes for legitimate users. To address the problem of malicious combinations, we apply the Chinese Remainder theorem (CRT) to establish a layer of secret-sharing that restricts access to authorized users in conjunction with homomorphic signatures. Furthermore, we introduce a formal definition for an identity-based linear homomorphic signature for a restricted combiners’ group (IBLHS-RCG). This framework integrates linear homomorphic signatures with the CRT within the context of e-commerce, enabling us to develop a specialized scheme for IBLHS-RCG. We demonstrate that our scheme is unforgeable against adaptive chosen-message attacks. Additionally, simulations conducted using the Python Pairing-based Cryptography Library (PYPBC) show that the signing and verification costs of our approach are low.},
  archive      = {J_PEERJCS},
  author       = {Yuan Tian and Weitao Song and Tanping Zhou and Bin Hu and Xuan Zhou and Yujie Ding and Weidong Zhong and Xiaoyuan Yang},
  doi          = {10.7717/peerj-cs.3068},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3068},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Identity-based linear homomorphic signature for a restricted combiners’ group for e-commerce},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A social information sensitive model for conversational recommender systems. <em>PEERJCS</em>, <em>11</em>, e3067. (<a href='https://doi.org/10.7717/peerj-cs.3067'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommender systems (CRS) facilitate natural language interactions for more effective item suggestions. While these systems show promise, they face challenges in effectively utilizing and integrating informative data with conversation history through semantic fusion. In this study we present an innovative framework for extracting social information from conversational datasets by inferring ratings and constructing user-item interaction and user-user relationship graphs. We introduce a social information sensitive semantic fusion (SISSF) method that employs contrastive learning (CL) to bridge the semantic gap between generated social information and conversation history. We evaluated the framework on two public datasets (ReDial and INSPIRED) using both automatic and human evaluation metrics. Our SISSF framework demonstrated significant improvements over baseline models across all metrics. For the ReDial dataset, SISSF achieved superior performance in recommendation tasks (R@1: 0.062, R@50: 0.437) and conversational quality metrics (Distinct-2: 4.223, Distinct-3: 5.595, Distinct-4: 6.155). Human evaluation showed marked improvement in both fluency (1.81) and informativeness (1.63). We observed similar performance gains on the INSPIRED dataset, with notable improvements in recommendation accuracy (R@1: 0.046, R@10: 0.129, R@50: 0.269) and response diversity (Distinct-2: 2.061, Distinct-3: 4.293, Distinct-4: 6.242). The experimental results consistently validate the effectiveness of our approach in both recommendation and conversational tasks. These findings suggest that incorporating social context through CL can significantly improve the personalization and relevance of recommendations in conversational systems.},
  archive      = {J_PEERJCS},
  author       = {Abdulaziz Mohammed and Mingwei Zhang and Gehad Abdullah Amran and Husam M. Alawadh and Ruizhe Wang and Amerah Alabrah and Ali A. Al-Bakhrani},
  doi          = {10.7717/peerj-cs.3067},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3067},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A social information sensitive model for conversational recommender systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of streaming data anomaly detection in network security. <em>PEERJCS</em>, <em>11</em>, e3066. (<a href='https://doi.org/10.7717/peerj-cs.3066'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity has always been a subject of great concern, and anomaly detection has gained increasing attention due to its ability to detect novel attacks. However, network anomaly detection faces significant challenges when dealing with massive traffic, logs, and other forms of streaming data. This article provides a comprehensive review and a multi-faceted analysis of recent algorithms for anomaly detection in network security. It systematically categorizes and elucidates the various types of datasets, measurement techniques, detection algorithms, and output results of streaming data. Furthermore, the review critically compares network security application scenarios and problem-solving capabilities of streaming data anomaly detection methods. Building on this analysis, the study identifies and delineates promising future research directions. This article endeavors to achieve rapid and efficient detection of streaming data, thereby providing better security for network operations. This research is highly significant in addressing the challenges and difficulties of analyzing anomalies in streaming data. It also serves as a valuable reference for further development in the field of network security. It is anticipated that this comprehensive review will serve as a valuable resource for security researchers in their future investigations within network security.},
  archive      = {J_PEERJCS},
  author       = {Pengju Zhou},
  doi          = {10.7717/peerj-cs.3066},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3066},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A survey of streaming data anomaly detection in network security},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving machine learning detection of alzheimer disease using enhanced manta ray gene selection of alzheimer gene expression datasets. <em>PEERJCS</em>, <em>11</em>, e3064. (<a href='https://doi.org/10.7717/peerj-cs.3064'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most prominent neurodegenerative diseases globally is Alzheimer’s disease (AD). The early diagnosis of AD is a challenging task due to complex pathophysiology caused by the presence and accumulation of neurofibrillary tangles and amyloid plaques. However, the late enriched understanding of the genetic underpinnings of AD has been made possible due to recent advancements in data mining analysis methods, machine learning, and microarray technologies. However, the “curse of dimensionality” caused by the high-dimensional microarray datasets impacts the accurate prediction of the disease due to issues of overfitting, bias, and high computational demands. To alleviate such an effect, this study proposes a gene selection approach based on the parameter-free and large-scale manta ray foraging optimization algorithm. Given the dimensional disparities and statistical relationship distributions of the six investigated datasets, in addition to four evaluated machine learning classifiers; the proposed Sign Random Mutation and Best Rank enhancements that substantially improved MRFO’s exploration and exploitation contributed to efficient identification of relevant genes and to machine learning improved prediction accuracy.},
  archive      = {J_PEERJCS},
  author       = {Zahraa Ahmed and Mesut Çevik},
  doi          = {10.7717/peerj-cs.3064},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3064},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving machine learning detection of alzheimer disease using enhanced manta ray gene selection of alzheimer gene expression datasets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced text clustering and sentiment analysis framework for online education: A BIF-DCN approach in computer education. <em>PEERJCS</em>, <em>11</em>, e3062. (<a href='https://doi.org/10.7717/peerj-cs.3062'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding students’ emotional responses to course content and assignments is crucial for developing effective teaching strategies and improving online learning resources. To address this need, we propose a novel deep learning-based framework called BERT and BTF-IDF Integrated Framework with Deep Clustering Network (BIF-DCN), designed to accurately analyze student sentiment on educational platforms. The framework combines three key components: Bidirectional Encoder Representations from Transformers (BERT) for initial text feature extraction, Bi-level Term Frequency–Inverse Document Frequency (BTF-IDF) for enhanced feature representation, and an Improved Deep Embedded Clustering (IDEC) model for sentiment classification. BERT captures rich semantic features from student comments, which are further refined using BTF-IDF to highlight informative terms. These features are then clustered using the IDEC model to identify underlying sentiment-based topics. Experimental results show that BIF-DCN achieves higher clustering accuracy than existing IDEC-based and traditional single-model approaches on both public and self-constructed datasets. In addition to performance improvements, our method enables in-depth sentiment analysis of clustered topics, offering practical insights for optimizing teaching materials. This framework provides educators with valuable tools to better understand student needs and deliver more personalized and effective instruction, ultimately enhancing teaching quality and learner satisfaction.},
  archive      = {J_PEERJCS},
  author       = {Qingyun Zhang and Yang Li and Muhammad Sheraz Arshad Malik},
  doi          = {10.7717/peerj-cs.3062},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3062},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced text clustering and sentiment analysis framework for online education: A BIF-DCN approach in computer education},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GaitTriViT and GaitVViT: Transformer-based methods emphasizing spatial or temporal aspects in gait recognition. <em>PEERJCS</em>, <em>11</em>, e3061. (<a href='https://doi.org/10.7717/peerj-cs.3061'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image recognition tasks, subjects with long distances and low resolution remain a challenge, whereas gait recognition, identifying subjects by walking patterns, is considered one of the most promising biometric technologies due to its stability and efficiency. Previous gait recognition methods mostly focused on constructing a sophisticated model structure for better model performance during evaluation. Moreover, these methods are primarily based on traditional convolutional neural networks (CNNs) due to the dominance of CNNs in computer vision. However, since the alternative form of Transformer, named Vision Transformers (ViTs), has been introduced into the computer vision field, the ViTs have gained strong attention for its outstanding performance in various tasks. Thus, unlike previous methods, this project introduces two Transformer-based methods: a completely ViTs-based method GaitTriViT, and a Video Vision Transformer (Video ViT) based method GaitVViT. The GaitTriViT leverages the ViTs to gain more fine-grained spatial features, while GaitVViT enhances the capacity of temporal extraction. This work evaluates their performances and the results show the still-existing gaps and several encouraging outperforms compared with current state-of-the-art (SOTA), demonstrating the difficulties and challenges these Transformer-based methods will encounter continuously. However, the future of Vision Transformers in gait recognition is still promising.},
  archive      = {J_PEERJCS},
  author       = {Hongyun Sheng},
  doi          = {10.7717/peerj-cs.3061},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3061},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GaitTriViT and GaitVViT: Transformer-based methods emphasizing spatial or temporal aspects in gait recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-guided RGB-P grasp generation. <em>PEERJCS</em>, <em>11</em>, e3060. (<a href='https://doi.org/10.7717/peerj-cs.3060'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of robotics, object grasping is a complex and challenging task. Although state-of-the-art computer vision-based models have made significant progress in predicting grasps, the lack of semantic information from textual data makes them susceptible to ambiguities in object recognition. For example, when asked to grasp a specific object on a table with many objects, robots relying only on visual data can easily get confused and grasp the wrong object. To address this limitation, we propose a multimodal approach that seamlessly integrates 3D data (shape) and red-green-blue (RGB) images (color, texture) into a unified representation called red-green-blue and point cloud (RGB-P), while also incorporating semantic information from textual descriptions processed by a large language model (LLM) to enhance object disambiguation. This combination of data allows our model to accurately infer and capture target objects based on natural language descriptions, overcoming the limitations of vision-only approaches. Our approach achieves superior performance, with an average precision (AP) of 53.2% on the GraspNet-1Billion dataset, significantly outperforming state-of-the-art methods. Additionally, we introduce an automated dataset creation pipeline that addresses the challenges of data collection and annotation. This pipeline leverages cutting-edge models: LLMs for text generation, Stable Diffusion for image synthesis, Depth Anything for depth estimation, using standard intrinsic parameters from the Kinect depth sensor to ensure geometric consistency, and GraspNet for grasp estimation. This automated process generates high-quality datasets with paired RGB-P, images, textual descriptions and potential grasp poses, significantly reducing the manual effort and enabling large-scale data collection.},
  archive      = {J_PEERJCS},
  author       = {Van Duc Vu and Van Thiep Nguyen and Nam Hai Pham and Dinh-Cuong Hoang and Phan Xuan Tan},
  doi          = {10.7717/peerj-cs.3060},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3060},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Text-guided RGB-P grasp generation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MDMU-net: 3D multi-dimensional decoupled multi-scale U-net for pancreatic cancer segmentation. <em>PEERJCS</em>, <em>11</em>, e3059. (<a href='https://doi.org/10.7717/peerj-cs.3059'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pancreatic cancer, as a highly lethal malignant tumor, presents significant challenges for early diagnosis and treatment. Accurate segmentation of the pancreas and tumors is crucial for surgical planning and treatment strategy development. However, due to the variable morphology, blurred boundaries, and low contrast with surrounding tissues in CT images, traditional manual segmentation methods are inefficient and heavily reliant on expert experience. To address this challenge, this study proposes a lightweight automated 3D segmentation algorithm—Multi-Dimensional Decoupled Multi-Scale U-Net (MDMU-Net). First, depthwise separable convolution is employed to reduce model complexity. Second, a multi-dimensional decoupled multi-scale module is designed as the primary encoder module, which independently extracts features along depth, height, and width dimensions through parallel multi-scale convolutional kernels, achieving fine-grained modeling of complex anatomical structures. Finally, cross-dimensional channel and spatial attention mechanisms are introduced to enhance recognition capability for small tumors and blurred boundaries. Experimental results on the MSDPT and NIHP datasets demonstrate that MDMU-Net exhibits competitive advantages in both pancreatic segmentation DSC (0.7108/0.7709) and tumor segmentation DSC (showing an 11.8% improvement over AttentionUNet), while achieving a 15.3% enhancement in HD95 boundary accuracy compared to 3DUX-Net. While maintaining clinically viable precision, the model significantly improves computational efficiency, with parameter count (26.97M) and FLOPs (84.837G) reduced by 65.5% and 71%, respectively, compared to UNETR, providing reliable algorithmic support for precise diagnosis and treatment of pancreatic cancer.},
  archive      = {J_PEERJCS},
  author       = {Lian Lu and Miao Wu and Gan Sen and Fei Ren and Tao Hu},
  doi          = {10.7717/peerj-cs.3059},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3059},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MDMU-net: 3D multi-dimensional decoupled multi-scale U-net for pancreatic cancer segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A defensive model and implementation baseline for the metaverse and extended reality systems. <em>PEERJCS</em>, <em>11</em>, e3054. (<a href='https://doi.org/10.7717/peerj-cs.3054'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The metaverse and extended reality (XR) systems are vulnerable to emerging security threats, as developers have prioritized competitive business gains over security. The virtual entities, immersive experiences, and lack of centralized governance pose significant challenges in establishing standardized guidelines for XR systems and its stakeholders. In this research, a panoramic view is presented to identify mitigation strategies and defensive capabilities, including authenticity, privacy, integrity, interoperability, virtual forensics, and incident reporting to counter potential threats. To facilitate the implementation of a secure XR system, a novel baseline model is introduced, outlining key attributes and functions aligned with the available libraries. A statistical analysis is performed to assess the quality and effectiveness of development resources in embedding novel XR security features. Furthermore, this research assesses the security posture of prominent XR systems and examines the applicable regulatory frameworks in immersive environment. Finally, security recommendations are proposed to counter the threat landscape of XR and the metaverse.},
  archive      = {J_PEERJCS},
  author       = {Sara Qamar and Hasan Tahir and Zahid Anwar and Naveed Ahmed and Shahzaib Tahir and Muhammad Aleem},
  doi          = {10.7717/peerj-cs.3054},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3054},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A defensive model and implementation baseline for the metaverse and extended reality systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDoS attack detection in edge-IIoT digital twin environment using deep learning approach. <em>PEERJCS</em>, <em>11</em>, e3052. (<a href='https://doi.org/10.7717/peerj-cs.3052'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The industrial Internet of Things (IIoT) and digital twins are redefining how digital models and physical systems interact. IIoT connects physical intelligence, and digital twins virtually represent their physical counterparts. With the rapid growth of Edge-IIoT, it is crucial to create security and privacy regulations to prevent vulnerabilities and threats (i.e., distributed denial of service (DDoS)). DDoS attacks use botnets to overload the target system with requests. In this study, we introduce a novel approach for detecting DDoS attacks in an Edge-IIoT digital twin-based generated dataset. The proposed approach is designed to retain already learned knowledge and easily adapt to new models in a continuous manner without retraining the deep learning model. The target dataset is publicly available and contains 157,600 samples. The proposed models M1, M2, and M3 obtained precision scores of 0.94, 0.93, and 0.93; recall scores of 0.91, 0.97, and 0.99; F1-scores of 0.93, 0.95, and 0.96; and accuracy scores of 0.93, 0.95, and 0.96, respectively. The results demonstrated that transferring previous model knowledge to the next model consistently outperformed baseline approaches.},
  archive      = {J_PEERJCS},
  author       = {Feras Al-Obeidat and Adnan Amin and Ahmed Shuhaiber and Inam ul Haq},
  doi          = {10.7717/peerj-cs.3052},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3052},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DDoS attack detection in edge-IIoT digital twin environment using deep learning approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic generation of explanations in autonomous systems: Enhancing human interaction in smart home environments. <em>PEERJCS</em>, <em>11</em>, e3041. (<a href='https://doi.org/10.7717/peerj-cs.3041'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In smart environments, autonomous systems often adapt their behavior to the context, and although such adaptations are generally beneficial, they may cause users to struggle to understand or trust them. To address this, we propose an explanation generation system that produces natural language descriptions (explanations) to clarify the adaptive behavior of smart home systems in runtime. These explanations are customized based on user characteristics and the contextual information derived from the user interactions with the system. Our approach leverages a prompt-based strategy using a fine-tuned large language model, guided by a modular template that integrates key data such as the type of explanation to be generated, user profile, runtime system information, interaction history, and the specific nature of the system adaptation. As a preliminary step, we also present a conceptual model that characterize explanations in the domain of autonomous systems by defining their core concepts. Finally, we evaluate the user experience of the generated explanations through an experiment involving 118 participants. Results show that generated explanations are perceived positive and with high level of acceptance.},
  archive      = {J_PEERJCS},
  author       = {Oscar Peña-Cáceres and Antoni Mestre and Manoli Albert and Vicente Pelechano and Miriam Gil},
  doi          = {10.7717/peerj-cs.3041},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3041},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automatic generation of explanations in autonomous systems: Enhancing human interaction in smart home environments},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of methods and software for polygenic risk score analysis. <em>PEERJCS</em>, <em>11</em>, e3039. (<a href='https://doi.org/10.7717/peerj-cs.3039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Polygenic risk scores (PRSs) are emerging as powerful tools for predicting individual susceptibility to various diseases and traits based on genetic variants. These scores integrate information from multiple genetic markers associated with the trait or disease of interest, offering personalized risk assessment and enhancing disease management strategies. PRS is an active area of research and is being studied in various fields, such as disease prediction. This review explores the advancement of PRS research, focusing on methodological approaches, software tools, and applications across diverse disciplines. A systematic literature review identified 40 relevant articles classified based on PRS methods and software. Key methods for PRS computation, including penalized regression and threshold-based approaches, Bayesian approaches, and machine learning approaches, are discussed, along with notable software and their features. Applications of PRS in disease prevention are highlighted. Challenges and future directions, such as increasing diversity in genetic data, integrating environmental factors, and evaluating clinical implications, are also discussed to guide future research and implementation efforts.},
  archive      = {J_PEERJCS},
  author       = {Sara Benoumhani and Areej Al-Wabil and Niddal Imam and Bashayer Alfawaz and Amaan Zubairi and Dalal Aldossary and Mariam AlEissa},
  doi          = {10.7717/peerj-cs.3039},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3039},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A review of methods and software for polygenic risk score analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrated multi-task feature learning and interactive active optimization for scene retargeting in preschool educational applications. <em>PEERJCS</em>, <em>11</em>, e3035. (<a href='https://doi.org/10.7717/peerj-cs.3035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In artificial intelligence (AI), effective adaptation of educational imagery across diverse screen formats is essential, particularly in preschool education, where visual content must simultaneously engage and instruct young learners. This study introduces a novel scene retargeting model tailored to preserve pedagogically significant visual elements during image resizing. The proposed framework leverages the binarized normed gradients (BING) objectness metric to efficiently identify and prioritize key regions within educational images, such as objects and facial features. A core component of our approach is integrating a locality-preserved and interactive active optimization (LIAO) mechanism, which simulates human visual attention by generating gaze shift paths (GSPs) that guide feature prioritization. These GSPs are further transformed into hierarchical deep features using a multi-layer representation, followed by refinement through a Gaussian mixture model (GMM) to enhance scene understanding and retargeting fidelity. Experimental evaluations demonstrate that the proposed model not only surpasses five state-of-the-art methods in performance but also achieves a 3% improvement in accuracy compared to the next-best approach, all while reducing inference time by over 50%. The results confirm the model’s effectiveness and efficiency, offering a robust solution for educational content adaptation that aligns with cognitive and pedagogical requirements in early childhood learning environments.},
  archive      = {J_PEERJCS},
  author       = {Suhui Yao and Lan Lv},
  doi          = {10.7717/peerj-cs.3035},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3035},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Integrated multi-task feature learning and interactive active optimization for scene retargeting in preschool educational applications},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey on surface reconstruction based on 3D gaussian splatting. <em>PEERJCS</em>, <em>11</em>, e3034. (<a href='https://doi.org/10.7717/peerj-cs.3034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surface reconstruction is a foundational topic in computer graphics and has gained substantial research interest in recent years. With the emergence of advanced neural radiance fields (NeRFs) and 3D Gaussian splatting (3D GS), numerous innovative many novel algorithms for 3D model surface reconstruction have been developed. The rapid expansion of this field presents challenges in tracking ongoing advancements. This survey aims to present core methodologies for the surface reconstruction of 3D models and establish a structured roadmap that encompasses 3D representations, reconstruction methods, datasets, and related applications. Specifically, we introduce 3D representations using 3D Gaussians as the central framework. Additionally, we provide a comprehensive overview of the rapidly evolving surface reconstruction methods based on 3D Gaussian splatting. We categorize the primary phases of surface reconstruction algorithms for 3D models into scene representation, Gaussian optimization, and surface structure extraction. Finally, we review the available datasets, applications, and challenges and suggest potential future research directions in this domain. Through this survey, we aim to provide valuable resources that support and inspire researchers in the field, fostering advancements in 3D reconstruction technologies.},
  archive      = {J_PEERJCS},
  author       = {Zheng Xu and Gang Chen and Feng Li and Lingyu Chen and Yuanhang Cheng},
  doi          = {10.7717/peerj-cs.3034},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3034},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A survey on surface reconstruction based on 3D gaussian splatting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing reliable and energy-efficient UAV communications with RIS and deep reinforcement learning. <em>PEERJCS</em>, <em>11</em>, e3031. (<a href='https://doi.org/10.7717/peerj-cs.3031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth in wireless communication demands has led to a surge in research on technologies capable of enhancing communication reliability, coverage, and energy efficiency. Among these, uncrewed aerial vehicles (UAV) and reconfigurable intelligent surfaces (RIS) have emerged as promising solutions. Prior research on using deep reinforcement learning (DRL) to integrate RIS with UAV concentrated on enhancing signal quality and coverage, but it ignored the challenges caused by electromagnetic interference (EMI). This article introduces a novel framework addressing the challenges posed by EMI from Gallium nitride (GaN) power amplifiers in RIS-assisted UAV communication systems. By integrating DRL with quadrature phase shift keying (QPSK) modulation, the proposed system dynamically optimizes UAV deployment and RIS configurations in real-time, mitigating EMI effects, improving signal-to-interference-plus-noise ratio (SINR), and enhancing energy efficiency. The framework demonstrates superior performance, with an SINR improvement of up to 6.5 dB in interference-prone environments, while achieving a 38% increase in energy efficiency compared to baseline models. Additionally, the system significantly reduces EMI impact, with a mitigation rate of over 70%, and extends coverage area by 35%. The integration of QPSK and DRL allows for real-time decision-making that balances communication quality and energy consumption. These results show the system’s potential to outperform traditional methods, particularly in dynamic and challenging environments such as urban, disaster recovery, and remote settings.},
  archive      = {J_PEERJCS},
  author       = {Wasim Ahmad and Umar Islam and Abdulkadhem A. Abdulkadhem and Babar Shah and Fernando Moreira and Ali Abbas},
  doi          = {10.7717/peerj-cs.3031},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3031},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing reliable and energy-efficient UAV communications with RIS and deep reinforcement learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detection of offensive content in the kazakh language using machine learning and deep learning approaches. <em>PEERJCS</em>, <em>11</em>, e3027. (<a href='https://doi.org/10.7717/peerj-cs.3027'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the urgent need to detect destructive content, including religious extremism, racism, cyberbullying, and nation oriented extremism messages, on social media platforms in the Kazakh language. Given the agglutinative structure and rich morphology of Kazakh, standard natural language processing (NLP) models require significant adaptation. The study employs a range of machine learning and deep learning techniques, such as logistic regression, support vector machines (SVM), and long short-term memory (LSTM) networks, to classify destructive content. This article demonstrates the effectiveness of combining n-gram and stemming methods with machine learning algorithms, achieving high accuracy in content classification. The findings underscore the importance of developing language-specific NLP tools tailored to Kazakh’s linguistic complexities. This research not only contributes to ensuring online safety by detecting destructive content in Kazakh digital spaces, but also provides a framework for applying similar techniques to other lesser-resourced languages.},
  archive      = {J_PEERJCS},
  author       = {Milana Bolatbek and Moldir Sagynay and Shynar Mussiraliyeva and Zhastay Yeltay},
  doi          = {10.7717/peerj-cs.3027},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3027},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Detection of offensive content in the kazakh language using machine learning and deep learning approaches},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Innovative multi objective optimization based automatic fake news detection. <em>PEERJCS</em>, <em>11</em>, e3016. (<a href='https://doi.org/10.7717/peerj-cs.3016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the digital revolution, access to information is expanding day by day and individuals can access information quickly through the internet and social media platforms. However, in most cases, there is no mechanism in place to evaluate the accuracy of news that spreads rapidly on social media. This increases the potential for fake news to mislead both individuals and society. In order to minimize the negative effects of fake news, it has become a critical necessity to detect them quickly and effectively. Metaheuristic methods can provide more effective solutions in fake news detection compared to traditional methods. Especially in small datasets, metaheuristics are known to produce faster and more effective solutions than artificial intelligence and machine learning based methods. In the literature, the majority of fake news detection studies have focused on the optimization of a single criterion. In this study, unlike other studies, a method that enables simultaneous optimization of two criteria (precision and recall) in fake news detection is developed. In the proposed approach, an innovative solution is presented by using the Crowding Distance Level method instead of the Crowding Distance method used in the standard Non-dominated Sorting Genetic Algorithm 2 (NSGA-2) algorithm. The proposed method is tested on four different datasets such as Covid-19, Syrian war daily news and FakeNewsNet (Gossipcop). The results show that the proposed method achieves high success especially on small datasets.},
  archive      = {J_PEERJCS},
  author       = {Cebrail Barut and Suna Yildirim and Bilal Alatas and Gungor Yildirim},
  doi          = {10.7717/peerj-cs.3016},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3016},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Innovative multi objective optimization based automatic fake news detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forecasting temperature and rainfall using deep learning for the challenging climates of northern india. <em>PEERJCS</em>, <em>11</em>, e3012. (<a href='https://doi.org/10.7717/peerj-cs.3012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate temperature and rainfall (T&R) forecasting is vital for the climate-sensitive regions of Northern India, particularly Jammu, Kashmir, and Ladakh, where volatile weather patterns significantly affect livelihoods, socio-economic development, and disaster management efforts. Despite their importance, traditional forecasting methods often fall short due to their high computational demands and inability to provide localized, real-time predictions, leaving a critical research gap in addressing these challenges. This study addresses the need for precise and efficient T&R forecasting using deep learning-based framework tailored to the unique climatic conditions of these regions. The major research focus is to develop and evaluate a model capable of capturing complex temporal dependencies in localized time-series weather data. Utilizing data from the Indian Meteorological Department (IMD) for Jammu, Srinagar, and Ladakh stations covering the period from January 1, 2000, to December 31, 2023, the proposed framework employs recurrent neural networks (RNN) and long short-term memory (LSTM) architectures, both optimized for time-series forecasting. Key findings reveal that while both RNN and LSTM models exhibit robust performance in single input single output (SISO) setups, RNN model consistently outperforms the LSTM in capturing intricate temporal relationships. The RNN model in MIMO configuration achieved significantly lower mean absolute error (MAE), root mean squared error (RMSE), and mean squared error (MSE) for Jammu, Srinagar, and Ladakh, with respective values of [0.0636, 0.1011, 0.0401] for Jammu, [0.1048, 0.1555, 0.0455] for Srinagar, and [0.0854, 0.1344, 0.0411] for Ladakh. These results underscore the RNN model’s precision, making it a practical tool for real-time weather forecasting. By enhancing the accuracy of T&R predictions in regions with challenging meteorological conditions, this study contributes to improved climate adaptation strategies, disaster preparedness, and sustainable development. Its findings hold broader implications for advancing localized forecasting technologies in other regions with similar climatic complexities.},
  archive      = {J_PEERJCS},
  author       = {Syed Nisar Hussain Bukhari and Kingsley A. Ogudo},
  doi          = {10.7717/peerj-cs.3012},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3012},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Forecasting temperature and rainfall using deep learning for the challenging climates of northern india},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Next generation sequencing under attack: Investigating insider threats and organizational behaviour. <em>PEERJCS</em>, <em>11</em>, e3008. (<a href='https://doi.org/10.7717/peerj-cs.3008'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next generation sequencing (NGS) has become a cornerstone of modern genomics, enabling high-throughput analysis of DNA and RNA with wide applications across medicine, research, and biotechnology. However, the growing adoption of NGS technologies has introduced significant cyber-biosecurity risks, particularly those arising from insider threats and organizational shortcomings. While technical vulnerabilities have received attention, the human and behavioral dimensions of cybersecurity in NGS environments remain underexplored. This study investigates the role of human factors and organizational behavior in shaping cyber-biosecurity risks in NGS workflows. A mixed-method approach was employed, combining survey data from 120 participants across four countries with statistical analyses including chi-square tests, cross-tabulations, and cluster analysis. The study assessed cybersecurity training availability, employee engagement, training effectiveness, and awareness of insider threats. Findings reveal substantial gaps in training frequency and participation, with 36% of respondents reporting no access to NGS-specific cybersecurity training. Only a minority of participants felt confident in detecting cyber threats, and 32.5% had never applied cybersecurity knowledge in practice. Chi-square results indicate significant associations between training frequency and threat recognition, training relevance, and knowledge application. Cluster analysis further categorized organizations into “robust,” “moderate,” and “emergent” cybersecurity maturity profiles. The study offers an evidence-based framework to enhance cyber-biosecurity in NGS settings by addressing human-centric risks. It recommends role-specific training, frequent policy updates, and improved organizational communication to mitigate insider threats. These insights support the development of targeted interventions and policies to strengthen the cybersecurity culture in genomics organizations.},
  archive      = {J_PEERJCS},
  author       = {Nasreen Anjum and Hani Alshahrani and Darakhshan Syed and Asadullah Shaikh and Mahreen Ul Hassan},
  doi          = {10.7717/peerj-cs.3008},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e3008},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Next generation sequencing under attack: Investigating insider threats and organizational behaviour},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acute lymphoblastic leukemia cancer diagnosis in children and adults using transforming blood fluorescence microscopy imaging. <em>PEERJCS</em>, <em>11</em>, e2997. (<a href='https://doi.org/10.7717/peerj-cs.2997'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leukemia is a highly aggressive kind of cancer that may impact the bone marrow. The most fatal type acute lymphoblastic leukemia (ALL), is characterized by the excessive growth of immature white blood cells in the bone marrow. For diagnostic purposes, hematologists and experts use a state-of-the-art microscope fitted with a high-powered magnifying lens to analyze blood and bone marrow samples. Experts attribute the rapid progress to the presence of adolescent white blood cells, not fully developed ones. A good treatment for ALL, no matter where it comes from, includes chemotherapy, medication given through a transplant. Experts have difficulties in accurately evaluating explosive cell features due to the onerous and time-consuming nature of manual diagnosis for this disease. A total of 89 individuals suspected of having ALL underwent sample collection, resulting in the acquisition of 3,256 images. The dataset is classified into four different types of cancer: early-stage, benign cells, pre-cancerous cells, and pro-cancer cells. The proposed approach employs several preprocessing and augmentation techniques to improve the results. The studies demonstrate that the technique achieved a recall rate of 100% for the pro-cell cancer subtype, an overall accuracy of 98.67% using enhanced data, and an overall accuracy of 97.87% using the original data. The experiments have shown that the proposed equipment is superior in reliability and accuracy compared to existing approaches, and it facilitates early detection in medical imaging.},
  archive      = {J_PEERJCS},
  author       = {Amjad Rehman and Muhammad Mujahid and Tanzila Saba and Faten S. Alamri and Noor Ayesha},
  doi          = {10.7717/peerj-cs.2997},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2997},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Acute lymphoblastic leukemia cancer diagnosis in children and adults using transforming blood fluorescence microscopy imaging},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LPITutor: An LLM based personalized intelligent tutoring system using RAG and prompt engineering. <em>PEERJCS</em>, <em>11</em>, e2991. (<a href='https://doi.org/10.7717/peerj-cs.2991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Development of large language models (LLMs) has transformed the landscape of personalized education through intelligent tutoring systems (ITS) which responds to diverse learning requirements. This article proposed a model named LLM based Personalized Intelligent Tutoring System (LPITutor) that is based on LLM for personalized ITS that leverages retrieval-augmented generation (RAG) and advanced prompt engineering techniques to generate customized responses aligned with students’ requirements. The aim of LPITutor is to provide customized learning content that adapts to different levels of learners skills and question complexity. The performance of proposed model was evaluated on accuracy, completeness, clarity, difficulty alignment, coherence, and relevance. The finding of LPITutor indicates that it effectively balances the response accuracy and clarity with significant alignment to the difficulty level of student queries. The proposed work also emphasises the broader implications of artificial intelligence (AI)-driven ITS in education and presents future directions for improving the adaptation and optimization of LPITutor.},
  archive      = {J_PEERJCS},
  author       = {Zhensheng Liu and Prateek Agrawal and Saurabh Singhal and Vishu Madaan and Mohit Kumar and Pawan Kumar Verma},
  doi          = {10.7717/peerj-cs.2991},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2991},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {LPITutor: An LLM based personalized intelligent tutoring system using RAG and prompt engineering},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Risk assessment based on a new decision-making approach with fermatean fuzzy sets. <em>PEERJCS</em>, <em>11</em>, e2990. (<a href='https://doi.org/10.7717/peerj-cs.2990'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background This study presents a new approach to decision-making based on the selection of decision-makers according to evaluated criteria in multi-criteria decision-making (MCDM) methods. Therefore, sub-decision-maker groups (SDMGs) are created for each evaluated criterion. The SDMG approach, which is created according to the criteria, offers a more flexible and dynamic structure than the existing approaches. This approach aims to use the expertise and knowledge of decision-makers more effectively. The decision-making approach presented in this study offers an innovative model and adds a new dimension to decision-making processes. This decision-making approach is applied to the plastic injection moulding machine risk assessment, as it involves different criteria. In addition to classical risk parameters such as probability, severity, frequency, and detectability, new parameters such as human error, machine error, and existing safety measures are also used in the risk assessment. Methods The integration of the analytic hierarchy process (AHP) and the technique for order preference by similarity to ideal solution (TOPSIS) methods into the interval valued fermatean fuzzy set (IVFFS) environment makes an important contribution to a more comprehensive consideration of risks and uncertainties in the risk assessment process. The IVFF-AHP method is used to weight the risk parameters and determine the hazard scores, and the TOPSIS method is used to rank the hazards. A holistic and systematic approach to risk assessment has been achieved by integrating these two methods. Modelling of these methods is carried out using MATLAB_R2024a software. Results According to the evaluated criteria, it was concluded that the determination of the decision makers separately is applicable to the decision-making process. Identifying the existing safety measures parameter as the most important risk parameter emphasizes the central role of this factor in risk assessment. In addition, machine error and human error parameters are also found to be important in risk assessment. These parameters, which are used for the first time in the literature, offer a broader perspective than traditional methods and provide significant advantages in risk assessment. According to the evaluations, electricity, asphyxiating and toxic gases, and hot water use are determined as the most risky hazards. The sensitivity and comparative analysis performed in the study confirm that the proposed methodology produces consistent and reasonable results.},
  archive      = {J_PEERJCS},
  author       = {Hilal Biderci and Ali F. Guneri},
  doi          = {10.7717/peerj-cs.2990},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2990},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Risk assessment based on a new decision-making approach with fermatean fuzzy sets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRDAGE: A prescription recommendation framework for traditional chinese medicine based on data augmentation and multi-graph embedding. <em>PEERJCS</em>, <em>11</em>, e2974. (<a href='https://doi.org/10.7717/peerj-cs.2974'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The prescriptions of traditional chinese medicine (TCM) have made a great contribution to the treatment of disease and the maintenance of good health. Current research on prescription recommendations mainly focuses on the correlation between symptoms and herbs. However, the semantic information inherent in both symptoms and herbs has received limited attention. Furthermore, most datasets in the field of TCM suffer from limited data volumes, which can adversely impact model training. Methods To tackle these challenges, we present a prescription recommendation framework called PRDAGE, which is based on data augmentation and multi-graph embedding. We started by collecting medical records and creating a dataset of 3,052 classic medical cases, where we normalized the symptoms and herbs. Additionally, we developed a multi-layer embedding method for symptoms and herbs, using Sentence Bert (SBert) and graph convolutional networks. The aim of this multi-layer embedding method is to capture and represent the semantic information of symptoms and herbs, as well as the complex relationships between them. Additionally, a median-based random data augmentation method was introduced to enrich the medical case data, effectively enhancing the model’s accuracy. Results The model was evaluated against baseline models on an unenhanced dataset (Dataset-B), and the results showed that the proposed PRDAGE framework exhibited superior overall performance. Compared to the second-best model, PRDAGE achieved improvements in accuracy and recall rates of 1.69% and 3.80%, respectively, on the Top@10 metric. Ablation experiments further revealed that both the data augmentation and multi-layer embedding modules contributed to the improved model performance. Conclusion In conclusion, the experimental results suggest that PRDAGE is an effective prescription recommendation framework. The multi-layer embedding approach effectively represents the semantic information of symptoms and the complex relationships between symptoms and herbs. Additionally, the use of median-based data augmentation has a positive impact on the overall performance and generalization ability of the model.},
  archive      = {J_PEERJCS},
  author       = {Zhihua Wen and Yunchun Dong and Lihong Peng and Longxin Zhang and Junfeng Yan},
  doi          = {10.7717/peerj-cs.2974},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2974},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {PRDAGE: A prescription recommendation framework for traditional chinese medicine based on data augmentation and multi-graph embedding},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AMAY-net: Adaptive multi-scale attention YOLO network for liver and gallbladder segmentation in laparoscopic cholecystectomy. <em>PEERJCS</em>, <em>11</em>, e2961. (<a href='https://doi.org/10.7717/peerj-cs.2961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel liver and gallbladder segmentation framework, named Adaptive Multi-Scale Attention YOLO Network (AMAY-Net), designed for semantic segmentation of laparoscopic cholecystectomy images. Building upon the powerful feature extraction capabilities of You Only Look Once (YOLO), AMAY-Net incorporates several advanced modules to enhance performance in medical image segmentation tasks. First, a multi-scale feature extraction module is employed to capture anatomical structures of various sizes, ensuring effective detection of large organs like the liver and smaller structures such as the gallbladder and surgical instruments. Second, an adaptive class-balancing loss function is implemented to dynamically adjust the weights of underrepresented classes, improving the segmentation accuracy of small structures. Additionally, the network integrates a spatial and channel attention mechanism, enhancing the focus on critical regions in the image. Finally, residual connections are introduced in the YOLO backbone to improve feature propagation and gradient flow efficiency. Experimental results demonstrate that AMAY-Net achieves superior performance on the CholecSeg8k dataset, with significant improvements in the segmentation accuracy of key anatomical structures such as the liver and gallbladder.},
  archive      = {J_PEERJCS},
  author       = {Yuyang Zhou and Yulai You and Xiaokai Tan and Juncheng Tang},
  doi          = {10.7717/peerj-cs.2961},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2961},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {AMAY-net: Adaptive multi-scale attention YOLO network for liver and gallbladder segmentation in laparoscopic cholecystectomy},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning for digital twin applications: A privacy-preserving and low-latency approach. <em>PEERJCS</em>, <em>11</em>, e2877. (<a href='https://doi.org/10.7717/peerj-cs.2877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The digital twin (DT) concept has recently gained widespread application for mapping the state of physical entities, enabling real-time analysis, prediction, and optimization, thereby enhancing the management and control of physical systems. However, when sensitive information is extracted from physical entities, it faces potential leakage risks, as DT service providers are typically honest yet curious. Federated learning (FL) offers a new distributed learning paradigm that protects privacy by transmitting model updates from edge servers to local devices, allowing training on local datasets. Nevertheless, the training parameters communicated between local mobile devices and edge servers may contain raw data that malicious adversaries could exploit. Furthermore, variations in mapping bias across local devices and the presence of malicious clients can degrade FL training accuracy. To address these security and privacy threats, this paper proposes the FL-FedDT scheme—a privacy-preserving and low-latency FL method that employs an enhanced Paillier homomorphic encryption algorithm to safeguard the privacy of local device parameters without transmitting data to the server. Our approach introduces an improved Paillier encryption method with a new hyperparameter and pre-calculates multiple random intermediate values during the key generation stage, significantly reducing encryption time and thereby expediting model training. Additionally, we implement a trusted FL global aggregation method that incorporates learning quality and interaction records to identify and mitigate malicious updates, dynamically adjusting weights to counteract the threat of malicious clients. To evaluate the efficiency of our proposed scheme, we conducted extensive experiments, with results validating that our approach achieves training accuracy and security on par with baseline methods, while substantially reducing FL iteration time. This enhancement contributes to improved DT mapping and service quality for physical entities. (The code for this study is publicly available on GitHub at: https://github.com/fujianU/federated-learning. The URL address of the MNIST dataset is: https://gitcode.com/Resource-Bundle-Collection/d47b0/overview?utm_source=pan_gitcode&index=top&type=href&;.)},
  archive      = {J_PEERJCS},
  author       = {Jie Li and Dong Wang},
  doi          = {10.7717/peerj-cs.2877},
  journal      = {PeerJ Computer Science},
  month        = {8},
  pages        = {e2877},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Federated learning for digital twin applications: A privacy-preserving and low-latency approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SA3C-ID: A novel network intrusion detection model using feature selection and adversarial training. <em>PEERJCS</em>, <em>11</em>, e3089. (<a href='https://doi.org/10.7717/peerj-cs.3089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous proliferation of emerging technologies such as cloud computing, 5G networks, and the Internet of Things, the field of cybersecurity is facing an increasing number of complex challenges. Network intrusion detection systems, as a fundamental part of network security, have become increasingly significant. However, traditional intrusion detection methods exhibit several limitations, including insufficient feature extraction from network data, high model complexity, and data imbalance, which result in issues like low detection efficiency, as well as frequent false positives and missed alarms. To address the above issues, this article proposed an adversarial intrusion detection model (Soft Adversarial Asynchronous Actor-Critic Intrusion Detection, SA3C-ID) based on reinforcement learning. Firstly, the raw dataset is preprocessed via one-hot encoding and standardization. Subsequently, the refined data undergoes feature selection employing an improved pigeon-inspired optimizer (PIO) algorithm. This operation eliminates redundant and irrelevant features, consequently reducing data dimensionality while maintaining critical information. Next, the network intrusion detection process is modeled as a Markov decision process and integrated with the Soft Actor-Critic (SAC) reinforcement learning algorithm, with a view to constructing agents; In the context of adversarial training, two agents, designated as the attacker and the defender, are defined to perform asynchronous adversarial training. During this training process, both agents calculate the reward value, update their respective strategies, and transfer parameters based on the classification results. Finally, to verify the robustness and generalization ability of the SA3C-ID model, ablation experiments and comparative evaluations are conducted on two benchmark datasets, NSL-KDD and CSE-CIC-IDS2018. The experimental results demonstrate that SA3C-ID exhibits superior performance in comparison to other prevalent intrusion detection models. The F1-score attained by SA3C-ID was 92.58% and 98.76% on the NSL-KDD and CSE-CIC-IDS2018 datasets, respectively.},
  archive      = {J_PEERJCS},
  author       = {Wanwei Huang and Haobin Tian and Lei Wang and Sunan Wang and Kun Wang and Songze Li},
  doi          = {10.7717/peerj-cs.3089},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3089},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SA3C-ID: A novel network intrusion detection model using feature selection and adversarial training},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Music audio emotion regression using the fusion of convolutional neural networks and bidirectional long short-term memory models. <em>PEERJCS</em>, <em>11</em>, e3086. (<a href='https://doi.org/10.7717/peerj-cs.3086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music emotion regression (MER) is a vital field that bridges psychology and music information retrieval. Music has the powerful ability to evoke a wide range of human emotions, from joy and sadness to anger and calmness. Understanding how music influences emotional states is essential for grasping its psychological effects on individuals. This research presents an innovative model that combines convolutional neural networks (CNNs) with bidirectional long short-term memory (BiLSTM) networks to analyze and predict the emotional impact of musical audio. The model uses CNNs to detect temporal patterns and BiLSTMs to interpret sequences in both forward and backward directions, enhancing its ability to capture the complex structure of musical data. Additionally, a multi-head attention mechanism is incorporated to improve the model’s expressiveness and generalizability, making it especially effective for handling intricate sequential tasks and large datasets. The model’s performance was evaluated through sentiment prediction using extensive, publicly available datasets comprising over 9,000 musical excerpts. Results show that the proposed model significantly outperforms existing methods in MER, achieving an R-squared value of 0.845, indicating an excellent fit with the empirical data.},
  archive      = {J_PEERJCS},
  author       = {Yi Qiu and Yu Lin and Yun Lin},
  doi          = {10.7717/peerj-cs.3086},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3086},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Music audio emotion regression using the fusion of convolutional neural networks and bidirectional long short-term memory models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond buzzwords: NLP reveals common threads in sustainable and circular construction discourse. <em>PEERJCS</em>, <em>11</em>, e3085. (<a href='https://doi.org/10.7717/peerj-cs.3085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular economy and sustainability have both seen rapid growth in academic literature, often leading to ambiguity and the overuse of these terms. This obscures their true objectives and makes it challenging to discern their distinct intentions. Manually analyzing the vast body of recent publications to understand how these concepts connect to environmentally beneficial practices is laborious and time-consuming. This study aims to compare and analyze existing literature on sustainable and circular construction using natural language processing (NLP) techniques to elucidate the similarities and overlaps between these concepts within the construction industry. To achieve this, we employed three NLP methods: (1) TextRank, a graph-based ranking algorithm that extracts key structural relationships between terms in a document; (2) term frequency–inverse document frequency, a statistical measure that identifies the most significant terms based on their frequency and uniqueness within the corpus; and (3) semantic annotation (Wikifier), a method that links text tokens to structured knowledge bases such as Wikipedia for better contextual understanding. These methods are used to analyze a dataset of 480 academic articles focusing on sustainability and circular economy in the construction sector. Our analysis revealed that circular construction is more specific and practical, emphasizing resource efficiency, waste management, and industry-specific processes, targeting the operational aspects of recycling and resource recovery. In contrast, sustainable construction encompasses a broader and more holistic scope, including urban planning, community development, and long-term environmental impacts. This study demonstrates how NLP methods can systematically disentangle closely related frameworks in construction literature, providing a replicable methodological framework for future data-driven investigations. By clarifying the distinctions and overlaps between the terms “circular construction” and “sustainable construction”, our research offers enhanced understanding for policymakers, industry practitioners, and academics aiming to integrate sustainable and circular principles effectively within the construction sector.},
  archive      = {J_PEERJCS},
  author       = {Shakarim Aubakirov and Alexandr Pak and Iskander Akhmetov and Aidana Tleuken and Huseyin Atakan Varol and Assel Akzhalova and Ferhat Karaca},
  doi          = {10.7717/peerj-cs.3085},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3085},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Beyond buzzwords: NLP reveals common threads in sustainable and circular construction discourse},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing thyroid diagnosis: Integrating AI-driven CAD framework with numerical data and ultrasound images. <em>PEERJCS</em>, <em>11</em>, e3063. (<a href='https://doi.org/10.7717/peerj-cs.3063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an advanced computer-aided diagnosis (CAD) framework for thyroid disease diagnosis that integrates numerical patient data and ultrasound images. The framework uses cutting edge technologies, including Vision Transformers (ViTs) and SHapley Additive exPlanations (SHAPs), to increase diagnostic accuracy, interpretability, and clinical applicability. The proposed CAD framework employs the sparse search algorithm (SSA) for optimized feature selection from numerical data and the tree-structured Parzen estimator for tuning the hyperparameters. ViTs are utilized for analyzing thyroid ultrasound images, whereas SHAP provides explainable AI insights into model predictions. Extensive experiments were conducted on two datasets: the thyroid disease patient dataset and the DDTI: Thyroid Ultrasound Images dataset. Performance was evaluated via five-fold and ten-fold cross-validation utilizing metrics including accuracy, precision, and recall. The framework achieved promising performance, with models trained without data augmentation consistently outperforming their augmented counterparts. For the thyroid disease patient dataset, the best-performing model reported an accuracy of 99.71%, precision of 97.05%, recall of 99.29%, and F1-score of 98.16%. For the DDTI dataset, ViTs achieved an accuracy of 95.06% without augmentation, surpassing existing methodologies. Key features such as thyroxine, thyroid surgery, and thyroid-stimulating hormone (TSH) were identified as critical predictors of thyroid conditions. This study underscores the potentiality of AI-driven approaches in healthcare, paving the way for improved diagnostic outcomes and personalized treatment strategies.},
  archive      = {J_PEERJCS},
  author       = {Saleh Ateeq Almutairi},
  doi          = {10.7717/peerj-cs.3063},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3063},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Advancing thyroid diagnosis: Integrating AI-driven CAD framework with numerical data and ultrasound images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precise distance measurement with stereo camera: Experimental results. <em>PEERJCS</em>, <em>11</em>, e3057. (<a href='https://doi.org/10.7717/peerj-cs.3057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, image processing is used in many areas, especially artificial intelligence. This is because images are thought to contain a lot of information. In addition, many distance measurement studies have used image processing techniques. However, no studies have reached these high sensitivity and accuracy rates that can be used in engineering. The motivation of the study is to obtain the results of the experimental application of the image processing method, which can measure distances with high sensitivity and can also be used in engineering fields. In the study, the distances of 19 different target objects were measured using Total Station, Laser Meter, and Developed Prototype (Image Meter). Total Station measurement results were used as a reference and the Laser Meter and Image Meter results were compared. As a result of the comparison, it was determined that the developed Image Meter had a smaller error rate in 11 of the 19 comparisons. Results were obtained with an average error of 1.24% as a result of 19 measurements made with the developed Image Meter. The experimental results were also compared with theoretical calculation. As a result of the comparisons, it was determined that the results with the developed Image Meter were acceptable and could be improved with mechanical arrangements.},
  archive      = {J_PEERJCS},
  author       = {Haydar Yanık and Bülent Turan},
  doi          = {10.7717/peerj-cs.3057},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3057},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Precise distance measurement with stereo camera: Experimental results},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EquiRate: Balanced rating injection approach for popularity bias mitigation in recommender systems. <em>PEERJCS</em>, <em>11</em>, e3055. (<a href='https://doi.org/10.7717/peerj-cs.3055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems often suffer from popularity bias problem, favoring popular items and overshadowing less known or niche content, which limits recommendation diversity and content exposure. The root reason for this issue is the imbalances in the rating distribution; a few popular items receive a disproportionately large share of interactions, while the vast majority garner relatively few. In this study, we propose the EquiRate method as a pre-processing approach, addressing this problem by injecting synthetic ratings into less popular items to make the dataset regarding rating distribution more balanced. More specifically, this method utilizes several synthetic rating injection and synthetic rating generation strategies: (i) the first ones focus on determining which items to inject synthetic ratings into and calculating the total number of these ratings, while (ii) the second ones concentrate on computing the concrete values of the ratings to be included. We also introduce a holistic and highly efficient evaluation metric, i.e., the FusionIndex, concurrently measuring accuracy and several beyond-accuracy aspects of recommendations. The experiments realized on three benchmark datasets conclude that several EquiRate’s variants, with proper parameter-tuning, effectively reduce popularity bias and enhance recommendation diversity. We also observe that some prominent popularity-debiasing methods, when assessed using the FusionIndex, often fail to balance the referrals’ accuracy and beyond-accuracy factors. On the other hand, our best-performing EquiRate variants significantly outperform the existing methods regarding the FusionIndex, and their superiority is more apparent for the high-dimension data collections, which are more realistic for real-world scenarios.},
  archive      = {J_PEERJCS},
  author       = {Mert Gulsoy and Emre Yalcin and Alper Bilge},
  doi          = {10.7717/peerj-cs.3055},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3055},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {EquiRate: Balanced rating injection approach for popularity bias mitigation in recommender systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D reconstruction of toys based on adaptive scaled neural radiation field. <em>PEERJCS</em>, <em>11</em>, e3053. (<a href='https://doi.org/10.7717/peerj-cs.3053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of computer vision technology, 3D reconstruction of toys under single-view conditions still faces significant challenges in terms of detail loss and color distortion. For this reason, this article proposes an adaptive scale neural radiance fields (AS-NeRF) model to enhance the accuracy and realism of 3D toy reconstruction. The method constructs a multi-task feature extraction network based on the Vision Transformer, which simultaneously extracts and fuses multidimensional features such as texture, shape, color, and depth through a task dynamic modulation mechanism and a dynamic adapter layer, providing a rich and accurate contextual feature representation. The NeRF model is enhanced to incorporate an adaptive scaling mechanism that dynamically optimizes rendering sampling accuracy according to the local complexity of the scene. Spectral sensing techniques are integrated to reproduce the true colors of materials accurately. Finally, the conditional diffusion model is deeply integrated with NeRF, and high-dimensional conditional vectors are used to guide the inverse diffusion process in generating unobserved images with consistent geometric structure and physical properties. Experiments on the Co3D toy dataset demonstrate that AS-NeRF significantly outperforms existing mainstream methods in terms of peak signal-to-noise ratio (PSNR), structural similarity (SSIM), loss of perceptions (LPIPS), and Chamfer distance, thereby verifying the validity and advantages of the proposed method for high-quality toy 3D reconstruction tasks.},
  archive      = {J_PEERJCS},
  author       = {Jiajun Zou and Shaojiang Liu and Feng Wang and Weichuan Ni and Shitong Ye},
  doi          = {10.7717/peerj-cs.3053},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3053},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {3D reconstruction of toys based on adaptive scaled neural radiation field},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JDroid: Android malware detection using hybrid opcode feature vector. <em>PEERJCS</em>, <em>11</em>, e3051. (<a href='https://doi.org/10.7717/peerj-cs.3051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of devices using the Android operating system makes these devices the primary target for malware developers. Researchers are investigating different techniques to protect end users from these attackers. While many of these techniques are successful in detecting malware, they also have some limitations. Because many applications today use advanced obfuscation techniques, advanced disguise, and variant generation techniques to bypass detection tools, this creates difficulties for security experts. However, the rich semantic information hidden in opcodes offers a promising way to distinguish benign applications from malicious ones. In this study, we propose a tool called JDroid that treats opcodes (Dalvik Opcode and Java ByteCode) as features based on static analysis. The proposed tool aims to detect malicious applications with a unique ensemble model in a stacked generalised structure that uses different opcode sequences as a hybrid, and where each feature is first trained separately and then used by an ensemble decision. For this purpose, opcodes are extracted from APK files by code analysis and directly converted into vectors as 0 and 1 according to their usage cases. A subset of 461 features, obtained through filtering and feature selection processes, is then created using fewer features. This increases efficiency and performance, avoids overfitting, and reduces computational cost. The datasets Drebin, Genome, MalDroid2020, CICInvesAndMal2019, and Omer are tested with an application pool consisting of 14 thousand applications, and the classification performance is compared with different machine learning methods. Experimental results show that the proposed approach has an accuracy value of 98.6% and an area under the curve (AUC) value of 99.6% in malware detection without being affected by the obfuscation process.},
  archive      = {J_PEERJCS},
  author       = {Recep Sinan Arslan},
  doi          = {10.7717/peerj-cs.3051},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3051},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {JDroid: Android malware detection using hybrid opcode feature vector},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight graph convolutional network with multi-attention mechanisms for intelligent action recognition in online physical education. <em>PEERJCS</em>, <em>11</em>, e3050. (<a href='https://doi.org/10.7717/peerj-cs.3050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of online physical education in higher education has improved accessibility but presents challenges in recognizing complex movements and delivering individualized feedback. Existing action recognition models are often computationally intensive and struggle to generalize across diverse skeletal patterns. To address this, we propose a lightweight graph convolutional network (GCN) that integrates an improved Ghost module with multi-attention mechanisms, including a global attention mechanism (GAM) and a channel attention mechanism (CAM), to enhance spatial and temporal feature extraction. The model is trained end-to-end on 3D skeleton sequences and optimized for real-time efficiency. The computational cost is evaluated in terms of giga floating-point operations (GFLOPs), with the proposed model requiring only 6.2 GFLOPs per inference, over 60% less than the baseline ST-GCN. Experimental results on the NTU60RGB+D dataset demonstrate that the model achieves 90.8% accuracy in cross-subject and 96.8% in cross-view settings. These findings highlight the model’s effectiveness in balancing accuracy and efficiency, with promising applications in online physical education, rehabilitation monitoring, elderly movement analysis, and VR-based interfaces.},
  archive      = {J_PEERJCS},
  author       = {Yuhao You},
  doi          = {10.7717/peerj-cs.3050},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3050},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Lightweight graph convolutional network with multi-attention mechanisms for intelligent action recognition in online physical education},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification of core sub-team on scientific collaboration networks with shapley method. <em>PEERJCS</em>, <em>11</em>, e3048. (<a href='https://doi.org/10.7717/peerj-cs.3048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the core sub-teams that drive productivity in scientific collaboration networks is essential for research evaluation and team management. However, existing methods typically rank individual researchers by bibliometric impact or select structurally cohesive clusters, but rarely account for both collaboration patterns and joint scientific output. To address this limitation, we propose a novel two-dimensional framework that integrates network topology with research performance to identify core sub-teams. Specifically, we measure each sub-team’s marginal structural contribution using the Shapley value and quantify its collective impact using a sub-team H-index. To efficiently identify high-contributing sub-teams, we employ the Monte Carlo Tree Search algorithm, along with an approximation strategy to estimate Shapley values under computational constraints. We evaluate our method on 61 real-world scientific collaboration teams from Web of Science and Baidu Scholar data. Experimental results validate the effectiveness of our method in identifying core sub-teams, with the highest collaborative and citation impact. The proposed method offers a valuable analytical tool for research managers and funding agencies seeking to locate high-impact collaborative clusters, and it provides a generalizable framework for studies requiring the integration of structural and performance-based indicators in network analysis.},
  archive      = {J_PEERJCS},
  author       = {Lixin Zhou and Chen Liu and Xue Song},
  doi          = {10.7717/peerj-cs.3048},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3048},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Identification of core sub-team on scientific collaboration networks with shapley method},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of psychiatry clinical notes by diagnosis: A deep learning and machine learning approach. <em>PEERJCS</em>, <em>11</em>, e3045. (<a href='https://doi.org/10.7717/peerj-cs.3045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of clinical notes into specific diagnostic categories is critical in healthcare, especially for mental health conditions like anxiety and adjustment disorder. In this study, we compare the performance of various artificial intelligence models, including both traditional machine learning approaches (random forest, support vector machine, K-nearest neighbors, decision tree, and eXtreme Gradient Boost) and deep learning models (DistilBERT and SciBERT), to classify clinical notes into these two diagnoses. Additionally, we implemented three oversampling strategies: No Oversampling, Random Oversampling, and Synthetic Minority Over-sampling Technique (SMOTE), to assess their impact on model performance. Hyperparameter tuning was also applied to optimize model accuracy. Our results indicate that oversampling techniques had minimal impact on model performance overall. The only exception was SMOTE, which showed a positive effect specifically with Bidirectional Encoder Representations from Transformers (BERT)-based models. However, hyperparameter optimization significantly improved accuracy across the models, enhancing their ability to generalize and perform on the dataset. The decision tree and eXtreme Gradient Boost models achieved the highest accuracy among machine learning approaches, both reaching 96%, while the DistilBERT and SciBERT models also attained 96% accuracy in the deep learning category. These findings underscore the importance of hyperparameter tuning in maximizing model performance. This study contributes to the ongoing research on AI-assisted diagnostic tools in mental health by providing insights into the efficacy of different model architectures and data balancing methods.},
  archive      = {J_PEERJCS},
  author       = {Sergio Rubio-Martín and María Teresa García-Ordás and Antonio Serrano-García and Clara Margarita Franch-Pato and Arturo Crespo-Álvaro and José Alberto Benítez-Andrades},
  doi          = {10.7717/peerj-cs.3045},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3045},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Classification of psychiatry clinical notes by diagnosis: A deep learning and machine learning approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grammatical error correction for low-resource languages: A review of challenges, strategies, computational and future directions. <em>PEERJCS</em>, <em>11</em>, e3044. (<a href='https://doi.org/10.7717/peerj-cs.3044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grammatical error correction (GEC) is crucial for enhancing the readability and comprehension of texts, particularly in improving text quality in low-resource languages. However, challenges such as data scarcity, linguistic diversity, and limited computational resources hinder advancements in this domain. To address these challenges, researchers have developed strategies such as synthetic data generation, multilingual pre-trained models, and cross-lingual transfer learning. This review synthesizes findings from key studies to explore effective GEC methods for low-resource languages, emphasizing approaches for handling limited annotated corpora, typological complexities, and evaluation challenges. Synthetic data generation techniques, including noise injection, adversarial error generation, and translationese-based augmentation, have proven vital for overcoming data scarcity. Multilingual and transfer learning approaches demonstrate effectiveness in adapting knowledge from high-resource languages to low-resource settings, especially when combined with fine-tuning on curated datasets. Additionally, linguistic diversity has been partially addressed through methods like morphology-aware embeddings, byte-level tokenization, and contextual data preprocessing. However, limited research exists on robust evaluation metrics tailored to diverse typologies, such as agglutinative and morphologically rich languages, and the creation of gold-standard datasets remains an ongoing challenge. Recent advancements in dataset construction and the use of large language models further enrich this field, offering scalable solutions for low-resource contexts. Despite notable progress, this review identifies gaps in evaluation methodologies and typology-specific solutions, calling for future innovations in multilingual modeling, dataset creation, and computationally efficient GEC systems tailored to the unique needs of low-resource languages.},
  archive      = {J_PEERJCS},
  author       = {Syauqie Muhammad Marier and Xiangfan Chen and Linan Zhu and Xiangjie Kong},
  doi          = {10.7717/peerj-cs.3044},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3044},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Grammatical error correction for low-resource languages: A review of challenges, strategies, computational and future directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task advanced convolutional neural network for robust lymphoblastic leukemia diagnosis, classification, and segmentation. <em>PEERJCS</em>, <em>11</em>, e3043. (<a href='https://doi.org/10.7717/peerj-cs.3043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acute lymphoblastic leukemia (ALL), a hematologic malignancy characterized by the overproduction of immature lymphocytes, a type of white blood cell. Accurate and timely diagnosis of ALL is crucial for effective management. This article introduces a novel multi-task advanced convolutional neural network (MTA-CNN) framework for ALL detection in medical imaging data by simultaneously performing, expression classification, and disease detection. The MTA-CNN is based on a deep learning architecture that can handle multiple tasks simultaneously, allowing it to learn more comprehensive and generalizable features. With, expression classification, and disease detection tasks, the MTA-CNN effectively leverages the complementary information from each task to improve overall performance. The proposed framework employs CNNs to extract informative features from medical images. These features capture the spatial and temporal characteristics of the data, which are essential for accurate ALL diagnosis. The cascaded structure of the MTA-CNN allows the model to learn features at different levels of abstraction, from low-level to high-level, enabling it to capture both fine-grained and coarse-grained information. To ensure the reliability of the detection results, non-maximum suppression is employed to eliminate redundant detections, focusing only on the most likely candidates. Additionally, the MTA-CNN’s ability to accurately localize key facial landmarks provides valuable information for further analysis, including identifying abnormal structures or changes in anatomical features associated with ALL. Experimental results on a comprehensive dataset of medical images demonstrate the superiority of the MTA-CNN over other learning methods. The proposed framework achieved an accuracy of 0.978, precision of 0.979, recall of 0.967, F1-score of 0.973, specificity of 0.991, Cohen’s kappa of 0.979, and negative predictive value (NPV) of 0.990. These metrics significantly outperform baseline models, highlighting the MTA-CNN’s ability to accurately identify and classify ALL cases. The MTA-CNN offers a promising approach for improving the efficiency and accuracy of ALL diagnosis.},
  archive      = {J_PEERJCS},
  author       = {Sercan Yalcin and Zuhal Cetin Yalcin and Muhammed Yildirim and Bilal Alatas},
  doi          = {10.7717/peerj-cs.3043},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3043},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-task advanced convolutional neural network for robust lymphoblastic leukemia diagnosis, classification, and segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective optimization for smart cities: A systematic review of algorithms, challenges, and future directions. <em>PEERJCS</em>, <em>11</em>, e3042. (<a href='https://doi.org/10.7717/peerj-cs.3042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing complexity and interdependence of urban systems, multi-objective optimization (MOO) has become a critical tool for smart-city planning, sustainability, and real-time decision-making. This article presents a systematic literature review (SLR) of 117 peer-reviewed studies published between 2015 and 2025, assessing the evolution, classification, and performance of MOO techniques in smart-city contexts. Existing algorithms are organised into four families—bio-inspired, mathematical theory-driven, physics-inspired, and machine-learning-enhanced—and benchmarked for computational efficiency, scalability, and scenario suitability across six urban domains: infrastructure, energy, transportation, Internet of Things (IoT)/cloud systems, agriculture, and water management. While established methods such as Non-dominated Sorting Genetic Algorithm II (NSGA-II) and Multiobjective Evolutionary Algorithm based on Decomposition (MOED/D) remain prevalent, hybrid frameworks that couple deep learning with evolutionary search display superior adaptability in high-dimensional, dynamic environments. Persistent challenges include limited cross-domain generalisability, inadequate uncertainty handling, and low interpretability of artificial intelligence (AI)-assisted models. Twelve research gaps are synthesised—from privacy-preserving optimisation and sustainable trade-off resolution to integration with digital twins, large language models, and neuromorphic computing—and a roadmap towards scalable, interpretable, and resilient optimisation frameworks is outlined. Finally, a ready-to-use benchmarking toolkit and a deployment-oriented algorithm-selection matrix are provided to guide researchers, engineers, and policy-makers in real-world smart-city applications. This review targets interdisciplinary researchers, optimisation developers, and smart-city practitioners seeking to apply or advance MOO techniques in complex urban systems.},
  archive      = {J_PEERJCS},
  author       = {YiFan Chen and Weng Howe Chan and Eileen Lee Ming Su and Qi Diao},
  doi          = {10.7717/peerj-cs.3042},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3042},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-objective optimization for smart cities: A systematic review of algorithms, challenges, and future directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive filter parameter reconstruction technology for rocket inertial navigation/satellite integrated navigation system. <em>PEERJCS</em>, <em>11</em>, e3040. (<a href='https://doi.org/10.7717/peerj-cs.3040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of micro-electro-mechanical systems (MEMS) strapdown inertial navigation systems (SINS) with global navigation satellite systems (GNSS) has emerged as a significant area of research due to its compact size, affordability, and high precision. In the context of guided rocket-borne MEMS-SINS/GNSS integrated navigation systems, the performance of navigation is characterized by the need for high overload, accuracy, and real-time capability. A variety of enhanced algorithms based on Kalman filtering are currently employed as integrated filtering methods, which comprehensively address deviations in the system model to improve navigation performance. The noise characteristics of MEMS inertial guidance devices change dramatically under long-term storage conditions, while the dynamic flight environment of rockets and the high real-time requirements of navigation solving make the design of on-board combined navigation filters challenging. To address this issue, this article introduces the Adaptive Reconfigurable Extended Kalman Filter (AREKF) method. Initially, a precise system state model is developed to reflect the unique characteristics of the rocket flight environment, facilitating rapid convergence of the filtering process. Subsequently, during the rocket alignment process, a real-time reconstruction of filter parameters is implemented to enable adaptive and precise modeling of navigation parameters. This strategy ensures lower computational costs during rocket flight, enhances the accuracy of the navigation system, and produces real-time navigation outputs that exhibit high overload and precision. The results from the Six-Degree (6D) Model simulation and car-mounted experiments demonstrate that, compared to the traditional Extended Kalman Filter (EKF) algorithm and existing improved algorithms, the AREKF method significantly enhances the real-time navigation accuracy of rockets under high overload conditions.},
  archive      = {J_PEERJCS},
  author       = {Zhijie Yang and Guoguang Chen and Mingli Niu and Xiaolong Yan and Xiaoli Tian and Guocui Zhang},
  doi          = {10.7717/peerj-cs.3040},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3040},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Adaptive filter parameter reconstruction technology for rocket inertial navigation/satellite integrated navigation system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A progressive attention-based cross-modal fusion network for cardiovascular disease detection using synchronized electrocardiogram and phonocardiogram signals. <em>PEERJCS</em>, <em>11</em>, e3038. (<a href='https://doi.org/10.7717/peerj-cs.3038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synchronized electrocardiogram (ECG) and phonocardiogram (PCG) signals provide complementary diagnostic insights crucial for improving the accuracy of cardiovascular disease (CVD) detection. However, existing deep learning methods often utilize single-modal data or employ simplistic early or late fusion strategies, which inadequately capture the complex, hierarchical interdependencies between these modalities, thereby limiting detection performance. This study introduces PACFNet, a novel progressive attention-based cross-modal feature fusion network, for end-to-end CVD detection. PACFNet features a three-branch architecture: two modality-specific encoders for ECG and PCG, and a progressive selective attention-based cross-modal fusion encoder. A key innovation is its four-layer progressive fusion mechanism, which integrates multi-modal information from low-level morphological details to high-level semantic representations. This is achieved by selective attention-based cross-modal fusion (SACMF) modules at each progressive level, employing cascaded spatial and channel attention to dynamically emphasize salient feature contributions across modalities, thus significantly enhancing feature learning. Signals are pre-processed using a beat-to-beat segmentation approach to analyze individual cardiac cycles. Experimental validation on the public PhysioNet 2016 dataset demonstrates PACFNet’s state-of-the-art performance, with an accuracy of 97.7%, sensitivity of 98%, specificity of 97.3%, and an F1-score of 99.7%. Notably, PACFNet not only excels in multi-modal settings but also maintains robust diagnostic capabilities even with missing modalities, underscoring its practical effectiveness and reliability. The source code is publicly available on Zenodo (https://zenodo.org/records/15450169).},
  archive      = {J_PEERJCS},
  author       = {Wei Peng Li and Joon Huang Chuah and Guo Jeng Tan and Chengyu Liu and Hua-Nong Ting},
  doi          = {10.7717/peerj-cs.3038},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3038},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A progressive attention-based cross-modal fusion network for cardiovascular disease detection using synchronized electrocardiogram and phonocardiogram signals},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based BiLSTM-XGBoost model for reliability assessment and lifetime prediction of digital microfluidic systems. <em>PEERJCS</em>, <em>11</em>, e3037. (<a href='https://doi.org/10.7717/peerj-cs.3037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional methods for reliability and lifetime testing of digital microfluidic systems heavily rely on real-time monitoring data. This often leads to evaluation lag and limits their application, especially for complex droplets. To address these issues, this study proposes a novel prediction model for digital microfluidic (DMF) devices. The model combines an attention-based bidirectional long short-term memory (BiLSTM) with eXtreme Gradient Boosting (XGBoost) using a Stacking approach. This integrated model efficiently identifies the health state and predicts the failure time of digital microfluidic devices. This approach overcomes the limitations of traditional methods, such as over-reliance on sensor feedback and detection hysteresis. Experimental results demonstrate high prediction accuracy. The model achieved a mean absolute percentage error (MAPE) of 1.6464, Root mean squared error (RMSE) of 0.3667, mean absolute error (MAE) of 0.2557, and a coefficient of determination (R-squared) of 0.9949. Compared to baseline methods, the proposed BiLSTM-XGBoost model achieves the highest prediction accuracy, enabling effective health monitoring, problem identification, and failure prediction. Ultimately, this improves system reliability and lifetime with greater timeliness and accuracy.},
  archive      = {J_PEERJCS},
  author       = {Lifeng He and Qili Yang and Junxi Chen and Wenjing Liu and Zhijie Luo},
  doi          = {10.7717/peerj-cs.3037},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3037},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Attention-based BiLSTM-XGBoost model for reliability assessment and lifetime prediction of digital microfluidic systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alpha-DehazeNet: Single image dehazing via RGBA haze modeling and adaptive learning. <em>PEERJCS</em>, <em>11</em>, e3036. (<a href='https://doi.org/10.7717/peerj-cs.3036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is a vital research area in computer vision. Many existing deep learning-based dehazing methods rely on atmospheric scattering models with manually predefined, non-trainable parameters, which limits their adaptability and transferability. We propose Alpha-DehazeNet, a novel model that leverages red green blue alpha (RGBA) haze layer effect maps by defining a grayscale transparency map in the RGBA color space as the initial haze layer. Alpha-DehazeNet employs a U-Net generator enhanced with a spatial attention mechanism to encode haze-related features. This generator is integrated into an adversarial architecture with residual connections, enabling end-to-end training. Additionally, a depth consistency loss is introduced to improve dehazing accuracy. Alpha-DehazeNet outperforms several state-of-the-art models on synthetic datasets (ITS and OTS from RESIDE), achieving 37.35 dB peak signal-to-noise ratio (PSNR) on SOTS-indoor and 37.39 dB PSNR on SOTS-outdoor, while using only 8.86 million parameters. On real-world datasets, Alpha-DehazeNet delivers competitive results, although it shows limitations in handling non-white fog and cloud conditions. The code is publicly available at: https://doi.org/10.5281/zenodo.15361810.},
  archive      = {J_PEERJCS},
  author       = {Jin He and Ruibin Li},
  doi          = {10.7717/peerj-cs.3036},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3036},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Alpha-DehazeNet: Single image dehazing via RGBA haze modeling and adaptive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local-global multi-scale attention network for medical image segmentation. <em>PEERJCS</em>, <em>11</em>, e3033. (<a href='https://doi.org/10.7717/peerj-cs.3033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous advancement of deep learning technologies, deep learning-based medical image segmentation methods have achieved remarkable results. However, existing segmentation approaches still face several key challenges, including the insufficient extraction of local and global information from images and the inaccurate selection of core features. To address these challenges, this article proposes a novel medical image segmentation architecture—local-global multi-scale attention network (LGMANet). LGMANet introduces an innovative local-global information processing block (LGIPB) to effectively facilitate the deep mining of both local and global information during the downsampling process. In addition, an efficient multi-scale reconstruction attention (EMRA) module is designed to help the model accurately extract core features and multi-scale information while effectively suppressing irrelevant content. Experiments on the ISIC2018, CVC-ClinicDB, BUSI, and GLaS datasets demonstrate that LGMANet achieves IoU scores of 85.28%, 82.67%, 70.07%, and 88.90%, respectively, showcasing its superior segmentation performance.},
  archive      = {J_PEERJCS},
  author       = {Minghui Zhu and Dapeng Cheng and Yanyan Mao and Lu Sun and Wanting Jing},
  doi          = {10.7717/peerj-cs.3033},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3033},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Local-global multi-scale attention network for medical image segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic token encryption for preventing permission leakage in serverless architectures. <em>PEERJCS</em>, <em>11</em>, e3029. (<a href='https://doi.org/10.7717/peerj-cs.3029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless architecture simplifies application development and operation, but its permission control model based on static execution roles struggles to adapt to highly dynamic runtime environments, which can easily lead to the risk of permission and key leakage. To address this challenge, this article proposes a runtime dynamic token-based access control scheme. The scheme combines function context and user-defined security rules to achieve function-level dynamic authorization and request-level identity authentication. The generated dynamic tokens possess strong randomness, unpredictability, and one-time use characteristics, effectively reducing the harm caused by token leakage. Moreover, the designed multi-factor token verification model integrates dynamic factors such as call chain features and behavior patterns, which can defend against various security threats. Through social surveys, qualitative analysis, and extensive experiments, this article confirms that the proposed scheme significantly enhances the security of serverless applications while maintaining a controllable impact on platform performance. This research enriches the theoretical knowledge in the field of serverless security and provides new ideas for development practices, which is expected to promote the expansion of serverless architecture to enterprise-level scenarios and contribute to the healthy development of its ecosystem.},
  archive      = {J_PEERJCS},
  author       = {Yu Liu and Fu Li and Chenhao Sun},
  doi          = {10.7717/peerj-cs.3029},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3029},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic token encryption for preventing permission leakage in serverless architectures},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid TCN-TE-ANN model for high-precision solar irradiance prediction. <em>PEERJCS</em>, <em>11</em>, e3026. (<a href='https://doi.org/10.7717/peerj-cs.3026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of solar irradiance is critical for optimizing solar energy systems, enhancing grid stability, and supporting sustainable energy transitions. While numerous studies have explored various methodologies for solar radiation prediction, challenges remain in achieving high accuracy across diverse geographic locations and temporal resolutions. This study presents a novel hybrid model combining temporal convolutional networks (TCN), Transformer encoders (TE), and artificial neural networks (ANN) to predict global horizontal irradiance (GHI) with high precision. Utilizing a comprehensive dataset from three significant U.S. solar energy sites—Desert Sunlight, Copper Mountain, and Solar Star—spanning 22 years at a 30-min temporal resolution, the proposed model demonstrated superior performance metrics, with R2 ranging from 0.94768 to 0.97417, root mean square error (RMSE) between 0.04776 and 0.06543 W/m2, and mean absolute error (MAE) between 0.02510 and 0.03526 W/m2. By leveraging TCN’s temporal feature extraction, TE’s attention mechanisms, and ANN’s dense layer refinements, the model demonstrates significant advancements over existing methods.},
  archive      = {J_PEERJCS},
  author       = {Murat Isik},
  doi          = {10.7717/peerj-cs.3026},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3026},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel hybrid TCN-TE-ANN model for high-precision solar irradiance prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive review of dimensionality reduction algorithms: Challenges, limitations, and innovative solutions. <em>PEERJCS</em>, <em>11</em>, e3025. (<a href='https://doi.org/10.7717/peerj-cs.3025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction (DR) simplifies complex data from genomics, imaging, sensors, and language into interpretable forms that support visualization, clustering, and modeling. Yet widely used methods like principal component analysis, t-distributed stochastic neighbor embedding, uniform manifold approximation and projection, and autoencoders are often applied as “black boxes,” neglecting interpretability, fairness, stability, and privacy. This review introduces a unified classification—linear, nonlinear, hybrid, and ensemble approaches—and assesses them against eight core challenges: dimensionality selection, overfitting, instability, noise sensitivity, bias, scalability, privacy risks, and ethical compliance. We outline solutions such as intrinsic dimensionality estimation, robust neighborhood graphs, fairness-aware embeddings, scalable algorithms, and automated tuning. Drawing on case studies from bioinformatics, vision, language, and Internet of Things analytics, we offer a practical roadmap for deploying dimensionality reduction methods that are scalable, interpretable, and ethically sound—advancing responsible artificial intelligence in high-stakes applications.},
  archive      = {J_PEERJCS},
  author       = {Aasim Ayaz Wani},
  doi          = {10.7717/peerj-cs.3025},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3025},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comprehensive review of dimensionality reduction algorithms: Challenges, limitations, and innovative solutions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced approach for automatic annotation of error codes based on seq2edit. <em>PEERJCS</em>, <em>11</em>, e3024. (<a href='https://doi.org/10.7717/peerj-cs.3024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep natural language translation models have been used for automatic code error correction and have demonstrated outstanding potential. However, a large and accurately annotated training dataset is essential for these models to perform well. The key to improving the performance of these models lies in automatically and accurately annotating code errors and establishing a larger training dataset. Recently, a code error automatic annotation method based on Seq2edit has been proposed to optimize the dataset. However, the accuracy of the annotation is affected because tokens in the input code from the same statement may be aligned to different statements. This article proposes a Seq2edit annotation method based on the source code’s sentence structure. By dividing the code into statements with independent meanings and introducing a cost coefficient to improve the Levenshtein algorithm, this method optimizes the calculation of edit distance and enhances the ability to align tokens. Experimental results show that this method can fully utilize the contextual information of the source code during the automatic annotation process, leading to a significant improvement in annotation accuracy.},
  archive      = {J_PEERJCS},
  author       = {Jian Wang and Tao Lin and Rongsen Zhao and Huiling Zhao},
  doi          = {10.7717/peerj-cs.3024},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3024},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An enhanced approach for automatic annotation of error codes based on seq2edit},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intent-aware knowledge graph-based model for electrical power material recommendation. <em>PEERJCS</em>, <em>11</em>, e3023. (<a href='https://doi.org/10.7717/peerj-cs.3023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of electrical power material management, it is paramount that users receive accurate recommendations regarding the electrical power materials they require. Recently, a growing number of studies have been dedicated to graph neural network (GNN)-based recommendation systems due to their ability to seamlessly combine node information with topological structure, enhancing the effectiveness of recommendations. However, a notable drawback of current GNN-based recommendation is their inability to explicitly capture users’ intent in recommendations, which limits the performance. In fact, users’ intent is crucial in determining their actions. One example is when users first form an intent to buy a particular set of items and then choose a specific item from the set based on their preferences. To fill this gap, this article proposes an intent-aware knowledge graph-based model for electrical material recommendation, named IKG-EMR. IKG-EMR models user preferences and intent by leveraging knowledge graph and user behavior sequences, respectively. Specifically, a graph neural network is adopted to generate user intent embedding and item embedding from the tripartite graph of “User-Item-Topic”, and a multi-head attention network (Transformer) is used for extracting preference from user behavior sequences. Finally, an adaptive fusion with attention network is devised to generate comprehensive user representation by integrating user preference and intent features. Extensive experiments conducted on the real-life electric power materials show that our proposed model outperforms state-of-the-art methods.},
  archive      = {J_PEERJCS},
  author       = {Lin Zhao and Ning Luan and Weihua Cheng and Shuming Feng and Hui Wang and Yongcheng Yang and Guixiang Zhu},
  doi          = {10.7717/peerj-cs.3023},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3023},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Intent-aware knowledge graph-based model for electrical power material recommendation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new machine learning method for rainfall classification: Temporal random tree. <em>PEERJCS</em>, <em>11</em>, e3022. (<a href='https://doi.org/10.7717/peerj-cs.3022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional classification algorithms usually assume that all samples in a dataset contribute equally to the training of a machine learning model, which is not always the case. In fact, samples in temporal data, such as precipitation data, may not have equal importance; more recent samples contain more accurate and useful information than earlier ones. To address this issue, the article proposes a novel method, named temporal random tree (TRT), in which recent training samples have a greater impact on the model’s decision-making process. It divides the dataset into temporal segments, assigns higher weights to classifiers trained on more recent data, and employs a weighted majority voting strategy. The experiments demonstrated the effectiveness of TRT on the real-world WeatherAUS precipitation dataset, achieving an accuracy of 83.54%, which represents a 5% improvement over the traditional random tree method. Additionally, our method achieved an average improvement of 9.98% compared to state-of-the-art results in the recent literature. These findings highlight TRT’s potential as a valuable method for spatiotemporal rainfall classification.},
  archive      = {J_PEERJCS},
  author       = {Kokten Ulas Birant and Bita Ghasemkhani and Özlem Varlıklar and Derya Birant},
  doi          = {10.7717/peerj-cs.3022},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3022},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A new machine learning method for rainfall classification: Temporal random tree},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fishing operation type recognition based on multi-branch convolutional neural network using trajectory data. <em>PEERJCS</em>, <em>11</em>, e3020. (<a href='https://doi.org/10.7717/peerj-cs.3020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of fishing vessel operations is vital for sustainable fishery management. Existing methods inadequately exploit spatiotemporal contextual information in vessel trajectories and fail to effectively fuse multimodal data. To address this, this study proposes a novel framework integrating Geohash-based geocoding with embedding techniques inspired by natural language processing to extract spatiotemporal features from trajectory sequences. We develop a multi-branch 1D convolutional neural network (MB-1dCNN) to minimize feature engineering dependency while enhancing operational-type recognition. Comparative experiments evaluate Geohash encoding lengths and network architectures (single-branch vs. multi-branch, fully-connected vs. 1D-CNN). Results indicate optimal Geohash encoding at length 5. The multi-branch structure significantly outperforms single-branch counterparts, and MB-1dCNN demonstrates superior performance over multi-branch model with fully connected layers (MB-FCNN), achieving additional gains in accuracy and F1-score. Key findings reveal: (1) 1D-CNN processing surpasses fully-connected networks in sequential feature extraction, (2) Multi-branch architectures enhance information fusion capabilities. The proposed MB-1dCNN establishes state-of-the-art performance for trajectory-based fishing operation recognition, offering valuable insights for spatial computing applications in maritime surveillance.},
  archive      = {J_PEERJCS},
  author       = {Bohui Jiang and Weifeng Zhou},
  doi          = {10.7717/peerj-cs.3020},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3020},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fishing operation type recognition based on multi-branch convolutional neural network using trajectory data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced clustering and transfer learning based approach for rice leaf disease segmentation and classification. <em>PEERJCS</em>, <em>11</em>, e3018. (<a href='https://doi.org/10.7717/peerj-cs.3018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rice, the world’s most important food crop, requires an early and accurate identification of the diseases that infect rice panicles and leaves to increase production and reduce losses. Most conventional methods of diagnosing diseases involve the use of manual instruments, which are ineffective, imprecise, and time-consuming. In light of such drawbacks, this article introduces an improved deep learning and transfer learning method for diagnosing and categorizing rice leaf diseases proficiently. First, all input images are preprocessed; the images are resized to a fixed size before applying a sophisticated contrast enhanced adaptive histogram equalization procedure. Diseased regions are then segmented through the developed gravity weighted kernelised density clustering algorithm. In terms of feature extraction, EfficientNetB0 is fine-tuned by subtracting the last fully connected layers, and the classification is conducted with the new fully connected layers. Also, the tent chaotic particle snow ablation optimizer is added into the learning process in order to improve the learning process and shorten the time of convergence. The performance of the proposed framework was tested on two benchmark datasets and presented accuracy results of 98.87% and 97.54%, respectively. Comparisons of the proposed method with six fine-tuned models show the performance advantage and validity of the proposed method.},
  archive      = {J_PEERJCS},
  author       = {Samia Nawaz Yousafzai and Fahd N. Al-Wesabi and Hadeel Alsolai and Shouki A. Ebad and Inzamam Mashood Nasir and Emad Fadhal and Adel Thaljaoui},
  doi          = {10.7717/peerj-cs.3018},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3018},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Advanced clustering and transfer learning based approach for rice leaf disease segmentation and classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing BERT-based models for arabic and low-resource languages in crime text classification. <em>PEERJCS</em>, <em>11</em>, e3017. (<a href='https://doi.org/10.7717/peerj-cs.3017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bidirectional encoder representations from Transformers (BERT) has recently attracted considerable attention from researchers and practitioners, demonstrating notable effectiveness in various natural language processing (NLP) tasks, including text classification. This efficacy can be attributed to its unique architectural features, particularly its ability to process text using both left and right context, having been pre-trained on extensive datasets. In the context of the criminal domain, the classification of data is a crucial activity, and Transformers are increasingly recognized for their potential to support law enforcement efforts. BERT has been released in English and Chinese, as well as a multilingual version that accommodates over 100 languages. However, there is a pressing need to analyze the availability and performance of BERT in Arabic and other low-resource languages. This study primarily focuses on analyzing BERT-based models tailored for the Arabic language; however, due to the limited number of existing studies in this area, the research extends to include other low-resource languages. The study evaluates these models’ performance in comparison to machine learning (ML), deep learning (DL), and other Transformer models. Furthermore, it assesses the availability of relevant data and examines the effectiveness of BERT-based models in low-resource linguistic contexts. The study concludes with recommendations for future research directions, supported by empirical statistical evidence.},
  archive      = {J_PEERJCS},
  author       = {Njood K. Al-harbi and Manal Alghieth},
  doi          = {10.7717/peerj-cs.3017},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3017},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Assessing BERT-based models for arabic and low-resource languages in crime text classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Further observations on the security of speck32-like ciphers using machine learning. <em>PEERJCS</em>, <em>11</em>, e3015. (<a href='https://doi.org/10.7717/peerj-cs.3015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread deployment of Internet of Things across various industries, the security of communications between different devices is one of the critical concerns to consider. The lightweight cryptography emerges as a specialized solution to address security requirements for resource-constrained environments. Consequently, the comprehensive security evaluation of the lightweight cryptographic primitives—from the structure of ciphers and cryptographic components—has become imperative. In this article, we focus on the security evaluation of rotation parameters in the Speck32-like lightweight cipher family. We establish a machine learning-driven security evaluation framework for the rotational parameter selection principles—the core of Speck32’s design architecture. To assess different parameters security, we develop neural-differential distinguishers with considering of two distinct input difference models: (1) the low-Hamming-weight input differences and (2) the input differences from optimal differential characteristics. Our methodology achieves the security evaluation of 256 rotation parameters using the accuracy of neural distinguishers as the evaluation criteria. Our results illustrate the parameter (7,3) has stronger ability to resist machine learning-aided distinguishing attack compared to the standard (7,2) configuration. To our knowledge, this represents the first comprehensive study applying machine learning techniques for security assessment of Speck32-like ciphers. Furthermore, we investigate the reason for the difference in the accuracy of neural distinguishers with different rotation parameters. Our experimental results demonstrate that the bit bias in output differences and truncated differences is the important factor affecting the accuracy of distinguishers.},
  archive      = {J_PEERJCS},
  author       = {Zezhou Hou and Jiongjiong Ren and Shaozhen Chen},
  doi          = {10.7717/peerj-cs.3015},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3015},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Further observations on the security of speck32-like ciphers using machine learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-phase deep learning framework for advanced phishing detection using the novel OptSHQCNN approach. <em>PEERJCS</em>, <em>11</em>, e3014. (<a href='https://doi.org/10.7717/peerj-cs.3014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Phishing attacks are now regarded as one of the most prevalent cyberattacks that often compromise the security of different communication and internet networks. Phishing websites are created with the goal of generating cyber threats in order to ascertain the user’s financial information. Fake websites are frequently created and circulated online, which results in the loss of essential user assets. Phishing websites can result in monetary loss, intellectual property theft, damage to one’s reputation, and disruption of regular business activities. Over the past decade, a number of anti-phishing tactics have been proposed to detect and reduce these attempts. They are still imprecise and ineffective, though. Deep Learning (DL), which can precisely learn the intrinsic features of the websites and recognize phishing websites, is one of the innovative techniques utilized to solve this issue. Methods In this study, we proposed a novel OptSHQCNN phishing detection method. Pre-deployment and post-deployment are the two phases of the proposed methodology. The dataset undergoes preprocessing in the pre-deployment phase, which includes data balancing, and handling invalid features, irrelevant features, and missing values. The convolutional block attention module (CBAM) then extracts the main characteristics from web page code and linkages. The red kite optimization algorithm (RKOA) selects the significant key attributes in the third stage. The final phase involves classifying the data using the Shallow hybrid quantum-classical convolutional neural network (SHQCNN) model. To improve the effectiveness of the classification approach, the hyperparameters present in the SHQCNN model are fine-tuned using the shuffled shepherd optimization algorithm (SSOA). Results In the post-deployment phase, the URL is encoded using Optimized Bidirectional Encoder Representations from Transformers (OptBERT), after which the features are extracted. The retrieved properties are fed into a trained classifier. Next, a prediction of “phishing” or “Legitimate” is produced by the classifier. With a maximum of above 99% accuracy, precision, recall, and F1-score, respectively, the investigation’s findings showed that the suggested technique performed better than other popular phishing detection methods. The creation of a security plugin for clients, browsers, and other instant messaging applications that operate on network edges, PCs, smartphones, and other personal terminals can be aided by these findings.},
  archive      = {J_PEERJCS},
  author       = {Srikanth Meda and Vangipuram Sesha Srinivas and Killi Chandra Bhushana Rao and Repudi Ramesh and Narasimha Rao Yamarthi},
  doi          = {10.7717/peerj-cs.3014},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3014},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A dual-phase deep learning framework for advanced phishing detection using the novel OptSHQCNN approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting sport event outcomes using deep learning. <em>PEERJCS</em>, <em>11</em>, e3011. (<a href='https://doi.org/10.7717/peerj-cs.3011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the outcomes of sports events is inherently difficult due to the unpredictable nature of gameplay and the complex interplay of numerous influencing factors. In this study, we present a deep learning framework that combines a one-dimensional convolutional neural network (1D CNN) with a Transformer architecture to improve prediction accuracy. The 1D CNN effectively captures local spatial patterns in structured match data, while the Transformer leverages self-attention mechanisms to model long-range dependencies. This hybrid design enables the model to uncover nuanced feature interactions critical to outcome prediction. We evaluate our approach on a benchmark sports dataset, where it outperforms traditional machine learning methods and standard deep learning models in both accuracy and robustness. Our results demonstrate the promise of integrating convolutional and attention-based mechanisms for enhanced performance in sports analytics and predictive modeling.},
  archive      = {J_PEERJCS},
  author       = {Jianxiong Gao and Yi Cheng and Jianwei Gao},
  doi          = {10.7717/peerj-cs.3011},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3011},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting sport event outcomes using deep learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A social recommendation model based on adaptive residual graph convolution networks. <em>PEERJCS</em>, <em>11</em>, e3010. (<a href='https://doi.org/10.7717/peerj-cs.3010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating social information in the recommendation algorithm based on graph neural network (GNN) alleviates the data sparsity and cold-start problems to a certain extent, and effectively improves the recommendation performance of the model. However, there are still shortcomings in the existing studies: on the one hand, the potential effect of noise in the raw data is ignored; on the other hand, only relying on the single interaction information between the user and the item and failing to make full use of the rich multi-aided information. These factors lead to an unsatisfactory learning effect of the model. To address the above problems, we propose a social recommendation model based on adaptive residual graph convolutional networks (SocialGCNRI). Specifically, we use the idea of fast Fourier transform (FFT), a filtering algorithm in the field of signal processing, to attenuate the raw data noise in the frequency domain, followed by utilizing the user-social relations, item-association relations, and user-item-interaction relations to form a heterogeneous graph to supplement the model information, and finally using a graph convolution algorithm with an adaptive residual graph to improve the expressive power of the model. Extensive experiments on two real datasets show that SocialGCNRI outperforms state-of-the-art social recommendation methods on a variety of common evaluation metrics.},
  archive      = {J_PEERJCS},
  author       = {Rui Chen and Kangning Pang and Qingfang Liu and Lei Zhang and Hao Wu and Cundong Tang and Pu Li},
  doi          = {10.7717/peerj-cs.3010},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3010},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A social recommendation model based on adaptive residual graph convolution networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting danceability and song ratings using deep learning and auditory features. <em>PEERJCS</em>, <em>11</em>, e3009. (<a href='https://doi.org/10.7717/peerj-cs.3009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting a song’s danceability and overall rating poses a significant challenge due to the complex interplay between musical characteristics and listener preferences. In this study, we propose a deep learning framework that jointly addresses the tasks of danceability estimation and popularity prediction. Our model integrates a Bidirectional Long Short-Term Memory (BiLSTM) network to capture sequential and contextual patterns from categorical inputs, alongside a Residual Network (ResNet) that extracts hierarchical representations from numerical auditory features. These complementary feature streams are fused using a cross-attention mechanism, enabling the model to effectively learn intricate relationships across heterogeneous data modalities. Experimental evaluations demonstrate that our approach consistently outperforms traditional machine learning baselines and recent deep learning models. The results demonstrate the effectiveness of cross-attention in structured music data modelling and highlight the framework’s potential in advancing music recommendation and audio analysis systems.},
  archive      = {J_PEERJCS},
  author       = {Wei Wu},
  doi          = {10.7717/peerj-cs.3009},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3009},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting danceability and song ratings using deep learning and auditory features},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-learning type-2 fuzzy systems with adaptive rule reduction for time series forecasting. <em>PEERJCS</em>, <em>11</em>, e3004. (<a href='https://doi.org/10.7717/peerj-cs.3004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In rapidly changing scenarios, uncertainty and chaotic oscillations often obstruct time series prediction. However, Type-1 fuzzy systems face challenges in handling high uncertainty levels, therefore, Type-2 fuzzy systems become a better solution. Nonetheless, the complexity of Type-2 fuzzy models can produce overwhelming rules, compromising interpretability and computational efficiency. We present a Self-Learning Type-2 Fuzzy System with adaptive rule reduction that optimizes the rule base as forecast accuracy begins to deteriorate after adaptation. Our model combines participatory learning (PL) and Kernel Recursive Least Squares (KRLS) for online learning, an Adaptive reduced rule strategy to eliminate repeating rules and gain computational efficiency. Our approach incorporates a compatibility measure rooted in Type-2 fuzzy sets, paving the way for an improved consideration of uncertainty. Complex datasets, including Mackey-Glass chaotic time series and Taiwan Capitalization Weighted Stock Index (TAIEX), are used to evaluate the model, which demonstrates its superior forecasting performance compared to state-of-the-art models. Experiments show that our solution, through the development of a few rules, obtains lower error measures maintaining a small rule base, thus proving to be a scalable approach amenable to on-line deployment in fast paced environments such as those appearing in the financial markets, industrial processes and others that demand highly accurate time series forecasts in the presence of uncertainty.},
  archive      = {J_PEERJCS},
  author       = {Abdulwhab Alkharashi and Gaganjot Kaur and Hadeel Alsolai and Hatim Dafaalla and Somia Asklany and Othman Alrusaini and Ali Alqazzaz and Menwa Alshammeri},
  doi          = {10.7717/peerj-cs.3004},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3004},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Self-learning type-2 fuzzy systems with adaptive rule reduction for time series forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving gaussian naive bayes classification on imbalanced data through coordinate-based minority feature mining. <em>PEERJCS</em>, <em>11</em>, e3003. (<a href='https://doi.org/10.7717/peerj-cs.3003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a widely used classification model, the Gaussian Naive Bayes (GNB) classifier experiences a significant decline in performance when handling imbalanced data. Most traditional approaches rely on sampling techniques; however, these methods alter the quantity and distribution of the original data and are prone to issues such as class overlap and overfitting, thus presenting clear limitations. This article proposes a coordinate transformation algorithm based on radial local relative density changes (RLDC). A key feature of this algorithm is that it preserves the original dataset’s quantity and distribution. Instead of modifying the data, it enhances classification performance by generating new features that more prominently represent minority classes. The algorithm transforms the dataset from absolute coordinates to RLDC-relative coordinates, revealing latent local relative density change features. Due to the imbalanced distribution, sparse feature space, and class overlap, minority class samples can exhibit distinct patterns in these transformed features. Based on these new features, the GNB classifier can increase the conditional probability of the minority class, thereby improving its classification performance on imbalanced datasets. To validate the effectiveness of the proposed algorithm, this study conducts comprehensive comparative experiments using the GNB classifier on 20 imbalanced datasets of varying scales, dimensions, and characteristics. The evaluation includes 10 oversampling algorithms, two undersampling algorithms, and two hybrid sampling algorithms. Experimental results show that the RLDC-based coordinate transformation algorithm ranks first in the average performance across three classification evaluation metrics. Compared to the average values of the comparison algorithms, it achieves improvements of 21.84%, 33.45%, and 54.63% across the three metrics, respectively. This algorithm offers a novel approach to addressing the imbalanced data problem in GNB classification and holds significant theoretical and practical value.},
  archive      = {J_PEERJCS},
  author       = {Wei Wang and Li Yan and Fen Liu and Yanxi Li},
  doi          = {10.7717/peerj-cs.3003},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3003},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving gaussian naive bayes classification on imbalanced data through coordinate-based minority feature mining},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid deep learning framework for skin disease localization and classification using wearable sensors. <em>PEERJCS</em>, <em>11</em>, e3002. (<a href='https://doi.org/10.7717/peerj-cs.3002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of skin diseases is essential for timely intervention and treatment. This article proposes a patch-based, interpretable deep learning framework for skin disease detection using wearable sensors and clinical data. Specifically, a fully convolutional residual neural network (FCRN) is employed to extract local features from high-resolution skin images captured via wearable sensors, using a patch-level training approach. Pre-processing techniques—including image resampling, intensity normalization, and noise reduction—standardize the input data to ensure consistency across sensor variations. To enhance local feature learning, the FCRN incorporates residual modules, which mitigate gradient vanishing and improve model performance. The framework generates disease probability maps that visualize regions of high diagnostic risk, providing interpretable insights into skin anomalies. In the proposed methodology, a convolutional neural network (CNN) integrates image-derived features with clinical data such as patient demographics, symptoms, and medical history. This CNN-based multimodal fusion approach improves the model’s ability to capture spatial relationships and enhances classification performance. Experimental evaluations demonstrate that the proposed framework achieves state-of-the-art results across multiple evaluation metrics, including accuracy, sensitivity, and specificity. The interpretable disease probability maps highlight affected skin regions, enhancing model transparency and clinical usability. This approach demonstrates the potential of combining wearable sensor technology with deep learning for efficient, scalable, and explainable skin disease detection, laying the foundation for real-time clinical applications.},
  archive      = {J_PEERJCS},
  author       = {Xiaoling Zhao and Huixin Zhang and Qian Zheng and Caihong Jing},
  doi          = {10.7717/peerj-cs.3002},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3002},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid deep learning framework for skin disease localization and classification using wearable sensors},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning in time series forecasting with transformer models and RNNs. <em>PEERJCS</em>, <em>11</em>, e3001. (<a href='https://doi.org/10.7717/peerj-cs.3001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the increasing need for accurate weather forecasts, the use of neural networks, especially transformer and recurrent neural networks (RNNs), has been highlighted for their ability to capture complex patterns in time series. This study examined 14 neural network models applied to forecast weather variables, evaluated using metrics such as median absolute error (MedianAbsE), mean absolute error (MeanAbsE), maximum absolute error (MaxAbsE), root mean squared percent error (RMSPE), and root mean square error (RMSE). Transformer-based models such as Informer, iTransformer, Former, and patch time series transformer (PatchTST) stood out for their accuracy in capturing long-term patterns, with Informer showing the best performance. In contrast, RNN models such as auto-temporal convolutional networks (TCN) and bidirectional TCN (BiTCN) were better suited to short-term forecasting, despite being more prone to significant errors. Using iTransformer it was possible to achieve a MedianAbsE of 1.21, MeanAbsE of 1.24, MaxAbsE of 2.86, RMSPE de 0.66, and RMSE de 1.43. This study demonstrates the potential of neural networks, especially transformers, to improve accuracy, providing a practical and theoretical basis for selecting the most suitable models for predictive applications.},
  archive      = {J_PEERJCS},
  author       = {Rogerio Pereira dos Santos and João P. Matos-Carvalho and Valderi R. Q. Leithardt},
  doi          = {10.7717/peerj-cs.3001},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3001},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning in time series forecasting with transformer models and RNNs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective classification for neonatal brain injury using EEG feature selection based on elastic net regression and improved crow search algorithm. <em>PEERJCS</em>, <em>11</em>, e3000. (<a href='https://doi.org/10.7717/peerj-cs.3000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neonatal brain injury carries the risk of neurological sequelae such as epileptic seizures, cerebral palsy, intellectual disability, and even death. Classification methods based on electroencephalography (EEG) signals and machine learning algorithms are crucial for assessing neonatal brain injury. However, classification methods that utilise all features from the original EEG signals may result in lengthy training and classification times, thereby reducing the performance of the classification system. This article presents a novel classification system with a proposed feature selection method for assessing neonatal brain injury, in which the feature selection method is combined using elastic net (EN) regression and an improved crow search algorithm (ICSA), named EN-ICSA. In the EN-ICSA method, EN regression is used to conduct the pre-screening of features. The ICSA is utilised to select the essential figures further by introducing the dynamic perception probability for deciding whether to search locally or globally, a novel neighbor-following strategy for the local search and a global search strategy according to the crow’s search experience, resulting in accelerating the search efficiency while effectively avoiding falling into local optima. Experimental results demonstrate that the proposed system, based on support vector machine (SVM) with the EN-ICSA feature selection method, performs exceptionally well compared to other traditional machine learning and feature selection methods, achieving an accuracy of 91.94%, precision of 92.32%, recall of 89.85%, and F1-score of 90.82%.},
  archive      = {J_PEERJCS},
  author       = {Ling Li and Tao Yue and Hui Wu and Yanping Zhao and Qinmei Liu and Hairong Zhang and Wei Xu},
  doi          = {10.7717/peerj-cs.3000},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3000},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Effective classification for neonatal brain injury using EEG feature selection based on elastic net regression and improved crow search algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of predictive maintenance efficiency with the comparison of machine learning models in machining production process in brake industry. <em>PEERJCS</em>, <em>11</em>, e2999. (<a href='https://doi.org/10.7717/peerj-cs.2999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The utilization of technologies such as artificial intelligence (AI) and machine learning (ML) in industrial sectors has become a crucial requirement to enhance the efficiency and stability of production processes. Regular maintenance of machines and early detection of faults play a critical role in ensuring uninterrupted production and business continuity. Predictive maintenance practices, combined with sensors and data analysis methods, enable the collection, analysis, and transformation of machine-related data into meaningful insights. As a result, the anticipation of potential machine failures, the execution of planned maintenance activities, and the prevention of unexpected downtime become possible. These methods not only improve productivity in production processes but also contribute to reducing maintenance costs. Methods This study aims to predict machine faults using data analysis methods and enhance the accuracy performance of these predictions for an industrial company that produces braking components. Comprehensive examination and analysis of data were conducted to understand the symptoms and relationships of machine failures. ML classification methods were employed in the relevant study. Results Challenges such as the imbalance of class distributions in the dataset, the presence of missing and outlier values, and the high costs of necessary equipment and training pose significant barriers to implementation. Addressing these issues is critical for achieving effective predictive maintenance solutions. In order to achieve more accurate results, data splitting and k-fold cross-validation methods were applied during the learning and testing phases to overcome the imbalance problem in the dataset, undersampling techniques were applied, and outlier detection and normalization processes were used to improve data quality. The model performances, evaluated through accuracy, precision, recall, and F1-score, area under the curve (AUC), Cohen’s Matthew’s correlation coefficient (MCC) were compared. Hyperparameter optimization was also performed, resulting in significant improvements in model performance. This study contributes to the literature in terms of predictive maintenance application, classification, and data partitioning techniques. The findings highlight the importance of data preprocessing and advanced modeling techniques in predictive maintenance and emphasize how addressing data challenges can enhance the overall performance and reliability of ML models.},
  archive      = {J_PEERJCS},
  author       = {Can Aydın and Burak Evrentuğ},
  doi          = {10.7717/peerj-cs.2999},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2999},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evaluation of predictive maintenance efficiency with the comparison of machine learning models in machining production process in brake industry},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tracing truth: Dynamic temporal networks for multi-modal fake news detection. <em>PEERJCS</em>, <em>11</em>, e2998. (<a href='https://doi.org/10.7717/peerj-cs.2998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the internet continues to evolve rapidly and social media becomes increasingly prevalent, the ways people access information has become increasingly diverse. However, the proliferation of fake news has emerged as a critical problem, presenting major challenges to the integrity of the information ecosystem. To address the complex propagation mechanisms of fake news, existing studies leverage multi-modal information and dynamic propagation social graphs for effective detection. Nonetheless, capturing the temporal relationships of propagation nodes in dynamic social networks accurately and dynamically integrating multi-modal information for improved detection accuracy remains a technical challenge. In response, This study proposes a multimodal approach to fake news detection—the dynamic temporal network (DTN) model. Firstly, this model designs a time similarity strength metric to measure the temporal similarity among nodes in propagation sequences and introduces a weighting mechanism to dynamically fuse multi-modal information. Secondly, it constructs a social propagation graph model, enhancing node representation through the dynamic variations of time similarity and graph structure, and utilizes the Transformer encoder to extract the overall semantic features of news propagation. Furthermore, the model views the news propagation process as a complex system, analyzing the temporal dynamics of news in real social networks, effectively revealing the abnormal propagation patterns of fake news. Further analysis demonstrates that the proposed DTN model exhibits high accuracy and effectiveness in multi-modal fake news detection.},
  archive      = {J_PEERJCS},
  author       = {Jiaen Hu and Juan Zhang and Zichen Li},
  doi          = {10.7717/peerj-cs.2998},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2998},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Tracing truth: Dynamic temporal networks for multi-modal fake news detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DA-FIS: A high-speed dynamic adaptive fault injection server framework for reliable FPGA-based embedded systems. <em>PEERJCS</em>, <em>11</em>, e2996. (<a href='https://doi.org/10.7717/peerj-cs.2996'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault injection is a critical technique for assessing the reliability of field programmable gate array (FPGA)-based embedded systems, particularly in radiation-prone and safety-critical applications. Conventional fault injection methods, such as bit upset fault injection testing (BUFIT), single critical fault injection testing (SCFIT), and dynamic partial reconfiguration (DPR), suffer from high resource overhead, slow injection speeds, and limited adaptability, making them inadequate for real-time fault resilience evaluation. This article introduces the dynamic adaptive fault injection server (DA-FIS), a high-speed, scalable, and resource-efficient fault injection framework designed to overcome these limitations. Unlike traditional methods, DA-FIS employs a configurable LFSR-based fault generator that enables adaptive and real-time fault injection based on workload sensitivity and system conditions. The proposed framework integrates masking logic and dynamic propagation tracking, allowing precise injection of single-event upsets (SEUs) and multiple-bit upsets (MBUs) into FPGA configuration memory and logic without disturbing non-targeted regions. DA-FIS is implemented on the Xilinx Zynq-7000 FPGA and evaluated across multiple benchmark workloads, including the Bubble Sort algorithm, 4-bit adder, 4-bit multiplier, and counter-based logic circuits. Experimental results demonstrate that DA-FIS achieves a fault injection rate of 111.1 faults per second, outperforming BUFIT (53.4 faults/s), SCFIT (27 faults/s), and DPR (18.5 faults/s), with 30% lower FPGA resource overhead compared to SCFIT. The adaptive architecture ensures seamless scalability across different FPGA platforms, making it suitable for space electronics, automotive safety systems, and high-performance computing. Additionally, DA-FIS supports real-time error model adjustments, enabling researchers to analyze fault propagation, error correction strategies, and security vulnerabilities in FPGA-based architectures. This work establishes DA-FIS as a superior fault injection framework, offering high-speed, precision-controlled fault testing while maintaining minimal FPGA overhead and enhanced scalability. Future research will explore machine learning-assisted fault modeling and self-healing FPGA architectures to further enhance FPGA fault resilience in safety-critical and autonomous systems.},
  archive      = {J_PEERJCS},
  author       = {Fatimah Alhayan and Gaganjot Kaur and Sultan Alanazi and Mohammed Burhanur Rehman and Wahida Mansouri and Da’ad Albalawneh and Ali Alqazzaz and Hanadi Alkhudhayr},
  doi          = {10.7717/peerj-cs.2996},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2996},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DA-FIS: A high-speed dynamic adaptive fault injection server framework for reliable FPGA-based embedded systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formal modeling of a causal consistent distributed system and verification of its history via model checking using colored petri net. <em>PEERJCS</em>, <em>11</em>, e2995. (<a href='https://doi.org/10.7717/peerj-cs.2995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various consistency models for replicated distributed systems (DSs) have been developed and are usually implemented in the middleware layer. Causal consistency (CC) is a widely used consistency model appropriate for distributed applications like discussion groups and forums. One of the known distributed algorithms for CC is based on logical time synchronization with Fidge vector clocks that use the concepts of the hold-back and delivery queues for each replica. The basics of the algorithm and its assumptions are presented in the article. Then, a novel formal hierarchical colored Petri net model of a DS with CC support and three constituting replicas is presented. The proposed model operates based on the presented distributed algorithm for CC support with potential randomness for delays in message delivery. The article tries to answer the question: is a given distributed history (DH) a valid image of a causal-consistent distributed system (CCDS)? The proposed model validates a DH via model checking. The question is answered by the execution of the proposed model and the generation of its state space graph (SSG). Required model checking functions are developed for automatically analyzing SSG for (1) extracting the existence of the answer and (2) extraction of the shortest proof scenarios that can generate the given input DH. The model was used to analyze four case study examples. The article presents three effective techniques for decreasing the state space explosion problem. Results show that the colored Petri net model of a CCDS can automatically validate a DH using model checking.},
  archive      = {J_PEERJCS},
  author       = {Khalid Amjed Mohammed Alsaegg and Saeid Pashazadeh and Mina Zolfy Lighvan},
  doi          = {10.7717/peerj-cs.2995},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2995},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Formal modeling of a causal consistent distributed system and verification of its history via model checking using colored petri net},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early breast cancer detection in CT scans using convolutional neural bidirectional feature pyramid network. <em>PEERJCS</em>, <em>11</em>, e2994. (<a href='https://doi.org/10.7717/peerj-cs.2994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is one of the leading causes of death among women worldwide. Early detection plays a crucial role in reducing mortality rates. While mammography is a widely used diagnostic tool, computed tomography (CT) scans are increasingly being explored for detecting breast cancer due to their high-resolution imaging and ability to visualize tissue in 3D. Despite the potential of CT scans in visualizing breast tissue in 3D with high resolution, extracting meaningful patterns from these scans is difficult due to the complex and nonlinear nature of the tissue features. The challenge lies in developing computational methods that can accurately detect and localize breast cancer lesions, especially when the tumors vary in size, shape, and density. In this article, we proposed a framework called convolutional neural bidirectional feature pyramid network, which integrates multi-scale feature extraction and bidirectional feature fusion for breast cancer detection in CT scans. The proposed framework classified the images into diseased and non-diseased and then identified the infected region on breast tissue. Using convolutional neural networks, we defined several layers to classify the diseased and normal CT scan images. We collected data on breast CT scans taken from the radiology department, Ayub Teaching Hospital Abbottabad, Pakistan. We evaluated the model using a variety of classification metrics such as precision, recall, F1-measure, and average precision to determine its effectiveness in finding breast cancer lesions, and we found 96.11% accuracy. Our findings show that compared with current state-of-the-art methods, the proposed framework has satisfactory results in identifying breast cancer areas, and the proposed framework over the baselines has achieved a 1.71% improvement.},
  archive      = {J_PEERJCS},
  author       = {Tahani Jaser Alahmadi and Adeel Ahmed and Amjad Rehman and Abeer Rashad Mirdad and Bayan Al Ghofaily and Shehryar Ali},
  doi          = {10.7717/peerj-cs.2994},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2994},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Early breast cancer detection in CT scans using convolutional neural bidirectional feature pyramid network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foundational models and federated learning: Survey, taxonomy, challenges and practical insights. <em>PEERJCS</em>, <em>11</em>, e2993. (<a href='https://doi.org/10.7717/peerj-cs.2993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning has the potential to unlock siloed data and distributed resources by enabling collaborative model training without sharing private data. As more complex foundational models gain widespread use, the need to expand training resources and integrate privately owned data grows as well. In this article, we explore the intersection of federated learning and foundational models, aiming to identify, categorize, and characterize technical methods that integrate the two paradigms. As a unified survey is currently unavailable, we present a literature survey structured around a novel taxonomy that follows the development life-cycle stages, along with a technical comparison of available methods. Additionally, we provide practical insights and guidelines for implementing and evolving these methods, with a specific focus on the healthcare domain as a case study, where the potential impact of federated learning and foundational models is considered significant. Our survey covers multiple intersecting topics, including but not limited to federated learning, self-supervised learning, fine-tuning, distillation, and transfer learning. Initially, we retrieved and reviewed a set of over 4,200 articles. This collection was narrowed to more than 250 thoroughly reviewed articles through inclusion criteria, featuring 42 unique methods. The methods were used to construct the taxonomy and enabled their comparison based on complexity, efficiency, and scalability. We present these results as a self-contained overview that not only summarizes the state of the field but also provides insights into the practical aspects of adopting, evolving, and integrating foundational models with federated learning.},
  archive      = {J_PEERJCS},
  author       = {Cosmin-Andrei Hatfaludi and Alex Serban},
  doi          = {10.7717/peerj-cs.2993},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2993},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Foundational models and federated learning: Survey, taxonomy, challenges and practical insights},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HTCNN-attn: A fine-grained hierarchical multi-label deep learning model for disaster emergency information intelligent extraction from social media. <em>PEERJCS</em>, <em>11</em>, e2992. (<a href='https://doi.org/10.7717/peerj-cs.2992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenge of extracting fine-grained emergency information from noisy social media during disasters, we propose HTCNN-Attn, a hierarchical multi-label deep learning model. It integrates a three-level tree-structured labeling architecture, Transformer-based global feature extraction, convolutional neural network (CNN) layers for local pattern capture, and a hierarchical attention mechanism. The model employs a hierarchical loss function to enforce label consistency across three levels: binary disaster filtering (Level 1), mid-level category classification (Level 2), and fine-grained subcategory extraction (Level 3). Experiments on the Appen and HumAID datasets demonstrate superior performance, achieving an accuracy of 0.9725, a Micro-F1 of 0.7402, and a hierarchical consistency (HC) score of 0.821, outperforming state-of-the-art baselines. Ablation experiments validate the importance of hierarchical modeling, Transformer encoding, CNN layers, pre-trained embeddings, and the hierarchical attention mechanism. Cross-event generalization tests on the CrisisBench dataset demonstrate robust generalization (HC = 0.678), while its lightweight design enables efficient real-time deployment (12.0 ms latency). A case study of the 2015 Nepal earthquake validates its practical utility, where the model accurately classified tweets into hierarchical labels and routed structured information to support emergency response coordination. This demonstrates the effectiveness of the proposed model in supporting rapid, efficient, and fine-grained emergency response after disasters, thereby enhancing disaster response capabilities.},
  archive      = {J_PEERJCS},
  author       = {Shanshan Li and Qingjie Liu and Xiaoling Sun},
  doi          = {10.7717/peerj-cs.2992},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2992},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HTCNN-attn: A fine-grained hierarchical multi-label deep learning model for disaster emergency information intelligent extraction from social media},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on license plate recognition based on graphically supervised signal-assisted training. <em>PEERJCS</em>, <em>11</em>, e2989. (<a href='https://doi.org/10.7717/peerj-cs.2989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background With the rapid growth of the number of cars and the increasing complexity of urban transportation, it is particularly important to achieve high-accuracy license plate recognition in complex scenarios. However, since license plate recognition models are mostly deployed on embedded devices with limited computational resources, designing a lightweight and accurate model has become an urgent problem in the field of license plate recognition. Methods This study proposes an improved license plate recognition algorithm. We use the License Plate Recognition Network (LPRNet) as the base model. To enhance its accuracy, we incorporate graphically supervised signals for assisted training. This approach refines the training process, yielding a model that is both lightweight and highly accurate. An auxiliary training branch is added, utilizing these graphical signals to guide the model in learning improved image features. Results Experiments show that compared with LPRNet, this study improves the accuracy in all test sets of the Chinese City Parking Dataset (CCPD) dataset, where the average accuracy is improved by 5.86%, the maximum accuracy by 10.9%, the average character precision by 2.1%, and the average recall by 6.9%, indicating that this study can achieve higher accuracy while keeping it lightweight. This study also provides new ideas for other deep learning image recognition tasks.},
  archive      = {J_PEERJCS},
  author       = {Dianwei Chi and Zehao Jia and Lizhen Liu},
  doi          = {10.7717/peerj-cs.2989},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2989},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on license plate recognition based on graphically supervised signal-assisted training},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TaGra: An open python package for easily generating graphs from data tables through manifold learning. <em>PEERJCS</em>, <em>11</em>, e2986. (<a href='https://doi.org/10.7717/peerj-cs.2986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of analyzing high-dimensional data affects many scientific disciplines, from pharmacology to chemistry and biology. Traditional dimensionality reduction methods often oversimplify data, making it difficult to interpret individual points. This distortion can complicate the visualization of mutual distances between data points in the reduced space. Graphs provide an effective framework for representing objects and their relationships. One of their possible use is visualizing similarity patterns in tabular datasets. Here we introduce TaGra, an off-the-shelf package designed to generate a graph of similarity relations from tabular data. TaGra enables the visualization of datasets in 2D space, identification of typical data points and outliers, and assessment of the separation between items with different target variables. We describe TaGra’s functionality, options and setup. The software including examples, instructions and a guide, is openly available on PyPI at https://pypi.org/project/TaGra/ and on GitHub at https://github.com/davidetorre92/TaGra.},
  archive      = {J_PEERJCS},
  author       = {Davide Torre and Davide Chicco},
  doi          = {10.7717/peerj-cs.2986},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2986},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TaGra: An open python package for easily generating graphs from data tables through manifold learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DLProv: A suite of provenance services for deep learning workflow analyses. <em>PEERJCS</em>, <em>11</em>, e2985. (<a href='https://doi.org/10.7717/peerj-cs.2985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) workflows consist of multiple interdependent and repetitive steps, including data preparation, model training, evaluation, and deployment. Each step involves decisions impacting the final model’s performance, interpretability, and applicability. These models rely on data, preprocessing operations, and configuration, underscoring the need for mechanisms to ease the analysis throughout the entire life cycle—from model generation and selection to deployment. Moreover, ensuring trust, reproducibility, and transparency becomes important as DL models transition into production environments. Traceability across the steps of the DL workflow is essential to address these challenges. However, existing traceability solutions often present limitations. Many fail to integrate the steps of the DL workflow, focusing on either data preparation or model training. Additionally, they frequently rely on proprietary formats to represent traceability data and rarely produce a provenance document that can accompany the model into production. To bridge these gaps, we present DLProv, a suite of provenance services designed to ensure end-to-end traceability across DL workflows. DLProv supports structured query language (SQL)-based querying during training and generates provenance graphs that capture data preparation steps, model training, and evaluation. These provenance graphs comply with the PROV de facto standard, ensuring interoperability across different environments. One of the key strengths of DLProv lies in its framework-agnostic architecture. The suite’s services can be invoked independently of the DL framework, enabling integration across several training and deployment workflows. Furthermore, DLProv includes specialized instances designed for specific DL frameworks, such as Keras and physics-informed neural networks (PINNs), offering adaptability to a wide range of applications. We evaluated DLProv using well-established datasets, including Modified National Institute of Standards and Technology (MNIST) and Canadian Institute for Advanced Research (CIFAR)-100. These datasets were chosen to illustrate the suite’s capability to capture and manage provenance data across tasks of varying complexity, from basic image classification to more complex DL workflows. Additionally, we evaluated DLProv within a handwritten transcription workflow, further showcasing its flexibility. Across all these use cases, DLProv showed its ability to ease SQL-based queries during model training while maintaining framework independence. An important aspect of our evaluation was measuring the overhead introduced by integrating DLProv into DL workflows. The results showed a maximum overhead of 1.4% in execution time, highlighting the suite’s minimal impact on DL workflow performance. For comparative analysis, we benchmarked this overhead against MLflow, further reinforcing DLProv’s suitability for real-world DL applications.},
  archive      = {J_PEERJCS},
  author       = {Débora Pina and Liliane Kunstmann and Adriane Chapman and Daniel de Oliveira and Marta Mattoso},
  doi          = {10.7717/peerj-cs.2985},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2985},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DLProv: A suite of provenance services for deep learning workflow analyses},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-based enhanced boosting algorithm for depression detection. <em>PEERJCS</em>, <em>11</em>, e2981. (<a href='https://doi.org/10.7717/peerj-cs.2981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is a rapidly increasing mental disorder that can interfere with a person’s ability and negatively affect functions in various aspects of life. Fortunately, machine learning and deep learning techniques have demonstrated excellent results in the early detection of depression using social media data. Most recently, researchers have utilized boosting algorithms including pre-defined boosting algorithms or built their own boosting algorithm for the detection of depression. However, both types of boosting algorithms struggle with the analysis of complex feature sets, the enhancement of weak learners, and the handling of larger datasets. Thus, this study has developed a novel feature-based enhanced boosting algorithm (F-EBA). The proposed model covers two pipelines, the feature engineering pipeline which improves the quality of features by picking up the most relevant features while the classification pipeline uses an ensemble approach designed to boost/elevate the model’s performances. The experimental results highlighted that various parameter including WordVec and BERT embeddings, attention mechanisms, and feature elimination techniques, significantly contributed to the selection of the most relevant features. This approach resulted in generating an optimized feature set that augmented both the model’s accuracy and its interpretability. In addition, utilizing over 46 million records, the F-EBA model significantly enhanced the performance of weak learners through a weight maximization strategy, achieving an impressive accuracy rate of 95%. Moreover, the integration of an adversarial layer that employs defense mechanisms against synonymous text and sarcastic phrases within the datasets has further boosted the F-EBA model’s accuracy to approximately 97%, surpassing the results reported in prior studies. Moreover, the optimized feature sets derived from the F-EBA model make a substantial contribution to boosting the performance of baseline classifiers, marking a novel advancement in the field.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Sadiq Rohei and Kasturi Dewi Varathan and Shivakumara Palaiahnakote and Nor Badrul Anuar},
  doi          = {10.7717/peerj-cs.2981},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2981},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Feature-based enhanced boosting algorithm for depression detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data driven healthcare insurance system using machine learning and blockchain technologies. <em>PEERJCS</em>, <em>11</em>, e2980. (<a href='https://doi.org/10.7717/peerj-cs.2980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare recommendations and insurance have recently been one of the most emerging research areas in health informatics. The fraud in health insurance is becoming increasingly common day by day. To handle healthcare insurance fraud, there is an urgent need for an intelligent system that cannot only identify and monitor doctors’ and hospitals’ behavior regarding the health services they provide to patients but can also recommend doctors and hospitals to insured employees based on the quality of services they provided previously. This system creates patient and doctor profiles separately, based on their rating. The proposed system combines singular value decomposition (SVD), K-nearest neighbors based collaborative filtering (KNN-based CF), item-based collaborative filtering (Item-based CF), content-based filtering using term frequency-inverse document frequency (TF-IDF), and K-means clustering and probability distributions to recommend doctors and insurance plans. The system measures similarity scores between patients and doctors using cosine similarity, which helps to determine similarity scores and refine the recommendations. This study also uses blockchain technology to automate insurance claims reimbursement. The results are validated using real data from the employees of a local hospital. The system provides recommendations with a root mean square error (RMSE) value of 0.478 and a mean absolute error (MAE) value of 0.0422. The insurance plans developed using the proposed system have reduced the overall expenditure of the local hospital, with a reduction in total expenses. Blockchain technology further helps prevent healthcare fraud. In the proposed system, a healthcare insurance claims reimbursement system is built using smart contract technology on the Ethereum blockchain, ensuring security & transparency and lowering the number of healthcare frauds. The system includes roles for the insurance company, healthcare provider, and patients. It also provides a platform for claim submission, approval, or refusal. In Pakistan, no such system existed before recommending doctors from different hospitals based on their professional conduct or the good health services they provide.},
  archive      = {J_PEERJCS},
  author       = {Irum Matloob and Shoab Khan and Bushra Bashir and Rukaiya Rukaiya and Javed Ali Khan and Hessa Alfraihi},
  doi          = {10.7717/peerj-cs.2980},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2980},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data driven healthcare insurance system using machine learning and blockchain technologies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating, retrieving persona and generating responses for long-term open-domain dialogue. <em>PEERJCS</em>, <em>11</em>, e2979. (<a href='https://doi.org/10.7717/peerj-cs.2979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain dialogue systems have shown remarkable capabilities in generating natural and consistent responses in short-term conversations. However, in long-term conversations such as multi-session chat (MSC), where the dialogue history exceeds the model’s maximum input length (i.e., 1024 tokens), existing dialogue generation systems often overlook the information from earlier dialogues, leading to the loss of context. To prevent such loss and generate natural, consistent responses, we propose a GRGPerDialogue framework, consisting of three main stages: generating persona from past dialogues, retrieving persona relevant to the current utterance, and generating responses based on both persona and recent dialogues. In the first stage, we generate the persona of each speaker in real-time with diverse expressions, leveraging Llama 2 In-Context Learning (ICL). Subsequently, we propose a new dataset called Persona-Utterance Pair (PUP) and use it to train Facebook dense passage retrieval (DPR) model for retrieving persona sentences relevant to the current utterance. Finally, we train generative models such as Generative Pre-trained Transformer 2 (GPT-2) and Bidirectional and Auto-Regressive Transformers (BART) to generate responses based on retrieved persona sentences and the recent dialogues. Experimental results on a long-term dialogue dataset demonstrate that the GRGPerDialogue framework outperforms baseline models by approximately 0.6% to 1% in terms of the Rouge-1 metric. Furthermore, human evaluation results supported the effectiveness of GRGPerDialogue. These results indicate that GRGPerDialogue can generate responses that are not only more fluent and consistent, but also more relevant to the dialogue history than baseline models.},
  archive      = {J_PEERJCS},
  author       = {Dohyun Cha and Dawon Lee and Jihie Kim},
  doi          = {10.7717/peerj-cs.2979},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2979},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Generating, retrieving persona and generating responses for long-term open-domain dialogue},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classifying reservoir facies using attention-based residual neural networks. <em>PEERJCS</em>, <em>11</em>, e2977. (<a href='https://doi.org/10.7717/peerj-cs.2977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate classification of reservoir facies remains a fundamental challenge in petroleum geoscience, with significant implications for resource extraction efficiency and reservoir characterization. Traditional approaches relying on manual interpretation and conventional machine learning methods often struggle with the complexity and heterogeneity of well-log data. This architectural approach, in contrast to traditional single-stream or non-residual designs, significantly enhances the model’s ability to concentrate on key geological features while preserving hierarchical representations of the data. Consequently, it more effectively addresses data heterogeneity and contextual dependencies. The framework was trained and evaluated using measurements from eight wells that represent diverse geological settings. Comparative experiments against conventional machine learning models and state-of-the-art deep learning techniques demonstrated the superiority of our method, achieving an area under the receiver operating characteristic curve (AUROC) of 0.883 and an area under the precision-recall curve (AUPRC) of 0.502. These enhancements enable more accurate identification of subtle facies boundaries and lithological variations, particularly in complex geological formations, thereby facilitating improved reservoir delineation and reducing uncertainty in field development planning. Furthermore, reproducibility analyses confirmed consistent performance across independent trials, underscoring the model’s robustness and its viability for real-world reservoir characterization workflows.},
  archive      = {J_PEERJCS},
  author       = {An Hai Nguyen and Khang Nguyen and Nga Mai},
  doi          = {10.7717/peerj-cs.2977},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2977},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Classifying reservoir facies using attention-based residual neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Web application firewall based on machine learning models. <em>PEERJCS</em>, <em>11</em>, e2975. (<a href='https://doi.org/10.7717/peerj-cs.2975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing reliance on web applications for storing sensitive data and financial transactions has elevated the importance of web application security. A machine learning-based web application firewall was designed to protect web applications against injection vulnerabilities. A hybrid dataset, including CISC 2010, HTTPParams 2015, and real-time Hypertext Transfer Protocol (HTTP) requests, was employed. The study evaluated five classification algorithms—K-nearest neighbors, logistic regression, naïve Bayes, support vector machine, and decision tree—for detecting cross site scripting (XSS), Structured Query Language (SQL) Injection, Operating System Command Injection, and Local File Inclusion attacks. Decision tree was identified as the algorithm with the highest precision, accuracy, recall, F1-score, receiver operating characteristic (ROC), and area under the curve (AUC) values. According to the confusion matrix analysis, the real-time tested web application firewalls (WAF) achieved a remarkably high F1 score of 93.13% and accuracy of 93.27%. The findings indicate that machine learning-based WAFs effectively protect web applications against injection threats. Future work includes expanding the WAF to cover other attack types and testing it on different datasets.},
  archive      = {J_PEERJCS},
  author       = {Muhammed Ersin Durmuşkaya and Selim Bayraklı},
  doi          = {10.7717/peerj-cs.2975},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2975},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Web application firewall based on machine learning models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of efficient generalized digital fractional order differentiators using an improved whale optimization algorithm. <em>PEERJCS</em>, <em>11</em>, e2971. (<a href='https://doi.org/10.7717/peerj-cs.2971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new design and realization method for generalized digital fractional-order differentiator (GFOD) based on a composite structure of infinite impulse response (IIR) subfilters. The proposed method utilizes an improved whale optimization algorithm (IWOA) to compute the optimal coefficients of IIR subfilters of the realization structure. IWOA is developed by incorporating a piecewise linear chaotic mapping (PWLCM) and an adaptive inertia weight based on the hyperbolic tangent function (AIWHT) into the framework of original whale optimization algorithm (WOA). Simulation experiments are conducted to compare the performance of our method with that of well-known techniques, real-coded genetic algorithm (RCGA), particle swarm optimization (PSO), and original WOA. The results show that the new metaheuristic is superior to the other metaheuristics in terms of attaining the most accurate GFOD approximation. Moreover, the proposed IIR-based GFOD is compared with state-of-the-art GFOD, and observed to save about 50% of implementation complexity. Therefore, our method can be utilized in real-world digital signal processing applications.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Ali Mohammed Moqbel and Talal Ahmed Ali Ali and Zhu Xiao and Amani Ali Ahmed Ali},
  doi          = {10.7717/peerj-cs.2971},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2971},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of efficient generalized digital fractional order differentiators using an improved whale optimization algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of EfficientNet CNN models for multi-label chest X-ray disease diagnosis. <em>PEERJCS</em>, <em>11</em>, e2968. (<a href='https://doi.org/10.7717/peerj-cs.2968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of chest X-ray images, which are critical for the early diagnosis of many diseases, is a difficult and time-consuming process due to the multiple labeling requirements and similar looking pathologies. In traditional methods, expert physicians analyze high-resolution chest X-ray images to diagnose these diseases using observational methods, a process that can lead to human error and hence misdiagnosis or underdiagnosis. In this study, we aim to autonomously detect 14 different diseases that significantly affect human health and some cases even lead to death using chest X-ray images in a multi-class manner using deep learning techniques. Previous studies on chest X-ray images focus on a single disease or have low success rates, and the architectures presented in previous studies have high computational costs. The novelty of this work is that it presents a hybrid lightweight, fast and attention-based architecture with high classification performance. In this study, we used the ChestX-Ray14 dataset consisting of 112,104 labeled chest X-ray images of 14 disease classes. Eight deep learning architectures (EfficientNetB0-B7) and coordinate attention mechanism are used in the training and testing processes. The proposed EfficientNetB7 architecture achieved an average overall classification performance with an AUC value of 0.8265. The EfficientNet enhanced with coordinate attention architecture achieved a classification success with an AUC value of 0.8309. Moreover, when the proposed architecture and the individual disease classes are considered separately, higher classification success is achieved for eight of the 14 diseases in the dataset. Finally, the results of this study outperformed the classification performance of other similar studies in the literature in terms of AUC score. The results obtained in our study show that the proposed deep learning based lightweight and fast architecture can support radiologists in decision making in disease diagnosis. The use of autonomous disease diagnosis systems can support the protection of human health by preventing incomplete or erroneous diagnoses.},
  archive      = {J_PEERJCS},
  author       = {Murat Ucan and Buket Kaya and Osman Aygun and Mehmet Kaya and Reda Alhajj},
  doi          = {10.7717/peerj-cs.2968},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2968},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparison of EfficientNet CNN models for multi-label chest X-ray disease diagnosis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Specx: A c++ task-based runtime system for heterogeneous distributed architectures. <em>PEERJCS</em>, <em>11</em>, e2966. (<a href='https://doi.org/10.7717/peerj-cs.2966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallelization is needed everywhere, from laptops and mobile phones to supercomputers. Among parallel programming models, task-based programming has demonstrated a powerful potential and is widely used in high-performance scientific computing. Not only does it allow efficient parallelization across distributed heterogeneous computing nodes, but it also allows for elegant source code structuring by describing hardware-independent algorithms. In this article, we present Specx, a task-based runtime system written in modern C++. Specx supports distributed heterogeneous computing by simultaneously exploiting central processing units (CPUs) and graphics processing units (GPUs) (CUDA/HIP) and incorporating communication into the task graph. We describe the specificities of Specx and demonstrate its potential by running parallel applications.},
  archive      = {J_PEERJCS},
  author       = {Paul Cardosi and Bérenger Bramas},
  doi          = {10.7717/peerj-cs.2966},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2966},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Specx: A c++ task-based runtime system for heterogeneous distributed architectures},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Result assessment tool (RAT): Empowering search engine data analysis. <em>PEERJCS</em>, <em>11</em>, e2962. (<a href='https://doi.org/10.7717/peerj-cs.2962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Result Assessment Tool (RAT) is a Python-based software toolkit that enables researchers to analyze results from commercial search engines, social media platforms, and library search systems. RAT provides an integrated environment for designing studies, collecting results, and performing automated analysis. The software consists of two main modules: RAT Frontend and RAT Backend. RAT Frontend uses Flask to provide a researcher view for designing studies and an evaluation view for collecting ratings from study participants. RAT Backend includes modules for collecting search results, extracting source code, and adding classifiers for automated analysis. The system has been used in various studies, including search engine effectiveness studies, interactive information retrieval studies, and classification studies.},
  archive      = {J_PEERJCS},
  author       = {Sebastian Sünkler and Dirk Lewandowski and Sebastian Schultheiß and Nurce Yagci},
  doi          = {10.7717/peerj-cs.2962},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2962},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Result assessment tool (RAT): Empowering search engine data analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid extraction model for semantic knowledge discovery of water conservancy big data. <em>PEERJCS</em>, <em>11</em>, e2960. (<a href='https://doi.org/10.7717/peerj-cs.2960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the growing demand for efficient public opinion analysis in water conservancy and related domains, as well as the inefficiencies and limited scalability of existing automated web data extraction algorithms for multi-source datasets, this research integrates advanced technologies including big data analytics, natural language processing, and deep learning. A novel, transferable web information extraction model based on deep learning (WIEM-DL) is proposed, leveraging knowledge graphs, machine learning, and ontology-based methods. This model is designed to adapt to varying website structures, enabling effective cross-website information extraction. By refining water conservancy-related online public opinion content and extracting key feature information from critical sentences, the WIEM-DL model excels in locating main content while filtering out noise. This approach not only reduces processing time but also significantly improves extraction accuracy and efficiency. Furthermore, the model establishes methods for micro-level public opinion information extraction and feature representation, creating a fusion space for data-level integration. This serves as a robust foundation for multi-granularity semantic knowledge integration in public opinion big data. Experimental results demonstrate that the WIEM-DL model substantially outperforms traditional information extraction methods, setting a new benchmark for extraction performance.},
  archive      = {J_PEERJCS},
  author       = {Yanna Feng and Feng Zhang and Yongheng Zhang and Jiangang Dong and PengJu Wang},
  doi          = {10.7717/peerj-cs.2960},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2960},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid extraction model for semantic knowledge discovery of water conservancy big data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sepsis detection using deep learning and residual convolutional networks. <em>PEERJCS</em>, <em>11</em>, e2958. (<a href='https://doi.org/10.7717/peerj-cs.2958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is a life-threatening complication caused by infection that leads to extensive tissue damage. If not treated promptly, it can become fatal. Early identification and diagnosis of sepsis are critical to improving patient outcomes. Although recent technological advancements have aided sepsis detection, challenges remain in timely diagnosis using standard clinical practices. In this article, we present a new deep learning model to detect the occurrence of sepsis and the African vulture optimization algorithm (AVOA) to enhance the model performance. The system comprises four crucial steps: First, the enhanced convolutional learning framework (ECLF) with atrous convolutional and multi-level strategies that aim to learn high-level features from the nonlinear mapping of the medical data. Second is the spatio-channel attention network (SCAN), which has a neural architecture designed to focus on significant regions, such as spatial and channel regions, but not restricted to them. Third is the hierarchical dilated convolutional block (HDCB), which utilises a stacked dilated deep convolutional architecture for spatial feature context retrieval. Last is the residual path convolutional chain (RPCC), which uses a multi-residual convolutional approach for feature propagation, preserving important information. The sepsis detection model we bring forth involves many components, as mentioned above, and thus achieves a higher accuracy for timely intervention during sepsis. The combination of AVOA into the model ensures that it is robust and easily transferable, delivering high performance for adaptation to complicated structures inside medical datasets. The proposed model was evaluated on a clinical dataset and achieved outstanding performance, with an accuracy of 99.4%, precision of 98%, recall of 99.2%, F1-score of 99.0%, and an area under the curve (AUC) of 0.998. These results demonstrate the model’s superior ability to detect sepsis accurately and reliably, outperforming traditional clinical scoring methods and conventional machine learning approaches.},
  archive      = {J_PEERJCS},
  author       = {Ahmed S. Almasoud and Ghada Moh Samir Elhessewi and Munya A. Arasi and Abdulsamad Ebrahim Yahya and Menwa Alshammeri and Donia Badawood and Faisal Mohammed Nafie and Mohammed Assiri},
  doi          = {10.7717/peerj-cs.2958},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2958},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Efficient sepsis detection using deep learning and residual convolutional networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot cross-lingual stance detection via adversarial language adaptation. <em>PEERJCS</em>, <em>11</em>, e2955. (<a href='https://doi.org/10.7717/peerj-cs.2955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stance detection has been widely studied as the task of determining if a social media post is positive, negative or neutral towards a specific issue, such as support towards vaccines. Research in stance detection has often been limited to a single language and, where more than one language has been studied, research has focused on few-shot settings, overlooking the challenges of developing a zero-shot cross-lingual stance detection model. This article makes the first such effort by introducing a novel approach to zero-shot cross-lingual stance detection, multilingual translation-augmented bidirectional encoder representations from Transformers (BERT) (MTAB), aiming to enhance the performance of a cross-lingual classifier in the absence of explicit training data for target languages. Our technique employs translation augmentation to improve zero-shot performance and pairs it with adversarial learning to further boost model efficacy. Through experiments on datasets labeled for stance towards vaccines in four languages—English, German, French, Italian, we demonstrate the effectiveness of our proposed approach, showcasing improved results in comparison to a strong baseline model as well as ablated versions of our model. Our experiments demonstrate the effectiveness of model components, not least the translation-augmented data as well as the adversarial learning component, to the improved performance of the model. We have made our source code accessible on GitHub: https://github.com/amcs18pd05/MTAB-cross-lingual-vaccine-stance-detection-2.},
  archive      = {J_PEERJCS},
  author       = {Bharathi A. and Arkaitz Zubiaga},
  doi          = {10.7717/peerj-cs.2955},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2955},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Zero-shot cross-lingual stance detection via adversarial language adaptation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing the knowledge of artificial intelligence chatbots in pharmacology: Examples of two groups of drugs. <em>PEERJCS</em>, <em>11</em>, e2954. (<a href='https://doi.org/10.7717/peerj-cs.2954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectives The study aimed to evaluate eight artificial intelligence chatbots (ChatGPT-3.5, Microsoft Copilot, Gemini, You.com, Perplexity, Character.ai, Claude 3.5, and ChatRTX) in answering questions related to two pharmacological topics taught during the basic pharmacology curriculum for medical students: antifungal drugs and hypolipidemic drugs. Methods Chatbots’ performance was assessed by answering 60 single-choice questions on antifungal and hypolipidemic drugs topics. The questions were designed to have four answers (a, b, c, and d), and the artificial intelligence (AI) role was to choose the proper one. The assessment was performed twice with a 1-year hiatus to determine if artificial intelligence chatbots’ effectiveness changed over time. All the answers were checked for being right or wrong according to up-to-date pharmacology knowledge. To improve the clarity of results, to each score, a mark was assigned based on the grading system applied in our unit. Statistica software version 13.3 and Microsoft Excel 2010 were used for statistical analysis. Results In 2023, the best results on the subject of antifungal drugs were obtained by Gemini (formerly Bard) and on the topic of hypolipidemic drugs by You.com (formerly YouChat). In 2024 Microsoft Copilot answered correctly the highest number of questions in both topics. The total results of all artificial intelligence chatbots in 2023 and 2024 were compared using t-test for dependent samples. Statistical analysis revealed that artificial intelligence chatbots improved over time in both pharmacological topics, but this change was not statistically significant (p = 0.784 for antifungal drugs subject and p = 0.056 for hypolipidemic drugs). Conclusions The accuracy of AI chatbots’ responses regarding antifungal and hypolipidemic drugs improved over one year, though not significantly. None of the tested AI systems provided correct answers to all questions within these pharmacological fields.},
  archive      = {J_PEERJCS},
  author       = {Marcin Mateusz Granat and Aleksandra Paź and Dagmara Mirowska-Guzel},
  doi          = {10.7717/peerj-cs.2954},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2954},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Testing the knowledge of artificial intelligence chatbots in pharmacology: Examples of two groups of drugs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HybridFormer: A convolutional neural network-transformer architecture for low dose computed tomography image denoising. <em>PEERJCS</em>, <em>11</em>, e2952. (<a href='https://doi.org/10.7717/peerj-cs.2952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-dose computed tomography (CT) is a potent strategy to minimize X-ray radiation and its detrimental effects on patients. However, reducing radiation significantly boosts noise in reconstructed images, causing blur and obscuring critical tissue details. This obscurity poses significant challenges for doctors in making accurate diagnoses. Traditional techniques like sinogram domain filtration and iterative reconstruction algorithms require inaccessible raw data. Thus, this article introduces HybridFormer, a revolutionary image-denoising model utilizing the Residual Convolution-Swin Transformer Network, designed to enhance images while preserving vital details. Firstly, this algorithm constructs residual convolution for local feature extraction and Swin Transformer for global feature extraction, boosting denoising efficacy. Secondly, to address texture detail errors, we introduced a combined attention transformer unit (CATU) with a cross-channel attentive fusion layer (CCAFL), integrated with residual blocks to form a residual convolution and Swin Transformer Fusion Block (RSTB). Finally, using RSTB, we developed a deep feature refinement module (DFRM) to preserve image details. To avoid smoothing, we combined multi-scale perceptual loss from ResNet-50 with Charbonnier loss into a composite loss function. Validated on the AAPM2016 Mayo dataset, HybridFormer outperformed other state-of-the-art algorithms, achieving improvements of 0.02 dB, 0.16%, and 0.28% in PSNR, SSIM, and FSIM, respectively. Compared with other advanced algorithms, the proposed algorithm achieved the best performance indicators, confirming its superiority.},
  archive      = {J_PEERJCS},
  author       = {Shanaz Sharmin Jui and Zhitao Guo and Rending Jiang and Jiale Liu and Bohua Li},
  doi          = {10.7717/peerj-cs.2952},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2952},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HybridFormer: A convolutional neural network-transformer architecture for low dose computed tomography image denoising},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study on an efficient citrus huanglong disease detection algorithm based on three-channel aggregated attention. <em>PEERJCS</em>, <em>11</em>, e2943. (<a href='https://doi.org/10.7717/peerj-cs.2943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Aiming at the problems of complex and diverse field symptoms of citrus Huanglong disease (HLB), low efficiency and insufficient recognition accuracy of traditional detection methods, this study proposes an efficient detection algorithm based on improved You Only Look Once (YOLO)v8. Methods Firstly, a new character to float (C2f) Attention inverse residual moving block (IRMB) module is designed, which significantly enhances the model’s sensitivity to tiny disease features while reducing the number of parameters by fusing the lightweight IRMB with the adaptive attention gating mechanism, and solves the problem of losing key texture information due to downsampling in the traditional C2f module. Secondly, the three-channel aggregated attention module Powerneck is proposed in the Neck section, which realizes efficient cross-scale feature interactions, effectively suppresses background noise interference, and improves robustness in complex field scenes through SimFusion_4in feature alignment, information fusion module (IFM) global context fusion, and Power channel dynamic weighting strategy. In addition, the detection head design is optimized by structural reparameterization technique to further accelerate the inference process. Results The experimental results show that on the citrus dataset containing 12 diseases and two health states, the mAP50 of this model reaches 97% and the accuracy is 91.5%, which is 1.1% and 1.2% higher than that of the original YOLOv8, respectively, and the inference speed is improved by 14.6% to 370 frames per second (FPS). Comparison of the different models shows that the C2f Attention IRMB, through the mechanism of dual attention The comparison of different models shows that C2f Attention IRMB strengthens the feature expression ability through the dual-attention mechanism, and the Powerneck module reduces redundant computation through dynamic channel pruning, and the two synergistically optimize the model performance significantly. Compared with mainstream models such as YOLOv5m and YOLOv7x, this method is more advantageous in the balance of accuracy and speed, and can meet the demand of real-time detection in the field. Discussion The algorithm provides an efficient tool for early and accurate identification of citrus Huanglong disease, which is of great practical significance for reducing pesticide misuse and improving the efficiency of orchard management, and also provides new ideas for the design of lightweight target detection models in agricultural scenarios.},
  archive      = {J_PEERJCS},
  author       = {Yizong Wang and Zhengrong Xiao and Hong Wang and Fei Li and Jiya Tian},
  doi          = {10.7717/peerj-cs.2943},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2943},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A study on an efficient citrus huanglong disease detection algorithm based on three-channel aggregated attention},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cword2vec: A novel morphological rule-based word embedding approach for urdu text sentiment analysis. <em>PEERJCS</em>, <em>11</em>, e2937. (<a href='https://doi.org/10.7717/peerj-cs.2937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word embeddings are essential to natural language processing tasks because they contain a single word’s syntactic and semantic information. Word embeddings have been developed widely for numerous spoken languages across the globe like English. The research community needs to pay more attention to the Urdu language despite its significant number of speakers, which amounts to approximately 231.3 million individuals. Urdu is a complex language because word boundaries in Urdu are unspecified, as it does not employ delimiters between words. The compound word, a multiword expression, is a more complex word consisting of many strings or independent base words. Traditionally, compound words are identified during the word segmentation using bigram or trigram approaches. The challenge with these techniques is that they do not produce meaningful words. This study uses morphological rule-based compound words in Urdu text documents. For text representation, a self-trained morphological rule-based compound word embedding (Cword2vec) based on the word2vec model is proposed for Urdu text sentiment analysis. The performance of self-trained morphological rule-based compound word embedding was then evaluated using four well-known deep learning models, i.e., long short-term memory (LSTM), bidirectional LSTM (BiLSTM), convolutional neural networks (CNN), and convolutional LSTM (C-LSTM) for sentiment analysis. We also compare the performance of morphological rule-based compound words with traditional compound word identification techniques such as bigrams and trigrams. Regardless of the classification model, word embedding using our proposed morphological rule-based compound words outperformed in terms of precision, recall, F1 score, and accuracy than bigrams and trigrams.},
  archive      = {J_PEERJCS},
  author       = {Saquib Khushhal and Abdul Majid and Syed Ali Abass and Rabia Riaz and Mohammad Babar and Shafiq Ahmad},
  doi          = {10.7717/peerj-cs.2937},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2937},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Cword2vec: A novel morphological rule-based word embedding approach for urdu text sentiment analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness modeling for topics with different scales in short texts. <em>PEERJCS</em>, <em>11</em>, e2936. (<a href='https://doi.org/10.7717/peerj-cs.2936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of topic modeling to short texts is beset by challenges such as data sparsity and an absence of contextual information. Traditional research methods tend to prioritise high-attention and popular topics, frequently overlooking the identification of emerging topics. Consequently, subjects of a minor scale are prone to being overlooked during the topic identification process. Furthermore, in the context of topic modelling, information that varies in terms of the attention it receives is not treated equally. In order to address the aforementioned issues, a fairness-oriented topic discovery approach (MixTM-G) is proposed. This approach has been designed to facilitate the discovery of topics with different levels of attention. The proposed methodology involves the integration of normalized pointwise mutual information (NPMI) within a graph model to analyse text data. This approach leverages the correlation between data points to assess the semantic relationships between words, thus addressing the limitations posed by sparse data. The employment of graph algorithms facilitates the identification of semantically related clusters within the document graph, thereby enhancing the semantic associations between sparse data. Finally, a mixed topic modeling approach (MixTM), based on bi-grams and tri-grams combinations, is proposed to further improve topic discovery by strengthening the contextual relationships between words. The experimental results demonstrate the efficacy of the proposed method in topic modelling. In comparison to conventional methods, the proposed approach exhibits superior performance in detecting small-scale topics under equivalent conditions.},
  archive      = {J_PEERJCS},
  author       = {Chuangying Zhu and Yongyu Liang and Xinyuan Liang and Limiao Zhong and Fei Xie},
  doi          = {10.7717/peerj-cs.2936},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2936},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fairness modeling for topics with different scales in short texts},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opposition-based learning techniques in metaheuristics: Classification, comparison, and convergence analysis. <em>PEERJCS</em>, <em>11</em>, e2935. (<a href='https://doi.org/10.7717/peerj-cs.2935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, opposition-based learning (OBL) has emerged as a powerful enhancement strategy in metaheuristic algorithms (MAs), gaining significant attention for its potential to accelerate convergence and improve solution quality. Existing research lacks a structured analysis of how different OBL variants influence optimization performance when integrated into various MAs. This study categorizes and analyzes nine distinct OBL techniques: basic opposition-based learning, quasi-opposition-based learning, generalized opposition-based learning, current optimum opposition-based learning, quasi-reflection opposition-based learning, centroid opposition-based learning, random opposition-based learning, super opposition-based learning, and stochastic opposition-based learning. To systematically assess the effectiveness of these techniques, five widely used OBL variants—basic opposition-based learning, quasi-opposition-based learning, generalized opposition-based learning, current optimum opposition-based learning, quasi-reflection opposition-based learning—were selected for implementation within five well-established MAs: differential evolution, genetic algorithm, particle swarm optimization, artificial bee colony, and harmony search. These hybridized algorithms were evaluated across different integration phases, including the initialization passes and generation updates phase, and in both phases. To experimentally demonstrate the capability of OBL strategies to enhance MAs that face common issues such as slow convergence, limited exploration, and imbalanced exploration-exploitation, we have used 12 benchmark functions from CEC2022 suite. Key performance metrics—including maximum, minimum, mean, standard deviation, and convergence curves—were rigorously analyzed to quantify the improvements introduced by each OBL-enhanced MA. Additionally, a Friedman test was conducted to statistically validate the performance differences among the variants. The results indicate that quasi-reflection opposition-based learning consistently outperforms other OBL variants, demonstrating superior convergence speed and solution quality across most benchmark functions.},
  archive      = {J_PEERJCS},
  author       = {Rihab Lakbichi and Farouq Zitouni and Saad Harous and Aridj Ferhat and Abdelhadi Limane and Abdulaziz S. Almazyad and Guojiang Xiong and Ali Wagdy Mohamed},
  doi          = {10.7717/peerj-cs.2935},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2935},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Opposition-based learning techniques in metaheuristics: Classification, comparison, and convergence analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimising AI writing assessment using feedback and knowledge graph integration. <em>PEERJCS</em>, <em>11</em>, e2893. (<a href='https://doi.org/10.7717/peerj-cs.2893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the authors provide a novel framework for the effectiveness of AI writing assessment systems by embedding state-of-the-art deep learning networks, user feedback mechanisms, and knowledge graph frameworks. Most writing assessment tools cannot give personalized, detailed feedback. To tackle this problem, we employ writing assessment transformer models BERT and GPT-3, which allow exploring and scoring the writing on various features, including phrase structure, semantics, vocabulary usage, etc. In our system, we propose a dynamic relational knowledge graph that incorporates writing concepts and their relations, making it easier for the system to devise contextualized thesaurus-wise suggestions. The addition of graph neural networks (GNNs) empowers the model by boosting the GNN’s learning ability regarding the knowledge graph and improving comprehension of complex semantics. Additionally, we have included an iterative design whereby user feedback is collected, and the system adjusts the feedback given in light of historical feedback and changes in a user’s writing behavior over time. The system reconceptualizes the problem of user AI interaction by incorporating its dynamic nature and movement towards the known user and not vice-versa, achieving higher efficiency. To assess user satisfaction and improvements in the quality of the prepared texts, the authors conduct a series of user studies evaluating the efficiency of this integrated system. However, the preliminary data obtained from the task performance analysis show that the results of the proposed framework are far better than those of traditional methods, achieving a better level of engagement and feedback while performing the assessment. This study underscores the potential of deep learning, feedback, and knowledge graph integration in leveraging writing education. It can potentially reform learners’ capabilities, enabling them to write better and more effectively.},
  archive      = {J_PEERJCS},
  author       = {Ci Zhang},
  doi          = {10.7717/peerj-cs.2893},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2893},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimising AI writing assessment using feedback and knowledge graph integration},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Charting new territories: Fuzzy systems in english language teaching and learning. <em>PEERJCS</em>, <em>11</em>, e2887. (<a href='https://doi.org/10.7717/peerj-cs.2887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background In reality, the English language is a mystery; despite its inherent worth and the advantages of fluency, there is a pervasive impression that English instruction in secondary schools is of low quality, contributing to students’ lack of proficiency in the language in higher education and beyond. Pedagogical approaches persist in the classroom, topic after subject, including English. Analyzing texts in great depth via translation and emphasizing vocabulary are joint exercises in English classes. Students waste a lot of time copying things off the board in English classes despite the growing recognition of the significance of both listening and speaking effectively. Methods The Fuzzy Bayesian Intelligent Tutoring System (FB-ITS) is an artificial intelligence (AI) system that adaptively supports students in English teaching and learning settings. It is built in this experimental research employing AI methodologies based on fuzzy logic and the Bayesian network methodology. Using conventional approaches that rely primarily on numerical scores to evaluate academics’ teaching and research activities at various levels is becoming increasingly challenging. Expert systems based on fuzzy logic, suggested in this study, can handle teacher and student evaluations even when faced with imprecise information and uncertainty; this is necessary since academic performance is being indexed in multiple international databases using impact indices at different scales. Results The results showed that, on average, students using the FB-ITS took less time to complete the post-test than students using the conventional e-learning system. This research proposes an English teaching and learning approach that has been very successful based on experimental findings of related big data clustering algorithms. The assessment accuracy has risen by 4%, and the teaching resource utilization rate has been increased by 5%.},
  archive      = {J_PEERJCS},
  author       = {Xiaomei Wen and Deng Pan},
  doi          = {10.7717/peerj-cs.2887},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2887},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Charting new territories: Fuzzy systems in english language teaching and learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSLDV-hop: A robust localization algorithm for WSN using PSO and refinement process. <em>PEERJCS</em>, <em>11</em>, e2770. (<a href='https://doi.org/10.7717/peerj-cs.2770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various areas, wireless sensor networks (WSNs) are popular for achieving goals related to security in buildings when there is fire, in military areas to know the position of terrorists in moles and to observe the behavior of animals in forest areas. All these objectives can be achieved only when the position of the sensor is known to the base station, which helps to achieve the appropriate action in unwanted situations. The controlling point is the base station, which would be able to take action only in case the correct position of the unwanted event is known to the base station. Researches have designed various localization/positioning approaches but still have some challenges related to the accuracy of sensor nodes in localization. Distance vector hop is a popular localization algorithm. Its dependence on the estimated average size of a hop results in a significant localization error. This work suggests an improved algorithm combining a refinement procedure with particle swarm optimization, called DVHOP-PSO. This improved algorithm, called PSLDV-Hop, uses exact anchor sensor node coordinates and fractional hop count information to correct estimated distances. By utilizing an improved iterative evolution algorithm, the PSLDV-Hop algorithm reduces localization errors by achieving a higher degree of accuracy in node localization. Simulation results demonstrate their superiority over other classical improved algorithms and the original distance vector hop. The simulation of this approach is done using the MATLAB tool by considering different parameters such as the number of anchor nodes, number of sensor nodes, area, and range of sensor nodes. Integrating particle swarm optimization with distance vector hop, the proposed localization algorithm consistently outperforms conventional methods, showcasing significant percentage improvements . The suggested algorithm consistently performs better than all other approaches at ranges 20 and 40. Overall, the suggested method performs noticeably better than distance vector hop at range 40, especially when range grows by up to 65%. Additionally, across communication ranges of 20, 30, and 40 units, the proposed algorithm consistently outshines PSO-DV-Hop and GA-DV-Hop, exhibiting notable percentage improvements in localization accuracy.},
  archive      = {J_PEERJCS},
  author       = {Bhupinder Kaur and Deepak Prashar and Arfat Ahmad Khan and Seifedine Kadry and Jungeun Kim},
  doi          = {10.7717/peerj-cs.2770},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2770},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {PSLDV-hop: A robust localization algorithm for WSN using PSO and refinement process},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable credit risk assessment model with boundary sample identification. <em>PEERJCS</em>, <em>11</em>, e2988. (<a href='https://doi.org/10.7717/peerj-cs.2988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Interpretability is a key requirement for ensuring that credit risk assessment models are trustworthy and compliant with regulatory standards. Simultaneously, effectively distinguishing between noise samples and boundary samples is crucial for improving the accuracy of credit risk predictions. Methods This article introduces a novel credit risk assessment model, Interpretable Credit Risk Assessment Model with Identifying Boundary Samples (IAIBS). The model begins with a logistic regression sub-model that offers strong self-interpretable features. For samples that are not correctly classified, the Attribute Recognition and Perception based on the Distribution of neighboring sample features (ARPD) algorithm is applied to filter out noisy samples and identify boundary samples. A deep learning sub-model is then trained to deeply learn the risk features of these boundary samples. Finally, representative features of all samples are extracted using agglomerative clustering, and the most suitable sub-model is selected for prediction based on the similarity between each sample and the cluster centers. Results Experimental results on four public datasets demonstrate that the IAIBS model significantly outperforms 11 baseline models, as confirmed by the Nemenyi test. The model achieved area under the curve (AUC) scores of 89.17, 79.86, 97.48, and 66.03 on the PCL, FICO, CCF, and VL datasets, respectively. With appropriate parameter tuning, the IAIBS model maintains strong generalization ability, and each module contributes positively to overall performance. Additionally, the IAIBS model effectively interprets key predictors and prediction outcomes.},
  archive      = {J_PEERJCS},
  author       = {Runchi Zhang and Iris Li and Zhiyuan Ding},
  doi          = {10.7717/peerj-cs.2988},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2988},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An interpretable credit risk assessment model with boundary sample identification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep context-attentive transformer transfer learning for financial forecasting. <em>PEERJCS</em>, <em>11</em>, e2983. (<a href='https://doi.org/10.7717/peerj-cs.2983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents 2CAT (CNN-Correlation-based Attention Transformer), a deep learning model for financial time-series forecasting. The model integrates signal decomposition, convolutional layers, and correlation-based attention mechanisms to capture temporal patterns. A transfer learning framework is incorporated to enhance generalization across markets through pretraining, encoder freezing, and fine-tuning. Evaluation on six stock indices—Dow Jones Industrial Average (DJIA), Nikkei 225 (N225), Hang Seng Index (HSI), Shanghai Stock Exchange (SSE), Bombay Stock Exchange (BSE), and the Stock Exchange of Thailand (SET)—demonstrates strong predictive accuracy. On DJIA, 2CAT records an MSE of 0.0655, MAE of 0.2023, and R2 of 0.9169, outperforming Deep-Transformer, which yields an MSE of 0.1360 and R2 of 0.8274. The SET index, which posed challenges for previous models, demonstrates notable improvement with 2CAT, achieving an R2 of 0.9094. Wilcoxon signed-rank test confirms statistically significant gains in non-transfer learning scenarios at the 0.05 level. Transfer learning experiments reveal statistically significant improvements, reinforcing the feasibility of cross-market knowledge transfer. An ablation study highlights the impact of architectural refinements and rotary positional encoding, while prediction horizon analysis confirms stable forecasting performance. These results establish 2CAT as a robust financial forecasting framework adaptable to diverse market conditions.},
  archive      = {J_PEERJCS},
  author       = {Ling Feng and Ananta Sinchai},
  doi          = {10.7717/peerj-cs.2983},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2983},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep context-attentive transformer transfer learning for financial forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMAD-LDS: Enhanced secure message authentication and dissemination with lightweight digital signature in the internet of vehicles. <em>PEERJCS</em>, <em>11</em>, e2982. (<a href='https://doi.org/10.7717/peerj-cs.2982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles (IoV) plays a crucial role in enhancing driver experience and road safety by enabling vehicles to share real-time traffic information. However, the nature of the IoV communication exposes vehicles to potential security risks. Therefore, robust authentication mechanisms are essential to prevent malicious vehicles from disseminating misleading information. Ensuring secure communication in IoV necessitates addressing critical security and privacy requirements, including entity and data authentication, privacy preservation, and confidentiality. Recent research in this area frequently encounters challenges related to high computation and communication overheads, as well as vulnerability to diverse security threats. Consequently, this article proposes secure message authentication and dissemination with lightweight digital signature (SMAD-LDS), a robust message authentication and dissemination scheme incorporating a novel lightweight digital signature technique designed to minimize these overheads. There are four main phases in the proposed SMAD-LDS scheme such as the initiation phase, registration phase, send/receive message phase, and key updating phase. For the registration phase, our results demonstrate that the computation cost in SMAD-LDS is reduced by at least 46.8% compared to other works in the literature. Moreover, the computation overhead in the send/receive messages phase in SMAD-LDS is reduced by at least 94%. Additionally, the overall cost of communications in the proposed article has improved. The cost of communications in the registration phase and send/receive message phase has improved by 28% and 70%, respectively, compared to other works. Furthermore, SMAD-LDS is resistant to known attacks such as modification attacks, impersonation attacks, eavesdropping attacks, fake roadside unit (RSU) attacks, traceability attacks, and replay attacks.},
  archive      = {J_PEERJCS},
  author       = {Islam Z. Ahmed and Yasser Hifny and Rowayda A. Sadek},
  doi          = {10.7717/peerj-cs.2982},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2982},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SMAD-LDS: Enhanced secure message authentication and dissemination with lightweight digital signature in the internet of vehicles},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly detection method for power system information based on multimodal data. <em>PEERJCS</em>, <em>11</em>, e2976. (<a href='https://doi.org/10.7717/peerj-cs.2976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of modern power systems, effective anomaly detection is essential to ensure operational security. Conventional methods often depend on single-domain data, which limits their ability to fully capture the dynamic behavior of power systems. This study introduces a novel multimodal approach that integrates time-domain and frequency-domain data to improve anomaly detection accuracy and robustness. By leveraging this integration, our method captures both temporal patterns and spectral signatures, offering a more comprehensive analysis of system behavior—an advancement that significantly enhances detection performance compared to traditional techniques. Experimental results show that our approach achieves a detection accuracy of 97.6%, outperforming baseline methods. Beyond its technical merits, this method has practical implications for real-world power systems, enabling early identification of security threats, improving system reliability, and reducing the risk of operational failures. These findings contribute to the field of power system security and provide a versatile framework for anomaly detection in critical infrastructures.},
  archive      = {J_PEERJCS},
  author       = {Liyue Chen and XuXiang Zhou and Peng Zhou and Xin Sun and SenSen Zheng},
  doi          = {10.7717/peerj-cs.2976},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2976},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Anomaly detection method for power system information based on multimodal data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IDILI-MT: Identifying drug-induced liver injury compounds with a multi-head transformer. <em>PEERJCS</em>, <em>11</em>, e2973. (<a href='https://doi.org/10.7717/peerj-cs.2973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug-induced liver injury (DILI) is a leading cause of late-stage drug attrition and post-approval withdrawals, making early in silico risk assessment essential for drug safety. We present iDILI-MT (identifying drug-induced liver injury compounds with a multi-head Transformer), a self-contained computational framework that integrates a feed-forward network for sequential feature extraction, a multi-head Transformer encoder for contextual representation learning, and a squeeze-and-excitation attention module for channel-wise feature recalibration. Evaluated on a curated set of 1,919 small-molecule compounds, iDILI-MT outperformed traditional machine-learning classifiers and state-of-the-art graph neural networks, achieving a mean area under the receiver-operating-characteristic curve (AUC-ROC) of 0.8499, area under the precision-recall curve (AUC-PR) of 0.8905, and F1 score of 0.8173 across ten trials. Attention-weight analysis reveals that the combined multi-head and squeeze-and-excitation attention mechanisms effectively highlight key substructural and chemical motifs associated with hepatotoxicity. These findings indicate that iDILI-MT provides an useful method for detecting compounds at risk of DILI, potentially accelerating safety assessments in drug development.},
  archive      = {J_PEERJCS},
  author       = {Wanrong Zheng and Fobao Lai},
  doi          = {10.7717/peerj-cs.2973},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2973},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IDILI-MT: Identifying drug-induced liver injury compounds with a multi-head transformer},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gated attention based generative adversarial networks for imbalanced credit card fraud detection. <em>PEERJCS</em>, <em>11</em>, e2972. (<a href='https://doi.org/10.7717/peerj-cs.2972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit card fraud detection is highly important to maintain financial security. However, it is challenging to train suitable models due to the class imbalance in credit card transaction data. To address this issue, this work proposes a novel deep learning framework, gated attention-based generative adversarial networks (GA-GAN) for credit card fraud detection in class-imbalanced data. GA-GAN integrates GAN and the gated attention mechanism to generate high-quality synthetic data that realistically simulates fraudulent behaviors. Experimental results on two public credit card datasets demonstrate that GA-GAN outperforms state-of-the-art methods on credit card fraud detection tasks in class-imbalanced data, indicating the advantage of GA-GAN. The code is publicly available at https://github.com/Gejiangmeng/gagan/tree/main.},
  archive      = {J_PEERJCS},
  author       = {Jiangmeng Ge and Lanxiang Yin and Shiqing Zhang and Xiaoming Zhao},
  doi          = {10.7717/peerj-cs.2972},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2972},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Gated attention based generative adversarial networks for imbalanced credit card fraud detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear B-cell epitope prediction for SARS and COVID-19 vaccine design: Integrating balanced ensemble learning models and resampling strategies. <em>PEERJCS</em>, <em>11</em>, e2970. (<a href='https://doi.org/10.7717/peerj-cs.2970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a comprehensive machine learning framework to enhance the prediction accuracy of B-cell epitopes and antibody recognition related to Severe Acute Respiratory Syndrome (SARS) and Coronavirus Disease 2019 (COVID-19). To address the issue of data imbalance, various resampling techniques were applied using three types of strategies: over-sampling, under-sampling, and hybrid-sampling. The implemented resampling methods were designed to improve class balance and enhance model training. The rebalanced datasets were then used in model building with ensemble classifiers employing Boosting, Bagging, and Balancing strategies. Hyperparameter optimization for the classifiers was conducted using GridSearchCV, while feature selection was performed with the recursive feature elimination (RFE) algorithm. Model performance was evaluated using seven different metrics: Accuracy, Precision, Recall, F1-score, receiver operating characteristic area under the curve (ROC AUC), precision recall area under the curve (PR AUC), and Matthews correlation coefficient (MCC). Furthermore, statistical significance analyses including paired t-test, Wilcoxon, and permutation tests confirmed the reliability of the model improvements. To interpret the model’s predictive behavior, peptides with the highest confidence among correctly classified instances were identified as potential epitope candidates. The results indicate that the combination of Synthetic Minority Over-Sampling Technique—Edited Nearest Neighbors (SMOTE-ENN), and ExtraTrees yielded the best performance, achieving an ROC AUC score of 0.9899. The combination of Instance Hardness Threshold (IHT) and ExtraTrees followed closely with a score of 0.9799. These findings emphasize the effectiveness of integrating resampling models and balancing ensemble classifiers in improving the accuracy of B-cell epitope prediction and antibody recognition for SARS and COVID-19 infections. This study contributes to vaccine development efforts and the advancement of immunoinformatics research by identifying promising epitope candidates.},
  archive      = {J_PEERJCS},
  author       = {Fatih Gurcan},
  doi          = {10.7717/peerj-cs.2970},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2970},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Linear B-cell epitope prediction for SARS and COVID-19 vaccine design: Integrating balanced ensemble learning models and resampling strategies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSKT: Multimodal data fusion for improved nursing management in hemorrhagic stroke. <em>PEERJCS</em>, <em>11</em>, e2969. (<a href='https://doi.org/10.7717/peerj-cs.2969'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The study aims to address the challenges of nursing decision-making and the optimization of personalized nursing plans in the management of hemorrhagic stroke. Due to the rapid progression and high complexity of hemorrhagic stroke, traditional nursing methods struggle to cope with the challenges posed by its high incidence and high disability rate. Methods To address this, we propose an innovative approach based on multimodal data fusion and a non-stationary Gaussian process model. Utilizing multidimensional data from the MIMIC-IV database (including patient medical history, nursing records, laboratory test results, etc.), we developed a hybrid predictive model with a multiscale kernel transformer non-stationary Gaussian process (MSKT-NSGP) architecture to handle non-stationary time-series data and capture the dynamic changes in a patient’s condition. Results The proposed MSKT-NSGP model outperformed traditional algorithms in prediction accuracy, computational efficiency, and uncertainty handling. For hematoma expansion prediction, it achieved 85.5% accuracy, an area under the curve (AUC) of 0.87, and reduced mean squared error (MSE) by 18% compared to the sparse variational Gaussian process (SVGP). With an inference speed of 55 milliseconds per sample, it supports real-time predictions. The model maintained a confidence interval coverage near 95% with narrower widths, indicating precise uncertainty estimation. These results highlight its potential to enhance nursing decision-making, optimize personalized plans, and improve patient outcomes.},
  archive      = {J_PEERJCS},
  author       = {Ting Zhou and Dandan Li and Jingfang Zuo and Aihua Gu and Li Zhao},
  doi          = {10.7717/peerj-cs.2969},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2969},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MSKT: Multimodal data fusion for improved nursing management in hemorrhagic stroke},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evolutionary bi-LSTM-DQN framework for enhanced recognition and classification in rural information management. <em>PEERJCS</em>, <em>11</em>, e2967. (<a href='https://doi.org/10.7717/peerj-cs.2967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep learning and reinforcement learning technologies advance, intelligent rural information management is transforming substantially. This article presents an innovative framework, the evolutionary bidirectional long short-term memory deep Q-network (EBLM-DQN), which integrates evolutionary algorithms, reinforcement learning, and bidirectional long short-term memory (Bi-LSTM) networks to significantly improve the accuracy and efficiency of rural information management, particularly for recognizing and classifying information relevant to farmers. The proposed framework begins with data preprocessing using disambiguation techniques and data complementation, followed by temporal feature extraction via a Bi-LSTM layer. It then employs a deep Q-network (DQN) to adjust and optimize weights dynamically. After feature extraction and weight optimization, evolutionary algorithms are used to select the optimal weights, enabling precise recognition and classification of conditions encountered by farmers seeking assistance. Experimental results indicate that the EBLM-DQN framework outperforms existing frameworks on public datasets and real-world applications, providing higher classification accuracy. This framework offers valuable technical support and a reference for future optimization and development of rural information management systems.},
  archive      = {J_PEERJCS},
  author       = {Taiping Deng and Xi He and Jiao Li and Feifei Ye and Jingyang Tang},
  doi          = {10.7717/peerj-cs.2967},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2967},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An evolutionary bi-LSTM-DQN framework for enhanced recognition and classification in rural information management},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A methodological approach for inferring causal relationships from opinions and news-derived events with an application to climate change. <em>PEERJCS</em>, <em>11</em>, e2964. (<a href='https://doi.org/10.7717/peerj-cs.2964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms like Twitter (now X) provide a global forum for discussing ideas. In this work, we propose a novel methodology for detecting causal relationships in online discourse. Our approach integrates multiple causal inference techniques to analyze how public sentiment and discourse evolve in response to key events and influential figures, using five causal detection methods: Direct-LiNGAM, PC, PCMCI, VAR, and stochastic causality. The datasets contain variables, such as different topics, sentiments, and real-world events, among which we seek to detect causal relationships at different frequencies. The proposed methodology is applied to climate change opinions and data, offering insights into the causal relationships among public sentiment, specific topics, and natural disasters. This approach provides a framework for analyzing various causal questions. In the specific case of climate change, we can hypothesize that a surge in discussions on a specific topic consistently precedes a change in overall sentiment, level of aggressiveness, or the proportion of users expressing certain stances. We can also conjecture that real-world events, like natural disasters and the rise to power of politicians leaning towards climate change denial, may have a noticeable impact on the discussion on social media. We illustrate how the proposed methodology can be applied to examine these questions by combining datasets on tweets and climate disasters.},
  archive      = {J_PEERJCS},
  author       = {Juan Marten and Fernando Delbianco and Fernando Tohme and Ana G. Maguitman},
  doi          = {10.7717/peerj-cs.2964},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2964},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A methodological approach for inferring causal relationships from opinions and news-derived events with an application to climate change},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tencent meeting forensics based on memory reverse analysis. <em>PEERJCS</em>, <em>11</em>, e2963. (<a href='https://doi.org/10.7717/peerj-cs.2963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tencent Meeting, an instant meeting software, is widely used at present, but no research has been conducted on its forensics. Since the real-time data generated by such software during meetings will not be stored in the computer disk, the traditional disk forensics method against such software is no longer applicable and needs to obtain evidence through memory analysis. To extract meeting data transmitted during meetings, this article proposes a method for Tencent Meeting forensics based on memory reverse analysis. First, by analyzing the process storage and metadata format of Tencent Meeting in memory, an inverse metadata extraction algorithm is designed. Then, by analyzing the data structure of Tencent Meeting in memory, a meeting data stream engraving algorithm is developed. Finally, the experimental results indicate that the proposed method can effectively extract metadata information such as meeting time, meeting number, topic, and data flow information such as participants, message records, as well as transmitted files from the memory of Tencent Meeting, providing crucial digital evidence for digital crime investigation. Compared with other forensic analysis methods for instant meeting software, our proposed forensic method for Tencent Meeting conducts memory reverse analysis with the entire memory file, enabling the extraction of more comprehensive and abundant forensic data.},
  archive      = {J_PEERJCS},
  author       = {Shilong Yu and Binglong Li and Lin Zhu and Heyu Zhang and Sen Yang and Zhangxiao Li and Wenzheng Feng},
  doi          = {10.7717/peerj-cs.2963},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2963},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Tencent meeting forensics based on memory reverse analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The geometry of meaning: Evaluating sentence embeddings from diverse transformer-based models for natural language inference. <em>PEERJCS</em>, <em>11</em>, e2957. (<a href='https://doi.org/10.7717/peerj-cs.2957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language inference (NLI) is a fundamental task in natural language processing that focuses on determining the relationship between pairs of sentences. In this article, we present a simple and straightforward approach to evaluate the effectiveness of various transformer-based models such as bidirectional encoder representations from transformers (BERT), Generative Pre-trained Transformer (GPT), robustly optimized BERT approach (RoBERTa), and XLNet in generating sentence embeddings for NLI. We conduct comprehensive experiments with different pooling techniques and evaluate the embeddings using different norms across multiple layers of each model. Our results demonstrate that the choice of pooling strategy, norm, and model layer significantly impacts the performance of NLI, with the best results achieved using max pooling and the L2 norm across specific model layers. On the Stanford Natural Language Inference (SNLI) dataset, the model reached 90% accuracy and 86% F1-score, while on the MedNLI dataset, the highest F1-score recorded was 84%. This article provides insights into how different models and evaluation strategies can be effectively combined to improve the understanding and classification of sentence relationships in NLI tasks.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Alsuhaibani},
  doi          = {10.7717/peerj-cs.2957},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2957},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The geometry of meaning: Evaluating sentence embeddings from diverse transformer-based models for natural language inference},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The accuracy-bias trade-offs in AI text detection tools and their impact on fairness in scholarly publication. <em>PEERJCS</em>, <em>11</em>, e2953. (<a href='https://doi.org/10.7717/peerj-cs.2953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) text detection tools are considered a means of preserving the integrity of scholarly publication by identifying whether a text is written by humans or generated by AI. This study evaluates three popular tools (GPTZero, ZeroGPT, and DetectGPT) through two experiments: first, distinguishing human-written abstracts from those generated by ChatGPT o1 and Gemini 2.0 Pro Experimental; second, evaluating AI-assisted abstracts where the original text has been enhanced by these large language models (LLMs) to improve readability. Results reveal notable trade-offs in accuracy and bias, disproportionately affecting non-native speakers and certain disciplines. This study highlights the limitations of detection-focused approaches and advocates a shift toward ethical, responsible, and transparent use of LLMs in scholarly publication.},
  archive      = {J_PEERJCS},
  author       = {Ahmad R. Pratama},
  doi          = {10.7717/peerj-cs.2953},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2953},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The accuracy-bias trade-offs in AI text detection tools and their impact on fairness in scholarly publication},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced recurrent attention-deep q learning with optimal node constrains and effective penalty based model for data transmission scheduling on wireless sensor networks. <em>PEERJCS</em>, <em>11</em>, e2950. (<a href='https://doi.org/10.7717/peerj-cs.2950'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective scheduling of data transmission is critical to maximizing network performance and resource usage in the context of wireless sensor networks (WSNs). In order to improve the effectiveness of data transmission scheduling in wireless sensor networks (WSNs), this paper proposes a unique method called Recurrent Attention-Deep Q Learning with Optimal Node Constraints and Effective Penalty based WSN Scheduling (RA-DQL-ONC&EP). This technique performs dynamic scheduling of data transmission tasks considering energy consumption and network interference by combining a penalty-based model, optimal node limitations, and recurrent attention techniques. Simulation results show that the proposed approach performs remarkably well. With a 91.21% success rate, it also guarantees dependable data transport throughout the network. Additionally, the delay rate is reduced to 1.99%, demonstrating effective data transfer with low latency. It is an effective model, yet it uses 70% less energy than other models since it is energy-efficient. The algorithm’s performance is further demonstrated by throughput analysis, which shows a 72% throughput over 1,000 time steps. Based on enhanced reliability, efficiency, and energy conservation in network operations, our results highlight the potential of RA-DQL-ONC&EP as a promising approach for improving data transmission scheduling in WSNs. This optimized scheduling enhances network reliability, ensuring timely and accurate data delivery, which can support various applications such as environmental monitoring, healthcare systems, and smart city infrastructure, ultimately fostering societal well-being and progress. Additionally, the algorithm’s efficiency contributes to cost savings and resource conservation, making it a socially responsible choice for managing wireless sensor networks.},
  archive      = {J_PEERJCS},
  author       = {D.R. Anita Sofia Liz and Yesubai Rubavathi C},
  doi          = {10.7717/peerj-cs.2950},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2950},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced recurrent attention-deep q learning with optimal node constrains and effective penalty based model for data transmission scheduling on wireless sensor networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized customer churn prediction using tabular generative adversarial network (GAN)-based hybrid sampling method and cost-sensitive learning. <em>PEERJCS</em>, <em>11</em>, e2949. (<a href='https://doi.org/10.7717/peerj-cs.2949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Imbalanced and overlapped data in customer churn prediction significantly impact classification results. Various sampling and hybrid sampling methods have demonstrated effectiveness in addressing these issues. However, these methods have not performed well with classical machine learning algorithms. Methods To optimize the performance of classical machine learning on customer churn prediction tasks, this study introduces an extension framework called CostLearnGAN, a tabular generative adversarial network (GAN)-hybrid sampling method, and cost-sensitive Learning. Utilizing a cost-sensitive learning perspective, this research aims to enhance the performance of several classical machine learning algorithms in customer churn prediction tasks. Based on the experimental results classical machine learning algorithms exhibit shorter execution times, making them suitable for predicting churn in large customer bases. Results This study conducted an experiment with six comparative sampling methods, six datasets, and three machine learning algorithms. The results show that CostLearnGAN achieved a satisfying result across all evaluation metrics with a 1.44 average mean rank score. Additionally, this study provided a robustness measurement for algorithms, demonstrating that CostLearnGAN outperforms other sampling methods in improving the performance of classical machine learning models with a 5.68 robustness value on average.},
  archive      = {J_PEERJCS},
  author       = {I Nyoman Mahayasa Adiputra and Paweena Wanchai and Pei-Chun Lin},
  doi          = {10.7717/peerj-cs.2949},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2949},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimized customer churn prediction using tabular generative adversarial network (GAN)-based hybrid sampling method and cost-sensitive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IAESR: IoT-oriented authenticated encryption based on iShadow round function. <em>PEERJCS</em>, <em>11</em>, e2947. (<a href='https://doi.org/10.7717/peerj-cs.2947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing popularity of the Internet of Things (IoT) devices and the widespread application of embedded systems, the demand for security and resource efficiency in these devices is also increasing. Traditional authenticated encryption (AE) algorithms are often unsuitable for lightweight devices due to their complexity and resource consumption, creating a need for lightweight AE algorithms. Lightweight devices typically have limited processing power, storage capacity, and energy resources, which necessitates the design of simple and efficient encryption algorithms that can function within these constraints. Despite these resource limitations, security remains of paramount importance. Therefore, lightweight AE algorithms must minimize resource consumption while ensuring adequate security. This article presents a theoretical lightweight AE scheme based on Shadow, a lightweight block encryption algorithm, to address the requirements for secure communication in resource-constrained environments. The scheme first enhances the Shadow algorithm by introducing the improved Shadow (iShadow) algorithm. It then combines this with the duplex sponge structure to propose the IoT-oriented authenticated encryption based on the iShadow round function (IAESR). The integration of iShadow with the duplex sponge structure achieves a balance between security and efficiency through three key mechanisms: (1) The sponge’s capacity (64/128-b for IAESR-32/64) provides provable indistinguishability under chosen-plaintext attack (IND-CPA) and chosen-ciphertext attack (IND-CCA) security bounds, effectively resisting generic attacks with an adversarial advantage limited to O(q2/2c); (2) the duplex mode’s single-pass processing reduces memory overhead by reusing the permutation state; and (3) iShadow’s ARX operations reduce energy consumption to 0.4–0.5 µJ/byte on 32-b microcontrollers, outperforming AES-GCM by 20–30%. Empirical tests on an Intel i5-1035G1 CPU demonstrate stable execution times. This design ensures the security and integrity of communication while balancing efficiency, and resource utilization. This design ensures IND-CCA secure confidentiality and integrity against plaintext (INT-PTXT), as demonstrated by the security bounds of the sponge construction. Specifically, IAESR guarantees both confidentiality and authenticity. Additionally, it is particularly well-suited for scenarios with lightweight requirements, such as those found in the IoT.},
  archive      = {J_PEERJCS},
  author       = {Yanshuo Zhang and Liqiu Li and Hengyu Bao and Xiaohong Qin and Zhiyuan Zhang and Xiaoyi Duan},
  doi          = {10.7717/peerj-cs.2947},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2947},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IAESR: IoT-oriented authenticated encryption based on iShadow round function},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A malware detection method with function parameters encoding and function dependency modeling. <em>PEERJCS</em>, <em>11</em>, e2946. (<a href='https://doi.org/10.7717/peerj-cs.2946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As computers are widely used in people’s work and daily lives, malware has become an increasing threat to network security. Although researchers have introduced traditional machine learning and deep learning methods to conduct extensive research on functions in malware detection, these methods have largely ignored the analysis of function parameters and functional dependencies. To address these limitations, we propose a new malware detection method. Specifically, we first design a parameter encoder to convert various types of function parameters into feature vectors, and then discretize various parameter features through clustering methods to enhance the representation of API encoding. Additionally, we design a deep neural network to capture functional dependencies, enabling the generation of robust semantic representations of function sequences. Experiments on a large-scale malware detection dataset demonstrate that our method outperforms other techniques, achieving 98.62% accuracy and a 98.40% F1-score. Furthermore, the results of ablation experiments show the important role of function parameters and functional dependencies in malware detection.},
  archive      = {J_PEERJCS},
  author       = {Ronghao Hou and Dongjie Liu and Xiaobo Jin and Jian Weng and Guanggang Geng},
  doi          = {10.7717/peerj-cs.2946},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2946},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A malware detection method with function parameters encoding and function dependency modeling},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable multi-transformer ensemble for text-based movie genre classification. <em>PEERJCS</em>, <em>11</em>, e2945. (<a href='https://doi.org/10.7717/peerj-cs.2945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label movie genre classification is challenging due to the inherent ambiguity and overlap between different genres. Most of the existing works in genre classification use audio-visual modalities. The potential of text-based modalities in movie genre classification is still underexplored. This paper proposes an ensemble deep-learning model that uses movie plots to predict movie genres. After pre-processing the text plots, three transformer-based models, Bidirectional Encoder Representations from Transformers (BERT), DistilBERT, and Robustly Optimized BERT Pre-training Approach (ROBERTa), are used to generate genre predictions, combined through a weighted soft-voting method. The proposed ensemble architecture achieves state-of-the-art performance on two benchmark datasets, Trailers12K and LMTD9, with a micro-average precision of 80.10% and 80.37%, respectively, significantly outperforming both traditional machine learning approaches and advanced deep learning models. The ensemble’s superior performance is attributed to its ability to combine the diverse strengths of individual models and capture nuanced genre-specific information from textual features. The lack of interpretability in deep learning models for genre classification is addressed using Local Interpretable Model-Agnostic Explanations (LIME), which provides both local and global explanations for the model’s predictions. The findings of the study highlight the potential of textual data in automated genre classification and emphasize the importance of interpretability methods in multi-label genre classification.},
  archive      = {J_PEERJCS},
  author       = {Faheem Shaukat and Naveed Ejaz and Zeeshan Ashraf and Mrim M. Alnfiai and Nouf Nawar Alotaibi and Salma Mohsen M. Alnefaie},
  doi          = {10.7717/peerj-cs.2945},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2945},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An interpretable multi-transformer ensemble for text-based movie genre classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new approach of anomaly detection in shopping center surveillance videos for theft prevention based on RLCNN model. <em>PEERJCS</em>, <em>11</em>, e2944. (<a href='https://doi.org/10.7717/peerj-cs.2944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of video data produced daily by today’s surveillance systems is enormous, making analysis difficult for computer vision specialists. It is challenging to continuously search these massive video streams for unexpected accidents because they occur seldom and have little chance of being observed. Contrarily, deep learning-based anomaly detection decreases the need for human labor and has comparably trustworthy decision-making capabilities, hence promoting public safety. In this article, we introduce a system for efficient anomaly detection that can function in surveillance networks with a modest level of complexity. The proposed method starts by obtaining spatiotemporal features from a group of frames. The multi-layer extended short-term memory model can precisely identify continuing unusual activity in complicated video scenarios of a busy shopping mall once we transmit the in-depth features extracted. We conducted in-depth tests on numerous benchmark datasets for anomaly detection to confirm the proposed framework’s functionality in challenging surveillance scenarios. Compared to state-of-the-art techniques, our datasets, UCF50, UCF101, UCFYouTube, and UCFCustomized, provided better training and increased accuracy. Our model was trained for more classes than usual, and when the proposed model, RLCNN, was tested for those classes, the results were encouraging. All of our datasets worked admirably. However, when we used the UCFCustomized and UCFYouTube datasets compared to other UCF datasets, we achieved greater accuracy of 96 and 97, respectively.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Sajid and Ali Haider Khan and Kaleem Razzaq Malik and Javed Ali Khan and Ayed Alwadain},
  doi          = {10.7717/peerj-cs.2944},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2944},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A new approach of anomaly detection in shopping center surveillance videos for theft prevention based on RLCNN model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topic adversarial neural network for cross-topic cyberbullying detection. <em>PEERJCS</em>, <em>11</em>, e2942. (<a href='https://doi.org/10.7717/peerj-cs.2942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of social media, cyberbullying has emerged as a pervasive threat, causing significant psychological harm to individuals and undermining social cohesion. Its linguistic expressions vary widely across topics, complicating automatic detection efforts. Most existing methods struggle to generalize across diverse online contexts due to their reliance on topic-specific features. To address this issue, we propose the Topic Adversarial Neural Network (TANN), a novel end-to-end framework for topic-invariant cyberbullying detection. TANN integrates a multi-level feature extractor with a topic discriminator and a cyberbullying detector. It leverages adversarial training to disentangle topic-related information while retaining universal linguistic cues relevant to harmful content. We construct a multi-topic dataset from major Chinese social media platforms, such as Weibo and Tieba, to evaluate the generalization performance of TANN in real-world scenarios. Experimental results demonstrate that TANN outperforms existing methods in cross-topic detection tasks, significantly improving robustness and accuracy. This work advances cross-topic cyberbullying detection by introducing a scalable solution that mitigates topic interference and enables reliable performance across dynamic online environments.},
  archive      = {J_PEERJCS},
  author       = {Shufeng Xiong and Wenzhuo Liu and Bingkun Wang and Yinchao Che and Lei Shi},
  doi          = {10.7717/peerj-cs.2942},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2942},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Topic adversarial neural network for cross-topic cyberbullying detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAPNet: Single and multiplant leaf disease classification method based on simplified SqueezeNet for grape, apple and potato plants. <em>PEERJCS</em>, <em>11</em>, e2941. (<a href='https://doi.org/10.7717/peerj-cs.2941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans need food to sustain their lives. Therefore, agriculture is one of the most important issues in nations. Agriculture also plays a major role in the economic development of countries by increasing economic income. Early diagnosis of plant diseases is crucial for agricultural productivity and continuity. Early disease detection directly impacts the quality and quantity of crops. For this reason, many studies have been carried out on plant leaf disease classification. In this study, a simple and effective leaf disease classification method was developed. Disease classification was performed using seven state-of-the-art pretrained convolutional neural network architectures: VGG16, ResNet50, SqueezeNet, Xception, ShuffleNet, DenseNet121 and MobileNetV2. A simplified SqueezeNet model, GAPNet, was subsequently proposed for grape, apple and potato leaf disease classification. GAPNet was designed to be a lightweight and fast model with 337.872 parameters. To address the data imbalance between classes, oversampling was carried out using the synthetic minority oversampling technique. The proposed model achieves accuracy rates of 99.72%, 99.53%, and 99.83% for grape, apple and potato leaf disease classification, respectively. A success rate of 99.64% was achieved in multiplant leaf disease classification when the grape, apple and potato datasets were combined. Compared with the state-of-the-art methods, the lightweight GAPNet model produces promising results for various plant species.},
  archive      = {J_PEERJCS},
  author       = {Özge Nur Özaras and Asuman Günay Yılmaz},
  doi          = {10.7717/peerj-cs.2941},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2941},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GAPNet: Single and multiplant leaf disease classification method based on simplified SqueezeNet for grape, apple and potato plants},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusing transformer-XL with bi-directional recurrent networks for cyberbullying detection. <em>PEERJCS</em>, <em>11</em>, e2940. (<a href='https://doi.org/10.7717/peerj-cs.2940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying cyberbullying in languages other than English presents distinct difficulties owing to linguistic subtleties and scarcity of annotated datasets. This article presents a new method for identifying cyberbullying in Bengali text data using the Kaggle dataset. This strategy combines Transformer-Extra Large (XL) with bi-directional recurrent neural networks (BiGRU-BiLSTM). Extensive data preparation was performed, including data cleaning, data analysis, and label encoding. Upsampling methods were used to handle imbalanced classes, and data augmentation enhanced the training dataset. We carried out tokenization of the text using a pre-trained tokenizer to capture semantic representations accurately. The model we presented, Transformer-XL-bidirectional gated recurrent units (BiGRU)-bidirectional long short-term memory (BiLSTM), which is called Fusion Transformer-XL, surpassed the performance of the baseline models, attaining an accuracy of 98.17% and an F1-score of 98.18%. Local interpretable model-agnostic explanation (LIME) text explanations were used to understand the reasoning behind the model’s choices and performed the cross-dataset evaluation of the model using the English dataset. This helped improve the clarity and reliability of the proposed method. Furthermore, implementing k-fold cross-validation ensures our model’s robustness and adaptability across diverse data categories. The results of our study demonstrate the effectiveness of combining Transformer-XL with bi-directional recurrent networks for detecting cyberbullying in Bengali. This emphasizes the significance of using hybrid architectures to address intricate natural language processing problems in languages with limited resources. This study enhances the development of methods for detecting cyberbullying and opens up opportunities for additional investigation into language diversity and social media analytics.},
  archive      = {J_PEERJCS},
  author       = {Md. Mithun Hossain and Md. Shakil Hossain and Md. Shakhawat Hossain and M. Firoz Mridha and Mejdl Safran and Sultan Alfarhood and Dunren Che},
  doi          = {10.7717/peerj-cs.2940},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2940},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fusing transformer-XL with bi-directional recurrent networks for cyberbullying detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic review: Progress in EEG-based speech imagery brain-computer interface decoding and encoding research. <em>PEERJCS</em>, <em>11</em>, e2938. (<a href='https://doi.org/10.7717/peerj-cs.2938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article systematically reviews the latest developments in electroencephalogram (EEG)-based speech imagery brain-computer interface (SI-BCI). It explores the brain connectivity of SI-BCI and reveals its key role in neural encoding and decoding. It analyzes the research progress on vowel-vowel and vowel-consonant combinations, as well as Chinese characters, words, and long-words speech imagery paradigms. In the neural encoding section, the preprocessing and feature extraction techniques for EEG signals are discussed in detail. The neural decoding section offers an in-depth analysis of the applications and performance of machine learning and deep learning algorithms. Finally, the challenges faced by current research are summarized, and future directions are outlined. The review highlights that future research should focus on brain region mechanisms, paradigms innovation, and the optimization of decoding algorithms to promote the practical application of SI-BCI technology.},
  archive      = {J_PEERJCS},
  author       = {Ke Su and Liang Tian},
  doi          = {10.7717/peerj-cs.2938},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2938},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Systematic review: Progress in EEG-based speech imagery brain-computer interface decoding and encoding research},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing book genre classification with BERT and InceptionV3: A deep learning approach for libraries. <em>PEERJCS</em>, <em>11</em>, e2934. (<a href='https://doi.org/10.7717/peerj-cs.2934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate book genre classification is essential for library organization, information retrieval, and personalized recommendations. Traditional classification methods, often reliant on manual categorization and metadata-based approaches, struggle with the complexities of hybrid genres and evolving literary trends. To address these limitations, this study proposes a hybrid deep learning model that integrates visual and textual features for enhanced genre classification. Specifically, we employ InceptionV3, an advanced convolutional neural network architecture, to extract visual features from book cover images and bidirectional encoder representations from transformers (BERT) to analyze textual data from book titles. A scaled dot-product attention mechanism is used to effectively fuse these multimodal features, dynamically weighting their contributions based on contextual relevance. Experimental results on the BookCover30 dataset demonstrate that our proposed model outperforms baseline approaches, achieving a balanced accuracy of 0.7951 and an F1-score of 0.7920, surpassing both standalone image- and text-based classifiers. This study highlights the potential of deep learning in improving automated genre classification, offering a scalable and adaptable solution for libraries and digital platforms. Future research may focus on expanding dataset diversity, optimizing computational efficiency, and addressing biases in classification models.},
  archive      = {J_PEERJCS},
  author       = {Xinting Yang and Zehua Zhang},
  doi          = {10.7717/peerj-cs.2934},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2934},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing book genre classification with BERT and InceptionV3: A deep learning approach for libraries},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VBM-YOLO: An enhanced YOLO model with reduced information loss for vehicle body markers detection. <em>PEERJCS</em>, <em>11</em>, e2932. (<a href='https://doi.org/10.7717/peerj-cs.2932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vehicle safety detection, the accurate identification of body markers on medium and large vehicles plays a critical role in ensuring safe road travel. To address the issues of the feature and gradient information loss in previous You Only Look Once (YOLO) series models, a novel Vehicle Body Markers YOLO (VBM-YOLO) model has been designed. Firstly, the model integrates the cross-spatial-channel attention (CSCA) mechanism proposed in this study. The CSCA uses cross-dimensional information to address interaction issues during the fusion of spatial and channel dimensions, significantly enhancing the model’s representational capacity. Secondly, we propose a multi-scale selective feature pyramid network (MSSFPN). By a progressive fusion approach and multi-scale feature selection learning, MSSFPN alleviates the issues of feature loss and target layer information confusion caused by traditional top-down and bottom-up feature pyramids. Finally, an auxiliary gradient branch (AGB) is proposed. During training, AGB incorporates feature information from different target layers to help the current layer retain complete gradient information. Additionally, the AGB branch does not participate in model inference, thereby reducing additional overhead. Experimental results demonstrate that VBM-YOLO improves mean average precision (mAP) by 2.3% and 4.3% at intersection over union (IoU) thresholds of 0.5 and 0.5:0.95, respectively, compared to YOLOv8s on the vehicle body markers dataset. VBM-YOLO also achieves a better balance between accuracy and computational resources than other mainstream models, exhibiting good generalization performance on public datasets like PASCAL VOC and D-Fire.},
  archive      = {J_PEERJCS},
  author       = {Bin Wang and Chao Li and Chao Zhou and Jun Sun},
  doi          = {10.7717/peerj-cs.2932},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2932},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {VBM-YOLO: An enhanced YOLO model with reduced information loss for vehicle body markers detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CST-net: Community-guided structural-temporal convolutional networks for popularity prediction. <em>PEERJCS</em>, <em>11</em>, e2931. (<a href='https://doi.org/10.7717/peerj-cs.2931'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to predict the popularity of online contents has important implications in a wide range of areas. The challenge of this problem comes from the inequality of the popularity of content and the numerous complex factors. Existing works fall into three main paradigms: feature-driven approaches, generative models, and methods based on deep learning, each with known strengths and limitations. In this article, we propose an end-to-end deep learning framework, called CST-Net, to combat the defects of existing methods. We first learn a low-dimensional embedding for each user based on historic interactions. Then, users are clustered into communities based on the learned user embeddings, and information cascades are represented as a series of episodes in the form of community interaction matrix. Afterwards, a convolutional architecture is applied to learn the representation of the entire information cascade. Finally, the extracted structural and temporal features are further combined to predict the incremental popularity. We validate the effectiveness of the proposed CST-Net by applying it on two different types of population-scale datasets, i.e., a microblogging dataset and an academic citation dataset. Experimental results demonstrate that the proposed CST-Net model consistently outperforms the existing competitive popularity prediction methods.},
  archive      = {J_PEERJCS},
  author       = {Xuxu Zheng and Peng Bao and Lin Qi and Chen Tian and Huawei Shen},
  doi          = {10.7717/peerj-cs.2931},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2931},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {CST-net: Community-guided structural-temporal convolutional networks for popularity prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel dilated weighted recurrent neural network (RNN)-based smart contract for secure sharing of big data in ethereum blockchain using hybrid encryption schemes. <em>PEERJCS</em>, <em>11</em>, e2930. (<a href='https://doi.org/10.7717/peerj-cs.2930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background With the enhanced data amount being created, it is significant to various organizations and their processing, and managing big data becomes a significant challenge for the managers of the data. The development of inexpensive and new computing systems and cloud computing sectors gave qualified industries to gather and retrieve the data very precisely however securely delivering data across the network with fewer overheads is a demanding work. In the decentralized framework, the big data sharing puts a burden on the internal nodes among the receiver and sender and also creates the congestion in network. The internal nodes that exist to redirect information may have inadequate buffer ability to momentarily take the information and again deliver it to the upcoming nodes that may create the occasional fault in the transmission of data and defeat frequently. Hence, the next node selection to deliver the data is tiresome work, thereby resulting in an enhancement in the total receiving period to allocate the information. Methods Blockchain is the primary distributed device with its own approach to trust. It constructs a reliable framework for decentralized control via multi-node data repetition. Blockchain is involved in offering a transparency to the application of transmission. A simultaneous multi-threading framework confirms quick data channeling to various network receivers in a very short time. Therefore, an advanced method to securely store and transfer the big data in a timely manner is developed in this work. A deep learning-based smart contract is initially designed. The dilated weighted recurrent neural network (DW-RNN) is used to design the smart contract for the Ethereum blockchain. With the aid of the DW-RNN model, the authentication of the user is verified before accessing the data in the Ethereum blockchain. If the authentication of the user is verified, then the smart contracts are assigned to the authorized user. The model uses elliptic Curve ElGamal cryptography (EC-EC), which is a combination of elliptic curve cryptography (ECC) and ElGamal encryption for better security, to make sure that big data transfers on the Ethereum blockchain are safe. The modified Al-Biruni earth radius search optimization (MBERSO) algorithm is used to make the best keys for this EC-EC encryption scheme. This algorithm manages keys efficiently and securely, which improves data security during blockchain operations. Results The processes of encryption facilitate the secure transmission of big data over the Ethereum blockchain. Experimental analysis is carried out to prove the efficacy and security offered by the suggested model in transferring big data over blockchain via smart contracts.},
  archive      = {J_PEERJCS},
  author       = {Swetha S and Joe Prathap P M},
  doi          = {10.7717/peerj-cs.2930},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2930},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel dilated weighted recurrent neural network (RNN)-based smart contract for secure sharing of big data in ethereum blockchain using hybrid encryption schemes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RA-QoS: A robust autoencoder-based QoS predictor for highly accurate web service QoS prediction. <em>PEERJCS</em>, <em>11</em>, e2928. (<a href='https://doi.org/10.7717/peerj-cs.2928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web services are fundamental for online service-oriented applications, where accurately predicting quality of service (QoS) is critical for recommending optimal services among multiple candidates. Since QoS data often contains noise—stemming from factors like remote user or service locations—current deep neural network (DNN)-based QoS predictors, which generally rely on L2-norm loss functions, face limitations in robustness due to sensitivity to outliers. To address this issue, we propose a novel robust autoencoder-based QoS predictor (RA-QoS) that leverages a hybrid loss function combining bias, training bias, L1-norm and L2-norm to build a robust Autoencoder. This hybrid approach allows RA-QoS to better handle noisy data, minimizing the impact of outliers and biases on prediction accuracy. The RA-QoS model further incorporates preprocessing and training biases, improving its adaptability to real-world QoS data. To evaluate the proposed RA-QoS predictor, extensive experiments are conducted on two real-world QoS datasets. The results demonstrate that our RA-QoS predictor exhibits superior robustness to outliers and higher accuracy in QoS prediction compared to the related state-of-the-art models.},
  archive      = {J_PEERJCS},
  author       = {Shun Fu and Junnan Li and Lufeng Wang},
  doi          = {10.7717/peerj-cs.2928},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2928},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RA-QoS: A robust autoencoder-based QoS predictor for highly accurate web service QoS prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid decision support system disaster management: Application of lattice ordered q-rung linear diophantine fuzzy hypersoft sets. <em>PEERJCS</em>, <em>11</em>, e2927. (<a href='https://doi.org/10.7717/peerj-cs.2927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of the lattice-ordered q-rung linear Diophantine fuzzy hypersoft set is a significant extension of fuzzy set theory. This study describes many of its fundamental algebraic operations, such as restricted union, extended union, restricted intersection, OR operation, and AND operation, along with examples. Further, an algorithm based on the proposed operations is presented in this study to handle multi-attributed decision-making problems extremely well, along with an illustrative multi-attribute decision-making example in the area of disaster management, which helps in choosing the most appropriate plan to tackle the known natural disaster by considering a greater number of attributes together. Further, the contribution of the method in the disaster management field is presented in the comparative analysis along with computational efficiency and scalability and an analysis of the comparison between the existing decision-making methods and the proposed one to express the superiority and advantages of the suggested approach over the existing methods.},
  archive      = {J_PEERJCS},
  author       = {J. Vimala and A. N. Surya and Nasreen Kausar and Dragan Pamucar and Seifedine Kadry and Jungeun Kim},
  doi          = {10.7717/peerj-cs.2927},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2927},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hybrid decision support system disaster management: Application of lattice ordered q-rung linear diophantine fuzzy hypersoft sets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RiceChain-plus: An enhanced framework for blockchain-based rice supply chain systems-ensuring security, privacy, and efficiency. <em>PEERJCS</em>, <em>11</em>, e2926. (<a href='https://doi.org/10.7717/peerj-cs.2926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rice supply chain is a complex system that demands effective management to ensure reliability and efficiency, given the involvement of multiple stakeholders. Blockchain technology, with its decentralized and tamper-resistant nature, offers a promising solution for improving transparency, traceability, and credibility in agricultural supply chains. However, existing blockchain systems face several technological challenges, including security vulnerabilities, privacy concerns, and performance limitations. To address these issues, this article presents RiceChain-Plus, an enhanced architecture that incorporates a private Ethereum blockchain, proof of authority (PoA) consensus mechanism, mutual authentication, zero-knowledge proofs (ZKPs), a hybrid role-based access control (RBAC) and attribute-based access control (ABAC) system, and one-way hash functions. This approach enhances the rice supply chain’s security, privacy, and efficiency by safeguarding sensitive data and ensuring confidentiality. Performance assessments show that RiceChain-Plus surpasses existing benchmark models, achieving the lowest average execution costs (44,634 gas), reduced energy consumption (9.38828E−05 J), higher throughput (0.071201 transactions/s), faster execution (44.5 ms), and quicker transaction times (14.045 s), while also improving scalability. A comprehensive security analysis further confirms the framework’s resilience against various cyberattacks. These results highlight RiceChain-Plus as a secure, efficient, and effective solution for optimizing rice supply chain operations.},
  archive      = {J_PEERJCS},
  author       = {Bello Musa Yakubu and Abdullah Abdulrahman Alabdulatif and Pattarasinee Bhattarakosol},
  doi          = {10.7717/peerj-cs.2926},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2926},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RiceChain-plus: An enhanced framework for blockchain-based rice supply chain systems-ensuring security, privacy, and efficiency},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective federated learning traffic prediction in vehicular network for intelligent transportation system. <em>PEERJCS</em>, <em>11</em>, e2922. (<a href='https://doi.org/10.7717/peerj-cs.2922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial-temporal data of future freight traffic speed in the metropolitan region must be properly understood to develop freight-related traffic management strategies. This work introduces a new approach to traffic prediction using multi-objective federated learning. Instead of relying on a centralized cloud server for data processing, collaborative training is implemented among several participants. The proposed method utilizes the advantages of reinforcement learning in dynamic decision-making scenarios and the expressive capabilities of graphical models to identify traffic intensity. Furthermore, a new methodology integrates federated learning concepts with multi-objective optimization to forecast traffic patterns accurately. The proposed approach exhibits a higher level of performance than existing methods for estimating traffic speed. It achieves a communication delay of 23.4%, packet delivery ratio (PDR) of 92.45%, packet loss rate of 12.34%, prediction accuracy of 97.45%, and resource utilization of 89.56%. The visualisation findings demonstrate that this new approach is able to successfully capture interconnections of metropolitan areas in different neighboring cities.},
  archive      = {J_PEERJCS},
  author       = {Arulmurgan Aalavanthar and Famila S. and Shanmugam Sundaramurthy and Stefano Cirillo and Giandomenico Solimando and Giuseppe Polese},
  doi          = {10.7717/peerj-cs.2922},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2922},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-objective federated learning traffic prediction in vehicular network for intelligent transportation system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep vision-based real-time hand gesture recognition: A review. <em>PEERJCS</em>, <em>11</em>, e2921. (<a href='https://doi.org/10.7717/peerj-cs.2921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture recognition is an approach to comprehending human body language, applied in various fields such as human-computer interaction. However, some issues remain in edge blurring generated by complex backgrounds, rotation inaccuracy induced by fast movement, and delay caused by computing cost. Recently, the emergence of deep learning has ameliorated these issues, convolution neural network (CNN) enhanced edge clarity, long-short term memory (LSTM) improved rotation accuracy, and attention mechanism optimized response time. In this context, this review starts with the deep learning models, specifically CNN, LSTM, and attention mechanisms, which are compared and discussed from the utilization rate of each, their contribution to improving accuracy or efficiency, and their role in the recognition stage, like feature extraction. Furthermore, to evaluate the performance of these deep learning models, the evaluation metrics, datasets, and ablation studies are analyzed and discussed. The choice of evaluation metrics and dataset is critical since different tasks require different evaluation parameters, and the model learns more patterns and features from diverse data. Therefore, the evaluation metrics are categorized into accuracy and efficiency. The datasets are analyzed from self-created to public datasets. The ablation study is summarized in four aspects: similar underlying models, integrating specific models, pre-processing, others. Finally, the existing research gaps and further research on accuracy, efficiency, application range, and environmental adaptation are discussed.},
  archive      = {J_PEERJCS},
  author       = {Cui Cui and Mohd Shahrizal Sunar and Goh Eg Su},
  doi          = {10.7717/peerj-cs.2921},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2921},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep vision-based real-time hand gesture recognition: A review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel brain tumor magnetic resonance imaging dataset (Gazi brains 2020): Initial benchmark results and comprehensive analysis. <em>PEERJCS</em>, <em>11</em>, e2920. (<a href='https://doi.org/10.7717/peerj-cs.2920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new benchmark MRI dataset called the Gazi Brains Dataset 2020, containing MRI images of 100 patients, and introduces initial experimental results performed on this dataset in comparison with available brain MRI datasets. Furthermore, the dataset is analyzed using eight different deep learning models for high-grade glioma tumor prediction, classification, and detection tasks. Additionally, this study demonstrates the results of an explainable Artificial Intelligence (XAI) approach applied to the trained models. To demonstrate the utility of the proposed dataset, different deep learning models were applied to the problem, and these models were tested on various data and models applied for various tasks such as region of interest extraction, whole tumor segmentation, prediction, detection, and classification with accuracy, precision, recall, and F1-score. The experimental results indicate that the dataset is highly effective for multiple purposes, and the models reached significant results with successful F1-scores ranging between 93.2% and 96.4%. ROI and whole tumor segmentations were successfully performed and compared with seven algorithms with accuracies of 87.61% and 97.18%. The Grad-CAM model also demonstrated satisfactory accuracy across the tests that were conducted. Moreover, this study explores the application of XAI to the trained models, providing interpretability and insights into the decision-making processes. The findings signify that this dataset holds significant potential for various future research directions, including age estimation, gender detection, causal inference with XAI, and disease-related survival analysis.},
  archive      = {J_PEERJCS},
  author       = {Seref Sagiroglu and Ramazan Terzi and Emrah Celtikci and Alp Özgün Börcek and Yilmaz Atay and Bilgehan Arslan and Mustafa Caglar Sahin and Kerem Nernekli and Umut Demirezen and Okan Bilge Ozdemir and Kevser Özdem Karaca and Nuh Azgınoğlu},
  doi          = {10.7717/peerj-cs.2920},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2920},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel brain tumor magnetic resonance imaging dataset (Gazi brains 2020): Initial benchmark results and comprehensive analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sporting a virtual future: Exploring sports and virtual reality patents using deep learning-based analysis. <em>PEERJCS</em>, <em>11</em>, e2919. (<a href='https://doi.org/10.7717/peerj-cs.2919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the convergence of sports and emerging technologies from the Fourth Industrial Revolution, with a focus on virtual reality (VR) applications. Using patent big data, we introduce SportsBERT, a bidirectional encoder representation from transformers (BERT)-based algorithm tailored for enhanced natural language processing in sports-related knowledge-based documents. Through topic modeling, we extract key themes and clusters from sports-related VR patents, providing insights into the knowledge structure and technological trends in VR applications for sports. Our analysis identifies key drivers of technological advancement, including spatial hardware, tactile human–computer interactions, aerobic exercise, rehabilitation, and swing sports. Additionally, we highlight challenges such as the high cost and usability limitations of current VR devices. This study presents the first deep learning-based topic modeling approach specialized for sports patents and offers a comprehensive roadmap for current developments and future trajectories in VR sports technologies.},
  archive      = {J_PEERJCS},
  author       = {Jea Woog Lee and Sangmin Song and JungMin Yun and Doug Hyun Han and YoungBin Kim},
  doi          = {10.7717/peerj-cs.2919},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2919},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Sporting a virtual future: Exploring sports and virtual reality patents using deep learning-based analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aerial image segmentation of embankment dams based on multispectral remote sensing: A case study in the belo monte hydroelectric complex, pará, brazil. <em>PEERJCS</em>, <em>11</em>, e2917. (<a href='https://doi.org/10.7717/peerj-cs.2917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual inspection is essential to ensure the stability of earth-rock dams. Periodic visual assessment of this type of structure through vegetation cover analysis is an effective monitoring method. Recently, multispectral remote sensing data and machine learning techniques have been applied to develop methodologies that enable automatic vegetation analysis and anomaly detection based on computer vision. As a first step toward this automation, this study introduces a methodology for land cover segmentation of earth-rock embankment dam structures within the Belo Monte Hydroelectric Complex, located in the state of Pará, northern Brazil. Random forest (RF) ensemble models were trained on manually annotated data captured by a multispectral sensor embedded in an uncrewed aerial vehicle (UAV). The main objectives of this study are to assess the classification performance of the algorithm in segmenting earth-rock dams and the contribution of non-visible band reflectance data to the overall model performance. A comprehensive feature engineering and ranking approach is presented to select the most descriptive features that represent the four dataset classes. Model performance was assessed using classical performance metrics derived from the confusion matrix, such as accuracy, Kappa coefficient, precision, recall, F1-score, and intersection over union (IoU). The final RF model achieved 90.9% mean IoU for binary segmentation and 91.1% mean IoU for multiclass segmentation. Post-processing techniques were applied to refine the predicted masks, enhancing the mean IoU to 93.2% and 91.9%, respectively. The flexible methodology presented in this work can be applied to different scenarios when treated as a framework for pixel-wise land cover classification, serving as a crucial step toward automating visual inspection processes. The implementation of automated monitoring solutions improves the visual inspection process and mitigates the catastrophic consequences resulting from dam failures.},
  archive      = {J_PEERJCS},
  author       = {Carlos André de Mattos Teixeira and Thabatta Moreira Alves de Araujo and Evelin Cardoso and Marcos Antonio Costantin Filho and João Weyl Costa and Carlos Renato Lisboa Frances},
  doi          = {10.7717/peerj-cs.2917},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2917},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Aerial image segmentation of embankment dams based on multispectral remote sensing: A case study in the belo monte hydroelectric complex, pará, brazil},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated guided vehicle (AGV) path optimization method based on improved rapidly-exploring random trees. <em>PEERJCS</em>, <em>11</em>, e2915. (<a href='https://doi.org/10.7717/peerj-cs.2915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the issues of low computational efficiency, slow convergence speed, curvy paths, and the tendency to fall into local optima in rapidly-exploring random tree (RRT) algorithms for automated guided vehicle (AGV) path planning, this article proposes an improved RRT algorithm that combines adaptive step-size optimization with K-dimensional tree (KD-Tree) based fast nearest neighbor search. Firstly, an adaptive step-size optimization strategy is introduced to dynamically adjust the step size during node searches, improving both the planning quality and computational efficiency of the algorithm. Secondly, the KD-Tree nearest neighbor search method is employed to accelerate node searching and reduce the time cost of path planning. Finally, a cubic spline interpolation function is applied to smooth the optimal path, further enhancing the planning quality. Experimental results show that the improved RRT algorithm significantly outperforms traditional RRT, RRT*, and Informed-RRT* in terms of path length, planning time, and path smoothness. Specifically, the average path length is reduced by 164.33 m, and the average search time is shortened by 3.3 s, making it more suitable for AGV path planning in practical applications.},
  archive      = {J_PEERJCS},
  author       = {Zhigang Ren and Anjiang Cai and Feilong Xu},
  doi          = {10.7717/peerj-cs.2915},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2915},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automated guided vehicle (AGV) path optimization method based on improved rapidly-exploring random trees},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Siamese meta-learning network for social disputes based on multi-head attention. <em>PEERJCS</em>, <em>11</em>, e2910. (<a href='https://doi.org/10.7717/peerj-cs.2910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning has been widely used in scenarios where labeled data is scarce, where meta-learning based few-shot classification is widely used, such as the Siamese network. Although the Siamese network has achieved good results in some applications, there are still some problems: (1) When computing prototype vectors with external knowledge of class labels, it depends on the quality and correctness of class labels. (2) When processing data, the Siamese network is not sufficient to capture dependencies between long distance. (3) When the data is complex or the samples are unbalanced, the Siamese network does not achieve the best performance. Therefore, this article proposes a multi-head attention siamese meta-learning network (MASM). Specifically, this article uses synonym substitution to solve the problem that the computation of prototype vectors will be transitionally dependent on class label. In addition, we use the multi-head attention mechanism to capture long-distance dependence by exploiting its global perception capability, which further improves the model performance. We conducted experiments on four benchmark datasets, all of which achieved good performance, and also applied the model for the first time in the field of social disputes, and experimented on a homemade private dispute dataset, which also achieved good results.},
  archive      = {J_PEERJCS},
  author       = {Jing Wang and Rui Zhang and Huijian Han and Yuxiang Liu and Zhaoxing Peng},
  doi          = {10.7717/peerj-cs.2910},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2910},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Siamese meta-learning network for social disputes based on multi-head attention},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdvFaceGAN: A face dual-identity impersonation attack method based on generative adversarial networks. <em>PEERJCS</em>, <em>11</em>, e2904. (<a href='https://doi.org/10.7717/peerj-cs.2904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to reveal security vulnerabilities in current commercial facial recognition systems and promote advancements in facial recognition technology security. Previous research on both digital-domain and physical-domain attacks has lacked consideration of real-world attack scenarios: Digital-domain attacks with good stealthiness often fail to achieve physical implementation, while wearable-based physical-domain attacks typically appear unnatural and cannot evade human visual inspection. We propose AdvFaceGAN, a generative adversarial network (GAN)-based impersonation attack method that generates dual-identity adversarial faces capable of bypassing defenses and being uploaded to facial recognition system databases in our proposed attack scenario, thereby achieving dual-identity impersonation attacks. To enhance visual quality, AdvFaceGAN introduces a structural similarity loss in addition to conventional generative loss and perturbation loss, optimizing the generation pattern of adversarial perturbations. Under the combined effect of these three losses, our method produces adversarial faces with excellent stealthiness that can pass administrator’s human review. To improve attack effectiveness, AdvFaceGAN employs an ensemble of facial recognition models with maximum model diversity to calculate identity loss, thereby enhancing similarity to target identities. Innovatively, we incorporate source identity loss into the identity loss calculation, discovering that minor reductions in target identity similarity can be traded for significant improvements in source identity similarity, thus making the adversarial faces generated by our method highly similar to both the source identity and the target identity, addressing limitations in existing impersonation attack methods. Experimental results demonstrate that in black-box attack scenarios, AdvFaceGAN-generated adversarial faces exhibit better stealthiness and stronger transferability compared to existing methods, achieving superior traditional and dual-identity impersonation attack success rates across multiple black-box facial recognition models and three commercial facial recognition application programming interfaces (APIs).},
  archive      = {J_PEERJCS},
  author       = {Hong Huang and Yang Yang and Yunfei Wang},
  doi          = {10.7717/peerj-cs.2904},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2904},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {AdvFaceGAN: A face dual-identity impersonation attack method based on generative adversarial networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent reflecting surface backscatter-enabled physical layer security enhancement via deep reinforcement learning. <em>PEERJCS</em>, <em>11</em>, e2902. (<a href='https://doi.org/10.7717/peerj-cs.2902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel strategy for wireless communication security utilizing intelligent reflecting surfaces (IRS). The IRS is strategically deployed to mitigate jamming attacks and eavesdropper threats while improving signal reception for legitimate users (LUs) by redirecting jamming signals toward desired communication signals leveraging physical layer security (PLS). By integrating the IRS into the backscatter communication system, we enhance the overall secrecy rate of LU, by dynamically adjusting IRS reflection coefficients and active beamforming at the base station (BS). A design problem is formulated to jointly optimize IRS reflecting beamforming and BS active beamforming, considering time-varying channel conditions and desired secrecy rate requirements. We propose a novel approach based on deep reinforcement learning (DRL) named Deep-PLS. This approach aims to determine an optimal beamforming policy capable of thwarting eavesdroppers in evolving environmental conditions. Extensive simulation studies validate the efficacy of our proposed strategy, demonstrating superior performance compared to traditional IRS approaches, IRS backscattering-based anti-eavesdropping methods, and other benchmark strategies in terms of secrecy performance.},
  archive      = {J_PEERJCS},
  author       = {Manzoor Ahmed and Touseef Hussain and Muhammad Shahwar and Feroz Khan and Muhammad Sheraz and Wali Ullah Khan and Teong Chee Chuah and It Ee Lee},
  doi          = {10.7717/peerj-cs.2902},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2902},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Intelligent reflecting surface backscatter-enabled physical layer security enhancement via deep reinforcement learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-based real-time enhancement for disease prediction using confluent cloud, apache kafka, feature optimization, and explainable artificial intelligence. <em>PEERJCS</em>, <em>11</em>, e2899. (<a href='https://doi.org/10.7717/peerj-cs.2899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Internet of Things (IoT)-based technologies have advanced healthcare by facilitating the development of monitoring systems, subsequently generating an exponential amount of streaming data. This streaming data can be preprocessed and analyzed using technologies that integrate ensemble models, Explainable Artificial Intelligence (XAI), feature selection (FS) method and big data streaming processing platforms to develop predictive real-time systems. This integration adds new value to healthcare that helps organizations enhance clinical decision-making, improve patient care, and elevate the overall quality of healthcare. This article presents a real-time system for the early detection and treatment of chronic kidney disease (CKD) using a real-world simulation application. The real-time system is developed in two phases. The first phase aims to propose a stacking model, apply a genetic algorithm (GA) and Particle swarm optimization (PSO) as feature selection, and explore a stacking model with the best features with explainable artificial intelligence (XAI). The best model with the best-optimized features is used to develop the second phase. The results showed that stacking model with GA is achieved the hightest performance with 100 accuracy, 100 precision, 100 recall, and 100 F1-score. The second phase is designed based on Confluent Cloud, which offers several benefits for creating a real-time streaming system based on Apache Kafka, providing multiple APIs—the Producer API and Consumer API—for data producers and consumers, respectively. Python scripts are developed to pipeline streaming data. The first Python script to generate streaming health attributes that are pushed into a Kafka topic. A second Python script to consume health attributes from a Kafka topic and apply a stacking model to predict CKD in real-time. The results showed that the stacking model with features selected by GA recorded the best performance with 100 accuracy. The pipeline’s streaming steps have validated our approach’s effectiveness in real-time, leveraging Confluent Cloud and Apache Kafka.},
  archive      = {J_PEERJCS},
  author       = {Abdulaziz AlMohimeed},
  doi          = {10.7717/peerj-cs.2899},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2899},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Cloud-based real-time enhancement for disease prediction using confluent cloud, apache kafka, feature optimization, and explainable artificial intelligence},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel conditional tabular generative adversarial network based image augmentation for railway track fault detection. <em>PEERJCS</em>, <em>11</em>, e2898. (<a href='https://doi.org/10.7717/peerj-cs.2898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Railway track fault recognition is a critical aspect of railway maintenance, aiming to identify and rectify defects such as cracks, misalignments, and wear on tracks to ensure safe and efficient train operations. Classical methods for fault detection, including manual inspections and simple sensor-based systems, face significant challenges, such as high labour costs, human error, and limited detection accuracy under varying environmental conditions. These methods are often time-consuming and unable to provide real-time monitoring, leading to potential safety risks and operational inefficiencies. To address these challenges, efficient artificial intelligence-based image classification is being explored to enhance railway track fault detection accuracy, efficiency, and reliability. This research aims to develop an advanced generative neural network for efficient railway track fault detection. We propose a novel conditional tabular generative adversarial network (CTGAN)-based image augmentation approach to producing realistic synthetic image data using railway track images. We developed five advanced neural network techniques for comparison with railway track image classification. The random forest approach surpasses state-of-the-art studies with a high accuracy score of 0.99 for railway track fault detection. Hyperparameter optimization is applied to achieve optimal performance, and the performance is evaluated using the k-fold cross-validation approach. The proposed research enhances operational efficiency, reduces maintenance costs, and significantly improves the safety and reliability of rail transportation.},
  archive      = {J_PEERJCS},
  author       = {Ali Raza and Rukhshanda Sehar and Abdul Moiz and Ala Saleh Alluhaidan and Sahar A. El-Rahman and Diaa Salama AbdElminaam},
  doi          = {10.7717/peerj-cs.2898},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2898},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Novel conditional tabular generative adversarial network based image augmentation for railway track fault detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of an improved graph-based model integrating LSTM, LoRaWAN, and blockchain for smart agriculture. <em>PEERJCS</em>, <em>11</em>, e2896. (<a href='https://doi.org/10.7717/peerj-cs.2896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research is anchored on the burning need for irrigation optimization and crop water use efficiency improvement, which remains a challenge in smart agriculture processes. Traditional irrigation methods normally lead to inefficiency, resulting in wasted water and non-maximum crops. These traditional ways normally lack attributes of real-time adaptability and secure data management—things that are very key to modernizing agricultural practices. In this work, artificial intelligence (AI), Internet of Things (IoT), and blockchain techniques will be integrated to design a comprehensive system for monitoring and predicting soil moisture levels. In the proposed model, long short-term memory (LSTM) networks are considered for soil moisture level prediction, taking into consideration past data, weather, and crop type. LSTM networks are chosen here for their high performance in timestamp series prediction tasks with an mean average error (MAE) of 0.02 m3/m3 over a 7-day forecast horizon. For real-time monitoring, IoT sensors based on long range wide area network (LoRaWAN) technology are field-deployed for conducting long-range communications while consuming very limited energy to extend the sensor battery life over 5 years and bring down the data transmission latency below 5 s. It has an inbuilt permissioned blockchain framework—Hyperledger Fabric—which offers a secure and transparent system for data management and maintaining a record of soil moisture data, irrigation events, and metadata from sensors. This ensures the immutability and integrity of sets of data. Smart contracts automate irrigation upon reaching preconfigured soil moisture thresholds, and hence zero data integrity breaches occur with a transaction throughput of 1,000 transactions per second, taken into view with smart contract execution latency of less than 2 s. Moreover, it utilizes reinforcement learning with Deep Q-Learning to derive an optimized irrigation schedule. In this regard, it enables learning optimal irrigation policies and implements them to improve efficiency in the usage of water by 25% and increases crop yield by 15% compared to the traditional methods. Clearly from field trials, results indicate evident efficiency of the integrated system: a 20% water usage reduction and a 12% increase in crop yield within one growing season. This is rather an innovative take on irrigation practices, increasing a great deal of accuracy and sustainability for such and providing a really strong solution toward better agricultural productivity and resource management.},
  archive      = {J_PEERJCS},
  author       = {Ravi Kumar Munaganuri and Narasimha Rao Yamarthi and Sai Chandana Bolem},
  doi          = {10.7717/peerj-cs.2896},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2896},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of an improved graph-based model integrating LSTM, LoRaWAN, and blockchain for smart agriculture},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent advances in the inverse design of silicon photonic devices and related platforms using deep generative models. <em>PEERJCS</em>, <em>11</em>, e2895. (<a href='https://doi.org/10.7717/peerj-cs.2895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an overview of recent research on the inverse design of optical devices using deep generative models. The increasing complexity of modern optical devices necessitates advanced design methodologies that can efficiently navigate vast parameter spaces and generate novel, high-performance structures. Established optimization methods, such as adjoint and topology optimization, have successfully addressed many design challenges. However, the increasing complexity of modern optical devices creates opportunities for complementary approaches. Deep generative models offer additional capabilities by leveraging their ability to learn complex patterns and generate novel designs. This review examines various deep learning methodologies, including multi-layer perceptrons (MLP), convolutional neural networks (CNN), auto-encoders (AE), Generative Adversarial Networks (GAN), and reinforcement learning (RL) approaches. We analyze their applications in the inverse design of photonic devices, comparing their effectiveness and integration in the design process. Our findings indicate that while MLP-based methods were commonly used in early research, recent studies have increasingly employed CNN, GAN, AE, and RL methods, as well as advanced MLP models. Each of these methods offers unique advantages and presents specific challenges in the context of optical device inverse design. This review critically evaluates these deep learning-based inverse design technologies, highlighting their strengths and limitations in the context of optical device design. By synthesizing current research and identifying key trends, this article aims to guide future developments in the application of deep generative models for optical device inverse design.},
  archive      = {J_PEERJCS},
  author       = {Sun Jae Baek and Minhyeok Lee},
  doi          = {10.7717/peerj-cs.2895},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2895},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Recent advances in the inverse design of silicon photonic devices and related platforms using deep generative models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Majority clustering for imbalanced image classification. <em>PEERJCS</em>, <em>11</em>, e2891. (<a href='https://doi.org/10.7717/peerj-cs.2891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a prevalent challenge in image classification tasks, where certain classes are significantly underrepresented compared to others. This imbalance often leads to biased models that perform poorly in predicting minority classes, affecting the overall performance and reliability of image classification systems. In this article, an under-sampling approach based on reducing the samples of majority class is used along with the unsupervised clustering approach for partitioning the majority class into clusters within the datasets. The proposed technique, Majority Clustering for Imbalanced Image Classification (MCIIC) improves the traditional binary classification problems by converting it into multi-class problem, thereby creating the more balanced classification solution to the problems where one need to detect rare samples present in the dataset. By utilizing the elbow method, we determine the optimal number of clusters for the majority class and assign each cluster a new class label. This complete process ensures a balanced and symmetrical class distribution, effectively addressing imbalances both between and within classes and helps to perform imbalanced classification. The effectiveness of the proposed model is evaluated on various benchmark datasets, demonstrating their ability to improve the predictive performance of the proposed MCIIC on imbalanced image datasets. Through empirical evaluation, we showcase the impact of proposed technique on model accuracy, precision, recall, and F1-score, highlighting its importance as a pre-processing step in handling imbalanced image datasets. The results highlight the significance of proposed model as a practical approach to address the challenges posed by imbalanced data distributions in machine learning tasks.},
  archive      = {J_PEERJCS},
  author       = {Keshav Sharma and Jyoti Arora and Pooja Kherwa and Zainab Alansari},
  doi          = {10.7717/peerj-cs.2891},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2891},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Majority clustering for imbalanced image classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging large language models for spelling correction in turkish. <em>PEERJCS</em>, <em>11</em>, e2889. (<a href='https://doi.org/10.7717/peerj-cs.2889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of natural language processing (NLP) has rapidly progressed, particularly with the rise of large language models (LLMs), which enhance our understanding of the intrinsic structures of languages in a cross-linguistic manner for complex NLP tasks. However, commonly encountered misspellings in human-written texts adversely affect language understanding for LLMs for various NLP tasks as well as misspelling applications such as auto-proofreading and chatbots. Therefore, this study focuses on the task of spelling correction in the agglutinative language Turkish, where its nature makes spell correction significantly more challenging. To address this, the research introduces a novel dataset, referred to as NoisyWikiTr, to explore encoder-only models based on bidirectional encoder representations from transformers (BERT) and existing auto-correction tools. For the first time in this study, as far as is known, encoder-only models based on BERT are presented as subword prediction models, and encoder-decoder models based on text-cleaning (Text-to-Text Transfer Transformer) architecture are fine-tuned for this task in Turkish. A comprehensive comparison of these models highlights the advantages of context-based approaches over traditional, context-free auto-correction tools. The findings also reveal that among LLMs, a language-specific sequence-to-sequence model outperforms both cross-lingual sequence-to-sequence models and encoder-only models in handling realistic misspellings.},
  archive      = {J_PEERJCS},
  author       = {Ceren Guzel Turhan},
  doi          = {10.7717/peerj-cs.2889},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2889},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging large language models for spelling correction in turkish},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic dependent surveillance-broadcast (ADS-b) anomalous messages and attack type detection: Deep learning-based architecture. <em>PEERJCS</em>, <em>11</em>, e2886. (<a href='https://doi.org/10.7717/peerj-cs.2886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Dependent Surveillance-Broadcast (ADS-B) is a vital communication protocol within air traffic control (ATC) systems. Unlike traditional technologies, ADS-B utilizes the Global Positioning System (GPS) to deliver more accurate and precise location data while reducing operational and deployment costs. It enhances radar coverage and serves as a standalone solution in areas lacking radar services. Despite these advantages, ADS-B faces significant security vulnerabilities due to its open design and the absence of built-in security features. Given its critical role, developing an advanced security framework to classify ADS-B messages and identify various attack types is essential to safeguard the system. This research makes several key contributions to address these challenges. First, it presents a comprehensive review of state-of-the-art machine learning and deep learning techniques, critically analyzing existing methodologies for ADS-B intrusion detection. Second, a detailed attack model is developed, categorizing potential threats and aligning them with key security requirements, including confidentiality, integrity, availability, and authentication. Third, the study proposes a robust and accurate Intrusion Detection System (IDS) using three advanced deep learning models—TabNet, Neural Oblivious Decision Ensembles (NODE), and DeepGBM—to classify ADS-B messages and detect specific attack types. The models are evaluated using standard metrics, including accuracy, precision, recall, and F1-score. Among the tested models, DeepGBM achieves the highest accuracy at 98%, outperforming TabNet (92%) and NODE (96%). The findings offer valuable insights into ADS-B security and define essential requirements for a future security framework, contributing actionable recommendations for mitigating threats in this critical communication protocol.},
  archive      = {J_PEERJCS},
  author       = {Waqas Ahmed and Ammar Masood and Jawad Manzoor and Sedat Akleylek},
  doi          = {10.7717/peerj-cs.2886},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2886},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automatic dependent surveillance-broadcast (ADS-b) anomalous messages and attack type detection: Deep learning-based architecture},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of multi-objective feature regression models for designing performance assessment methods in college and university educational reform. <em>PEERJCS</em>, <em>11</em>, e2883. (<a href='https://doi.org/10.7717/peerj-cs.2883'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evaluation of teacher performance in higher education is a critical component of educational reform, requiring robust and accurate assessment methodologies. Multi-objective regression offers a promising approach to optimizing the construction of performance evaluation index systems. However, conventional regression models often rely on a shared input space for all targets, neglecting the fact that distinct and complex feature sets may influence each target. This study introduces a novel Multi-Objective Feature Regression model under Label-Specific Features (MOFR-LSF), which integrates target-specific features and inter-target correlations to address this limitation. By extending the single-objective stacking framework, the proposed method learns label-specific features for each target and employs cluster analysis on binned samples to uncover underlying correlations among objectives. Experimental evaluations on three datasets—Education Reform (EDU-REFORM), Programme for International Student Assessment (PISA), and National Assessment of Educational Progress (NAEP)—demonstrate the superior performance of MOFR-LSF, achieving relative root mean square error (RRMSE) values of 0.634, 0.332, and 0.925, respectively, outperforming existing multi-objective regression algorithms. The proposed model not only enhances predictive accuracy but also strengthens the scientific validity and fairness of performance evaluations, offering meaningful contributions to educational reform in colleges and universities. Moreover, its adaptable framework suggests potential applicability across a range of other domains.},
  archive      = {J_PEERJCS},
  author       = {Fengjun Qi and Zhenping Liu and Wenzheng Zhang and Zhenjie Sun},
  doi          = {10.7717/peerj-cs.2883},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2883},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimization of multi-objective feature regression models for designing performance assessment methods in college and university educational reform},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced futures price-spread forecasting based on an attention-driven optimized LSTM network: Integrating an improved grey wolf optimizer algorithm for enhanced accuracy. <em>PEERJCS</em>, <em>11</em>, e2865. (<a href='https://doi.org/10.7717/peerj-cs.2865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial market prediction faces significant challenges due to the complex temporal dependencies and heterogeneous data relationships inherent in futures price-spread data. Traditional machine learning methods struggle to effectively mine these patterns, while conventional long short-term memory (LSTM) models lack focused feature prioritization and suffer from suboptimal hyperparameter selection. This article proposes the Improved Grey Wolf Optimizer with Multi-headed Self-attention and LSTM (IGML) model, which integrates a multi-head self-attention mechanism to enhance feature interaction and introduces an improved grey wolf optimizer (IGWO) with four strategic enhancements for automated hyperparameter tuning. Benchmark tests on optimization problems validate IGWO’s superior convergence efficiency. Evaluated on real futures price-spread datasets, the IGML reduces mean square error (RMSE) and mean absolute error (MAE) by up to 88% and 85%, respectively, compared to baseline models, demonstrating its practical efficacy in capturing intricate financial market dynamics.},
  archive      = {J_PEERJCS},
  author       = {Yongli Tang and Zhenlun Gao and Zhongqi Cai and Jinxia Yu and Panke Qin},
  doi          = {10.7717/peerj-cs.2865},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2865},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced futures price-spread forecasting based on an attention-driven optimized LSTM network: Integrating an improved grey wolf optimizer algorithm for enhanced accuracy},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of different initial solutions on the metaheuristic algorithms for the single allocation p-hub center and routing problem. <em>PEERJCS</em>, <em>11</em>, e2840. (<a href='https://doi.org/10.7717/peerj-cs.2840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces methods for initializing a single-trajectory-based metaheuristic, specifically a simulated annealing (SA) algorithm, using constructive heuristics. These methods are designed to target promising regions within the search space of an nondeterministic polynomial time (NP)-hard problem, namely the single allocation p-hub center and routing problem. The objective of this problem is to allocate demand centers to hubs and design vehicle routes such that the maximum distance between all origin-destination pairs is minimized. To analyze the impact of different initial solutions, various constructive heuristics, including greedy and hybrid strategies, have been proposed. Additionally, a problem decomposition approach leveraging domain-specific knowledge has been incorporated through a matheuristic initial solution strategy to enhance the efficiency of the simulated annealing algorithm. This approach generates high-quality initial solutions by first solving the p-hub center problem and then using the obtained hubs and their assignments as inputs to the min-max multiple traveling salesman problem. In this problem, the objective function is formulated differently from the literature by minimizing the longest distance between the two nodes. Several experiments have been conducted on the Turkish network, and upon examining the results, it has been observed that each initial solution generation strategy provides improvements in problem instances with specific characteristics, such as the number of vehicles and nodes. We also observed lower objective function values for all medium- and large-sized test problems taken from the literature, highlighting the effectiveness of the proposed strategies.},
  archive      = {J_PEERJCS},
  author       = {Abdul Kader Kassoumeh and Zühal Kartal and Ahmet Arslan},
  doi          = {10.7717/peerj-cs.2840},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2840},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The effect of different initial solutions on the metaheuristic algorithms for the single allocation p-hub center and routing problem},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating cyber-physical systems with embedding technology for controlling autonomous vehicle driving. <em>PEERJCS</em>, <em>11</em>, e2823. (<a href='https://doi.org/10.7717/peerj-cs.2823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-physical systems (CPSs) in autonomous vehicles must handle highly dynamic and uncertain settings, where unanticipated impediments, shifting traffic conditions, and environmental changes all provide substantial decision-making issues. Deep reinforcement learning (DRL) has emerged as a strong tool for dealing with such uncertainty, yet current DRL models struggle to ensure safety and optimal behaviour in indeterminate settings due to the difficulties of understanding dynamic reward systems. To address these constraints, this study incorporates double deep Q networks (DDQN) to improve the agent’s adaptability under uncertain driving conditions. A structured reward system is established to accommodate real-time fluctuations, resulting in safer and more efficient decision-making. The study acknowledges the technological limitations of automobile CPSs and investigates hardware acceleration as a potential remedy in addition to algorithmic enhancements. Because of their post-manufacturing adaptability, parallel processing capabilities, and reconfigurability, field programmable gate arrays (FPGAs) are used to execute reinforcement learning in real-time. Using essential parameters, including collision rate, behaviour similarity, travel distance, speed control, total rewards, and timesteps, the suggested method is thoroughly tested in the TORCS Racing Simulator. The findings show that combining FPGA-based hardware acceleration with DDQN successfully improves computational efficiency and decision-making reliability, tackling significant issues brought on by uncertainty in autonomous driving CPSs. In addition to advancing reinforcement learning applications in CPSs, this work opens up possibilities for future investigations into real-world generalisation, adaptive reward mechanisms, and scalable hardware implementations to further reduce uncertainty in autonomous systems.},
  archive      = {J_PEERJCS},
  author       = {Manal Abdullah Alohali and Hamed Alqahtani and Abdulbasit Darem and Monir Abdullah and Yunyoung Nam and Mohamed Abouhawwash},
  doi          = {10.7717/peerj-cs.2823},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2823},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Integrating cyber-physical systems with embedding technology for controlling autonomous vehicle driving},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing analogy-based software cost estimation using grey wolf optimization algorithm. <em>PEERJCS</em>, <em>11</em>, e2794. (<a href='https://doi.org/10.7717/peerj-cs.2794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate software cost estimation (SCE) is a critical factor in the successful delivery of software projects, as highlighted by industry statistics indicating that only some of the projects comply with the predicted budget. Among the software estimation methods, analogy-based estimation (ABE) is one of the most popular ones. Although this method has been customized in recent years with the help of optimization algorithms to achieve better results, the use of more powerful optimization algorithms can be effective in achieving better results in software size estimation. This study presents an innovative approach to SCE that integrates the grey wolf optimization (GWO) algorithm to enhance the precision of ABE. The GWO algorithm, inspired by the hunting behavior and social hierarchy of grey wolves, is mathematically modeled and incorporated into the ABE approach. The key focus of this research is the optimization of the similarity function, a crucial component of the ABE, using both Euclidean and Manhattan distance measures. The article addresses the challenges in selecting an optimal similarity function and emphasizes the importance of proper feature weighting to improve estimation accuracy. The proposed GWO-based ABE method is rigorously evaluated on multiple software project datasets using cross-validation techniques. The performance of the GWO-based ABE is compared to other evolutionary algorithms based on widely accepted evaluation metrics. The results confirm that the integration of the GWO algorithm into ABE enhances estimation accuracy and model robustness. By optimizing feature weights in the similarity function, GWO-ABE effectively addresses key limitations of traditional analogy-based methods. The proposed approach demonstrates superior performance across multiple datasets, particularly under the Euclidean distance function, making it a reliable solution for software project cost estimation. Experimental evaluations show that GWO-ABE achieves notable improvements in key performance metrics, leading to reduced mean magnitude of relative error (MMRE), median magnitude of relative error (MdMRE), and higher percentage of prediction (PRED) compared to other ABE-customized methods. These findings highlight the role of metaheuristic optimization in improving software estimation techniques, contributing to more precise and efficient project planning and management.},
  archive      = {J_PEERJCS},
  author       = {Taghi Javdani Gandomani and Maedeh Dashti and Sadegh Ansaripour and Hazura Zulzalil},
  doi          = {10.7717/peerj-cs.2794},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2794},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing analogy-based software cost estimation using grey wolf optimization algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Informed decision-making in prioritising product variants. <em>PEERJCS</em>, <em>11</em>, e2778. (<a href='https://doi.org/10.7717/peerj-cs.2778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature models (FMs) play a crucial role in software product lines (SPLs) by representing variability and enabling the generation of diverse product configurations. However, the vast number of possible configurations often makes it challenging to identify the most suitable variant, especially when multiple criteria must be considered. Multi-criteria decision-making (MCDM) methods, such as analytic hierarchy process (AHP), technique for order of preference by similarity to ideal solution (TOPSIS), and VIseKriterijumska Optimizacija I Kompromisno Resenje (“multicriteria optimization and compromise solution”) (VIKOR), are effective for ranking configurations based on user-defined preferences. However, the application of disparate MCDM techniques to the same feature model with identical criteria can yield conflicting rankings, thereby complicating the decision-making process. To address this issue, we propose a novel framework that systematically integrates multiple MCDM methods to prioritise product configurations and provides informed decision support to reconcile ranking discrepancies. The framework automates the prioritisation process and offers a structured approach to explain differences between rankings, enhancing transparency and user confidence in the final selection. The framework’s effectiveness has been validated through real-world case studies, demonstrating its ability to streamline configuration prioritisation and support consistent, preference-driven decision-making in complex SPL environments.},
  archive      = {J_PEERJCS},
  author       = {Diana Borrego and Ángel Jesús Varela-Vaca and María Teresa Gómez-López and Rafael M. Gasca},
  doi          = {10.7717/peerj-cs.2778},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2778},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Informed decision-making in prioritising product variants},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent reflective surfaces in 5G and beyond: Optimizing uplink satellite connectivity for IoT. <em>PEERJCS</em>, <em>11</em>, e2726. (<a href='https://doi.org/10.7717/peerj-cs.2726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolving landscape of communication technologies, the integration of intelligent reflective surfaces (IRS) into uplink satellite communication for Internet of Things (IoT) ecosystems presents a promising solution to overcome traditional communication challenges. The purpose of this study is to explore the impact of IRS on enhancing signal quality and communication efficiency in satellite-supported IoT environments. This article adopts a simulation-based approach, using MATLAB and Simulink to model the uplink transmission of IoT devices to satellites with and without IRS assistance. The methodology focuses on analysing key performance metrics, including signal-to-noise ratio (SNR), spectral efficiency, signal strength, and interference mitigation. A reinforcement learning algorithm was employed to optimise IRS phase shifts and beamforming to maximise communication performance. The findings reveal that the integration of IRS leads to significant improvements in SNR, spectral efficiency, and overall signal quality, with a 2 dB increase in SNR and enhanced data transmission rates compared to non-IRS systems. IRS also mitigates interference and extends the coverage area of satellite networks. These results demonstrate the practical implications of IRS technology, which can be applied in scenarios such as smart cities, remote sensing, and disaster recovery, where reliable satellite communication is crucial. The study highlights the strategic importance of IRS in revolutionising IoT-satellite communication systems and sets the foundation for future work on scaling IRS technology for broader applications.},
  archive      = {J_PEERJCS},
  author       = {Callistus Odinaka Obidiozor and Adeeb Sait and Tawfik Al-Hadhrami and Eman H. Alkhammash and Faisal Saeed},
  doi          = {10.7717/peerj-cs.2726},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2726},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Intelligent reflective surfaces in 5G and beyond: Optimizing uplink satellite connectivity for IoT},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anime popularity prediction before huge investments: A multimodal approach using deep learning. <em>PEERJCS</em>, <em>11</em>, e2715. (<a href='https://doi.org/10.7717/peerj-cs.2715'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Japanese anime industry, predicting whether an upcoming product will be popular is crucial. This article introduces one of the most comprehensive free datasets for predicting anime popularity using only features accessible before huge investments, relying solely on freely available internet data and adhering to rigorous standards based on real-life experiences. To explore this dataset and its potential, a deep neural network architecture incorporating GPT-2 and ResNet-50 is proposed. The model achieved a best mean squared error (MSE) of 0.012, significantly surpassing a benchmark with traditional methods of 0.415, and a best R-square (R2) score of 0.142, outperforming the benchmark of −37.591. The aim of this study is to explore the scope and impact of features available before huge investments in relation to anime popularity. For that reason, and complementing the MSE and R2 metrics, Pearson and Spearman correlation coefficients are used. The best results, with Pearson at 0.382 and Spearman at 0.362, along with a well-fitted learning curves, suggests that while these features are relevant, they are not decisive for determining anime popularity and they likely interacts with additional features accessible after further investments. This is one of the first multimodal approaches to address this kind of tasks, aiming to support an entertainment industry by helping to avoid financial failures and guide successful production strategies.},
  archive      = {J_PEERJCS},
  author       = {Jesús Armenta-Segura and Grigori Sidorov},
  doi          = {10.7717/peerj-cs.2715},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2715},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Anime popularity prediction before huge investments: A multimodal approach using deep learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative segmentation and classification for enhanced crop disease diagnosis using optimized hybrid U-nets model. <em>PEERJCS</em>, <em>11</em>, e2543. (<a href='https://doi.org/10.7717/peerj-cs.2543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The major challenges that the agricultural sector faces are that with the kind of methodologies that exist, gross limitations may occur to the exact diagnosis of crop diseases. They are unable to achieve correct precision in disease classification, relatively lower accuracy, and delayed response time—all these obstacles result in a deficiency in effectual disease management and control. Our research proposes a new framework instigated and developed to improve crop disease detection and classification by multifaceted analysis. In the core of our methodology is the implementation of adaptive anisotropic diffusion for the denoising of obtained agro images, therefore making it a step towards assurance in data quality. Along with this is the use of a Fuzzy U-Net++ model for image segmentation, whereby fuzzy decisions in generously instill an increase in performance for image segmentation. Feature selection itself is innovated by the introduction of the Moving Gorilla Remora Algorithm (MGRA) combined with convolutional operations, setting a new benchmark in the selection of optimal features pertaining to disease identification operations. To further refine this model, classification is adeptly handled by a process inspired by the LeNet architecture, significantly improving identification against various diseases. Our approach’s performance is therefore strongly assessed through a number of renowned datasets, such as PlantVillage and PlantDoc, on which test metrics show superior performance: 8.5% improvement in disease classification precision, 8.3% higher accuracy, 9.4% improved recall, with a reduction in time delay by 4.5%, area under the curve (AUC) increasing by 5.9%, a 6.5% improvement in specificity, far ahead of other methods. This work not only sets new standards in crop disease analysis but also opens possibilities for the preemptive measures to come in agricultural health, promising a future where crop management is more effective and efficient. Our results thus have implications that reach beyond the immediate benefits accruable from improved diagnosis of diseases. It is a harbinger of a new era in agricultural technology where precision, accuracy, and timeliness will meet to enhance crop resilience and yield.},
  archive      = {J_PEERJCS},
  author       = {Malathi Chilakalapudi and Sheela Jayachandran},
  doi          = {10.7717/peerj-cs.2543},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2543},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Iterative segmentation and classification for enhanced crop disease diagnosis using optimized hybrid U-nets model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic detection of teacher behavior in classroom videos using AlphaPose and faster R-CNN algorithms. <em>PEERJCS</em>, <em>11</em>, e2933. (<a href='https://doi.org/10.7717/peerj-cs.2933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an automated classification framework for evaluating teacher behavior in classroom settings by integrating AlphaPose and Faster region-based convolutional neural networks (R-CNN) algorithms. The method begins by applying AlphaPose to classroom video footage to extract detailed skeletal pose information of both teachers and students across individual frames. These pose-based features are subsequently processed by a Faster R-CNN model, which classifies teacher behavior into appropriate or inappropriate categories. The approach is validated on the Classroom Behavior (PCB) dataset, comprising 74 video clips and 51,800 annotated frames. Experimental results indicate that the proposed system achieves an accuracy of 74.89% in identifying inappropriate behaviors while also reducing manual behavior logging time by 47% and contributing to a 63% decrease in such behaviors. The findings highlight the potential of computer vision techniques for scalable, objective, and real-time classroom behavior analysis, offering a viable tool for enhancing educational quality and teacher performance monitoring.},
  archive      = {J_PEERJCS},
  author       = {Jing Huang and Harwati Hashim and Helmi Norman and Mohammad Hafiz Zaini and Xiaojun Zhang},
  doi          = {10.7717/peerj-cs.2933},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2933},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automatic detection of teacher behavior in classroom videos using AlphaPose and faster R-CNN algorithms},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The application of blockchain technology in data trading: A systematic review. <em>PEERJCS</em>, <em>11</em>, e2925. (<a href='https://doi.org/10.7717/peerj-cs.2925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of exponential data growth, the imperative to establish secure and efficient data trading mechanisms has become paramount. While traditional centralized architectures present critical limitations in security and operational efficiency, the emergence of blockchain technology offers transformative potential for decentralized solutions. This study conducts a systematic literature review to critically examine blockchain’s evolving role in data trading ecosystems. Adhering to the PRISMA 2020 framework, we analyzed 18 rigorously selected studies from an initial pool of 164 Web of Science publications identified through “data trading” and “blockchain” keyword searches. Our analysis reveals three principal findings: first, current blockchain implementations predominantly cluster within computer science applications, indicating disciplinary concentration. Second, technical development emphasizes solution-oriented systems over theoretical model construction, suggesting an application-prioritized research paradigm. Third, we identify persistent challenges across three critical dimensions: (i) security-efficiency paradox in decentralized architectures, (ii) transparency-privacy equilibrium maintenance, and (iii) scalability constraints under high-concurrency scenarios. This study aims to offer in-depth insights into blockchain’s potential applications in data trading and future research directions.},
  archive      = {J_PEERJCS},
  author       = {Wei Xiong and Huaibin Shao and Hong Ge},
  doi          = {10.7717/peerj-cs.2925},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2925},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The application of blockchain technology in data trading: A systematic review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning model for gastrointestinal polyp segmentation. <em>PEERJCS</em>, <em>11</em>, e2924. (<a href='https://doi.org/10.7717/peerj-cs.2924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the biggest hazards to cancer-related mortality globally is colorectal cancer, and improved patient outcomes are greatly influenced by early identification. Colonoscopy is a highly effective screening method, yet segmentation and detection remain challenging aspects due to the heterogeneity and variability of readers’ interpretations of polyps. In this work, we introduce a novel deep learning architecture for gastrointestinal polyp segmentation in the Kvasir-SEG dataset. Our method employs an encoder-decoder structure with a pre-trained ConvNeXt model as the encoder to learn multi-scale feature representations. The feature maps are passed through a ConvNeXt Block and then through a decoder network consisting of three decoder blocks. Our key contribution is the employment of a cross-attention mechanism that creates shortcut connections between the decoder and encoder to maximize feature retention and reduce information loss. In addition, we introduce a Residual Transformer Block in the decoder that learns long-term dependency by using self-attention mechanisms and enhance feature representations. We evaluate our model on the Kvasir-SEG dataset, achieving a Dice coefficient of 0.8715 and mean intersection over union (mIoU) of 0.8021. Our methodology demonstrates state-of-the-art performance in gastrointestinal polyp segmentation and its feasibility of being used as part of clinical pipelines to assist with automated detection and diagnosis of polyps.},
  archive      = {J_PEERJCS},
  author       = {Zitong Wang and Zeyi Wang and Pengyu Sun},
  doi          = {10.7717/peerj-cs.2924},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2924},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning model for gastrointestinal polyp segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining convolutional neural network with transformer to improve YOLOv7 for gas plume detection and segmentation in multibeam water column images. <em>PEERJCS</em>, <em>11</em>, e2923. (<a href='https://doi.org/10.7717/peerj-cs.2923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multibeam bathymetry has become an effective underwater target detection method by using echo signals to generate a high-resolution water column image (WCI). However, the gas plume in the image is often affected by the seafloor environment and exhibits sparse texture and changing motion, making traditional detection and segmentation methods more time-consuming and labor-intensive. The emergence of convolutional neural networks (CNNs) alleviates this problem, but the local feature extraction of the convolutional operations, while capturing detailed information well, cannot adapt to the elongated morphology of the gas plume target, limiting the improvement of the detection and segmentation accuracy. Inspired by the transformer’s ability to achieve global modeling through self-attention, we combine CNN with the transformer to improve the existing YOLOv7 (You Only Look Once version 7) model. First, we sequentially reduce the ELAN (Efficient Layer Aggregation Networks) structure in the backbone network and verify that using the enhanced feature extraction module only in the deep network is more effective in recognising the gas plume targets. Then, the C-BiFormer module is proposed, which can achieve effective collaboration between local feature extraction and global semantic modeling while reducing computing resources, and enhance the multi-scale feature extraction capability of the model. Finally, two different depths of networks are designed by stacking C-BiFormer modules with different numbers of layers. This improves the receptive field so that the model’s detection and segmentation accuracy achieve different levels of improvement. Experimental results show that the improved model is smaller in size and more accurate compared to the baseline.},
  archive      = {J_PEERJCS},
  author       = {Wenguang Chen and Xiao Wang and Junjie Chen and Jialong Sun and Guozhen Zha},
  doi          = {10.7717/peerj-cs.2923},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2923},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Combining convolutional neural network with transformer to improve YOLOv7 for gas plume detection and segmentation in multibeam water column images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BlockDroid: Detection of android malware from images using lightweight convolutional neural network models with ensemble learning and blockchain for mobile devices. <em>PEERJCS</em>, <em>11</em>, e2918. (<a href='https://doi.org/10.7717/peerj-cs.2918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increase in the volume and diversity of malware targeting Android systems, research on detecting this harmful software is steadily growing. Traditional malware detection studies require significant human intervention and resource consumption to analyze all malware files. Moreover, malware developers have developed polymorphism and code obfuscation techniques to evade traditional signature-based detection approaches used by antivirus companies. Consequently, traditional methods have become increasingly inadequate for malware detection. So far, many machine learning methods have been successfully applied to address the issue of malware detection. Recent efforts in this area have turned to deep learning methods. Because these methods can automatically extract meaningful features from data and efficiently learn complex relationships, they can achieve better performance in malware detection as well as in solving many other problems. This article presents BlockDroid, an approach that combines convolutional neural network (CNN) models, ensemble learning, and blockchain technology to increase the accuracy and efficiency of malware detection for mobile devices. By converting Android DEX files into image data, BlockDroid leverages the superior image analysis capabilities of CNN models to discern patterns indicative of malware. The CICMalDroid 2020 dataset, comprising 13,077 applications, was utilized to create a balanced dataset of 3,590 images, with an equal number of benign and malware instances. The proposed detection system was developed using lightweight models, including EfficientNetB0, MobileNetV2, and a custom model as CNN models. Experimental studies were conducted by applying both individual models and the proposed BlockDroid system to our dataset. The empirical results illustrate that BlockDroid surpasses the performance of the individual models, demonstrating a substantial accuracy rate of 97.38%. Uniquely, BlockDroid integrates blockchain technology to record the predictions made by the malware detection model, thereby eliminating the need for re-analysis of previously evaluated applications and ensuring more efficient resource utilization. Our approach offers a promising and innovative strategy for effective and efficient Android malware detection.},
  archive      = {J_PEERJCS},
  author       = {Emre Şafak and İbrahim Alper Doğru and Necaattin Barışçı and İsmail Atacak},
  doi          = {10.7717/peerj-cs.2918},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2918},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {BlockDroid: Detection of android malware from images using lightweight convolutional neural network models with ensemble learning and blockchain for mobile devices},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative performance of twelve machine learning models in predicting COVID-19 mortality risk in children: A population-based retrospective cohort study in brazil. <em>PEERJCS</em>, <em>11</em>, e2916. (<a href='https://doi.org/10.7717/peerj-cs.2916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has catalyzed the application of advanced digital technologies such as artificial intelligence (AI) to predict mortality in adult patients. However, the development of machine learning (ML) models for predicting outcomes in children and adolescents with COVID-19 remains limited. This study aimed to evaluate the performance of multiple machine learning models in forecasting mortality among hospitalized pediatric COVID-19 patients. In this cohort study, we used the SIVEP-Gripe dataset, a public resource maintained by the Ministry of Health, to track severe acute respiratory syndrome (SARS) in Brazil. To create subsets for training and testing the machine learning (ML) models, we divided the primary dataset into three parts. Using these subsets, we developed and trained 12 ML algorithms to predict the outcomes. We assessed the performance of these models using various metrics such as accuracy, precision, sensitivity, recall, and area under the receiver operating characteristic curve (AUC). Among the 37 variables examined, 24 were found to be potential indicators of mortality, as determined by the chi-square test of independence. The Logistic Regression (LR) algorithm achieved the highest performance, with an accuracy of 92.5% and an AUC of 80.1%, on the optimized dataset. Gradient boosting classifier (GBC) and AdaBoost (ADA), closely followed the LR algorithm, producing similar results. Our study also revealed that baseline reduced oxygen saturation, presence of comorbidities, and older age were the most relevant factors in predicting mortality in children and adolescents hospitalized with SARS-CoV-2 infection. The use of ML models can be an asset in making clinical decisions and implementing evidence-based patient management strategies, which can enhance patient outcomes and overall quality of medical care. LR, GBC, and ADA models have demonstrated efficiency in accurately predicting mortality in COVID-19 pediatric patients.},
  archive      = {J_PEERJCS},
  author       = {Adriano Lages dos Santos and Maria Christina L. Oliveira and Enrico A. Colosimo and Robert H. Mak and Clara C. Pinhati and Stella C. Gallante and Hercílio Martelli-Júnior and Ana Cristina Simões e Silva and Eduardo A. Oliveira},
  doi          = {10.7717/peerj-cs.2916},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2916},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparative performance of twelve machine learning models in predicting COVID-19 mortality risk in children: A population-based retrospective cohort study in brazil},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing east-west interface security in heterogeneous SDN via blockchain. <em>PEERJCS</em>, <em>11</em>, e2914. (<a href='https://doi.org/10.7717/peerj-cs.2914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defined networking (SDN) increasingly integrates multiple controllers from diverse vendors to enhance network scalability, flexibility, and reliability. However, such heterogeneous deployments pose significant security threats, especially at the east-west interface which is connecting these controllers. Existing solutions are inadequate for ensuring robust protection across multi-vendor SDN environments as most of them are meant to a specific type of attacks, use centralized solution, or designed for homogeneous SDN environments. This study proposes a blockchain-based security framework to address existing security gaps within heterogeneous SDN environments. The framework establishes a decentralized, robust, and interoperable security layer for distributed SDN controllers. By utilizing the Ethereum blockchain with customized smart contract-based checks, the proposed approach enables mutual authentication among controllers, secures data exchange, and controls network access. The framework effectively mitigates common SDN threats such as distributed denial-of-service (DDoS), man-in-the-middle (MitM), false data injection, and unauthorized access. Experimental results highlight the practicality of the solution, achieving a stable throughput of approximately 20 transactions per second with an average authentication latency of 28–40 ms. These results demonstrate that the proposed framework not only enhances inter-controller communication security but also maintains the network performance, making it a reliable and scalable solution for real-world SDN deployments.},
  archive      = {J_PEERJCS},
  author       = {Hamad Alrashede and Fathy Eassa and Abdullah Marish Ali and Hosam Aljihani and Faisal Albalwy},
  doi          = {10.7717/peerj-cs.2914},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2914},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing east-west interface security in heterogeneous SDN via blockchain},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TARGE: Large language model-powered explainable hate speech detection. <em>PEERJCS</em>, <em>11</em>, e2911. (<a href='https://doi.org/10.7717/peerj-cs.2911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of user-generated content on social networking sites has intensified the challenge of accurately and efficiently detecting inflammatory and discriminatory speech at scale. Traditional manual moderation methods are impractical due to the sheer volume and complexity of online discourse, necessitating automated solutions. However, existing deep learning models for hate speech detection typically function as black-box systems, providing binary classifications without interpretable insights into their decision-making processes. This opacity significantly limits their practical utility, particularly in nuanced content moderation tasks. To address this challenge, our research explores leveraging the advanced reasoning and knowledge integration capabilities of state-of-the-art language models, specifically Mistral-7B, to develop transparent hate speech detection systems. We introduce a novel framework wherein large language models (LLMs) generate explicit rationales by identifying and analyzing critical textual features indicative of hate speech. These rationales are subsequently integrated into specialized classifiers designed to perform explainable content moderation. We rigorously evaluate our methodology on multiple benchmark English-language social media datasets. Results demonstrate that incorporating LLM-generated explanations significantly enhances both the interpretability and accuracy of hate speech detection. This approach not only identifies problematic content effectively but also clearly articulates the analytical rationale behind each decision, fulfilling the critical demand for transparency in automated content moderation.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Haseeb Hashir and Memoona and Sung Won Kim},
  doi          = {10.7717/peerj-cs.2911},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2911},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TARGE: Large language model-powered explainable hate speech detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Native language identification from text using a fine-tuned GPT-2 model. <em>PEERJCS</em>, <em>11</em>, e2909. (<a href='https://doi.org/10.7717/peerj-cs.2909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Native language identification (NLI) is a critical task in computational linguistics, supporting applications such as personalized language learning, forensic analysis, and machine translation. This study investigates the use of a fine-tuned GPT-2 model to enhance NLI accuracy. Using the NLI-PT dataset, we preprocess and fine-tune GPT-2 to classify the native language of learners based on their Portuguese-written texts. Our approach leverages deep learning techniques, including tokenization, embedding extraction, and multi-layer transformer-based classification. Experimental results show that our fine-tuned GPT-2 model significantly outperforms traditional machine learning methods (e.g., SVM, Random Forest) and other pre-trained language models (e.g., BERT, RoBERTa, BioBERT), achieving a weighted F1 score of 0.9419 and an accuracy of 94.65%. These results show that large transformer models work well for native language identification and can help guide future research in personalized language tools and artificial intelligence (AI)-based education.},
  archive      = {J_PEERJCS},
  author       = {Yuzhe Nie},
  doi          = {10.7717/peerj-cs.2909},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2909},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Native language identification from text using a fine-tuned GPT-2 model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proto-caps: Interpretable medical image classification using prototype learning and privileged information. <em>PEERJCS</em>, <em>11</em>, e2908. (<a href='https://doi.org/10.7717/peerj-cs.2908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable artificial intelligence (xAI) is becoming increasingly important as the need for understanding the model’s reasoning grows when applying them in high-risk areas. This is especially crucial in the field of medicine, where decision support systems are utilised to make diagnoses or to determine appropriate therapies. Here it is essential to provide intuitive and comprehensive explanations to evaluate the system’s correctness. To meet this need, we have developed Proto-Caps, an intrinsically explainable model for image classification. It explains its decisions by providing visual prototypes that resemble specific appearance features. These characteristics are predefined by humans, which on the one hand makes them understandable and on the other hand leads to the model basing its decision on the same features as the human expert. On two public datasets, this method shows better performance compared to existing explainable approaches, despite the additive explainability modality through the visual prototypes. In addition to the performance evaluations, we conducted an analysis of truthfulness by examining the joint information between the target prediction and its explanation output. This was done in order to ensure that the explanation actually reasons the target classification. Through extensive hyperparameter studies, we also found optimal model settings, providing a starting point for further research. Our work emphasises the prospects of combining xAI approaches for greater explainability and demonstrates that incorporating explainability does not necessarily lead to a loss of performance.},
  archive      = {J_PEERJCS},
  author       = {Luisa Gallée and Catharina Silvia Lisson and Timo Ropinski and Meinrad Beer and Michael Götz},
  doi          = {10.7717/peerj-cs.2908},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2908},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Proto-caps: Interpretable medical image classification using prototype learning and privileged information},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Yoga pose recognition using dual structure convolutional neural network. <em>PEERJCS</em>, <em>11</em>, e2907. (<a href='https://doi.org/10.7717/peerj-cs.2907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular form of physical and mental exercise, the correct execution of yoga movements is crucial. With the development of deep learning technologies, automatic recognition of yoga postures has become popular. To recognize five different yoga postures, this article proposed a dual structure convolutional neural network with a feature fusion function, which consists of the convolutional neural network A (CNN A) and convolutional neural network B (CNN B). Among them, the structure CNN A observes different channels finding the global feature of yoga images, and the structure CNN B calculates the depth information in each pixel of the yoga images. Following that, the extracted global feature and local feature are fused by a feature fusion function of taking a matrix dot multiplication. Finally, the softmax layer accurately recognizes yoga postures based on the fused features. Experimental results show that the proposed model achieves 97.23% accuracy with 96.08% precision and defeats against the competitors in the recognition of yoga postures. Moreover, the feature fusion function is proved to be successful in terms of the recognition to yoga postures. We also find that the feature fusion with a matrix dot multiplication operation can significantly improve the recognition accuracy of yoga postures than that with a direct connection operation.},
  archive      = {J_PEERJCS},
  author       = {Xiang Meng and Zhaobing Liu},
  doi          = {10.7717/peerj-cs.2907},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2907},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Yoga pose recognition using dual structure convolutional neural network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive regularized spectral reduction for stabilizing ill-conditioned bone-conducted speech signals. <em>PEERJCS</em>, <em>11</em>, e2906. (<a href='https://doi.org/10.7717/peerj-cs.2906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bone-conducted (BC) speech signals are inherently challenging to analyze due to their wide frequency range, which leads to ill-conditioning in numerical analysis and linear prediction (LP) techniques. This ill-conditioning is primarily caused by the expansion of eigenvalues, which complicates the stability and accuracy of traditional methods. To address this issue, we propose a novel regularized spectral reduction (RSR) method, built upon the regularized least squares (RLS) framework. The RSR method compresses the frequency range of BC speech signals, effectively reducing eigenvalue spread and enhancing the robustness of LP analysis. Key to the RSR approach is a regularization parameter, fine-tuned iteratively to achieve optimal performance. Experimental results demonstrate that RSR significantly outperforms existing techniques in eigenvalue compression, resulting in more accurate LP analysis for both synthetic and real BC speech datasets. These improvements hold promise for applications in hearing aids, voice recognition systems, and speaker identification in noisy environments, where reliable BC speech analysis is critical.},
  archive      = {J_PEERJCS},
  author       = {Kanwar Muhammad Afaq and Ammar Amjad and Li-Chia Tai and Hsien-Tsung Chang},
  doi          = {10.7717/peerj-cs.2906},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2906},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Adaptive regularized spectral reduction for stabilizing ill-conditioned bone-conducted speech signals},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient deep learning model for classifying lung cancer images using normalized stain agnostic feature method and FastAI-2. <em>PEERJCS</em>, <em>11</em>, e2903. (<a href='https://doi.org/10.7717/peerj-cs.2903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Lung cancer has the highest global fatality rate, with diagnosis primarily relying on histological tissue sample analysis. Accurate classification is critical for treatment planning and patient outcomes. Methods This study develops a computer-assisted diagnosis system for non-small cell lung cancer histology classification, utilizing the FastAI-2 framework with a modified ResNet-34 architecture. The methodology includes stain normalization using LAB colour space for colour consistency, followed by deep learning-based classification. The proposed model is trained on the LC25000 dataset and compared with VGG11 and SqueezeNet1_1, demonstrating modified ResNet-34’s optimal balance between depth and performance. FastAI-2 enhances computational efficiency, enabling rapid convergence with minimal training time. Results The proposed system achieved 99.78% accuracy, confirming the effectiveness of automated lung cancer histopathology classification. This study highlights the potential of artificial intelligence (AI)-driven diagnostic tools to assist pathologists by improving accuracy, reducing workload, and enhancing decision-making in clinical settings.},
  archive      = {J_PEERJCS},
  author       = {Pranshu Saxena and Sanjay Kumar Singh and Mamoon Rashid and Sultan S. Alshamrani and Mrim M. Alnfiai},
  doi          = {10.7717/peerj-cs.2903},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2903},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Efficient deep learning model for classifying lung cancer images using normalized stain agnostic feature method and FastAI-2},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved hippopotamus optimization algorithm based on adaptive development and solution diversity enhancement. <em>PEERJCS</em>, <em>11</em>, e2901. (<a href='https://doi.org/10.7717/peerj-cs.2901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an improved hippopotamus optimization algorithm to address the limitations of the traditional hippopotamus optimization algorithm in terms of convergence performance and solution diversity in complex high-dimensional problems. Inspired by the natural behavior of hippopotamuses, this article introduces chaotic map initialization, an adaptive exploitation mechanism, and a solution diversity enhancement strategy based on the original algorithm. The chaotic map is employed to optimize the initial population distribution, thereby enhancing the global search capability. The adaptive exploitation mechanism dynamically adjusts the weights between the exploration and exploitation phases to balance global and local searches. The solution diversity enhancement is achieved through the introduction of nonlinear perturbations, which help the algorithm avoid being trapped in local optima. The proposed algorithm is validated on several standard benchmark functions (CEC17, CEC22), and the results demonstrate that the improved algorithm significantly outperforms the original hippopotamus optimization algorithm and other mainstream optimization algorithms in terms of convergence speed, solution accuracy, and global search ability. Moreover, statistical analysis further confirms the superiority of the improved algorithm in balancing exploration and exploitation, particularly when dealing with high-dimensional multimodal functions. This study provides new insights and enhancement strategies for the application of the hippopotamus optimization algorithm in solving complex optimization problems.},
  archive      = {J_PEERJCS},
  author       = {Shengyu Pei and Gang Sun and Lang Tong},
  doi          = {10.7717/peerj-cs.2901},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2901},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An improved hippopotamus optimization algorithm based on adaptive development and solution diversity enhancement},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-fidelity steganography in EEG signals using advanced transform-based methods. <em>PEERJCS</em>, <em>11</em>, e2900. (<a href='https://doi.org/10.7717/peerj-cs.2900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of digital health solutions and smart health devices (SHDs) ensures the continuity of personal biometric data while simultaneously raising concerns about their security and privacy. Consequently, the development of novel encryption techniques and data protection policies is crucial to comply with regulations such as The Health Insurance Portability and Accountability Act (HIPAA) and to safeguard against cyber threats. This study introduces a robust and efficient method for embedding private information into electroencephalogram (EEG) signals by employing the stationary wavelet transform (SWT), singular value decomposition (SVD), and tent map techniques. The proposed approach aims to increase embedding capacity while maintaining signal integrity, ensuring resilience against various forms of distortion, and achieving computational efficiency. Experiments were conducted on three publicly available EEG datasets (Graz A, DEAP, and Bonn), and performance was evaluated using widely recognized metrics, including peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), percentage root mean square difference (PRD), normalized cross-correlation (NCC), bit error rate (BER), and Euclidean distance (ED). The results indicate that the method preserves perceptual quality, achieving PSNR values above 60 dB and demonstrating minimal signal distortion. Robustness tests involving noise addition, random cropping, and low-pass filtering confirm the method’s high resilience, with BER approaching zero and NCC near unity. Moreover, the proposed method demonstrates significantly reduced hiding and extraction times compared to conventional approaches, enhancing its suitability for real-time, secure biomedical data transmission.},
  archive      = {J_PEERJCS},
  author       = {Enes Efe},
  doi          = {10.7717/peerj-cs.2900},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2900},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {High-fidelity steganography in EEG signals using advanced transform-based methods},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of the stages of alzheimer’s disease based on three-dimensional lightweight neural networks. <em>PEERJCS</em>, <em>11</em>, e2897. (<a href='https://doi.org/10.7717/peerj-cs.2897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease is a neurodegenerative disease that seriously threatens the life and health of the elderly. This study used three-dimensional lightweight neural networks to classify the stages of Alzheimer’s disease and explore the relationship between the stages and the variations of brain tissue. The study used CAT12 to preprocess magnetic resonance images of the brain and got three kinds of preprocessed images: standardized images, segmented standardized gray matter images, and segmented standardized white matter images. The three kinds of images were used to train four kinds of three-dimensional lightweight neural networks respectively, and the evaluation metrics of the neural networks are calculated. The accuracies of the neural networks for classifying the stages of Alzheimer’s disease (cognitively normal, mild cognitive impairment, Alzheimer’s disease) in the study are above 96%, and the precisions and recalls of classifying the three stages are above 94%. The study found that for the classification of cognitively normal, the best classification results can be obtained by training with the segmented standardized gray matter images, and for mild cognitive impairment and Alzheimer’s disease, the best classification results can be obtained by training with the standardized images. The study analyzed that in the process of cognitively normal to mild cognitive impairment, variations in the segmented standardized gray matter images are more obvious at the beginning, while variations in the segmented standardized white matter images are not obvious. As the disease progresses, variations in the segmented standardized white matter images tend to become more significant, and variations in the segmented standardized gray matter images and white matter images are both significant in the development of Alzheimer’s disease.},
  archive      = {J_PEERJCS},
  author       = {Jun Li and Juntong Liu and Yang Su and Jie Chang and Mingquan Ye},
  doi          = {10.7717/peerj-cs.2897},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2897},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Classification of the stages of alzheimer’s disease based on three-dimensional lightweight neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling and implementation of a real-time digital twin for the stewart platform with real-time trajectory computation. <em>PEERJCS</em>, <em>11</em>, e2892. (<a href='https://doi.org/10.7717/peerj-cs.2892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of a digital twin is increasingly acknowledged as an innovative and promising tool with significant potential in various end-use applications. At the heart of digital twin technology is the acquisition of real-time data from physical entities. However, the occurrence of disturbances necessitates the incorporation of resilience features within the digital twin architecture. The primary objective of this article is to develop resilient digital twins specifically for the Stewart platform. This work focuses on constructing the virtual component of the digital twin using MATLAB/Simulink and subsequently integrating this virtual model with its physical counterpart to establish a comprehensive digital twin system. Unlike other models, this system includes a motion trajectory computation module. This module is designed to receive signals from physical entities and convert them into motion trajectory data for input into the model, thereby aiming to accurately reflect the state of the physical entities under disruptive conditions. This functionality significantly enhances the reliability of the system beyond that of traditional digital twin systems. Furthermore, the article explores novel strategies and a framework for enhancing the resilience of the Stewart platform to disturbances.},
  archive      = {J_PEERJCS},
  author       = {Xiurui Ding and Jinsheng Xing},
  doi          = {10.7717/peerj-cs.2892},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2892},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Modeling and implementation of a real-time digital twin for the stewart platform with real-time trajectory computation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of the fusion of multimodal sentiment perception and physiological signals in chinese-english cross-cultural communication: Transformer approach incorporating self-attention enhancement. <em>PEERJCS</em>, <em>11</em>, e2890. (<a href='https://doi.org/10.7717/peerj-cs.2890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the acceleration of globalization, cross-cultural communication has become a crucial issue in various fields. Emotion, as an essential component of communication, plays a key role in improving understanding and interaction efficiency across different cultures. However, accurately recognizing emotions across cultural backgrounds remains a major challenge in affective computing, particularly due to limitations in multimodal feature fusion and temporal dependency modeling in traditional approaches. To address this, we propose the TAF-ATRM framework, which integrates Transformer and multi-head attention mechanisms for cross-cultural emotion recognition. Specifically, the framework employs bidirectional encoder representations from transformers (BERT) for semantic feature extraction from text, Mel-frequency Cepstral Coefficients (MFCC) and Residual Neural Network (ResNet) for capturing critical features from speech and facial expressions, respectively, thereby enhancing multimodal emotion recognition capability. To improve the fusion of multimodal data, the Transformer is utilized for temporal feature modeling, while multi-head attention reinforces feature representation by capturing complex inter-modal dependencies. The framework is evaluated on the MOSI and MOSEI datasets, where experimental results demonstrate that TAF-ATRM outperforms traditional methods in emotion classification accuracy and robustness, particularly in cross-cultural emotion recognition tasks. This study provides a strong technical foundation for future advancements in multimodal emotion analysis and cross-cultural affective computing.},
  archive      = {J_PEERJCS},
  author       = {Xin Bi and Tian Zhang},
  doi          = {10.7717/peerj-cs.2890},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2890},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Analysis of the fusion of multimodal sentiment perception and physiological signals in chinese-english cross-cultural communication: Transformer approach incorporating self-attention enhancement},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of lower limb torque: A novel hybrid method based on continuous wavelet transform and deep learning approach. <em>PEERJCS</em>, <em>11</em>, e2888. (<a href='https://doi.org/10.7717/peerj-cs.2888'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomechanical analysis of the human lower limbs plays a critical role in movement assessment, injury prevention, and rehabilitation guidance. Traditional gait analysis techniques, such as optical motion capture systems and biomechanical force platforms, are limited by high costs, operational complexity, and restricted applicability. In view of this, this study proposes a cost-effective and user-friendly approach that integrates inertial measurement units (IMUs) with a novel deep learning framework for real-time lower limb joint torque estimation. The proposed method combines time-frequency domain analysis through continuous wavelet transform (CWT) with a hybrid architecture comprising multi-head self-attention (MHSA), bidirectional long short-term memory (Bi-LSTM), and a one-dimensional convolutional residual network (1D Conv ResNet). This integration enhances feature extraction, noise suppression, and temporal dependency modeling, particularly for non-stationary and nonlinear signals in dynamic environments. Experimental validation on public datasets demonstrates high accuracy, with a root mean square error (RMSE) of 0.16 N·m/kg, Coefficient of Determination (R2) of 0.91, and Pearson correlation coefficient of 0.95. Furthermore, the framework outperforms existing models in computational efficiency and real-time applicability, achieving a single-cycle inference time of 152.6 ms, suitable for portable biomechanical monitoring systems.},
  archive      = {J_PEERJCS},
  author       = {Shu Xu and Tao Wang and Zenghui Ding and Yu Wang and Tongsheng Wan and Dezhang Xu and Xianjun Yang and Ting Sun and Meng Li},
  doi          = {10.7717/peerj-cs.2888},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2888},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Estimation of lower limb torque: A novel hybrid method based on continuous wavelet transform and deep learning approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperdimensional computing in biomedical sciences: A brief review. <em>PEERJCS</em>, <em>11</em>, e2885. (<a href='https://doi.org/10.7717/peerj-cs.2885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperdimensional computing (HDC, also known as vector-symbolic architectures—VSA) is an emerging computational paradigm that relies on dealing with vectors in a high-dimensional space to represent and combine every kind of information. It finds applications in a wide array of fields including bioinformatics, natural language processing, machine learning, artificial intelligence, and many other scientific disciplines. Here we introduced the basic foundations of the HDC, focusing on its application to biomedical sciences, with a particular emphasis to bioinformatics, cheminformatics, and medical informatics, providing a critical and comprehensive review of the current HDC landscape, highlighting pros and cons of applying this computational paradigm in these specific scientific domains. In this study, we first selected around forty scientific articles on hyperdimensional computing applied to biomedical data existing in the literature, and then analyzed key aspects of their studies, such as vector construction, data encoding, programming language employed, and other features. We also counted how many of these scientific articles are open access, how many have public software code available, how many groups of authors, journals, and conferences are most present among them. Finally, we discussed the advantages and limitations of the HDC approach, outlining potential future directions and open challenges for the adoption of HDC in biomedical sciences. To the best of our knowledge, our review is the first open brief survey on this topic among the biomedical sciences, and therefore we believe it can be of interest and useful for the readership.},
  archive      = {J_PEERJCS},
  author       = {Fabio Cumbo and Davide Chicco},
  doi          = {10.7717/peerj-cs.2885},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2885},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hyperdimensional computing in biomedical sciences: A brief review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RMIS-net: A fast medical image segmentation network based on multilayer perceptron. <em>PEERJCS</em>, <em>11</em>, e2882. (<a href='https://doi.org/10.7717/peerj-cs.2882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation, a pivotal component in diagnostic workflows and therapeutic decision-making, plays a critical role in clinical applications ranging from pathological diagnosis to surgical navigation and treatment evaluation. To address the persistent challenges of computational complexity and efficiency limitations in existing methods, we propose RMIS-Net—an innovative lightweight segmentation network with three core components: a convolutional layer for preliminary feature extraction, a shift-based fully connected layer for parameter-efficient spatial modeling, and a tokenized multilayer perceptron for global context capture. This architecture achieves significant parameter reduction while enhancing local feature representation through optimized shift operations. The network incorporates layer normalization and dropout regularization to ensure training stability, complemented by Gaussian error linear unit (GELU) activation functions for improved non-linear modeling. To further refine segmentation precision, we integrate residual connections for gradient flow optimization, a Dice loss function for class imbalance mitigation, and bilinear interpolation for accurate mask reconstruction. Comprehensive evaluations on two benchmark datasets (2018 Data Science Bowl for cellular structure segmentation and ISIC-2018 for lesion boundary delineation) demonstrate RMIS-Net’s superior performance, achieving state-of-the-art metrics including an average F1-score of 0.91 and mean intersection-over-union of 0.82. Remarkably, the proposed architecture requires only 0.03 s per image inference while achieving 27× parameter compression, 10× acceleration in inference speed, and 53× reduction in computational complexity compared to conventional approaches, establishing new benchmarks for efficient yet accurate medical image analysis.},
  archive      = {J_PEERJCS},
  author       = {Binbin Zhang and Guoliang Xu and Yiying Xing and Nanjie Li and Deguang Li},
  doi          = {10.7717/peerj-cs.2882},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2882},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RMIS-net: A fast medical image segmentation network based on multilayer perceptron},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel attention-based deep learning model for improving sentiment classification after the case of the 2023 Kahramanmaras/Turkey earthquake on twitter. <em>PEERJCS</em>, <em>11</em>, e2881. (<a href='https://doi.org/10.7717/peerj-cs.2881'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter has emerged as one of the most widely used platforms for sharing information and updates. As users freely express their thoughts and emotions, a vast amount of data is generated, particularly in the aftermath of disasters, which can be collected quickly and directly from individuals. Traditionally, earthquake impact assessments have been conducted through field studies by non-governmental organizations (NGOs), a process that is often time-consuming and costly. Sentiment analysis (SA) on Twitter presents a valuable research area, enabling the extraction and interpretation of real-time public perceptions. In recent years, attention-based methods in deep learning networks have gained significant attention among researchers. This study proposes a novel sentiment classification model, MConv-BiLSTM-GAM, which leverages an attention mechanism to analyze public sentiment following the 7.8 and 7.5 Mw earthquakes that struck Kahramanmaraş, Turkey. The model employs the FastText word embedding technique to convert tweets into vector representations. These vectorized inputs are then processed by a hybrid model integrating convolutional neural networks (CNNs) and recurrent neural networks (RNNs) with a global attention mechanism. This ensures careful consideration of semantic dependencies in sentiment classification. The proposed model operates in three stages: (i) MConv—Local Contextual Feature Extraction, (ii) bidirectional long short-term memory (BiLSTM)—sequence learning, and (iii) Global Attention Mechanism (GAM)—Attention Mechanism. Experimental results demonstrate that the model achieves an accuracy of 93.32%, surpassing traditional deep learning models in the literature by approximately 3%. This research aims to provide objective insights to policymakers and decision-makers, facilitating adequate support for individuals and communities affected by disasters. Moreover, analyzing public sentiment during earthquakes contributes to understanding societal responses and emotional trends in disaster scenarios.},
  archive      = {J_PEERJCS},
  author       = {Serpil Aslan and Muhammed Yildirim},
  doi          = {10.7717/peerj-cs.2881},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2881},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel attention-based deep learning model for improving sentiment classification after the case of the 2023 Kahramanmaras/Turkey earthquake on twitter},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RBI: A novel algorithm for regulatory-metabolic network model in designing the optimal mutant strain. <em>PEERJCS</em>, <em>11</em>, e2880. (<a href='https://doi.org/10.7717/peerj-cs.2880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last 20 years, researchers have proposed regulatory-metabolic network models to integrate gene regulatory networks (GRNs) and metabolic networks in in silico metabolic engineering, aiming to enhance the production rate of desired metabolites. However, the proposed models are unable to comprehensively include the Boolean rules in the empirical gene regulatory networks (GRNs) and gene-protein-reaction (GPR) interactions. Thus, the types of gene interactions, such as inhibition and activation, are disregarded from the analysis. This may result in sub-optimal model performance. Hence, this article presented a novel model using reliability theory to include Boolean rules in empirical GRNs and GPR rules in the integrating process. The proposed algorithm of this model is termed as a reliability-based integrating (RBI) algorithm. The suggested algorithm had three variants: RBI-T1, RBI-T2, and RBI-T3. The performance of the RBI algorithms was assessed by comparing them with the existing algorithms, using empirical results and validated transcription factors (TF) knockout schemes, and their complexity time was identified. Also, the RBI method was implemented in the design of optimal mutant strains of Escherichia coli and Saccharomyces cerevisiae. The simulation results indicated that the effectiveness and efficiency of the RBI algorithms are adequately strong and competitive relative to the existing algorithms. Furthermore, the RBI algorithm effectively identified eight schemes capable of enhancing succinate and ethanol production rates by maintaining the survival of microbial strains. Those results demonstrated that the RBI algorithms are recommended for the construction of optimum mutant strains in in silico metabolic engineering.},
  archive      = {J_PEERJCS},
  author       = {Ridho Ananda and Kauthar Mohd Daud and Suhaila Zainudin},
  doi          = {10.7717/peerj-cs.2880},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2880},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RBI: A novel algorithm for regulatory-metabolic network model in designing the optimal mutant strain},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GFADE: Generalized feature adaptation and discrimination enhancement for deepfake detection. <em>PEERJCS</em>, <em>11</em>, e2879. (<a href='https://doi.org/10.7717/peerj-cs.2879'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of deep generative techniques, such as generative adversarial networks (GANs), the creation of realistic fake images and videos has become increasingly accessible, raising significant security and privacy concerns. Although existing deepfake detection methods perform well within a single dataset, they often experience substantial performance degradation when applied across datasets or manipulation types. To address this challenge, we propose a novel deepfake detection framework that combines multiple loss functions and the MixStyle technique. By integrating Cross-Entropy Loss, ArcFace loss, and Focal Loss, our model enhances its discriminative power to better handle complex forgery characteristics and effectively mitigate data imbalance. Additionally, the MixStyle technique introduces diverse visual styles during training, further improving the model’s generalization across different datasets and manipulation scenarios. Experimental results demonstrate that our method achieves superior detection accuracy across a range of cross-dataset and cross-manipulation tests, significantly improving model robustness and generalizability.},
  archive      = {J_PEERJCS},
  author       = {ZhiYong Tian and Junkai Yi},
  doi          = {10.7717/peerj-cs.2879},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2879},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GFADE: Generalized feature adaptation and discrimination enhancement for deepfake detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of unsupervised static topic models’ emergence detection ability. <em>PEERJCS</em>, <em>11</em>, e2875. (<a href='https://doi.org/10.7717/peerj-cs.2875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting emerging topics is crucial for understanding research trends, technological advancements, and shifts in public discourse. While unsupervised topic modeling techniques such as Latent Dirichlet allocation (LDA), BERTopic, and CoWords clustering are widely used for topic extraction, their ability to retrospectively detect emerging topics without relying on ground truth labels has not been systematically compared. This gap largely stems from the lack of a dedicated evaluation metric for measuring emergence detection. In this study, we introduce a quantitative evaluation metric to assess the effectiveness of topic models in detecting emerging topics. We evaluate three topic modeling approaches using both qualitative analysis and our proposed emergence detection metric. Our results indicate that, qualitatively, CoWords identifies emerging topics earlier than LDA and BERTopics. Quantitatively, our evaluation metric demonstrates that LDA achieves an average F1 score of 80.6% in emergence detection, outperforming BERTopic by 24.0%. These findings highlight the strengths and limitations of different topic models for emergence detection, while our proposed metric provides a robust framework for future benchmarking in this area.},
  archive      = {J_PEERJCS},
  author       = {Xue Li and Ciro D. Esposito and Paul Groth and Jonathan Sitruk and Balazs Szatmari and Nachoem Wijnberg},
  doi          = {10.7717/peerj-cs.2875},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2875},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evaluation of unsupervised static topic models’ emergence detection ability},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technological trends in epidemic intelligence for infectious disease surveillance: A systematic literature review. <em>PEERJCS</em>, <em>11</em>, e2874. (<a href='https://doi.org/10.7717/peerj-cs.2874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background This research focuses on improving epidemic monitoring systems by incorporating advanced technologies to enhance the surveillance of diseases more effectively than before. Considering the drawbacks associated with surveillance methods in terms of time consumption and efficiency, issues highlighted in this study includes the integration of Artificial Intelligence (AI) in early detection, decision support and predictive modeling, big data analytics in data sharing, contact tracing and countering misinformation, Internet of Things (IoT) devices in real time disease monitoring and Geographic Information Systems (GIS) for geospatial artificial intelligence (GeoAI) applications and disease mapping. The increasing intricacy and regular occurrence of disease outbreaks underscore the pressing necessity for improvements in public health monitoring systems. This research delves into the developments and their utilization in detecting and handling infectious diseases while exploring how these progressions contribute to decision making and policy development, in public healthcare. Methodology This review systematically analyzes how technological tools are being used in epidemic monitoring by conducting a structured search across online literature databases and applying eligibility criteria to identify relevant studies on current technological trends in public health surveillance. Results The research reviewed 69 articles from 2019 to 2023 focusing on emerging trends in epidemic intelligence. Most of the studies emphasized the integration of artificial intelligence with technologies like big data analytics, geographic information systems, and the Internet of Things for monitoring infectious diseases. Conclusions The expansion of publicly accessible information on the internet has opened a new pathway for epidemic intelligence. This study emphasizes the importance of integrating information technology tools such as AI, big data analytics, GIS, and the IoT in epidemic intelligence surveillance to effectively track infectious diseases. Combining these technologies helps public health agencies in detecting and responding to health threats.},
  archive      = {J_PEERJCS},
  author       = {Hazeeqah Amny Kamarul Aryffin and Murtadha Arif Bin Sahbudin and Sakinah Ali Pitchay and Azni Haslizan Abhalim and Ilfita Sahbudin},
  doi          = {10.7717/peerj-cs.2874},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2874},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Technological trends in epidemic intelligence for infectious disease surveillance: A systematic literature review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning for internet of things (IoT) device identification: A comparative study. <em>PEERJCS</em>, <em>11</em>, e2873. (<a href='https://doi.org/10.7717/peerj-cs.2873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid deployment of millions of connected devices brings significant security challenges to the Internet of Things (IoT). IoT devices are typically resource-constrained and designed for specific tasks, from which new security challenges are introduced. As such, IoT device identification has garnered substantial attention and is regarded as an initial layer of cybersecurity. One of the major steps in distinguishing IoT devices involves leveraging machine learning (ML) techniques on device network flows known as device fingerprinting. Numerous studies have proposed various solutions that incorporate ML and feature selection (FS) algorithms with different degrees of accuracy. Yet, the domain needs a comparative analysis of the accuracy of different classifiers and FS algorithms to comprehend their true capabilities in various datasets. This article provides a comprehensive performance evaluation of several reputable classifiers being used in the literature. The study evaluates the efficacy of filter-and wrapper-based FS methods across various ML classifiers. Additionally, we implemented a Binary Green Wolf Optimizer (BGWO) and compared its performance with that of traditional ML classifiers to assess the potential of this binary meta-heuristic algorithm. To ensure the robustness of our findings, we evaluated the effectiveness of each classifier and FS method using two widely utilized datasets. Our experiments demonstrated that BGWO effectively reduced the feature set by 85.11% and 73.33% for datasets 1 and 2, respectively, while achieving classification accuracies of 98.51% and 99.8%, respectively. The findings of this study highlight the strong capabilities of BGWO in reducing both the feature dimensionality and accuracy gained through classification. Furthermore, it demonstrates the effectiveness of wrapper methods in the reduction of feature sets.},
  archive      = {J_PEERJCS},
  author       = {Hamid Tahaei and Anqi Liu and Hamid Forooghikian and Mehdi Gheisari and Faiz Zaki and Nor Badrul Anuar and Zhaoxi Fang and Longjun Huang},
  doi          = {10.7717/peerj-cs.2873},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2873},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Machine learning for internet of things (IoT) device identification: A comparative study},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gene selection based on adaptive neighborhood-preserving multi-objective particle swarm optimization. <em>PEERJCS</em>, <em>11</em>, e2872. (<a href='https://doi.org/10.7717/peerj-cs.2872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of high-dimensional microarray gene expression data presents critical challenges, including excessive dimensionality, increased computational burden, and sensitivity to random initialization. Traditional optimization algorithms often produce inconsistent and suboptimal results, while failing to preserve local data structures limiting both predictive accuracy and biological interpretability. To address these limitations, this study proposes an adaptive neighborhood-preserving multi-objective particle swarm optimization (ANPMOPSO) framework for gene selection. ANPMOPSO introduces four key innovations: (1) a weighted neighborhood-preserving ensemble embedding (WNPEE) technique for dimensionality reduction that retains local structure; (2) Sobol sequence (SS) initialization to enhance population diversity and convergence stability; (3) a differential evolution (DE)-based adaptive velocity update to dynamically balance exploration and exploitation; and (4) a novel ranking strategy that combines Pareto dominance with neighborhood preservation quality to prioritize biologically meaningful gene subsets. Experimental evaluations on six benchmark microarray datasets and eleven multi-modal test functions (MMFs) demonstrate that ANPMOPSO consistently outperforms state-of-the-art methods. For example, it achieves 100% classification accuracy on Leukemia and Small-Round-Blue-Cell Tumor (SRBCT) using only 3–5 genes, improving accuracy by 5–15% over competitors while reducing gene subsets by 40–60%. Additionally, on MMFs, ANPMOPSO attains superior hypervolume values (e.g., 1.0617 ± 0.2225 on MMF1, approximately 10–20% higher than competitors), confirming its robustness in balancing convergence and diversity. Although the method incurs higher training time due to its structural and adaptive components, it achieves a strong trade-off between computational cost and biological relevance, making it a promising tool for high-dimensional gene selection in bioinformatics.},
  archive      = {J_PEERJCS},
  author       = {Sumet Mehta and Fei Han and Muhammad Sohail and Bhekisipho Twala and Asad Ullah and Fasee Ullah and Arfat Ahmad Khan and Qinghua Ling},
  doi          = {10.7717/peerj-cs.2872},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2872},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Gene selection based on adaptive neighborhood-preserving multi-objective particle swarm optimization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient parallel DCNN algorithm in big data environment. <em>PEERJCS</em>, <em>11</em>, e2871. (<a href='https://doi.org/10.7717/peerj-cs.2871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data plays a vital role in developing remote sensing, landslide prediction, and enabling applications, the integration of deep convolutional neural networks (DCNN) has significantly improved its prediction accuracy. However, several challenges remain in processing vast satellite imagery and other geospatial data. These challenges include excessive redundant features, slow convolution operation, and poor loss function convergence. An efficient parallel DCNN algorithm (PDCNN-MI), combined with MapReduce and Im2col algorithms, is introduced to address these challenges. First, a parallel feature extraction strategy based on the Marr-Hildreth operator (PFE-MHO) is proposed to extract target features from data as inputs to the network, effectively solving the problem of high data redundancy. Next, a parallel model training strategy based on Im2col method (PMT-IM) is designed to remove the redundant convolutional kernels by designing the center value of distance, improving convolution operation speed. Finally, a small batch gradient descent strategy (IMBGD) is presented to exclude the influence of training data of anomalous nodes on the batch gradient and solve the problem of poor convergence of the loss function. By utilizing these enhancements, the experimental results indicate that PDCNN-MI outperforms existing algorithms in classification accuracy and is well-suited for fast and large-scale image dataset processing.},
  archive      = {J_PEERJCS},
  author       = {Yimin Mao and Yaser Ahangari Nanehkaran and Neelakandan Chandrasekaran and Ying Huo and Zhan Qing Wen and ke Gong and Miao Decheng},
  doi          = {10.7717/peerj-cs.2871},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2871},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An efficient parallel DCNN algorithm in big data environment},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing healthcare data privacy and interoperability with federated learning. <em>PEERJCS</em>, <em>11</em>, e2870. (<a href='https://doi.org/10.7717/peerj-cs.2870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores the application of federated learning (FL) with the Fast Healthcare Interoperability Resources (FHIR) protocol to address the underutilization of the huge volumes of healthcare data generated by the digital health revolution, especially those from wearable sensors, due to privacy concerns and interoperability challenges. Despite advances in electronic medical records, mobile health applications, and wearable sensors, current digital health cannot fully exploit these data due to the lack of data analysis and exchange between heterogeneous systems. To address this gap, we present a novel converged platform combining FL and FHIR, which enables collaborative model training that preserves the privacy of wearable sensor data while promoting data standardization and interoperability. Unlike traditional centralized learning (CL) solutions that require data centralization, our platform uses local model learning, which naturally improves data privacy. Our empirical evaluation demonstrates that federated learning models perform as well as, or even numerically better than, centralized learning models in terms of classification accuracy, while also performing equally well in regression, as indicated by metrics such as accuracy, area under the curve (AUC), recall, and precision, among others, for classification, and mean absolute error (MAE), mean squared error (MSE), and root mean square error (RMSE) for regression. In addition, we developed an intuitive AutoML-powered web application that is FL and CL compatible to illustrate the feasibility of our platform for predictive modeling of physical activity and energy expenditure, while complying with FHIR data reporting standards. These results highlight the immense potential of our FHIR-integrated federated learning platform as a practical framework for future interoperable and privacy-preserving digital health ecosystems to optimize the use of connected health data.},
  archive      = {J_PEERJCS},
  author       = {Adil Akhmetov and Zohaib Latif and Benjamin Tyler and Adnan Yazici},
  doi          = {10.7717/peerj-cs.2870},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2870},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing healthcare data privacy and interoperability with federated learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling LTE-advanced cell capacity estimation using packet bundling and carrier aggregation. <em>PEERJCS</em>, <em>11</em>, e2868. (<a href='https://doi.org/10.7717/peerj-cs.2868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for mobile usage raises many challenges for service providers. Satisfying subscribers by providing a better quality of service (QoS) is one of the major concerns of an operator. One way to solve this problem is to adopt an efficient radio resource use mechanism. In this context, smart network planning management requires the estimation and enhancement of capacity in terms of the number of subscribers within a cell for better QoS provisioning through radio resource usage optimization. The model proposed in this article explicitly uses some long term evolution (LTE)-Advanced (LTE-A)-specific enhancements such as carrier aggregation (CA) and channel quality indication (CQI)-based resource allocation. Further, this study proposes a CQI-based clustering approach with packet bundling and CA to optimize radio resource utilization and enhance LTE-A cell capacity. LTE-A cell is logically divided into clusters such as the Silver class, Platinum class, Gold class, and Diamond class based on the CQI from the user end to eNodeB (eNB). Further, cell capacity (CCa) estimation algorithms are proposed in a simplistic scenario as well as in each cluster using packet bundling factor ω considering CA. From the result analysis, it is found that appropriate modulation and voice codecs can be used in appropriate clusters to enhance the cell capacity. Furthermore, it is observed that the packet bundling factor helps in improving the radio resource usage and thereby improving the capacity of a cell. The research work proposed in this article can be extended further to estimate the user capacity in the context of the 5th generation cellular network.},
  archive      = {J_PEERJCS},
  author       = {Rajiv Senapati},
  doi          = {10.7717/peerj-cs.2868},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2868},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Modeling LTE-advanced cell capacity estimation using packet bundling and carrier aggregation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effectiveness and optimization of bidirectional long short-term memory (BiLSTM) based fast detection of deep fake face videos for real-time applications. <em>PEERJCS</em>, <em>11</em>, e2867. (<a href='https://doi.org/10.7717/peerj-cs.2867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a rapid detection method for deepfake face videos designed for real-time applications using bidirectional long short-term memory (BiLSTM) networks. The aim is to overcome the limitations of current technologies in terms of efficiency and accuracy. An optimized BiLSTM architecture and training strategy are employed, enhancing recognition capabilities through data preprocessing and feature enhancement while also minimizing computational complexity and resource consumption during detection. Experiments were conducted on the FaceForensics++ dataset, which includes both authentic and four types of manipulated videos. The results show that the proposed BiLSTM-based approach outperforms existing methods in real-time detection. Specifically, the integration of temporal analysis and conditional random fields (CRF) resulted in significant accuracy improvements: a 1.6% increase in checking accuracy, a 2.0% improvement in checking completeness, and a 2.5% increase in the F1-score. The BiLSTM-based rapid detection approach demonstrated high efficiency and accuracy across multiple standard datasets, achieving notable performance gains over current technologies. These findings highlight the method’s potential and value for real-time deepfake detection applications.},
  archive      = {J_PEERJCS},
  author       = {Haoxiang Wang},
  doi          = {10.7717/peerj-cs.2867},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2867},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Effectiveness and optimization of bidirectional long short-term memory (BiLSTM) based fast detection of deep fake face videos for real-time applications},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel facial expression recognition framework using deep learning based dynamic cross-domain dual attention network. <em>PEERJCS</em>, <em>11</em>, e2866. (<a href='https://doi.org/10.7717/peerj-cs.2866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variations in domain targets have recently posed significant challenges for facial expression recognition tasks, primarily due to domain shifts. Current methods focus largely on global feature adoption to achieve domain-invariant learning; however, transferring local features across diverse domains remains an ongoing challenge. Additionally, during training on target datasets, these methods often suffer from reduced feature representation in the target domain due to insufficient discriminative supervision. To tackle these challenges, we propose a dynamic cross-domain dual attention network for facial expression recognition. Our model is specifically designed to learn domain-invariant features through separate modules for global and local adversarial learning. We also introduce a semantic-aware module to generate pseudo-labels, which computes semantic labels from both global and local features. We assess our model’s effectiveness through extensive experiments on the Real-world Affective Faces Database (RAF-DB), FER-PLUS, AffectNet, Expression in the Wild (ExpW), SFEW 2.0, and Japanese Female Facial Expression (JAFFE) datasets. The results demonstrate that our scheme outperforms the existing state-of-the-art methods by attaining recognition accuracies 93.18, 92.35, 82.13, 78.37, 72.47, 70.68 respectively.},
  archive      = {J_PEERJCS},
  author       = {Ahmed Omar Alzahrani and Ahmed Mohammed Alghamdi and M. Usman Ashraf and Iqra Ilyas and Nadeem Sarwar and Abdulrahman Alzahrani and Alaa Abdul Salam Alarood},
  doi          = {10.7717/peerj-cs.2866},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2866},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel facial expression recognition framework using deep learning based dynamic cross-domain dual attention network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of domain-specific modeling in kinetography and bipedal humanoid robot control. <em>PEERJCS</em>, <em>11</em>, e2864. (<a href='https://doi.org/10.7717/peerj-cs.2864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article presents a new approach in the development of software for bipedal humanoid robot controllers, based on the construction and application of graphic domain-specific languages (DSLs). The notations used to describe dance movements and gestures are typical examples of DSLs. With certain extensions, related to the description of foot topology, sensors and actuators, such DSLs are applicable for modeling dance movements that would be performed by a robot. The existing software development methodologies in robotics have a purely mechanistic approach to understanding and implementing robotic tasks. Such an approach in humanoid robotics complicates the understanding of the problem, as well as the specification and implementation of solutions. Our approach, which uses DSLs, adopts complex movements and gestures performed by the feet of dancers using professional dancers, people with above-average motor skills, as reference. We believe that the developed software can also be successfully applied to assistive robots that would help people with special needs whose mobility is significantly lower than average.},
  archive      = {J_PEERJCS},
  author       = {Verislav Djukić and Dragana Oros and Marko Penčić and Zhenli Lu},
  doi          = {10.7717/peerj-cs.2864},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2864},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Application of domain-specific modeling in kinetography and bipedal humanoid robot control},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic bandwidth allocation in time division multiplexed passive optical networks: A dual-standard analysis of ITU-T and IEEE standard algorithms. <em>PEERJCS</em>, <em>11</em>, e2863. (<a href='https://doi.org/10.7717/peerj-cs.2863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last 25 years, operators have effectively established passive optical networks (PONs), catering to around 1 billion users and earning income surpassing 8.5 billion Euros. Major standardization bodies like IEEE and ITU-T have introduced several PON solutions to mitigate last-mile broadband access and bandwidth allocation problems for end users. In this case, a compelling dynamic bandwidth allocation (DBA) algorithm can provide contention-free access (fairness) to the end user for the upstream channel with high bandwidth efficiency, minimal upstream delays, and scalability. This, in turn, boosts network quality of service (QoS) and allows operators to accommodate more users (revenue). This article examines the evolution of time-division multiplexed PON solutions such as A/BPON, EPON, GPON, XGPON, 10G-EPON, and NG-PON2 under both IEEE and ITU-T standards, addressing their approaches to DBA challenges. We analyze the bottlenecks and compare reported works based on their key strengths/applications, weaknesses, and operational mechanisms, as well as highlight their quantitative insights. We also discuss next-generation PONs (NG-PONs) and their emerging applications, such as 5G/6G fronthaul architecture in the cloud radio access network (CRAN) environment, fiber to the room (FTTR), and industrial PON, with a focus on DBA designs. Finally, the article summarizes current progress, highlights challenges, and proposes future research directions for developing more efficient DBA algorithms for these new applications.},
  archive      = {J_PEERJCS},
  author       = {Kamran Ali Memon and Syed Saeed Jaffer and Muhammad Ali Qureshi and Khurram Karim Qureshi},
  doi          = {10.7717/peerj-cs.2863},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2863},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic bandwidth allocation in time division multiplexed passive optical networks: A dual-standard analysis of ITU-T and IEEE standard algorithms},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast2Vec, a modified model of FastText that enhances semantic analysis in topic evolution. <em>PEERJCS</em>, <em>11</em>, e2862. (<a href='https://doi.org/10.7717/peerj-cs.2862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Topic modeling approaches, such as latent Dirichlet allocation (LDA) and its successor, the dynamic topic model (DTM), are widely used to identify specific topics by extracting words with similar frequencies from documents. However, these topics often require manual interpretation, which poses challenges in constructing semantics topic evolution, mainly when topics contain negations, synonyms, or rare terms. Neural network-based word embeddings, such as Word2vec and FastText, have advanced semantic understanding but have their limitations. Word2Vec struggles with out-of-vocabulary (OOV) words, and FastText generates suboptimal embeddings for infrequent terms. Methods This study introduces Fast2Vec, a novel model that integrates the semantic capabilities of Word2Vec with the subword analysis strength of FastText to enhance semantic analysis in topic modeling. The model was evaluated using research abstracts from the Science and Technology Index (SINTA) journal database and validated using twelve public word similarity benchmarks, covering diverse semantic and syntactic dimensions. Evaluation metrics include Spearman and Pearson correlation coefficients to assess the alignment with human judgments. Results Experimental findings demonstrated that Fast2Vec outperforms or closely matches Word2Vec and FastText across most benchmark datasets, particularly in task requiring fine-grained semantic similarity. In OOV scenarios, Fast2Vec improved semantic similarity by 39.64% compared to Word2Vec, and 6.18% compared to FastText. Even in scenarios without OOV terms, Fast2Vec achieved a 7.82% improvement over FastText and a marginal 0.087% improvement over Word2Vec. Additionally, the model effectively categorized topics into four distinct evolution patterns (diffusion, shifting, moderate fluctuations, and stability), enabling a deeper understanding of evolution topic interests and their dynamic characteristics. Conclusion Fast2Vec presents a robust and generalizable word embedding framework for semantic-based topic modeling. By combining the contextual sensitivity of Word2Vec with the subword flexibility of FastText, Fast2Vec effectively addresses prior limitations in handling OOV terms and semantic variation and demonstrates strong potential for boarder applications in natural language processing tasks.},
  archive      = {J_PEERJCS},
  author       = {Ayu Pertiwi and Azhari Azhari and Sri Mulyana},
  doi          = {10.7717/peerj-cs.2862},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2862},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fast2Vec, a modified model of FastText that enhances semantic analysis in topic evolution},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMEDNet: A multimodal approach for emotion detection in the urdu language. <em>PEERJCS</em>, <em>11</em>, e2861. (<a href='https://doi.org/10.7717/peerj-cs.2861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion detection is a critical component of interaction between human and computer systems, more especially affective computing, and health screening. Integrating video, speech, and text information provides better coverage of the basic and derived affective states with improved estimation of verbal and non-verbal behavior. However, there is a lack of systematic preferences and models for the detection of emotions in low-resource languages such as Urdu. To this effect, we propose Urdu Multimodal Emotion Detection Network (UMEDNet), a new emotion detection model for Urdu that works with video, speech, and text inputs for a better understanding of emotion. To support our proposed UMEDNet, we created the Urdu Multimodal Emotion Detection (UMED) corpus, which is a seventeen-hour annotated corpus of five basic emotions. To the best of our knowledge, the current study provides the first corpus for detecting emotion in the context of multimodal emotion detection for the Urdu language and is extensible for extended research. UMEDNet leverages state-of-the-art techniques for feature extraction across modalities; for extracting facial features from video, both Multi-task Cascaded Convolutional Networks (MTCNN) and FaceNet were used with fine-tuned Wav2Vec2 for speech features and XLM-Roberta for text. These features are then projected into common latent spaces to enable the effective fusion of multimodal data and to enhance the accuracy of emotion prediction. The model demonstrates strong performance, achieving an overall accuracy of 85.27%, while precision, recall, and F1 scores, are all approximately equivalent. In the end, we analyzed the impact of UMEDNet and found that our model integrates data on different modalities and leads to better performance.},
  archive      = {J_PEERJCS},
  author       = {Adil Majeed and Hasan Mujtaba},
  doi          = {10.7717/peerj-cs.2861},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2861},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {UMEDNet: A multimodal approach for emotion detection in the urdu language},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal scene recognition using semantic segmentation and deep learning integration. <em>PEERJCS</em>, <em>11</em>, e2858. (<a href='https://doi.org/10.7717/peerj-cs.2858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic modeling and recognition of indoor scenes present a significant challenge due to the complex composition of generic scenes, which contain a variety of features including themes and objects, makes semantic modeling and indoor scene recognition difficult. The gap between high-level scene interpretation and low-level visual features increases the complexity of scene recognition. In order to overcome these obstacles, this study presents a novel multimodal deep learning technique that enhances scene recognition accuracy and robustness by combining depth information with conventional red-green-blue (RGB) image data. Convolutional neural networks (CNNs) and spatial pyramid pooling (SPP) are used for analysis after a depth-aware segmentation methodology is used to identify several objects in an image. This allows for more precise image classification. The effectiveness of this method is demonstrated by experimental findings, which show 91.73% accuracy on the RGB-D scene dataset and 90.53% accuracy on the NYU Depth v2 dataset. These results demonstrate how the multimodal approach can improve scene detection and classification, with potential uses in fields including robotics, sports analysis, and security systems.},
  archive      = {J_PEERJCS},
  author       = {Aysha Naseer and Mohammed Alnusayri and Haifa F. Alhasson and Mohammed Alatiyyah and Dina Abdulaziz AlHammadi and Ahmad Jalal and Jeongmin Park},
  doi          = {10.7717/peerj-cs.2858},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2858},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multimodal scene recognition using semantic segmentation and deep learning integration},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision-based approach to knee osteoarthritis and parkinson’s disease detection utilizing human gait patterns. <em>PEERJCS</em>, <em>11</em>, e2857. (<a href='https://doi.org/10.7717/peerj-cs.2857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the number of cases of musculoskeletal and neurological disorders, such as knee osteoarthritis (KOA) and Parkinson’s disease (PD), has significantly increased. Numerous clinical methods have been proposed in research to diagnose these disorders; however, a current trend in diagnosis is through human gait patterns. Several researchers proposed different methods in this area, including gait detection utilizing sensor-based data and vision-based systems that include both marker-based and marker-free techniques. The majority of current studies are concerned with the classification of Parkinson’s disease. Furthermore, many vision-based algorithms rely on human gait silhouettes or gait representations and employ traditional similarity-based methodologies. However, in this study, a novel approach is proposed in which spatiotemporal features are extracted via deep learning methods with a transfer learning paradigm. Following that, advanced deep learning approaches, including sequential models like gated recurrent unit (GRU), are used for additional analysis. The experimentation is performed on the publicly available KOA–PD–normal dataset comprising gait videos with various abnormalities, and the proposed model has the highest accuracy of approximately 94.81%.},
  archive      = {J_PEERJCS},
  author       = {Zeeshan Ali and Jihoon Moon and Saira Gillani and Sitara Afzal and Muazzam Maqsood and Seungmin Rho},
  doi          = {10.7717/peerj-cs.2857},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2857},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Vision-based approach to knee osteoarthritis and parkinson’s disease detection utilizing human gait patterns},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining autonomous student patterns score on LMS within online higher education. <em>PEERJCS</em>, <em>11</em>, e2855. (<a href='https://doi.org/10.7717/peerj-cs.2855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher education institutions actively integrate information and communication technologies through learning management systems (LMS), which are crucial for online education. This study used data mining techniques to predict the autonomous scores of students in the online Law and Psychology programs at the Technical University of Manabi. The process involved data integration and selection of more than 16,000 records, preprocessing, transformation with RobustScaler, predictive modelling that included recursive feature elimination with cross-validation to select features (RFEcv), and hyperparameter fitting to achieve the best fit, and finally, evaluation of the models using metrics of root mean square error (RMSE), mean absolute error (MAE), and the coefficient of determination (R2). The feature selection framework suggested by RFEcv contributed to the performance of the models. The variables analyzed focused on download rate, homework submission rate, test performance rate, median daily accesses, median days of access per month, observation of comments on teacher-reviewed assignments, length of final exam, and not requiring the supplemental exam. Hyperparameter adjustment improved the performance of the models after applying RFEcv. The models evaluated showed minimal differences in RMSE ([0.5411 .. 0.6025]). The gradient boosting model achieved the best performance of R2 = 0.6693, MAE = 0.4041 and RMSE = 0.5411 with the Law online program data, as with the Psychology online program data, with an R2 = 0.6418, MAE = 0.4232 and RMSE = 0.6025, while the combination of both data sets reflected the best performance with the extreme gradient boosting (XGBoost) model with the values of R2 = 0.6294, MAE = 0.4295 and RMSE = 0.5985. Future research and implementations could include autonomous score data through plugins and reports integrated into LMSs. This approach may provide indicators of interest for understanding and improving online learning from a personalized, real-time perspective.},
  archive      = {J_PEERJCS},
  author       = {Ricardo Ordoñez-Avila and Jaime Meza and Sebastian Ventura},
  doi          = {10.7717/peerj-cs.2855},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2855},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Mining autonomous student patterns score on LMS within online higher education},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized deep learning approach for lung cancer detection using flying fox optimization and bidirectional generative adversarial networks. <em>PEERJCS</em>, <em>11</em>, e2853. (<a href='https://doi.org/10.7717/peerj-cs.2853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer remains one of the most prevalent and life-threatening diseases, often diagnosed at an advanced stage due to the challenges in early detection. Contributory factors include genetic mutations, smoking, alcohol consumption, and exposure to hazardous environmental conditions. Computer-aided diagnosis (CAD) systems have significantly improved early cancer detection, but limitations such as high-dimensional feature sets and overfitting issues persist. This study presents an optimised deep learning approach for lung cancer classification, integrating flying fox optimization (FFXO) for feature selection and bidirectional generative adversarial networks (Bi-GAN) for classification. The methodology consists of three key phases: (1) Data preprocessing, where missing values are handled using the multiple imputations by chain equation (MICE) technique and feature scaling is applied using standard and min-max scalers; (2) Feature selection, where the FFXO algorithm reduces feature dimensionality to enhance classification efficiency; and (3) Lung tumor classification, utilizing Bi-GAN to improve predictive accuracy. The proposed system was evaluated using key performance metrics—accuracy, precision, recall, and F1-score—and demonstrated superior performance to conventional models. Experimental results on a publicly available lung cancer dataset showed an accuracy of 98.7% highlighting the approach’s robustness in precise lung tumor classification. This study provides a novel framework for improving the reliability and efficiency of lung cancer detection, offering significant potential for clinical applications.},
  archive      = {J_PEERJCS},
  author       = {Manal Abdullah Alohali and Hamed Alqahtani and Shouki A. Ebad and Faiz Abdullah Alotaibi and Venkatachalam K. and Jaehyuk Cho},
  doi          = {10.7717/peerj-cs.2853},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2853},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimized deep learning approach for lung cancer detection using flying fox optimization and bidirectional generative adversarial networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on channel estimation based on joint perception and deep enhancement learning in complex communication scenarios. <em>PEERJCS</em>, <em>11</em>, e2852. (<a href='https://doi.org/10.7717/peerj-cs.2852'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary wireless communication systems, channel estimation and optimization have become increasingly pivotal with the growing number and complexity of devices. Communication systems frequently encounter multiple challenges, such as multipath propagation, signal fading, and interference, which may result in the degradation of communication quality, a reduction in data transmission rates, and even communication interruptions. Therefore, effective estimation and optimization of channels in complex communication environments are of paramount importance to ensure communication quality and enhance system performance. In this article, we address the intelligent, reflective surface (IRS)-assisted channel estimation problem and propose an intelligent channel estimation model based on the fusion of convolutional neural network (CNN) and gated recurrent unit (GRU) row features, utilizing the reinforcement learning Deep Deterministic Policy Gradient (DDPG) strategy for Channel Reconstruction Prediction and Generation Network (CRPG-Net). The framework initially acquires the received signal by converting the guide-frequency symbols at the transmitter into time-domain sequences to be transmitted, and after propagating through the direct channel and the IRS reflection channel, processes the data at the receiver. Subsequently, the spatial and temporal features in the received signal are extracted using the CRPG-Net model, with the adaptive optimization capability of the model enhanced by deep reinforcement learning. The introduction of reinforcement learning enables the model to continuously optimize decisions in dynamic channel environments, improve the robustness of channel estimation, and quickly adjust the IRS reflection parameters when the channel state changes to adapt to complex communication conditions. Experimental results demonstrate that the framework achieves significant channel estimation accuracy and robustness across several public datasets and real test scenarios, with the channel estimation error markedly smaller than that of traditional least squares (LS) and linear minimum mean square error (LMMSE) methods. This method introduces innovative techniques for channel estimation in intelligent communication systems, playing a crucial role in enhancing communication quality and overall system performance.},
  archive      = {J_PEERJCS},
  author       = {Xin Liu and Shanghong Zhao and Yanxia Liang and Shahid Karim},
  doi          = {10.7717/peerj-cs.2852},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2852},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on channel estimation based on joint perception and deep enhancement learning in complex communication scenarios},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative data storage with incentive mechanism for blockchain-based IoV. <em>PEERJCS</em>, <em>11</em>, e2849. (<a href='https://doi.org/10.7717/peerj-cs.2849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the volume of data in the Internet of Vehicles (IoV) continues to grow, challenges such as insufficient storage capacity and potential privacy breaches become more pronounced. To address these issues, this article proposes a novel collaborative data storage scheme with an incentivization mechanism, termed Blockchain-Based Collaborative Data Storage with Incentive Mechanism for IoV (CDS-BIoV). The CDS-BIoV framework consists of vehicles, roadside units (RSUs), and cloud infrastructure. In the first phase, vehicles collect and transmit data to their nearest RSU nodes. To encourage active participation in data reception and storage, an incentive mechanism is introduced to motivate RSU nodes. Two algorithms are developed: the Incentive Mechanism Collaborative Data Storage Algorithm (I-CDSA) and the Data Offloading Algorithm (DOA). The I-CDSA uses a competitiveness matrix to incentivize RSU nodes to minimize storage consumption, while the DOA employs incentives to secure additional cloud storage for offloading data. Experimental results show that the CDS-BIoV scheme reduces storage consumption by up to 93% compared to the Generic Parallel Database (GPDB), particularly as the number of blocks increases, effectively alleviating storage capacity limitations.},
  archive      = {J_PEERJCS},
  author       = {Quan Shi and Lankai Wang and Chen Chen},
  doi          = {10.7717/peerj-cs.2849},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2849},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A collaborative data storage with incentive mechanism for blockchain-based IoV},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intersection collision prediction and prevention based on vehicle-to-vehicle (V2V) and cloud computing communication. <em>PEERJCS</em>, <em>11</em>, e2846. (<a href='https://doi.org/10.7717/peerj-cs.2846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern transportation systems, the management of traffic safety has become increasingly critical as both the number and complexity of vehicles continue to rise. These systems frequently encounter multiple challenges. Consequently, the effective assessment and management of collision risks in various scenarios within transportation systems are paramount to ensuring traffic safety and enhancing road utilization efficiency. In this paper, we tackle the issue of intelligent traffic collision prediction and propose a vehicle collision risk prediction model based on vehicle-to-vehicle (V2V) communication and the graph attention network (GAT). Initially, the framework gathers vehicle trajectory, speed, acceleration, and relative position information via V2V communication technology to construct a graph representation of the traffic environment. Subsequently, the GAT model extracts interaction features between vehicles and optimizes the vehicle driving strategy through deep reinforcement learning (DRL), thereby augmenting the model’s decision-making capabilities. Experimental results demonstrate that the framework achieves over 80% collision recognition accuracy concerning true warning rate on both public and real-world datasets. The metrics for false detection are thoroughly analyzed, revealing the efficacy and robustness of the proposed framework. This method introduces a novel technological approach to collision prediction in intelligent transportation systems and holds significant implications for enhancing traffic safety and decision-making efficiency.},
  archive      = {J_PEERJCS},
  author       = {Min Zeng and Mohd Sani Mohamad Hashim and Mohd Nasir Ayob and Abdul Halim Ismail and Qiling Zang},
  doi          = {10.7717/peerj-cs.2846},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2846},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Intersection collision prediction and prevention based on vehicle-to-vehicle (V2V) and cloud computing communication},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROM-pose: Restoring occluded mask image for 2D human pose estimation. <em>PEERJCS</em>, <em>11</em>, e2843. (<a href='https://doi.org/10.7717/peerj-cs.2843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) is a field focused on estimating human poses by detecting key points in images. HPE includes methods like top-down and bottom-up approaches. The top-down approach uses a two-stage process, first locating and then detecting key points on humans with bounding boxes, whereas the bottom-up approach directly detects individual key points and integrates them to estimate the overall pose. In this article, we address the problem of bounding box detection inaccuracies in certain situations using the top-down method. The detected bounding boxes, which serve as input for the model, impact the accuracy of pose estimation. Occlusions occur when a part of the target’s body is obscured by a person or object and hinder the model’s ability to detect complete bounding boxes. Consequently, the model produces bounding boxes that do not recognize occluded parts, resulting in their exclusion from the input used by the HPE model. To mitigate this issue, we introduce the Restoring Occluded Mask Image for 2D Human Pose Estimation (ROM-Pose), comprising a restoration model and an HPE model. The restoration model is designed to delineate the boundary between the target’s grayscale mask (occluded image) and the blocker’s grayscale mask (occludee image) using the specially created Whole Common Objects in Context (COCO) dataset. Upon identifying the boundary, the restoration model restores the occluded image. This restored image is subsequently overlaid onto the RGB image for use in the HPE model. By integrating occluded parts’ information into the input, the bounding box includes these areas during detection, thus enhancing the HPE model’s ability to recognize them. ROM-Pose achieved a 1.6% improvement in average precision (AP) compared to the baseline.},
  archive      = {J_PEERJCS},
  author       = {Yunju Lee and Jihie Kim},
  doi          = {10.7717/peerj-cs.2843},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2843},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {ROM-pose: Restoring occluded mask image for 2D human pose estimation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomous vehicle surveillance through fuzzy C-means segmentation and DeepSORT on aerial images. <em>PEERJCS</em>, <em>11</em>, e2835. (<a href='https://doi.org/10.7717/peerj-cs.2835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high mobility of uncrewed aerial vehicles (UAVs) has led to their usage in various computer vision applications, notably in intelligent traffic surveillance, where it enhances productivity and simplifies the process. Yet, there are still several challenges that must be resolved to automate these systems. One significant challenge is the accurate extraction of vehicle foregrounds in complex traffic scenarios. As a result, this article proposes a novel vehicle detection and tracking system for autonomous vehicle surveillance, which employs Fuzzy C-mean clustering to segment the aerial images. After segmentation, we employed the YOLOv4 deep learning algorithm, which is efficient in detecting small-sized objects in vehicle detection. Furthermore, an ID assignment and recovery algorithm based on Speed-Up Robust Feature (SURF) is used for multi-vehicle tracking across image frames. Vehicles are determined by counting in each image to estimate the traffic density at different time intervals. Finally, these vehicles were tracked using DeepSORT, which combines the Kalman filter with deep learning to produce accurate results. Furthermore, to understand the traffic flow direction, the path trajectories of each tracked vehicle is projected. Our proposed model demonstrates a noteworthy vehicle detection and tracking rate during experimental validation, attaining precision scores of 0.82 and 0.80 over UAVDT and KIT-AIS datasets for vehicle detection. For vehicle tracking, the precision is 0.87 over the UAVDT dataset and 0.83 for the KIT-AIS dataset.},
  archive      = {J_PEERJCS},
  author       = {Asifa Mehmood Qureshi and Moneerah Alotaibi and Sultan Refa Alotaibi and Dina Abdulaziz AlHammadi and Muhammad Asif Jamal and Ahmad Jalal and Bumshik Lee},
  doi          = {10.7717/peerj-cs.2835},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2835},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Autonomous vehicle surveillance through fuzzy C-means segmentation and DeepSORT on aerial images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anticancer drug synergy prediction based on CatBoost. <em>PEERJCS</em>, <em>11</em>, e2829. (<a href='https://doi.org/10.7717/peerj-cs.2829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The research of cancer treatments has always been a hot topic in the medical field. Multi-targeted combination drugs have been considered as an ideal option for cancer treatment. Since it is not feasible to use clinical experience or high-throughput screening to identify the complete combinatorial space, methods such as machine learning models offer the possibility to explore the combinatorial space effectively. Methods In this work, we proposed a machine learning method based on CatBoost to predict the synergy scores of anticancer drug combinations on cancer cell lines, which utilized oblivious trees and ordered boosting technique to avoid overfitting and bias. The model was trained and tested using the data screened from NCI-ALMANAC dataset. The drugs were characterized with morgan fingerprints, drug target information, monotherapy information, and the cell lines were described with gene expression profiles. Results In the stratified 5-fold cross-validation, our method obtained excellent results, where, the receiver operating characteristic area under the curve (ROC AUC) is 0.9217, precision-recall area under the curve (PR AUC) is 0.4651, mean squared error (MSE) is 0.1365, and Pearson correlation coefficient is 0.5335. The performance is significantly better than three other advanced models. Additionally, when using SHapley Additive exPlanations (SHAP) to interpret the biological significance of the prediction results, we found that drug features played more prominent roles than cell line features, and genes associated with cancer development, such as PTK2, CCND1, and GNA11, played an important part in drug synergy prediction. Combining the experimental results, the model proposed in this study has a good prediction effect and can be used as an alternative method for predicting anticancer drug combinations.},
  archive      = {J_PEERJCS},
  author       = {Changheng Li and Nana Guan and Hongyi Zhang},
  doi          = {10.7717/peerj-cs.2829},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2829},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Anticancer drug synergy prediction based on CatBoost},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved salp swarm algorithm based optimization of mobile task offloading. <em>PEERJCS</em>, <em>11</em>, e2818. (<a href='https://doi.org/10.7717/peerj-cs.2818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The realization of computation-intensive applications such as real-time video processing, virtual/augmented reality, and face recognition becomes possible for mobile devices with the latest advances in communication technologies. This application requires complex computation for better user experience and real-time decision-making. However, the Internet of Things (IoT) and mobile devices have computational power and limited energy. Executing these computational-intensive tasks on edge devices may result in high energy consumption or high computation latency. In recent times, mobile edge computing (MEC) has been used and modernized for offloading this complex task. In MEC, IoT devices transmit their tasks to edge servers, which consecutively carry out faster computation. Methods However, several IoT devices and edge servers put an upper limit on executing concurrent tasks. Furthermore, implementing a smaller size task (1 KB) over an edge server leads to improved energy consumption. Thus, there is a need to have an optimum range for task offloading so that the energy consumption and response time will be minimal. The evolutionary algorithm is the best for resolving the multiobjective task. Energy, memory, and delay reduction together with the detection of the offloading task is the multiobjective to achieve. Therefore, this study presents an improved salp swarm algorithm-based Mobile Application Offloading Algorithm (ISSA-MAOA) technique for MEC. Results This technique harnesses the optimization capabilities of the improved salp swarm algorithm (ISSA) to intelligently allocate computing tasks between mobile devices and the cloud, aiming to concurrently minimize energy consumption, and memory usage, and reduce task completion delays. Through the proposed ISSA-MAOA, the study endeavors to contribute to the enhancement of mobile cloud computing (MCC) frameworks, providing a more efficient and sustainable solution for offloading tasks in mobile applications. The results of this research contribute to better resource management, improved user interactions, and enhanced efficiency in MCC environments.},
  archive      = {J_PEERJCS},
  author       = {Aishwarya R. and Mathivanan G.},
  doi          = {10.7717/peerj-cs.2818},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2818},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improved salp swarm algorithm based optimization of mobile task offloading},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG-based schizophrenia diagnosis using deep learning with multi-scale and adaptive feature selection. <em>PEERJCS</em>, <em>11</em>, e2811. (<a href='https://doi.org/10.7717/peerj-cs.2811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schizophrenia is a chronic and severe mental illness that significantly impacts the daily lives and work of those affected. Unfortunately, schizophrenia with negative symptoms often gets misdiagnosed, relying heavily on the clinician’s experience. There is a pressing need to develop an objective and effective diagnostic method for this specific type of schizophrenia. This paper proposes a new deep-learning method called Cascaded Atrous Convolutional Network with Adaptive Weight Fusion (CA-AWFM) for classifying schizophrenia from electroencephalogram (EEG) data that combines cascaded networks with atrous convolutions and an adaptive weight fusion module (AWFM). This is because schizophrenia involves intricate and subtle brain wave patterns that make it difficult to detect the disorder from EEG signals. As such, our model uses an “atrous” convolution operation to extract multi-scale temporal information and a cascade network structure that progressively improves the attribute representations across layers. For classification purposes, AWFM enables our model to modify the importance of features dynamically. We evaluated our technique using a publicly available dataset of EEG recordings acquired from patients who have schizophrenia and everyday individuals. The proposed model has significantly outperformed existing methods with a 99.5% accuracy rate. With the help of atrous convolutions, local and global dependencies within the EEGs can be effectively modeled in this way. At the same time, AWFM makes flexible prioritization of characteristics possible for improved classification performance. With such impressive figures achieved, it can be concluded that our approach should be considered as accurate enough for routine clinical use in identifying schizophrenic patients early on so they can receive intervention measures on time or when diagnosed late, then dealt with appropriately.},
  archive      = {J_PEERJCS},
  author       = {Alanoud Al Mazroa and Majdy M. Eltahir and Shouki A. Ebad and Faiz Abdullah Alotaibi and Venkatachalam K and Jaehyuk Cho},
  doi          = {10.7717/peerj-cs.2811},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2811},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {EEG-based schizophrenia diagnosis using deep learning with multi-scale and adaptive feature selection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class-weighted Dempster–Shafer in dual-level fusion for multimodal fake real estate listings detection. <em>PEERJCS</em>, <em>11</em>, e2797. (<a href='https://doi.org/10.7717/peerj-cs.2797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Detecting fake multimodal property listings is a significant challenge in online real estate platforms due to the increasing sophistication of fraudulent activities. The existing multimodal data fusion methods have several limitations and strengths in identifying fraudulent listings. Single-level fusion models whether at the feature, decision, or intermediate level struggle with balancing the contributions of different modalities leading to suboptimal decision-making. To address these problems, a dual-level fusion from multimodal for fake real estate listings detection is proposed. The dual-level fusion allows the integration of detailed features from text and image data to be performed at an early stage, followed by the metadata fusion at the decision stage in order to obtain a more comprehensive final classification. Furthermore, a new weighting scheme is introduced to optimize Dempster–Shafer in decision fusion to help the model achieve optimal performance and as a result, our method improves the classification. The Dempster–Shafer without class weightage lacks the flexibility to adapt to varying levels of uncertainty or importance across different classes. Methods In Class Weighted Dempster–Shafer in Dual Level Fusion (CWDS-DLF), we employ advanced models (XLNet for text and ResNet101 for images) for feature extraction and use the Dempster–Shafer theory for decision fusion. A new weighting scheme, based on Bayesian optimization, was used to assign optimal weights to the ‘fake’ and ‘not fake’ classes, thereby enhancing the Dempster–Shafer theory in the decision fusion process. Results The CWDS-DLF was evaluated on the property listing website dataset and achieved an F1 score of 96% and an accuracy of 93%. A t-test confirms the significance of these improvements (p < 0.05), demonstrating the effectiveness of our method in detecting fake property listings. Compared to other models, including 2D-convolutional neural network (CNN), XGBoost, and various multimodal approaches, our model consistently outperforms in precision, recall, and F1-score. This underscores the potential of integrating multimodal analysis with sophisticated fusion techniques to enhance the detection of fake property listings, ultimately improving consumer protection and operational efficiency in online real estate platforms.},
  archive      = {J_PEERJCS},
  author       = {Maifuza Mohd Amin and Nor Samsiah Sani and Mohammad Faidzul Nasrudin},
  doi          = {10.7717/peerj-cs.2797},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2797},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Class-weighted Dempster–Shafer in dual-level fusion for multimodal fake real estate listings detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAPE-ViT: Multimodal scene understanding with novel wavelet-augmented vision transformer. <em>PEERJCS</em>, <em>11</em>, e2796. (<a href='https://doi.org/10.7717/peerj-cs.2796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces Multimodal Adaptive Patch Embedding with Vision Transformer (MAPE-ViT), a novel approach for RGB-D scene classification that effectively addresses fundamental challenges of sensor misalignment, depth noise, and object boundary preservation. Our framework integrates maximally stable extremal regions (MSER) with wavelet coefficients to create comprehensive patch embedding that capture both local and global image features. These MSER-guided patches, incorporating original pixels and multi-scale wavelet information, serve as input to a Vision Transformer, which leverages its attention mechanisms to extract high-level semantic features. The feature discrimination capability is further enhanced through optimization using the Gray Wolf algorithm. The processed features then flow into a dual-stream architecture, where an extreme learning machine handles multi-object classification, while conditional random fields (CRF) manage scene-level categorization. Extensive experimental results demonstrate the effectiveness of our approach, showing significant improvements in classification accuracy compared to existing methods. Our system provides a robust solution for RGB-D scene understanding, particularly in challenging conditions where traditional approaches struggle with sensor artifacts and noise.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Waqas Ahmed and Touseef Sadiq and Hameedur Rahman and Sulaiman Abdullah Alateyah and Mohammed Alnusayri and Mohammed Alatiyyah and Dina Abdulaziz AlHammadi},
  doi          = {10.7717/peerj-cs.2796},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2796},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MAPE-ViT: Multimodal scene understanding with novel wavelet-augmented vision transformer},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient gradient-based algorithm with descent direction for unconstrained optimization with applications to image restoration and robotic motion control. <em>PEERJCS</em>, <em>11</em>, e2783. (<a href='https://doi.org/10.7717/peerj-cs.2783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel gradient-based algorithm designed to enhance the performance of optimization models, particularly in computer science applications such as image restoration and robotic motion control. The proposed algorithm introduces a modified conjugate gradient (CG) method, ensuring the CG coefficient, β κ, remains integral to the search direction, thereby maintaining the descent property under appropriate line search conditions. Leveraging the strong Wolfe conditions and assuming Lipschitz continuity, we establish the global convergence of the algorithm. Computational experiments demonstrate the algorithm’s superior performance across a range of test problems, including its ability to restore corrupted images with high precision and effectively manage motion control in a 3DOF robotic arm model. These results underscore the algorithm’s potential in addressing key challenges in image processing and robotics.},
  archive      = {J_PEERJCS},
  author       = {Sulaiman Mohammed Ibrahim and Aliyu M. Awwal and Maulana Malik and Ruzelan Khalid and Aida Mauziah Benjamin and Mohd Kamal Mohd Nawawi and Elissa Nadia Madi},
  doi          = {10.7717/peerj-cs.2783},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2783},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An efficient gradient-based algorithm with descent direction for unconstrained optimization with applications to image restoration and robotic motion control},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using transformers and bi-LSTM with sentence embeddings for prediction of openness human personality trait. <em>PEERJCS</em>, <em>11</em>, e2781. (<a href='https://doi.org/10.7717/peerj-cs.2781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding human personality traits is significant as it helps in decision making related to consumers’ behavior, career counselling, team building and top candidates’ selection for recruitment. Among various traits, openness is essential as it shows both diverse aspects of sensitive nature or intuitive nature. The individuals having a sensing nature tends to be more practical and prefer to focus on concrete information whereas the users having intuitive trait type is characterized by a focus on abstract ideas, creative thinking and future-oriented perspectives. In this research work, we aim to explore diverse natural language processing (NLP) based features and apply state of the art deep learning algorithms for openness trait prediction. Using standard Myers-Briggs Type Indicator (MBTI) dataset, we propose the use of the latest deep features of sentence embeddings which captures contextual semantics of the content to be used with deep learning models. For comparison, we explore textual features of Frequency-Inverse Document (TF-IDF) and parts of speech (POS) tagging with machine learning models and deep features of word2vec and global vectors for word representation (GloVe) with deep learning models. The comprehensive empirical analysis reveals that TF-IDF used with gradient boosting achieves high accuracy of 90% whereas, the deep feature of sentence embeddings when used and with deep model bidirectional long short-term memory (Bi-LSTM) achieves 90.5% accuracy. The best results have been achieved using the latest Transformer-based DistilBERT, which achieves the highest accuracy of 92% outperforming the existing studies in relevant literature.},
  archive      = {J_PEERJCS},
  author       = {Anam Naz and Hikmat Ullah Khan and Tariq Alsahfi and Mousa Alhajlah and Bader Alshemaimri and Ali Daud},
  doi          = {10.7717/peerj-cs.2781},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2781},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Using transformers and bi-LSTM with sentence embeddings for prediction of openness human personality trait},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HFSA: Hybrid feature selection approach to improve medical diagnostic system. <em>PEERJCS</em>, <em>11</em>, e2764. (<a href='https://doi.org/10.7717/peerj-cs.2764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the presence of artificial intelligence methods, the diagnosis of patients can be done quickly and accurately. This article introduces a new diagnostic system (DS) that includes three main layers called the rejection layer (RL), selection layer (SL), and diagnostic layer (DL) to accurately diagnose cases suffering from various diseases. In RL, outliers can be removed using the genetic algorithm (GA). At the same time, the best features can be selected by using a new feature selection method called the hybrid feature selection approach (HFSA) in SL. In the next step, the filtered data is passed to the naive Bayes (NB) classifier in DL to give accurate diagnoses. In this work, the main contribution is represented in introducing HFSA as a new selection approach that is composed of two main stages; fast stage (FS) and accurate stage (AS). In FS, chi-square, as a filtering methodology, is applied to quickly select the best features while Hybrid Optimization Algorithm (HOA), as a wrapper methodology, is applied in AS to accurately select features. It is concluded that HFSA is better than other selection methods based on experimental results because HFSA can enable three different classifiers called NB, K-nearest neighbors (KNN), and artificial neural network (ANN) to provide the maximum accuracy, precision, and recall values and the minimum error value. Additionally, experimental results proved that DS, including GA as an outlier rejection method, HFSA as feature selection, and NB as diagnostic mode, outperformed other diagnosis models.},
  archive      = {J_PEERJCS},
  author       = {Asmaa H. Rabie and Mohammed Aldawsari and Ahmed I. Saleh and M. S. Saraya and Metwally Rashad},
  doi          = {10.7717/peerj-cs.2764},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2764},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HFSA: Hybrid feature selection approach to improve medical diagnostic system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TASCI: Transformers for aspect-based sentiment analysis with contextual intent integration. <em>PEERJCS</em>, <em>11</em>, e2760. (<a href='https://doi.org/10.7717/peerj-cs.2760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel Transformer-Based Aspect-Level Sentiment Classification with Intent (TASCI) model, designed to enhance sentiment analysis by integrating aspect-level sentiment classification with intent analysis. Traditional sentiment analysis methods often overlook the nuanced relationship between the intent behind a statement and the sentiment expressed toward specific aspects of an entity. TASCI addresses this gap by first extracting aspects using a self-attention mechanism and then employing a Transformer-based model to infer the speaker’s intent from preceding sentences. This dual approach allows TASCI to contextualize sentiment analysis, providing a more accurate reflection of user opinions. We validate TASCI’s performance on three benchmark datasets: Restaurant, Laptop, and Twitter, achieving state-of-the-art results with an accuracy of 89.10% and a macro-F1 score of 83.38% on the Restaurant dataset, 84.81% accuracy and 78.63% macro-F1 score on the Laptop dataset, and 79.08% accuracy and 77.27% macro-F1 score on the Twitter dataset. These results demonstrate that incorporating intent analysis significantly enhances the model’s ability to capture complex sentiment expressions across different domains, thereby setting a new standard for aspect-level sentiment classification.},
  archive      = {J_PEERJCS},
  author       = {Hassan Nazeer Chaudhry and Farzana Kulsoom and Zahid Ullah Khan and Muhammad Aman and Sajid Ullah Khan and Abdullah Albanyan},
  doi          = {10.7717/peerj-cs.2760},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2760},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TASCI: Transformers for aspect-based sentiment analysis with contextual intent integration},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eternal-MAML: A meta-learning framework for cross-domain defect recognition. <em>PEERJCS</em>, <em>11</em>, e2757. (<a href='https://doi.org/10.7717/peerj-cs.2757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect recognition tasks for industrial product suffer from a serious lack of samples, greatly limiting the generalizability of deep learning models. Addressing the imbalance of defective samples often involves leveraging pre-trained models for transfer learning. However, when these models, pre-trained on natural image datasets, are transferred to pixel-level defect recognition tasks, they frequently suffer from overfitting due to data scarcity. Furthermore, significant variations in the morphology, texture, and underlying causes of defects across different industrial products often lead to a degradation in performance, or even complete failure, when directly transferring a defect classification model trained on one type of product to another. The Model-Agnostic Meta-Learning (MAML) framework can learn a general representation of defects from multiple industrial defect recognition tasks and build a foundational model. Despite lacking sufficient training data, the MAML framework can still achieve effective knowledge transfer among cross-domain tasks. We noticed there exists serious label arrangement issues in MAML because of the random selection of recognition tasks, which seriously affects the performance of MAML model during both training and testing phase. This article proposes a novel MAML framework, termed as Eternal-MAML, which guides the update of the classifier module by learning a meta-vector that shares commonality across batch tasks in the inner loop, and addresses the overfitting phenomenon caused by label arrangement issues in testing phase for vanilla MAML. Additionally, the feature extractor in this framework combines the advantages of the Squeeze-and-Excitation module and Residual block to enhance training stability and improve the generalization accuracy of model transfer with the learned initialization parameters. In the simulation experiments, several datasets are applied to verified the cross-domain meta-learning performance of the proposed Eternal-MAML framework. The experimental results show that the proposed framework outperforms the state-of-the-art baselines in terms of average normalized accuracy. Finally, the ablation studies are conducted to examine how the primary components of the framework affect its overall performance. Code is available at https://github.com/zhg-SZPT/Eternal-MAML.},
  archive      = {J_PEERJCS},
  author       = {Jipeng Feng and Haigang Zhang and Zhifeng Wang},
  doi          = {10.7717/peerj-cs.2757},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2757},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Eternal-MAML: A meta-learning framework for cross-domain defect recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing phishing detection with dynamic optimization and character-level deep learning in cloud environments. <em>PEERJCS</em>, <em>11</em>, e2640. (<a href='https://doi.org/10.7717/peerj-cs.2640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As cloud computing becomes increasingly prevalent, the detection and prevention of phishing URL attacks are essential, particularly in the Internet of Vehicles (IoV) environment, to maintain service reliability. In such a scenario, an attacker could send misleading phishing links, potentially compromising the system’s functionality or, at worst, leading to a complete shutdown. To address these emerging threats, this study introduces a novel Dynamic Arithmetic Optimization Algorithm with Deep Learning-Driven Phishing URL Classification (DAOA-DLPC) model for cloud-enabled IoV infrastructure. The candidate’s research utilizes character-level embeddings instead of word embeddings, as the former can capture intricate URL patterns more effectively. These embeddings are integrated with a deep learning model, the Multi-Head Attention and Bidirectional Gated Recurrent Units (MHA-BiGRU). To improve precision, hyperparameter tuning has been done using DAOA. The proposed method offers a feasible solution for identifying the phishing URLs, and the method achieves computational efficiency through the attention mechanism and dynamic hyperparameter optimization. The need for this work comes from the observation that the traditional machine learning approaches are not effective in dynamic environments like phishing threat landscapes in a dynamic environment such as the one of phishing threats. The presented DLPC approach is capable of learning new forms of phishing attacks in real time and reduce false positives. The experimental results show that the proposed DAOA-DLPC model outperforms the other models with an accuracy of 98.85%, recall of 98.49%, and F1-score of 98.38% and can effectively detect safe and phishing URLs in dynamic environments. These results imply that the proposed model is useful in distinguishing between safe and unsafe URLs than the conventional models.},
  archive      = {J_PEERJCS},
  author       = {Vishnukumar Ravula and Mangayarkarasi Ramaiah},
  doi          = {10.7717/peerj-cs.2640},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2640},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing phishing detection with dynamic optimization and character-level deep learning in cloud environments},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SODU2-NET: A novel deep learning-based approach for salient object detection utilizing U-NET. <em>PEERJCS</em>, <em>11</em>, e2623. (<a href='https://doi.org/10.7717/peerj-cs.2623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. To address this challenge posed by complex backgrounds in salient object detection is crucial for advancing the field. This article proposes a novel deep learning-based architecture called SODU2-NET (Salient object detection U2-Net) for salient object detection that utilizes the U-NET base structure. This model addresses a gap in previous work that focused primarily on complex backgrounds by employing a densely supervised encoder-decoder network. The proposed SODU2-NET employs sophisticated background subtraction techniques and utilizes advanced deep learning architectures that can discern relevant foreground information when dealing with complex backgrounds. Firstly, an enriched encoder block with full feature fusion (FFF) with atrous spatial pyramid pooling (ASPP) varying dilation rates to efficiently capture multi-scale contextual information, improving salient object detection in complex backgrounds and reducing the loss of information during down-sampling. Secondly the block includes an attention module that refines the decoder, is constructed to enhances the detection of salient objects in complex backgrounds by selectively focusing attention on relevant features. This allows the model to reconstruct detailed and contextually relevant information, which is essential to determining salient objects accurately. Finally, the architecture has been improved by adding a residual block at the encoder end, which is responsible for both saliency prediction and map refinement. The proposed network is designed to learn the transformation between input images and ground truth, enabling accurate segmentation of salient object regions with clear borders and accurate prediction of fine structures. SODU2-NET is demonstrated to have superior performance in five public datasets, including DUTS, SOD, DUT OMRON, HKU-IS, PASCAL-S, and a new real world dataset, the Changsha dataset. Based on a comparative assessment of the model FCN, Squeeze-net, Deep Lab, Mask R-CNN the proposed SODU2-NET is found and achieve an improvement of precision (6%), recall (5%) and accuracy (3%). Overall, approach shows promise for improving the accuracy and efficiency of salient object detection in a variety of settings.},
  archive      = {J_PEERJCS},
  author       = {Hyder Abbas and Shen Bing Ren and Muhammad Asim and Syeda Iqra Hassan and Ahmed A. Abd El-Latif},
  doi          = {10.7717/peerj-cs.2623},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2623},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SODU2-NET: A novel deep learning-based approach for salient object detection utilizing U-NET},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMSA-net: A deformable multiscale adaptive classroom behavior recognition network. <em>PEERJCS</em>, <em>11</em>, e2876. (<a href='https://doi.org/10.7717/peerj-cs.2876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the intelligent transformation of education, accurate recognition of students’ classroom behavior has become one of the key technologies for enhancing the quality of instruction and the efficacy of learning. However, in the recognition of target behavior in real classroom scenarios, due to the use of wide-angle or panoramic images for image acquisition, students in the back row are far away from monitoring devices, and their subtle body movements such as the small opening and closing of the mouth (to determine whether they are speaking), fine finger operations (to distinguish between reading books or operating mobile phones) are difficult to recognize. Moreover, there are occlusions and scale differences in the front and back rankings, which can easily cause confusion and interference with target features in the detection process, greatly limiting the accurate recognition ability of existing visual algorithms for classroom behavior. This article proposes a deformable multiscale adaptive classroom behavior recognition network. To improve the network’s capacity to model minute behavioral phenomena, the backbone section introduces a deformable self-attention dattention module, dynamically modifying the receptive field’s geometry to enhance the model’s concentration on the region of interest. To improve the network’s capacity for feature extraction and integration of behavior occlusion and classroom behavior at different scales, a proposal has been put forward the Multiscale Attention Feature Pyramid Structure (MSAFPS), to achieve multi-level feature aggregation after multiscale feature fusion, reducing the impact of mutual occlusion and scale differences in classroom behavior between front and back rows. In the detect section, we adopt the Wise Intersection Over Union (Wise-IoU) loss as our loss criterion, augmenting the evaluation framework with richer contextual cues to broaden its scope and elevate the network’s detection prowess. Extensive experimentation reveals that our proposed method outperforms rival algorithms on two widely adopted benchmark datasets: SCB-Dataset3-S (the Student Classroom Behavior Dataset–https://github.com/Whiffe/SCB-dataset) and we created object detection dataset DataMountainSCB (https://github.com/Chunyu-Dong/DataFountainSCB1) containing six types of behaviors.},
  archive      = {J_PEERJCS},
  author       = {Chunyu Dong and Jing Liu and Shenglong Xie},
  doi          = {10.7717/peerj-cs.2876},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2876},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DMSA-net: A deformable multiscale adaptive classroom behavior recognition network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Employing SAE-GRU deep learning for scalable botnet detection in smart city infrastructure. <em>PEERJCS</em>, <em>11</em>, e2869. (<a href='https://doi.org/10.7717/peerj-cs.2869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Internet of Things (IoT) devices in smart cities has revolutionized urban infrastructure while escalating the risk of botnet attacks that threaten essential services and public safety. This research addresses the critical need for intrusion detection and mitigation systems by introducing a novel hybrid deep learning model, Stacked Autoencoder–Gated Recurrent Unit (SAE-GRU), specifically designed for IoT networks in smart cities. The study targets the dual challenges of processing high-dimensional data and recognizing temporal patterns to identify and mitigate botnet activities in real time. The methodology integrates Stacked Autoencoders for reducing dimensionality and gated recurrent units for analyzing sequential data to ensure both accuracy and efficiency. An emulated smart city environment with diverse IoT devices and communication protocols provided a realistic testbed for evaluating the model. Results demonstrate significant improvements in detection performance with an average accuracy of 98.65 percent and consistently high precision and recall values. These findings enhance the understanding of IoT security by offering a scalable and resource-efficient solution for botnet detection. The functional investigation establishes a foundation for future research into adaptive security mechanisms that address emerging threats and highlights the practical potential of advanced deep learning techniques in safeguarding next-generation smart city ecosystems.},
  archive      = {J_PEERJCS},
  author       = {Usman Tariq and Tariq Ahamed Ahanger},
  doi          = {10.7717/peerj-cs.2869},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2869},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Employing SAE-GRU deep learning for scalable botnet detection in smart city infrastructure},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fish species identification on low resolution—a study with enhanced super-resolution generative adversarial network (ESRGAN), YOLO and VGG-16. <em>PEERJCS</em>, <em>11</em>, e2860. (<a href='https://doi.org/10.7717/peerj-cs.2860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An intelligent detection and recognition model for the fish species from camera footage is urgently required as fishery contributes to a large portion of the world economy, and these kinds of advanced models can aid fishermen on a large scale. Such models incorporating a pick-and-place machine can be beneficial to sorting different fish species in bulk without human intervention, significantly reducing costs for large-scale fishing industries. Existing methods for detecting and recognizing fish species have many limitations, such as limited scalability, detection accuracy, failure to detect multiple species, degraded performance at a lower resolution, or pinpointing the exact location of the fish. Modifying the head of a compelling deep learning model, namely VGG-16, with pre-trained weights, can be used to detect both the species of the fish and find the exact location of the fish in an image by implementing a modified You Only Look Once (YOLO) to incorporate the bounding box regression head. We have proposed using the Enhanced Super Resolution Generative Adversarial Network (ESRGAN) algorithm and the proposed neural network to amplify the image resolution by a factor of 4. With this method, an overall detection accuracy of 96.5% has been obtained. The experiment has been conducted based on a total of 9,460 images spread across nine species. After further improving the model, a pick-and-place machine could be integrated to quickly sort the fish according to their species in different large-scale fish industries.},
  archive      = {J_PEERJCS},
  author       = {Subhrangshu Adhikary and Saikat Banerjee and Rajani Singh and Ashutosh Dhar Dwivedi},
  doi          = {10.7717/peerj-cs.2860},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2860},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fish species identification on low resolution—a study with enhanced super-resolution generative adversarial network (ESRGAN), YOLO and VGG-16},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scaling laws for haralick texture features of linear gradients. <em>PEERJCS</em>, <em>11</em>, e2856. (<a href='https://doi.org/10.7717/peerj-cs.2856'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel analytical framework for understanding the relationship between the image gradients and the symmetries of the Gray Level Co-occurrence Matrix (GLCM). Analytical expression for four key features–sum average (SA), sum variance (SV), difference variance (DV), and entropy–were derived to capture their dependence on image’s gray-level quantization (Ng), the gradient magnitude (∇), and the displacement vector (d) through the corresponding GLCM. Scaling laws obtained from the exact analytical dependencies of Haralick features on Ng, ∇ and |d| show that SA and DV scale linearly with Ng, SV scales quadratically, and entropy follows a logarithmic trend. The scaling laws allow a consistent derivation of normalization factors that make Haralick features independent of the quantization scheme Ng. Numerical simulations using synthetic one-dimensional gradients validated our theoretical predictions. This theoretical framework establishes a foundation for consistent derivation of analytic expressions and scaling laws for Haralick features. Such an approach would streamline texture analysis across datasets and imaging modalities, enhancing the portability and interpretability of Haralick features in machine learning and medical imaging applications.},
  archive      = {J_PEERJCS},
  author       = {Sorinel A. Oprisan and Ana Oprisan},
  doi          = {10.7717/peerj-cs.2856},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2856},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Scaling laws for haralick texture features of linear gradients},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human pose estimation in physiotherapy fitness exercise correction using novel transfer learning approach. <em>PEERJCS</em>, <em>11</em>, e2854. (<a href='https://doi.org/10.7717/peerj-cs.2854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective To introduce and evaluate an efficient neural network approach for human pose estimation and correction during physical therapy exercises using wearable sensor data. Methods We leveraged benchmark data consisting of 276,625 records from wearable inertial and magnetic sensors. A novel method termed Random Forest Long Short-Term Memory (RFL), which integrates long short-term memory and Random Forest neural networks, was implemented for transfer feature engineering. The smartphone sensor data was used to generate new temporal and probabilistic features. These features were then utilized in machine learning methods to classify physical therapy exercises. Rigorous experiments, including k-fold validation and hyperparameter optimization, were conducted to validate the performance of the RFL approach. Results The RFL approach demonstrated superior performance, achieving a remarkable 99% accuracy with the Random Forest method. The rigorous experiments confirmed the efficacy and reliability of the method in classifying physical therapy exercises. Conclusions The proposed RFL method introduces a novel feature generation approach enhancing the accuracy of physical therapy exercise classification and correction. This innovative integration not only improves rehabilitation monitoring but also paves the way for more adaptive and intelligent physiotherapy assistance systems. By leveraging sensor data and advanced machine learning techniques, it has the potential to mitigate risks associated with disabilities and major diseases, thereby offering a feasible alternative to frequent clinic visits for consistent therapist guidance.},
  archive      = {J_PEERJCS},
  author       = {Aisha Naseer and Ali Raza and Hadeeqa Afzal and Aseel Smerat and Norma Latif Fitriyani and Yeonghyeon Gu and Muhammad Syafrudin},
  doi          = {10.7717/peerj-cs.2854},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2854},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Human pose estimation in physiotherapy fitness exercise correction using novel transfer learning approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging resource gaps in cross-lingual sentiment analysis: Adaptive self-alignment with data augmentation and transfer learning. <em>PEERJCS</em>, <em>11</em>, e2851. (<a href='https://doi.org/10.7717/peerj-cs.2851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-lingual sentiment analysis plays a crucial role in accurately interpreting emotions across diverse linguistic contexts. However, performance disparities remain a major challenge, particularly in fewer-resource (including medium-resource and low-resource) languages. This study proposes an adaptive self-alignment framework for large language models, incorporating novel data augmentation techniques and transfer learning strategies to mitigate resource imbalances. Comprehensive experiments conducted on 11 languages demonstrate that our approach consistently surpasses state-of-the-art baselines, achieving an average F1-score improvement of 7.35 points. Notably, our method exhibits exceptional effectiveness in fewer-resource languages, significantly narrowing the performance gap between fewer- and high-resource settings. With robust domain adaptation capabilities and strong potential for real-world industrial applications, this research establishes a new benchmark for multilingual sentiment analysis, advancing the development of more inclusive and equitable natural language processing solutions.},
  archive      = {J_PEERJCS},
  author       = {Li Chen and Shifeng Shang and Yawen Wang},
  doi          = {10.7717/peerj-cs.2851},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2851},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Bridging resource gaps in cross-lingual sentiment analysis: Adaptive self-alignment with data augmentation and transfer learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can executives’ digital background develop the level of AI utilization in enterprises. <em>PEERJCS</em>, <em>11</em>, e2848. (<a href='https://doi.org/10.7717/peerj-cs.2848'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital talent has emerged as a pivotal resource for advancing artificial intelligence (AI) within enterprises, and it constitutes a critical digital asset that firms prioritize in their AI development strategies, aiming to attract and retain such talent. To elucidate the interconnections and mechanisms linking the digital backgrounds of executives with the level of AI utilization in enterprises, this study undertakes an examination of the relationship between these two variables, drawing on a sample of Chinese A-share listed companies spanning from 2012 to 2022. The research findings are as follows: (1) The presence of a substantial number of executives with digital backgrounds within a company significantly enhances the development of AI utilization within the enterprise. (2) The influence of executives’ digital backgrounds on the level of AI utilization in enterprises is modulated through the enhancement of the enterprise’s total factor productivity. (3) Executives’ digital backgrounds elevate the level of AI utilization by bolstering their proficiency in applying digital technology. (4) Heterogeneity analysis, conducted based on the nature of enterprise property rights, geographical regions, and technological proficiency, reveals that the positive effect of executives’ digital backgrounds on AI utilization is more pronounced in private enterprises compared to state-owned enterprises. Regionally, the impact of executives’ digital backgrounds on AI utilization diminishes progressively from the eastern to the central and western regions. Robustness checks, including the substitution of key variables, model alteration, the application of the instrumental variable method, and the Heckman two-stage method, confirm the persistence of these findings. This article contributes to the understanding of the impact and pathways through which executives’ digital backgrounds influence AI utilization in enterprises against the backdrop of high-quality economic development. It enriches the scholarly discourse on executives’ digital backgrounds and AI utilization in enterprises, offering both theoretical support and practical insights for the advancement of AI within enterprises.},
  archive      = {J_PEERJCS},
  author       = {Ruichao Yu and Linrong Wu and Guiying Li and Zixin Wang},
  doi          = {10.7717/peerj-cs.2848},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2848},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Can executives’ digital background develop the level of AI utilization in enterprises},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards interpretable drug interaction prediction via dual-stage attention and bayesian calibration with active learning. <em>PEERJCS</em>, <em>11</em>, e2847. (<a href='https://doi.org/10.7717/peerj-cs.2847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Drug-drug interactions (DDIs) account for 17–23% of adverse drug reactions leading to hospitalization, with over 74,000 DDI-related events reported in the FDA Adverse Event Reporting System (FAERS) during 2023. While recent computational methods focus on improving prediction accuracy, they suffer from high false-positive rates (>45%) and often function as black-box models without biological interpretability. Methods We propose Dual-stage attention and Bayesian calibration with active learning Drug-Drug Interaction (DABI-DDI), a novel framework integrating: (1) A dual-stage attention mechanism with LSTM networks for capturing temporal dependencies in drug interactions, (2) a Bayesian calibration approach with beta-binomial modeling for refining interaction signals and reducing false positives, (3) an active learning strategy for efficient sample selection, and (4) a network pharmacology component linking drug interactions to underlying biological mechanisms. The model was validated using data from FAERS, DrugBank, and STRING databases, with comprehensive evaluation on both computational performance and biological interpretability. Results DABI-DDI achieved superior performance (AUC = 0.947, PR_AUC = 0.944). Bayesian calibration improved adverse event detection accuracy (94% vs. 54% AUC), while network pharmacology revealed key molecular mechanisms through enzyme-transporter interactions. Ablation studies demonstrated each component’s significance, with active learning maintaining performance while reducing training data requirements. Conclusion We present DABI-DDI, an integrated feature extraction framework that successfully addresses key challenges in DDIs prediction through three major innovations: Temporal pattern recognition, reducing false positives, and biological interpretability. Most importantly, the framework demonstrates strong clinical applicability by efficiently identifying high-risk drug combinations while providing mechanistic insights through enzyme-transporter pathway analysis. This approach bridges the gap between computational prediction and clinical understanding, offering a promising tool for safer drug combination therapy.},
  archive      = {J_PEERJCS},
  author       = {Rongpei Li and Yufang Zhang and Heqi Sun and Shenggeng Lin and Guihua Jia and Yitian Fang and Chen Zhang and Xiaotong Song and Jianwei Zhao and Lyubin Hu and Yajing Yuan and Xueying Mao and Jiayi Li and Aman Kaushik and Dandan An and Dongqing Wei},
  doi          = {10.7717/peerj-cs.2847},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2847},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Towards interpretable drug interaction prediction via dual-stage attention and bayesian calibration with active learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive fusion-based data augmentation method for abstract dialogue summarization. <em>PEERJCS</em>, <em>11</em>, e2845. (<a href='https://doi.org/10.7717/peerj-cs.2845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dialogue summarization is necessary for information retrieval, and the training of abstract dialogue summarization models heavily rely on large amounts of labeled data. However, manual summarization of long dialogue is labor-costing and time-consuming. To solve this problem, this article proposes a data augmentation method for dialogue summary based on adaptive augmentation fusion (AAF), integrating the strengths of both Minor Perturbation Augmentation (MPA) and Semantic Reconstructive Augmentation (SRA) to balance model learning effectiveness and generalization capabilities. We first integrated existing enhancement methods to address the problem of insufficient annotated data for dialogue summarization. The experimental results on both the DialogSum and SAMSum datasets demonstrate that the AAF method achieves significant improvements in ROUGE scores under resource-constrained conditions, outperforming baseline approaches. Furthermore, it was validated that the selection of the amount of augmented data has a significant impact on model training results under resource-constrained conditions. We have publically released our code at https://github.com/alolke/AAF.},
  archive      = {J_PEERJCS},
  author       = {Weihao Li and Dan Jiang and Han Zhang and Kejing Xiao and Shaozhong Cao},
  doi          = {10.7717/peerj-cs.2845},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2845},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An adaptive fusion-based data augmentation method for abstract dialogue summarization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient glaucoma screening with modular convolution-involution cascade architecture. <em>PEERJCS</em>, <em>11</em>, e2844. (<a href='https://doi.org/10.7717/peerj-cs.2844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated glaucoma detection from retinal fundus images plays a crucial role in facilitating early intervention and improving the management of this progressive ocular condition. Although convolutional neural networks (CNNs) have significantly advanced image analysis, current CNN-based models encounter two major limitations. First, they rely primarily on convolutional operations, which restrict the ability to capture cross-channel correlations effectively due to the channel-specific focus of these operations. Second, they often depend on fully-connected (FC) layers for classification, which can introduce unnecessary complexity and limit adaptability, potentially impacting overall classification performance. This study introduces the Modular Convolution-Involution Cascade Network (MCICNet), an innovative CNN architecture designed to address these challenges in the context of glaucoma detection. The model employs a combination of convolution and involution operations in a cascade structure, allowing for the effective capture of inter-channel dependencies within the feature extraction process. Furthermore, the classification phase integrates light gradient boosting machine (LightGBM) as a replacement for traditional FC layers, offering enhanced precision and generalization while reducing model complexity. Extensive experiments conducted on the LAG and ACRIMA datasets demonstrate that MCICNet achieves significant improvements compared to existing CNN and transformer-based models. The model attained a classification accuracy of 95.6% on the LAG dataset and 96.2% on ACRIMA, outperforming nine widely used CNN architectures (AlexNet, MobileNetV2, SqueezeNet, ResNet18, GoogLeNet, DenseNet121, EfficientNetB0, ShuffleNet, and VGG16), as well as three transformer-based models (ViT, MaxViT, and SwinT). Additionally, MCICNet showed superior performance over its variant without involution (MCICNet-NoInvolution). With only 0.9 million parameters, MCICNet demonstrates substantial efficiency in resource utilization alongside its high learning capability, establishing it as an advanced and computationally efficient solution for glaucoma detection.},
  archive      = {J_PEERJCS},
  author       = {Mohamed Mouhafid and Yatong Zhou and Chunyan Shan and Zhitao Xiao},
  doi          = {10.7717/peerj-cs.2844},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2844},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Towards efficient glaucoma screening with modular convolution-involution cascade architecture},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social-aware trajectory prediction using goal-directed attention networks with egocentric vision. <em>PEERJCS</em>, <em>11</em>, e2842. (<a href='https://doi.org/10.7717/peerj-cs.2842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel social-goal attention networks (SGANet) model that employs a vision-based multi-stacked neural network framework to predict multiple future trajectories for both homogeneous and heterogeneous road users. Unlike existing methods that focus solely on one dataset type and treat social interactions, temporal dynamics, destination point, and uncertainty behaviors independently, SGANet integrates these components into a unified multimodal prediction framework. A graph attention network (GAT) captures socially-aware interaction correlation, a long short-term memory (LSTM) network encodes temporal dependencies, a goal-directed forecaster (GDF) estimates coarse future goals, and a conditional variational autoencoder (CVAE) generates multiple plausible trajectories, with multi-head attention (MHA) and feed-forward networks (FFN) refining the final multimodal trajectory prediction. Evaluations on homogeneous datasets (JAAD and PIE) and the heterogeneous TITAN dataset demonstrate that SGANet consistently outperforms previous benchmarks across varying prediction horizons. Extensive experiments highlight the critical role of socially-aware interaction weighting in capturing road users’ influence on ego-vehicle maneuvers while validating the effectiveness of each network component, thereby demonstrating the advantages of multi-stacked neural network integration for trajectory prediction. The dataset is available at https://usa.honda-ri.com/titan.},
  archive      = {J_PEERJCS},
  author       = {Lia Astuti and Chui-Hong Chiu and Yu-Chen Lin and Ming-Chih Lin},
  doi          = {10.7717/peerj-cs.2842},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2842},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Social-aware trajectory prediction using goal-directed attention networks with egocentric vision},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing unreadable QR codes: A deep learning based super resolution strategy. <em>PEERJCS</em>, <em>11</em>, e2841. (<a href='https://doi.org/10.7717/peerj-cs.2841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quick-response (QR) codes have become an integral component of the digital transformation process, facilitating fast and secure information sharing across various sectors. However, factors such as low resolution, misalignment, panning and rotation, often caused by the limitations of scanning devices, can significantly impact their readability. These distortions prevent reliable extraction of embedded data, increase processing times and pose potential security risks. In this study, four super-resolution models Enhanced Deep Super Resolution (ESDR) network, Very Deep Super Resolution (VDSR) network, Efficient Sub-Pixel Convolutional Network (ESPCN) and Super Resolution Convolutional Neural Network (SRCNN) are used to mitigate resolution loss, rotation errors and misalignment issues. To simulate scanner-induced distortions, a dataset of 16,000 computer-generated QR codes with various filters was used. In addition, super-resolution models were applied to 4,593 QR codes that OpenCV’s QRCodeDetector function could not decode in real-world scans. The results showed that EDSR, VDSR, ESPCN and SRCNN successfully read 4,261, 4,229, 4,255 and 4,042 of these QR codes, respectively. Furthermore, the EDSR, VDSR, ESPCN and SRCNN models trained by OpenCV’s deep learning-based WeChat QR Code Detector function to read 2,899 QR codes that were initially unreadable and simulated on the computer were able to successfully read 2,891, 2,884, 2,433 and 2,560 of them, respectively. These findings show that super-resolution models can effectively improve the readability of degraded or low-resolution QR codes.},
  archive      = {J_PEERJCS},
  author       = {Yasin Sancar},
  doi          = {10.7717/peerj-cs.2841},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2841},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Reconstructing unreadable QR codes: A deep learning based super resolution strategy},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMPACT: An interactive multi-disease prevention and counterfactual treatment system using explainable AI and a multimodal LLM. <em>PEERJCS</em>, <em>11</em>, e2839. (<a href='https://doi.org/10.7717/peerj-cs.2839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-disease conditions strain the body’s defenses, complicating recovery and increasing mortality risk. Therefore, effective concurrent prevention of multiple diseases is essential for mitigating complications and improving overall well-being. Explainable artificial intelligence (XAI) with an advanced multimodal large language model (LLM) can create an interactive system enabling the general public to engage in natural language without any specialized knowledge prerequisites. Counterfactual explanation, an XAI method, offers valuable insights by suggesting adjustments to patient features to minimize disease risks. However, addressing multiple diseases simultaneously poses challenging barriers. This article proposes an interactive multi-disease prevention system that uses Google Gemini Pro, a multimodal LLM, and a non-dominated sorting genetic algorithm, namely NSGA-II, to overcome such problems. This system recommends changes in feature values to concurrently minimize the risk of diseases such as heart attacks and diabetes. The system facilitates personalized feature value selection, significantly reducing disease attack probabilities to as low as possible. Such an approach holds the potential to simultaneously address the unresolved issue of preventing and managing multiple diseases for the general public.},
  archive      = {J_PEERJCS},
  author       = {Prasant Kumar Mohanty and Sharmila Anand John Francis and Rabindra Kumar Barik and K. Hemant Kumar Reddy and Diptendu Sinha Roy and Manob Jyoti Saikia},
  doi          = {10.7717/peerj-cs.2839},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2839},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IMPACT: An interactive multi-disease prevention and counterfactual treatment system using explainable AI and a multimodal LLM},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multipath subflow transmission scheduling optimization algorithm based on cost-performance balance. <em>PEERJCS</em>, <em>11</em>, e2838. (<a href='https://doi.org/10.7717/peerj-cs.2838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a cost-performance balance algorithm for multipath data transmission within an Software Defined Network (SDN)-5G-Multipath Transmission Control Protocol (MPTCP) network framework. A unified communication interface is designed to schedule transmission paths, optimizing data flow allocation dynamically. To achieve a balance between network throughput and transmission costs, traffic volume and associated expenses are mapped into physical and virtual buffer queues for real-time substream updates. The relationship between unreceived subflows and consumption costs is mathematically modelled, with both parameters represented using vector matrices. Lyapunov stability theory is applied to determine the optimal cost-performance balance. In contrast, key evaluation metrics—including substream transmission efficiency, consumption expenditure, and overall balance control—are introduced to assess performance. Comparative analysis against classical price balance control algorithms demonstrates that the proposed strategy significantly enhances data transmission efficiency while reducing costs. Experimental results validate the effectiveness of the model in achieving cost-performance balance control for multi-channel transmission of large-scale files (ranging from 2 to 20 GB) under varying network conditions. The findings highlight the potential of this approach for optimizing high-performance, cost-efficient data transmission in next-generation communication networks.},
  archive      = {J_PEERJCS},
  author       = {Xinyu Sun},
  doi          = {10.7717/peerj-cs.2838},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2838},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multipath subflow transmission scheduling optimization algorithm based on cost-performance balance},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel user-centric happiness model for personalized tour recommendations. <em>PEERJCS</em>, <em>11</em>, e2837. (<a href='https://doi.org/10.7717/peerj-cs.2837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel personalized tour recommendation model, the Happiness Model (HM), is presented. The HM optimizes itineraries by considering traveler satisfaction as a function of time and maximizing it over the trip duration. The model integrates the Item Constraints Data Model (ICDM) to reduce data dimensionality and search space. By considering various activities within different points of interest (POIs) and minimizing wasted time, the HM overcomes the limitations of existing methods. Unlike existing POI-centric models, the HM is time-centric, creating tour recommendations that maximize user satisfaction throughout the trip. Experimental results demonstrate the model’s effectiveness in generating personalized tour recommendations aligned with user preferences. The HM achieves an average satisfaction score of 0.85 across multiple datasets, outperforming traditional models such as the Time-Dependent Orienteering Problem with Time Windows (TOPTW), which achieves an average score of 0.72. Additionally, the HM reduces waiting times by 30% and increases the number of recommended POIs by 20% compared to existing methods. These results highlight the HM’s ability to provide more efficient and enjoyable travel experiences.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Alatiyyah},
  doi          = {10.7717/peerj-cs.2837},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2837},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel user-centric happiness model for personalized tour recommendations},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical dispersed flow simulation of fire-flake particle dynamics and its learning representation. <em>PEERJCS</em>, <em>11</em>, e2836. (<a href='https://doi.org/10.7717/peerj-cs.2836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose methods for simulating the detailed flow of dispersed fire-flake particles in response to the movement of a flame, using chaotic advection and various buoyant flow techniques. Furthermore, we utilize these techniques to gather a synthetic dataset of detailed fire-flake particles and extend the solver to represent the movement of fire-flake particles based on learning-based approaches. Fire-flake particles not only exhibit unique and complex movements on their own, but they are also significantly influenced by the movement of the flame and the surrounding airflow. Modeling the flow of fire-flake particles realistically is challenging due to their chaotic and constantly changing nature. Instead of explicitly modeling the complex fire-flake particles in the flame based on fluid mechanics, this article efficiently approximates the chaotic motion of fire-flake particles using two approaches: 1) chaotic advection to simulate the flow and 2) controlled buoyant flow, which varies based on the temperature and lifespan of the fire-flake particles. Additionally, we collect a fire-flake dataset through this simulation and extends the solver to learn the representation of fire-flake motion using neural networks. During the advection process of fire-flake particles, a new stochastic solver is used to calculate the subgrid interactions between them. In this article, not only we propose algorithms that can express these techniques through numerical simulation, but we also extend this solver using artificial intelligence techniques to enable learning representation. By using the proposed technique, it is possible to efficiently simulate fire-flake particles with various movements in chaotic regions, and it allows for more detailed representation of fire-flake particles compared to existing methods. Unlike the typical random walk approach that adds noise randomly to the movement, our method considers the size and direction of the flame. This allows us to express fire-flake particles stably in most scenes without the need for parameter adjustments.},
  archive      = {J_PEERJCS},
  author       = {Jong-Hyun Kim and Jung Lee},
  doi          = {10.7717/peerj-cs.2836},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2836},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Numerical dispersed flow simulation of fire-flake particle dynamics and its learning representation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A feature selection method utilizing path accumulation cost, redundancy minimization, and interaction maximization for the diagnosis of coronary heart disease. <em>PEERJCS</em>, <em>11</em>, e2834. (<a href='https://doi.org/10.7717/peerj-cs.2834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Coronary heart disease (CHD) is a major cause of mortality worldwide, with an increasing trend of affecting younger populations. The asymptomatic early stages and rapid progression of CHD make diagnosis challenging, necessitating efficient diagnostic approaches. Methods We propose a novel algorithm that focuses on accumulating soft path costs to discern crucial indicators from extensive diagnostic tests, aiming to improve early CHD identification. Our approach emphasizes feature interaction using an interaction accumulation evaluation function to identify features with maximal interaction and minimal redundancy. A new stopping criterion based on information gain ratio is also introduced. Results Experimental outcomes demonstrate that our algorithm outperforms several classical algorithms in terms of classification accuracy and feature dimension reduction, while also identifying highly correlated feature subsets. Conclusion The proposed approach offers an efficient solution for early detection of CHD by identifying critical indicators, reducing diagnostic complexity, and improving predictive accuracy, thus potentially leading to more effective CHD management.},
  archive      = {J_PEERJCS},
  author       = {Jiayao Jiang and Zheng Yue and Hongling Zhu and Yan Wang and Hongsen Cai and Wenguang Hou},
  doi          = {10.7717/peerj-cs.2834},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2834},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A feature selection method utilizing path accumulation cost, redundancy minimization, and interaction maximization for the diagnosis of coronary heart disease},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCA-YOLOv8n: A real-time and efficient fruit chunks detection algorithm for meal-assistance robot. <em>PEERJCS</em>, <em>11</em>, e2832. (<a href='https://doi.org/10.7717/peerj-cs.2832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The advancement of assistive technologies for individuals with disabilities has increased the demand for efficient and accurate object detection algorithms, particularly in meal-assistance robots designed to identify and handle food items such as fruit chunks. However, existing algorithms for fruit chunk detection often suffer from prolonged inference times and insufficient accuracy. Methods We propose an improved YOLOv8n algorithm optimized for real-time, high-accuracy fruit chunk detection. The Universal Inverted Bottleneck (UIB) module has been integrated into the original C2f structure, significantly reducing the model’s parameter count while preserving detection accuracy. Furthermore, the coordinate attention (CA) mechanism has been incorporated into the detection head to enhance the focus on fruit chunk regions within complex backgrounds while suppressing irrelevant features, thus improving detection performance. Additionally, the ADown module from YOLOv9 has been embedded into the YOLOv8 backbone network, further increasing accuracy and reducing the number of parameters. Results Experimental results indicate that these enhancements substantially improve detection accuracy while reducing model size. Specifically, the optimized model achieves a 1.9 MB reduction in size, a decrease of 2.5 GFLOPs in parameter count, and an increase in mAP50 and mAP50-95 by 2.1% and 3.3%, respectively. The improved algorithm (UCA-YOLOv8n) enables real-time, accurate detection of various fruit chunks. Comparative analyses with other mainstream object detection algorithms further demonstrate the superiority and effectiveness of the proposed method.},
  archive      = {J_PEERJCS},
  author       = {Fei Liu and Mingyue Hu},
  doi          = {10.7717/peerj-cs.2832},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2832},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {UCA-YOLOv8n: A real-time and efficient fruit chunks detection algorithm for meal-assistance robot},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid model based on CNN-LSTM for assessing the risk of increasing claims in insurance companies. <em>PEERJCS</em>, <em>11</em>, e2830. (<a href='https://doi.org/10.7717/peerj-cs.2830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a hybrid model to assist insurance companies accurately assess the risk of increasing claims for their premiums. The model integrates long short-term memory (LSTM) networks and convolutional neural networks (CNN) to analyze historical claim data and identify emerging risk trends. We analyzed data obtained from insurance companies and found that the hybrid CNN-LSTM model outperforms standalone models in accurately assessing and categorizing risk levels. The proposed CNN-LSTM model achieved an accuracy of 98.5%, outperforming the standalone CNN (95.8%) and LSTM (92.6%). We implemented 10-fold cross-validation to ensure robustness, confirming consistent performance across different data splits. Furthermore, we validated the model on an external dataset to assess its generalizability. The results demonstrate that the model effectively classifies insurance risks in different market environments, highlighting its potential for real-world applications. Our study contributes to the insurance industry by providing valuable insights for effective risk management strategies and highlights the model’s broader applicability in global insurance markets.},
  archive      = {J_PEERJCS},
  author       = {Walaa Gamaleldin and Osama Attayyib and Mrim M. Alnfiai and Faiz Abdullah Alotaibi and Ruixing Ming},
  doi          = {10.7717/peerj-cs.2830},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2830},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid model based on CNN-LSTM for assessing the risk of increasing claims in insurance companies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI framework for DRIVE model based mental health detection in text: A case study on how coping strategies are expressed during COVID-19. <em>PEERJCS</em>, <em>11</em>, e2828. (<a href='https://doi.org/10.7717/peerj-cs.2828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background This article defines an artificial intelligence framework to detect individual’s mental health (MH) status on social networks. The proposed framework, which consists of four main modules, aims to analyze the emotions that are expressed by social network users in their text posts and identify their mental coping strategies, resources, and demands based on The Demands-Resources-Individual Effects (DRIVE) model. Although sentiment analysis (SA) is effective in analyzing the polarity of the text, it is limited in detecting the mental health status in terms of the coping strategies, available resources, or encountered stressors. This study illustrates such limitations in detecting the coping strategies and shows the effectiveness of the coping-based analysis. The work also reveals the phrases and topics that were used by individuals to express their coping strategies which provides a novel outlook of the individuals’ psychological coping within their environment. Methods The social network X is used to collect the coping strategies expressed by people who experienced stress during COVID-19 from November 2019 to May 2022. Text was processed using natural language processing (NLP). A sample of posts was coded into a positive or negative coping category and one of eight subtypes. SA and statistical analysis were performed to compare SA results with coded coping strategies. Latent Dirichlet Allocation and bigram NLP were applied to identify main themes and terminologies. Coping classification models were created and tested. Results The findings reveal that 70% of posts show positive coping strategies. The main positive coping themes included self-care, seeking help, positive reframing, engaging in prayers and meditation, employing humor through sarcasm, and implementing a practical mindset. Conversely, the remaining 30% of posts expressed negative coping themes, such as conspiracy thoughts, wishful or hopeless thinking, and negative perceptions. The coping classification models achieved a reliable predictive level with an average accuracy of 74.8%. Categorizing coping strategies using SA methods, particularly TextBlob and VADER, revealed high miscategorization rates, especially for negative coping strategies. Bigrams and LDA analysis identified distinct word patterns in positive and negative coping strategies, with emojis playing a significant role in emotional expression across both categories. Conclusion The article defined a framework for a MH detector based on the DRIVE model. It highlighted the resilience and adaptive responses of individuals in times of crisis. It also focused on coping and identified physical, emotional, and social support and positive reframing as major positive strategies; and the spread of false information and loss of social support as negative coping strategies. The applied coping classification models showed reliable performance in distinguishing between positive and negative coping categories.},
  archive      = {J_PEERJCS},
  author       = {Loulwah AlSumait and Altaf AlFarhan and Hasah AlHeneidi},
  doi          = {10.7717/peerj-cs.2828},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2828},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {AI framework for DRIVE model based mental health detection in text: A case study on how coping strategies are expressed during COVID-19},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic prediction of carbon prices based on the multi-frequency combined model. <em>PEERJCS</em>, <em>11</em>, e2827. (<a href='https://doi.org/10.7717/peerj-cs.2827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a central participant and important leader in the global climate governance system, China is facing the urgent need to predict and regulate the price of carbon emissions to promote the sound development of its carbon market. In this article, a rolling prediction model based on Least Absolute Shrinkage and Selection Operator-cheetah optimization algorithm-extreme gradient boosting (Lasso-COA-XGBoost) carbon price decomposition integration is proposed to address the defects of low prediction accuracy and insufficient model stability of a single machine learning model in the carbon price prediction problem. During the modeling process, the adaptive Lasso method is first employed to select factors from 15 primary indicators of carbon prices, identifying the most important influencing factors. Next, the COA-XGBoost model is built and the parameters of the XGBoost model are optimized using the COA algorithm. Finally, the complete ensemble empirical Mode Decomposition with adaptive noise (CEEMDAM) method is utilized to decompose the residual sequence of the COA-XGBoost model and reconstruct it into high-frequency and low-frequency components. Appropriate frequency models are applied to achieve error correction, thereby constructing the combined Lasso-COA-XGBoost-CEEMDAN model. To further enhance the predictive accuracy and practicality of the model, a rolling time window is introduced for forecasting in the Hubei and Guangzhou carbon emission trading markets, ensuring that the forecasting model can adapt to market changes in real-time. The experimental results show that, taking the carbon price prediction in Hubei as an example, the proposed hybrid model has a significant improvement in prediction accuracy compared with the comparison model (XGBoost model): the RMSE is improved by 99.9987%, the MAE is improved by 99.9039%, the MAPE is improved by 99.9960%, and the R2 is improved by 0.2004%, and the advantages of this hybrid model are also verified in other experiments. The results provide an effective experimental method for future carbon price prediction.},
  archive      = {J_PEERJCS},
  author       = {Yonghui Duan and Yingying Fan and Xiang Wang and Kaige Liu and Xiaotong Zhang},
  doi          = {10.7717/peerj-cs.2827},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2827},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic prediction of carbon prices based on the multi-frequency combined model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building extraction from remote sensing images based on multi-scale attention gate and enhanced positional information. <em>PEERJCS</em>, <em>11</em>, e2826. (<a href='https://doi.org/10.7717/peerj-cs.2826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting buildings from high-resolution remote sensing images is currently a research hotspot in the field of remote sensing applications. Deep learning methods have significantly improved the accuracy of building extraction, but there are still deficiencies such as blurred edges, incomplete structures and loss of details in the extraction results. To obtain accurate contours and clear boundaries of buildings, this article proposes a novel building extraction method utilizing multi-scale attention gate and enhanced positional information. By employing U-Net as the main framework, this article introduces a multi-scale attention gate module in the encoder, which effectively improves the ability to capture multi-scale information, and designs a module in the decoder to enhance the positional information of the features, allowing for more precise localization and extraction of the shape and edge information of buildings. To validate the effectiveness of the proposed method, comprehensive evaluations were conducted on three benchmark datasets, Massachusetts, WHU, and Inria. The comparative analysis with six state-of-the-art models (SegNet, DeepLabv3+, U-Net, DSATNet, SDSC-Unet, and BuildFormer) demonstrates consistent performance improvements in intersection over union (IoU) metrics. Specifically, the proposed method achieves IoU increments of 2.19%, 3.31%, 3.10%, 2.00%, 3.35%, and 3.48% respectively on Massachusetts dataset, 1.26%, 4.18%, 1.18%, 2.01%, 2.03%, and 2.29% on WHU dataset, and 0.87%, 5.25%, 2.02%, 5.55%, 4.39%, and 1.18% on Inria dataset. The experimental results indicate that the proposed method can effectively integrate multi-scale features and optimize the extracted building edges, achieving superior performance compared to existing methodologies in building extraction tasks.},
  archive      = {J_PEERJCS},
  author       = {Rui Xu and Renzhong Mao and Zhenxing Zhuang and Fenghua Huang and Yihui Yang},
  doi          = {10.7717/peerj-cs.2826},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2826},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Building extraction from remote sensing images based on multi-scale attention gate and enhanced positional information},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A genetic programming-based ensemble method for long-term electricity demand forecasting. <em>PEERJCS</em>, <em>11</em>, e2825. (<a href='https://doi.org/10.7717/peerj-cs.2825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel genetic programming-based ensemble method for forecasting long-term electricity consumption in Ethiopia. The technique utilizes a two-stage ensemble approach to project Ethiopia’s electricity consumption through 2031. In the initial stage, genetic algorithms, particle swarm optimization, and simulated annealing methods are applied to various regression models (linear, quadratic, and exponential). The preliminary forecast values generated in this stage were further refined in the second stage. Here, the genetic programming method was utilized to develop a formula based on the initial forecast values, which then provided the final forecast results. The most accurate predictions in the first stage were obtained using the GA_Quadratic, PSO_Quadratic, and SA_Quadratic methods, resulting in mean absolute percentage error (MAPE) values of 3.61, 3.63, and 4.68, respectively. In the second stage, the GP-based prediction achieved an even lower MAPE value of 2.83. Other error metrics, including MSE, root mean square error (RMSE), and R2, were also evaluated, with the proposed model outperforming all methods from the first stage on these metrics. The study projected Ethiopia’s total annual electricity consumption through 2031 under two different scenarios. Both scenarios indicate that by 2031, electricity consumption will have tripled compared to 2021 levels.},
  archive      = {J_PEERJCS},
  author       = {Hayat Ahmed Issa and Hasan Hüseyin Çevik and Ahmet Yilmaz and Mehmet Cunkas},
  doi          = {10.7717/peerj-cs.2825},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2825},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A genetic programming-based ensemble method for long-term electricity demand forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversified caching algorithm with cooperation between edge servers. <em>PEERJCS</em>, <em>11</em>, e2824. (<a href='https://doi.org/10.7717/peerj-cs.2824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing makes up for the high latency of the central cloud network by deploying server resources in close proximity to users. The storage and other resources configured by edge servers are limited, and a reasonable cache replacement strategy is conducive to improving the cache hit ratio of edge services, thereby reducing service latency and enhancing service quality. The spatiotemporal correlation of user service request distribution brings opportunities and challenges to edge service caching. The collaboration between edge servers is often ignored in the existing research work for caching decisions, which can easily lead to a low edge cache hit rate, thereby reducing the efficiency of edge resource use and service quality. Therefore, this article proposes a diversified caching method to ensure the diversity of edge cache services, utilizing inter-server collaboration to enhance the cache hit rate. After the service request reaches the server, if it misses, the proposed algorithm will judge whether the neighbor node can provide services through the cache information of the neighbor node, and then the server and the neighbor node jointly decide how to cache the service. At the same time, the performance of the proposed diversified caching method is evaluated through a large number of simulation experiments, and the experimental results show that the proposed method can improve the cache hit rate by 27.01–37.43%, reduce the average service delay by 25.57–30.68%, and with the change of the scale of the edge computing platform, the proposed method can maintain good performance.},
  archive      = {J_PEERJCS},
  author       = {Yongxuan Sang and Yukang Guo and Bo Wang and Ying Song},
  doi          = {10.7717/peerj-cs.2824},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2824},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Diversified caching algorithm with cooperation between edge servers},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Validation of automated paper screening for esophagectomy systematic review using large language models. <em>PEERJCS</em>, <em>11</em>, e2822. (<a href='https://doi.org/10.7717/peerj-cs.2822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Large language models (LLMs) offer a potential solution to the labor-intensive nature of systematic reviews. This study evaluated the ability of the GPT model to identify articles that discuss perioperative risk factors for esophagectomy complications. To test the performance of the model, we tested GPT-4 on narrower inclusion criterion and by assessing its ability to discriminate relevant articles that solely identified preoperative risk factors for esophagectomy. Methods A literature search was run by a trained librarian to identify studies (n = 1,967) discussing risk factors to esophagectomy complications. The articles underwent title and abstract screening by three independent human reviewers and GPT-4. The Python script used for the analysis made Application Programming Interface (API) calls to GPT-4 with screening criteria in natural language. GPT-4’s inclusion and exclusion decision were compared to those decided human reviewers. Results The agreement between the GPT model and human decision was 85.58% for perioperative factors and 78.75% for preoperative factors. The AUC value was 0.87 and 0.75 for the perioperative and preoperative risk factors query, respectively. In the evaluation of perioperative risk factors, the GPT model demonstrated a high recall for included studies at 89%, a positive predictive value of 74%, and a negative predictive value of 84%, with a low false positive rate of 6% and a macro-F1 score of 0.81. For preoperative risk factors, the model showed a recall of 67% for included studies, a positive predictive value of 65%, and a negative predictive value of 85%, with a false positive rate of 15% and a macro-F1 score of 0.66. The interobserver reliability was substantial, with a kappa score of 0.69 for perioperative factors and 0.61 for preoperative factors. Despite lower accuracy under more stringent criteria, the GPT model proved valuable in streamlining the systematic review workflow. Preliminary evaluation of inclusion and exclusion justification provided by the GPT model were reported to have been useful by study screeners, especially in resolving discrepancies during title and abstract screening. Conclusion This study demonstrates promising use of LLMs to streamline the workflow of systematic reviews. The integration of LLMs in systematic reviews could lead to significant time and cost savings, however caution must be taken for reviews involving stringent a narrower and exclusion criterion. Future research is needed and should explore integrating LLMs in other steps of the systematic review, such as full text screening or data extraction, and compare different LLMs for their effectiveness in various types of systematic reviews.},
  archive      = {J_PEERJCS},
  author       = {Rashi Ramchandani and Eddie Guo and Esra Rakab and Jharna Rathod and Jamie Strain and William Klement and Risa Shorr and Erin Williams and Daniel Jones and Sebastien Gilbert},
  doi          = {10.7717/peerj-cs.2822},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2822},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Validation of automated paper screening for esophagectomy systematic review using large language models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative evaluation of approaches & tools for effective security testing of web applications. <em>PEERJCS</em>, <em>11</em>, e2821. (<a href='https://doi.org/10.7717/peerj-cs.2821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is generally accepted that adopting both static application security testing (SAST) and dynamic application security testing (DAST) approaches is vital for thorough and effective security testing. However, this suggestion has not been comprehensively evaluated, especially with regard to the individual risk categories mentioned in Open Web Application Security Project (OWASP) Top 10:2021 and common weakness enumeration (CWE) Top 25:2023 lists. Also, it is rare to find any evidence-based recommendations for effective tools for detecting vulnerabilities from a specific risk category or severity level. These shortcomings increase both the time and cost of systematic security testing when its need is heightened by increasingly frequent and preventable incidents. This study aims to fill these gaps by empirically testing seventy-five real-world Web applications using four SAST and five DAST tools. Only popular, free, and open-source tools were selected and each Web application was scanned using these nine tools. From the report generated by these tools, we considered two parameters to measure effectiveness: count and severity of the vulnerability found. We also mapped the vulnerabilities to OWASP Top 10:2021 and CWE Top 25:2023 lists. Our results show that using only DAST tools is the preferred option for four OWASP Top 10:2021 risk categories while using only SAST tools is preferred for only three risk categories. Either approach is effective for two of the OWASP Top 10:2021 risk categories. For CWE Top 25:2023 list, all three approaches were equally effective and found vulnerabilities belonging to three risk categories each. We also found that none of the tools were able to detect any vulnerability in one OWASP Top 10:2021 risk category and in eight CWE Top 25:2023 categories. This highlights a critical limitation of popular tools. The most effective DAST tool was OWASP Zed Attack Proxy (ZAP), especially for detecting vulnerabilities in broken access control, insecure design, and security misconfiguration risk categories. Yasca was the best-performing SAST tool, and outperformed all other tools at finding high-severity vulnerabilities. For medium-severity and low-severity levels, the DAST tools Iron Web application Advanced Security testing Platform (WASP) and Vega performed better than all the other tools. These findings reveal key insights, such as, the superiority of DAST tools for detecting certain types of vulnerabilities and the indispensability of SAST tools for detecting high-severity issues (due to detailed static code analysis). This study also addresses significant limitations in previous research by testing multiple real-world Web applications across diverse domains (technology, health, and education), enhancing generalization of the findings. Unlike studies that rely primarily on proprietary tools, our use of open-source SAST and DAST tools ensures better reproducibility and accessibility for organizations with limited budget.},
  archive      = {J_PEERJCS},
  author       = {Sana Qadir and Eman Waheed and Aisha Khanum and Seema Jehan},
  doi          = {10.7717/peerj-cs.2821},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2821},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparative evaluation of approaches & tools for effective security testing of web applications},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv8-POS: A lightweight model for coal-rock image recognition. <em>PEERJCS</em>, <em>11</em>, e2820. (<a href='https://doi.org/10.7717/peerj-cs.2820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach, designated YOLOv8-POS, is introduced to address the issue of false detections in coal-rock image recognition tasks, frequently caused by factors such as image defocus, dim lighting, and worker occlusion, and to further enhance the model’s accuracy and reduce its complexity. The methodology introduces a C2f-PConv module, which ingeniously combines the strengths of C2f and partial convolution (PConv) to selectively process channels. This reduces unnecessary computational overhead while preserving the integrity of critical feature information, thus significantly cutting down on the model’s parameters and computational demands. Additionally, an Overlapping Spatial Reduction Attention module is incorporated into the model’s architecture to optimize the fusion of spatial features, substantially improving the handling of complex scenarios. The adoption of a slim-neck design further streamlines the computational and storage requirements, leveraging meticulously engineered lightweight modules to enhance the model’s practical applicability. Empirical results demonstrate that YOLOv8-POS markedly improves performance on coal-rock image datasets, achieving an AP50 of 77.1% and an AP50:95 of 63.6%, while concurrently reducing the model’s parameters to 2.60 M and the floating point operations (FLOPS) to 6.4 G. Comparative evaluations with other prominent algorithms confirm the superior performance of this refined approach, solidifying its advantage in practical deployments.},
  archive      = {J_PEERJCS},
  author       = {Yanqin Zhao and Wenyu Wang},
  doi          = {10.7717/peerj-cs.2820},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2820},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {YOLOv8-POS: A lightweight model for coal-rock image recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REDf: A deep learning model for short-term load forecasting to facilitate renewable integration and attaining the SDGs 7, 9, and 13. <em>PEERJCS</em>, <em>11</em>, e2819. (<a href='https://doi.org/10.7717/peerj-cs.2819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future in line with the United Nations (UN) Sustainable Development Goal (SDG) 7 (Affordable and Clean Energy). However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity, which is crucial for achieving SDG 9 (Industry, Innovation and Infrastructure). In this article, we propose a deep learning model for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. Our approach aligns with SDG 13 (Climate Action) on climate action, enabling more efficient management of renewable energy resources. We use long short-term memory networks, well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four historical short-term energy demand data datasets from different energy distribution companies, including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is compared with three other state-of-the-art forecasting algorithms: Facebook Prophet, support vector regression, and random forest regression. The experimental results show that the proposed REDf model can accurately predict energy demand with a mean absolute error of 1.4%, indicating its potential to enhance the stability and efficiency of the power grid and contribute to achieving SDGs 7, 9, and 13. The proposed model also has the potential to manage the integration of renewable energy sources effectively.},
  archive      = {J_PEERJCS},
  author       = {Md Saef Ullah Miah and Junaida Sulaiman and Md Imamul Islam and Md Masuduzzaman and Molla Shahadat Hossain Lipu and Ramdhan Nugraha},
  doi          = {10.7717/peerj-cs.2819},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2819},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {REDf: A deep learning model for short-term load forecasting to facilitate renewable integration and attaining the SDGs 7, 9, and 13},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting sarcasm in user-generated content integrating transformers and gated graph neural networks. <em>PEERJCS</em>, <em>11</em>, e2817. (<a href='https://doi.org/10.7717/peerj-cs.2817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of the Internet and social media has posed significant challenges to automated sentiment analysis, particularly in relation to detecting sarcasm in user-generated content. Sarcasm often expresses negative emotions through seemingly positive or exaggerated language, making its detection a complex task in natural language processing. To address this issue, the present study proposes a novel sarcasm detection model that combines bidirectional encoder representations from transformers (BERT) with gated graph neural networks (GGNN), further enhanced by a self-attention mechanism to more effectively capture ironic cues. BERT is utilized to extract deep contextual information from the text, while GGNN is employed to learn global semantic structures by incorporating dependency and emotion graphs. Experiments were conducted on two benchmark sarcasm detection datasets, namely Headlines and Riloff. The experimental results demonstrate that the proposed BERT-GGNN model achieves an accuracy of 92.00% and an F1 score of 91.51% on the Headlines dataset, as well as an accuracy of 86.49% and an F1 score of 86.59% on the Riloff dataset, significantly outperforming the conventional BERT-GCN models. The results of ablation studies further corroborate the efficacy of integrating GGNN, particularly for handling complex ironic expressions frequently encountered in social media contexts.},
  archive      = {J_PEERJCS},
  author       = {Zhenkai Qin and Qining Luo and Zhidong Zang and Hongpeng Fu},
  doi          = {10.7717/peerj-cs.2817},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2817},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Detecting sarcasm in user-generated content integrating transformers and gated graph neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IoT in urban development: Insight into smart city applications, case studies, challenges, and future prospects. <em>PEERJCS</em>, <em>11</em>, e2816. (<a href='https://doi.org/10.7717/peerj-cs.2816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the integration of Internet of Things (IoT) technology, smart cities possess the capability to advance their public transportation modalities, address prevalent traffic congestion challenges, refine infrastructure, and optimize communication frameworks, thereby augmenting their progression towards heightened urbanization. Through the integration of sensors, cell phones, artificial intelligence (AI), data analytics, and cloud computing, smart cities worldwide are evolving to be more efficient, productive, and responsive to their residents’ needs. While the promise of smart cities has been marked over the past decade, notable challenges, especially in the realm of security, threaten their optimal realization. This research provides a comprehensive survey on IoT in smart cities. It focuses on the IoT-based smart city components. Moreover, it provides explanation for integrating different technologies with IoT for smart cities such as AI, sensing technologies, and networking technologies. Additionally, this study provides several case studies for smart cities. In addition, this study investigates the challenges of adopting IoT in smart cities and provides prevention methods for each challenge. Moreover, this study provides future directions for the upcoming researchers. It serves as a foundational guide for stakeholders and emphasizes the pressing need for a balanced integration of innovation and safety in the smart city landscape.},
  archive      = {J_PEERJCS},
  author       = {Sayeed Salih and Abdelzahir Abdelmaboud and Omayma Husain and Abdelwahed Motwakel and Hashim Elshafie and Mahir Sharif and Mosab Hamdan},
  doi          = {10.7717/peerj-cs.2816},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2816},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IoT in urban development: Insight into smart city applications, case studies, challenges, and future prospects},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction. <em>PEERJCS</em>, <em>11</em>, e2815. (<a href='https://doi.org/10.7717/peerj-cs.2815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios.},
  archive      = {J_PEERJCS},
  author       = {Qi Fei and Guisheng Yin and Zhian Sun},
  doi          = {10.7717/peerj-cs.2815},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2815},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The pivotal role of software defined networks to safeguard against cyber attacks: A comprehensive review. <em>PEERJCS</em>, <em>11</em>, e2814. (<a href='https://doi.org/10.7717/peerj-cs.2814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defined networks (SDNs) offer novel approaches to managing networks by separating the control plane from the data plane to enable programmable control over network resources effectively and dynamically. This framework supports monitoring of traffic flow and detection of threats while also enabling easy adaptation of network configurations, which is critical in safeguarding against cyber threats. However, this separation also brings forth security risks that cyber attackers may exploit. In this examination, the basic concepts of SDN are explained, pointing out their benefits compared to conventional networks and exploring the security issues that are part of SDN architectures. Different types of threats that focus on SDN layers are categorized and how they impact network security while suggesting different ways to address them. Furthermore, the review highlights issues and suggests potential research paths to enhance SDN security measures and ensure their effectiveness against ever-changing cyber dangers.},
  archive      = {J_PEERJCS},
  author       = {Ahmed Aljughaiman and Seetah Almarri},
  doi          = {10.7717/peerj-cs.2814},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2814},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The pivotal role of software defined networks to safeguard against cyber attacks: A comprehensive review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLPDBO-BP: An efficient valuation model for data asset value. <em>PEERJCS</em>, <em>11</em>, e2813. (<a href='https://doi.org/10.7717/peerj-cs.2813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data asset value assessment is of strategic significance to the development of data factorization, in order to solve the problems of strong assessment subjectivity and low assessment efficiency and accuracy in traditional assessment methods. This article introduces the SLPDBO-BP data asset assessment model for data asset value assessment. Firstly, the sinusoidal chaos mapping strategy, the Levy flight strategy and the fusion of adaptive weight variation operators are integrated to increase the population diversity of the algorithm, broaden the search range, and augment the global optimization capability of the algorithm. Secondly, in an attempt to comprehensively evaluate the optimization performance of SLPDBO, a series of numerical optimization experiments are carried out with 20 test functions and with popular optimization algorithms and dung beetle optimizer (DBO) algorithms with different improvement strategies. Finally, in order to verify the effectiveness of the proposed algorithm in data asset value assessment, the SLPDBO algorithm is combined with backpropagation (BP) to establish the SLPDBO-BP model for data asset value assessment, and the acquired data sets are used in the proposed model for data asset value assessment. The experimental results show that the SLPDBO-BP model performs well in assessment accuracy, and its assessment indexes mean absolute error (MAE), root mean square error (RMSE) and mean absolute percentage error (MAPE) are reduced by 35.1%, 37.6% and 38.7%, respectively, compared with the dung beetle optimizer backpropagation (DBO-BP) model, and its evaluation efficiency is improved, and the proposed model demonstrates better evaluation simulation effects by remarkably outperforming other models in terms of evaluation accuracy and error level.},
  archive      = {J_PEERJCS},
  author       = {Cuiping Zhou and Shaobo Li and Cankun Xie and Panliang Yuan and Zihao Liao},
  doi          = {10.7717/peerj-cs.2813},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2813},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SLPDBO-BP: An efficient valuation model for data asset value},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization model for enterprise financial management utilizing genetic algorithms and fuzzy logic. <em>PEERJCS</em>, <em>11</em>, e2812. (<a href='https://doi.org/10.7717/peerj-cs.2812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the complexities of enterprise financial management by optimizing financial models with a particular focus on enhancing risk prediction performance. A multi-objective mathematical model is first developed to establish key optimization goals, including cost reduction, improved capital utilization, and increased economic benefits. This model systematically defines decision variables and optimization objectives, providing a comprehensive framework for enterprise financial management. To improve predictive accuracy, the study integrates genetic algorithms with back-propagation (BP) neural networks, leveraging genetic algorithms to optimize the neural network’s parameters and structure. Additionally, a hierarchical reinforcement learning model based on fuzzy reasoning (HRL-FR) is proposed to enhance decision-making capabilities. This model employs hierarchical decision-making and policy optimization, incorporating fuzzy reasoning to address uncertainties in complex and dynamic financial environments. Experimental validation using the Compustat dataset confirms the effectiveness of the proposed model. Key financial variables, including the working capital asset ratio and debt-to-equity ratio, are identified as significant influencers of prediction accuracy, reinforcing the model’s robustness. The genetic algorithm’s search and optimization process identifies parameter combinations that maximize neural network performance, further improving predictive capabilities. Comprehensive evaluations conducted on the Center for Research in Security Prices (CRSP) and Compustat datasets for 2022 confirm the HRL-FR model’s superior ability to predict and analyze enterprise financial management information accurately. The model demonstrates higher profitability, enhanced efficiency, and predictive curves that closely align with optimal financial models. These findings highlight the HRL-FR model’s potential as a powerful tool for enterprise financial management optimization, offering valuable insights for risk mitigation and strategic decision-making.},
  archive      = {J_PEERJCS},
  author       = {Sujuan Wang and Musadaq Mansoor},
  doi          = {10.7717/peerj-cs.2812},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2812},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimization model for enterprise financial management utilizing genetic algorithms and fuzzy logic},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep ensemble learning for gastrointestinal diagnosis using endoscopic image classification. <em>PEERJCS</em>, <em>11</em>, e2809. (<a href='https://doi.org/10.7717/peerj-cs.2809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is a valuable tool for the effective assistance of gastroenterologists in the powerful diagnosis of medical images with fast convergence. It also intends to minimize the time and estimated effort required for improved gastrointestinal tract (GIT) diagnosis. GIT abnormalities are widely known to be fatal disorders leading to significant mortalities. It includes both upper and lower GIT disorders. The challenges of addressing GIT issues are complex and need significant study. Multiple challenges exist regarding computer-aided diagnosis (CAD) and endoscopy including a lack of annotated images, dark backgrounds, less contrast, noisy backgrounds, and irregular patterns. Deep learning and transfer learning have assisted gastroenterologists in effective diagnosis in various ways. The goal of proposed framework is the effective classification of endoscopic GIT images with enhanced accuracy. The proposed research aims to formulate a transfer learning-based deep ensemble model, accurately classifying GIT disorders for therapeutic purposes. The proposed model is based on weighted voting ensemble of the two state-of-the-art (STA) base models, NasNet-Mobile and EfficientNet. The extraction of regions of interest, specifically the sick portions, have been performed using images captured from endoscopic procedure. Performance evaluation of the proposed model is performed with cross-dataset evaluation. The datasets utilized include the training dataset HyperKvasir and two test datasets, Kvasir v1 and Kvasir v2. However, the dataset alone cannot create a robust model due to the unequal distribution of images across categories, making transfer learning a promising approach for model development. The evaluation of the proposed framework has been conducted by cross-dataset evaluation utilizing accuracy, precision, recall, Area under curve (AUC) score and F1 score performance metrics. The proposed work outperforms much of the existing transfer learning-based models giving 97.83% on Kvasir v1 and 98.45% accuracy on Kvasir v2.},
  archive      = {J_PEERJCS},
  author       = {Samra Siddiqui and Junaid Ali Khan and Shabbab Algamdi},
  doi          = {10.7717/peerj-cs.2809},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2809},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep ensemble learning for gastrointestinal diagnosis using endoscopic image classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JGURD: Joint gradient update relational direction-enhanced method for knowledge graph completion. <em>PEERJCS</em>, <em>11</em>, e2808. (<a href='https://doi.org/10.7717/peerj-cs.2808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relational direction plays an important role in multi-relational knowledge graphs (KGs). Current knowledge graph completion (KGC) methods suffer from insufficient utilization of relation correlation information. To address this issue, this article proposes a novel KGC framework, namely JGURD, which uses the encoder-decoder structure to achieve Joint Gradient Update with Relational Direction information. It combines graph convolutional networks (GCNs) with KG embedding methods, defining a update mechanism for entities and relationships to joint gradient updates. To incorporate entity information into the update of relationships, the forward propagation gradients of the triple score function are recorded, and entity gradient information is fused into relationship updates. To fully utilize relational direction information, a relation correlation graph (RCG) is constructed based on the topological patterns of relationship pairs. We design a multi-relation encoder combining GCN and multi-layer attention mechanism on RCG to comprehensively capture local and global structures of the RCG. To enhance the interpretability and adaptability of JGURD, three different decoders are employed. Experimental results show that JGURD outperforms the second-place HHAN-KGC, and the Hits@3 and MRR metrics on the FB15k dataset increased by 6.8% and 8.9%, respectively.},
  archive      = {J_PEERJCS},
  author       = {Lianhong Ding and Mengxiao Li and Shengchang Gao and Juntao Li and Ruiping Yuan and Jianye Yu},
  doi          = {10.7717/peerj-cs.2808},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2808},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {JGURD: Joint gradient update relational direction-enhanced method for knowledge graph completion},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing a novel technique for evaluation of tourism informatization in scenic spots from a big data perspective. <em>PEERJCS</em>, <em>11</em>, e2807. (<a href='https://doi.org/10.7717/peerj-cs.2807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, tourism has become a significant driver of many countries’ economies. To maximize revenue from tourism, it is crucial to prioritize the effective management of scenic spots and tourist attractions, and also raise awareness about these places. Social media platforms have played a pivotal role in promoting tourism, as users frequently share videos and reviews related to tourism. Analyzing and managing these reviews is essential for understanding tourists’ opinions about specific destinations. In this study, we evaluated a scenic spot by analyzing tourists’ sentiments. Data was collected from popular social media sites such as TripAdvisor and Twitter using web scraping and the Twitter API. The raw data was preprocessed to remove irrelevant information and redundancies and was properly annotated for further processing. We applied two approaches to analyze the sentiments of tourists. First, we vectorized the text representing the sentiment using the term frequency-inverse document frequency (TF-IDF) and utilized big data analytics to extract meaningful insights. Secondly, we employed a pre-trained large language model, bidirectional encoder representations from transformers (BERT), with a linear classifier to classify tourists’ sentiments. The results of the big data analytics approaches were compared with those of BERT and previously proposed methods. BERT outperformed other machine learning models, achieving an average accuracy of 83.5% on the test set. These insights are valuable for evaluating the informatization of tourist spots, destination management, hospitality, and overall tourist attractions.},
  archive      = {J_PEERJCS},
  author       = {Li Fu and Yao Yi and Lina Liu and Ran Chen},
  doi          = {10.7717/peerj-cs.2807},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2807},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Designing a novel technique for evaluation of tourism informatization in scenic spots from a big data perspective},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demystifying diagnosis: An efficient deep learning technique with explainable AI to improve breast cancer detection. <em>PEERJCS</em>, <em>11</em>, e2806. (<a href='https://doi.org/10.7717/peerj-cs.2806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As per a WHO survey conducted in 2023, more than 2.3 million breast cancer (BC) cases are reported every year. In nearly 95% of countries, the second leading cause of death for females is BC. Breast and cervical cancers cause 80% of reported deaths in middle-income countries. Early detection of breast cancer can help patients better manage their condition and increase their chances of survival. However, traditional AI models frequently conceal their decision-making processes and are mainly tailored for classification tasks. Our approach combines composite deep learning techniques with explainable artificial intelligence (XAI) to enhance interpretability and predictive accuracy. By utilizing XAI to examine features and provide insights into its classifications, the model clarifies the rationale behind its decisions, resulting in an understanding of concealed patterns linked to breast cancer detection. The XAI strengthens practitioners’ and health researchers’ confidence and understanding of artificial intelligence (AI)-based models. In this work, we introduce a hybrid deep learning bi-directional long short-term memory-convolutional neural network (BiLSTM-CNN) model to identify breast cancer using patient data effectively. We first balanced the dataset before using the BiLSTM-CNN model. The hybrid deep learning (DL) model presented here performed well in comparison to other studies, with 0.993 accuracy, precision 0.99, recall 0.99, and F1-score 0.99.},
  archive      = {J_PEERJCS},
  author       = {Ahmed Alzahrani and Muhammad Ali Raza and Muhammad Zubair Asghar},
  doi          = {10.7717/peerj-cs.2806},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2806},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Demystifying diagnosis: An efficient deep learning technique with explainable AI to improve breast cancer detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improvement strategies for heuristic algorithms based on machine learning and information concepts: A review of the seahorse optimization algorithm. <em>PEERJCS</em>, <em>11</em>, e2805. (<a href='https://doi.org/10.7717/peerj-cs.2805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the mechanical limitations of traditional inertia weight optimization methods, this study draws inspiration from machine learning models and proposes an inertia weight optimization strategy based on the K-nearest neighbors (KNN) principle with dynamic adjustment properties. Unlike conventional approaches that determine inertia weight solely based on the number of iterations, the proposed strategy allows inertia weight to more accurately reflect the relative distance between individuals and the target value. Consequently, it transforms the discrete “iteration-weight” mapping ( $t\rightarrow w$t→w ) into a continuous “distance-weight” mapping ( $d\rightarrow w$d→w ), thereby enhancing the adaptability and optimization capability of the algorithm. Furthermore, inspired by the entropy weight method, this study introduces an entropy-based weight allocation mechanism in the crossover and mutation process to improve the efficiency of high-quality information inheritance. To validate its effectiveness, the proposed strategy is incorporated into the Seahorse Optimization Algorithm (SHO) and systematically evaluated using 31 benchmark functions from CEC2005 and CEC2021 test suites. Experimental results demonstrate that the improved SHO algorithm, integrating the logistic-KNN inertia weight optimization strategy and the entropy-based crossover-mutation mechanism, exhibits significant advantages in terms of convergence speed, solution accuracy, and algorithm stability. To further investigate the performance of the proposed improvements, this study conducts ablation experiments to analyze each modification separately. The results confirm that each individual strategy significantly enhances the overall performance of the SHO algorithm.},
  archive      = {J_PEERJCS},
  author       = {Shixing Zheng},
  doi          = {10.7717/peerj-cs.2805},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2805},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improvement strategies for heuristic algorithms based on machine learning and information concepts: A review of the seahorse optimization algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased machine learning-assisted approach for conditional discretization of human performances. <em>PEERJCS</em>, <em>11</em>, e2804. (<a href='https://doi.org/10.7717/peerj-cs.2804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance discretization maps numerical performance values to ordinal categories or performance ranking labels. Norm-referenced performance discretization is extensively applied in human performance evaluation such as grading academic achievements and determining salary increases for employees. These tasks stipulate a common condition that certain performance ranking labels might have no associated performance values and are referred to as conditional discretization. Currently, the only statistical method available for norm-referenced performance discretization is Z score, which merely addresses partial conditions. To achieve a fully conditionally norm-referenced performance discretization, this article proposes four novel approaches enlisting a multi-modal technique that incorporates unsupervised machine-learning algorithms and a heuristic method as well as a novel decision function ensuring conditional unbiasedness. The machine-learning-based methods demonstrate superiority over the heuristic one across most testing data sets, achieving a conditional unbiasedness degree ranging from 0.11 to 0.82. On the other hand, the heuristic method notably outperforms for a specific data set, exhibiting a conditional unbiasedness degree up to 0.76. Leveraging the strengths of these constituent methods enable the effectiveness of the proposed multi-modal approach for conditionally norm-referenced performance discretization.},
  archive      = {J_PEERJCS},
  author       = {Thepparit Banditwattanawong and Masawee Masdisornchote},
  doi          = {10.7717/peerj-cs.2804},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2804},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Unbiased machine learning-assisted approach for conditional discretization of human performances},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFTA-net: A self-supervised approach to detect copy-move and splicing forgery to leverage triplet loss, auxiliary loss, and spatial attention. <em>PEERJCS</em>, <em>11</em>, e2803. (<a href='https://doi.org/10.7717/peerj-cs.2803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image forgery is an increasing threat, fueling misinformation and potentially impacting legal decisions and everyday life. Detecting forged media, including images and videos, is crucial for preserving trust and integrity across various platforms. Common forgery techniques like copy-move and splicing require robust detection methods to identify tampered areas without explicit guidance. The previously proposed studies focused on a single type of forgery detection utilizing block-based and key-point feature selection-based classical machine learning (ML) approaches. Furthermore, applied deep learning (DL) methods only focus on deep feature extraction without considering the focus on tampered regions detection or any domain-specific loss. Therefore, this study addresses the aforementioned challenges by proposing a lightweight DL approach, a self-supervised, triplet and auxiliary losses-based forgery detection network (SFTA-Net), featuring a self-guidance mechanism for detecting tampered regions with a commutative loss within images. The SFTA-Net method is proposed to classify forged and original photos belonging to copy-move and splicing forgeries. To effectively analyze the added components in the proposed model, three experiments were conducted, one with a self-guided (SG) head-based convolutional neural network (CNN), a second with SG-head and auxiliary loss, and a third one with SG-head auxiliary loss and triplet losses-based CNN. For experimentation, CASIA 1.0 and CASIA 2.0 datasets were used with 80-10-10% train-validation and test ratios. The testing results achieved on CASIA 1.0 were 95% accuracy and 97% accuracy on the CASIA 2.0 dataset. To prove the approach’s robustness and generalization, the CASIA 2.0-trained weights were used to test on the MICC-FC2000 dataset and yielded limited results. To improve the results, fine-tuning was performed on CASIA 2.0 weights utilizing the MICC-FC2000 dataset which achieved 98% accurate results. Our findings demonstrate that the SFTA-Net surpasses the baseline ResNet18 model and previous state-of-the-art (SOTA) methods. Overall, our SG approach offers a promising solution for detecting forged images across diverse real-world scenarios, contributing to the mitigation of image forgery and preservation of trust in digital content.},
  archive      = {J_PEERJCS},
  author       = {Amerah Alabrah},
  doi          = {10.7717/peerj-cs.2803},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2803},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SFTA-net: A self-supervised approach to detect copy-move and splicing forgery to leverage triplet loss, auxiliary loss, and spatial attention},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for explaining individual predictions in neural networks. <em>PEERJCS</em>, <em>11</em>, e2802. (<a href='https://doi.org/10.7717/peerj-cs.2802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Recently, the explainability of the prediction results of machine learning models has attracted attention. Most high-performance prediction models are black boxes that cannot be explained. Artificial neural networks are also considered black box models. Although they can explain image classification results to some extent, they still struggle to explain the classification and regression results for tabular data. In this study, we explain the individual prediction results derived from a neural network-based prediction model. Methods The output of a neural network is fundamentally determined by multiplying the input values by the network weights. In other words, the output is a weighted sum of the input values. The weights control how much each input value contributes to the output. The degree of influence of an input value xi on the output can be evaluated as (xi · weight value wi)/weighted sum. From this insight, we can calculate the contribution of each input value to the output as it flows through the neural network. Results With the proposed method, the neural network is no longer a black box. The proposed method effectively explains the predictions made by the neural network and is independent of the depth of the hidden layers and the number of nodes in each hidden layer. This provides a clear rationale for this interpretation. It can be applied to both regression and classification models. The proposed method is implemented as a Python library, making it easy to use.},
  archive      = {J_PEERJCS},
  author       = {Sejong Oh},
  doi          = {10.7717/peerj-cs.2802},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2802},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A method for explaining individual predictions in neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal hate speech detection: A novel deep learning framework for multilingual text and images. <em>PEERJCS</em>, <em>11</em>, e2801. (<a href='https://doi.org/10.7717/peerj-cs.2801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of social media platforms has facilitated the expression of opinions but also enabled the spread of hate speech. Detecting multimodal hate speech in low-resource multilingual contexts poses significant challenges. This study presents a deep learning framework that integrates bidirectional long short-term memory (BiLSTM) and EfficientNetB1 to classify hate speech in Urdu-English tweets, leveraging both text and image modalities. We introduce multimodal multilingual hate speech (MMHS11K), a manually annotated dataset comprising 11,000 multimodal tweets. Using an early fusion strategy, text and image features were combined for classification. Experimental results demonstrate that the BiLSTM+EfficientNetB1 model outperforms unimodal and baseline multimodal approaches, achieving an F1-score of 81.2% for Urdu tweets and 75.5% for English tweets. This research addresses critical gaps in multilingual and multimodal hate speech detection, offering a foundation for future advancements.},
  archive      = {J_PEERJCS},
  author       = {Furqan Khan Saddozai and Sahar K. Badri and Daniyal Alghazzawi and Asad Khattak and Muhammad Zubair Asghar},
  doi          = {10.7717/peerj-cs.2801},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2801},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multimodal hate speech detection: A novel deep learning framework for multilingual text and images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-tuned deep transfer learning: An effective strategy for the accurate chronic kidney disease classification. <em>PEERJCS</em>, <em>11</em>, e2800. (<a href='https://doi.org/10.7717/peerj-cs.2800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kidney diseases are becoming an alarming concern around the globe. Premature diagnosis of kidney disease can save precious human lives by taking preventive measures. Deep learning demonstrates a substantial performance in various medical disciplines. Numerous deep learning approaches are suggested in the literature for accurate chronic kidney disease classification by compromising on architectural complexity, classification speed, and resource constraints. In this study, deep transfer learning is exploited by incorporating unexplored yet effective variants of ConvNeXt and EfficientNetV2 for accurate and efficient classification of chronic kidney diseases. The benchmark computed tomography (CT)-based kidney database containing 12,446 CT scans of kidney tumor, stone cysts, and normal patients is utilized to train the designed fine-tuned networks. However, due to the highly imbalanced distribution of images among classes, the operation of data trimming is exploited for balancing the number of CT scans in each class, which is essential for designing an unbiased predictive network. By utilizing fine-tuned pre-trained models for our specific task, the training time is reduced leading to a computationally inexpensive solution. After the comprehensive hyperparameters tuning with respect to changes in learning rates, batch sizes, and optimizers, it is depicted that the designed fine-tuned EfficientNetV2B0 network of 23.8 MB in size with only 6.2 million architectural parameters shows substantial diagnostic performance by achieving a generalized test accuracy of 99.75% on balanced CT kidney database. Furthermore, the designed fine-tuned EfficientNetV2B0 attains high precision, recall, and F1-score of 99.75%, 99.63%, and 99.75%, respectively. Moreover, the final fine-tuned EfficientNetV2B0 ensures its scalability by achieving an impressive diagnostic accuracy of 99.73% on the test set of the original CT kidney dataset as well. Through the extensive evaluation of the proposed transfer learning strategy, it is concluded that the proposed design of fine-tuned EfficientNetV2B0 outperforms its counterparts in terms of accuracy and computational efficiency for chronic kidney disease diagnosis tasks. The final fine-tuned EfficientNetV2B0 serves as an accurate, efficient, and computationally inexpensive solution tailored for real-time deployment on medical or mobile edge devices.},
  archive      = {J_PEERJCS},
  author       = {Zeshan Aslam Khan and Muhammad Waqar and Hashir Ullah Khan and Naveed Ishtiaq Chaudhary and Abeer TMA Khan and Iqra Ishtiaq and Farrukh Aslam Khan and Muhammad Asif Zahoor Raja},
  doi          = {10.7717/peerj-cs.2800},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2800},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fine-tuned deep transfer learning: An effective strategy for the accurate chronic kidney disease classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage object detection in low-light environments using deep learning image enhancement. <em>PEERJCS</em>, <em>11</em>, e2799. (<a href='https://doi.org/10.7717/peerj-cs.2799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a two-stage object detection system specifically tailored for low-light conditions. In the initial stage, supervised deep learning image enhancement techniques are utilized to improve image quality and enhance features. The second stage employs a computer vision algorithm for object detection. Three image enhancement algorithms—ZeroDCE++, Gladnet, and two-branch exposure-fusion network for low-light image enhancement (TBEFN)—were assessed in the first stage to enhance image quality. YOLOv7 was utilized in the object detection phase. The ExDark dataset, recognized for its extensive collection of low-light images, served as the basis for training and evaluation. No-reference image quality evaluators were applied to measure improvements in image quality, while object detection performance was assessed using metrics such as recall and mean average precision (mAP). The results indicated that the two-stage system incorporating TBEFN significantly improved detection performance, achieving a mAP of 0.574, compared to 0.49 for YOLOv7 without the enhancement stage. Furthermore, this study investigated the relationship between object detection performance and image quality evaluation metrics, revealing that the image quality evaluator NIQE exhibited a strong correlation with mAP for object detection. This correlation aids in identifying the features that influence computer vision performance, thereby facilitating its enhancement.},
  archive      = {J_PEERJCS},
  author       = {Ghaith Al-refai and Hisham Elmoaqet and Abdullah Al-Refai and Ahmad Alzu’bi and Tawfik Al-Hadhrami and Abedalrhman Alkhateeb},
  doi          = {10.7717/peerj-cs.2799},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2799},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Two-stage object detection in low-light environments using deep learning image enhancement},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv8n-DDSW: An efficient fish target detection network for dense underwater scenes. <em>PEERJCS</em>, <em>11</em>, e2798. (<a href='https://doi.org/10.7717/peerj-cs.2798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aquaculture is of great significance to economic development. It is assessed by manual periodic sampling traditionally, consumes workforce and material resources, and quickly leads to inadequate supervision, which results in substantial property losses. Fish target detection technology can effectively solve the issue of manual monitoring. However, a majority of current studies are based on ideal underwater environments and are inapplicable to complex underwater aquaculture scenarios. Therefore, the YOLOv8n-DDSW fish target detection algorithm was proposed in this article to resolve the detection difficulties resulting from fish occlusion, deformation and detail loss in complex intensive aquaculture scenarios. (1) The C2f-deformable convolutional network (DCN) module is proposed to take the place of the C2f module in the YOLOv8n backbone to raise the detection accuracy of irregular fish targets. (2) The dual-pooling squeeze-and-excitation (DPSE) attention mechanism is put forward and integrated into the YOLOv8n neck network to reinforce the features of the visible parts of the occluded fish target. (3) Small detection is introduced to make the network more capable of sensing small targets and improving recall. (4) Wise intersection over union (IOU) rather than the original loss function is used for improving the bounding box regression performance of the network. Training and testing are based on the publicly available Kaggle dataset. According to the experimental results, the mAP50, precision (P), recall (R) and mAP50-95 values of the improved algorithm are 3.9%, 3.7%, 6.1%, and 7.7% higher than those of the original YOLOv8n algorithm, respectively. Thus, the algorithm is effective in solving low detection accuracy in intensive aquaculture scenarios and theoretically supports the intelligent and modern development of fisheries.},
  archive      = {J_PEERJCS},
  author       = {Jinwang Yi and Wei Han and Fangfei Lai},
  doi          = {10.7717/peerj-cs.2798},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2798},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {YOLOv8n-DDSW: An efficient fish target detection network for dense underwater scenes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing skin lesion classification: A CNN approach with human baseline comparison. <em>PEERJCS</em>, <em>11</em>, e2795. (<a href='https://doi.org/10.7717/peerj-cs.2795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents an augmented hybrid approach for improving the diagnosis of malignant skin lesions by combining convolutional neural network (CNN) predictions with selective human interventions based on prediction confidence. The algorithm retains high-confidence CNN predictions while replacing low-confidence outputs with expert human assessments to enhance diagnostic accuracy. A CNN model utilizing the EfficientNetB3 backbone is trained on datasets from the ISIC-2019 and ISIC-2020 SIIM-ISIC melanoma classification challenges and evaluated on a 150-image test set. The model’s predictions are compared against assessments from 69 experienced medical professionals. Performance is assessed using receiver operating characteristic (ROC) curves and area under curve (AUC) metrics, alongside an analysis of human resource costs. The baseline CNN achieves an AUC of 0.822, slightly below the performance of human experts. However, the augmented hybrid approach improves the true positive rate to 0.782 and reduces the false positive rate to 0.182, delivering better diagnostic performance with minimal human involvement. This approach offers a scalable, resource-efficient solution to address variability in medical image analysis, effectively harnessing the complementary strengths of expert humans and CNNs.},
  archive      = {J_PEERJCS},
  author       = {Deep Ajabani and Zaffar Ahmed Shaikh and Amr Yousef and Karar Ali and Marwan A. Albahar},
  doi          = {10.7717/peerj-cs.2795},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2795},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing skin lesion classification: A CNN approach with human baseline comparison},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effects of mismatched train and test data cleaning pipelines on regression models: Lessons for practice. <em>PEERJCS</em>, <em>11</em>, e2793. (<a href='https://doi.org/10.7717/peerj-cs.2793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data quality problems are present in all real-world, large-scale datasets. Each of these potential problems can be addressed in multiple ways through data cleaning. However, there is no single best data cleaning approach that always produces a perfect result, meaning that a choice needs to be made about which approach to use. At the same time, machine learning (ML) models are being trained and tested on these cleaned datasets, usually with one single data cleaning pipeline applied. In practice, however, data cleaning pipelines are updated regularly, often without retraining of production models. It is therefore common to apply different test (or production) data than the data on which the models were originally trained. The changes in these new test data and the data cleaning process applied can have potential ramifications for model performance. In this article, we show the impact that altering a data cleaning pipeline between the training and testing steps of an ML workflow can have. Through the fitting and evaluation of over 6,000 models, we find that mismatches between cleaning pipelines on training and test data can have a meaningful impact on regression model performance. Counter-intuitively, such mismatches can improve test set performance and potentially alter model selection choices.},
  archive      = {J_PEERJCS},
  author       = {James Nevin and Michael Lees and Paul Groth},
  doi          = {10.7717/peerj-cs.2793},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2793},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The effects of mismatched train and test data cleaning pipelines on regression models: Lessons for practice},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLASC: A flare-sensitive clustering algorithm. <em>PEERJCS</em>, <em>11</em>, e2792. (<a href='https://doi.org/10.7717/peerj-cs.2792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploratory data analysis workflows often use clustering algorithms to find groups of similar data points. The shape of these clusters can provide meaningful information about the data. For example, a Y-shaped cluster might represent an evolving process with two distinct outcomes. This article presents flare-sensitive clustering (FLASC), an algorithm that detects branches within clusters to identify such shape-based subgroups. FLASC builds upon HDBSCAN*—a state-of-the-art density-based clustering algorithm—and detects branches in a post-processing step using within-cluster connectivity. Two algorithm variants are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* regarding computational cost and provide similar outputs across repeated runs. In addition, we demonstrate the benefit of branch detection on two real-world data sets. Our implementation is included in the hdbscan Python package and available as a standalone package at https://github.com/vda-lab/pyflasc.},
  archive      = {J_PEERJCS},
  author       = {Daniël M. Bot and Jannes Peeters and Jori Liesenborgs and Jan Aerts},
  doi          = {10.7717/peerj-cs.2792},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2792},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {FLASC: A flare-sensitive clustering algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RARE: Right algorithm for the right errand; a multi-model machine learning-based approach for tourism routes and spots recommendation. <em>PEERJCS</em>, <em>11</em>, e2791. (<a href='https://doi.org/10.7717/peerj-cs.2791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the globalization of the economy, tourism has emerged as a significant sector of entertainment and economic growth. Optimizing tourist attractions and routes has become crucial in modern travel planning, driven by the increasing demand for personalized recommendations. However, traditional static route-based algorithms struggle to adapt to the rapid expansion of the tourism industry, necessitating the development of dynamic, machine-learning-driven solutions. This study introduces a novel tourism recommendation system integrating multiple machine learning algorithms to provide personalized tourist spot and route recommendations. The proposed approach models the tourist map as a 2D grid of interconnected nodes, allowing for dynamic and adaptive recommendations. The framework employs long short-term memory (LSTM) for spot relevance prediction, support vector machine (SVM) for spot name classification, and depth first search (DFS) for optimal route generation. A k-means clustering approach is also utilized to designate a cluster leader (CL) responsible for managing node information within a specific zone. By inputting a simple textual query, tourists receive optimized travel routes tailored to their preferences, incorporating relevant attractions. The model is implemented in a Python-based environment and evaluated using an augmented Travel Recommendation dataset from Kaggle. Experimental results demonstrate the model’s effectiveness in enhancing tourism planning and user experience, showcasing its potential for advancing intelligent tourism solutions.},
  archive      = {J_PEERJCS},
  author       = {Ling Luo},
  doi          = {10.7717/peerj-cs.2791},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2791},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RARE: Right algorithm for the right errand; a multi-model machine learning-based approach for tourism routes and spots recommendation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational quantum classifier-based early identification and classification of chronic kidney disease using sparse autoencoder and LASSO shrinkage. <em>PEERJCS</em>, <em>11</em>, e2789. (<a href='https://doi.org/10.7717/peerj-cs.2789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two leading causes of chronic kidney disease (CKD) are excessive blood pressure and diabetes. Researchers worldwide utilize the rate of globular filtration and kidney inflammation biomarkers to identify chronic kidney disease that gradually reduces renal function. The mortality rate for CKD is high, and thus, a person with this illness is more likely to pass away at a younger age. Healthcare professionals must diagnose the various illnesses connected to this deadly disease as promptly as possible to lighten the impact of CKD. A quantum machine learning (QML) based technique is presented in this research to help with the early diagnosis and prognosis of CKD. The proposed research comprises four phases: data pre-processing, data augmentation, feature selection, and classification. In the first phase, Kalman filter and data normalization techniques are applied to handle the missing and noisy data. In the second phase, data augmentation uses sparse autoencoders to balance the data for smaller classes. In the third phase, LASSO shrinkage is used to select the significant features in the dataset. Variational Quantum classifiers, a supervised QML technique, are employed in the classification phase to classify chronic kidney diseases. The proposed system has been evaluated on the UCI dataset, which comprises 400 CKD patients in the early stages with 25 attributes. The suggested system was assessed using F1-score, precision, recall, and accuracy as evaluation metrics. With a 99.2% classification accuracy, it was found that this model performed better than the other traditional classifiers used for chronic kidney disease classification.},
  archive      = {J_PEERJCS},
  author       = {P. Parthasarathi and Haya Mesfer Alshahrani and K. Venkatachalam and Jaehyuk Cho},
  doi          = {10.7717/peerj-cs.2789},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2789},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Variational quantum classifier-based early identification and classification of chronic kidney disease using sparse autoencoder and LASSO shrinkage},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic trajectory index method based on large-scale real-time trajectory data. <em>PEERJCS</em>, <em>11</em>, e2785. (<a href='https://doi.org/10.7717/peerj-cs.2785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing a trajectory index can efficiently improve the performances of trajectory data processing, provide basic supports for trajectory data mining. With the constantly growing of trajectory data scale and increasing demands for trajectory retrieval efficiency and accuracy, the indexing methods have become more and more crucial. The indexing method faces significant challenges in terms of spatiotemporal trajectory locality, imbalanced trajectory distribution and low trajectory data value density. To address these, we proposed an indexing method based on large-scale real-time trajectory data, it extends the vertical storage mode of HBase, designs the core index, and optimizes the design of the row key, refines the data retrieval process and provides specific mappings for each independent part of the dataset. Besides, it designs the primary index, implements a dynamic indexing mechanism, dynamically load relevant index based on query strategies to flexibly meet the complex query requirements. Comparative experiments demonstrate that the proposed index method is superior in range retrievals and trajectory retrievals, the responding speed is faster.},
  archive      = {J_PEERJCS},
  author       = {Huawei Zhai and Licheng Cui and Kemal Polat and Fayadh Alenezi},
  doi          = {10.7717/peerj-cs.2785},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2785},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic trajectory index method based on large-scale real-time trajectory data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early detection and analysis of accurate breast cancer for improved diagnosis using deep supervised learning for enhanced patient outcomes. <em>PEERJCS</em>, <em>11</em>, e2784. (<a href='https://doi.org/10.7717/peerj-cs.2784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of breast cancer (BC) is essential for effective treatment and improved prognosis. This study compares the performance of various machine learning (ML) algorithms, including convolutional neural networks (CNNs), logistic regression (LR), support vector machines (SVMs), and Gaussian naive Bayes (GNB), on two key datasets, Wisconsin Diagnostic Breast Cancer (WDBC) and Breast Cancer Histopathological Image Classification (BreaKHis). For the BreaKHis dataset, the CNN achieved an impressive accuracy of 92%, with precision, recall, and F1 score values of 91%, 93%, and 91%, respectively. In contrast, LR achieved 88% accuracy, with corresponding precision, recall, and F1 score values of 86%, 87%, and 89%, respectively. SVM and GNB demonstrated 90% and 84% accuracy, respectively, with similar precision, recall, and F1-score metric performances. In the WDBC dataset, LR achieved the highest accuracy of 97.5%, with nearly 97% values for precision, recall, and F1 score. In contrast, CNN attained 96% accuracy with equal recall, precision, and F1 score values of 96%. SVM and GNB followed closely with 95% and 94% accuracy, respectively. Minimising the false negative rate (FNR) and false omission rate (FOR) is vital for improving model reliability, with the LR excelling in the WDBC dataset (FNR: 5.9%, FOR: 4.8%) and the CNN performing best in the BreaKHis dataset (FNR: 8.3%, FOR: 7.0%). The results demonstrate that CNN outperforms traditional models across both datasets, highlighting its potential for early and accurate BC detection.},
  archive      = {J_PEERJCS},
  author       = {Mandika Chetry and Ruiling Feng and Samra Babar and Hao Sun and Imran Zafar and Mohamed Mohany and Hassan Imran Afridi and Najeeb Ullah Khan and Ijaz Ali and Muhammad Shafiq and Sabir Khan},
  doi          = {10.7717/peerj-cs.2784},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2784},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Early detection and analysis of accurate breast cancer for improved diagnosis using deep supervised learning for enhanced patient outcomes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSGRec: Dual-path selection graph for multimodal recommendation. <em>PEERJCS</em>, <em>11</em>, e2779. (<a href='https://doi.org/10.7717/peerj-cs.2779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of digital streaming technology, multi-modal recommendation systems have gained significant attention. Current graph-based multi-modal recommendation approaches typically model user interests using either user interaction signals or multi-modal item information derived from heterogeneous graphs. Although methods based on graph convolutional networks (GCNs) have achieved notable success, they still face two key limitations: (1) the narrow interpretation of interaction information, leading to incomplete modeling of user behavior, and (2) a lack of fine-grained collaboration between user behavior and multi-modal information. To address these issues, we propose a novel method by decomposing interaction information into two distinct signal pathways, referred to as a dual-path selection architecture, named Dual-path Selective Graph Recommender (DSGRec). DSGRec is designed to deliver more accurate and personalized recommendations by facilitating the positive collaboration of interactive data and multi-modal information. To further enhance the represetation of these signals, we introduce two key components: (1) behavior-aware multimodal signal augmentation, which extract rich multimodal semantic information; and (b) hypergraph-guided cooperative signal enhancement, which captures hybrid global information. Our model learns dual-path selection signals via a primary module and introduces two auxiliary modules to adjust these signals. We introduce independent contrastive learning tasks for the auxiliary signals, enabling DSGRec to explore the mechanisms behind feature embeddings from different perspectives. This approach ensures that each auxiliary module aligns with the user-item interaction view independently, calibrating its contribution based on historical interactions. Extensive experiments conducted on three benchmark datasets demonstrate the superiority of DSGRec over several state-of-the-art recommendation baselines, highlighting the effectiveness of our method.},
  archive      = {J_PEERJCS},
  author       = {Zihao Liu and Wen Qu},
  doi          = {10.7717/peerj-cs.2779},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2779},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DSGRec: Dual-path selection graph for multimodal recommendation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart waste management and classification system using advanced IoT and AI technologies. <em>PEERJCS</em>, <em>11</em>, e2777. (<a href='https://doi.org/10.7717/peerj-cs.2777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective management of municipal solid waste is a critical global issue, affecting both urban and rural areas. To address the growing volume of solid waste, proactive planning is essential. Traditionally, solid waste is often disposed of without segregation, preventing recycling and the recovery of raw materials. Proper waste segregation is a fundamental requirement for effective solid waste management, allowing materials to be recycled efficiently. Emerging technologies such as artificial intelligence (AI), machine learning (ML), and the Internet of Things (IoT) offer powerful tools for identifying recyclable materials like glass, plastic, and metal within solid waste. The primary goal of this research is to contribute to a cleaner environment, reduce infant mortality, improve maternal health, and support efforts to combat HIV/AIDS, malaria, and other diseases. This study introduces an intelligent and smart solid waste management system (iSSWMs) designed to smartly collect and segregate solid waste. The proposed system focuses on three types of materials: plastic, glass, and metal. The first phase involves waste collection using smart bins connected to a mobile application, which sends notifications when the bins are full. In the second phase, we develop a deep learning-based mechanical model to segregate the waste, using the VGG-19 model, which achieved a performance accuracy of 99.7% during training. To the best of our knowledge, iSSWMs is a promising framework that integrates both waste collection and segregation through the use of cutting-edge technologies, delivering high accuracy and efficiency.},
  archive      = {J_PEERJCS},
  author       = {Abdullah Alourani and M. Usman Ashraf and Mohammed Aloraini},
  doi          = {10.7717/peerj-cs.2777},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2777},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Smart waste management and classification system using advanced IoT and AI technologies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Photovoltaic panel defect detection algorithm based on infrared imaging and improved YOLOv8. <em>PEERJCS</em>, <em>11</em>, e2776. (<a href='https://doi.org/10.7717/peerj-cs.2776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of high missed detection rates, complex backgrounds, unclear defect features, and uneven difficulty levels in target detection during the industrial process of photovoltaic panel defect detection, this article proposes an infrared detection method based on computer vision, with enhancements built upon the YOLOv8 model. First, a multi-channel squeeze-and-excitation network is introduced to improve feature extraction capabilities and is integrated into the neck network. Second, GhostConv and BoTNet are incorporated into the backbone network to reduce model parameters while enhancing defect detection performance. Finally, the Focaler-Complete Intersection over Union (Focaler-CIoU) loss function is employed to tackle the issue of imbalanced difficulty in target detection tasks. The method is evaluated on the PV-Multi-Defect-main dataset and further validated through a generalization test on the PVEL-AD dataset. Results demonstrate that, compared with the baseline YOLOv8 model, the proposed approach achieves significant improvements in precision (3.6%), recall (10.4%), mAP50 (4.8%), and mAP50-95 (4.5%) while maintaining nearly the same parameter count. On the PVEL-AD dataset, the method effectively addresses the challenge of feature extraction failure for dislocation-type defects, achieving substantial gains in precision (7.8%), recall (17.1%), mAP50 (19.5%), and mAP50-95 (13.2%). Furthermore, comparisons with several state-of-the-art detection algorithms reveal that the proposed method consistently delivers improved detection performance, validating its effectiveness as a robust solution for photovoltaic panel defect detection.},
  archive      = {J_PEERJCS},
  author       = {Jingdong Wang and Zhu Cheng},
  doi          = {10.7717/peerj-cs.2776},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2776},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Photovoltaic panel defect detection algorithm based on infrared imaging and improved YOLOv8},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label classification for image tamper detection based on swin-T segmentation network in the spatial domain. <em>PEERJCS</em>, <em>11</em>, e2775. (<a href='https://doi.org/10.7717/peerj-cs.2775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of deep learning methods for detecting image forgery fail to accurately detect and localize the tampering operations. Furthermore, they only support a single image tampering type. Our method introduces three key innovations: (1) A spatial perception module that combines the spatial rich model (SRM) with constrained convolution, enabling focused detection of tampering traces while suppressing interference from image content; (2) A hierarchical feature learning architecture that integrates Swin Transformer with UperNet for effective multi-scale tampering pattern recognition; and (3) A comprehensive optimization strategy including auxiliary supervision, self-supervised learning, and hard example mining, which significantly improves model convergence and detection accuracy. Comprehensive experiments are performed on two established datasets; namely MixTamper and DocTamper with 19,600 and 170,000 images, respectively. The experimental findings demonstrate that the proposed model enhances the IoU index by 13% compared to the leading algorithms. Additionally, it can accurately detect multiple tampering types from a single image.},
  archive      = {J_PEERJCS},
  author       = {Li Li and Kejia Zhang and Jianfeng Lu and Shanqing Zhang},
  doi          = {10.7717/peerj-cs.2775},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2775},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-label classification for image tamper detection based on swin-T segmentation network in the spatial domain},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personality-based pair programming: Toward intrinsic motivation alignment in very small entities. <em>PEERJCS</em>, <em>11</em>, e2774. (<a href='https://doi.org/10.7717/peerj-cs.2774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aim This study explores whether personality‐based role assignments (Pilot, Navigator, Solo) can raise intrinsic motivation in pair programming, focusing on designing a framework and process extension for the resource‐constrained environment of very small entities (VSEs). Method We employed a mixed‐methods design across three quasi-experimental datasets (n = 73 participants), applying linear mixed‐effects (LME) modeling to assess motivational outcomes and thematically analyzing (n = 25) interviews for socio‐psychological insights. Findings Openness strongly correlates with Pilot roles; Extraversion & Agreeableness favor Navigator roles; and Neuroticism aligns more comfortably with Solo roles—each yielding substantial boosts in intrinsic motivation (up to 60–65%). Twelve qualitative themes underscore the influence of mentorship, pairing constellations, and flow disruptions on developer experiences. Implications Building on these results, we propose the role‐optimization motivation alignment (ROMA) framework, mapped to the ISO/IEC 29110 Software Basic Profile and Agile Guidelines, with practical tasks (T1–T7) to facilitate systematic role–trait alignments in small agile teams. Although our data primarily involve Gen‐Z undergraduates, the recurring patterns suggest broader applicability, further supported by a separately published application for ongoing generalizability. Conclusion Personality‐driven role optimization may significantly enhance collaboration and developer satisfaction in VSEs, though further studies in professional settings and investigations into AI‐assisted or distributed pair programming are warranted.},
  archive      = {J_PEERJCS},
  author       = {Marcel Valovy and Alena Buchalcevova},
  doi          = {10.7717/peerj-cs.2774},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2774},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Personality-based pair programming: Toward intrinsic motivation alignment in very small entities},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural networks embedded with domain knowledge for cyber threat intelligence entity and relationship mining. <em>PEERJCS</em>, <em>11</em>, e2769. (<a href='https://doi.org/10.7717/peerj-cs.2769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating frequency and severity of cyber-attacks have presented formidable challenges to the safeguarding of cyberspace. Named Entity Recognition (NER) technology is utilized for the rapid identification of threat entities and their relationships within cyber threat intelligence, enabling security researchers to be promptly informed of the occurrence of cyber threats, thereby enhancing the efficiency of security defense and analysis. However, current models for identifying network threat entities and extracting relationships suffer from limitations such as the inadequate representation of textual semantic information, insufficient granularity in threat entity recognition, and errors in relationship extraction propagation. To address these issues, this article proposes a novel model for Network Threat Entity Recognition and Relationship Extraction (CtiErRe). Additionally, it redefines seven network threat entities and two types of relationships between threat entities. Specifically, first, domain knowledge is collected to build a domain knowledge graph, which is then embedded using graph convolutional networks (GCN) to enhance the feature representation of threat intelligence text. Next, the features from domain knowledge graph embedding and those generated by the bidirectional encoder representations from transformers (BERT) model are fused using the Layernorm algorithm. Finally, the fused features are processed using the GlobalPointer algorithm to generate both the threat entity type matrix and the threat entity relation type matrix, thereby enabling the identification of threat entities and their relationships. To validate our proposed model, we conducted extensive experiments, and the results demonstrate its superiority over existing models. Our model performs remarkably in threat entity recognition tasks, with accuracy and F1 scores reaching 92.13% and 93.11%, respectively. In the relationship extraction task, our model achieves accuracy and F1 scores of 91.45% and 92.45%, respectively.},
  archive      = {J_PEERJCS},
  author       = {Gan Liu and Kai Lu and Saiqi Pi},
  doi          = {10.7717/peerj-cs.2769},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2769},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Graph neural networks embedded with domain knowledge for cyber threat intelligence entity and relationship mining},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing cybersecurity through autonomous knowledge graph construction by integrating heterogeneous data sources. <em>PEERJCS</em>, <em>11</em>, e2768. (<a href='https://doi.org/10.7717/peerj-cs.2768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity plays a critical role in today’s modern human society, and leveraging knowledge graphs can enhance cybersecurity and privacy in the cyberspace. By harnessing the heterogeneous and vast amount of information on potential attacks, organizations can improve their ability to proactively detect and mitigate any threat or damage to their online valuable resources. Integrating critical cyberattack information into a knowledge graph offers a significant boost to cybersecurity, safeguarding cyberspace from malicious activities. This information can be obtained from structured and unstructured data, with a particular focus on extracting valuable insights from unstructured text through natural language processing (NLP). By storing a wide range of cyber threat information in a semantic triples form which machines can interpret autonomously, cybersecurity experts gain improved visibility and are better equipped to identify and address cyber threats. However, constructing an efficient knowledge graph poses challenges. In our research, we construct a cybersecurity knowledge graph (CKG) autonomously using heterogeneous data sources. We further enhance the CKG by applying logical rules and employing graph analytic algorithms. To evaluate the effectiveness of our proposed CKG, we formulate a set of queries as questions to validate the logical rules. Ultimately, the CKG empowers experts to efficiently analyze data and gain comprehensive understanding of cyberattacks, thereby help minimize potential attack vectors.},
  archive      = {J_PEERJCS},
  author       = {Hatoon Alharbi and Ali Hur and Hasan Alkahtani and Hafiz Farooq Ahmad},
  doi          = {10.7717/peerj-cs.2768},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2768},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing cybersecurity through autonomous knowledge graph construction by integrating heterogeneous data sources},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WG-storm: A resource-aware scheduler for distributed stream processing engines. <em>PEERJCS</em>, <em>11</em>, e2767. (<a href='https://doi.org/10.7717/peerj-cs.2767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing engines (SPEs) allow applications to process a large amount of data in real-time. However, to schedule big data applications; the SPEs create several challenges regarding resource utilisation, dynamic configurations, heterogeneous environment, resource awareness, load balancing, etc. As the volume of data increases over time, it also poses a challenge to predict the resource and application requirements for processing. All these factors play an important role, they can cause problems in achieving maximum throughput due to inefficiency in any of them. Most SPEs ignore the topology’s structure, which may minimise throughput during scheduling and may increase network latency. In this article, a topology-aware and resource-aware scheduler (named WG-Storm) is proposed based on a directed acyclic graph (DAG) that enhances resource usage and overall throughput using efficient task assignment. WG-Storm is built on Apache Storm. Results are generated using the two linear topologies and compared with the five state-of-art schedulers including A3-Storm, Default, Isolation, Multi-tenant, and Resource-aware. The experimental results show up to 30% increased throughput using the least required computing resources in a heterogeneous cluster.},
  archive      = {J_PEERJCS},
  author       = {Rizwan Ali and Asif Muhammad and Muhammad Aleem and Omair Shafiq},
  doi          = {10.7717/peerj-cs.2767},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2767},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {WG-storm: A resource-aware scheduler for distributed stream processing engines},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Epileptic seizures diagnosis and prognosis from EEG signals using heterogeneous graph neural network. <em>PEERJCS</em>, <em>11</em>, e2765. (<a href='https://doi.org/10.7717/peerj-cs.2765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy, often associated with neurodegenerative disorders following brain strokes, manifests as abnormal electrical activity bursts in the cerebral cortex, disrupting regular brain function. Electroencephalogram (EEG) recordings capture these distinctive brain signals, offering crucial insights into seizure detection and management. This study presents a novel approach leveraging a graph neural network (GNN) model with a heterogeneous graph representation to detect epileptic seizures from EEG data. Utilizing the well-established CHB-MIT EEG dataset for training and evaluation, the proposed method includes preprocessing steps such as signal segmentation, resampling, label encoding, normalization, and exploratory data analysis. We employed a standard train-test split with stratified sampling to ensure class distribution and reduce bias. Experimental comparisons with long short-term memory (LSTM) and recurrent neural network (RNN) models highlight the GNN’s superior performance, achieving a classification accuracy of 98.0% and demonstrating incremental improvements in precision and F1-score. These findings emphasize the efficacy of GNN in capturing spatial and temporal dependencies within EEG data, surpassing conventional deep learning techniques. Furthermore, the study highlights the model’s interpretability, which is essential for clinical decision-making. By advancing EEG-based seizure prediction methods, this research offers a robust framework for enhancing patient outcomes in epilepsy management while addressing the limitations of existing approaches.},
  archive      = {J_PEERJCS},
  author       = {Areej Alasiry and Gabriel Avelino Sampedro and Ahmad Almadhor and Roben A. Juanatas and Shtwai Alsubai and Vincent Karovic},
  doi          = {10.7717/peerj-cs.2765},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2765},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Epileptic seizures diagnosis and prognosis from EEG signals using heterogeneous graph neural network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting no-shows at outpatient appointments in internal medicine using machine learning models. <em>PEERJCS</em>, <em>11</em>, e2762. (<a href='https://doi.org/10.7717/peerj-cs.2762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high prevalence of patient absenteeism in medical appointments poses significant challenges for healthcare providers and patients, causing delays in service delivery and increasing operational inefficiencies. Addressing this issue is crucial in the internal medicine department, a fundamental pillar of comprehensive adult healthcare that manages various chronic and complex conditions. To mitigate absenteeism, we present an innovative application of machine learning models specifically designed to predict the risk of patient absenteeism in the internal medicine department of Fundación Valle del Lili, a high-complexity hospital in Colombia. Leveraging an institutional database, we conducted a statistical analysis to identify critical variables influencing absenteeism risk, including clinical and sociodemographic factors and characteristics of previously attended appointments. Our study evaluated seven distinct machine learning models, explored various data processing techniques, and addressed class imbalance through oversampling and undersampling strategies. Hyperparameter optimization was conducted for each model configuration, culminating in selecting the Bagging RandomForest model, which demonstrated outstanding performance when combined with standardized data and balanced using the Synthetic Minority Oversampling Technique (SMOTE). Additionally, Shapley values (SHAP) were applied to enhance the interpretability of the model, enabling the identification of the most influential variables in predicting medical absenteeism, such as the number of previous absences, the day and month of the appointment, and diagnosed diseases. The selected model achieved a predictive accuracy of 84.80 ± 0.81%, an AUC value of 0.89, an F1-score of 84.75%, and a recall of 83.02% in cross-validation experiments. These results highlight the potential of our experimental approach to identify the most suitable model for proactively predicting patients at high risk of absenteeism, optimizing resource allocation, and improving the quality of medical care in internal medicine in the future. Our methodology provides a foundation for reducing operational inefficiencies and strengthening intervention strategies. This benefits healthcare providers and patients through more timely and effective care. Ultimately, this approach contributes to improving patient outcomes and institutional efficiency.},
  archive      = {J_PEERJCS},
  author       = {Felipe Ocampo Osorio and Santiago Pedroza Gomez and David Esteban Rebellón Sanchez and Richard Ramirez Fernandez and Reinel Tabares-Soto and Mario Alejandro Bravo-Ortíz and Gustavo Adolfo Cruz Suarez},
  doi          = {10.7717/peerj-cs.2762},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2762},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting no-shows at outpatient appointments in internal medicine using machine learning models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of deep learning models in automatic classification of coffee bean species. <em>PEERJCS</em>, <em>11</em>, e2759. (<a href='https://doi.org/10.7717/peerj-cs.2759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most widely consumed beverages worldwide, coffee is characterized by its diverse flavor profiles and complex production processes. In this study, deep learning-based image processing techniques are employed for the automatic classification of coffee bean species with high accuracy. To achieve this, images of three different coffee bean species (Starbucks Pike Place, Espresso, and Kenya) were classified using five CNN-based models: Xception, DenseNet201, InceptionV3, InceptionResNetV2, and DenseNet121. The dataset comprises 1,554 coffee bean images. Cross-validation was applied to assess the models’ performance, and classification accuracy was evaluated using performance metrics. Among the tested models, InceptionV3 achieved the highest classification accuracy (93%) and precision (95%), with the lowest loss rate (0.12), making it the most effective model in this study. As a result of the experiments, the average classification success rates of the models were determined as follows: 93% for InceptionV3, 92% for DenseNet121, 91% for Xception, 91% for InceptionResNetV2, and 90% for DenseNet201. These findings indicate that InceptionV3 demonstrates the highest performance. It is anticipated that this study will make significant contributions to applications in coffee bean classification.},
  archive      = {J_PEERJCS},
  author       = {Adem Korkmaz and Tarık Talan and Selahattin Koşunalp and Teodor Iliev},
  doi          = {10.7717/peerj-cs.2759},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2759},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparison of deep learning models in automatic classification of coffee bean species},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TMAC: A transformer-based partially observable multi-agent communication method. <em>PEERJCS</em>, <em>11</em>, e2758. (<a href='https://doi.org/10.7717/peerj-cs.2758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective communication plays a crucial role in coordinating the actions of multiple agents. Within the realm of multi-agent reinforcement learning, agents have the ability to share information with one another through communication channels, leading to enhanced learning outcomes and successful goal attainment. Agents are limited by their observations and communication ranges due to increasingly complex location arrangements, making multi-agent collaboration based on communication increasingly difficult. In this article, for multi-agent communication in some partially observable scenarios, we propose a Transformer-based Partially Observable Multi-Agent Communication algorithm (TMAC), which improves agents extracting features and generating output messages. Meanwhile, a self-message fusing module is proposed to obtain features from multiple sources. Therefore, agents can achieve better collaboration through communication. At the same time, we performed experimental verification in the surviving and the StarCraft Multi-Agent Challenge (SMAC) environments where agents had limited local observation and could only communicate with neighboring agents. In two test environments, our method achieves an improvement in performance 6% and 10% over the baseline algorithm, respectively. Our code is available at https://gitee.com/xs-lion/tmac.},
  archive      = {J_PEERJCS},
  author       = {Xuesi Li and Shuai Xue and Ziming He and Haobin Shi},
  doi          = {10.7717/peerj-cs.2758},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2758},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TMAC: A transformer-based partially observable multi-agent communication method},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study of trust mining algorithms for beacon nodes in large-scale network environments. <em>PEERJCS</em>, <em>11</em>, e2755. (<a href='https://doi.org/10.7717/peerj-cs.2755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a large-scale network environment, node positioning is prone to large deviations. Mining beacon node trust is the basis for precise node positioning in the network environment. Therefore, this article studies the trust degree mining algorithm of beacon nodes in a large-scale network environment. First, according to the distance error evaluation and probability function of beacon nodes in the large-scale network environment, the direct trust degree of beacon nodes is obtained. The trust degree is converted into influence, and the influence of beacon nodes is mined using the seepage theory to determine the beacon node with the highest impact in the large-scale network environment. Then, according to the influence of nodes, received signal strength indicator (RSSI) is used to optimize the conventional distance vector hop (DV-Hop) node location algorithm. The influence weights the average hop distance of beacon nodes. The weight of the influence of beacon nodes defines the average hop distance of unknown nodes. The average hop distance information of unknown nodes is taken from more high-influence beacon nodes, solving the problem of significant positioning errors caused by the uncertainty of location targets. Finally, the security status of nodes is reflected according to the degree of trust of different nodes to beacon nodes. The experimental results show that the algorithm can accurately locate other nodes in a wide network environment when the number of beacon nodes and communication distance change, and the trust degree of nodes mined can accurately reflect the security status of nodes.},
  archive      = {J_PEERJCS},
  author       = {Yanyan Jiang},
  doi          = {10.7717/peerj-cs.2755},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2755},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A study of trust mining algorithms for beacon nodes in large-scale network environments},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid integration framework based on LOOCV and SARIMA: Relationship exploring and predictive analysis between discipline attention and literature research. <em>PEERJCS</em>, <em>11</em>, e2754. (<a href='https://doi.org/10.7717/peerj-cs.2754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the relationship between the discipline of network attention and literature research can provide new insights for the innovative development of future disciplines. Many current studies focus on network attention, but its innovative application in the field of subject teaching has not been fully verified. Based on this, this paper proposed a relationship analysis and predictive analysis (RAPA) framework based on leave-one-out cross-validation (LOOCV) and Seasonal Auto-Regressive Integrated Moving Average (SARIMA) to explore the relationship between subject attention and literature research from the perspective of junior high school information technology. Based on the RAPA framework, five key keywords of this subject were extracted by combining the Baidu Index and China National Knowledge Infrastructure (CNKI) in first. Secondly, LOOCV was used to explore the relationship between subject attention represented by keywords and literature researches. Then, SARIMA was used to predict the future trends of subject attention and its literature researches. Finally, the prediction errors of different methods were compared. Based on the RAPA framework, the correlation analysis found that the r-values of subject attention and literature researches were all greater than 0.75, indicating a positive correlation between them. The predictive analysis found that the subject attention of junior high school information technology will be flat or decline in the next 2 years. Meanwhile, the amount of literature in this discipline has decreased compared to previous years, with an average of approximately 136. The prediction comparison showed that the prediction method in this study has a smaller mean absolute error (MAE) than other methods, and the MAE difference is 3.51. This indicated that subject attention, as an auxiliary variable of scientific research literature, is conducive to the quantitative analysis of literature research. At the same time, this study revealed the influence and role of big data represented by Internet attention in educational research.},
  archive      = {J_PEERJCS},
  author       = {Yulin Zhao and Junke Li and Kai Liu and Chaowang Shang},
  doi          = {10.7717/peerj-cs.2754},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2754},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid integration framework based on LOOCV and SARIMA: Relationship exploring and predictive analysis between discipline attention and literature research},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network-based symmetric encryption algorithm with encrypted traffic protocol identification. <em>PEERJCS</em>, <em>11</em>, e2750. (<a href='https://doi.org/10.7717/peerj-cs.2750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptography is a cornerstone of power grid security, with the symmetry and asymmetry of cryptographic algorithms directly influencing the resilience of power systems against cyberattacks. Cryptographic algorithm identification, a critical component of cryptanalysis, is pivotal to assessing algorithm security and hinges on the core characteristics of symmetric and asymmetric encryption methods. A key challenge lies in discerning subtle spatial distribution patterns within ciphertext data to infer the underlying cryptographic algorithms, which is essential for ensuring the communication security of power systems. In this study, we first introduce a plaintext guessing model (SCGM model) based on symmetric encryption algorithms, leveraging the strengths of convolutional neural networks to evaluate the plaintext guessing capabilities of four symmetric encryption algorithms. This model is assessed for its learning efficacy and practical applicability. We investigate protocol identification for encrypted traffic data, proposing a novel scheme that integrates temporal and spatial features. Special emphasis is placed on the performance of algorithms within both symmetric and asymmetric frameworks. Experimental results demonstrate the effectiveness of our proposed scheme, highlighting its potential for enhancing power grid security.},
  archive      = {J_PEERJCS},
  author       = {Jiakai Hao and Ming Jin and Yuting Li and Yuxin Yang},
  doi          = {10.7717/peerj-cs.2750},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2750},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Neural network-based symmetric encryption algorithm with encrypted traffic protocol identification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting malicious code variants using convolutional neural network (CNN) with transfer learning. <em>PEERJCS</em>, <em>11</em>, e2727. (<a href='https://doi.org/10.7717/peerj-cs.2727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware presents a significant threat to computer networks and devices that lack robust defense mechanisms, despite the widespread use of anti-malware solutions. The rapid growth of the Internet has led to an increase in malicious code attacks, making them one of the most critical challenges in network security. Accurate identification and classification of malware variants are crucial for preventing data theft, security breaches, and other cyber risks. However, existing malware detection methods are often inefficient or inaccurate. Prior research has explored converting malicious code into grayscale images, but these approaches are often computationally intensive, especially in binary form. To address these challenges, we propose the Malware Variants Detection System (MVDS), a novel technique that transforms malicious code into color images, enhancing malware detection capabilities compared to traditional methods. Our approach leverages the richer information in color images to achieve higher classification accuracy than grayscale-based methods. We further improve the detection process by employing transfer learning to automatically identify and classify malware images based on their distinctive features. Empirical results demonstrate that MVDS achieves 97.98% accuracy with high detection speed, highlighting its potential for practical implementation in strengthening network security.},
  archive      = {J_PEERJCS},
  author       = {Nazish Younas and Shazia Riaz and Saqib Ali and Rafiullah Khan and Farman Ali and Daehan Kwak},
  doi          = {10.7717/peerj-cs.2727},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2727},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Detecting malicious code variants using convolutional neural network (CNN) with transfer learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An activity theory-based exploration of “Eyeland”, a task-based serious game for EFL visually impaired students. <em>PEERJCS</em>, <em>11</em>, e2631. (<a href='https://doi.org/10.7717/peerj-cs.2631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates how the Eyeland app, an accessible task-based serious game for English as a foreign language (EFL), can remediate traditional lessons for both visually impaired students (VISs) and sighted students (those without visual impairments) in a public high school in Colombia. Using an activity theory framework and its derived model, the Activity Theory-Based Model of Serious Games (ATMSG), the study explores the characteristics of traditional EFL lessons designed for these students, the adjustments made while integrating the Eyeland app, and the resulting changes in student experiences. The research employed action research cycles involving teachers in reflective processes that included planning, observing, acting, and re-planning to adapt and integrate Eyeland into the classroom. Qualitative research methods—field observations, focus groups, usability surveys, teacher interviews, and document analysis—were used to collect data and analyze how Eyeland was implemented and its effects on teaching and learning. Findings indicate that Eyeland effectively remediated traditional lessons by offering accessible, interactive features such as auditory, tactile, and visual support. These enhancements improved engagement for both sighted and visually impaired students. Traditional EFL lessons, which relied heavily on visual materials and teacher-centered methods, were transformed into more interactive, task-based activities that encouraged greater collaboration and student autonomy. Adjustments included redesigning lesson plans and rearranging classroom layouts to foster inclusion. Both sighted and visually impaired students reported positive experiences, particularly valuing the increased autonomy and engagement provided by Eyeland’s interactive tasks. The study highlights significant changes made to the lessons, with visually impaired students reporting predominantly positive experiences, though some teachers were reluctant to engage with the app. Eyeland contributed to the creation of inclusive classrooms by shifting from a teacher-centered approach to a student-centered learning environment that promoted better language learning outcomes. ATMSG was instrumental in analyzing how Eyeland fostered inclusive learning practices and provided valuable insights into re-mediation strategies, pedagogical planning, and the development of accessible content for EFL learners.},
  archive      = {J_PEERJCS},
  author       = {Karen Villalba and Heydy Robles and Miguel Jimeno and Martha Cecilia Delgado-Cañas and Adriana Perez and Francisco Quintero},
  doi          = {10.7717/peerj-cs.2631},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2631},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An activity theory-based exploration of “Eyeland”, a task-based serious game for EFL visually impaired students},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development and evaluation of customized software to automatically align macula and optic disc centered scanning laser ophthalmoscope fundus images. <em>PEERJCS</em>, <em>11</em>, e2621. (<a href='https://doi.org/10.7717/peerj-cs.2621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ophthalmology, the angle between the center of the optic nerve head and the center of sharpest vision (foveola) is a posterior fundus landmark parameter of the retina of the human eye. Together with the optic disc-fovea distance, it characterizes the position of the optic nerve head in relationship to the foveola. The optic disc-fovea angle markedly influences the regional distribution of retinal layer thickness patterns, specifically the retinal nerve fiber layer thickness measured at the optic disc. Thus, the optic disc-fovea angle needs to be determined and routinely taken into account in morphological glaucoma diagnosis and in the assessment of structure-function relationship in optic nerve diseases. However, despite the urgency of this information, currently the optic disc-fovea line and its angle are routinely not measured. Obtaining it post-measurement requires manual registration of the macula and optic disc optical coherence tomography (OCT) imaging data. OCT manufacturer-delivered software does not provide automated image registration. Therefore researchers are forced to manually perform the alignment over different scanning regions. To fill this gap, we provide two software packages which can be applied to routinely acquired clinical OCT data to automatically align macula and optic disc images. In this work, we introduce and comparatively evaluate two separate software packages (BloodVesselReg and OCTFundusReg) to automatically align macula and optic disc centered OCT volume scans based on their respective scanning laser ophthalmoscope (SLO) fundus images. BloodVesselReg implements an image registration and mosaicing algorithm based on retinal blood vessels. OCTFundusReg optimizes a general-purpose image registration toolkit to operate on SLO images. Both methods were independently developed by different subgroups of authors of this study using a training dataset of 18,047 eyes from a population-based study. The methods were tested on a dataset of 3,570 eyes from glaucoma patients, with success/failure assessed by visual inspection and compared to failure reporting of the methods themselves. BloodVesselReg had a slightly higher accuracy (94.7%) than OCTFundusReg (93.9%). Both methods together failed on only 1% of the eyes. BloodVesselReg reported 165 out of its 190 failures. OCTFundusReg provides a continuous failureAlert parameter which resulted in an area under the receiver operating characteristics curve (AUC) of 0.91 from a logistic regression model. When including the difference of fitting related parameters between the two methods, the AUC improved to 0.95. Both methods had success rates of over 90% when applied in isolation to a clinical testing dataset. When applying them together, the rate of at least one of the method succeeding was 99%. The methods are highly promising for applications under real-world clinical conditions and might help to facilitate disease detection and monitoring over time.},
  archive      = {J_PEERJCS},
  author       = {M. Elena Martinez-Perez and Franziska G. Rauscher and Pingping Zhao and Tobias Elze},
  doi          = {10.7717/peerj-cs.2621},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2621},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Development and evaluation of customized software to automatically align macula and optic disc centered scanning laser ophthalmoscope fundus images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing AI-powered translation education tools: A framework for parallel sentence generation using SauLTC and LLMs. <em>PEERJCS</em>, <em>11</em>, e2788. (<a href='https://doi.org/10.7717/peerj-cs.2788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Translation education (TE) demands significant effort from educators due to its labor-intensive nature. Developing computational tools powered by artificial intelligence (AI) can alleviate this burden by automating repetitive tasks, allowing instructors to focus on higher-level pedagogical aspects of translation. This integration of AI has the potential to significantly enhance the efficiency and effectiveness of translation education. The development of effective AI-based tools for TE is hampered by a lack of high-quality, comprehensive datasets tailored to this specific need, especially for Arabic. While the Saudi Learner Translation Corpus (SauLTC), a unidirectional English-to-Arabic parallel corpus, constitutes a valuable resource, its current format is inadequate for generating the parallel sentences required for a didactic translation corpus. This article proposes leveraging large language models like the Generative Pre-trained Transformer (GPT) to transform SauLTC into a parallel sentence corpus. Using cosine similarity and human evaluation, we assessed the quality of the generated parallel sentences, achieving promising results with an 85.2% similarity score using Language-agnostic BERT Sentence Embedding (LaBSE) in conjunction with GPT, outperforming other investigated embedding models. The results demonstrate the potential of AI to address critical dataset challenges in quest of effective data driven solutions to support translation education.},
  archive      = {J_PEERJCS},
  author       = {Moneerh Aleedy and Fatma Alshihri and Souham Meshoul and Maha Al-Harthi and Salwa Alramlawi and Badr Aldaihani and Hadil Shaiba and Eric Atwell},
  doi          = {10.7717/peerj-cs.2788},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2788},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Designing AI-powered translation education tools: A framework for parallel sentence generation using SauLTC and LLMs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LIU-NET: Lightweight inception U-net for efficient brain tumor segmentation from multimodal 3D MRI images. <em>PEERJCS</em>, <em>11</em>, e2787. (<a href='https://doi.org/10.7717/peerj-cs.2787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting brain tumors is a critical task in medical imaging that relies on advanced deep-learning methods. However, effectively handling complex tumor regions requires more comprehensive and advanced strategies to overcome challenges such as computational complexity, the gradient vanishing problem, and variations in size and visual impact. To overcome these challenges, this research presents a novel and computationally efficient method termed lightweight Inception U-Net (LIU-Net) for the accurate brain tumor segmentation task. LIU-Net balances model complexity and computational load to provide consistent performance and uses Inception blocks to capture features at different scales, which makes it relatively lightweight. Its capability to efficiently and precisely segment brain tumors, especially in challenging-to-detect regions, distinguishes it from existing models. This Inception-style convolutional block assists the model in capturing multiscale features while preserving spatial information. Moreover, the proposed model utilizes a combination of Dice loss and Focal loss to handle the class imbalance issue. The proposed LIU-Net model was evaluated on the benchmark BraTS 2021 dataset, where it generates remarkable outcomes with a Dice score of 0.8121 for the enhancing tumor (ET) region, 0.8856 for the whole tumor (WT) region, and 0.8444 for the tumor core (TC) region on the test set. To evaluate the robustness of the proposed architecture, LIU-Net was cross-validated on an external cohort BraTS 2020 dataset. The proposed method obtained a Dice score of 0.8646 for the ET region, 0.9027 for the WT region, and 0.9092 for the TC region on the external cohort BraTS 2020 dataset. These results highlight the effectiveness of integrating the Inception blocks into the U-Net architecture, making it a promising candidate for medical image segmentation.},
  archive      = {J_PEERJCS},
  author       = {Gul e Sehar Shahid and Jameel Ahmad and Chaudary Atif Raza Warraich and Amel Ksibi and Shrooq Alsenan and Arfan Arshad and Rehan Raza and Zaffar Ahmed Shaikh},
  doi          = {10.7717/peerj-cs.2787},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2787},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {LIU-NET: Lightweight inception U-net for efficient brain tumor segmentation from multimodal 3D MRI images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DGCFNet: Dual global context fusion network for remote sensing image semantic segmentation. <em>PEERJCS</em>, <em>11</em>, e2786. (<a href='https://doi.org/10.7717/peerj-cs.2786'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic segmentation task of remote sensing images often faces various challenges such as complex backgrounds, high inter-class similarity, and significant differences in intra-class visual attributes. Therefore, segmentation models need to capture both rich local information and long-distance contextual information to overcome these challenges. Although convolutional neural networks (CNNs) have strong capabilities in extracting local information, they are limited in establishing long-range dependencies due to the inherent limitations of convolution. While Transformer can extract long-range contextual information through multi-head self attention mechanism, which has significant advantages in capturing global feature dependencies. To achieve high-precision semantic segmentation of remote sensing images, this article proposes a novel remote sensing image semantic segmentation network, named the Dual Global Context Fusion Network (DGCFNet), which is based on an encoder-decoder structure and integrates the advantages of CNN in capturing local information and Transformer in establishing remote contextual information. Specifically, to further enhance the ability of Transformer in modeling global context, a dual-branch global extraction module is proposed, in which the global compensation branch can not only supplement global information but also preserve local information. In addition, to increase the attention to salient regions, a cross-level information interaction module is adopted to enhance the correlation between features at different levels. Finally, to optimize the continuity and consistency of segmentation results, a feature interaction guided module is used to adaptively fuse information from intra layer and inter layer. Extensive experiments on the Vaihingen, Potsdam, and BLU datasets have shown that the proposed DGCFNet method can achieve better segmentation performance, with mIoU reaching 82.20%, 83.84% and 68.87%, respectively.},
  archive      = {J_PEERJCS},
  author       = {Yuan Liao and Tongchi Zhou and Lu Li and Jinming Li and Jiuhao Shen and Askar Hamdulla},
  doi          = {10.7717/peerj-cs.2786},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2786},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DGCFNet: Dual global context fusion network for remote sensing image semantic segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing colorectal polyp classification using gaze-based attention networks. <em>PEERJCS</em>, <em>11</em>, e2780. (<a href='https://doi.org/10.7717/peerj-cs.2780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal polyps are potential precursor lesions of colorectal cancer. Accurate classification of colorectal polyps during endoscopy is crucial for early diagnosis and effective treatment. Automatic and accurate classification of colorectal polyps based on convolutional neural networks (CNNs) during endoscopy is vital for assisting endoscopists in diagnosis and treatment. However, this task remains challenging due to difficulties in the data acquisition and annotation processes, the poor interpretability of the data output, and the lack of widespread acceptance of the CNN models by clinicians. This study proposes an innovative approach that utilizes gaze attention information from endoscopists as an auxiliary supervisory signal to train a CNN-based model for the classification of colorectal polyps. Gaze information from the reading of endoscopic images was first recorded through an eye-tracker. Then, the gaze information was processed and applied to supervise the CNN model’s attention via an attention consistency module. Comprehensive experiments were conducted on a dataset that contained three types of colorectal polyps. The results showed that EfficientNet_b1 with supervised gaze information achieved an overall test accuracy of 86.96%, a precision of 87.92%, a recall of 88.41%, an F1 score of 88.16%, the area under the receiver operating characteristic (ROC) curve (AUC) is 0.9022. All evaluation metrics surpassed those of EfficientNet_b1 without gaze information supervision. The class activation maps generated by the proposed network also indicate that the endoscopist’s gaze-attention information, as auxiliary prior knowledge, increases the accuracy of colorectal polyp classification, offering a new solution to the field of medical image analysis.},
  archive      = {J_PEERJCS},
  author       = {Zhenghao Guo and Yanyan Hu and Peixuan Ge and In Neng Chan and Tao Yan and Pak Kin Wong and Shaoyong Xu and Zheng Li and Shan Gao},
  doi          = {10.7717/peerj-cs.2780},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2780},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing colorectal polyp classification using gaze-based attention networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating artificial intelligence (AI) in healthcare: Advancing older adults’ health management in saudi arabia through AI-powered chatbots. <em>PEERJCS</em>, <em>11</em>, e2773. (<a href='https://doi.org/10.7717/peerj-cs.2773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The healthcare sector is experiencing rapid digital advancements, with patients increasingly seeking quick and seamless interactions. Artificial intelligence (AI)-driven healthcare chatbots are becoming an integral part of elderly care, transforming provider-patient engagement and supporting health behavior goals tailored to individual preferences, needs, and limitations. Methods This study developed a comprehensive research framework incorporating various theoretical perspectives to explore the determinants of sustained use of AI-powered healthcare chatbots among older adults. The framework also examined the mediating influence of perceived humanness. The model was evaluated using partial least squares structural equation modeling (PLS-SEM) on cross-sectional data collected from 158 individuals aged 60 and above. Results The findings show that satisfaction with AI-powered chatbots is significantly influenced by facilitating conditions, perceived hedonic motivation, confirmation, performance expectancy, and effort expectancy. Perceived security also plays a critical role in shaping satisfaction and the intention to continue using these chatbots. Moreover, the analysis revealed that perceived humanness mediates the relationship between satisfaction and continuous use intentions among elderly users in Saudi Arabia. Discussion This research provides valuable insights into the factors influencing older adults’ acceptance of AI chatbots in Saudi Arabia, particularly in the post-COVID-19 era. These findings enrich academic discourse and offer actionable recommendations for healthcare organizations adapting to the evolving digital landscape.},
  archive      = {J_PEERJCS},
  author       = {Sabah Abdullah Al-Somali},
  doi          = {10.7717/peerj-cs.2773},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2773},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Integrating artificial intelligence (AI) in healthcare: Advancing older adults’ health management in saudi arabia through AI-powered chatbots},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multichannel speech enhancement for automatic speech recognition: A literature review. <em>PEERJCS</em>, <em>11</em>, e2772. (<a href='https://doi.org/10.7717/peerj-cs.2772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multichannel speech enhancement (MCSE) is crucial for improving the robustness and accuracy of automatic speech recognition (ASR) systems. Due to the importance of ASR systems, extensive research has been conducted in MCSE, leading to rapid advancements in methods, models, and datasets. Most previous reviews point to the lack of a systematic literature review of MCSE for ASR systems. This systematic literature review aims to (1) perform a comprehensive review of the existing approaches in MCSE for ASR, (2) analyze the performance of the MCSE and ASR for various techniques, models, as well as noise data and environments, and (3) discuss the challenges, limitations, and future research directions in this research area. We conducted keyword searches on several electronic databases such as Google Scholar, IEEE Xplore, ScienceDirect, SpringerLink, ACM Digital Library, and ISI Web of Knowledge to identify relevant journal and conference articles. We selected 240 articles based on inclusion criteria from the initial search results and ended with 35 experimental articles when exclusion criteria were applied. Through backward snowballing and the quality assessment, the final tally was 40 articles, comprising 23 journals, and 17 conference articles. The review shows that there is an increasing trend in MCSE for ASR with word error rate (WER), perceptual evaluation of speech quality (PESQ), and short-time objective intelligence (STOI) as common forms of performance measures. One of the major issues that we found in the review is the generality and comparability of the MCSE works, making it difficult to come up with unified solutions to noises in speech recognition. This systematic literature review has extensively examined MCSE and ASR techniques. Key findings include identifying MCSE methods that help ASR performance across various models, techniques, noise, and environments. We also identify several key areas researchers can explore in the future due to their promising potential.},
  archive      = {J_PEERJCS},
  author       = {Zubair Zaland and Mumtaz Begum Mustafa and Miss Laiha Mat Kiah and Hua-Nong Ting and Mansoor Ali Mohamed Yusoof and Zuraidah Mohd Don and Saravanan Muthaiyah},
  doi          = {10.7717/peerj-cs.2772},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2772},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multichannel speech enhancement for automatic speech recognition: A literature review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coronary artery disease classification using ConvMixer based classifier from CT angiography images. <em>PEERJCS</em>, <em>11</em>, e2771. (<a href='https://doi.org/10.7717/peerj-cs.2771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary artery disease (CAD) has recently emerged as a predominant source of morbidity and death worldwide. Assessing the existence and severity of CAD in people is crucial for determining the optimal treatment strategy. Currently, computed tomography (CT) delivers excellent spatial resolution pictures of the heart and coronary arteries at a rapid pace. Conversely, several problems exist in the analysis of cardiac CT images for indications of CAD. Research investigations employ machine learning (ML) and deep learning (DL) techniques to achieve high accuracy and consistent performance, hence addressing existing restrictions. This research proposes convMixer with median filter and morphological operations for the classification of the coronary artery disease from computed tomography angiography images. A total of 5,959 CT angiography images were used for classification. The model achieved an accuracy of 96.30%, sensitivity of 94.39%, and specificity of 99.16% for combination of the morphological operations and convMixer, 88.92% of accuracy and 89.56% of sensitivity, and 93.10% of specificity for the combination of median filter and convMixer and 94.63% of accuracy, 95.82% of sensitivity, and 93.10% of specificity for convMixer. The findings indicate the viability of automated non-invasive identification of individuals necessitating invasive coronary angiography images and maybe future coronary artery operations. This may potentially decrease the number of people who receive invasive coronary angiography images. Lastly, post-image analysis was conducted using DL heat maps to understand the decisions made by the proposed model. The proposed integrated DL intelligent system enhances the efficiency of illness diagnosis, reduces manual involvement in diagnostic processes, supports medical professionals in diagnostic decision-making, and offers supplementary techniques for future medical diagnostic systems based on coronary angioplasty.},
  archive      = {J_PEERJCS},
  author       = {C. Rajeev and Karthika Natarajan},
  doi          = {10.7717/peerj-cs.2771},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2771},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Coronary artery disease classification using ConvMixer based classifier from CT angiography images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing knee osteoarthritis detection with AI, image denoising, and optimized classification methods and the importance of physical therapy methods. <em>PEERJCS</em>, <em>11</em>, e2766. (<a href='https://doi.org/10.7717/peerj-cs.2766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Osteoarthritis (OA) is considered one of the most challenging arthritic disorders due to its high disease burden and lack of effective treatment options that can change the course of the disease. Knee osteoarthritis (KOA) reduces people’s quality of life and shortens their daily activities. Therefore, early detection of KOA dramatically impacts patients’ quality of life. This study developed an artificial intelligence-supported system to detect KOA. In the developed system, firstly, the images in the original dataset were denoised with a Gaussian filter. Then, feature maps were extracted from both the original and Gaussian applied datasets with the DenseNet201 selected from eight different pre-trained models, and these two feature maps were concatenated. In this way, it is aimed to bring together different features of the same image. Then, feature selection was made using the neighborhood component analysis (NCA) method for the developed system to produce more successful results, and the optimized feature map was classified into six different classifiers. As a result, a high accuracy rate of 85% was achieved in the proposed model. This value is promising for the automatic diagnosis of KOA with computer-aided systems. As a result, a high accuracy rate of 85% was achieved in the developed system of the support vector machine (SVM) classifier. The proposed model was more successful than the other models used in the study.},
  archive      = {J_PEERJCS},
  author       = {Burak Bugday and Harun Bingol and Muhammed Yildirim and Bilal Alatas},
  doi          = {10.7717/peerj-cs.2766},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2766},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing knee osteoarthritis detection with AI, image denoising, and optimized classification methods and the importance of physical therapy methods},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IoT-based control and monitoring system for hydroponic plant growth using image processing and mobile applications. <em>PEERJCS</em>, <em>11</em>, e2763. (<a href='https://doi.org/10.7717/peerj-cs.2763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The HydroFarm project presents an innovative IoT-based control and monitoring system for hydroponic plant growth, integrating advanced image processing techniques and mobile applications to enhance urban farming practices. This system addresses critical challenges faced by urban farmers, such as limited space and the need for precise environmental management. By employing a comprehensive approach that combines various sensors (DHT22, DS18B20, pH, TDS) with an ESP32 microcontroller, HydroFarm enables real-time monitoring of essential parameters like temperature, humidity, pH, and nutrient levels. A significant novelty of this project lies in its use of a convolutional neural network (CNN) for plant health assessment through image processing. This technique allows for accurate detection of plant conditions, categorizing leaves as healthy or unhealthy based on visual data captured via a mobile application. The application, developed in Kotlin, not only facilitates user interaction but also provides automated and manual control over nutrient delivery systems based on real-time sensor data. Testing results indicate that the HydroFarm system achieves a high accuracy rate of 96% in detecting plant health conditions, with the sensors providing accurate and consistent data to maintain effective control over hydroponic parameters. The system usability scale (SUS) evaluation yielded an impressive score of 81.875, categorizing the application as excellent and user-friendly. Overall, HydroFarm represents a significant advancement in hydroponic farming technology by integrating IoT capabilities with deep learning for enhanced decision-making and operational efficiency in urban agriculture. The findings underscore the potential for scaling this model to improve food security and promote sustainable agricultural practices in densely populated areas.},
  archive      = {J_PEERJCS},
  author       = {Wizman Rofiansyah and Fayza Rizka Zalianty and Firman Ahmad La Ito and Inung Wijayanto and Harfan Hian Ryanu and Indrarini Dyah Irawati},
  doi          = {10.7717/peerj-cs.2763},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2763},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IoT-based control and monitoring system for hydroponic plant growth using image processing and mobile applications},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IPro-CSAF: Identification of promoters based on convolutional spiking neural networks and spiking attention mechanism. <em>PEERJCS</em>, <em>11</em>, e2761. (<a href='https://doi.org/10.7717/peerj-cs.2761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A promoter is a DNA segment which plays a key role in regulating gene expression. Accurate identification of promoters is significant for understanding the regulatory mechanisms involved in gene expression and genetic disease treatment. Therefore, it is an urgent challenge to develop computational methods for identifying promoters. Most current methods were designed for promoter recognition on few species and required complex feature extraction methods in order to attain high recognition accuracy. Spiking neural networks have inherent recurrence and use spike-based sparse coding. Therefore, they have good property of processing spatio-temporal information and are well suited for learning sequence information. In this study, iPro-CSAF, a convolutional spiking neural network combined with spiking attention mechanism is designed for promoter recognition. The method extracts promoter features by two parallel branches including spiking attention mechanism and a convolutional spiking layer. The promoter recognition of iPro-CSAF is evaluated by exhaustive promoter recognition experiments including both prokaryotic and eukaryotic promoter recognition from seven species. Our results show that iPro-CSAF outperforms promoter recognition methods which used parallel CNN layers, methods which combined CNNs with capsule networks, attention mechanism, LSTM or BiLSTM, and CNNs-based methods which needed priori biological or text feature extraction, while our method has much fewer network parameters. It indicates that iPro-CSAF is an effective computational method with low complexity and good generalization for promoter recognition.},
  archive      = {J_PEERJCS},
  author       = {Qian Zhou and Jie Meng and Hao Luo},
  doi          = {10.7717/peerj-cs.2761},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2761},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IPro-CSAF: Identification of promoters based on convolutional spiking neural networks and spiking attention mechanism},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Activation function cyclically switchable convolutional neural network model. <em>PEERJCS</em>, <em>11</em>, e2756. (<a href='https://doi.org/10.7717/peerj-cs.2756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are a state-of-the-art approach that performs well for many tasks. The activation function (AF) is an important hyperparameter that creates an output against the coming inputs to the neural network model. AF significantly affects the training and performance of the neural network model. Therefore, selecting the most optimal AF for processing input data in neural networks is important. Determining the optimal AF is often a difficult task. To overcome this difficulty, studies on trainable AFs have been carried out in the literature in recent years. This study presents a different approach apart from fixed or trainable AF approaches. For this purpose, the activation function cyclically switchable convolutional neural network (AFCS-CNN) model structure is proposed. The AFCS-CNN model structure does not use a fixed AF value during training. It is designed in a self-regulating model structure by switching the AF during model training. The proposed model structure is based on the logic of starting training with the most optimal AF selection among many AFs and cyclically selecting the next most optimal AF depending on the performance decrease during neural network training. Any convolutional neural network (CNN) model can be easily used in the proposed model structure. In this way, a simple but effective perspective has been presented. In this study, first, ablation studies have been carried out using the Cifar-10 dataset to determine the CNN models to be used in the AFCS-CNN model structure and the specific hyperparameters of the proposed model structure. After the models and hyperparameters were determined, expansion experiments were carried out using different datasets with the proposed model structure. The results showed that the AFCS-CNN model structure achieved state-of-the-art success in many CNN models and different datasets.},
  archive      = {J_PEERJCS},
  author       = {İsmail Akgül},
  doi          = {10.7717/peerj-cs.2756},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2756},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Activation function cyclically switchable convolutional neural network model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven flight path monitoring technique using recurrent neural network for the safety management of commercial aircraft. <em>PEERJCS</em>, <em>11</em>, e2753. (<a href='https://doi.org/10.7717/peerj-cs.2753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aviation spins, particularly at low altitudes, significantly contribute to fatalities due to limited recovery time. Standard recovery procedures typically only become eligible after a spin is fully developed, by which time multiple turns may have already resulted in substantial altitude loss. The primary challenge in upset prevention is heavy reliance on the pilot’s situational awareness, which is only effective before the spin has been fully developed. To address this issue, this study proposes an early detection capability to significantly enhance immediate response actions, potentially mitigating altitude loss and enabling pilots to recognize the initial signs of upset conditions. This research introduces a real-time predictive tool based on a novel recurrent neural network (RNN) model that utilizes data from the NASA Generic Transport Model (GTM)-a research platform designed for experimental flight case studies-to predict nonlinear flight responses during the critical initial seconds of a spin. Rigorous validation against ground truth data demonstrates the RNN model’s superior predictive capabilities in detecting incipient spin phase, offering an essential tool for proactive spin management and reducing the risk of ground collisions. This early detection capability empowers pilots to identify the initial signs of upset conditions and make informed operational decisions, ultimately improving aviation safety. This advancement underscores the potential of advanced machine learning technologies to transform safety protocols by enabling earlier and more effective intervention strategies, thereby preempting catastrophic events.},
  archive      = {J_PEERJCS},
  author       = {Naeun Kim and Mohamed H. Hamza and Bong-Hwan Koh},
  doi          = {10.7717/peerj-cs.2753},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2753},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data-driven flight path monitoring technique using recurrent neural network for the safety management of commercial aircraft},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning techniques for imbalanced multiclass malware classification through adaptive feature selection. <em>PEERJCS</em>, <em>11</em>, e2752. (<a href='https://doi.org/10.7717/peerj-cs.2752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting polymorphic or metamorphic variants of known malware is an ever-growing challenge, just like detecting new malware. Artificial intelligence techniques are preferred over conventional signature-based malware detection as the number of malware variants proliferates. This article proposes an Adaptive Multiclass Malware Classification (AMMC) framework that trains base machine learning models with fewer computational resources to detect malware. Furthermore, this work proposes a novel adaptive feature selection (AFS) technique using the greedy strategy on term frequency and inverse document frequency (TF-IDF) feature weights to address the selection of influential features and ensure better performance metrics in imbalanced multiclass malware classification problems. To assess AMMC’s efficacy using AFS, three open imbalanced multiclass malware datasets (VirusShare with eight classes, VirusSample with six classes, and MAL-API-2019 with eight classes) on Windows API sequence features were used. Experimental results demonstrate the effectiveness of AMMC with AFS, achieving state-of-the-art performance on VirusShare, VirusSample, and MAL-API-2019 with a macro F1-score of 0.92, 0.94, and 0.84 and macro area under the curve (AUC) of 0.99, 0.99, and 0.98, respectively. The performance measurements obtained with AMMC for all datasets were highly promising.},
  archive      = {J_PEERJCS},
  author       = {Binayak Panda and Sudhanshu Shekhar Bisoyi and Sidhanta Panigrahy and Prithviraj Mohanty},
  doi          = {10.7717/peerj-cs.2752},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2752},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Machine learning techniques for imbalanced multiclass malware classification through adaptive feature selection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning with LSTM for intrusion detection in IoT-based wireless sensor networks: A multi-dataset analysis. <em>PEERJCS</em>, <em>11</em>, e2751. (<a href='https://doi.org/10.7717/peerj-cs.2751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection in Internet of Things (IoT)-based wireless sensor networks (WSNs) is essential due to their widespread use and inherent vulnerability to security breaches. Traditional centralized intrusion detection systems (IDS) face significant challenges in data privacy, computational efficiency, and scalability, particularly in resource-constrained IoT environments. This study aims to create and assess a federated learning (FL) framework that integrates with long short-term memory (LSTM) networks for efficient intrusion detection in IoT-based WSNs. We design the framework to enhance detection accuracy, minimize false positive rates (FPR), and ensure data privacy, while maintaining system scalability. Using an FL approach, multiple IoT nodes collaboratively train a global LSTM model without exchanging raw data, thereby addressing privacy concerns and improving detection capabilities. The proposed model was tested on three widely used datasets: WSN-DS, CIC-IDS-2017, and UNSW-NB15. The evaluation metrics for its performance included accuracy, F1 score, FPR, and root mean square error (RMSE). We evaluated the performance of the FL-based LSTM model against traditional centralized models, finding significant improvements in intrusion detection. The FL-based LSTM model achieved higher accuracy and a lower FPR across all datasets than centralized models. It effectively managed sequential data in WSNs, ensuring data privacy while maintaining competitive performance, particularly in complex attack scenarios. FL and LSTM networks work well together to make a strong way to find intrusions in IoT-based WSNs, which improves both privacy and detection. This study underscores the potential of FL-based systems to address key challenges in IoT security, including data privacy, scalability, and performance, making the proposed framework suitable for real-world IoT applications.},
  archive      = {J_PEERJCS},
  author       = {Raja Waseem Anwar and Mohammad Abrar and Abdu Salam and Faizan Ullah},
  doi          = {10.7717/peerj-cs.2751},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2751},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Federated learning with LSTM for intrusion detection in IoT-based wireless sensor networks: A multi-dataset analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sequential recommendation method using contrastive learning and wasserstein self-attention mechanism. <em>PEERJCS</em>, <em>11</em>, e2749. (<a href='https://doi.org/10.7717/peerj-cs.2749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has demonstrated the effectiveness of utilizing contrastive learning for training Transformer-based sequence encoders in sequential recommendation tasks. Items are represented using vectors and the relations between items are measured by the dot product self-attention, the feature representation in sequential recommendation can be enhanced. However, in real-world scenarios, user behavior sequences are unpredictable, and the limitations of dot product-based approaches hinder the complete capture of collaborative transferability. Moreover, the Bayesian personalized ranking (BPR) loss function, commonly utilized in recommendation systems, lacks constraints when considering positive and negative sampled items, potentially leading to suboptimal optimization outcomes. This presents a complex challenge that needs to be addressed. To tackle these issues, this article proposes a novel method involving stochastic self-attention. This article introduces uncertainty into the proposed model by utilizing elliptical Gaussian distribution controlled by mean and covariance vector to explain the unpredictability of items. At the same time, the proposed model combines a Wasserstein self-attention module to compute the positional relationships between items within a sequence in order to effectively incorporate uncertainty into the training process. The Wasserstein self-attention mechanism satisfies the triangular inequality and can not only addresses uncertainty but also promote collaborative transfer learning. Furthermore, embedding a stochastic Gaussian distribution into each item will bring additional uncertainty into the proposed model. Multi-pair contrastive learning relies on high-quality positive samples, and the proposed model combines the cloze task mask and dropout mask mechanisms to generate high-quality positive samples. It demonstrates superior performance and adaptability compared to traditional single-pair contrastive learning methods. Additionally, a dynamic loss reweighting strategy is introduced to balance the cloze task loss and the contrastive loss effectively. We conduct experiments and the results show that the proposed model outperforms the state-of-the-art models, especially on cold start items. For each metric, the hit ratio (HR) and normalized discounted cumulative gain (NDCG) on the Beauty dataset improved by an average of 1.3% and 10.27%, respectively; on the Toys dataset improved by an average of 8.24% and 5.89%, respectively; on the ML-1M dataset improved by an average of 68.62% and 8.22%, respectively; and on the ML-100M dataset improved by an average of 93.57% and 44.87% Our code is available at DOI: 10.5281/zenodo.13634624.},
  archive      = {J_PEERJCS},
  author       = {Shengbin Liang and Jinfeng Ma and Qiuchen Zhao and Tingting Chen and Xixi Lu and Shuanglong Ren and Chenyang Zhao and Lei Fu and Huichao Ding},
  doi          = {10.7717/peerj-cs.2749},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2749},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A sequential recommendation method using contrastive learning and wasserstein self-attention mechanism},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protein-protein interaction prediction using enhanced features with spaced conjoint triad and amino acid pairwise distance. <em>PEERJCS</em>, <em>11</em>, e2748. (<a href='https://doi.org/10.7717/peerj-cs.2748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein-protein interactions (PPIs) are pivotal in cellular processes, influencing a wide range of functions, from metabolism to immune responses. Despite the advancements in experimental techniques for PPI detection, their inherent limitations, such as high false-positive rates and significant resource demands, necessitate the development of computational approaches. This study presents a novel computational model named MFPIC (Multi-Feature Protein Interaction Classifier) for predicting PPIs, integrating enhanced sequence-based features, including a novel spaced conjoint triad (SCT) and amino acid pairwise distance (AAPD), with existing methods such as position-specific scoring matrices (PSSM) and AAindex-based features. The SCT captures complex sequence motifs by considering non-adjacent amino acid interactions, while AAPD provides critical spatial information about amino acid residues within protein sequences. The proposed model was evaluated across three benchmark datasets—Saccharomyces cerevisiae, Helicobacter pylori, and human proteins—demonstrating superior performance in comparison to state-of-the-art models. The results underscore the efficacy of integrating diverse and complementary features, achieving significant improvements in predictive accuracy, with the model achieving 95.90%, 99.33%, and 90.95% accuracy on the Saccharomyces cerevisiae, Helicobacter pylori, and human dataset, respectively. This approach not only enhances our understanding of PPI mechanisms but also offers valuable insights for the development of targeted therapeutic strategies.},
  archive      = {J_PEERJCS},
  author       = {Yunus Emre Göktepe},
  doi          = {10.7717/peerj-cs.2748},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2748},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Protein-protein interaction prediction using enhanced features with spaced conjoint triad and amino acid pairwise distance},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient unified architecture for post-quantum cryptography: Combining dilithium and kyber. <em>PEERJCS</em>, <em>11</em>, e2746. (<a href='https://doi.org/10.7717/peerj-cs.2746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the ongoing standardization process of post-quantum schemes yields initial outcomes, it becomes increasingly important to not only optimize standalone implementations but also explore the potential of combining multiple schemes into a single, unified architecture. In this article, we investigate the combination of two National Institute of Standards and Technology (NIST)-selected schemes: the Dilithium digital signature scheme and the Kyber key encapsulation mechanism. We propose a novel set of optimization techniques for a unified hardware implementation of these leading post-quantum schemes, achieving a balanced approach between area efficiency and high performance. Our design demonstrates superior resource efficiency and performance compared to previously reported unified architecture (DOI 10.1109/TCSI.2022.3219555), also achieving results that are better than, or comparable, to those of standalone implementations. The efficient and combined implementation of lattice-based digital signatures and key establishment methods can be deployed for establishing secure sessions in high-speed communication networks at servers and gateways. Moreover, the unique and compact design that requires small hardware resources can be directly used in small and cost-effective field programmable gate array (FPGA) platforms that can be used as security co-processors for embedded devices and in the Internet of Things.},
  archive      = {J_PEERJCS},
  author       = {Patrik Dobias and Lukas Malina and Jan Hajny},
  doi          = {10.7717/peerj-cs.2746},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2746},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Efficient unified architecture for post-quantum cryptography: Combining dilithium and kyber},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal intrusion detection for imbalanced data using bagging method with deep neural network optimized by flower pollination algorithm. <em>PEERJCS</em>, <em>11</em>, e2745. (<a href='https://doi.org/10.7717/peerj-cs.2745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the number of connected devices and Internet of Things (IoT) devices grows, it is becoming more and more important to develop efficient security mechanisms to manage risks and vulnerabilities in IoT networks. Intrusion detection systems (IDSs) have been developed and implemented in IoT networks to discern between regular network traffic and potential malicious attacks. This article proposes a new IDS based on a hybrid method of metaheuristic and deep learning techniques, namely, the flower pollination algorithm (FPA) and deep neural network (DNN), with an ensemble learning paradigm. To handle the problem of imbalance class distribution in intrusion datasets, a roughly-balanced (RB) Bagging strategy is utilized, where DNN models trained by FPA on a cost-sensitive fitness function are used as base learners. The RB Bagging strategy derives multiple RB training subsets from the original dataset and proper class weights are incorporated into the fitness function to attain unbiased DNN models. The performance of our IDS is evaluated using four commonly utilized public datasets, NSL-KDD, UNSW NB-15, CIC-IDS-2017, and BoT-IoT, in terms of different metrics, i.e., accuracy, precision, recall, and F1-score. The results demonstrate that our IDS outperforms existing ones in accurately detecting network intrusions with effective handling of class imbalance problem.},
  archive      = {J_PEERJCS},
  author       = {Hussein Ridha Sayegh and Wang Dong and Bahaa Hussein Taher and Muhanad Mohammed Kadum and Ali Mansour Al-madani},
  doi          = {10.7717/peerj-cs.2745},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2745},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimal intrusion detection for imbalanced data using bagging method with deep neural network optimized by flower pollination algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The line follower robot: A meta-analytic approach. <em>PEERJCS</em>, <em>11</em>, e2744. (<a href='https://doi.org/10.7717/peerj-cs.2744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line-follower robots represent a critical segment in autonomous robotics, with broad applications ranging from industrial automation to educational tools. This meta-analytic review synthesizes research on line-follower robots, addressing a noticeable gap in the literature where comprehensive analyses are scarce. The review leverages the Theory of the Consolidated Meta-analytic Approach (TEMAC) to systematically explore 287 documents spanning from 2001 to 2024, highlighting key contributions, trends, and gaps in the field. Through this analysis, it becomes evident that while significant advancements have been made in control strategies, sensor integration, and noise reduction techniques, the literature still lacks comprehensive studies on the scalability of these technologies, especially in large-scale industrial environments. Recent research trends emphasize integrating artificial intelligence and machine learning into line-follower robots, indicating a shift towards more sophisticated, adaptable systems. Despite these advancements, challenges remain in addressing environmental variability, improving real-time adaptability, and exploring novel applications in dynamic environments. This review not only maps the historical evolution and current state of line-follower robots but also identifies future research directions that could drive the next generation of robotic systems. The findings offer valuable insights for researchers, engineers, and educators aiming to enhance the efficiency, reliability, and application scope of line-follower robots.},
  archive      = {J_PEERJCS},
  author       = {Williamson Johnny Hatzinakis Brigido and Jose M. Parente de Oliveira},
  doi          = {10.7717/peerj-cs.2744},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2744},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The line follower robot: A meta-analytic approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved smart city security using a deep maxout network-based intrusion detection system with walrus optimization. <em>PEERJCS</em>, <em>11</em>, e2743. (<a href='https://doi.org/10.7717/peerj-cs.2743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Smart cities, enabled by the Internet of Things (IoT), leverage technology to optimize urban living and enhance infrastructure. As urban environments become interconnected hubs of digital innovation, securing critical components like public transportation infrastructure becomes increasingly important. Methods This research addresses the need for robust intrusion detection systems (IDS) tailored to the unique challenges of securing public transportation within smart cities. Focused on the Tabuk region in Saudi Arabia, the study introduces an IDS model integrating the deep maxout network with walrus optimization (DMN-WO). The DMN is configured with an architecture that includes multiple layers with maxout activation functions. These layers are capable of capturing complex patterns in the data, making the DMN particularly effective for identifying anomalies in IoT network traffic. The DMN-WO model is ensured to be resource-efficient and suitable for real-time deployment on constrained devices like Raspberry Pi, typical in IoT systems. Results Training and validation are conducted using the CIC-IDS-2018 dataset, CIC-IDS -2029 dataset and real-time data from Raspberry Pi devices deployed in the smart city’s public transportation network. Real-time data application maintains robust performance, with 98.06% accuracy, 98.50% detection rate, 98.81% precision, 98.24% specificity, and a 98.57% F1-score. Conclusions This research advances cybersecurity measures in smart city applications by providing a resilient solution for detecting and mitigating security threats in public transportation infrastructure. It lays the groundwork for further refinements and real-world deployments in the dynamic landscape of smart cities.},
  archive      = {J_PEERJCS},
  author       = {Wahid Rajeh and Majed Aborokbah and Manimurugan S. and Umar Albalawi and Ahamed Aljuhani and Osama Shibl Abdalghany Younes and Karthikeyan Periyasami},
  doi          = {10.7717/peerj-cs.2743},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2743},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improved smart city security using a deep maxout network-based intrusion detection system with walrus optimization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on sports activity behavior prediction based on electromyography signal collection and intelligent sensing channel. <em>PEERJCS</em>, <em>11</em>, e2742. (<a href='https://doi.org/10.7717/peerj-cs.2742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports behavior prediction requires precise and reliable analysis of muscle activity during exercise. This study proposes a multi-channel correlation feature extraction method for electromyographic (EMG) signals to overcome challenges in sports behavior prediction. A wavelet threshold denoising algorithm is enhanced with nonlinear function transitions and control coefficients to improve signal quality, achieving effective noise reduction and a higher signal-to-noise ratio. Furthermore, multi-channel linear and nonlinear correlation features are combined, leveraging mutual information estimation via copula entropy for feature construction. A stacking ensemble learning model, incorporating extreme gradient boosting (XGBoost), K-nearest network (KNN), Random Forest (RF), and naive Bayes (NB) as base learners, further enhances classification accuracy. Experimental results demonstrate that the proposed approach achieves over 95% prediction accuracy, significantly outperforming traditional methods. The robustness of multi-channel correlation features is validated across diverse datasets, proving their effectiveness in mitigating channel crosstalk and noise interference. This work provides a scientific basis for improving sports training strategies and reducing injury risks.},
  archive      = {J_PEERJCS},
  author       = {Fengjin Ye and Yuchao Zhao and Zohaib Latif},
  doi          = {10.7717/peerj-cs.2742},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2742},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on sports activity behavior prediction based on electromyography signal collection and intelligent sensing channel},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-edge MQTT messaging for latency mitigation and broker memory footprint reduction. <em>PEERJCS</em>, <em>11</em>, e2741. (<a href='https://doi.org/10.7717/peerj-cs.2741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of smart-city applications has increased the number of Internet of Things (IoT) devices connected to a network cloud. Thanks to its flexibility in matching data publishers and subscribers, broker-based data communication could be a solution for such IoT data delivery, and MQTT is one of the widely used messaging protocols in this class. While MQTT by default does not differentiate message flows by size, it is observed that transient local network congestion may cause size-dependent latency additions, and that the accumulation of large message copies in the cloud broker could run out of the broker memory. In response, in the scope of cloud-edge messaging, this research article presents problem analysis, system design and implementation, and empirical and analytical performance evaluation. The article introduces three message scheduling policies for subscribers deployed at network edge, and a memory allocation scheme for MQTT broker deployed at network cloud. The proposed design has been implemented based on Eclipse Mosquitto, an open-source MQTT broker implementation. Empirical and analytical validations have demonstrated the performance of the proposed design in latency mitigation, and the result also shows that, empirically, the proposed design may save the run-time broker memory footprint by about 75%. Applicability of the proposed design to other messaging services are discussed by the end of the article.},
  archive      = {J_PEERJCS},
  author       = {Yi-Hsuan Tseng and Chao Wang and Yu-Tse Wei and Yu-Ting Chiang},
  doi          = {10.7717/peerj-cs.2741},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2741},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Cloud-edge MQTT messaging for latency mitigation and broker memory footprint reduction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DE-RALBA: Dynamic enhanced resource aware load balancing algorithm for cloud computing. <em>PEERJCS</em>, <em>11</em>, e2739. (<a href='https://doi.org/10.7717/peerj-cs.2739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing provides an opportunity to gain access to the large-scale and high-speed resources without establishing your own computing infrastructure for executing the high-performance computing (HPC) applications. Cloud has the computing resources (i.e., computation power, storage, operating system, network, and database etc.) as a public utility and provides services to the end users on a pay-as-you-go model. From past several years, the efficient utilization of resources on a compute cloud has become a prime interest for the scientific community. One of the key reasons behind inefficient resource utilization is the imbalance distribution of workload while executing the HPC applications in a heterogenous computing environment. The static scheduling technique usually produces lower resource utilization and higher makespan, while the dynamic scheduling achieves better resource utilization and load-balancing by incorporating a dynamic resource pool. The dynamic techniques lead to increased overhead by requiring a continuous system monitoring, job requirement assessments and real-time allocation decisions. This additional load has the potential to impact the performance and responsiveness on computing system. In this article, a dynamic enhanced resource-aware load balancing algorithm (DE-RALBA) is proposed to mitigate the load-imbalance in job scheduling by considering the computing capabilities of all VMs in cloud computing. The empirical assessments are performed on CloudSim simulator using instances of two scientific benchmark datasets (i.e., heterogeneous computing scheduling problems (HCSP) instances and Google Cloud Jobs (GoCJ) dataset). The obtained results revealed that the DE-RALBA mitigates the load imbalance and provides a significant improvement in terms of makespan and resource utilization against existing algorithms, namely PSSLB, PSSELB, Dynamic MaxMin, and DRALBA. Using HCSP instances, the DE-RALBA algorithm achieves up to 52.35% improved resources utilization as compared to existing technique, while more superior resource utilization is achieved using the GoCJ dataset.},
  archive      = {J_PEERJCS},
  author       = {Altaf Hussain and Muhammad Aleem and Atiq Ur Rehman and Umer Arshad},
  doi          = {10.7717/peerj-cs.2739},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2739},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DE-RALBA: Dynamic enhanced resource aware load balancing algorithm for cloud computing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A zero-trust based scheme for detecting illegal terminals in the internet of things of smart grid. <em>PEERJCS</em>, <em>11</em>, e2736. (<a href='https://doi.org/10.7717/peerj-cs.2736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Internet of Things (IoT) for electricity has faced a series of new challenges. Attackers use a compromised terminal as a springboard to enter the network, steal data, issue malicious commands, and cause great harm. In order to combat the threat of compromised terminals, this article proposes a zero-trust based detection scheme for illegal terminals, based on the principle of “never trust, always verify” security mechanism. Firstly, the detection scheme uses the state secret SM9 secret system to authenticate the access device. Then, it proposes a continuous trust evaluation based on the centroid drift trust algorithm on the characteristics of the traffic of the input device. Finally, it generates a real-time access policy by the access control engine to achieve a dynamic access policy. Finally, the access control engine generates real-time access policies to achieve dynamic access control. Experimental results show that the designed system has a high security detection accuracy and can effectively deal with the threat of compromised terminals.},
  archive      = {J_PEERJCS},
  author       = {Hongyu Zhu and Jianwei Tian and Qian Chen and Zheng Tian and Weiqiang Luo and Mingguang Li},
  doi          = {10.7717/peerj-cs.2736},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2736},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A zero-trust based scheme for detecting illegal terminals in the internet of things of smart grid},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based deep fusion for architectural text representation. <em>PEERJCS</em>, <em>11</em>, e2735. (<a href='https://doi.org/10.7717/peerj-cs.2735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amidst the swift global urbanization and rapid evolution of the architecture industry, there is a growing demand for the automated processing of architectural textual information. This demand arises from the abundance of specialized vocabulary in architectural texts, posing a challenge for accurate representation using traditional models. To address this, we propose a novel fusion method that integrates Transformer-based models with graph neural networks (GNNs) for architectural text representation. While independently utilizing Bidirectional Encoder Representations from Transformers (BERT) and the robustly optimized BERT approach (RoBERTa) to generate initial document representations, we also employ term frequency-inverse document frequency (TF-IDF) to extract keywords from each document and construct a corresponding keyword set. Subsequently, a graph is created based on the keyword vocabulary and document embeddings, which is then fed into the graph attention network (GAT). The final document embedding is generated by GAT, and the text embedding is crafted by the attention module and neural network structure of the GAT. Experimental results from comparison studies show that the proposed model outperforms all baselines. Additionally, ablation studies demonstrate the effectiveness of each module, further reinforcing the robustness and superiority of our approach.},
  archive      = {J_PEERJCS},
  author       = {Shaoyun Hu and Qingxiong Weng},
  doi          = {10.7717/peerj-cs.2735},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2735},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Graph-based deep fusion for architectural text representation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protein language model-based prediction for plant miRNA encoded peptides. <em>PEERJCS</em>, <em>11</em>, e2733. (<a href='https://doi.org/10.7717/peerj-cs.2733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant miRNA encoded peptides (miPEPs), which are short peptides derived from small open reading frames within primary miRNAs, play a crucial role in regulating diverse plant traits. Plant miPEPs identification is challenging due to limitations in the available number of known miPEPs for training. Existing prediction methods rely on manually encoded features, including miPEPPred-FRL, to infer plant miPEPs. Recent advances in deep learning modeling of protein sequences provide an opportunity to improve the representation of key features, leveraging large datasets of protein sequences. In this study, we propose an accurate prediction model, called pLM4PEP, which integrates ESM2 peptide embedding with machine learning methods. Our model not only demonstrates precise identification capabilities for plant miPEPs, but also achieves remarkable results across diverse datasets that include other bioactive peptides. The source codes, datasets of pLM4PEP are available at https://github.com/xialab-ahu/pLM4PEP.},
  archive      = {J_PEERJCS},
  author       = {Yishan Yue and Henghui Fan and Jianping Zhao and Junfeng Xia},
  doi          = {10.7717/peerj-cs.2733},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2733},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Protein language model-based prediction for plant miRNA encoded peptides},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Customizable pattern synthesis: A deep generative approach for lantern designs. <em>PEERJCS</em>, <em>11</em>, e2732. (<a href='https://doi.org/10.7717/peerj-cs.2732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern design is essential in various domains, especially in traditional lantern production, where patterns convey cultural history and artistic values. Our research presents an innovative generative model that produces customizable lantern patterns, integrating classical aesthetics with modern design features via a generative adversarial network (GAN)-based framework. The model was trained on an extensive dataset of over 17,000 pattern images over ten various categories. Experimental assessment demonstrates the model’s remarkable proficiency, achieving an Inception Score of 5.259, much surpassing the performance of other GAN-based approaches. This exceptional result demonstrates the effective integration of traditional pattern elements with AI-driven design processes. The model offers enhanced design flexibility via noise vector hybridization and post-processing techniques, allowing for accurate control over pattern production while preserving cultural authenticity. These capabilities make our model a valuable tool for modernizing lantern pattern design while maintaining classic artistic elements.},
  archive      = {J_PEERJCS},
  author       = {Mengran Yan and Chun Tang and Jida Yan and Siti Suhaily Surip},
  doi          = {10.7717/peerj-cs.2732},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2732},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Customizable pattern synthesis: A deep generative approach for lantern designs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial feature learning for semantic communication in human 3D reconstruction. <em>PEERJCS</em>, <em>11</em>, e2731. (<a href='https://doi.org/10.7717/peerj-cs.2731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of human body 3D reconstruction technology across various fields, the demands for data transmission and processing efficiency continue to rise, particularly in scenarios where network bandwidth is limited and low latency is required. This article introduces an Adversarial Feature Learning-based Semantic Communication method (AFLSC) for human body 3D reconstruction, which focuses on extracting and transmitting semantic information crucial for the 3D reconstruction task, thereby significantly optimizing data flow and alleviating bandwidth pressure. At the sender’s end, we propose a multitask learning-based feature extraction method to capture the spatial layout, keypoints, posture, and depth information from 2D human images, and design a semantic encoding technique based on adversarial feature learning to encode these feature information into semantic data. We also develop a dynamic compression technique to efficiently transmit this semantic data, greatly enhancing transmission efficiency and reducing latency. At the receiver’s end, we design an efficient multi-level semantic feature decoding method to convert semantic data back into key image features. Finally, an improved ViT-diffusion model is employed for 3D reconstruction, producing human body 3D mesh models. Experimental results validate the advantages of our method in terms of data transmission efficiency and reconstruction quality, demonstrating its excellent potential for application in bandwidth-limited environments.},
  archive      = {J_PEERJCS},
  author       = {Shaojiang Liu and Jiajun Zou and Zhendan Liu and Meixia Dong and Zhiping Wan},
  doi          = {10.7717/peerj-cs.2731},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2731},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Adversarial feature learning for semantic communication in human 3D reconstruction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data leakage detection in machine learning code: Transfer learning, active learning, or low-shot prompting?. <em>PEERJCS</em>, <em>11</em>, e2730. (<a href='https://doi.org/10.7717/peerj-cs.2730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing reliance on machine learning (ML) across diverse disciplines, ML code has been subject to a number of issues that impact its quality, such as lack of documentation, algorithmic biases, overfitting, lack of reproducibility, inadequate data preprocessing, and potential for data leakage, all of which can significantly affect the performance and reliability of ML models. Data leakage can affect the quality of ML models where sensitive information from the test set inadvertently influences the training process, leading to inflated performance metrics that do not generalize well to new, unseen data. Data leakage can occur at either the dataset-level (i.e., during dataset construction) or at the code-level. Existing studies introduced methods to detect code-level data leakage using manual and code analysis approaches. However, automated tools with advanced ML techniques are increasingly recognized as essential for efficiently identifying quality issues in large and complex codebases, enhancing the overall effectiveness of code review processes. In this article, we aim to explore ML-based approaches for limited annotated datasets to detect code-level data leakage in ML code. We proposed three approaches, namely, transfer learning, active learning, and low-shot prompting. Additionally, we introduced an automated approached to handle the imbalance issues of code data. Our results show that active learning outperformed the other approaches with an F-2 score of 0.72 and reduced the number of needed annotated samples from 1,523 to 698. We conclude that existing ML-based approaches can effectively mitigate the challenges associated with limited data availability.},
  archive      = {J_PEERJCS},
  author       = {Nouf Alturayeif and Jameleddine Hassine},
  doi          = {10.7717/peerj-cs.2730},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2730},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data leakage detection in machine learning code: Transfer learning, active learning, or low-shot prompting?},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TurkSentGraphExp: An inherent graph aware explainability framework from pre-trained LLM for turkish sentiment analysis. <em>PEERJCS</em>, <em>11</em>, e2729. (<a href='https://doi.org/10.7717/peerj-cs.2729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment classification is a widely studied problem in natural language processing (NLP) that focuses on identifying the sentiment expressed in text and categorizing it into predefined classes, such as positive, negative, or neutral. As sentiment classification solutions are increasingly integrated into real-world applications, such as analyzing customer feedback in business reviews (e.g., hotel reviews) or monitoring public sentiment on social media, the importance of both their accuracy and explainability has become widely acknowledged. In the Turkish language, this problem becomes more challenging due to the complex agglutinative structure of the language. Many solutions have been proposed in the literature to solve this problem. However, it is observed that the solutions are generally based on black-box models. Therefore the explainability requirement of such artificial intelligence (AI) models has become as important as the accuracy of the model. This has further increased the importance of studies based on the explainability of the AI model’s decision. Although most existing studies prefer to explain the model decision in terms of the importance of a single feature/token, this does not provide full explainability due to the complex lexical and semantic relations in the texts. To fill these gaps in the Turkish NLP literature, in this article, we propose a graph-aware explainability solution for Turkish sentiment analysis named TurkSentGraphExp. The solution provides both classification and explainability for sentiment classification of Turkish texts by considering the semantic structure of suffixes, accommodating the agglutinative nature of Turkish, and capturing complex relationships through graph representations. Unlike traditional black-box learning models, this framework leverages an inherent graph representation learning (GRL) model to introduce rational phrase-level explainability. We conduct several experiments to quantify the effectiveness of this framework. The experimental results indicate that the proposed model achieves a 10 to 40% improvement in explainability compared to state-of-the-art methods across varying sparsity levels, further highlighting its effectiveness and robustness. Moreover, the experimental results, supported by a case study, reveal that the semantic relationships arising from affixes in Turkish texts can be identified as part of the model’s decision-making process, demonstrating the proposed solution’s ability to effectively capture the agglutinative structure of Turkish.},
  archive      = {J_PEERJCS},
  author       = {Yasir Kilic and Cagatay Neftali Tulu},
  doi          = {10.7717/peerj-cs.2729},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2729},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TurkSentGraphExp: An inherent graph aware explainability framework from pre-trained LLM for turkish sentiment analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of artificial intelligence technology in the economic development of urban intelligent transportation system. <em>PEERJCS</em>, <em>11</em>, e2728. (<a href='https://doi.org/10.7717/peerj-cs.2728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the social economy and the gradual improvement of residents’ living standards, the increasing number of urban cars has exacerbated urban traffic congestion. This article analyzed the application of artificial intelligence (AI) technology in five aspects of urban intelligent transportation systems. Artificial intelligence technology was used in traffic data collection and processing to provide accurate data support for traffic decision-making. A traffic flow prediction model was established for traffic flow prediction and optimized scheduling algorithms were used to dispatch vehicles on congested urban roads intelligently. Artificial intelligence algorithms can be used to optimize urban traffic signal control systems in intelligent traffic signal control; artificial intelligence technology can be applied to develop intelligent driving systems in the fields of intelligent driving and traffic safety; in terms of data analysis and decision support, it can use AI technology to analyze a large number of traffic data to provide decision support for urban traffic managers, and analyze the impact of the application of AI technology in urban intelligent transportation system on urban economic growth. This article evaluated the economic benefits of artificial intelligence technology in urban intelligent transportation systems. The evaluation results show that the total economic cost of the urban intelligent transportation system after the application of AI technology was 2,961 yuan less than before the application of AI technology, significantly reducing the investment cost of roads. This article analyzes the application of artificial intelligence technology in the economic development of intelligent urban transportation systems, which can meet the needs of healthy urban development and ensure road traffic safety.},
  archive      = {J_PEERJCS},
  author       = {Ziming Zhao and Jinyu Chen},
  doi          = {10.7717/peerj-cs.2728},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2728},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Application of artificial intelligence technology in the economic development of urban intelligent transportation system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel cross-dimensional coarse-fine-grained complementary network for image-text matching. <em>PEERJCS</em>, <em>11</em>, e2725. (<a href='https://doi.org/10.7717/peerj-cs.2725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fundamental aspects of multimodal applications such as image-text matching, and cross-modal heterogeneity gap between images and texts have always been challenging and complex. Researchers strive to overcome the challenges by proposing numerous significant efforts directed toward narrowing the semantic gap between visual and textual modalities. However, existing methods are usually limited to computing the similarity between images (image regions) and text (text words), ignoring the semantic consistency between fine-grained matching of word regions and coarse-grained overall matching of image and text. Additionally, these methods often ignore the semantic differences across different feature dimensions. Such limitations may result in an overemphasis on specific details at the expense of holistic understanding during image-text matching. To tackle this challenge, this article proposes a new Cross-Dimensional Coarse-Fine-Grained Complementary Network (CDGCN). Firstly, the proposed CDGCN performs fine-grained semantic alignment of image regions and sentence words based on cross-dimensional dependencies. Next, a Coarse-Grained Cross-Dimensional Semantic Aggregation module (CGDSA) is developed to complement local alignment with global image-text matching ensuring semantic consistency. This module aggregates local features across different dimensions as well as within the same dimension to form coherent global features, thus preserving the semantic integrity of the information. The proposed CDGCN is evaluated on two multimodal datasets, Flickr30K and MS-COCO against state-of-the-art methods. The proposed CDGCN achieved substantial improvements with performance increment of 7.7–16% for both datasets.},
  archive      = {J_PEERJCS},
  author       = {Meizhen Liu and Anis Salwa Mohd Khairuddin and Khairunnisa Hasikin and Weitong Liu},
  doi          = {10.7717/peerj-cs.2725},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2725},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Novel cross-dimensional coarse-fine-grained complementary network for image-text matching},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tibyan corpus: Balanced and comprehensive error coverage corpus using ChatGPT for arabic grammatical error correction. <em>PEERJCS</em>, <em>11</em>, e2724. (<a href='https://doi.org/10.7717/peerj-cs.2724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP) augments text data to overcome sample size constraints. Scarce and low-quality data present particular challenges when learning from these domains. Increasing the sample size is a natural and widely used strategy for alleviating these challenges. Moreover, data-augmentation techniques are commonly used in languages with rich data resources to address problems such as exposure bias. In this study, we chose Arabic to increase the sample size and correct grammatical errors. Arabic is considered one of the languages with limited resources for grammatical error correction (GEC) despite being one of the most popular among Arabs and non-Arabs because of its close connection to Islam. Therefore, this study aims to develop an Arabic corpus called “Tibyan” for grammatical error correction using ChatGPT. ChatGPT is used as a data augmenter tool based on a pair of Arabic sentences containing grammatical errors matched with a sentence free of errors extracted from Arabic books, called guide sentences. Multiple steps were involved in establishing our corpus, including collecting and pre-processing a pair of Arabic texts from various sources, such as books and open-access corpora. We then used ChatGPT to generate a parallel corpus based on the text collected previously, as a guide for generating sentences with multiple types of errors. By engaging linguistic experts to review and validate the automatically generated sentences, we ensured they were correct and error-free. The corpus was validated and refined iteratively based on feedback provided by linguistic experts to improve its accuracy. Finally, we used the Arabic Error Type Annotation tool (ARETA) to analyze the types of errors in the Tibyan corpus. Our corpus contained 49% of errors, including seven types: orthography, morphology, syntax, semantics, punctuation, merge, and split. The Tibyan corpus contains approximately 600 K tokens.},
  archive      = {J_PEERJCS},
  author       = {Ahlam Alrehili and Areej Alhothali},
  doi          = {10.7717/peerj-cs.2724},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2724},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Tibyan corpus: Balanced and comprehensive error coverage corpus using ChatGPT for arabic grammatical error correction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic cassava disease recognition using object segmentation and progressive learning. <em>PEERJCS</em>, <em>11</em>, e2721. (<a href='https://doi.org/10.7717/peerj-cs.2721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cassava is a vital crop for millions of farmers worldwide, but its cultivation is threatened by various destructive diseases. Current detection methods for cassava diseases are costly, time-consuming, and often limited to controlled environments, making them unsuitable for large-scale agricultural use. This study aims to develop a deep learning framework that enables early, accurate, and efficient detection of cassava diseases in real-world conditions. We propose a self-supervised object segmentation technique, combined with a progressive learning algorithm (PLA) that incorporates both triplet loss and classification loss to learn robust feature embeddings. Our approach achieves superior performance on the Cassava Leaf Disease Classification (CLDC) dataset from the Kaggle competition, with an accuracy of 91.43%, outperforming all other participants. The proposed method offers a practical and efficient solution for cassava disease detection, demonstrating the potential for large-scale, real-world application in agriculture.},
  archive      = {J_PEERJCS},
  author       = {Chang Che and Nian Xue and Zhen Li and Yilin Zhao and Xin Huang},
  doi          = {10.7717/peerj-cs.2721},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2721},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automatic cassava disease recognition using object segmentation and progressive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepSpoofNet: A framework for securing UAVs against GPS spoofing attacks. <em>PEERJCS</em>, <em>11</em>, e2714. (<a href='https://doi.org/10.7717/peerj-cs.2714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed Aerial Vehicles (UAVs) are frequently utilized in several domains such as transportation, distribution, monitoring, and aviation. A significant security vulnerability is the Global Positioning System (GPS) Spoofing attack, wherein the assailant deceives the GPS receiver by transmitting counterfeit signals, thereby gaining control of the UAV. This can result in the UAV being captured or, in certain instances, destroyed. Numerous strategies have been presented to identify counterfeit GPS signals. Although there have been notable advancements in machine learning (ML) for detecting GPS spoofing attacks, there are still challenges and limitations in the current state-of-the-art research. These include imbalanced datasets, sub-optimal feature selection, and the accuracy of attack detection in resource-constrained environments. The proposed framework investigates the optimal pairing of feature selection (FS) methodologies and deep learning techniques for detecting GPS spoofing attacks on UAVs. The primary objective of this study is to address the challenges associated with detecting GPS spoofing attempts in UAVs. The study focuses on tackling the issue of imbalanced datasets by implementing rigorous oversampling techniques. To do this, a comprehensive approach is proposed that combines advanced feature selection techniques with powerful neural network (NN) architectures. The selected attributes from this process are then transmitted to the succeeding tiers of a hybrid NN, which integrates convolutional neural network (CNN) and bidirectional long short-term memory (BiLSTM) components. The Analysis of Variance (ANOVA) + CNN-BiLSTM hybrid model demonstrates superior performance, producing exceptional results with a precision of 98.84%, accuracy of 99.25%, F1 score of 99.26%, and recall of 99.69%. The proposed hybrid model for detecting GPS spoofing attacks exhibits significant improvements in terms of prediction accuracy, true positive and false positive rates, as well as F1 score and recall values.},
  archive      = {J_PEERJCS},
  author       = {Aziz Ur Rehman Badar and Danish Mahmood and Adeel Iqbal and Sung Won Kim and Sedat Akleylek and Korhan Cengiz and Ali Nauman},
  doi          = {10.7717/peerj-cs.2714},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2714},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DeepSpoofNet: A framework for securing UAVs against GPS spoofing attacks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal fusion transformer-based strategy for efficient multi-cloud content replication. <em>PEERJCS</em>, <em>11</em>, e2713. (<a href='https://doi.org/10.7717/peerj-cs.2713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing, ensuring the high availability and reliability of data is dominant for efficient content delivery. Content replication across multiple clouds has emerged as a solution to achieve the above. However, managing optimal replication while considering dynamic changes in data popularity and cloud resource availability remains a formidable challenge. In order to address these challenges, this article employs TFT-based Dynamic Data Replication Strategy (TD2RS), leveraging the Temporal Fusion Transformer (TFT), a deep learning temporal forecasting model. This proposed system collects historical data on content popularity and resource availability from multiple cloud sources, which are then used as input to TFT. Then TFT is used to capture temporal patterns and forecasts future data demands. An intelligent replication is performed to optimize content replication across multiple cloud environments based on these forecasts. The framework’s performance was validated through extensive experiments using synthetic time-series data simulating with varied cloud resource characteristics. Some of the findings include that the proposed TFT approach improves the availability of data by 20% when compared to traditional replication techniques and also cuts down the latency level by 15%. These outcomes indicate that the TFT-based replication strategy targets to improve content delivery efficiency in the dynamic cloud computing environment, thus providing effective solution to dynamically address the availability, reliability, and performance challenges.},
  archive      = {J_PEERJCS},
  author       = {Naganandhini S. and Shanthi D.},
  doi          = {10.7717/peerj-cs.2713},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2713},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Temporal fusion transformer-based strategy for efficient multi-cloud content replication},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative artificial intelligence and machine learning methods to screen social media content. <em>PEERJCS</em>, <em>11</em>, e2710. (<a href='https://doi.org/10.7717/peerj-cs.2710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Social media research is confronted by the expansive and constantly evolving nature of social media data. Hashtags and keywords are frequently used to identify content related to a specific topic, but these search strategies often result in large numbers of irrelevant results. Therefore, methods are needed to quickly screen social media content based on a specific research question. The primary objective of this article is to present generative artificial intelligence (AI; e.g., ChatGPT) and machine learning methods to screen content from social media platforms. As a proof of concept, we apply these methods to identify TikTok content related to e-cigarette use during pregnancy. Methods We searched TikTok for pregnancy and vaping content using 70 hashtag pairs related to “pregnancy” and “vaping” (e.g., #pregnancytok and #ecigarette) to obtain 11,673 distinct posts. We extracted post videos, descriptions, and metadata using Zeeschuimer and PykTok library. To enhance textual analysis, we employed automatic speech recognition via the Whisper system to transcribe verbal content from each video. Next, we used the OpenCV library to extract frames from the videos, followed by object and text detection analysis using Oracle Cloud Vision. Finally, we merged all text data to create a consolidated dataset and entered this dataset into ChatGPT-4 to determine which posts are related to vaping and pregnancy. To refine the ChatGPT prompt used to screen for content, a human coder cross-checked ChatGPT-4’s outputs for 10 out of every 100 metadata entries, with errors used to inform the final prompt. The final prompt was evaluated through human review, confirming for posts that contain “pregnancy” and “vape” content, comparing determinations to those made by ChatGPT. Results Our results indicated ChatGPT-4 classified 44.86% of the videos as exclusively related to pregnancy, 36.91% to vaping, and 8.91% as containing both topics. A human reviewer confirmed for vaping and pregnancy content in 45.38% of the TikTok posts identified by ChatGPT as containing relevant content. Human review of 10% of the posts screened out by ChatGPT identified a 99.06% agreement rate for excluded posts. Conclusions ChatGPT has mixed capacity to screen social media content that has been converted into text data using machine learning techniques such as object detection. ChatGPT’s sensitivity was found to be lower than a human coder in the current case example but has demonstrated power for screening out irrelevant content and can be used as an initial pass at screening content. Future studies should explore ways to enhance ChatGPT’s sensitivity.},
  archive      = {J_PEERJCS},
  author       = {Kellen Sharp and Rachel R. Ouellette and Rujula Singh Rajendra Singh and Elise E. DeVito and Neil Kamdar and Amanda de la Noval and Dhiraj Murthy and Grace Kong},
  doi          = {10.7717/peerj-cs.2710},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2710},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Generative artificial intelligence and machine learning methods to screen social media content},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative inference strategy for medical image diagnosis in mobile edge computing environment. <em>PEERJCS</em>, <em>11</em>, e2708. (<a href='https://doi.org/10.7717/peerj-cs.2708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity and convenience of mobile medical image analysis and diagnosis in mobile edge computing (MEC) environments have greatly improved the efficiency and quality of healthcare services, necessitating the use of deep neural networks (DNNs) for image analysis. However, DNNs face performance and energy constraints when operating on the mobile side, and are limited by communication costs and privacy issues when operating on the edge side, and previous edge-end collaborative approaches have shown unstable performance and low search efficiency when exploring classification strategies. To address these issues, we propose a DNN edge-optimized collaborative inference strategy (MOCI) for medical image diagnosis, which optimizes data transfer and computation allocation by combining compression techniques and multi-agent reinforcement learning (MARL) methods. The MOCI strategy first uses coding and quantization-based compression methods to reduce the redundancy of image data during transmission at the edge, and then dynamically segments the DNN model through MARL and executes it collaboratively between the edge and the mobile device. To improve policy stability and adaptability, MOCI introduces the optimal transmission distance (Wasserstein) to optimize the policy update process, and uses the long short-term memory (LSTM) network to improve the model’s adaptability to dynamic task complexity. The experimental results show that the MOCI strategy can effectively solve the collaborative inference task of medical image diagnosis and significantly reduce the latency and energy consumption with less than a 2% loss in classification accuracy, with a maximum reduction of 38.5% in processing latency and 71% in energy consumption compared to other inference strategies. In real-world MEC scenarios, MOCI has a wide range of potential applications that can effectively promote the development and application of intelligent healthcare.},
  archive      = {J_PEERJCS},
  author       = {Shiqian Zhang and Yong Cui and Dandan Xu and Yusong Lin},
  doi          = {10.7717/peerj-cs.2708},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2708},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A collaborative inference strategy for medical image diagnosis in mobile edge computing environment},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced mutation strategy based differential evolution for global optimization problems. <em>PEERJCS</em>, <em>11</em>, e2696. (<a href='https://doi.org/10.7717/peerj-cs.2696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential evolution (DE) stands out as a prominent algorithm for addressing global optimization challenges. The efficacy of DE hinges crucially upon its mutation operation, which serves as a pivotal mechanism in generating diverse and high-quality solutions. This article explores various mutation operations aimed at augmenting the performance of DE in global optimization tasks. A distinct mutation strategy is introduced, with the primary objective of achieving a harmonious equilibrium between exploration and exploitation to enhance both convergence speed and solution quality. The proposed DE centres on a novel mutation-based strategy, introducing a new coefficient factor (“σ”) in conjunction with the base vector of the basic mutation strategy (“DE/rand/1”). This innovation aims to fortify the convergence of local variables during exploitation, thereby improving both the convergence rate and quality. The effectiveness of the proposed mutation operations is evaluated across a set of 27 benchmark functions commonly employed in global optimization. Experimental results conclusively demonstrate that these enhanced mutation strategies significantly outperform state-of-the-art algorithms in terms of solution accuracy and convergence speed. This study underscores the critical role of mutation operations in DE and provides valuable insights for designing more potent mutation strategies to tackle complex global optimization problems.},
  archive      = {J_PEERJCS},
  author       = {Pawan Mishra and Musrrat Ali and Pooja and Safiqul Islam},
  doi          = {10.7717/peerj-cs.2696},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2696},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced mutation strategy based differential evolution for global optimization problems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven balance evaluation: A comparative study between blind and non-blind individuals using the mini-BESTest. <em>PEERJCS</em>, <em>11</em>, e2695. (<a href='https://doi.org/10.7717/peerj-cs.2695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are 2.2 billion visually impaired individuals and 285 million blind people worldwide. The vestibular system plays a fundamental role in the balance of a person related to sight and hearing, and thus blind people require physical therapy to improve their balance. Several clinical tests have been developed to evaluate balance, such as the mini-BESTest. This test has been used to evaluate the balance of people with neurological diseases, but there have been no studies that evaluate the balance of blind individuals before. Furthermore, despite the scoring of these tests being not subjective, the performance of some activities are subject to the physiotherapist’s bias. Tele-rehabilitation is a growing field that aims to provide physical therapy to people with disabilities. Among the technologies used in tele-rehabilitation are inertial measurement units that can be used to monitor the balance of individuals. The amount of data collected by these devices is large and the use of deep learning models can help in analyzing these data. Therefore, the objective of this study is to analyze for the first time the balance of blind individuals using the mini-BESTest and inertial measurement units and to identify the activities that best differentiate between blind and sighted individuals. We use the OpenSense RT monitoring device to collect data from the inertial measurement unit, and we develop machine learning and deep learning models to predict the score of the most relevant mini-BESTest activities. In this study 29 blind and sighted individuals participated. The one-legged stance is the activity that best differentiates between blind and sighted individuals. An analysis on the acceleration data suggests that the evaluation of physiotherapists is not completely adjusted to the test criterion. Cluster analysis suggests that inertial data are not able to distinguish between three levels of evaluation. However, the performance of our models shows an F1-score of 85.6% in predicting the score evaluated by the mini-BESTest in a binary classification problem. The results of this study can help physiotherapists have a more objective evaluation of the balance of their patients and to develop tele-rehabilitation systems for blind individuals.},
  archive      = {J_PEERJCS},
  author       = {Milagros Jaén-Vargas and Josué Pagán and Shiyang Li and María Fernanda Trujillo-Guerrero and Niloufar Kazemi and Alessio Sansò and Benito Codina-Casals and Roy Abi Zeid Daou and Jose Javier Serrano Olmedo},
  doi          = {10.7717/peerj-cs.2695},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2695},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {AI-driven balance evaluation: A comparative study between blind and non-blind individuals using the mini-BESTest},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing diversity, negativity, and stereotypes in chinese-language AI technologies: An investigation of baidu, ernie and qwen. <em>PEERJCS</em>, <em>11</em>, e2694. (<a href='https://doi.org/10.7717/peerj-cs.2694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) and search engines have the potential to perpetuate biases and stereotypes by amplifying existing prejudices in their training data and algorithmic processes, thereby influencing public perception and decision-making. While most work has focused on Western-centric AI technologies, we examine social biases embedded in prominent Chinese-based commercial tools, the main search engine Baidu and two leading LLMs, Ernie and Qwen. Leveraging a dataset of 240 social groups across 13 categories describing Chinese society, we collect over 30 k views encoded in the aforementioned tools by prompting them to generate candidate words describing these groups. We find that language models exhibit a broader range of embedded views compared to the search engine, although Baidu and Qwen generate negative content more often than Ernie. We also observe a moderate prevalence of stereotypes embedded in the language models, many of which potentially promote offensive or derogatory views. Our work highlights the importance of prioritizing fairness and inclusivity in AI technologies from a global perspective.},
  archive      = {J_PEERJCS},
  author       = {Geng Liu and Carlo Alberto Bono and Francesco Pierri},
  doi          = {10.7717/peerj-cs.2694},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2694},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparing diversity, negativity, and stereotypes in chinese-language AI technologies: An investigation of baidu, ernie and qwen},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fake news detection: State-of-the-art review and advances with attention to arabic language aspects. <em>PEERJCS</em>, <em>11</em>, e2693. (<a href='https://doi.org/10.7717/peerj-cs.2693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of fake news has become a significant threat, influencing individuals, institutions, and societies at large. This issue has been exacerbated by the pervasive integration of social media into daily life, directly shaping opinions, trends, and even the economies of nations. Social media platforms have struggled to mitigate the effects of fake news, relying primarily on traditional methods based on human expertise and knowledge. Consequently, machine learning (ML) and deep learning (DL) techniques now play a critical role in distinguishing fake news, necessitating their extensive deployment to counter the rapid spread of misinformation across all languages, particularly Arabic. Detecting fake news in Arabic presents unique challenges, including complex grammar, diverse dialects, and the scarcity of annotated datasets, along with a lack of research in the field of fake news detection compared to English. This study provides a comprehensive review of fake news, examining its types, domains, characteristics, life cycle, and detection approaches. It further explores recent advancements in research leveraging ML, DL, and transformer-based techniques for fake news detection, with a special attention to Arabic. The research delves into Arabic-specific pre-processing techniques, methodologies tailored for fake news detection in the language, and the datasets employed in these studies. Additionally, it outlines future research directions aimed at developing more effective and robust strategies to address the challenge of fake news detection in Arabic content.},
  archive      = {J_PEERJCS},
  author       = {Eman Salamah Albtoush and Keng Hoon Gan and Saif A. Ahmad Alrababa},
  doi          = {10.7717/peerj-cs.2693},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2693},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fake news detection: State-of-the-art review and advances with attention to arabic language aspects},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and analysis of teaching early warning system based on multimodal data in an intelligent learning environment. <em>PEERJCS</em>, <em>11</em>, e2692. (<a href='https://doi.org/10.7717/peerj-cs.2692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online teaching environments, the lack of direct emotional interaction between teachers and students poses challenges for teachers to consciously and effectively manage their emotional expressions. The design and implementation of an early warning system for teaching provide a novel approach to intelligent evaluation and improvement of online education. This study focuses on segmenting different emotional segments and recognizing emotions in instructional videos. An efficient long-video emotional transition point search algorithm is proposed for segmenting video emotional segments. Leveraging the fact that teachers tend to maintain a neutral emotional state for significant portions of their teaching, a neutral emotional segment filtering algorithm based on facial features has been designed. A multimodal emotional recognition model is proposed for emotional recognition in instructional videos. It begins with preprocessing the raw speech and facial image features, employing a semi-supervised iterative feature normalization algorithm to eliminate individual teacher differences while preserving inherent differences between different emotions. A deep learning-based multimodal emotional recognition model for teacher instructional videos is introduced, incorporating an attention mechanism to automatically assign weights for feature-level modal fusion, providing users with accurate emotional classification. Finally, a teaching early warning system is implemented based on these algorithms.},
  archive      = {J_PEERJCS},
  author       = {Xinxin Kang and Yong Nie},
  doi          = {10.7717/peerj-cs.2692},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2692},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design and analysis of teaching early warning system based on multimodal data in an intelligent learning environment},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social big data management through collaborative mobile, regional, and cloud computing. <em>PEERJCS</em>, <em>11</em>, e2689. (<a href='https://doi.org/10.7717/peerj-cs.2689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crowd of smart devices surrounds us all the time. These devices popularize social media platforms (SMP), connecting billions of users. The enhanced functionalities of smart devices generate big data that overutilizes the mainstream network, degrading performance and increasing the overall cost, compromising time-sensitive services. Research indicates that about 75% of connections come from local areas, and their workload does not need to be migrated to remote servers in real-time. Collaboration among mobile edge computing (MEC), regional computing (RC), and cloud computing (CC) can effectively fill these gaps. Therefore, we propose a collaborative structure of mobile, regional, and cloud computing to address the issues arising from social big data (SBD). In this model, it may be easily accessed from the nearest device or server rather than downloading a file from the cloud server. Furthermore, instead of transferring each file to the cloud servers during peak hours, they are initially stored on a regional level and subsequently uploaded to the cloud servers during off-peak hours. The outcomes affirm that this approach significantly reduces the impact of substantial SBD on the performance of mainstream and social network platforms, specifically in terms of delay, response time, and cost.},
  archive      = {J_PEERJCS},
  author       = {Afzal Badshah and Ameen Banjar and Safa Habibullah and Abdullah Alharbi and Wael Alosaimi and Ali Daud},
  doi          = {10.7717/peerj-cs.2689},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2689},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Social big data management through collaborative mobile, regional, and cloud computing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data augmentation for arabic text classification: A review of current methods, challenges and prospective directions. <em>PEERJCS</em>, <em>11</em>, e2685. (<a href='https://doi.org/10.7717/peerj-cs.2685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effectiveness of data augmentation techniques, i.e., methods for artificially creating new data, has been demonstrated in many domains, from images to textual data. Data augmentation methods were established to manage different issues regarding the scarcity of training datasets or the class imbalance to enhance the performance of classifiers. This review article investigates data augmentation techniques for Arabic texts, specifically in the text classification field. A thorough review was conducted to give a concise and comprehensive understanding of these approaches in the context of Arabic classification. The focus of this article is on Arabic studies published from 2019 to 2024 about data augmentation in Arabic text classification. Inclusion and exclusion criteria were applied to ensure a comprehensive vision of these techniques in Arabic natural language processing (ANLP). It was found that data augmentation research for Arabic text classification dominates sentiment analysis and propaganda detection, with initial studies emerging in 2019; very few studies have investigated other domains like sarcasm detection or text categorization. We also observed the lack of benchmark datasets for performing the tasks. Most studies have focused on short texts, such as Twitter data or reviews, while research on long texts still needs to be explored. Additionally, various data augmentation methods still need to be examined for long texts to determine if techniques effective for short texts are also applicable to longer texts. A rigorous investigation and comparison of the most effective strategies is required due to the unique characteristics of the Arabic language. By doing so, we can better understand the processes involved in Arabic text classification and hence be able to select the most suitable data augmentation methods for specific tasks. This review contributes valuable insights into Arabic NLP and enriches the existing body of knowledge.},
  archive      = {J_PEERJCS},
  author       = {Samia F. Abdhood and Nazlia Omar and Sabrina Tiun},
  doi          = {10.7717/peerj-cs.2685},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2685},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data augmentation for arabic text classification: A review of current methods, challenges and prospective directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization study of intelligent accounting manager system modules in adaptive behavioral pattern learning and simulation. <em>PEERJCS</em>, <em>11</em>, e2684. (<a href='https://doi.org/10.7717/peerj-cs.2684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the ambit of the digital epoch, the advent of adaptive learning technologies heralds a paradigmatic shift in the realm of accounting management, garnering increasing scrutiny for augmenting learning outcomes via more sagacious educational methodologies and refining the accounting management protocols through the employment of sophisticated optimization techniques. This manuscript delineates an avant-garde health classification schema for accounting management, termed the A-CHMM-FD methodology, which amalgamates the merits of the Analytic Hierarchy Process (AHP) with the Coupled Hidden Markov Model (CHMM) to enhance the precision and efficacy of risk detection. Utilizing the AHP modality, we quantify diverse accounting metrics, subsequently subjected to independent scrutiny via the CHMM. This results in an exhaustive evaluation of entities as healthy, at-risk, or high-risk employing fuzzy delineations. Empirical validation on publicly available financial risk datasets and the pragmatic deployment of bespoke datasets affirm the superior efficiency and precision of the proposed framework. Applying this methodology within the health classification of accounting management emerges as efficacious, charting a novel technological trajectory for managing accounting risks and offering fresh perspectives on the nurturing of accounting understanding and the acquisition of knowledge.},
  archive      = {J_PEERJCS},
  author       = {Yifan Wang and Rongjie Qin and Musadaq Mansoor},
  doi          = {10.7717/peerj-cs.2684},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2684},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimization study of intelligent accounting manager system modules in adaptive behavioral pattern learning and simulation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial expression recognition using visible and IR by early fusion of deep learning with attention mechanism. <em>PEERJCS</em>, <em>11</em>, e2676. (<a href='https://doi.org/10.7717/peerj-cs.2676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) has garnered significant attention due to advances in artificial intelligence, particularly in applications like driver monitoring, healthcare, and human-computer interaction, which benefit from deep learning techniques. The motivation of this research is to address the challenges of accurately recognizing emotions despite variations in expressions across emotions and similarities between different expressions. In this work, we propose an early fusion approach that combines features from visible and infrared modalities using publicly accessible VIRI and NVIE databases. Initially, we developed single-modality models for visible and infrared datasets by incorporating an attention mechanism into the ResNet-18 architecture. We then extended this to a multi-modal early fusion approach using the same modified ResNet-18 with attention, achieving superior accuracy through the combination of convolutional neural network (CNN) and transfer learning (TL). Our multi-modal approach attained 84.44% accuracy on the VIRI database and 85.20% on the natural visible and infrared facial expression (NVIE) database, outperforming previous methods. These results demonstrate that our single-modal and multi-modal approaches achieve state-of-the-art performance in FER.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Tahir Naseem and Chan-Su Lee and Tariq Shahzad and Muhammad Adnan Khan and Adnan M. Abu-Mahfouz and Khmaies Ouahada},
  doi          = {10.7717/peerj-cs.2676},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2676},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Facial expression recognition using visible and IR by early fusion of deep learning with attention mechanism},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of a cryptocurrency price prediction model: Leveraging GRU and LSTM for bitcoin, litecoin and ethereum. <em>PEERJCS</em>, <em>11</em>, e2675. (<a href='https://doi.org/10.7717/peerj-cs.2675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrency represents a form of asset that has arisen from the progress of financial technology, presenting significant prospects for scholarly investigations. The ability to anticipate cryptocurrency prices with extreme accuracy is very desirable to researchers and investors. However, time-series data presents significant challenges due to the nonlinear nature of the cryptocurrency market, complicating precise price predictions. Several studies have explored cryptocurrency price prediction using various deep learning (DL) algorithms. Three leading cryptocurrencies, determined by market capitalization, Ethereum (ETH), Bitcoin (BTC), and Litecoin (LTC), are examined for exchange rate predictions in this study. Two categories of recurrent neural networks (RNNs), specifically long short-term memory (LSTM) and gated recurrent unit (GRU), are employed. Four performance metrics are selected to evaluate the prediction accuracy namely mean squared error (MSE), mean absolute error (MAE), mean absolute percentage error (MAPE), and root mean squared error (RMSE) for three cryptocurrencies which demonstrates that GRU model outperforms LSTM. The GRU model was implemented as a two-layer deep learning network, optimized using the Adam optimizer with a dropout rate of 0.2 to prevent overfitting. The model was trained using normalized historical price data sourced from CryptoDataDownload, with an 80:20 train-test split. In this work, GRU qualifies as the best algorithm for developing a cryptocurrency price prediction model. MAPE values for BTC, LTC and ETH are 0.03540, 0.08703 and 0.04415, respectively, which indicate that GRU offers the most accurate forecasts as compared to LSTM. These prediction models are valuable for traders and investors, offering accurate cryptocurrency price predictions. Future studies should also consider additional variables, such as social media trends and trade volumes that may impact cryptocurrency pricing.},
  archive      = {J_PEERJCS},
  author       = {Ramneet Kaur and Mudita Uppal and Deepali Gupta and Sapna Juneja and Syed Yasser Arafat and Junaid Rashid and Jungeun Kim and Roobaea Alroobaea},
  doi          = {10.7717/peerj-cs.2675},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2675},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Development of a cryptocurrency price prediction model: Leveraging GRU and LSTM for bitcoin, litecoin and ethereum},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced transformer for length-controlled abstractive summarization based on summary output area. <em>PEERJCS</em>, <em>11</em>, e2667. (<a href='https://doi.org/10.7717/peerj-cs.2667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in abstractive summarization models, particularly those built on encoder-decoder architectures, typically produce a single summary for each source text. Controlling the length of summaries is crucial for practical applications, such as crafting cover summaries for newspapers or magazines with varying slot sizes. Current research in length-controllable abstractive summarization employs techniques like length embeddings in the decoder module or a word-level extractive module in the encoder-decoder model. However, these approaches, while effective in determining when to halt decoding, fall short in selecting relevant information to include within the specified length constraint. This article diverges from prior models reliant on predefined lengths. Instead, it introduces a novel approach to length-controllable abstractive summarization by integrating an image processing phase. This phase determines the specific size of the summary output slot. The proposed model harnesses enhanced T5 and GPT models, seamlessly adapting summaries to designated slots. The computed area of a given slot is employed in both models to generate abstractive summaries tailored to fit the output slot perfectly. Experimental evaluations on the CNN/Daily Mail dataset demonstrate the model’s success in performing length-controlled summarization, yielding superior results.},
  archive      = {J_PEERJCS},
  author       = {Yusuf Sunusi and Nazlia Omar and Lailatul Qadri Zakaria},
  doi          = {10.7717/peerj-cs.2667},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2667},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced transformer for length-controlled abstractive summarization based on summary output area},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic approaches for query expansion: Taxonomy, challenges, and future research directions. <em>PEERJCS</em>, <em>11</em>, e2664. (<a href='https://doi.org/10.7717/peerj-cs.2664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The internet has been inundated with an ocean of information, and hence, information retrieval systems are failing to provide optimal results to the user. In order to meet the challenge, query expansion techniques have emerged as a game-changer and are improving the results of information retrieval significantly. Of late, semantic query expansion techniques have attracted increased interest among researchers since these techniques offer more pertinent and practical results to the users. These allow the user to retrieve more meaningful and useful information from the web. Currently, few research works provide a comprehensive review on semantic query expansion; usually, they cannot provide a full view on recent advances, diversified data application, and practical challenges. Therefore, it is imperative to go deep in review in order to explain these advances and assist researchers with concrete insights for future development. This article represents the comprehensive review of the query expansion methods, with a particular emphasis on semantic approaches. It overviews the recent frameworks that have been developed within a period of 2015–2024 and reviews the limitations of each approach. Further, it discusses challenges that are inherent in the semantic query expansion field and identifies some future research directions. This article emphasizes that the linguistic approach is the most effective and flexible direction for researchers to follow, while the ontology approach better suits domain-specific search applications. This, in turn, means that development of the ontology field may further open new perspectives for semantic query expansion. Moreover, by employing artificial intelligence (AI) and making most of the query context without relying on user intervention, improvements toward the optimal expanded query can be achieved.},
  archive      = {J_PEERJCS},
  author       = {Azzah Allahim and Asma Cherif and Abdessamad Imine},
  doi          = {10.7717/peerj-cs.2664},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2664},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Semantic approaches for query expansion: Taxonomy, challenges, and future research directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy inference rule based task offloading model (FI-RBTOM) for edge computing. <em>PEERJCS</em>, <em>11</em>, e2657. (<a href='https://doi.org/10.7717/peerj-cs.2657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key objective of edge computing is to reduce delays and provide consumers with high-quality services. However, there are certain challenges, such as high user mobility and the dynamic environments created by IoT devices. Additionally, the limitations of constrained device resources impede effective task completion. The challenge of task offloading plays a crucial role as one of the key challenges for edge computing, which is addressed in this research. An efficient rule-based task-offloading model (FI-RBTOM) is proposed in this context. The key decision of the proposed model is to choose either the task to be offloaded over an edge server or the cloud server or it can be processed over a local node. The four important input parameters are bandwidth, CPU utilization, task length, and task size. The proposed (FI-RBTOM), simulation is carried out using MATLAB (fuzzy logic) tool with 75% training and 25% testing with an overall error rate of 0.39875 is achieved.},
  archive      = {J_PEERJCS},
  author       = {Kashif Ibrahim and Ahthasham Sajid and Ihsan Ullah and Inam Ullah Khan and Keshav Kaushik and S S. Askar and Mohamed Abouhawwash},
  doi          = {10.7717/peerj-cs.2657},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2657},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fuzzy inference rule based task offloading model (FI-RBTOM) for edge computing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A laboratory experiment on using different financial-incentivization schemes in software-engineering experimentation. <em>PEERJCS</em>, <em>11</em>, e2650. (<a href='https://doi.org/10.7717/peerj-cs.2650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In software-engineering research, many empirical studies are conducted with open-source or industry developers. However, in contrast to other research communities like economics or psychology, only few experiments use financial incentives (i.e., paying money) as a strategy to motivate participants’ behavior and reward their performance. The most recent version of the SIGSOFT Empirical Standards mentions payouts only for increasing participation in surveys, but not for mimicking real-world motivations and behavior in experiments. Within this article, we report a controlled experiment in which we tackled this gap by studying how different financial incentivization schemes impact developers. For this purpose, we first conducted a survey on financial incentives used in the real-world, based on which we designed three incentivization schemes: (1) a performance-dependent scheme that employees prefer, (2) a scheme that is performance-independent, and (3) a scheme that mimics open-source development. Then, using a between-subject experimental design, we explored how these three schemes impact participants’ performance. Our findings indicate that the different schemes can impact participants’ performance in software-engineering experiments. Our results are not statistically significant, possibly due to small sample sizes and the consequent lack of statistical power, but with some notable trends that may inspire future hypothesis generation. Our contributions help understand the impact of financial incentives on participants in experiments as well as real-world scenarios, guiding researchers in designing experiments and organizations in compensating developers.},
  archive      = {J_PEERJCS},
  author       = {Dmitri Bershadskyy and Jacob Krüger and Gül Calıklı and Siegmar Otto and Sarah Zabel and Jannik Greif and Robert Heyer},
  doi          = {10.7717/peerj-cs.2650},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2650},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A laboratory experiment on using different financial-incentivization schemes in software-engineering experimentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-based ensemble model for dialectal arabic sentiment classification. <em>PEERJCS</em>, <em>11</em>, e2644. (<a href='https://doi.org/10.7717/peerj-cs.2644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms such as X, Facebook, and Instagram have become essential avenues for individuals to articulate their opinions, especially during global emergencies. These platforms offer valuable insights that necessitate analysis for informed decision-making and a deeper understanding of societal trends. Sentiment analysis is crucial for assessing public sentiment toward specific issues; however, applying it to dialectal Arabic presents considerable challenges in natural language processing. The complexity arises from the language’s intricate semantic and morphological structures, along with the existence of multiple dialects. This form of analysis, also referred to as sentiment classification, opinion mining, emotion mining, and review mining, is the focus of this study, which analyzes tweets from three benchmark datasets: the Arabic Sentiment Tweets Dataset (ASTD), the A Twitter-based Benchmark Arabic Sentiment Analysis Dataset (ASAD), and the Tweets Emoji Arabic Dataset (TEAD). The research involves experimentation with a variety of comparative models, including machine learning, deep learning, transformer-based models, and a transformer-based ensemble model. Feature extraction for both machine learning and deep learning approaches is performed using techniques such as AraVec, FastText, AraBERT, and Term Frequency-Inverse Document Frequency (TF-IDF). The study compares machine learning models such as support vector machine (SVM), naïve Bayes (NB), decision tree (DT), and extreme gradient boosting (XGBoost) with deep learning models such as convolutional neural networks (CNN) and bidirectional long short-term memory (BLSTM) networks. Additionally, it explores transformer-based models such as CAMeLBERT, XLM-RoBERTa, and MARBERT, along with their ensemble configurations. The findings demonstrate that the proposed transformer-based ensemble model achieved superior performance, with average accuracy, recall, precision, and F1-score of 90.4%, 88%, 87.3%, and 87.7%, respectively.},
  archive      = {J_PEERJCS},
  author       = {Omar Mansour and Eman Aboelela and Remon Talaat and Mahmoud Bustami},
  doi          = {10.7717/peerj-cs.2644},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2644},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Transformer-based ensemble model for dialectal arabic sentiment classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of wheat fusarium head blight severity levels in southern henan based on K-means-SMOTE and XGBoost algorithms. <em>PEERJCS</em>, <em>11</em>, e2638. (<a href='https://doi.org/10.7717/peerj-cs.2638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusarium head blight (FHB) is a destructive disease which adversely affects the yield of wheat. The occurrence and epidemic of wheat FHB are closely related to meteorological information. Firstly, by analyzing eight meteorological factors—rainfall (RAIN), average sunshine hours (ASH), average wind speed (AWS), average temperature (AT), highest temperature (HT), lowest temperature (LT), average relative humidity (ARH), and maximum temperature difference (MTD)—specific periods closely related to wheat FHB severity are identified. Based on this, a dataset for wheat FHB severity is constructed. After that, the wheat FHB severity levels are divided into four levels, and actual field data shows that the proportion of data for the high prevalence severity level is relatively small. To address data imbalance, the K-means-synthetic minority over-sampling technique (K-means-SMOTE) method is introduced to increase samples of underrepresented severity levels. Subsequently, a wheat FHB severity prediction model based on K-means-SMOTE and extreme gradient boosting (XGBoost) is constructed. Lastly, by combining the rankings of meteorological factors provided by the model and the biological characteristics of wheat FHB, the number of meteorological factors is reduced from eight to four (AWS 4.24–4.28, RAIN 4.5–4.19, ARH 4.12–4.16, LT 4.19–4.23), the accuracy and recall of the model remained unchanged at 0.8936, the F1 score increased from 0.8851 to 0.8898, and the precision decreased from 0.9249 to 0.9058. Although the precision has slightly decreased, most of the other evaluation indicators of the model remain unchanged or have improved, therefore the model is considered effective. Finally, comparative experiments with eight other models demonstrate the superiority of this approach.},
  archive      = {J_PEERJCS},
  author       = {Xiaoyun Sun and Shuaiming Su and Qiang Wang and Shufeng Xiong and Yanting Li and Hong Peng and Lei Shi},
  doi          = {10.7717/peerj-cs.2638},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2638},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Prediction of wheat fusarium head blight severity levels in southern henan based on K-means-SMOTE and XGBoost algorithms},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive detection of anomalous behavior in ethereum accounts using XAI-enabled ensemble stacking with bayesian optimization. <em>PEERJCS</em>, <em>11</em>, e2630. (<a href='https://doi.org/10.7717/peerj-cs.2630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decentralized, open-source architecture of blockchain technology, exemplified by the Ethereum platform, has transformed online transactions by enabling secure and transparent exchanges. However, this architecture also exposes the network to various security threats that cyber attackers can exploit. Detecting suspicious behaviors in account on the Ethereum blockchain can help mitigate attacks, including phishing, Ponzi schemes, eclipse attacks, Sybil attacks, and distributed denial of service (DDoS) incidents. The proposed system introduces an ensemble stacking model combining Random Forest (RF), eXtreme Gradient Boosting (XGBoost), and a neural network (NN) to detect potential threats within the Ethereum platform. The ensemble model is fine-tuned using Bayesian optimization to enhance predictive accuracy, while explainable artificial intelligence (XAI) tools—SHAP, LIME, and ELI5—provide interpretable feature insights, improving transparency in model predictions. The dataset used comprises 9,841 Ethereum transactions across 52 initial fields (reduced to 17 relevant features), encompassing both legitimate and fraudulent records. The experimental findings demonstrate that the proposed model achieves a superior accuracy of 99.6%, outperforming that of other cutting-edge methods. These findings demonstrate that the XAI-enabled ensemble stacking model offers a highly effective, interpretable solution for blockchain security, strengthening trust and reliability within the Ethereum ecosystem.},
  archive      = {J_PEERJCS},
  author       = {Vasavi Chithanuru and Mangayarkarasi Ramaiah},
  doi          = {10.7717/peerj-cs.2630},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2630},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Proactive detection of anomalous behavior in ethereum accounts using XAI-enabled ensemble stacking with bayesian optimization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing forensic file classification: Enhancing SFCS with βk hyperparameter tuning. <em>PEERJCS</em>, <em>11</em>, e2608. (<a href='https://doi.org/10.7717/peerj-cs.2608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In forensic topical modelling, the α parameter controls the distribution of topics in documents. However, low, high, or incorrect values of α lead to topic sparsity, model overfitting, and suboptimal topic distribution. To control the word distribution across topics, the β parameter is introduced. However, low, high, or inappropriate β values lead to sparse distribution, disjointed topics, and abundant highly probable words. The βj parameter, in conjunction with seed-guided words based on Term Frequency and Inverse Document Frequency, is introduced to address the issues. Nevertheless, the data often suffers from skewness or noise due to frequent co-occurrences of unrelated polysemic word pairs generated using Pointwise Mutual Information. By integrating α, β, and βj into file classification systems, classification models converge to local optima with O(n log n* |V|) time complexity. To combat these challenges, this research proposes the SDOT Forensic Classification System (SFCS) with a functional parameter βk that identifies seed words by evaluating semantic and contextual similarity of word vectors. As a result, the topic distribution (Θd) is compelled to model the curated seed words within the distribution, generating pertinent topics. Incorporating βk into SFCS allowed the proposed model to remove 278 k irrelevant files from the corpus and identify 5.6 k suspicious files by extracting 700 blacklisted keywords. Furthermore, this research implemented hyperparameter optimization and hyperplane maximization, resulting in a file classification accuracy of 94.6%, 94.4% precision and 96.8% recall within O(n log n) complexity.},
  archive      = {J_PEERJCS},
  author       = {D. Paul Joseph and Viswanathan Perumal},
  doi          = {10.7717/peerj-cs.2608},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2608},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimizing forensic file classification: Enhancing SFCS with βk hyperparameter tuning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain technology and its impact on sustainable supply chain management in SMEs. <em>PEERJCS</em>, <em>11</em>, e2466. (<a href='https://doi.org/10.7717/peerj-cs.2466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has had a significant impact on small and medium-sized enterprises (SMEs), leading to disruptions in supply chains, financial losses, and closures. To overcome these challenges, organizations, including those in developing economies like Malaysia, are turning to blockchain technology as a solution to enhance traditional supply chain management frameworks. This study aims to identify the factors that influence the acceptance of blockchain technology among SMEs. By drawing on established adoption theories such as the technology acceptance model (TAM), diffusion of innovation (DOI) theory, and theory of planned behavior (TPB), the researchers developed a research framework. They utilized partial least square structural equation modeling (PLS-SEM) to analyze the causal relationships between different constructs and test their hypotheses. The findings confirmed that the constructs of the technology acceptance model, specifically perceived usefulness, perceived ease of use and attitude were significantly associated with the intention to use blockchain technology. Additionally, the constructs of the diffusion of innovation theory, relative advantage and compatibility, showed significant associations with perceived ease of use, while complexity had a negligible relationship with perceived usefulness and perceived ease of use. The construct of subjective norms from the theory of planned behavior exhibited a significant relationship with perceived usefulness and an insignificant relationship with intention to use. Finally, perceived behavioral control demonstrated a positive relationship with intention to use. The study’s findings provide valuable insights for blockchain developers and organizations aiming to make informed decisions regarding the application of blockchain technology as a process innovation in SMEs.},
  archive      = {J_PEERJCS},
  author       = {Chao Fang and Nazir Ullah and M. Batumalay and Waleed Mugahed Al-Rahmi and Fahad Alblehai},
  doi          = {10.7717/peerj-cs.2466},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2466},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Blockchain technology and its impact on sustainable supply chain management in SMEs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging LLaMA2 for improved document classification in english. <em>PEERJCS</em>, <em>11</em>, e2740. (<a href='https://doi.org/10.7717/peerj-cs.2740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document classification is an important component of natural language processing, with applications that include sentiment analysis, content recommendation, and information retrieval. This article investigates the potential of Large Language Model Meta AI (LLaMA2), a cutting-edge language model, to enhance document classification in English. Our experiments show that LLaMA2 outperforms traditional classification methods, achieving higher precision and recall values on the WOS-5736 dataset. Additionally, we analyze the interpretability of LLaMA2’s classification process to reveal the most pertinent features for categorization and the model’s decision-making. These results emphasize the potential of advanced language models to enhance classification outcomes and provide a more profound comprehension of document structures, thereby contributing to the advancement of natural language processing methodologies.},
  archive      = {J_PEERJCS},
  author       = {Jia Xu},
  doi          = {10.7717/peerj-cs.2740},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2740},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging LLaMA2 for improved document classification in english},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A majority voting framework for reliable sentiment analysis of product reviews. <em>PEERJCS</em>, <em>11</em>, e2738. (<a href='https://doi.org/10.7717/peerj-cs.2738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a tailored majority voting approach for enhancing the consistency and reliability of sentiment analysis in online product reviews. The methodology addresses discrepancies in sentiment classification by leveraging sentiment labels from multiple automated tools and implementing a robust majority decision rule. This consensus-based approach significantly enhances the trustworthiness and consistency of sentiment analysis outcomes, serving as a dependable foundation for training more precise sentiment analysis models. The data labeled with our method was utilized to train deep learning models, achieving competitive accuracy with significantly less data. The findings demonstrate the effectiveness of the method in producing results comparable to commercial tools while ensuring data consistency for model training.},
  archive      = {J_PEERJCS},
  author       = {Darie Moldovan},
  doi          = {10.7717/peerj-cs.2738},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2738},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A majority voting framework for reliable sentiment analysis of product reviews},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autoregressive models for session-based recommendations using set expansion. <em>PEERJCS</em>, <em>11</em>, e2734. (<a href='https://doi.org/10.7717/peerj-cs.2734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of internet technologies, session-based recommendation systems have emerged as a key paradigm in delivering personalized recommendations by capturing users’ dynamic and short-term preferences. Traditional methods predominantly rely on modeling the sequential order of user interactions, deep learning approaches like recurrent neural networks and Transformer architectures. However, these sequence-based models often struggle in scenarios where the order of interactions is ambiguous or unreliable, limiting their real-world applicability. To address this challenge, we propose a novel session-based recommendation model, Deep Set Session-based Recommendation (DSETRec), which approaches the problem from a set-based perspective, eliminating dependence on the interaction sequence. By conceptualizing session data as unordered sets, our model captures the coupling relationships and co-occurrence patterns between items, enhancing prediction accuracy in settings where sequential information is either unavailable or noisy. The model is implemented using a deep autoregressive framework that iteratively masks known elements within a session, predicting and reconstructing additional items based on set data characteristics. Extensive experiments on benchmark datasets show that DSETRec achieves outperforms state-of-the-art baselines. DSETRec achieves a 13.2% and 11.85% improvement in P@20 and MRR@20, respectively, over its sequence-based variant on Yoochoose. Additionally, DSETRec generalizes effectively across both further short and long sessions. These results highlight the robustness of the set-based approach in capturing unordered interaction patterns and adapting to diverse session lengths. This finding provides a foundation for developing more flexible and generalized session-based recommendation systems.},
  archive      = {J_PEERJCS},
  author       = {Tianhao Yu and Xianghong Zhou and Xinrong Deng},
  doi          = {10.7717/peerj-cs.2734},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2734},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Autoregressive models for session-based recommendations using set expansion},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive method for determining the optimal number of topics in topic modeling. <em>PEERJCS</em>, <em>11</em>, e2723. (<a href='https://doi.org/10.7717/peerj-cs.2723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic models have been successfully applied to information classification and retrieval. The difficulty in successfully applying these technologies is to select the appropriate number of topics for a given corpus. Selecting too few topics can result in information loss and topic omission, known as underfitting. Conversely, an excess of topics can introduce noise and complexity, resulting in overfitting. Therefore, this article considers the inter-class distance and proposes a new method to determine the number of topics based on clustering results, named average inter-class distance change rate (AICDR). AICDR employs the Ward’s method to calculate inter-class distances, then calculates the average inter-class distance for different numbers of topics, and determines the optimal number of topics based on the average distance change rate. Experiments show that the number of topics determined by AICDR is more in line with the true classification of datasets, with high inter-class distance and low inter-class similarity, avoiding the phenomenon of topic overlap. AICDR is a technique predicated on clustering results to select the optimal number of topics and has strong adaptability to various topic models.},
  archive      = {J_PEERJCS},
  author       = {Yang Xu and Yueyi Zhang and Yefang Sun and Hanting Zhou},
  doi          = {10.7717/peerj-cs.2723},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2723},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An adaptive method for determining the optimal number of topics in topic modeling},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Atom search optimization: A comprehensive review of its variants, applications, and future directions. <em>PEERJCS</em>, <em>11</em>, e2722. (<a href='https://doi.org/10.7717/peerj-cs.2722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Atom Search Optimization (ASO) algorithm is a recent advancement in metaheuristic optimization inspired by principles of molecular dynamics. It mathematically models and simulates the natural behavior of atoms, with interactions governed by forces derived from the Lennard-Jones potential and constraint forces based on bond-length potentials. Since its inception in 2019, it has been successfully applied to various challenges across diverse fields in technology and science. Despite its notable achievements and the rapidly growing body of literature on ASO in the metaheuristic optimization domain, a comprehensive study evaluating the success of its various implementations is still lacking. To address this gap, this article provides a thorough review of half a decade of advancements in ASO research, synthesizing a wide range of studies to highlight key ASO variants, their foundational principles, and significant achievements. It examines diverse applications, including single- and multi-objective optimization problems, and introduces a well-structured taxonomy to guide future exploration in ASO-related research. The reviewed literature reveals that several variants of the ASO algorithm, including modifications, hybridizations, and multi-objective implementations, have been developed to tackle complex optimization problems. Moreover, ASO has been effectively applied across various domains, such as engineering, healthcare and medical applications, Internet of Things and communication, clustering and data mining, environmental modeling, and security, with engineering emerging as the most prevalent application area. By addressing the common challenges researchers face in selecting appropriate algorithms for real-world problems, this study provides valuable insights into the practical applications of ASO and offers guidance for designing ASO variants tailored to specific optimization problems.},
  archive      = {J_PEERJCS},
  author       = {Mohammed A. El-Shorbagy and Anas Bouaouda and Laith Abualigah and Fatma A. Hashim},
  doi          = {10.7717/peerj-cs.2722},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2722},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Atom search optimization: A comprehensive review of its variants, applications, and future directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Filipino sign language alphabet recognition using persistent homology classification algorithm. <em>PEERJCS</em>, <em>11</em>, e2720. (<a href='https://doi.org/10.7717/peerj-cs.2720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing number of deaf or hard-of-hearing individuals is a crucial problem since communication among and within the deaf population proves to be a challenge. Despite sign languages developing in various countries, there is still lack of formal implementation of programs supporting its needs, especially for the Filipino sign language (FSL). Recently, studies on FSL recognition explored deep networks. Current findings are promising but drawbacks on using deep networks still prevail. This includes low transparency, interpretability, need for big data, and high computational requirements. Hence, this article explores topological data analysis (TDA), an emerging field of study that harnesses techniques from computational topology, for this task. Specifically, we evaluate a TDA-inspired classifier called Persistent Homology Classification algorithm (PHCA) to classify static alphabet signed using FSL and compare its result with classical classifiers. Experiment is implemented on balanced and imbalanced datasets with multiple trials, and hyperparameters are tuned for a comprehensive comparison. Results show that PHCA and support vector machine (SVM) performed better than the other classifiers, having mean Accuracy of 99.45% and 99.31%, respectively. Further analysis shows that PHCA’s performance is not significantly different from SVM, indicating that PHCA performed at par with the best performing classifier. Misclassification analysis shows that PHCA struggles to classify signs with similar gestures, common to FSL recognition. Regardless, outcomes provide evidence on the robustness and stability of PHCA against perturbations to data and noise. It can be concluded that PHCA can serve as an alternative for FSL recognition, offering opportunities for further research.},
  archive      = {J_PEERJCS},
  author       = {Cristian B. Jetomo and Mark Lexter D. De Lara},
  doi          = {10.7717/peerj-cs.2720},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2720},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Filipino sign language alphabet recognition using persistent homology classification algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing market trend prediction using convolutional neural networks on japanese candlestick patterns. <em>PEERJCS</em>, <em>11</em>, e2719. (<a href='https://doi.org/10.7717/peerj-cs.2719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study discusses using Japanese candlestick (JC) patterns to predict future price movements in financial markets. The history of candlestick trading dates back to the 17th century and involves the analysis of patterns formed during JC trading. Candlestick patterns are practical tools for the technical analysis of traders in financial markets. They may serve as indicators of traders’ documents of a potential change in market sentiment and trend direction. This study aimed to predict the following candle-trend-based JC charts using convolutional neural networks (CNNs). In order to enhance the accuracy of predicting the directional movement of subsequent financial candlesticks, a rich dataset has been constructed by following a structured three-step process, and a CNN model has been trained. Initially, the dataset was analyzed, and sub-charts were generated using a sliding window technique. Subsequently, the Ta-lib library was used to identify whether predefined patterns were present within the windows. The third phase involved the classification of each window’s directional tendency, which was substantiated by employing various technical indicators to validate the direction of the trend. Following the data preparation and analysis phases, a CNN model was developed to extract features from sub-charts and facilitate precise predictions effectively. The experimental results of this approach demonstrated a remarkable predictive accuracy of up to 99.3%. Implementing cross-validation techniques is essential to verify the reliability and overall performance of the model. To achieve this goal, the dataset was divided into several small subsets. Subsequently, the model was trained and evaluated multiple times using different combinations of these subsets. This method allows for a more accurate assessment of the model’s predictive capabilities by examining its performance on unseen data.},
  archive      = {J_PEERJCS},
  author       = {Edrees Ramadan Mersal and Kürşat Mustafa Karaoğlan and Hakan Kutucu},
  doi          = {10.7717/peerj-cs.2719},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2719},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing market trend prediction using convolutional neural networks on japanese candlestick patterns},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of sleep apnea syndrome using the spectrograms of EEG signals and YOLOv8 deep learning model. <em>PEERJCS</em>, <em>11</em>, e2718. (<a href='https://doi.org/10.7717/peerj-cs.2718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we focus on classifying sleep apnea syndrome by using the spectrograms obtained from electroencephalogram (EEG) signals taken from polysomnography (PSG) recordings and the You Only Look Once (YOLO) v8 deep learning model. For this aim, the spectrograms of segments obtained from EEG signals with different apnea-hypopnea values (AHI) using a 30-s window function are obtained by short-time Fourier transform (STFT). The spectrograms are used as inputs to the YOLOv8 model to classify sleep apnea syndrome as mild, moderate, severe apnea, and healthy. For four-class classification models, the standard reference level is 25%, assuming equal probabilities for all classes or an equal number of samples in each class. In this context, this information is an important reference point for the validity of our study. Deep learning methods are frequently used for the classification of EEG signals. Although ResNet64 and YOLOv5 give effective results, YOLOv8 stands out with fast processing times and high accuracy. In the existing literature, parameter reduction approaches in four-class EEG classification have not been adequately addressed and there are limitations in this area. This study evaluates the performance of parameter reduction methods in EEG classification using YOLOv8, fills gaps in the existing literature for four-class classification, and reduces the number of parameters of the used models. Studies in the literature have generally classified sleep apnea syndrome as binary (apnea/healthy) and ignored distinctions between apnea severity levels. Furthermore, most of the existing studies have used models with a high number of parameters and have been computationally demanding. In this study, on the other hand, the use of spectrograms is proposed to obtain higher correct classification ratios by using more accurate and faster models. The same classification experiments are reimplemented for widely used ResNet64 and YOLOv5 deep learning models to compare with the success of the proposed model. In the implemented experiments, total correct classification (TCC) ratios are 93.7%, 93%, and 88.2% for YOLOv8, ResNet64, and YOLOv5, respectively. These experiments show that the YOLOv8 model reaches higher success ratios than the ResNet64 and YOLOv5 models. Although the TCC ratios of the YOLOv8 and ResNet64 models are comparable, the YOLOv8 model uses fewer parameters and layers than the others, providing a faster processing time and a higher TCC ratio. The findings of the study make a significant contribution to the current state of the art. As a result, this study gives rise to the idea that the YOLOv8 deep learning model can be used as a new tool for classification of sleep apnea syndrome from EEG signals.},
  archive      = {J_PEERJCS},
  author       = {Kubra Tanci and Mahmut Hekim},
  doi          = {10.7717/peerj-cs.2718},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2718},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Classification of sleep apnea syndrome using the spectrograms of EEG signals and YOLOv8 deep learning model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic periodic event graphs for multivariate time series pattern prediction. <em>PEERJCS</em>, <em>11</em>, e2717. (<a href='https://doi.org/10.7717/peerj-cs.2717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and predicting outcomes in complex real-world systems necessitates robust multivariate time series pattern analysis. Advanced techniques, such as dynamic graph neural networks, have shown significant efficacy for these tasks. However, existing approaches often overlook the inherent periodicity in data, leading to reduced pattern or event prediction accuracy, especially in periodic time series. We introduce a new method, called dynamic Periodic Event Graphs (PEGs), to tackle this challenge. The proposed method involves time series decomposition to extract seasonal components that capture periodically recurring patterns within the data. It also uses frequency analysis to extract representative periods from each seasonal component. Additionally, motif patterns, which are recurring sub-sequences in the time series data, are extracted. These motifs are used to define event nodes using the representative periods extracted from the seasonal components. By constructing periodic motif pattern-based dynamic bipartite event graphs, we specifically aim to enhance the performance of link prediction tasks, leveraging periodic characteristics in multivariate time series data. Our method has been rigorously tested on multiple periodic multivariate time series datasets, demonstrating over a 5% improvement in link prediction performance for both transductive and inductive scenarios. This demonstrates a substantial enhancement in predictive accuracy and generalization, providing confidence in the technique’s effectiveness. Reproducibility is ensured through publicly available source code, enabling future research and applications.},
  archive      = {J_PEERJCS},
  author       = {SoYoung Park and HyeWon Lee and Sungsu Lim},
  doi          = {10.7717/peerj-cs.2717},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2717},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic periodic event graphs for multivariate time series pattern prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing fraud detection in the ethereum blockchain using ensemble learning. <em>PEERJCS</em>, <em>11</em>, e2716. (<a href='https://doi.org/10.7717/peerj-cs.2716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Ethereum blockchain operates as a decentralized platform, utilizing blockchain technology to distribute smart contracts across a global network. It enables currency and digital value exchange without centralized control. However, the exponential growth of online commerce has created a fertile ground for a surge in fraudulent activities such as money laundering and phishing, thereby exacerbating significant security vulnerabilities. To combat this, our article introduces an ensemble learning approach to accurately detect fraudulent Ethereum blockchain transactions. Our goal is to integrate a decision-making tool into the decentralized validation process of Ethereum, allowing blockchain miners to identify and flag fraudulent transactions. Additionally, our system can assist governmental organizations in overseeing the blockchain network and identifying fraudulent activities. Our framework incorporates various data pre-processing techniques and evaluates multiple machine learning algorithms, including logistic regression, Isolation Forest, support vector machine, Random Forest, XGBoost, and recurrent neural network. These models are fine-tuned using grid search to enhance their performance. The proposed approach utilizes an ensemble of three distinct models (Random Forest, extreme gradient boosting (XGBoost), and support vector machine) to further improve classification performance. It achieves high scores of over 98% across key classification metrics like accuracy, precision, recall, and F1-score. Moreover, the approach is suitable for real-world usage, with an inference time of 0.13 s.},
  archive      = {J_PEERJCS},
  author       = {Zhexian Gu and Omar Dib},
  doi          = {10.7717/peerj-cs.2716},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2716},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing fraud detection in the ethereum blockchain using ensemble learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing power allocation for URLLC-D2D in 5G networks with rician fading channel. <em>PEERJCS</em>, <em>11</em>, e2712. (<a href='https://doi.org/10.7717/peerj-cs.2712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of wireless technologies within the 5G network brings significant challenges in managing the increased connectivity and traffic of mobile devices. This enhanced connectivity brings challenges for base stations, which must handle increased traffic and efficiently serve a growing number of mobile devices. One of the key solutions to address these challenges is integrating device-to-device (D2D) communication with ultra-reliable and low-latency communication (URLLC). This study examines the impact of the Rician fading channel on the performance of D2D communication under URLLC. It addresses the critical problem of optimizing power allocation to maximize the minimum data rate in D2D communication. A significant challenge arises due to interference issues, as the problem of maximizing the minimum data rate is non-convex, which leads to high computational complexity. This complexity makes it difficult to derive optimal solutions efficiently. To address this challenge, we introduce an algorithm that is based on derivatives to find the optimal power allocation. Comparisons are made with the branch and bound (B&B) algorithm, heuristic algorithm, and particle swarm optimization (PSO) algorithm. Our proposed algorithm improves power allocation performance and also achieves faster execution with lower computational complexity compared to the B&B, PSO, and heuristic algorithms.},
  archive      = {J_PEERJCS},
  author       = {Owais Muhammad and Hong Jiang and Muhammad Bilal and Mushtaq Muhammad Umer},
  doi          = {10.7717/peerj-cs.2712},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2712},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimizing power allocation for URLLC-D2D in 5G networks with rician fading channel},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-invasive enhanced hypertension detection through ballistocardiograph signals with mamba model. <em>PEERJCS</em>, <em>11</em>, e2711. (<a href='https://doi.org/10.7717/peerj-cs.2711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores using ballistocardiography (BCG), a non-invasive cardiovascular monitoring technique, combined with advanced machine learning and deep learning models for hypertension detection. The motivation behind this research is to develop a non-invasive and efficient approach for long-term hypertension monitoring, facilitating home-based health assessments. A dataset of 128 BCG recordings has been used, capturing body micro-vibrations from cardiac activity. Various classification models, including Mamba Classifier, Transformer, Stacking, Voting, and XGBoost, were applied to differentiate hypertensive individuals from normotensive ones. In this study, integrating BCG signals with deep learning and machine learning models for hypertension detection is distinguished from previous literature by employing the Mamba deep learning architecture and Transformer-based models. Unlike conventional methods in literature, this study enables more effective analysis of time-series data with the Mamba architecture, capturing long-term signal dependencies and achieving higher accuracy rates. In particular, the combined use of Mamba architecture and the Transformer model’s signal processing capabilities represents a novel approach not previously seen in the literature. While existing studies on BCG signals typically rely on traditional machine learning algorithms, this study aims to achieve higher success rates in hypertension detection by integrating signal processing and deep learning stages. The Mamba Classifier outperformed other models, achieving an accuracy of 95.14% and an AUC of 0.9922 in the 25% hold-out validation. Transformer and Stacking models also demonstrated strong performance, while the Voting and XGBoost models showed comparatively lower results. When combined with artificial intelligence techniques, the findings indicate the potential of BCG signals in providing non-invasive, long-term hypertension detection. The results suggest that the Mamba Classifier is the most effective model for this dataset. This research underscores the potential of BCG technology for continuous home-based health monitoring, providing a feasible alternative to traditional methods. Future research should aim to validate these findings with larger datasets and explore the clinical applications of BCG for cardiovascular disease monitoring.},
  archive      = {J_PEERJCS},
  author       = {Adi Alhudhaif and Kemal Polat},
  doi          = {10.7717/peerj-cs.2711},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2711},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Non-invasive enhanced hypertension detection through ballistocardiograph signals with mamba model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Origin-destination prediction from road average speed data using GraphResLSTM model. <em>PEERJCS</em>, <em>11</em>, e2709. (<a href='https://doi.org/10.7717/peerj-cs.2709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand for traffic management and resource allocation in Intelligent Transportation Systems (ITS), accurate origin-destination (OD) prediction has become crucial. This article presents a novel integrated framework, effectively merging the distinctive capabilities of graph convolutional network (GCN), residual neural network (ResNet), and long short-term memory network (LSTM), hereby designated as GraphResLSTM. GraphResLSTM leverages road average speed data for OD prediction. Contrary to traditional reliance on traffic flow data, road average speed data provides richer informational dimensions, reflecting not only vehicle volume but also indirectly indicating congestion levels. We use a real-world road network to generate road average speed data and OD data through simulations in Simulation of Urban Mobility (SUMO), thereby avoiding the influence of external factors such as weather. To enhance training efficiency, we employ a method combining the entropy weight method with the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) for key road segment selection. Using this generated dataset, carefully designed comparative experiments are conducted to compare various different models and data types. The results clearly demonstrate that both the GraphResLSTM model and the road average speed data markedly outperform alternative models and data types in OD prediction.},
  archive      = {J_PEERJCS},
  author       = {Guangtong Hu and Jun Zhang},
  doi          = {10.7717/peerj-cs.2709},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2709},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Origin-destination prediction from road average speed data using GraphResLSTM model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GATI-RS model using bi-LSTM and multi-head attention mechanism to enhance online shopping experience for the elderly with accurate click-through rate prediction. <em>PEERJCS</em>, <em>11</em>, e2707. (<a href='https://doi.org/10.7717/peerj-cs.2707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of e-commerce and the increasing aging population, more elderly people are engaging in online shopping. However, challenges they face during this process are becoming more apparent. This article proposes a recommendation system based on click-through rate (CTR) prediction, aiming to enhance the online shopping experience for elderly users. By analyzing user characteristics, product features, and their interactions, we constructed a model combining bidirectional long short-term memory (Bi-LSTM) and multi-head self-attention mechanism to predict the item click behavior of elderly users in the recommendation section. Experimental results demonstrated that the model excels in CTR prediction, effectively improving the relevance of recommended content. Compared to the baseline model long short-term memory (LSTM), the GATI-RS framework improved CTR prediction accuracy by 40%, and its loss function rapidly decreased and remained stable during training. Additionally, the GATI-RS framework showed significant performance improvement when considering only elderly users, with accuracy surpassing the baseline model by 42%. These results indicate that the GATI-RS framework, through optimized algorithms, significantly enhances the model’s global information integration and complex pattern recognition capabilities, providing strong support for developing recommendation systems for elderly online shoppers. This research not only offers new insights for e-commerce platforms to optimize services but also contributes to improving the quality of life and well-being of the elderly.},
  archive      = {J_PEERJCS},
  author       = {Ying Liu and Shahriman Zainal Abidin and Verly Veto Vermol and Shaolong Yang and Hanyu Liu},
  doi          = {10.7717/peerj-cs.2707},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2707},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GATI-RS model using bi-LSTM and multi-head attention mechanism to enhance online shopping experience for the elderly with accurate click-through rate prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative filtering based on GNN with attribute fusion and broad attention. <em>PEERJCS</em>, <em>11</em>, e2706. (<a href='https://doi.org/10.7717/peerj-cs.2706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems based on collaborative filtering (CF) have been a prominent area of research. In recent years, graph neural networks (GNN) based CF models have effectively addressed the limitations of nonlinearity and higher-order feature interactions in traditional recommendation methods, such as matrix decomposition-based methods and factorization machine approaches, achieving excellent recommendation performance. However, existing GNN-based CF models still have two problems that affect performance improvement. First, although distinguishing between inner interaction and cross interaction, these models still aggregate all attributes indiscriminately. Second, the models do not exploit higher-order interaction information. To address the problems above, this article proposes a collaborative filtering method based on GNN with attribute fusion and broad attention, named GNN-A2, which incorporates an inner interaction module with self-attention, a cross interaction module with attribute fusion, and a broad attentive cross module. In summary, GNN-A2 model performs inner interactions and cross interactions in different ways, then extracts their higher-order interaction information for prediction. We conduct extensive experiments on three benchmark datasets, i.e., MovieLens 1M, Book-crossing, and Taobao. The experimental results demonstrate that our proposed GNN-A2 model achieves comparable performance on area under the curve (AUC) metric. Notably, GNN-A2 achieves the optimal performance on Normalized Discounted Cumulative Gain at rank 10 (NDCG@10) over three datasets, with values of 0.9506, 0.9137, and 0.1526, corresponding to respective improvements of 0.68%, 1.57%, and 2.14% compared to the state-of-the-art (SOTA) models. The source code and evaluation datasets are available at: https://github.com/LMXue7/GNN-A2.},
  archive      = {J_PEERJCS},
  author       = {MingXue Liu and Min Wang and Baolei Li and Qi Zhong},
  doi          = {10.7717/peerj-cs.2706},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2706},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Collaborative filtering based on GNN with attribute fusion and broad attention},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based feature selection and classification for cerebral infarction screening: An experimental study. <em>PEERJCS</em>, <em>11</em>, e2704. (<a href='https://doi.org/10.7717/peerj-cs.2704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cerebral infarction screening (CIS) is critical for timely intervention and improved patient outcomes. We investigate the application of machine learning techniques for feature selection and classification of speech and cognitive function assessments to enhance cerebral infarction screening. We analyze a dataset containing 117 patients (95 patients were diagnosed with cerebral infarction, and 54 were identified as lacunar cerebral infarction of them) comprising speech and cognitive function features from patients with lacunar and non-lacunar cerebral infarction, as well as healthy controls. In this article, we present a framework called CIS which comprises a cerebral infarction screening model to identify cerebral infarction from populations and a diagnostic model to classify lacunar infarction, non-lacunar infarction, and healthy controls. Feature selection method, Recursive Feature Elimination with Cross-Validation (RFECV), is employed to identify the most relevant features. Various classifiers, such as support vector machine, K-nearest neighbor, decision tree, random forest, logistic regression, and eXtreme gradient boosting (XGBoost), were evaluated for their performance in binary and ternary classification tasks. The CIS based on XGBoost classifier achieved the highest accuracy of 88.89% in the binary classification task (i.e., distinguishing cerebral infarction from healthy controls) and 77.78% in the ternary classification task (i.e., distinguishing lacunar infarction, non-lacunar infarction, and healthy controls). The selected features significantly contributed to the classification performance, highlighting their potential in differentiating cerebral infarction subtypes. We develop a comprehensive system to effectively assess cerebral infarction subtypes. This study demonstrates the efficacy of machine learning methods in cerebral infarction screening through the analysis of speech and cognitive function features. These findings suggest that incorporating these techniques into clinical practice could improve early detection and diagnosis of cerebral infarction. Further research with larger and more diverse datasets is warranted to validate and extend these results.},
  archive      = {J_PEERJCS},
  author       = {Yang Niu and Xue Tao and Qinyuan Chang and Mingming Hu and Xin Li and Xiaoping Gao},
  doi          = {10.7717/peerj-cs.2704},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2704},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Machine learning-based feature selection and classification for cerebral infarction screening: An experimental study},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive machine learning approaches utilizing soft decision-making via intuitionistic fuzzy parameterized intuitionistic fuzzy soft matrices. <em>PEERJCS</em>, <em>11</em>, e2703. (<a href='https://doi.org/10.7717/peerj-cs.2703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential data growth generated by technological advancements presents significant challenges in analysis and decision-making, necessitating innovative and robust methodologies. Machine learning has emerged as a transformative tool to address these challenges, especially in scenarios requiring precision and adaptability. This study introduces two novel adaptive machine learning approaches, i.e., AIFPIFSC1 and AIFPIFSC2. These methods leverage the modeling ability of intuitionistic fuzzy parameterized intuitionistic fuzzy soft matrices (ifpifs-matrices). This state-of-the-art framework enhances the classification task in machine learning by employing soft decision-making through ifpifs-matrices. The proposed approaches are rigorously evaluated against leading fuzzy/soft-based classifiers using 15 widely recognized University of California, Irvine datasets, including accuracy and robustness, across six performance metrics. Statistical analyses conducted using Friedman and Nemenyi tests further substantiate the reliability and superiority of the proposed approaches. The results consistently demonstrate that these approaches outperform their counterparts, highlighting their potential for solving complex classification problems. This study contributes to the field by offering adaptable and effective solutions for modern data analysis challenges, paving the way for future advancements in machine learning and decision-making systems.},
  archive      = {J_PEERJCS},
  author       = {Samet Memiş and Ferhan Şola Erduran and Hivda Aydoğan},
  doi          = {10.7717/peerj-cs.2703},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2703},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Adaptive machine learning approaches utilizing soft decision-making via intuitionistic fuzzy parameterized intuitionistic fuzzy soft matrices},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain and explainable-AI integrated system for polycystic ovary syndrome (PCOS) detection. <em>PEERJCS</em>, <em>11</em>, e2702. (<a href='https://doi.org/10.7717/peerj-cs.2702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern era of digitalization, integration with blockchain and machine learning (ML) technologies is most important for improving applications in healthcare management and secure prediction analysis of health data. This research aims to develop a novel methodology for securely storing patient medical data and analyzing it for PCOS prediction. The main goals are to leverage Hyperledger Fabric for immutable, private data and to integrate Explainable Artificial Intelligence (XAI) techniques to enhance transparency in decision-making. The innovation of this study is the unique integration of blockchain technology with ML and XAI, solving critical issues of data security and model interpretability in healthcare. With the Caliper tool, the Hyperledger Fabric blockchain’s performance is evaluated and enhanced. The suggested Explainable AI-based blockchain system for Polycystic Ovary Syndrome detection (EAIBS-PCOS) system demonstrates outstanding performance and records 98% accuracy, 100% precision, 98.04% recall, and a resultant F1-score of 99.01%. Such quantitative measures ensure the success of the proposed methodology in delivering dependable and intelligible predictions for PCOS diagnosis, therefore making a great addition to the literature while serving as a solid solution for healthcare applications in the near future.},
  archive      = {J_PEERJCS},
  author       = {Gowthami Jaganathan and Shanthi Natesan},
  doi          = {10.7717/peerj-cs.2702},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2702},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Blockchain and explainable-AI integrated system for polycystic ovary syndrome (PCOS) detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimal peer selection for peer-to-peer video content distribution using fuzzy linear programming approach. <em>PEERJCS</em>, <em>11</em>, e2701. (<a href='https://doi.org/10.7717/peerj-cs.2701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of peer selection in peer-to-peer (P2P) video content distribution network is significant to solve since it affects the performance and efficiency of the network widely. In this article, a novel framework is introduced that uses fuzzy linear programming (FLP) to address the inherent uncertainties in peer selection. The primary motivation for the use of FLP lies in its capability to handle the imprecision and vagueness that are characteristic of dynamic P2P environments. Factors such as peer reliability, bandwidth, and proximity are often uncertain in this environment. By using fuzzy logic, the proposed framework models these criteria as fuzzy sets and then integrates uncertainty into the decision-making process. FLP is then applied to optimize peer selection, improving download speed, reducing download time, and enhancing peer reliability. The proposed method is evaluated and analyzed using extensive simulation with SciPy. The result reveals that proposed technique works better compared to some of the traditional methods in terms of download time, download speed and also reliability measure. It also exhibits approximately 20% of increase in download speed as well as a 15% decrease in download time compared to traditional approaches. It leads to faster content retrieval and enhanced the efficiency in content distribution. Also, in selection of reliable peers for content distribution, there is a notable 20% of increase in peer reliability with result of enhanced robustness. The proposed method provides efficient and robust solution to the problem of peer selection. It can be implemented in a broad range of P2P content distribution networks.},
  archive      = {J_PEERJCS},
  author       = {M. Anandaraj and Naganandhini S.},
  doi          = {10.7717/peerj-cs.2701},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2701},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An optimal peer selection for peer-to-peer video content distribution using fuzzy linear programming approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lung image segmentation with improved U-net, V-net and seg-net techniques. <em>PEERJCS</em>, <em>11</em>, e2700. (<a href='https://doi.org/10.7717/peerj-cs.2700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuberculosis remains a significant health challenge worldwide, affecting a large population. Therefore, accurate diagnosis of this disease is a critical issue. With advancements in computer systems, imaging devices, and rapid progress in machine learning, tuberculosis diagnosis is being increasingly performed through image analysis. This study proposes three segmentation models based on U-Net, V-Net, and Seg-Net architectures to improve tuberculosis detection using the Shenzhen and Montgomery databases. These deep learning-based methods aim to enhance segmentation accuracy by employing advanced preprocessing techniques, attention mechanisms, and non-local blocks. Experimental results indicate that the proposed models outperform traditional approaches, particularly in terms of the Dice coefficient and accuracy values. The models have demonstrated robust performance on popular datasets. As a result, they contribute to more precise and reliable lung region segmentation, which is crucial for the accurate diagnosis of respiratory diseases like tuberculosis. In evaluations using various performance metrics, the proposed U-Net and V-Net models achieved Dice coefficient scores of 96.43% and 96.42%, respectively, proving their competitiveness and effectiveness in medical image analysis. These findings demonstrate that the Dice coefficient values of the proposed U-Net and V-Net models are more effective in tuberculosis segmentation than Seg-Net and other traditional methods.},
  archive      = {J_PEERJCS},
  author       = {Fuat Turk and Mahmut Kılıçaslan},
  doi          = {10.7717/peerj-cs.2700},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2700},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Lung image segmentation with improved U-net, V-net and seg-net techniques},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized hybrid SVM-RF multi-biometric framework for enhanced authentication using fingerprint, iris, and face recognition. <em>PEERJCS</em>, <em>11</em>, e2699. (<a href='https://doi.org/10.7717/peerj-cs.2699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a hybrid multi-biometric system incorporating fingerprint, face, and iris recognition to enhance individual authentication. The system addresses limitations of uni-modal approaches by combining multiple biometric modalities, exhibiting superior performance and heightened security in practical scenarios, making it more dependable and resilient for real-world applications. The integration of support vector machine (SVM) and random forest (RF) classifiers, along with optimization techniques like bacterial foraging optimization (BFO) and genetic algorithms (GA), improves efficiency and robustness. Additionally, integrating feature-level fusion and utilizing methods such as Gabor filters for feature extraction enhances overall performance of the model. The system demonstrates superior accuracy and reliability, making it suitable for real-world applications requiring secure and dependable identification solutions.},
  archive      = {J_PEERJCS},
  author       = {Sonal and Ajit Singh and Chander Kant},
  doi          = {10.7717/peerj-cs.2699},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2699},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimized hybrid SVM-RF multi-biometric framework for enhanced authentication using fingerprint, iris, and face recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the prediction of vitamin d deficiency levels using an integrated approach of deep learning and evolutionary computing. <em>PEERJCS</em>, <em>11</em>, e2698. (<a href='https://doi.org/10.7717/peerj-cs.2698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vitamin D deficiency (VDD) has emerged as a serious global health concern that can lead to far-reaching consequences, including skeletal issues and long-term illness. Classical diagnostic approaches, although effective, often include invasive techniques and lacks to leverage the massive amount of healthcare data. There is an increasing demand for noninvasive prediction approaches for determining the severity of VDD. This work proposes a novel approach to detect VDD levels by combining deep learning techniques with evolutionary computing (EC). Specifically, we employ a hybrid deep learning model that includes convolutional neural networks (CNN) and bidirectional long short-term memory (BiLSTM) networks to predict VDD data effectively. To improve the models effectiveness and guarantee the optimal choice of the features and hyper-parameters, we incorporate evolutionary computing methods, particularly genetic algorithms (GA). The proposed method has been proven effective through a comprehensive assessment on a benchmark dataset, with 97% accuracy, 96% precision, 97% recall, and 96% F1-score. Our approach yielded improved performance, when compared to earlier methods. This research not only push forward predictive healthcare models but also shows the potential of merging deep learning with evolutionary computing to address intricate health-care issues.},
  archive      = {J_PEERJCS},
  author       = {Ahmed Alzahrani and Muhammad Zubair Asghar},
  doi          = {10.7717/peerj-cs.2698},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2698},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing the prediction of vitamin d deficiency levels using an integrated approach of deep learning and evolutionary computing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BiLSTM-enhanced legal text extraction model using fuzzy logic and metaphor recognition. <em>PEERJCS</em>, <em>11</em>, e2697. (<a href='https://doi.org/10.7717/peerj-cs.2697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The burgeoning field of natural language processing (NLP) has witnessed exponential growth, captivating researchers due to its diverse practical applications across industries. However, the intricate nature of legal texts poses unique challenges for conventional text extraction methods. To surmount these challenges, this article introduces a pioneering legal text extraction model rooted in fuzzy language processing and metaphor recognition, tailored for the domain of online environment governance. Central to this model is the utilization of a bidirectional long short-term memory (Bi-LSTM) network, adept at delineating illicit behaviors by establishing connections between legal provisions and judgments. Additionally, a self-attention module is integrated into the Bi-LSTM architecture, augmented by L2 regularization, to facilitate the efficient extraction of legal text information, thereby enabling the identification and classification of illegal content. This innovative approach effectively resolves the issue of legal text recognition. Experimental findings underscore the efficacy of the proposed method, achieving an impressive macro-F1 score of 0.8005, precision of 0.8047, and recall of 0.8014. Furthermore, the article delves into an analysis and discussion of the potential application prospects of the legal text extraction model, grounded in fuzzy language processing and metaphor recognition, within the realm of online environment governance.},
  archive      = {J_PEERJCS},
  author       = {Jia Chen},
  doi          = {10.7717/peerj-cs.2697},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2697},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {BiLSTM-enhanced legal text extraction model using fuzzy logic and metaphor recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on path planning of mobile robots based on improved a* algorithm. <em>PEERJCS</em>, <em>11</em>, e2691. (<a href='https://doi.org/10.7717/peerj-cs.2691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issues of low search efficiency, excessive node expansion, and the presence of redundant nodes in the traditional A* algorithm, this article proposes an improved A* algorithm for mobile robot path planning. Firstly, a multi-neighborhood hybrid search method is introduced, optimizing the traditional eight-neighborhood and twenty-four-neighborhood into a new sixteen-neighborhood. The choice between eight-neighborhood search and sixteen-neighborhood search is determined based on the presence of obstacles in the eight-neighborhood around the current node, effectively enhancing the search efficiency of the algorithm and reducing the number of nodes expanded during the search process. Subsequently, unnecessary nodes are eliminated based on the positional relationship between the current node and the target node, according to neighborhood direction search rules, further decreasing the number of expanded nodes. Additionally, improvements to the bidirectional search mechanism along with the incorporation of dynamic weight coefficients further enhance the search efficiency of the algorithm. Furthermore, a strategy for extracting key nodes is employed to effectively remove useless turn points, thus resolving the issue of redundant nodes. Finally, simulation experiments demonstrate that the proposed improved A* algorithm outperforms the traditional A* algorithm in terms of search speed, number of expanded nodes, and path length, validating the effectiveness of the proposed method.},
  archive      = {J_PEERJCS},
  author       = {Xing Fu and Zucheng Huang and Gongxue Zhang and Weijun Wang and Jian Wang},
  doi          = {10.7717/peerj-cs.2691},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2691},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on path planning of mobile robots based on improved a* algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stock market trading via actor-critic reinforcement learning and adaptable data structure. <em>PEERJCS</em>, <em>11</em>, e2690. (<a href='https://doi.org/10.7717/peerj-cs.2690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the stock market is attractive, and it is challenging to develop an efficient investment model with high accuracy due to changes in the values of the shares for political, economic, and social reasons. This article presents an innovative proposal for a short-term, automatic investment model to reduce capital loss during trading, applying a reinforcement learning (RL) model. On the other hand, we propose an adaptable data window structure to enhance the learning and accuracy of investment agents in three foreign exchange markets: crude oil, gold, and the Euro. In addition, the RL model employs an actor-critic neural network with rectified linear unit (ReLU) neurons to generate specialized investment agents, enabling more efficient trading, minimizing investment losses across different time periods, and reducing the model’s learning time. The proposed RL model obtained a reduction average loss of 0.03% in Euro, 0.25% in gold, and 0.13% in crude oil in the test phase with varying initial conditions.},
  archive      = {J_PEERJCS},
  author       = {Cesar Guevara},
  doi          = {10.7717/peerj-cs.2690},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2690},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Stock market trading via actor-critic reinforcement learning and adaptable data structure},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task snake optimization algorithm for global optimization and planar kinematic arm control problem. <em>PEERJCS</em>, <em>11</em>, e2688. (<a href='https://doi.org/10.7717/peerj-cs.2688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task optimization (MTO) algorithms aim to simultaneously solve multiple optimization tasks. Addressing issues such as limited optimization precision and high computational costs in existing MTO algorithms, this article proposes a multi-task snake optimization (MTSO) algorithm. The MTSO algorithm operates in two phases: first, independently handling each optimization problem; second, transferring knowledge. Knowledge transfer is determined by the probability of knowledge transfer and the selection probability of elite individuals. Based on this decision, the algorithm either transfers elite knowledge from other tasks or updates the current task through self-perturbation. Experimental results indicate that, compared to other advanced MTO algorithms, the proposed algorithm achieves the most accurate solutions on multitask benchmark functions, the five-task and 10-task planar kinematic arm control problems, the multitask robot gripper problem, and the multitask car side-impact design problem. The code and data for this article can be obtained from: https://doi.org/10.5281/zenodo.14197420.},
  archive      = {J_PEERJCS},
  author       = {Qingrui Li and Yongquan Zhou and Qifang Luo},
  doi          = {10.7717/peerj-cs.2688},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2688},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-task snake optimization algorithm for global optimization and planar kinematic arm control problem},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance and simulation analysis of 802.11ax OFDMA in contention-driven scenarios. <em>PEERJCS</em>, <em>11</em>, e2687. (<a href='https://doi.org/10.7717/peerj-cs.2687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 802.11ax standard introduces Orthogonal Frequency Division Multiple Access (OFDMA), shifting the role of access points (APs) in Wi-Fi networks. This shift integrates intricate scheduling logic, assigning coordinator roles to APs for multi-user uplink (MU-UL) transmissions and streamlining downlink traffic flows. These developments require robust network analysis and simulation tools to investigate the trade-offs associated with using OFDMA. In this study, we validate the implementation of OFDMA in ns-3 Wi-Fi module, enhancing flexibility and support for future updates through a redesign process. Previous studies validate the OFDMA implementation in the ns-3 Wi-Fi module by matching the simulation to predictions of analytical models. In this work, we demonstrate that OFDMA performance aligns with analytical predictions through simulation-based performance evaluations using ns-3 in some contention-driven use cases. The proposed system operates in both the uplink (UL) and downlink (DL) directions, implementing two scheduling logics to manage DL traffic flows and coordinate MU-UL transmissions. Simulation time is reduced by introducing parallel computing in the system. This study provides a reliable network analysis and simulation framework that thoroughly examines the trade-offs involved in using OFDMA.},
  archive      = {J_PEERJCS},
  author       = {Memoona and Sung Won Kim},
  doi          = {10.7717/peerj-cs.2687},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2687},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Performance and simulation analysis of 802.11ax OFDMA in contention-driven scenarios},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-scale CNN with atrous spatial pyramid pooling for enhanced chest-based disease detection. <em>PEERJCS</em>, <em>11</em>, e2686. (<a href='https://doi.org/10.7717/peerj-cs.2686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a sophisticated deep-learning model designed for the early detection of COVID-19 and pneumonia. The model employs a convolutional neural network-integrated with atrous spatial pyramid pooling. The atrous spatial pyramid pooling mechanism enhances the convolutional neural network model’s ability to capture fine and large-scale features, optimizing detection accuracy in chest X-ray images. This improvement, along with transfer learning, significantly enhances the overall performance. By utilizing data augmentation to address the scarcity of available X-ray images, our atrous spatial pyramid pooling-enhanced convolutional neural network achieved a validation accuracy of 98.66% for COVID-19 and 83.75% for pneumonia, which beats the validation results of the other state of the art approaches (the metrics used for evaluation were accuracy, precision, F1-score, recall, specificity, and area under the curve). The model’s multi-branch architecture facilitates more accurate and adaptable disease prediction, thereby increasing diagnostic precision and robustness. This approach offers the potential for faster and more reliable diagnoses of chest-related conditions.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Abdullah Shah Bukhari and Faisal Bukhari and Muhammad Asif and Hanan Aljuaid and Waheed Iqbal},
  doi          = {10.7717/peerj-cs.2686},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2686},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A multi-scale CNN with atrous spatial pyramid pooling for enhanced chest-based disease detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An online intelligent detection method for slurry density in concept drift data streams based on collaborative computing. <em>PEERJCS</em>, <em>11</em>, e2683. (<a href='https://doi.org/10.7717/peerj-cs.2683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial environments, slurry density detection models often suffer from performance degradation due to concept drift. To address this, this article proposes an intelligent detection method tailored for slurry density in concept drift data streams. The method begins by building a model using Gaussian process regression (GPR) combined with regularized stochastic configuration. A sliding window-based online GPR is then applied to update the linear model’s parameters, while a forgetting mechanism enables online recursive updates for the nonlinear model. Network pruning and stochastic configuration techniques dynamically adjust the nonlinear model’s structure. These approaches enhance the mechanistic model’s ability to capture dynamic relationships and reduce the data-driven model’s reliance on outdated data. By focusing on recent data to reflect current operating conditions, the method effectively mitigates concept drift in complex process data. Additionally, the method is applied in industrial settings through collaborative computing, ensuring real-time slurry density detection and model adaptability. Experimental results on industrial data show that the proposed method outperforms other algorithms in all density estimation metrics, significantly improving slurry density detection accuracy.},
  archive      = {J_PEERJCS},
  author       = {Lanhao Wang and Hao Wang and Taojie Wei and Wei Dai and Hongyan Wang},
  doi          = {10.7717/peerj-cs.2683},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2683},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An online intelligent detection method for slurry density in concept drift data streams based on collaborative computing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explainable multi-objective hybrid machine learning model for reducing heart failure mortality. <em>PEERJCS</em>, <em>11</em>, e2682. (<a href='https://doi.org/10.7717/peerj-cs.2682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the world grapples with pandemics and increasing stress levels among individuals, heart failure (HF) has emerged as a prominent cause of mortality on a global scale. The most effective approach to improving the chances of individuals’ survival is to diagnose this condition at an early stage. Researchers widely utilize supervised feature selection techniques alongside conventional standalone machine learning (ML) algorithms to achieve the goal. However, these approaches may not consistently demonstrate robust performance when applied to data that they have not encountered before, and struggle to discern intricate patterns within the data. Hence, we present a Multi-objective Stacked Enable Hybrid Model (MO-SEHM), that aims to find out the best feature subsets out of numerous different sets, considering multiple objectives. The Stacked Enable Hybrid Model (SEHM) plays the role of classifier and integrates with a multi-objective feature selection method, the Non-dominated Sorting Genetic Algorithm II (NSGA-II). We employed an HF dataset from the Faisalabad Institute of Cardiology (FIOC) and evaluated six ML models, including SEHM with and without NSGA-II for experimental purposes. The Pareto front (PF) demonstrates that our introduced MO-SEHM surpasses the other models, obtaining 94.87% accuracy with the nine relevant features. Finally, we have applied Local Interpretable Model-agnostic Explanations (LIME) with MO-SEHM to explain the reasons for individual outcomes, which makes our model transparent to the patients and stakeholders.},
  archive      = {J_PEERJCS},
  author       = {F M Javed Mehedi Shamrat and Majdi Khalid and Thamir M. Qadah and Majed Farrash and Hanan Alshanbari},
  doi          = {10.7717/peerj-cs.2682},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2682},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An explainable multi-objective hybrid machine learning model for reducing heart failure mortality},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A user-embedded temporal attention neural network for IoT trajectories prediction. <em>PEERJCS</em>, <em>11</em>, e2681. (<a href='https://doi.org/10.7717/peerj-cs.2681'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past two decades, sequential recommendation systems have garnered significant research interest, driven by their potential applications in personalized product recommendations. In this article, we seek to explicitly model an algorithm based on Internet of Things (IoT) data to predict the next cell reached by the user equipment (UE). This algorithm exploits UE embedding and cell embedding combining the visit time interval information, and uses sliding window sampling to process more UE trajectory data. Furthermore, we use the attention mechanism, removed the query matrix operation and the attention mask, to obtain key information in data and reduce the number of parameters to speed up training. In the prediction layer, combining the positive and negative sampling and computing cross entropy loss also provides assistance to increase the precision and dependability of the entire model. We take the six adjacent cells of the current cell as candidates due to the limitation of the space problem, from which we predict the next destination cell of track movement. Extensive empirical study shows the recall of our algorithm reaches 0.5766, which infers the optimal result and high performance of our model.},
  archive      = {J_PEERJCS},
  author       = {Dongdong Feng and Siyao Li and Yong Xiang and Jiahuan Zheng},
  doi          = {10.7717/peerj-cs.2681},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2681},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A user-embedded temporal attention neural network for IoT trajectories prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based novel ensemble method with best score transferred-adaptive neuro fuzzy inference system for energy consumption prediction. <em>PEERJCS</em>, <em>11</em>, e2680. (<a href='https://doi.org/10.7717/peerj-cs.2680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Energy consumption predictions for smart homes and cities benefit many from homeowners to energy suppliers, allowing homeowners to understand and manage their future energy consumption, improve energy efficiency, and reduce energy costs. Predictions can help energy suppliers effectively distribute energy on demand. Therefore, from the past to the present, numerous methods have been conducted using collected data, employing both statistical and artificial intelligence (AI)-based approaches, to achieve successful energy consumption predictions. Methods This study proposes a deep learning-based novel ensemble (DLBNE) method with the best score transferred-adaptive neuro fuzzy inference system (BST-ANFIS) as a high-performance and robust approach for energy consumption prediction. The proposed method uses deep learning (DL)-based algorithms, including convolutional neural networks (CNN), recurrent neural networks (RNN), long short-term memory (LSTM), bidirectional long short-term memory (BI-LSTM), and gated recurrent units (GRUs) as base predictors. The BST-ANFIS architecture combines the individual outcomes of these predictors. In order to build a robust and dynamic prediction model, the interaction between the base predictors and the ANFIS architecture is achieved using a best score transfer approach. The performance of the proposed method in energy consumption prediction was verified through five DL methods, five machine learning (ML) methods, and a DL-based weighted average (DLBWA) ensemble method. Results In experimental studies, the results were obtained from three-stage analyses: fold, average, and periodic performance analyses. In fold analyses, the proposed method, in terms of the root mean square error (RMSE) metric, demonstrated better performance in four folds on the Internet of Things (IoT)-based smart home (IBSH) dataset, two in the homestead city electricity consumption (HCEC) dataset, and two in the individual household power consumption (IHPC) dataset compared to the other methods. In the average performance analyses, it showed significantly higher performance than the other methods in all metrics for the IBSH and IHPC datasets, and in metrics except the mean absolute error (MAE) metric for the HCEC dataset. The performance results in terms of RMSE, MAE, mean square error (MSE), and mean absolute percentage error (MAPE) metrics from these analyses were obtained as 0.001531, 0.001010, 0.0000031, and 0.001573 for the IBSH dataset; 0.025208, 0.005889, 0.001884, and 0.000137 for the HCEC dataset; and 0.013640, 0.006572, 0.000356, and 0.000943 for the IHPC dataset, respectively. The results of the 120-h periodic analyses also showed that the proposed method yielded a better prediction result than the other methods. Furthermore, a comparison of the proposed method with similar studies in the literature revealed that it demonstrated competitive performance in relation to the methods employed in those studies.},
  archive      = {J_PEERJCS},
  author       = {Birce Dağkurs and İsmail Atacak},
  doi          = {10.7717/peerj-cs.2680},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2680},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning-based novel ensemble method with best score transferred-adaptive neuro fuzzy inference system for energy consumption prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient pooling distillation network for lightweight single image super-resolution reconstruction. <em>PEERJCS</em>, <em>11</em>, e2679. (<a href='https://doi.org/10.7717/peerj-cs.2679'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The single image super-resolution (SISR) is a classical problem in the field of computer vision, aiming to enhance high-resolution details from low-resolution images. In recent years, significant progress about SISR has been achieved through the utilization of deep learning technology. However, these deep methods often exhibit large-scale networks architectures, which are computationally intensive and hardware-demanding, and this limits their practical application in some scenarios (e.g., autonomous driving, streaming media) requiring stable and efficient image transmission with high-definition picture quality. In such application settings, computing resources are often restricted. Thus, there is a pressing demand to devise efficient super-resolution algorithms. To address this issue, we propose a gradient pooling distillation network (GPDN), which can enable the efficient construction of a single image super-resolution system. In the GPDN we leverage multi-level stacked feature distillation hybrid units to capture multi-scale feature representations, which are subsequently synthesized for dynamic feature space optimization. The central to the GPDN is the Gradient Pooling Distillation module, which operates through hierarchical pooling to decompose and refine critical features across various dimensions. Furthermore, we introduce the Feature Channel Attention module to accurately filter and strengthen pixel features crucial for recovering high-resolution images. Extensive experimental results demonstrate that our proposed method achieves competitive performance while maintaining relatively low resource occupancy of the model. This model strikes for a balance between excellent performance and resource utilization—particularly when trading off high recovery quality with small memory occupancy.},
  archive      = {J_PEERJCS},
  author       = {Zhiyong Hong and GuanJie Liang and Liping Xiong},
  doi          = {10.7717/peerj-cs.2679},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2679},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Gradient pooling distillation network for lightweight single image super-resolution reconstruction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). U-TSS: A novel time series segmentation model based U-net applied to automatic detection of interference events in geomagnetic field data. <em>PEERJCS</em>, <em>11</em>, e2678. (<a href='https://doi.org/10.7717/peerj-cs.2678'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of Internet of Things (IoT) technology, the volume of sensor data collection has increased significantly. These data are typically presented in the form of time series, gradually becoming a crucial component of big data. Traditional time series analysis methods struggle with complex patterns and long-term dependencies, whereas deep learning technologies offer new solutions. This study introduces the U-TSS, a U-net-based sequence-to-sequence fully convolutional network, specifically designed for one-dimensional time series segmentation tasks. U-TSS maps input sequences of arbitrary length to corresponding sequences of class labels across different temporal scales. This is achieved by implicitly classifying each individual time point in the input time series and then aggregating these classifications over varying intervals to form the final prediction. This enables precise segmentation at each time step, ensuring both global sequence awareness and accurate classification of complex time series data. We applied U-TSS to geomagnetic field observation data for the detection of high-voltage direct current (HVDC) interference events. In experiments, U-TSS achieved superior performance in detecting HVDC interference events, with accuracies of 99.42%, 94.61%, and 95.54% on the training, validation, and test sets, respectively, outperforming state-of-the-art models in accuracy, precision, recall, F1-score, and AUC. Our code can be accessed openly in the GitHub repository at https://github.com/wangmengyu1/U-TSS.},
  archive      = {J_PEERJCS},
  author       = {Weifeng Shan and Mengyu Wang and Jinzhu Xia and Jun Chen and Qi Li and Lili Xing and Ruilei Zhang and Maofa Wang and Suqin Zhang and Xiuxia Zhang},
  doi          = {10.7717/peerj-cs.2678},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2678},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {U-TSS: A novel time series segmentation model based U-net applied to automatic detection of interference events in geomagnetic field data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight coal-gangue detection model based on parallel deep residual networks. <em>PEERJCS</em>, <em>11</em>, e2677. (<a href='https://doi.org/10.7717/peerj-cs.2677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To realize the accurate identification of coal-gangue in the process of underground coal transportation and the low-cost deployment of the model, a lightweight coal-gangue detection model based on the parallel depth residual network, called P-RNet, is proposed. For the problem of images of coal-gangue taken under complex conditions, the feature extraction module (FEM) is designed using decoupling training and inference methods. Furthermore, for the problem of the nearest neighbor interpolation upsampling method being prone to produce mosaic blocks and edge jagged edges, a lightweight upsampling operator is used to optimize the feature fusion module (FFM). Finally, to solve the problem, the stochastic gradient descent algorithm is prone to local suboptimal solutions and saddle point problems in the error function optimization process, numerous experiments are carried out on selecting the initial learning rate, and the Lookahead optimizer is used to optimize parameters during backpropagation. Experimental results show that the proposed model can effectively improve the recognition effect, with a corresponding low deployment cost.},
  archive      = {J_PEERJCS},
  author       = {Shexiang Jiang and Xinrui Zhou},
  doi          = {10.7717/peerj-cs.2677},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2677},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A lightweight coal-gangue detection model based on parallel deep residual networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Construction of a user-friendly software-defined networking management using a graph-based abstraction layer. <em>PEERJCS</em>, <em>11</em>, e2674. (<a href='https://doi.org/10.7717/peerj-cs.2674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The software-defined networking (SDN) paradigm relies on the decoupling of the control plane and data plane. Northbound interfaces enable the implementation of network services through logical centralised control. Suitable northbound interfaces and application-oriented abstractions are the core of the SDN ecosystem. This article presents an architecture to represent the network as a graph. The purpose of this architecture is to implement an abstraction of the SDN controller at the application plane. We abstract all network elements using a graph model, with the attributes of the elements as the attributes of the graph. This virtualized logical abstraction layer, which is not limited by the physical network, enables network administrators to schedule network resources directly in a global view. The feasibility of the presented graph abstraction was verified through experiments in topological display, dynamic route, access control, and data persistence. The performance of the shortest path in the graph-based abstraction layer and graph database proves the necessity of the graph abstraction layer. Empirical evidence demonstrates that the graph-based abstraction layer can facilitate network slicing, maintain a dependable depiction of the real network, streamline network administration and network application development, and provide a sophisticated abstraction that is easily understandable to network administrators.},
  archive      = {J_PEERJCS},
  author       = {Yufeng Jia and Jiadong Ren and Xianshan Li and Haitao He and Pengwei Zhang and Rong Li},
  doi          = {10.7717/peerj-cs.2674},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2674},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Construction of a user-friendly software-defined networking management using a graph-based abstraction layer},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of perinatal depression among women in pakistan using hybrid RNN-LSTM model. <em>PEERJCS</em>, <em>11</em>, e2673. (<a href='https://doi.org/10.7717/peerj-cs.2673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perinatal depression (PND) refers to a complex mental health condition that can occur during pregnancy (prenatal period) or in the first year after childbirth (postnatal period). Prediction of PND holds considerable importance due to its significant role in safeguarding the mental health and overall well-being of both mothers and their infants. Unfortunately, PND is difficult to diagnose at an early stage and thus may elevate the risk of suicide during pregnancy. In addition, it contributes to the development of postnatal depressive disorders. Despite the gravity of the problem, the resources for developing and training AI models in this area remain limited. To this end, in this work, we have locally curated a novel dataset named PERI DEP using the Patient Health Questionnaire (PHQ-9), Edinburgh Postnatal Depression Scale (EPDS), and socio-demographic questionnaires. The dataset consists of 14,008 records of women who participated in the hospitals of Lahore and Gujranwala regions. We have used SMOTE and GAN oversampling for data augmentation on the training set to solve the class imbalance problem. Furthermore, we propose a novel deep-learning framework combining the recurrent neural networks (RNN) and long short-term memory (LSTM) architectures. The results indicate that our hybrid RNN-LSTM model with SMOTE augmentation achieves a higher accuracy of 95% with an F1 score of 96%. Our study reveals the prevalence rate of PND among women in Pakistan (73.1%) indicating the need to prioritize the prevention and intervention strategies to overcome this public health challenge.},
  archive      = {J_PEERJCS},
  author       = {Amna Zafar and Muhammad Wasim and Beenish Ayesha Akram and Maham Riaz and Ivan Miguel Pires and Paulo Jorge Coelho},
  doi          = {10.7717/peerj-cs.2673},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2673},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Prediction of perinatal depression among women in pakistan using hybrid RNN-LSTM model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combination of machine learning and data envelopment analysis to measure the efficiency of the tax service office. <em>PEERJCS</em>, <em>11</em>, e2672. (<a href='https://doi.org/10.7717/peerj-cs.2672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Tax Service Office, a division of the Directorate General of Taxes, is responsible for providing taxation services to the public and collecting taxes. Achieving tax targets efficiently while utilizing available resources is crucial. To assess the performance efficiency of decision-making units (DMUs), data envelopment analysis (DEA) is commonly employed. However, ensuring homogeneity among the DMUs is often necessary and requires the application of machine learning clustering techniques. In this study, we propose a three-stage approach: Clustering, DEA, and Regression, to measure the efficiency of all tax service office units. Real datasets from Indonesian tax service offices were used while maintaining strict confidentiality. Unlike previous studies that considered both input and output variables, we focus solely on clustering input variables, as it leads to more objective efficiency values when combining the results from each cluster. The results revealed three clusters with a silhouette score of 0.304 and Davies Bouldin Index of 1.119, demonstrating the effectiveness of fuzzy c-means clustering. Out of 352 DMUs, 225 or approximately 64% were identified as efficient using DEA calculations. We propose a regression algorithm to measure the efficiency of DMUs in new office planning, by determining the values of input and output variables. The optimization of multilayer perceptrons using genetic algorithms reduced the mean squared error by about 75.75%, from 0.0144 to 0.0035. Based on our findings, the overall performance of tax service offices in Indonesia has reached an efficiency level of 64%. These results show a significant improvement over the previous study, in which only about 18% of offices were considered efficient. The main contribution of this research is the development of a comprehensive framework for evaluating and predicting tax office efficiency, providing valuable insights for improving performance.},
  archive      = {J_PEERJCS},
  author       = {Shofinurdin Soffan and Arif Bramantoro and Ahmad A. Alzahrani},
  doi          = {10.7717/peerj-cs.2672},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2672},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Combination of machine learning and data envelopment analysis to measure the efficiency of the tax service office},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved termite life cycle optimizer algorithm for global function optimization. <em>PEERJCS</em>, <em>11</em>, e2671. (<a href='https://doi.org/10.7717/peerj-cs.2671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The termite life cycle optimizer algorithm (TLCO) is a new bionic meta-heuristic algorithm that emulates the natural behavior of termites in their natural habitat. This work presents an improved TLCO (ITLCO) to increase the speed and accuracy of convergence. A novel strategy for worker generation is established to enhance communication between individuals in the worker population and termite population. This strategy would prevent the original worker generation strategy from effectively balancing algorithm convergence and population diversity to reduce the risk of the algorithm in reaching a local optimum. A novel soldier generation strategy is proposed, which incorporates a step factor that adheres to the principles of evolution to further enhance the algorithm’s convergence speed. Furthermore, a novel replacement update mechanism is executed when the new individual is of lower quality than the original individual. This mechanism ensures a balance between the convergence of the algorithm and the diversity of the population. The findings from CEC2013, CEC2019, and CEC2020 test sets indicate that ITLCO exhibits notable benefits regarding convergence speed, accuracy, and stability in comparison with the basic TLCO algorithm and the four most exceptional meta-heuristic algorithms thus far.},
  archive      = {J_PEERJCS},
  author       = {Yanjiao Wang and Mengjiao Wei},
  doi          = {10.7717/peerj-cs.2671},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2671},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An improved termite life cycle optimizer algorithm for global function optimization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight-CancerNet: A deep learning approach for brain tumor detection. <em>PEERJCS</em>, <em>11</em>, e2670. (<a href='https://doi.org/10.7717/peerj-cs.2670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting brain tumors in medical imaging is challenging, requiring precise and rapid diagnosis. Deep learning techniques have shown encouraging results in this field. However, current models require significant computer resources and are computationally demanding. To overcome these constraints, we suggested a new deep learning architecture named Lightweight-CancerNet, designed to detect brain tumors efficiently and accurately. The proposed framework utilizes MobileNet architecture as the backbone and NanoDet as the primary detection component, resulting in a notable mean average precision (mAP) of 93.8% and an accuracy of 98%. In addition, we implemented enhancements to minimize computing time without compromising accuracy, rendering our model appropriate for real-time object detection applications. The framework’s ability to detect brain tumors with different image distortions has been demonstrated through extensive tests combining two magnetic resonance imaging (MRI) datasets. This research has shown that our framework is both resilient and reliable. The proposed model can improve patient outcomes and facilitate decision-making in brain surgery while contributing to the development of deep learning in medical imaging.},
  archive      = {J_PEERJCS},
  author       = {Asif Raza and Muhammad Javed Iqbal},
  doi          = {10.7717/peerj-cs.2670},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2670},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Lightweight-CancerNet: A deep learning approach for brain tumor detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging sentiment analysis of food delivery services reviews using deep learning and word embedding. <em>PEERJCS</em>, <em>11</em>, e2669. (<a href='https://doi.org/10.7717/peerj-cs.2669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Companies that deliver food (food delivery services, or FDS) try to use customer feedback to identify aspects where the customer experience could be improved. Consumer feedback on purchasing and receiving goods via online platforms is a crucial tool for learning about a company’s performance. Many English-language studies have been conducted on sentiment analysis (SA). Arabic is becoming one of the most extensively written languages on the World Wide Web, but because of its morphological and grammatical difficulty as well as the lack of openly accessible resources for Arabic SA, like as dictionaries and datasets, there has not been much research done on the language. Using a manually annotated FDS dataset, the current study conducts extensive sentiment analysis using reviews related to FDS that include Modern Standard Arabic and dialectal Arabic. It does this by utilizing word embedding models, deep learning techniques, and natural language processing to extract subjective opinions, determine polarity, and recognize customers’ feelings in the FDS domain. Convolutional neural network (CNN), bidirectional long short-term memory recurrent neural network (BiLSTM), and an LSTM-CNN hybrid model were among the deep learning approaches to classification that we evaluated. In addition, the article investigated different effective approaches for word embedding and stemming techniques. Using a dataset of Modern Standard Arabic and dialectal Arabic corpus gathered from Talabat.com, we trained and evaluated our suggested models. Our best accuracy was approximately 84% for multiclass classification and 92.5% for binary classification on the FDS. To verify that the proposed approach is suitable for analyzing human perceptions in diversified domains, we designed and carried out excessive experiments on other existing Arabic datasets. The highest obtained multi-classification accuracy is 88.9% on the Hotels Arabic-Reviews Dataset (HARD) dataset, and the highest obtained binary classification accuracy is 97.2% on the same dataset.},
  archive      = {J_PEERJCS},
  author       = {Dheya Mustafa and Safaa M. Khabour and Mousa Al-kfairy and Ahmed Shatnawi},
  doi          = {10.7717/peerj-cs.2669},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2669},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging sentiment analysis of food delivery services reviews using deep learning and word embedding},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFI-net: Multi-level feature invertible network image concealment technique. <em>PEERJCS</em>, <em>11</em>, e2668. (<a href='https://doi.org/10.7717/peerj-cs.2668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of deep learning and invertible networks for image hiding has been proven effective and secure. These methods can conceal large amounts of information while maintaining high image quality and security. However, existing methods often lack precision in selecting the hidden regions and primarily rely on residual structures. They also fail to fully exploit low-level features, such as edges and textures. These issues lead to reduced quality in model generation results, a heightened risk of network overfitting, and diminished generalization capability. In this article, we propose a novel image hiding method based on invertible networks, called MFI-Net. The method introduces a new upsampling convolution block (UCB) and combines it with a residual dense block that employs the parametric rectified linear unit (PReLU) activation function, effectively utilizing multi-level information (low-level and high-level features) of the image. Additionally, a novel frequency domain loss (FDL) is introduced, which constrains the secret information to be hidden in regions of the cover image that are more suitable for concealing the data. Extensive experiments on the DIV2K, COCO, and ImageNet datasets demonstrate that MFI-Net consistently outperforms state-of-the-art methods, achieving superior image quality metrics. Furthermore, we apply the proposed method to digital collection images, achieving significant success.},
  archive      = {J_PEERJCS},
  author       = {Dapeng Cheng and Minghui Zhu and Bo Yang and Xiaolian Gao and Wanting Jing and Yanyan Mao and Feng Zhao},
  doi          = {10.7717/peerj-cs.2668},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2668},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MFI-net: Multi-level feature invertible network image concealment technique},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated evaluation systems to enhance exam quality and reduce test anxiety. <em>PEERJCS</em>, <em>11</em>, e2666. (<a href='https://doi.org/10.7717/peerj-cs.2666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {University examination papers play a crucial role in the institution’s quality, impacting the institution’s accreditation status. In this context, ensuring the quality of examination papers is paramount. In practice, however, manual assessments are mostly laborious and time-consuming and generally lack consistency. The last decade has seen digital education acquire immense interest in academic discourse, especially when developing intelligent systems for educational assessment. The presented work proposes an automated system that allows text analysis and evaluation of university exam papers by formal and technical criteria. The research was conducted by analyzing 30 exam papers, which will be included in each of the exam papers, which consist of 60 questions each, in total it holds 1,800 questions. Moreover, it also includes research to understand the quality and relationship with students’ test anxiety. A total of 50 year one first-year students were taken to measure students’ academic stress by a scale. Planning on basic levels and adherence to technical standards were missing in the exam papers. The proposed automated system has improved exam paper quality to a great extent and reduced academic stress among students with an accuracy of 98% in identifying and matching specified criteria.},
  archive      = {J_PEERJCS},
  author       = {Doaa Mohamed Elbourhamy},
  doi          = {10.7717/peerj-cs.2666},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2666},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automated evaluation systems to enhance exam quality and reduce test anxiety},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A secure healthcare data transmission based on synchronization of fractional order chaotic systems. <em>PEERJCS</em>, <em>11</em>, e2665. (<a href='https://doi.org/10.7717/peerj-cs.2665'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transmission of healthcare data plays a vital role in cities worldwide, facilitating access to patient’s health information across healthcare systems and contributing to the enhancement of care services. Ensuring secure healthcare transmission requires that the transmitted data be reliable. However, verifying this reliability can potentially compromise patient privacy. Given the sensitive nature of health information, preserving privacy remains a paramount concern in healthcare systems. In this work, we present a novel secure communication scheme that leverages a chaos cryptosystem to address the critical concerns of reliability and privacy in healthcare data transmission. Chaos-based cryptosystems are particularly well-suited for such applications due to their inherent sensitivity to initial conditions, which significantly enhances resistance to adversarial violations. This property makes the chaos-based approach highly effective in ensuring the security of sensitive healthcare data. The proposed chaos cryptosystem in this work is built upon the synchronization of fractional-order chaotic systems with varying structures and orders. The synchronization between the primary system (PS) and the secondary system (SS) is achieved through the application of Lyapunov stability theory. For the encryption and decryption of sensitive healthcare data, the scheme employs the n-shift encryption principle. Furthermore, a detailed analysis of the key space was conducted to ensure the scheme’s robustness against potential attacks. Numerical simulations were also performed to validate the effectiveness of the proposed scheme.},
  archive      = {J_PEERJCS},
  author       = {Nur Afiqah Suzelan Amir and Fatin Nabila Abd Latiff and Kok Bin Wong and Wan Ainun Mior Othman},
  doi          = {10.7717/peerj-cs.2665},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2665},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A secure healthcare data transmission based on synchronization of fractional order chaotic systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting amyloid proteins using attention-based long short-term memory. <em>PEERJCS</em>, <em>11</em>, e2660. (<a href='https://doi.org/10.7717/peerj-cs.2660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is one of the genetically inherited neurodegenerative disorders that mostly occur when people get old. It can be recognized by severe memory impairment in the late stage, affecting cognitive function and general daily living. Reliable evidence confirms that the enhanced symptoms of AD are linked to the accumulation of amyloid proteins. The dense population of amyloid proteins forms insoluble fibrillar structures, causing significant pathological impacts in various tissues. Understanding amyloid protein’s mechanisms and identifying them at an early stage plays an essential role in treating AD as well as prevalent amyloid-related diseases. Recently, although several machine learning methods proposed for amyloid protein identification have shown promising results, most of them have not yet fully exploited the sequence information of the amyloid proteins. In this study, we develop a computational model for in silico identification of amyloid proteins using bidirectional long short-term memory in combination with an attention mechanism. In the testing phase, our findings showed that the model developed by our proposed method outperformed those developed by state-of-the-art methods with an area under the receiver operating characteristic curve of 0.9126.},
  archive      = {J_PEERJCS},
  author       = {Zhuowen Li},
  doi          = {10.7717/peerj-cs.2660},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2660},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting amyloid proteins using attention-based long short-term memory},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging a hybrid convolutional gated recursive diabetes prediction and severity grading model through a mobile app. <em>PEERJCS</em>, <em>11</em>, e2642. (<a href='https://doi.org/10.7717/peerj-cs.2642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes mellitus is a common illness associated with high morbidity and mortality rates. Early detection of diabetes is essential to prevent long-term health complications. The existing machine learning model struggles with accuracy and reliability issues, as well as data imbalance, hindering the creation of a dependable diabetes prediction model. The research addresses the issue using a novel deep learning mechanism called convolutional gated recurrent unit (CGRU), which could accurately detect diabetic disorder and their severity level. To overcome these obstacles, this study presents a brand-new deep learning technique, the CGRU, which enhances prediction accuracy by extracting temporal and spatial characteristics from the data. The proposed mechanism extracts both the spatial and temporal attributes from the input data to enable efficient classification. The proposed framework consists of three primary phases: data preparation, model training, and evaluation. Specifically, the proposed technique is applied to the BRFSS dataset for diabetes prediction. The collected data undergoes pre-processing steps, including missing data imputation, irrelevant feature removal, and normalization, to make it suitable for further processing. Furthermore, the pre-processed data is fed to the CGRU model, which is trained to identify intricate patterns indicating the stages of diabetes. To group the patients based on their characteristics and identity patterns, the research uses the clustering algorithm which helps them to classify the severity level. The efficacy of the proposed CGRU framework is demonstrated by validating the experimental findings against existing state-of-the-art approaches. When compared to existing approaches, such as Attention-based CNN and Ensemble ML model, the proposed model outperforms conventional machine learning techniques, demonstrating the efficacy of the CGRU architecture for diabetes prediction with a high accuracy rate o f 99.9%. Clustering algorithms are more beneficial as they help in identifying the subtle pattern in the dataset. When compared to other methods, it can lead to more accurate and reliable prediction. The study highlights how the cutting-edge CGRU model enhances the early detection and diagnosis of diabetes, which will eventually lead to improved healthcare outcomes. However, the study limits to work on diverse datasets, which is the only thing considered to be the drawback of this research.},
  archive      = {J_PEERJCS},
  author       = {Alhuseen Omar Alsayed and Nor Azman Ismail and Layla Hasan and Muhammad Binsawad and Farhat Embarak},
  doi          = {10.7717/peerj-cs.2642},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2642},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging a hybrid convolutional gated recursive diabetes prediction and severity grading model through a mobile app},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated essay scoring with SBERT embeddings and LSTM-attention networks. <em>PEERJCS</em>, <em>11</em>, e2634. (<a href='https://doi.org/10.7717/peerj-cs.2634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated essay scoring (AES) is essential in the field of educational technology, providing rapid and accurate evaluations of student writing. This study presents an innovative AES method that integrates Sentence-BERT (SBERT) with Long Short-Term Memory (LSTM) networks and attention mechanisms to improve the scoring process. SBERT generates embedding vectors for each essay, which are subsequently analyzed using a bidirectional LSTM (BiLSTM) to learn the features of these embedding vectors. An attention layer is introduced to enable the system to prioritize the most significant components of the essay. Evaluated using a benchmark dataset, our approach shows significant improvements in scoring accuracy, highlighting its ability to improve the reliability and efficiency of automated assessment systems.},
  archive      = {J_PEERJCS},
  author       = {Yuzhe Nie},
  doi          = {10.7717/peerj-cs.2634},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2634},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automated essay scoring with SBERT embeddings and LSTM-attention networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating machine learning models for predictive accuracy in cryptocurrency price forecasting. <em>PEERJCS</em>, <em>11</em>, e2626. (<a href='https://doi.org/10.7717/peerj-cs.2626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our research investigates the predictive performance and robustness of machine learning classification models and technical indicators for algorithmic trading in the volatile cryptocurrency market. The main aim is to identify reliable approaches for informed decision-making and profitable strategy development. With the increasing global adoption of cryptocurrency, robust trading models are essential for navigating its unique challenges and seizing investment opportunities. This study contributes to the field by offering a novel comparison of models, including logistic regression, random forest, and gradient boosting, under different data configurations and resampling techniques to address class imbalance. Historical data from cryptocurrency exchanges and data aggregators is collected, preprocessed, and used to train and evaluate these models. The impact of class imbalance, resampling techniques, and hyperparameter tuning on model performance is investigated. By analyzing historical cryptocurrency data, the methodology emphasizes hyperparameter tuning and backtesting, ensuring realistic model assessment. Results highlight the importance of addressing class imbalance and identify consistently outperforming models such as random forest, XGBoost, and gradient boosting. Our findings demonstrate that these models outperform others, indicating promising avenues for future research, particularly in sentiment analysis, reinforcement learning, and deep learning. This study provides valuable guidance for navigating the complex landscape of algorithmic trading in cryptocurrencies. By leveraging the findings and recommendations presented, practitioners can develop more robust and profitable trading strategies tailored to the unique characteristics of this emerging market.},
  archive      = {J_PEERJCS},
  author       = {Shavez Mushtaq Qureshi and Atif Saeed and Farooq Ahmad and Asad Rehman Khattak and Sultan H. Almotiri and Mohammed A. Al Ghamdi and Muhammad Shah Rukh},
  doi          = {10.7717/peerj-cs.2626},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2626},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evaluating machine learning models for predictive accuracy in cryptocurrency price forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the method reproducibility of deep learning models in biodiversity research. <em>PEERJCS</em>, <em>11</em>, e2618. (<a href='https://doi.org/10.7717/peerj-cs.2618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is revolutionizing biodiversity research by enabling advanced data analysis, species identification, and habitats monitoring, thereby enhancing conservation efforts. Ensuring reproducibility in AI-driven biodiversity research is crucial for fostering transparency, verifying results, and promoting the credibility of ecological findings. This study investigates the reproducibility of deep learning (DL) methods within the biodiversity research. We design a methodology for evaluating the reproducibility of biodiversity-related publications that employ DL techniques across three stages. We define ten variables essential for method reproducibility, divided into four categories: resource requirements, methodological information, uncontrolled randomness, and statistical considerations. These categories subsequently serve as the basis for defining different levels of reproducibility. We manually extract the availability of these variables from a curated dataset comprising 100 publications identified using the keywords provided by biodiversity experts. Our study shows that a dataset is shared in 50% of the publications; however, a significant number of the publications lack comprehensive information on deep learning methods, including details regarding randomness.},
  archive      = {J_PEERJCS},
  author       = {Waqas Ahmed and Vamsi Krishna Kommineni and Birgitta König-Ries and Jitendra Gaikwad and Luiz Gadelha and Sheeba Samuel},
  doi          = {10.7717/peerj-cs.2618},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2618},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evaluating the method reproducibility of deep learning models in biodiversity research},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling personalized and gamification-based cybersecurity risks within financial institutions. <em>PEERJCS</em>, <em>11</em>, e2598. (<a href='https://doi.org/10.7717/peerj-cs.2598'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gamification has emerged as a transformative e-business strategy, introducing innovative methods to engage customers and drive sales. This article explores the integration of game design principles into business contexts, termed “gamification,” a subject of increasing interest among both scholars and industry professionals. The discussion systematically addresses key themes, like the role of gamification in marketing strategies, enhancing website functionality, and its application within the financial sector, including e-banking, drawing insights from academic and industry perspectives. By conducting a systematic literature review of 48 academic articles published between 2015 and 2024, this study examines the use of personalized, gamification-based strategies to mitigate cyber threats in the financial domain. The review highlights the growing digitization of financial services and the corresponding rise in sophisticated cyber threats, including traditional attacks and advanced persistent threats (APTs). This article critically assesses the evolving landscape of cyber threats specific to the financial industry, identifying trends, challenges, and innovative solutions to strengthen cybersecurity practices. Of particular interest is the application of AI-enhanced gamification strategies to reinforce cybersecurity protocols, particularly in the face of novel threats in gaming platforms. Furthermore, the review evaluates techniques grounded in user behavior, motivation, and readiness to enhance cybersecurity. The article also offers a comprehensive taxonomy of financial services, categorizing cyber threats into game-based (e.g., phishing, malware, APTs) and non-game-based (e.g., social engineering, compliance issues) threats. AI-driven measures for prevention and detection emphasize regular security assessments, user training, and system monitoring with incident response plans. This research provides valuable insights into the intersection of gamification and cybersecurity, offering a forward-looking perspective for both academic researchers and industry professionals.},
  archive      = {J_PEERJCS},
  author       = {Amna Shahzadi and Kashif Ishaq and Naeem A. Nawaz and Fadhilah Rosdi and Fawad Ali Khan},
  doi          = {10.7717/peerj-cs.2598},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2598},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Unveiling personalized and gamification-based cybersecurity risks within financial institutions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Review of models for estimating 3D human pose using deep learning. <em>PEERJCS</em>, <em>11</em>, e2574. (<a href='https://doi.org/10.7717/peerj-cs.2574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) is designed to detect and localize various parts of the human body and represent them as a kinematic structure based on input data like images and videos. Three-dimensional (3D) HPE involves determining the positions of articulated joints in 3D space. Given its wide-ranging applications, HPE has become one of the fastest-growing areas in computer vision and artificial intelligence. This review highlights the latest advances in 3D deep-learning-based HPE models, addressing the major challenges such as accuracy, real-time performance, and data constraints. We assess the most widely used datasets and evaluation metrics, providing a comparison of leading algorithms in terms of precision and computational efficiency in tabular form. The review identifies key applications of HPE in industries like healthcare, security, and entertainment. Our findings suggest that while deep learning models have made significant strides, challenges in handling occlusion, real-time estimation, and generalization remain. This study also outlines future research directions, offering a roadmap for both new and experienced researchers to further develop 3D HPE models using deep learning.},
  archive      = {J_PEERJCS},
  author       = {Sani Salisu and Kamaluddeen Usman Danyaro and Maged Nasser and Israa M. Hayder and Hussain A. Younis},
  doi          = {10.7717/peerj-cs.2574},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2574},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Review of models for estimating 3D human pose using deep learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of machine learning on dietary and exercise behaviors in type 2 diabetes self-management: A systematic literature review. <em>PEERJCS</em>, <em>11</em>, e2568. (<a href='https://doi.org/10.7717/peerj-cs.2568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-awareness and self-management in diabetes are critical as they enhance patient well-being, decrease financial burden, and alleviate strain on healthcare systems by mitigating complications and promoting healthier life expectancy. Incomplete understanding persists regarding the synergistic effects of diet and exercise on diabetes management, as existing research often isolates these factors, creating a knowledge gap in comprehending their combined influence. Current diabetes research overlooks the interplay between diet and exercise in self-management. A holistic study is crucial to mitigate complications and healthcare burdens effectively. Multi-dimensional research questions covering complete diabetic management such as publication channels for diabetic research, existing machine learning solutions, physical activity tacking existing methods, and diabetic-associated datasets are included in this research. In this study, using a proper research protocol primary research articles related to diet, exercise, datasets, and blood analysis are selected and their quality is assessed for diabetic management. This study interrelates two major dimensions of diabetes management together that are diet and exercise.},
  archive      = {J_PEERJCS},
  author       = {Rizwan Riaz Mir and Nazeef Ul Haq and Kashif Ishaq and Nurhizam Safie and Abdul Basit Dogar},
  doi          = {10.7717/peerj-cs.2568},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2568},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Impact of machine learning on dietary and exercise behaviors in type 2 diabetes self-management: A systematic literature review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The technique of fuzzy analytic hierarchy process (FAHP) based on the triangular q-rung fuzzy numbers (TR-q-ROFNS) with applications in best african coffee brand selection. <em>PEERJCS</em>, <em>11</em>, e2555. (<a href='https://doi.org/10.7717/peerj-cs.2555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The African coffee market offers a rich and diverse range of coffee profiles. The coffee producers of Africa face numerous challenges like climate change, market fluctuations, diseases, soil degradation and limited access to finance. These challenges badly affect their productivity, quality and livelihood. There are different factors like social and cultural, which can affect the coffee production. This study aims to develop multi criteria decision making (MCDM) methods and their applications in coffee market specifically in identifying factors influencing consumers’ coffee brand preferences in South Africa, which is known for its vibrant coffee culture. For this purpose, first we developed the technique of analytic hierarchy process (AHP) in the environment of triangular q-rung orthopair fuzzy numbers. The triangular q-rung fuzzy numbers can effectively handle the uncertainity. The AHP technique has widely been used in decision making due to its flexibility in assigning weights and dealing with vagueness. The weights of critera plays a very important role in an MCDM problem. The development of AHP technique in triangular q-rung orthopair fuzzy environment can improve the decision making (DM) by handling vagueness in data and by using the most appropriate weights. Furthermore this new proposed method improves accuracy and minimize the information loss. The newly peoposed method is applied to different MCDM problems and comparative analysis is conducted to check the validity of results.},
  archive      = {J_PEERJCS},
  author       = {Yupei Huang and Muhammad Gulistan and Amir Rafique and Wathek Chammam and Khursheed Aurangzeb and Ateeq Ur Rehman},
  doi          = {10.7717/peerj-cs.2555},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2555},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The technique of fuzzy analytic hierarchy process (FAHP) based on the triangular q-rung fuzzy numbers (TR-q-ROFNS) with applications in best african coffee brand selection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-to-thing continuum-based sports monitoring system using machine learning and deep learning model. <em>PEERJCS</em>, <em>11</em>, e2539. (<a href='https://doi.org/10.7717/peerj-cs.2539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports monitoring and analysis have seen significant advancements by integrating cloud computing and continuum paradigms facilitated by machine learning and deep learning techniques. This study presents a novel approach for sports monitoring, specifically focusing on basketball, that seamlessly transitions from traditional cloud-based architectures to a continuum paradigm, enabling real-time analysis and insights into player performance and team dynamics. Leveraging machine learning and deep learning algorithms, our framework offers enhanced capabilities for player tracking, action recognition, and performance evaluation in various sports scenarios. The proposed Cloud-to-Thing continuum-based sports monitoring system utilizes advanced techniques such as Improved Mask R-CNN for pose estimation and a hybrid metaheuristic algorithm combined with a generative adversarial network (GAN) for classification. Our system significantly improves latency and accuracy, reducing latency to 5.1 ms and achieving an accuracy of 94.25%, which outperforms existing methods in the literature. These results highlight the system’s ability to provide real-time, precise, and scalable sports monitoring, enabling immediate feedback for time-sensitive applications. This research has significantly improved real-time sports event analysis, contributing to improved player performance evaluation, enhanced team strategies, and informed tactical adjustments.},
  archive      = {J_PEERJCS},
  author       = {Amal Alshardan and Hany Mahgoub and Saad Alahmari and Mohammed Alonazi and Radwa Marzouk and Abdullah Mohamed},
  doi          = {10.7717/peerj-cs.2539},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2539},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Cloud-to-thing continuum-based sports monitoring system using machine learning and deep learning model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RCE-IFE: Recursive cluster elimination with intra-cluster feature elimination. <em>PEERJCS</em>, <em>11</em>, e2528. (<a href='https://doi.org/10.7717/peerj-cs.2528'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational and interpretational difficulties caused by the ever-increasing dimensionality of biological data generated by new technologies pose a significant challenge. Feature selection (FS) methods aim to reduce the dimension, and feature grouping has emerged as a foundation for FS techniques that seek to detect strong correlations among features and identify irrelevant features. In this work, we propose the Recursive Cluster Elimination with Intra-Cluster Feature Elimination (RCE-IFE) method that utilizes feature grouping and iterates grouping and elimination steps in a supervised context. We assess dimensionality reduction and discriminatory capabilities of RCE-IFE on various high-dimensional datasets from different biological domains. For a set of gene expression, microRNA (miRNA) expression, and methylation datasets, the performance of RCE-IFE is comparatively evaluated with RCE-IFE-SVM (the SVM-adapted version of RCE-IFE) and SVM-RCE. On average, RCE-IFE attains an area under the curve (AUC) of 0.85 among tested expression datasets with the fewest features and the shortest running time, while RCE-IFE-SVM (the SVM-adapted version of RCE-IFE) and SVM-RCE achieve similar AUCs of 0.84 and 0.83, respectively. RCE-IFE and SVM-RCE yield AUCs of 0.79 and 0.68, respectively when averaged over seven different metagenomics datasets, with RCE-IFE significantly reducing feature subsets. Furthermore, RCE-IFE surpasses several state-of-the-art FS methods, such as Minimum Redundancy Maximum Relevance (MRMR), Fast Correlation-Based Filter (FCBF), Information Gain (IG), Conditional Mutual Information Maximization (CMIM), SelectKBest (SKB), and eXtreme Gradient Boosting (XGBoost), obtaining an average AUC of 0.76 on five gene expression datasets. Compared with a similar tool, Multi-stage, RCE-IFE gives a similar average accuracy rate of 89.27% using fewer features on four cancer-related datasets. The comparability of RCE-IFE is also verified with other biological domain knowledge-based Grouping-Scoring-Modeling (G-S-M) tools, including mirGediNET, 3Mint, and miRcorrNet. Additionally, the biological relevance of the selected features by RCE-IFE is evaluated. The proposed method also exhibits high consistency in terms of the selected features across multiple runs. Our experimental findings imply that RCE-IFE provides robust classifier performance and significantly reduces feature size while maintaining feature relevance and consistency.},
  archive      = {J_PEERJCS},
  author       = {Cihan Kuzudisli and Burcu Bakir-Gungor and Bahjat Qaqish and Malik Yousef},
  doi          = {10.7717/peerj-cs.2528},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2528},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RCE-IFE: Recursive cluster elimination with intra-cluster feature elimination},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tuck-KGC: Based on tensor decomposition for diabetes knowledge graph completion model integrating chinese and western medicine. <em>PEERJCS</em>, <em>11</em>, e2522. (<a href='https://doi.org/10.7717/peerj-cs.2522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The medical knowledge graph is essential for intelligent medical services, encompassing personalized diagnostics, precision therapies, and intelligent consultations, among others. However, medical knowledge graphs frequently suffer from incompleteness, primarily due to the absence of certain entities or relationships. The incomplete nature of knowledge graphs poses substantial challenges to these tasks. Knowledge graph completion technology is instrumental in addressing this issue. Specifically, tensor decomposition-based approaches for knowledge graph completion embed entities and relationships into the vector space, where tensor decomposition computations are employed to predict missing relationships within the knowledge graph. However, the tensor representation of entities and their relationships often overlooks crucial entity type information, potentially resulting in an abundance of irrational relationships during the prediction process. To mitigate this, we propose the Tucker Decomposition Knowledge Graph Completion (Tuck-KGC) method, which incorporates entity types into the tensor decomposition framework. This method maps the types of medical entities to vectors, which are seamlessly integrated into the knowledge graph representation learning process. This allows the model to thoroughly absorb entity information, thereby enhancing the accuracy of link prediction. To evaluate the Tuck-KGC, we built the Dia dataset, a knowledge graph tailored for precision medical analysis, which integrates both Traditional Chinese Medicine and Western medicine perspectives. The Dia dataset encompasses 10,294 entities with 214 relationships, covering a comprehensive spectrum including diseases, treatments, clinical manifestations, complications, etiology, and so on. Building upon the Dia dataset, experimental results indicate that the Tuck-KGC model boosts link prediction accuracy by roughly 8%, affirming the efficacy of incorporating entity type information into the model.},
  archive      = {J_PEERJCS},
  author       = {Jiangtao ZhangSun and Yu Xin Yang and Beiji Zou and Qinghua Peng and Xiao Xia Xiao},
  doi          = {10.7717/peerj-cs.2522},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2522},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Tuck-KGC: Based on tensor decomposition for diabetes knowledge graph completion model integrating chinese and western medicine},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diagnostic test accuracy of AI-assisted mammography for breast imaging: A narrative review. <em>PEERJCS</em>, <em>11</em>, e2476. (<a href='https://doi.org/10.7717/peerj-cs.2476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of artificial intelligence into healthcare, particularly in mammography, holds immense potential for improving breast cancer diagnosis. Artificial intelligence (AI), with its ability to process vast amounts of data and detect intricate patterns, offers a solution to the limitations of traditional mammography, including missed diagnoses and false positives. This review focuses on the diagnostic accuracy of AI-assisted mammography, synthesizing findings from studies across different clinical settings and algorithms. The motivation for this research lies in addressing the need for enhanced diagnostic tools in breast cancer screening, where early detection can significantly impact patient outcomes. Although AI models have shown promising improvements in sensitivity and specificity, challenges such as algorithmic bias, interpretability, and the generalizability of models across diverse populations remain. The review concludes that while AI holds transformative potential in breast cancer screening, collaborative efforts between radiologists, AI developers, and policymakers are crucial for ensuring ethical, reliable, and inclusive integration into clinical practice.},
  archive      = {J_PEERJCS},
  author       = {Daksh Dave and Adnan Akhunzada and Nikola Ivković and Sujan Gyawali and Korhan Cengiz and Adeel Ahmed and Ahmad Sami Al-Shamayleh},
  doi          = {10.7717/peerj-cs.2476},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2476},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Diagnostic test accuracy of AI-assisted mammography for breast imaging: A narrative review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing breast cancer prediction through stacking ensemble and deep learning integration. <em>PEERJCS</em>, <em>11</em>, e2461. (<a href='https://doi.org/10.7717/peerj-cs.2461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is one of the most common types of cancer in women and is recognized as a serious global public health issue. The increasing incidence of breast cancer emphasizes the importance of early detection, which enhances the effectiveness of treatment processes. In addressing this challenge, the importance of machine learning and deep learning technologies is increasingly recognized. The aim of this study is to evaluate the integration of ensemble models and deep learning models using stacking ensemble techniques on the Breast Cancer Wisconsin (Diagnostic) dataset and to enhance breast cancer diagnosis through this methodology. To achieve this, the efficacy of ensemble methods such as Random Forest, XGBoost, LightGBM, ExtraTrees, HistGradientBoosting, AdaBoost, GradientBoosting, and CatBoost in modeling breast cancer diagnosis was comprehensively evaluated. In addition to ensemble methods, deep learning models including convolutional neural network (CNN), recurrent neural network (RNN), gated recurrent unit (GRU), bidirectional long short-term memory (BILSTM), long short-term memory (LSTM) were analyzed as meta predictors. Among these models, CNN stood out for its high accuracy and rapid training time, making it an ideal choice for real-time diagnostic applications. Finally, the study demonstrated how breast cancer prediction was enhanced by integrating a set of base predictors, such as LightGBM, ExtraTrees, and CatBoost, with a deep learning-based meta-predictor, such as CNN, using stacking ensemble methodology. This stacking integration model offers significant potential for healthcare decision support systems with high accuracy, F1 score, and receiver operating characteristic area under the curve (ROC AUC), along with reduced training times. The results from this research offer important insights for enhancing decision-making strategies in the diagnosis and management of breast cancer.},
  archive      = {J_PEERJCS},
  author       = {Fatih Gurcan},
  doi          = {10.7717/peerj-cs.2461},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2461},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing breast cancer prediction through stacking ensemble and deep learning integration},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of GPT in promoting inclusive higher education for people with various learning disabilities: A review. <em>PEERJCS</em>, <em>11</em>, e2400. (<a href='https://doi.org/10.7717/peerj-cs.2400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generative pre-trained transformer (GPT) is a notable breakthrough in the field of artificial intelligence, as it empowers machines to effectively comprehend and engage in interactions with humans. The GPT exhibits the capacity to enhance inclusivity and accessibility for students with learning disabilities in the context of higher education, hence potentially facilitating substantial advancements in the field. GPT can provide personalized and diverse solutions that successfully cater to the distinct requirements of students with learning disabilities. This motivated us to conduct an extensive review to assess the effectiveness of GPT in enhancing accessibility and inclusivity in higher education for students with learning disabilities. This review offers a comprehensive analysis of the GPT and its significance for enhancing inclusivity in the field of higher education. In this research, we also examined the possible challenges and constraints associated with the integration of GPT into inclusive higher education, along with potential solutions. Overall, this review is intended for educators, students with and without learning disabilities, policymakers, higher education institutes, researchers, and educational technology developers. This review aims to provide a comprehensive understanding of GPT in promoting inclusive higher education for people with various learning disabilities, its impacts on inclusive higher education, emerging challenges, and potential solutions.},
  archive      = {J_PEERJCS},
  author       = {Thippa Reddy Gadekallu and Gokul Yenduri and Rajesh Kaluri and Dharmendra Singh Rajput and Kuruva Lakshmanna and Kai Fang and Junxin Chen and Wei Wang},
  doi          = {10.7717/peerj-cs.2400},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2400},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The role of GPT in promoting inclusive higher education for people with various learning disabilities: A review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the accuracy of soil texture determination using pH and electro conductivity values with ultrasound penetration-based digital soil texture analyzer. <em>PEERJCS</em>, <em>11</em>, e2663. (<a href='https://doi.org/10.7717/peerj-cs.2663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soil texture analysis is critical for advancing agricultural productivity, ensuring environmental sustainability, and maintaining ecosystem balance. Traditional sedimentation-based methods, such as the hydrometer technique, are fast and practical but prone to inaccuracies due to the effects of water-soluble substances. This study focuses on the practical framework of integrating pH (potential of hydrogen) and EC (electrical conductivity), as indicators of dissolved substances that influence soil texture estimation. Using the Ultrasound Penetration-based Digital Soil Texture Analyzer (USTA), this research combined ultrasound time series data with pH and EC measurements to predict sand, silt, and clay ratios through machine learning methods—support vector regression (SVR), Random Forest (RF), and multi-layer perceptron neural network (MLPNN). Simulations showed that RF yielded the best results, improving R2 values to 0.52, 0.33, and 0.31 for sand, silt, and clay, respectively. The enhanced model performance demonstrates the viability of integrating pH and EC with advanced machine learning techniques to improve soil texture analysis accuracy. These findings suggest that automated systems like USTA, with modular pH and EC sensors, can provide cost-effective, efficient alternatives to traditional methods, offering practical implications for soil management and agricultural optimization.},
  archive      = {J_PEERJCS},
  author       = {Emre Kilinc and Umut Orhan},
  doi          = {10.7717/peerj-cs.2663},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2663},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving the accuracy of soil texture determination using pH and electro conductivity values with ultrasound penetration-based digital soil texture analyzer},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TurkMedNLI: A turkish medical natural language inference dataset through large language model based translation. <em>PEERJCS</em>, <em>11</em>, e2662. (<a href='https://doi.org/10.7717/peerj-cs.2662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language inference (NLI) is a subfield of natural language processing (NLP) that aims to identify the contextual relationship between premise and hypothesis sentences. While high-resource languages like English benefit from robust and rich NLI datasets, creating similar datasets for low-resource languages is challenging due to the cost and complexity of manual annotation. Although translation of existing datasets offers a practical solution, direct translation of domain-specific datasets presents unique challenges, particularly in handling abbreviations, metric conversions, and cultural alignment. This study introduces a pipeline for translating a medical NLI dataset into Turkish, which is a low-resource language. Our approach employs fine-tuning the Llama-3.1 model with selected samples from the Medical Abbreviation dataset (MeDAL) to extract and resolve medical abbreviations. Consequently, NLI pairs are refined with extracted abbreviations and subjected to metric correction. Later, the processed sentences are then translated using Facebook’s No Language Left Behind (NLLB) translation model. To ensure quality, we conducted comprehensive evaluations using both machine learning models and medical expert review. Our results show that BERTurk achieved 75.17% accuracy on TurkMedNLI test data and 76.30% on the normalized test set, while BioBERTurk demonstrated comparable performance with 75.59% accuracy on test data and 72.29% on the normalized dataset. Medical experts further validated the translations through manual assessment of sampled sentences. This work demonstrates the effectiveness of large language models in adapting domain-specific datasets for low-resource languages, establishing a foundation for future research in multilingual biomedical NLP.},
  archive      = {J_PEERJCS},
  author       = {İskender Ülgen Oğul and Fatih Soygazi and Belgin Ergenç Bostanoğlu},
  doi          = {10.7717/peerj-cs.2662},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2662},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TurkMedNLI: A turkish medical natural language inference dataset through large language model based translation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for enhanced risk management: A novel approach to analyzing financial reports. <em>PEERJCS</em>, <em>11</em>, e2661. (<a href='https://doi.org/10.7717/peerj-cs.2661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk management is a critical component of today’s financial environment because of the enormity and complexity of data contained in financial statements. Business situations, plans, and schedule risk assessment with the help of conventional ways which involve analytical, technical, and heuristic models are inadequate to address the complex structures of the latest data. This research brings out the Hybrid Financial Risk Predictor (HFRP) model, using the convolutional neural networks (CNN) and long-short term memory (LSTM) networks to improve financial risk prediction. A combination of quantitative and qualitative ratings derived from the analysis of financial texts results in high accuracy and stability compared with the HFRP model. Evaluating key findings, the quantity of training & testing loss decreased considerably and they have their final value as 0.0013 and 0.003, respectively. According to the hypothesis, the selected HFRP model demonstrates the values of the revenue, net income, and earnings per share (EPS), and are closely similar to the actual values. The model achieves substantial risk mitigation: credit risk lowered from 0.75 to 0.20, liquidity risk from 0.70 to 0.25, market risk from 0.65 to 0.30, while operational risk is at 0.80 to 0.35. By analyzing the results of the HFRP model, it can be stated that the proposal promotes improved financial stability and presents a reliable model for the contemporary financial markets, which in turn helps in making sound decisions and improve the assessment of risks.},
  archive      = {J_PEERJCS},
  author       = {Xiangting Shi and Yakang Zhang and Manning Yu and Lihao Zhang},
  doi          = {10.7717/peerj-cs.2661},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2661},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning for enhanced risk management: A novel approach to analyzing financial reports},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient skyline query processing with user-specified conditional preference. <em>PEERJCS</em>, <em>11</em>, e2659. (<a href='https://doi.org/10.7717/peerj-cs.2659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of multi-attribute decision-making, the utilization of skyline queries has gained increasing popularity for assisting users in identifying objects with optimal attribute combinations. With the growing demand for personalization, integrating user’s preferences into skyline queries has emerged as an intriguing and promising research direction. However, the diverse expressions of preferences pose challenges to existing personalized skyline queries. Current methods assume that user preferences are too simplistic and do not represent the interdependencies between attributes. This poses a challenge to the existing skyline methods in effectively managing complex user preferences and dependencies. In this article, we propose an innovative and efficient method for skyline query processing, leveraging conditional preference networks (CP-Nets) to integrate specific user’s conditional preferences into the query process, termed as CP-Skyline. Firstly, we introduce a user-defined conditional preference model based on CP-Nets. By integrating user’s conditional preference information, we prune the candidate dataset, effectively compressing the query space. Secondly, we define a new dominance relation for CP-Skyline computation. Finally, extensive experiments were conducted on both synthetic and real-world datasets to assess the performance and effectiveness of the proposed methods. The experimental results unequivocally demonstrate a significant enhancement in skyline quality, and it presents a practical and potent solution for personalized decision support.},
  archive      = {J_PEERJCS},
  author       = {Senfu Ke and Xiaodong Fu and Jie Li},
  doi          = {10.7717/peerj-cs.2659},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2659},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Efficient skyline query processing with user-specified conditional preference},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing convolutional neural network based deep learning systems: A statistical metamorphic approach. <em>PEERJCS</em>, <em>11</em>, e2658. (<a href='https://doi.org/10.7717/peerj-cs.2658'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning technology spans many areas and today plays a significant role in addressing a wide range of problems in critical domains, i.e., healthcare, autonomous driving, finance, manufacturing, cybersecurity, etc. Metamorphic testing (MT) is considered a simple but very powerful approach in testing such computationally complex systems for which either an oracle is not available or is available but difficult to apply. Conventional metamorphic testing techniques have certain limitations in verifying deep learning-based models (i.e., convolutional neural networks (CNNs)) that have a stochastic nature (because of randomly initializing the network weights) in their training. In this article, we attempt to address this problem by using a statistical metamorphic testing (SMT) technique that does not require software testers to worry about fixing the random seeds (to get deterministic results) to verify the metamorphic relations (MRs). We propose seven MRs combined with different statistical methods to statistically verify whether the program under test adheres to the relation(s) specified in the MR(s). We further use mutation testing techniques to show the usefulness of the proposed approach in the healthcare space and test two CNN-based deep learning models (used for pneumonia detection among patients). The empirical results show that our proposed approach uncovers 85.71% of the implementation faults in the classifiers under test (CUT). Furthermore, we also propose an MRs minimization algorithm for the CUT, thus saving computational costs and organizational testing resources.},
  archive      = {J_PEERJCS},
  author       = {Faqeer ur Rehman and Clemente Izurieta},
  doi          = {10.7717/peerj-cs.2658},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2658},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Testing convolutional neural network based deep learning systems: A statistical metamorphic approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying auxiliary supervised depth-assisted transformer and cross modal attention fusion in monocular 3D object detection. <em>PEERJCS</em>, <em>11</em>, e2656. (<a href='https://doi.org/10.7717/peerj-cs.2656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular 3D object detection is the most widely applied and challenging solution for autonomous driving, due to 2D images lacking 3D information. Existing methods are limited by inaccurate depth estimations by inequivalent supervised targets. The use of both depth and visual features also faces problems of heterogeneous fusion. In this article, we propose Depth Detection Transformer (Depth-DETR), applying auxiliary supervised depth-assisted transformer and cross modal attention fusion in monocular 3D object detection. Depth-DETR introduces two additional depth encoders besides the visual encoder. Two depth encoders are supervised by ground truth depth and bounding box respectively, working independently to complement each other’s limitations and predicting more accurate target distances. Furthermore, Depth-DETR employs cross modal attention mechanisms to effectively fuse three different features. A parallel structure of two cross modal transformer is applied to fuse two depth features with visual features. Avoiding early fusion between two depth features enhances the final fused feature for better feature representations. Through multiple experimental validations, the Depth-DETR model has achieved highly competitive results in the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) dataset, with an AP score of 17.49, representing its outstanding performance in 3D object detection.},
  archive      = {J_PEERJCS},
  author       = {Zhijian Wang and Jie Liu and Yixiao Sun and Xiang Zhou and Boyan Sun and Dehong Kong and Jay Xu and Xiaoping Yue and Wenyu Zhang},
  doi          = {10.7717/peerj-cs.2656},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2656},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Applying auxiliary supervised depth-assisted transformer and cross modal attention fusion in monocular 3D object detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of deep learning techniques for apple leaf diseases classification and detection. <em>PEERJCS</em>, <em>11</em>, e2655. (<a href='https://doi.org/10.7717/peerj-cs.2655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture sustains populations and provides livelihoods, contributing to socioeconomic growth. Apples are one of the most popular fruits and contains various antioxidants that reduce the risk of chronic diseases. Additionally, they are low in calories, making them a healthy snack option for all ages. However, several factors can adversely affect apple production. These issues include diseases that drastically lower yield and quality and cause farmers to lose millions of dollars. To minimize yield loss and economic effects, it is essential to diagnose apple leaf diseases accurately and promptly. This allows targeted pesticide and insecticide use. However, farmers find it difficult to distinguish between different apple leaf diseases since their symptoms are quite similar. Computer vision applications have become an effective tool in recent years for handling these issues. They can provide accurate disease detection and classification through massive image datasets. This research analyzes and evaluates datasets, deep learning methods and frameworks built for apple leaf disease detection and classification. A systematic analysis of 45 articles published between 2016 and 2024 was conducted to evaluate the latest developments, approaches, and research needs in this area.},
  archive      = {J_PEERJCS},
  author       = {Assad Souleyman Doutoum and Bulent Tugrul},
  doi          = {10.7717/peerj-cs.2655},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2655},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A systematic review of deep learning techniques for apple leaf diseases classification and detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quality assessment algorithm for no-reference images based on transfer learning. <em>PEERJCS</em>, <em>11</em>, e2654. (<a href='https://doi.org/10.7717/peerj-cs.2654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image quality assessment (IQA) plays a critical role in automatically detecting and correcting defects in images, thereby enhancing the overall performance of image processing and transmission systems. While research on reference-based IQA is well-established, studies on no-reference image IQA remain underdeveloped. In this article, we propose a novel no-reference IQA algorithm based on transfer learning (IQA-NRTL). This algorithm leverages a deep convolutional neural network (CNN) due to its ability to effectively capture multi-scale semantic information features, which are essential for representing the complex visual perception in images. These features are extracted through a visual perception module. Subsequently, an adaptive fusion network integrates these features, and a fully connected regression network correlates the fused semantic information with global semantic information to perform the final quality assessment. Experimental results on authentically distorted datasets (KonIQ-10k, BIQ2021), synthetically distorted datasets (LIVE, TID2013), and an artificial intelligence (AI)-generated content dataset (AGIQA-1K) show that the proposed IQA-NRTL algorithm significantly improves performance compared to mainstream no-reference IQA algorithms, depending on variations in image content and complexity.},
  archive      = {J_PEERJCS},
  author       = {Yang Yang and Chang Liu and Hui Wu and Dingguo Yu},
  doi          = {10.7717/peerj-cs.2654},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2654},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A quality assessment algorithm for no-reference images based on transfer learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid blockchain-based solution for secure sharing of electronic medical record data. <em>PEERJCS</em>, <em>11</em>, e2653. (<a href='https://doi.org/10.7717/peerj-cs.2653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient privacy data security is a pivotal area of research within the burgeoning field of smart healthcare. This study proposes an innovative hybrid blockchain-based framework for the secure sharing of electronic medical record (EMR) data. Unlike traditional privacy protection schemes, our approach employs a novel tripartite blockchain architecture that segregates healthcare data across distinct blockchains for patients and healthcare providers while introducing a separate social blockchain to enable privacy-preserving data sharing with authorized external entities. This structure enhances both security and transparency while fostering collaborative efforts across different stakeholders. To address the inherent complexity of managing multiple blockchains, a unique cross-chain signature algorithm is introduced, based on the Boneh-Lynn-Shacham (BLS) signature aggregation technique. This algorithm not only streamlines the signature process across chains but also strengthens system security and optimizes storage efficiency, addressing a key challenge in multi-chain systems. Additionally, our external sharing algorithm resolves the prevalent issue of medical data silos by facilitating better data categorization and enabling selective, secure external sharing through the social blockchain. Security analyses and experimental results demonstrate that the proposed scheme offers superior security, storage optimization, and flexibility compared to existing solutions, making it a robust choice for safeguarding patient data in smart healthcare environments.},
  archive      = {J_PEERJCS},
  author       = {Gang Han and Yan Ma and Zhongliang Zhang and Yuxin Wang},
  doi          = {10.7717/peerj-cs.2653},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2653},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid blockchain-based solution for secure sharing of electronic medical record data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel transfer learning approach for hand drawn mathematical geometric shapes classification. <em>PEERJCS</em>, <em>11</em>, e2652. (<a href='https://doi.org/10.7717/peerj-cs.2652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand-drawn mathematical geometric shapes are geometric figures, such as circles, triangles, squares, and polygons, sketched manually using pen and paper or digital tools. These shapes are fundamental in mathematics education and geometric problem-solving, serving as intuitive visual aids for understanding complex concepts and theories. Recognizing hand-drawn shapes accurately enables more efficient digitization of handwritten notes, enhances educational tools, and improves user interaction with mathematical software. This research proposes an innovative machine learning algorithm for the automatic classification of mathematical geometric shapes to identify and interpret these shapes from handwritten input, facilitating seamless integration with digital systems. We utilized a benchmark dataset of mathematical shapes based on a total of 20,000 images with eight classes circle, kite, parallelogram, square, rectangle, rhombus, trapezoid, and triangle. We introduced a novel machine-learning algorithm CnN-RFc that uses convolution neural networks (CNN) for spatial feature extraction and the random forest classifier for probabilistic feature extraction from image data. Experimental results illustrate that using the CnN-RFc method, the Light Gradient Boosting Machine (LGBM) algorithm surpasses state-of-the-art approaches with high accuracy scores of 98% for hand-drawn shape classification. Applications of the proposed mathematical geometric shape classification algorithm span various domains, including education, where it enhances interactive learning platforms and provides instant feedback to students.},
  archive      = {J_PEERJCS},
  author       = {Aneeza Alam and Ali Raza and Nisrean Thalji and Laith Abualigah and Helena Garay and Josep Alemany-Iturriaga and Imran Ashraf},
  doi          = {10.7717/peerj-cs.2652},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2652},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Novel transfer learning approach for hand drawn mathematical geometric shapes classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved BCI calibration in multimodal emotion recognition using heterogeneous adversarial transfer learning. <em>PEERJCS</em>, <em>11</em>, e2649. (<a href='https://doi.org/10.7717/peerj-cs.2649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of brain-computer interface (BCI) technology to identify emotional states has gained significant interest, especially with the rise of virtual reality (VR) applications. However, the extensive calibration required for precise emotion recognition models presents a significant challenge, particularly for sensitive groups such as children, elderly, and patients. This study presents a novel approach that utilizes heterogeneous adversarial transfer learning (HATL) to synthesize electroencephalography (EEG) data from various other signal modalities, reducing the need for lengthy calibration phases. We benchmark the efficacy of three generative adversarial network (GAN) architectures, such as conditional GAN (CGAN), conditional Wasserstein GAN (CWGAN), and CWGAN with gradient penalty (CWGAN-GP) within this framework. The proposed framework is rigorously tested on two conventional open sourced datasets, SEED-V and DEAP. Additionally, the framework was applied to an immersive three-dimensional (3D) dataset named GraffitiVR, which we collected to capture the emotional and behavioral reactions of individuals experiencing urban graffiti in a VR environment. This expanded application provides insights into emotion recognition frameworks in VR settings, providing a wider range of contexts for assessing our methodology. When the accuracy of emotion recognition classifiers trained with CWGAN-GP-generated EEG data combined with non-EEG sensory data was compared against those trained using a combination of real EEG and non-EEG sensory data, the accuracy ratios were 93% on the SEED-V dataset, 99% on the DEAP dataset, and 97% on the GraffitiVR dataset. Moreover, in the GraffitiVR dataset, using CWGAN-GP-generated EEG data with non-EEG sensory data for emotion recognition models resulted in up to a 30% reduction in calibration time compared to classifiers trained on real EEG data with non-EEG sensory data. These results underscore the robustness and versatility of the proposed approach, significantly enhancing emotion recognition processes across a variety of environmental settings.},
  archive      = {J_PEERJCS},
  author       = {Mehmet Ali Sarikaya and Gökhan Ince},
  doi          = {10.7717/peerj-cs.2649},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2649},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improved BCI calibration in multimodal emotion recognition using heterogeneous adversarial transfer learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble graph auto-encoders for clustering and link prediction. <em>PEERJCS</em>, <em>11</em>, e2648. (<a href='https://doi.org/10.7717/peerj-cs.2648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph auto-encoders are a crucial research area within graph neural networks, commonly employed for generating graph embeddings while minimizing errors in unsupervised learning. Traditional graph auto-encoders focus on reconstructing minimal graph data loss to encode neighborhood information for each node, yielding node embedding representations. However, existing graph auto-encoder models often overlook node representations and fail to capture contextual node information within the graph data, resulting in poor embedding effects. Accordingly, this study proposes the ensemble graph auto-encoders (E-GAE) model. It utilizes the ensemble random walk graph auto-encoder, the random walk graph auto-encoder of the ensemble network, and the graph attention auto-encoder to generate three node embedding matrices Z. Then, these techniques are combined using adaptive weights to reconstruct a new node embedding matrix. This method addresses the problem of low-quality embeddings. The model’s performance is evaluated using three publicly available datasets (Cora, Citeseer, and PubMed), indicating its effectiveness through multiple experiments. It achieves up to a 2.0% improvement in the link prediction task and a 9.4% enhancement in the clustering task. Our code for this work can be found at https://github.com/xcgydfjjjderg/graphautoencoder.},
  archive      = {J_PEERJCS},
  author       = {Chengxin Xie and Jingui Huang and Yongjiang Shi and Hui Pang and Liting Gao and Xiumei Wen},
  doi          = {10.7717/peerj-cs.2648},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2648},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Ensemble graph auto-encoders for clustering and link prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain enabled policy-based access control mechanism to restrict unauthorized access to electronic health records. <em>PEERJCS</em>, <em>11</em>, e2647. (<a href='https://doi.org/10.7717/peerj-cs.2647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health record transmission and storage involve sensitive information, requiring robust security measures to ensure access is limited to authorized personnel. In the existing state of the art, there is a growing need for efficient access control approaches for the secure accessibility of patient health data by sustainable electronic health records. Locking medical data in a healthcare center forms information isolation; thus, setting up healthcare data exchange platforms is a driving force behind electronic healthcare centers. The healthcare entities access rights like subject, controller, and requester are defined and regulated by access control policies as defined by the General Data Protection Regulation (GDPR). In this work, we have introduced a blend of policy-based access control (PBAC) system backed by blockchain technology, where smart contracts govern the intrinsic part of security and privacy. As a result, any Subject can know at any time who currently has the right to access his data. The PBAC grants access to electronic health records based on predefined policies. Our proposed PBAC approach employs policies in which the subject, controller, and requester can grant access, revoke access, and check logs and actions made in a particular healthcare system. Smart contracts dynamically enforce access control policies and manage access permissions, ensuring that sensitive data is available only to authorized users. Delineating the proposed access control system and comparing it to other systems demonstrates that our approach is more adaptable to various healthcare data protection scenarios where there is a need to share sensitive data simultaneously and a robust need to safeguard the rights of the involved entities.},
  archive      = {J_PEERJCS},
  author       = {Nadeem Yaqub and Jianbiao Zhang and Muhammad Irfan Khalid and Weiru Wang and Markus Helfert and Mansoor Ahmed and Jungsuk Kim},
  doi          = {10.7717/peerj-cs.2647},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2647},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Blockchain enabled policy-based access control mechanism to restrict unauthorized access to electronic health records},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative study of the performance of ten metaheuristic algorithms for parameter estimation of solar photovoltaic models. <em>PEERJCS</em>, <em>11</em>, e2646. (<a href='https://doi.org/10.7717/peerj-cs.2646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study conducts a comparative analysis of the performance of ten novel and well-performing metaheuristic algorithms for parameter estimation of solar photovoltaic models. This optimization problem involves accurately identifying parameters that reflect the complex and nonlinear behaviours of photovoltaic cells affected by changing environmental conditions and material inconsistencies. This estimation is challenging due to computational complexity and the risk of optimization errors, which can hinder reliable performance predictions. The algorithms evaluated include the Crayfish Optimization Algorithm, the Golf Optimization Algorithm, the Coati Optimization Algorithm, the Crested Porcupine Optimizer, the Growth Optimizer, the Artificial Protozoa Optimizer, the Secretary Bird Optimization Algorithm, the Mother Optimization Algorithm, the Election Optimizer Algorithm, and the Technical and Vocational Education and Training-Based Optimizer. These algorithms are applied to solve four well-established photovoltaic models: the single-diode model, the double-diode model, the triple-diode model, and different photovoltaic module models. The study focuses on key performance metrics such as execution time, number of function evaluations, and solution optimality. The results reveal significant differences in the efficiency and accuracy of the algorithms, with some algorithms demonstrating superior performance in specific models. The Friedman test was utilized to rank the performance of the various algorithms, revealing the Growth Optimizer as the top performer across all the considered models. This optimizer achieved a root mean square error of 9.8602187789E−04 for the single-diode model, 9.8248487610E−04 for both the double-diode and triple-diode models and 1.2307306856E−02 for the photovoltaic module model. This consistent success indicates that the Growth Optimizer is a strong contender for future enhancements aimed at further boosting its efficiency and effectiveness. Its current performance suggests significant potential for improvement, making it a promising focus for ongoing development efforts. The findings contribute to the understanding of the applicability and performance of metaheuristic algorithms in renewable energy systems, providing valuable insights for optimizing photovoltaic models.},
  archive      = {J_PEERJCS},
  author       = {Adel Zga and Farouq Zitouni and Saad Harous and Karam Sallam and Abdulaziz S. Almazyad and Guojiang Xiong and Ali Wagdy Mohamed},
  doi          = {10.7717/peerj-cs.2646},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2646},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A comparative study of the performance of ten metaheuristic algorithms for parameter estimation of solar photovoltaic models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid parking space prediction model: Integrating ARIMA, long short-term memory (LSTM), and backpropagation neural network (BPNN) for smart city development. <em>PEERJCS</em>, <em>11</em>, e2645. (<a href='https://doi.org/10.7717/peerj-cs.2645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parking space prediction is a significant aspect of smart cities. It is essential for addressing traffic congestion challenges and low parking availability in urban areas. The present research mainly focuses on proposing a novel scalable hybrid model for accurately predicting parking space. The proposed model works in two phases: in first phase, auto-regressive integrated moving average (ARIMA) and long short-term memory (LSTM) models are integrated. Further, in second phase, backpropagation neural network (BPNN) is used to improve the accuracy of parking space prediction by reducing number of errors. The model utilizes the ARIMA model for handling linear values and the LSTM model for targeting non-linear values of the dataset. The Melbourne Internet of Things (IoT) based dataset, is used for implementing the proposed hybrid model. It consists of the data collected from the sensors that are employed in smart parking areas of the city. Before analysis, data was pre-processed to remove noise from the dataset and real time information collected from different sensors to predict the results accurately. The proposed hybrid model achieves the minimum mean squared error (MSE), mean absolute error (MAE), and root mean squared error (RMSE) values of 0.32, 0.48, and 0.56, respectively. Further, to verify the generalizability of the proposed hybrid model, it is also implemented on the Harvard IoT-based dataset. It achieves the minimum MSE, MAE, and RMSE values of 0.31, 0.47, and 0.56, respectively. Therefore, the proposed hybrid model outperforms both datasets by achieving minimum error, even when compared with the performance of other existing models. The proposed hybrid model can potentially improve parking space prediction, contributing to sustainable and economical smart cities and enhancing the quality of life for citizens.},
  archive      = {J_PEERJCS},
  author       = {Anchal Dahiya and Pooja Mittal and Yogesh Kumar Sharma and Umesh Kumar Lilhore and Sarita Simaiya and Mohd Anul Haq and Mohammed A. Aleisa and Abdullah Alenizi},
  doi          = {10.7717/peerj-cs.2645},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2645},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hybrid parking space prediction model: Integrating ARIMA, long short-term memory (LSTM), and backpropagation neural network (BPNN) for smart city development},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social media network public opinion emotion classification method based on multi-feature fusion and multi-scale hybrid neural network. <em>PEERJCS</em>, <em>11</em>, e2643. (<a href='https://doi.org/10.7717/peerj-cs.2643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the internet, an increasing number of users express their subjective opinions on social media platforms. By analyzing the sentiment of these texts, we can gain insights into public sentiment, industry changes, and market trends, enabling timely adjustments and preemptive strategies. This article initially constructs vectors using semantic fusion and word order features. Subsequently, it develops a lexicon vector based on word similarity and leverages supervised corpora training to obtain a more pronounced transfer weight vector of sentiment intensity. A multi-feature fused emotional word vector is ultimately formed by concatenating and fusing these weighted transfer vectors. Experimental comparisons on two multi-class microblog comment datasets demonstrate that the multi-feature fusion (WOOSD-CNN) word vector model achieves notable improvements in sentiment polarity accuracy and categorization effectiveness. Additionally, for aspect-level sentiment analysis of user generated content (UGC) text, a unified learning framework based on an information interaction channel is proposed, which enables the team productivity center (TPC) task. Specifically, an information interaction channel is designed to assist the model in leveraging the latent interactive characteristics of text. An in-depth analysis addresses the label drift phenomenon between aspect term words, and a position-aware module is constructed to mitigate the local development plan (LDP) issue.},
  archive      = {J_PEERJCS},
  author       = {Yuan Yao and Xi Chen and Peng Zhang},
  doi          = {10.7717/peerj-cs.2643},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2643},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Social media network public opinion emotion classification method based on multi-feature fusion and multi-scale hybrid neural network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure software development: Leveraging application call graphs to detect security vulnerabilities. <em>PEERJCS</em>, <em>11</em>, e2641. (<a href='https://doi.org/10.7717/peerj-cs.2641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inconsistency in software development standards frequently leads to vulnerabilities that can jeopardize an application’s cryptographic integrity. This situation can result in incomplete or flawed encryption processes. Vulnerabilities may manifest as missing, bypassed, or improperly executed encryption functions or the absence of critical cryptographic mechanisms, which eventually weaken security goals. This article introduces a thorough method for detecting vulnerabilities using dynamic and static analysis, focusing on a cryptographic function dominance tree. This strategy systematically minimizes the likelihood of integrity breaches in cryptographic applications. A layered and modular model is developed to maintain integrity by mapping the entire flow of cryptographic function calls across various components. The cryptographic function call graph and dominance tree are extracted and subsequently analyzed using an integrated dynamic and static technique. The extracted information undergoes strict evaluation against the anticipated function call sequence in the relevant cryptographic module to identify and localize potential security issues. Experimental findings demonstrate that the proposed method considerably enhances the accuracy and comprehensiveness of vulnerability detection in cryptographic applications, improving implementation security and resilience against misuse vulnerabilities.},
  archive      = {J_PEERJCS},
  author       = {Lei Yan and Guanghuai Zhao and Xiaohui Li and Pengxuan Sun},
  doi          = {10.7717/peerj-cs.2641},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2641},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Secure software development: Leveraging application call graphs to detect security vulnerabilities},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning with semantic ambiguity for unbiased scene graph generation. <em>PEERJCS</em>, <em>11</em>, e2639. (<a href='https://doi.org/10.7717/peerj-cs.2639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation (SGG) aims to identify and extract objects from images and elucidate their interrelations. This task faces two primary challenges. Firstly, the long-tail distribution of relation categories causes SGG models to favor high-frequency relations, such as “on” and “in”. Secondly, some subject-object pairs may have multiple reasonable relations, which often possess a certain degree of semantic similarity. However, the use of one-hot ground-truth relation labels does not effectively represent the semantic similarities and distinctions among relations. In response to these challenges, we propose a model-agnostic method named Mixup and Balanced Relation Learning (MBRL). This method assigns soft labels to samples exhibiting semantic ambiguities and optimizes model training by adjusting the loss weights for fine-grained and low-frequency relation samples. Its model-agnostic design facilitates seamless integration with diverse SGG models, enhancing their performance across various relation categories. Our approach is evaluated on widely-used datasets, including Visual Genome and Generalized Question Answering, both with over 100,000 images, providing rich visual contexts for scene graph model evaluation. Experimental results show that our method outperforms state-of-the-art approaches on multiple scene graph generation tasks, demonstrating significant improvements in both relation prediction accuracy and the handling of imbalanced data distributions.},
  archive      = {J_PEERJCS},
  author       = {Shanjin Zhong and Yang Cao and Qiaosen Chen and Jie Gong},
  doi          = {10.7717/peerj-cs.2639},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2639},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Learning with semantic ambiguity for unbiased scene graph generation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and implementation of an intelligent sports management system (ISMS) using wireless sensor networks. <em>PEERJCS</em>, <em>11</em>, e2637. (<a href='https://doi.org/10.7717/peerj-cs.2637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, growth in technology has significantly impacted various industries, including sports, health, e-commerce, and agriculture. Among these industries, the sports sector is experiencing significant transformation, which needs support in accurately monitoring athlete predicting and performance injuries arising due to traditional methods’ limitations. Keeping the above in mind, in this article, we present the Intelligent Sports Management System (ISMS) with the integration of wireless sensor networks (WSNs) and neural networks (NNs), which enhance athlete monitoring and injury prediction. Our proposed ISMS consists of several layers: user interface, business logic layer, data management layer, integration layer, analytics and AI layer, IoT layer, and security layer. To facilitate interactions for athletes, coaches, and administrators, our planned ISMS integrates a user-friendly interface accessible through web and mobile applications. Besides, scheduling and event management are managed by the business logic layer. Similarly, the data management layer can process and store comprehensive data from various sources. To ensure smooth data exchange, the integration layer connects the ISMS with third-party services, and the analytics and AI layer leverages machine learning to provide actionable insights on performance and outcomes. In addition, the IoT layer collects real-time data from sensors and wearable devices, which is essential for performance analysis and injury prevention. Finally, the security layer ensures data integrity and confidentiality with robust encryption and access controls. To evaluate the system performance in different scenarios, we performed many experiments, which show that the proposed ISMS model shows the system efficacy in improving accuracy (0.94), specificity (0.97), recall (0.91), precision (0.93), F1 score (0.95), mean absolute error (MAE) (0.6), mean square error (MSE) (0.8), and root mean square error (RMSE) (0.9), compared to traditional methods. From these results, it is clear that our suggested approach improves athlete performance monitoring, injury prevention plans, and training schedules by presenting a complete and novel solution for recent sports management.},
  archive      = {J_PEERJCS},
  author       = {ZhiGuo Zhu},
  doi          = {10.7717/peerj-cs.2637},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2637},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design and implementation of an intelligent sports management system (ISMS) using wireless sensor networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UltraTimTrack: A kalman-filter-based algorithm to track muscle fascicles in ultrasound image sequences. <em>PEERJCS</em>, <em>11</em>, e2636. (<a href='https://doi.org/10.7717/peerj-cs.2636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Brightness-mode (B-mode) ultrasound is a valuable tool to non-invasively image skeletal muscle architectural changes during movement, but automatically tracking muscle fascicles remains a major challenge. Existing fascicle tracking algorithms either require time-consuming drift corrections or yield noisy estimates that require post-processing. We therefore aimed to develop an algorithm that tracks fascicles without drift and with low noise across a range of experimental conditions and image acquisition settings. Methods We applied a Kalman filter to combine fascicle length and fascicle angle estimates from existing and openly-available UltraTrack and TimTrack algorithms into a hybrid algorithm called UltraTimTrack. We applied the hybrid algorithm to ultrasound image sequences collected from the human medial gastrocnemius of healthy individuals (N = 8, four women), who performed cyclical submaximal plantar flexion contractions or remained at rest during passive ankle joint rotations at given frequencies and amplitudes whilst seated in a dynamometer chair. We quantified the algorithm’s tracking accuracy, noise, and drift as the respective mean, cycle-to-cycle variability, and accumulated between-contraction variability in fascicle length and fascicle angle. We expected UltraTimTrack’s estimates to be less noisy than TimTrack’s estimates and to drift less than UltraTrack’s estimates across a range of conditions and image acquisition settings. Results The proposed algorithm yielded low-noise estimates like UltraTrack and was drift-free like TimTrack across the broad range of conditions we tested. Over 120 cyclical contractions, fascicle length and fascicle angle deviations of UltraTimTrack accumulated to 2.1 ± 1.3 mm (mean ± sd) and 0.8 ± 0.7 deg, respectively. This was considerably less than UltraTrack (67.0 ± 59.3 mm, 9.3 ± 8.6 deg) and similar to TimTrack (1.9 ± 2.2 mm, 0.9 ± 1.0 deg). Average cycle-to-cycle variability of UltraTimTrack was 1.4 ± 0.4 mm and 0.6 ± 0.3 deg, which was similar to UltraTrack (1.1 ± 0.3 mm, 0.5 ± 0.1 deg) and less than TimTrack (3.5 ± 1.0 mm, 1.4 ± 0.5 deg). UltraTimTrack was less affected by experimental conditions and image acquisition settings than its parent algorithms. It also yielded similar or lower root-mean-square deviations from manual tracking for previously published image sequences (fascicle length: 2.3–2.6 mm, fascicle angle: 0.8–0.9 deg) compared with a recently-proposed hybrid algorithm (4.7 mm, 0.9 deg), and the recently-proposed DL_Track algorithm (3.8 mm, 3.9 deg). Furthermore, UltraTimTrack’s processing time (0.2 s per image) was at least five times shorter than that of these recently-proposed algorithms. Conclusion We developed a Kalman-filter-based algorithm to improve fascicle tracking from B-mode ultrasound image sequences. The proposed algorithm provides low-noise, drift-free estimates of muscle architectural changes that may better inform muscle function interpretations.},
  archive      = {J_PEERJCS},
  author       = {Tim J. van der Zee and Paolo Tecchio and Daniel Hahn and Brent J. Raiteri},
  doi          = {10.7717/peerj-cs.2636},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2636},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {UltraTimTrack: A kalman-filter-based algorithm to track muscle fascicles in ultrasound image sequences},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resolving ambiguity in natural language for enhancement of aspect-based sentiment analysis of hotel reviews. <em>PEERJCS</em>, <em>11</em>, e2635. (<a href='https://doi.org/10.7717/peerj-cs.2635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the ever-expanding digital landscape, the abundance of user-generated content on consumer platforms such as Booking and TripAdvisor offers a rich source of information for both travellers and hoteliers. Sentiment analysis, a fundamental research task of natural language processing (NLP) is used for mining sentiments and opinions within this vast reservoir of text reviews. A more specific type of sentiment analysis, i.e., aspect-based sentiment analysis (ABSA), is used when processing customer reviews is required. In ABSA, we aim to capture aspect-level sentiments and intricate relationships between various aspects within reviews. This article proposes a novel approach to ABSA by introducing a novel technique of word sense disambiguation (WSD) and integrating it with the Transformer architecture bidirectional encoder representations from Transformers (BERT) and graph convolutional networks (GCNs). The proposed approach resolves the intriguing ambiguities of the words and represents the review data as a complex graph structure, facilitating the modeling of intricate relationships between different aspects. The combination of bidirectional long short-term memory (BiLSTM) and GCN proves effective in capturing inter-dependencies among various aspects, providing a nuanced understanding of customer sentiments. The experiments are conducted on the RABSA dataset (an enhanced and richer hotel review data collection), and results demonstrate that our approach outperforms previous baselines, showcasing the effectiveness of integrating WSD in ABSA. Furthermore, an ablation study confirms the significant contribution of the WSD module to the overall performance. Moreover, we explore different similarity measures and find that cosine similarity yields the best results when identifying the real sense of a word in a given sentence using WordNet. The findings of our work and future work related to our work create lots of interest for people in the tourism and hospitality industry. This research gives another boost to the concept of the potential of NLP techniques in sentiment analysis. It emphasizes that if we combine the potential of NLP techniques along with state-of-the-art machine learning frameworks, we can shape the future of this field.},
  archive      = {J_PEERJCS},
  author       = {Asma Nadeem and Malik Muhammad Saad Missen and Mana Saleh Al Reshan and Muhammad Ali Memon and Yousef Asiri and Muhammad Ali Nizamani and Mohammad Alsulami and Asadullah Shaikh},
  doi          = {10.7717/peerj-cs.2635},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2635},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Resolving ambiguity in natural language for enhancement of aspect-based sentiment analysis of hotel reviews},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating humanoid robots with human musicians for synchronized musical performances. <em>PEERJCS</em>, <em>11</em>, e2632. (<a href='https://doi.org/10.7717/peerj-cs.2632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entertainment robotics has garnered significant attention in recent years, with researchers focusing on developing robots capable of performing a variety of tasks, including magic, drawing, dancing, and music. This article presents our research on forming a musical band that includes both humanoid robots and human musicians, with the goal of achieving natural synchronization and collaboration during musical performances. We utilized two of our humanoid robots for this project: Polaris, a mid-sized humanoid robot, as the drummer, and Oscar, a Robotis-OP3 humanoid robot, as the keyboardist. The technical implementation incorporated essential components such as visual servoing, human-robot interaction, and Robot Operating System (ROS), enabling seamless communication and coordination between the humanoid robots and the human musicians. The success of this collaborative effort can be both seen and heard through the following YouTube link: https://youtu.be/pFOyt1KKCfY?feature=shared.},
  archive      = {J_PEERJCS},
  author       = {MengCheng Lau and John Anderson and Jacky Baltes},
  doi          = {10.7717/peerj-cs.2632},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2632},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Integrating humanoid robots with human musicians for synchronized musical performances},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of an enhanced feature point matching algorithm utilizing 3D laser scanning technology for sculpture design. <em>PEERJCS</em>, <em>11</em>, e2628. (<a href='https://doi.org/10.7717/peerj-cs.2628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the aesthetic appreciation for art continues to grow, there is an increased demand for precision and detailed control in sculptural works. The advent of 3D laser scanning technology introduces transformative new tools and methodologies for refining correction systems in sculpture design. This article proposes a feature point matching algorithm based on fragment measurement and the iterative closest point (ICP) methodology, leveraging 3D laser scanning technology, namely Fragment Measurement Iterative Closest Point Feature Point Matching (FM-ICP-FPM). The FM-ICP-FPM approach uses the overlapping area of the two sculpture perspectives as a reference for attaching feature points. It employs the 3D measurement system to capture physical point cloud data from the two surfaces to enable the initial alignment of feature points. Feature vectors are generated by segmenting the region around the feature points and computing the intra-block gradient histogram. Subsequently, distance threshold conditions are set based on the constructed feature vectors and the preliminary feature point matches established during the coarse alignment to achieve precise feature point matching. Experimental results demonstrate the exceptional performance of the FM-ICP-FPM algorithm, achieving a sampling interval of 200. The correct matching rate reaches an impressive 100%, while the mean translation error (MTE) is a mere 154 mm, and the mean rotation angle error (MRAE) is 0.065 degrees. The indicator represents the degree of deviation in translation and rotation of the registered model, respectively. These low error values demonstrate that the FM-ICP-FPM algorithm excels in registration accuracy and can generate highly consistent three-dimensional models.},
  archive      = {J_PEERJCS},
  author       = {Xiaoxiong Zheng and Zhenwei Weng},
  doi          = {10.7717/peerj-cs.2628},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2628},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of an enhanced feature point matching algorithm utilizing 3D laser scanning technology for sculpture design},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving drug–target affinity prediction by adaptive self-supervised learning. <em>PEERJCS</em>, <em>11</em>, e2622. (<a href='https://doi.org/10.7717/peerj-cs.2622'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational drug-target affinity prediction is important for drug screening and discovery. Currently, self-supervised learning methods face two major challenges in drug-target affinity prediction. The first difficulty lies in the phenomenon of sample mismatch: self-supervised learning processes drug and target samples independently, while actual prediction requires the integration of drug-target pairs. Another challenge is the mismatch between the broadness of self-supervised learning objectives and the precision of biological mechanisms of drug-target affinity (i.e., the induced-fit principle). The former focuses on global feature extraction, while the latter emphasizes the importance of local precise matching. To address these issues, an adaptive self-supervised learning-based drug-target affinity prediction (ASSLDTA) was designed. ASSLDTA integrates a novel adaptive self-supervised learning (ASSL) module with a high-level feature learning network to extract the feature. The ASSL leverages a large amount of unlabeled training data to effectively capture low-level features of drugs and targets. Its goal is to maximize the retention of original feature information, thereby bridging the objective gap between self-supervised learning and drug-target affinity prediction and alleviating the sample mismatch problem. The high-level feature learning network, on the other hand, focuses on extracting effective high-level features for affinity prediction through a small amount of labeled data. Through this two-stage feature extraction design, each stage undertakes specific tasks, fully leveraging the advantages of each model while efficiently integrating information from different data sources, providing a more accurate and comprehensive solution for drug-target affinity prediction. In our experiments, ASSLDTA is much better than other deep methods, and the result of ASSLDTA is significantly increased by learning adaptive self-supervised learning-based features, which validates the effectiveness of our ASSLDTA.},
  archive      = {J_PEERJCS},
  author       = {Qing Ye and Yaxin Sun},
  doi          = {10.7717/peerj-cs.2622},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2622},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving drug–target affinity prediction by adaptive self-supervised learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMC-YOLO: A detection model for assisted razor clam fishing in the mudflat environment. <em>PEERJCS</em>, <em>11</em>, e2614. (<a href='https://doi.org/10.7717/peerj-cs.2614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In intertidal mudflat culture (IMC), the fishing efficiency and the degree of damage to nature have always been a pair of irreconcilable contradictions. To improve the efficiency of razor clam fishing and at the same time reduce the damage to the natural environment, in this study, a razor clam burrows dataset is established, and an intelligent razor clam fishing method is proposed, which realizes the accurate identification and counting of razor clam burrows by introducing the object detection technology into the razor clam fishing activity. A detection model called intertidal mudflat culture-You Only Look Once (IMC-YOLO) is proposed in this study by making improvements upon You Only Look Once version 8 (YOLOv8). In this study, firstly, at the end of the backbone network, the Iterative Attention-based Intrascale Feature Interaction (IAIFI) module module was designed and adopted to improve the model’s focus on advanced features. Subsequently, to improve the model’s effectiveness in detecting difficult targets such as razor clam burrows with small sizes, the head network was refactored. Then, FasterNet Block is used to replace the Bottleneck, which achieves more effective feature extraction while balancing detection accuracy and model size. Finally, the Three Branch Convolution Attention Mechanism (TBCAM) is proposed, which enables the model to focus on the specific region of interest more accurately. After testing, IMC-YOLO achieved mAP50, mAP50:95, and F1best of 0.963, 0.636, and 0.918, respectively, representing improvements of 2.2%, 3.5%, and 2.4% over the baseline model. Comparison with other mainstream object detection models confirmed that IMC-YOLO strikes a good balance between accuracy and numbers of parameters.},
  archive      = {J_PEERJCS},
  author       = {Jianhao Xu and Lijie Cao and Lanlan Pan and Xiankun Li and Lei Zhang and Hongyong Gao and Weibo Song},
  doi          = {10.7717/peerj-cs.2614},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2614},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IMC-YOLO: A detection model for assisted razor clam fishing in the mudflat environment},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Process mining applications in healthcare: A systematic literature review. <em>PEERJCS</em>, <em>11</em>, e2613. (<a href='https://doi.org/10.7717/peerj-cs.2613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining applications in healthcare is a field widely investigated in the last years. Its diffusion is driven by increasing digitalization and the availability of large quantities of clinical data, enabling hospitals, clinics, and other healthcare organizations to optimize workflows, reduce operational costs, and improve asset management. The importance of process mining lies in its potential to identify inefficiencies in processes, standardize clinical practices, support evidence-based decisions and, in general, improve the quality of care provided. The article aims to systematically review the research landscape in the field of process mining in healthcare, providing an in-depth understanding of how process mining is applied in healthcare. It contributes to the existing literature by highlighting the following aspects: the specific research topics covered (i), the extent of use of various process mining algorithms in different healthcare applications, showing their adaptability and effectiveness in specific contexts (ii), and, finally, the types and characteristics of data employed in these studies, highlighting the needs and challenges related to data in healthcare process mining (iii). Through this systematic literature review, the article can support researchers in identifying the most valuable research topic to be explored by the scientific community working on process mining in healthcare. To achieve this goal, several articles focusing on the algorithms and data employed were selected and analyzed. The final discussion highlights current research gaps, suggesting future areas of investigation, and identifies critical issues and vulnerabilities of existing process mining applications in healthcare.},
  archive      = {J_PEERJCS},
  author       = {Lerina Aversano and Martina Iammarino and Antonella Madau and Giuseppe Pirlo and Gianfranco Semeraro},
  doi          = {10.7717/peerj-cs.2613},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2613},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Process mining applications in healthcare: A systematic literature review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of machine learning techniques for warfarin dosage prediction: A case study on the MIMIC-III dataset. <em>PEERJCS</em>, <em>11</em>, e2612. (<a href='https://doi.org/10.7717/peerj-cs.2612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Warfarin, a commonly prescribed anticoagulant, poses significant dosing challenges due to its narrow therapeutic range and high variability in patient responses. This study applies advanced machine learning techniques to improve the accuracy of international normalized ratio (INR) predictions using the MIMIC-III dataset, addressing the critical issue of missing data. By leveraging dimensionality reduction methods such as principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE), and advanced imputation techniques including denoising autoencoders (DAE) and generative adversarial networks (GAN), we achieved significant improvements in predictive accuracy. The integration of these methods substantially reduced prediction errors compared to traditional approaches. This research demonstrates the potential of machine learning (ML) models to provide more personalized and precise dosing strategies that reduce the risks of adverse drug events. Our method could integrate into clinical workflows to enhance anticoagulation therapy in cases of missing data, with potential applications in other complex medical treatments.},
  archive      = {J_PEERJCS},
  author       = {Aasim Ayaz Wani and Fatima Abeer},
  doi          = {10.7717/peerj-cs.2612},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2612},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Application of machine learning techniques for warfarin dosage prediction: A case study on the MIMIC-III dataset},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of low-carbon planning model for vehicle path based on adaptive multi-strategy ant colony optimization algorithm. <em>PEERJCS</em>, <em>11</em>, e2611. (<a href='https://doi.org/10.7717/peerj-cs.2611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary transportation systems, the imperatives of route planning and optimization have become increasingly critical due to vehicles’ burgeoning number and complexity. This includes various vehicle types, such as electric and autonomous vehicles, each with specific needs. Additionally, varying speeds and operational requirements further complicate the process, demanding more sophisticated planning solutions. These systems frequently confront myriad challenges, including traffic congestion, intricate routes, and substantial energy consumption, which collectively undermine transportation efficiency, escalate energy usage, and contribute to environmental pollution. Hence, strategically planning and optimizing routes within complex traffic milieus are paramount to enhancing transportation efficacy and achieving low-carbon and environmentally sustainable objectives. This article proposes a vehicle path low-carbon planning model, Adaptive Cooperative Graph Neural Network (ACGNN), predicated on an adaptive multi-strategy ant colony optimization algorithm, addressing the vehicle path low-carbon planning conundrum. The proposed framework initially employs graph data from road networks and historical trajectories as model inputs, generating high-quality graph data through subgraph screening. Subsequently, a graph neural network (GNN) is utilized to optimize nodes and edges computationally. At the same time, the global search capability of the model is augmented via an ant colony optimization algorithm to ascertain the final optimized path. Experimental results demonstrate that ACGNN yields significant path planning outcomes on both public and custom-built datasets, surpassing the traditional Dijkstra’s shortest path algorithm, random graph network (RGN), and conventional GNN methodologies. Moreover, comparative analyses of various optimization methods on the custom-built dataset reveal that the ant colony optimization algorithm markedly outperforms the simulated annealing algorithm (SA) and particle swarm optimization algorithm (PSO). The method offers an innovative technical approach to vehicle path planning and is instrumental in advancing low-carbon and environmentally sustainable goals while enhancing transportation efficiency.},
  archive      = {J_PEERJCS},
  author       = {Qi Guo and Rui Li and Changjiang Zheng and Gwanggil Jeon},
  doi          = {10.7717/peerj-cs.2611},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2611},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of low-carbon planning model for vehicle path based on adaptive multi-strategy ant colony optimization algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALL-net: Integrating CNN and explainable-AI for enhanced diagnosis and interpretation of acute lymphoblastic leukemia. <em>PEERJCS</em>, <em>11</em>, e2600. (<a href='https://doi.org/10.7717/peerj-cs.2600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new model, ALL-Net, for the detection of acute lymphoblastic leukemia (ALL) using a custom convolutional neural network (CNN) architecture and explainable Artificial Intelligence (XAI). A dataset consisting of 3,256 peripheral blood smear (PBS) images belonging to four classes—benign (hematogones), and the other three Early B, Pre-B, and Pro-B, which are subtypes of ALL, are utilized for training and evaluation. The ALL-Net CNN is initially designed and trained on the PBS image dataset, achieving an impressive test accuracy of 97.85%. However, data augmentation techniques are applied to augment the benign class and address the class imbalance challenge. The augmented dataset is then used to retrain the ALL-Net, resulting in a notable improvement in test accuracy, reaching 99.32%. Along with accuracy, we have considered other evaluation metrics and the results illustrate the potential of ALLNet with an average precision of 99.35%, recall of 99.33%, and F1 score of 99.58%. Additionally, XAI techniques, specifically the Local Interpretable Model-Agnostic Explanations (LIME) algorithm is employed to interpret the model’s predictions, providing insights into the decision-making process of our ALL-Net CNN. These findings highlight the effectiveness of CNNs in accurately detecting ALL from PBS images and emphasize the importance of addressing data imbalance issues through appropriate preprocessing techniques at the same time demonstrating the usage of XAI in solving the black box approach of the deep learning models. The proposed ALL-Net outperformed EfficientNet, MobileNetV3, VGG-19, Xception, InceptionV3, ResNet50V2, VGG-16, and NASNetLarge except for DenseNet201 with a slight variation of 0.5%. Nevertheless, our ALL-Net model is much less complex than DenseNet201, allowing it to provide faster results. This highlights the need for a more customized and streamlined model, such as ALL-Net, specifically designed for ALL classification. The entire source code of our proposed CNN is publicly available at https://github.com/Abhiram014/ALL-Net-Detection-of-ALL-using-CNN-and-XAI.},
  archive      = {J_PEERJCS},
  author       = {Abhiram Thiriveedhi and Swetha Ghanta and Sujit Biswas and Ashok K. Pradhan},
  doi          = {10.7717/peerj-cs.2600},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2600},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {ALL-net: Integrating CNN and explainable-AI for enhanced diagnosis and interpretation of acute lymphoblastic leukemia},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A temporal knowledge graph reasoning model based on recurrent encoding and contrastive learning. <em>PEERJCS</em>, <em>11</em>, e2595. (<a href='https://doi.org/10.7717/peerj-cs.2595'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal knowledge graphs (TKGs) are critical tools for capturing the dynamic nature of facts that evolve over time, making them highly valuable in a broad spectrum of intelligent applications. In the domain of temporal knowledge graph extrapolation reasoning, the prediction of future occurrences is of great significance and presents considerable obstacles. While current models consider the fact changes over time and recognize that historical facts may recur, they often overlook the influence of past events on future predictions. Motivated by these considerations, this work introduces a novel temporal knowledge graph reasoning model, named Temporal Reasoning with Recurrent Encoding and Contrastive Learning (TRCL), which integrates recurrent encoding and contrastive learning techniques. The proposed model has the ability to capture the evolution of historical facts, generating representations of entities and relationships through recurrent encoding. Additionally, TRCL incorporates a global historical matrix to account for repeated historical occurrences and employs contrastive learning to alleviate the interference of historical facts in predicting future events. The TKG reasoning outcomes are subsequently derived through a time decoder. A quantity of experiments conducted on four benchmark datasets demonstrate the exceptional performance of the proposed TRCL model across a range of metrics, surpassing state-of-the-art TKG reasoning models. When compared to the strong baseline Time-Guided Recurrent Graph Network (TiRGN) model, the proposed TRCL achieves 1.03% improvements on ICEWS14 using mean reciprocal rank (MRR) evaluation metric. This innovative proposed method not only enhances the accuracy of TKG extrapolation, but also sets a new standard for robustness in dynamic knowledge graph applications, paving the way for future research and practical applications in predictive intelligence systems.},
  archive      = {J_PEERJCS},
  author       = {Weitong Liu and Khairunnisa Hasikin and Anis Salwa Mohd Khairuddin and Meizhen Liu and Xuechen Zhao},
  doi          = {10.7717/peerj-cs.2595},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2595},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A temporal knowledge graph reasoning model based on recurrent encoding and contrastive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of deep learning in blink detection. <em>PEERJCS</em>, <em>11</em>, e2594. (<a href='https://doi.org/10.7717/peerj-cs.2594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blink detection is a highly concerned research direction in the field of computer vision, which plays a key role in various application scenes such as human-computer interaction, fatigue detection and emotion perception. In recent years, with the rapid development of deep learning, the application of deep learning techniques for precise blink detection has emerged as a significant area of interest among researchers. Compared with traditional methods, the blink detection method based on deep learning offers superior feature learning ability and higher detection accuracy. However, the current research on blink detection based on deep learning lacks systematic summarization and comparison. Therefore, the aim of this article is to comprehensively review the research progress in deep learning-based blink detection methods and help researchers to have a clear understanding of the various approaches in this field. This article analyzes the progress made by several classical deep learning models in practical applications of eye blink detection while highlighting their respective strengths and weaknesses. Furthermore, it provides a comprehensive summary of commonly used datasets and evaluation metrics for blink detection. Finally, it discusses the challenges and future directions of deep learning for blink detection applications. Our analysis reveals that deep learning-based blink detection methods demonstrate strong performance in detection. However, they encounter several challenges, including training data imbalance, complex environment interference, real-time processing issues and application device limitations. By overcoming the challenges identified in this study, the application prospects of deep learning-based blink detection algorithms will be significantly enhanced.},
  archive      = {J_PEERJCS},
  author       = {Jianbin Xiong and Weikun Dai and Qi Wang and Xiangjun Dong and Baoyu Ye and Jianxiang Yang},
  doi          = {10.7717/peerj-cs.2594},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2594},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A review of deep learning in blink detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OBC-YOLOv8: An improved road damage detection model based on YOLOv8. <em>PEERJCS</em>, <em>11</em>, e2593. (<a href='https://doi.org/10.7717/peerj-cs.2593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective and efficient detection of pavement distress is very important for the normal use and maintenance of roads. To achieve this goal, a new road damage detection method based on YOLOv8 is proposed in this article. Firstly, omni-dimensional dynamic convolution (ODConv) block is employed to better grasp the complex and diverse features of damage objects by making dynamic adjustment according to the features of input images. Secondly, to extract the global and local feature information simultaneously to better improve the feature extraction ability of the model, BoTNet is added to the end of the backbone, which can combine the advantages of convolutional neural network (CNN) and Transformer. Finally, the coordinate attention mechanism (CA) is incorporated into the Neck section to make more accurate speculations and enhance detection accuracy further which can effectively mitigate irrelevant feature interference. The new proposed model is named OBC-YOLOv8 and the experimental results on the RDD2022-China dataset demonstrate its superiority compared with baselines, with 1.8% and 1.6% increases in mean average precision 50 (mAP@0.5) and F1-score, respectively.},
  archive      = {J_PEERJCS},
  author       = {Shizheng Zhang and Zhihao Liu and Kunpeng Wang and Wanwei Huang and Pu Li},
  doi          = {10.7717/peerj-cs.2593},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2593},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {OBC-YOLOv8: An improved road damage detection model based on YOLOv8},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolving techniques in sentiment analysis: A comprehensive review. <em>PEERJCS</em>, <em>11</em>, e2592. (<a href='https://doi.org/10.7717/peerj-cs.2592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of social media and e-commerce platforms, an unprecedented volume of user-generated content has emerged, offering organizations, governments, and researchers invaluable insights into public sentiment. Yet, the vast and unstructured nature of this data challenges traditional analysis methods. Sentiment analysis, a specialized field within natural language processing, has evolved to meet these challenges by automating the detection and categorization of opinions and emotions in text. This review comprehensively examines the evolving techniques in sentiment analysis, detailing foundational processes such as data gathering and feature extraction. It explores a spectrum of methodologies, from classical word embedding techniques and machine learning algorithms to recent contextual embedding and advanced transformer models like Generative Pre-trained Transformer (GPT), Bidirectional Encoder Representations from Transformers (BERT), and T5. With a critical comparison of these methods, this article highlights their appropriate uses and limitations. Additionally, the review provides a thorough overview of current trends, insights into future directions, and a critical exploration of unresolved challenges. By synthesizing these developments, this review equips researchers with a solid foundation for assessing the current state of sentiment analysis and guiding future advancements in this dynamic field.},
  archive      = {J_PEERJCS},
  author       = {Mahander Kumar and Lal Khan and Hsien-Tsung Chang},
  doi          = {10.7717/peerj-cs.2592},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2592},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evolving techniques in sentiment analysis: A comprehensive review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy multi-objective optimization model to design a sustainable closed-loop manufacturing system. <em>PEERJCS</em>, <em>11</em>, e2591. (<a href='https://doi.org/10.7717/peerj-cs.2591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Republicans and Democrats practically everywhere have been demonstrating concerns about environmental conservation to achieve sustainable development goals (SDGs) since the turn of the century. To promote fuel (energy) savings and a reduction in the amount of carbon dioxide CO2 emissions in several enterprises, actions have been taken based on the concepts described. This study proposes an environmentally friendly manufacturing system designed to minimize environmental impacts. Specifically, it aims to develop a sustainable manufacturing process that accounts for energy consumption and CO2 emissions from direct and indirect energy sources. A multi-objective mathematical model has been formulated, incorporating financial and environmental constraints, to minimize overall costs, energy consumption, and CO2 emissions within the manufacturing framework. The input model parameters for real-world situations are generally unpredictable, so a fuzzy multi-objective model will be developed as a way to handle it. The validity of the proposed ecological industrial design will be tested using a scenario-based approach. Results demonstrate the high reliability, applicability, and effectiveness of the proposed network when analyzed using the developed techniques.},
  archive      = {J_PEERJCS},
  author       = {Sajida Kousar and Asma Alvi and Nasreen Kausar and Harish Garg and Seifedine Kadry and Jungeun Kim},
  doi          = {10.7717/peerj-cs.2591},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2591},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fuzzy multi-objective optimization model to design a sustainable closed-loop manufacturing system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel group tour trip recommender model for personalized travel systems. <em>PEERJCS</em>, <em>11</em>, e2589. (<a href='https://doi.org/10.7717/peerj-cs.2589'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning personalized travel itineraries for groups with diverse preferences is indeed challenging. This article proposes a novel group tour trip recommender model (GTTRM), which uses ant colony optimization (ACO) to optimize group satisfaction while minimizing conflicts between group members. Unlike existing models, the proposed GTTRM allows dynamic subgroup formation during the trip to handle conflicting preferences and provide tailored recommendations. Experimental results show that GTTRM significantly improves satisfaction levels for individual group members, outperforming state-of-the-art models in terms of both subgroup management and optimization efficiency.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Alatiyyah},
  doi          = {10.7717/peerj-cs.2589},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2589},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel group tour trip recommender model for personalized travel systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expectation maximization—vector approximate message passing based generalized linear model for channel estimation in intelligent reflecting surface-assisted millimeter multi-user multiple-input multiple-output systems. <em>PEERJCS</em>, <em>11</em>, e2582. (<a href='https://doi.org/10.7717/peerj-cs.2582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel estimation poses a main challenge in intelligent reflecting surface (IRS)-assisted millimeter wave (mmWave) multi-user multiple-input multiple-output (MIMO) systems due to the substantial number of antennas at the base station (BS) and the passive reflective elements within the IRS lacking sufficient signal processing capabilities. This article addresses this challenge by proposing a channel estimation technique for IRS-assisted mmWave MIMO systems. The problem of channel estimation is normally taken as a compressed sensing (CS) problem, typically addressed through algorithms such as Orthogonal Matching Pursuit (OMP), Generalized Approximate Message Passing (GAMP), and Vector Approximate Message Passing with Expectation-Maximization (EM-VAMP). EM-VAMP demonstrates better performance only when a Gaussian mixture (GM) distribution is chosen as the prior for the sparse channel, especially at high signal-to-noise ratios (SNRs). To address this, the article introduces the application of generalized linear models (GLMs), extensions of standard linear models, providing increased flexibility in modeling data that deviates from Gaussian distribution. Numerical results unveil that the proposed Its EM-VAMP-GLM is much more robust to the existing OMP, GAMP and EM-LAMP algorithms.},
  archive      = {J_PEERJCS},
  author       = {Shoukath Ali K and Sajan P Philip and Arfat Ahmad Khan and Leeban Moses and Korhan Cengiz and Sedat Akleylek and Nikola Ivković},
  doi          = {10.7717/peerj-cs.2582},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2582},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Expectation maximization—vector approximate message passing based generalized linear model for channel estimation in intelligent reflecting surface-assisted millimeter multi-user multiple-input multiple-output systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust model predictive control for polytopic uncertain systems via a high-rate network with the FlexRay protocol. <em>PEERJCS</em>, <em>11</em>, e2580. (<a href='https://doi.org/10.7717/peerj-cs.2580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the robust model predictive control (RMPC) problem is investigated for a class of polytopic uncertain systems over high-rate networks whose signal exchanges are scheduled by the FlexRay protocol (FRP). During signal measurement, a high-rate network is applied to broadcast the data from the sensors to the controller efficiently. The FRP including the characteristics of event-triggered mechanism and the time-triggered mechanism is embedded into the high-rate network to regulate the data transmission in a circular period which can improve the flexibility of data transmission. With the aid of the Round-Robin and Try-Once-Discard protocols, a new expression of the measurement model is formulated by the use of certain data holding strategies. Subsequently, taking both high-rate networks and FRP into account, sufficient conditions are obtained by solving a time-varying terminal constraint set of an auxiliary optimization problem. In addition, an algorithm including both off-line and on-line parts is provided to find a sub-optimal solution. Lastly, two numerical simulations are carried out to substantiate the validity of the proposed RMPC strategy which is based on FRP and a high-rate network.},
  archive      = {J_PEERJCS},
  author       = {Jianhua Wang and Fuqiang Fan and Yanye Yu and Shuxin Du and Xiaorui Guo},
  doi          = {10.7717/peerj-cs.2580},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2580},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Robust model predictive control for polytopic uncertain systems via a high-rate network with the FlexRay protocol},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast binary logistic regression. <em>PEERJCS</em>, <em>11</em>, e2579. (<a href='https://doi.org/10.7717/peerj-cs.2579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel numerical approach that improves the training efficiency of binary logistic regression, a popular statistical model in the machine learning community. Our method achieves training times an order of magnitude faster than traditional logistic regression by employing a novel Soft-Plus approximation, which enables reformulation of logistic regression parameter estimation into matrix-vector form. We also adopt the Lf-norm penalty, which allows using fractional norms, including the L2-norm, L1-norm, and L0-norm, to regularize the model parameters. We put Lf-norm formulation in matrix-vector form, providing flexibility to include or exclude penalization of the intercept term when applying regularization. Furthermore, to address the common problem of collinear features, we apply singular value decomposition (SVD), resulting in a low-rank representation commonly used to reduce computational complexity while preserving essential features and mitigating noise. Moreover, our approach incorporates a randomized SVD alongside a newly developed SVD with row reduction (SVD-RR) method, which aims to manage datasets with many rows and features efficiently. This computational efficiency is crucial in developing a generalized model that requires repeated training over various parameters to balance bias and variance. We also demonstrate the effectiveness of our fast binary logistic regression (FBLR) method on various datasets from the OpenML repository in addition to synthetic datasets.},
  archive      = {J_PEERJCS},
  author       = {Nurdan Ayse Saran and Fatih Nar},
  doi          = {10.7717/peerj-cs.2579},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2579},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fast binary logistic regression},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated modeling, verification, and code generation for uncrewed aerial systems: Less cost and more efficiency. <em>PEERJCS</em>, <em>11</em>, e2575. (<a href='https://doi.org/10.7717/peerj-cs.2575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed Aerial Systems (UASs) are widely implemented in safety-critical fields such as industrial production, military operations, and disaster relief. Due to the diversity and complexity of implementation scenarios, UASs have become increasingly intricate. The challenge of designing and implementing highly reliable UASs while effectively controlling development costs and improving efficiency has been a pressing issue faced by academia and industry. To address this challenge, this article aims to examine an integrated method for modeling, verification, and code generation for UASs. This article begins to utilize Architecture Analysis and Design Language (AADL) to model UASs, proposing generic UAS models. Then, formal specifications describe a system's safety properties and functions based on these models. Finally, this article introduces a method to generate flight controller codes for UASs based on the verified models. Experiments demonstrate its effectiveness in pinpointing potential vulnerabilities in UASs during the early design phase and generating viable flight controller codes from the verified models. The proposed approach can also improve the efficiency of designing and verifying high-reliability UASs.},
  archive      = {J_PEERJCS},
  author       = {Jianyu Zhang and Long Zhang and Yixuan Wu and Linru Ma and Feng Yang},
  doi          = {10.7717/peerj-cs.2575},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2575},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An integrated modeling, verification, and code generation for uncrewed aerial systems: Less cost and more efficiency},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning of the user behavior structure based on the time granularity analysis model. <em>PEERJCS</em>, <em>11</em>, e2573. (<a href='https://doi.org/10.7717/peerj-cs.2573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of a consumption pattern can realize the analysis of consumer characteristics and behaviors, identify the relationship between commodities, and provide technical support for commodity recommendation and market analysis. However the current studies on consumer behavior and consumption patterns are very limited, and most of them are based on market research data. This method of data collection has high cost, low data coverage, and lagging survey results. The algorithm proposed in this article analyzes purchasing data from e-commerce platforms and extracts short- and long-term consumption matrices of consumers. By further processing these two matrices and removing the difference in granularity in time and marginal substitution rate, these matrices are finally integrated to form one consumption pattern matrix that can describe the characteristics of consumer consumption behavior in a period of time. Extensive experiments on various domains demonstrate that our proposed method outperforms state-of-the-art baselines on synthetic and real-world datasets.},
  archive      = {J_PEERJCS},
  author       = {Lin Guo and Xiaoying Liu},
  doi          = {10.7717/peerj-cs.2573},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2573},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Learning of the user behavior structure based on the time granularity analysis model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foreign object debris detection in lane images using deep learning methodology. <em>PEERJCS</em>, <em>11</em>, e2570. (<a href='https://doi.org/10.7717/peerj-cs.2570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Foreign object debris (FOD) is an unwanted substance that damages vehicular systems, most commonly the wheels of vehicles. In airport runways, these foreign objects can damage the wheels or internal systems of planes, potentially leading to flight crashes. Surveys indicate that FOD-related damage costs over $4 billion annually, affecting airlines, airport tenants, and passengers. Current FOD clearance involves high-cost radars and significant manpower, and existing radar and camera-based surveillance methods are expensive to install. Methods This work proposes a video-based deep learning methodology to address the high cost of radar-based FOD detection. The proposed system consists of two modules for FOD detection: object classification and object localization. The classification module categorizes FOD into specific types of foreign objects. In the object localization module, these classified objects are pinpointed in video frames. Results The proposed system was experimentally tested with a large video dataset and compared with existing methods. The results demonstrated improved accuracy and robustness, allowing the FOD clearance team to quickly detect and remove foreign objects, thereby enhancing the safety and efficiency of airport runway operations.},
  archive      = {J_PEERJCS},
  author       = {Priyadharsini S. and Bhuvaneshwara Raja K. and Kousi Krishnan T. and Senthil Kumar Jagatheesaperumal and Bader Fahad Alkhamees and Mohammad Mehedi Hassan},
  doi          = {10.7717/peerj-cs.2570},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2570},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Foreign object debris detection in lane images using deep learning methodology},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep learning approach for brain tumor classification using EfficientNetB0 and novel quantum genetic algorithm. <em>PEERJCS</em>, <em>11</em>, e2556. (<a href='https://doi.org/10.7717/peerj-cs.2556'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most complex and life-threatening pathologies of the central nervous system is brain tumors. Correct diagnosis of these tumors plays an important role in determining the treatment plans of patients. Traditional classification methods often rely on manual assessments, which can be prone to error. Therefore, multiple classification of brain tumors has gained significant interest in recent years in both the medical and computer science fields. The use of artificial intelligence and machine learning, especially in the automatic classification of brain tumors, is increasing significantly. Deep learning models can achieve high accuracy when trained on datasets in diagnosis and classification. This study examined deep learning-based approaches for automatic multi-class classification of brain tumors, and a new approach combining deep learning and quantum genetic algorithms (QGA) was proposed. The powerful feature extraction ability of the pre-trained EfficientNetB0 was utilized and combined with this quantum genetic algorithms, a new approach was proposed. It is aimed to develop the feature selection method. With this hybrid method, high reliability and accuracy in brain tumor classification was achieved. The proposed model achieved high accuracy of 98.36% and 98.25%, respectively, with different data sets and significantly outperformed traditional methods. As a result, the proposed method offers a robust and scalable solution that will help classify brain tumors in early and accurate diagnosis and contribute to the field of medical imaging with patient outcomes.},
  archive      = {J_PEERJCS},
  author       = {Kerem Gencer and Gülcan Gencer},
  doi          = {10.7717/peerj-cs.2556},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2556},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hybrid deep learning approach for brain tumor classification using EfficientNetB0 and novel quantum genetic algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN inversion and shifting: Recommending product modifications to sellers for better user preference. <em>PEERJCS</em>, <em>11</em>, e2553. (<a href='https://doi.org/10.7717/peerj-cs.2553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In efforts to better accommodate users, numerous researchers have endeavored to model customer behavior, seeking to comprehend how they interact with diverse items within online platforms. This exploration has given rise to recommendation systems, which utilize customer similarity with other customers or customer-item interactions to suggest new items based on the existing item catalog. Since these systems primarily focus on enhancing customer experiences, they overlook providing insights to sellers that could help refine the aesthetics of their items and increase their customer coverage. In this study, we go beyond customer recommendations to propose a novel approach: suggesting aesthetic feedback to sellers in the form of refined item images informed by customer-item interactions learned by a recommender system from multiple consumers. These images could serve as guidance for sellers to adapt existing items to meet the dynamic preferences of multiple users simultaneously. To evaluate the effectiveness of our method, we design experiments showcasing how changing the number of consumers and the class of item image used affect the change in preference score. Through these experiments, we found that our methodology outperforms previous approaches by generating distinct, realistic images with user preference higher by 16.7%, thus bridging the gap between customer-centric recommendations and seller-oriented feedback.},
  archive      = {J_PEERJCS},
  author       = {Satyadwyoom Kumar and Abhijith Sharma and Apurva Narayan},
  doi          = {10.7717/peerj-cs.2553},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2553},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GAN inversion and shifting: Recommending product modifications to sellers for better user preference},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAFE-CAST: Secure AI-federated enumeration for clustering-based automated surveillance and trust in machine-to-machine communication. <em>PEERJCS</em>, <em>11</em>, e2551. (<a href='https://doi.org/10.7717/peerj-cs.2551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine-to-machine (M2M) communication within the Internet of Things (IoT) faces increasing security and efficiency challenges as networks proliferate. Existing approaches often struggle with balancing robust security measures and energy efficiency, leading to vulnerabilities and reduced performance in resource-constrained environments. To address these limitations, we propose SAFE-CAST, a novel secure AI-federated enumeration for clustering-based automated surveillance and trust framework. This study addresses critical security and efficiency challenges in M2M communication within the context of IoT. SAFE-CAST integrates several innovative components: (1) a federated learning approach using Lloyd’s K-means algorithm for secure clustering, (2) a quality diversity optimization algorithm (QDOA) for secure channel selection, (3) a dynamic trust management system utilizing blockchain technology, and (4) an adaptive multi-agent reinforcement learning for context-aware transmission scheme (AMARLCAT) to minimize latency and improve scalability. Theoretical analysis and extensive simulations using network simulator (NS)-3.26 demonstrate the superiority of SAFE-CAST over existing methods. The results show significant improvements in energy efficiency (21.6% reduction), throughput (14.5% increase), security strength (15.3% enhancement), latency (33.9% decrease), and packet loss rate (12.9% reduction) compared to state-of-the-art approaches. This comprehensive solution addresses the pressing need for robust, efficient, and secure M2M communication in the evolving landscape of IoT and edge computing.},
  archive      = {J_PEERJCS},
  author       = {Yusuf Kursat Tuncel and Kasım Öztoprak},
  doi          = {10.7717/peerj-cs.2551},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2551},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SAFE-CAST: Secure AI-federated enumeration for clustering-based automated surveillance and trust in machine-to-machine communication},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling smart parking for smart cities using internet of things (IoT) and machine learning. <em>PEERJCS</em>, <em>11</em>, e2544. (<a href='https://doi.org/10.7717/peerj-cs.2544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the escalating number of vehicles and the lack of parking spaces, the issue of parking has become a significant problem in major cities as it is a daily occurrence for educational institutions, companies, and government facilities, resulting in fuel wastage and time inefficiencies. In their work lives, employees often face problems when parking their cars in the work parking area. Finding a space for their vehicle can take a lot of time and effort, leading to late arrival for work. On the other hand, security guards have difficulty entering their employees’ cars. In this context, our proposed system attempts to address this pressing issue, which consists of two parts: one is a camera at the parking gate that recognizes the license plate using the Automatic Number Plate Recognition (ANPR) algorithm, where the camera captures the license plate and outputs the plate number using the optical character recognition (OCR) technique. After that, the resulting data is cross-referenced with database records for seamless entry authentication. This eliminates the need for security personnel to verify vehicle identities or stickers manually, streamlining access procedures. The second part is a camera in the car parks that distinguishes between vacant and available parking spaces and stores the data collected by the camera in the centralized database, enabling the real-time display of the nearest available parking spots on digital screens at entrance gates, significantly reducing the time and effort spent in locating parking spaces. Through this innovative solution, we aim to enhance urban mobility and alleviate the challenges associated with urban parking congestion, thereby resolving the problem of intelligent parking for smart cities with the help of machine learning.},
  archive      = {J_PEERJCS},
  author       = {Mofadal Alymani and Lenah Abdulaziz Almoqhem and Dhuha Ahmed Alabdulwahab and Abdulrahman Abdullah Alghamdi and Hussain Alshahrani and Khalid Raza},
  doi          = {10.7717/peerj-cs.2544},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2544},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enabling smart parking for smart cities using internet of things (IoT) and machine learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot reranking with dense encoder models for news background linking. <em>PEERJCS</em>, <em>11</em>, e2534. (<a href='https://doi.org/10.7717/peerj-cs.2534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {News background linking is the problem of finding useful links to resources that provide contextual background information for a given news article. Many systems were proposed to address this problem. Yet, the most effective and reproducible method, to date, used the entire input article as a search query to retrieve the background links by sparse retrieval. While being effective, that method is still far from being optimal. Furthermore, it only leverages the lexical matching signal between the input article and the candidate background links. Nevertheless, intuitively, there may exist resources with useful background information that do not lexically overlap with the input article’s vocabulary. While many studies proposed systems that adopt semantic matching for addressing news background linking, none were able to outperform the simple lexical-based matching method. In this paper, we investigate multiple methods to integrate both the lexical and semantic relevance signals for better reranking of candidate background links. To represent news articles in the semantic space, we compare multiple Transformer-based encoder models in a zero-shot setting without the need for any labeled data. Our results show that using a hierarchical aggregation of sentence-level representations generates a good semantic representation of news articles, which is then integrated with lexical matching to achieve a new state-of-the-art solution for the problem. We further show that a significant performance improvement is potentially attainable if the degree by which a semantic relevance signal is needed is accurately predicted per input article.},
  archive      = {J_PEERJCS},
  author       = {Marwa Essam and Tamer Elsayed},
  doi          = {10.7717/peerj-cs.2534},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2534},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Zero-shot reranking with dense encoder models for news background linking},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geographic recommender systems in e-commerce based on population. <em>PEERJCS</em>, <em>11</em>, e2525. (<a href='https://doi.org/10.7717/peerj-cs.2525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advancements have significantly enhanced e-commerce, helping customers find the best products. One key development is recommendation systems, which personalize the shopping experience and boost sales. This paper explores a novel geographic recommendation system that uses demographic data, such as population density, age, and income, to refine recommendations. By integrating geographic and demographic information, like the population size of a country, businesses can tailor their offerings to regional preferences. This targeted approach aims to make recommendations more relevant by considering the behaviors and needs of different geographic areas. We sourced population data from The National Institute of Statistics (Tunisia, INS). This approach improves the importance of product recommendations for particular locations by customizing them based on demographic and geographic measures. The technique creates a better context-aware recommendation system that boosts customer happiness and business proceeds by fusing consumer behavior with extensive demographic data. The method also includes a mathematical model that considers population intensity to refine further recommendations established on the regional model.},
  archive      = {J_PEERJCS},
  author       = {Mohamed Shili and Osama Sohaib},
  doi          = {10.7717/peerj-cs.2525},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2525},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Geographic recommender systems in e-commerce based on population},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified MobileNetV2 transfer learning model to detect road potholes. <em>PEERJCS</em>, <em>11</em>, e2519. (<a href='https://doi.org/10.7717/peerj-cs.2519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road damage often includes potholes, cracks, lane degradation, and surface shading. Potholes are a common problem in pavements. Detecting them is crucial for maintaining infrastructure and ensuring public safety. A thorough assessment of pavement conditions is required before planning any preventive repairs. Herein, we report the use of transfer learning and deep learning (DL) models to preprocess digital images of pavements for better pothole detection. Fourteen models were evaluated, including MobileNet, MobileNetV2, NASNetMobile, DenseNet121, DenseNet169, InceptionV3, DenseNet201, ResNet152V2, EfficientNetB0, InceptionResNetV2, Xception, and EfficientNetV2M. The study introduces a modified MobileNetV2 (MMNV2) model designed for fast and efficient feature extraction. The MMNV2 model exhibits improved classification, detection, and prediction accuracy by adding a five-layer pre-trained network to the MobileNetV2 framework. It combines deep learning, deep neural networks (DNN), and transfer learning, which resulted in better performance compared to other models. The MMNV2 model was tested using a dataset of 5,000 pavement images. A learning rate of 0.001 was used to optimize the model. It classified images into ‘normal’ or ‘pothole’ categories with 99.95% accuracy. The model also achieved 100% recall, 99.90% precision, 99.95% F1-score, and a 0.05% error rate. The MMNV2 model uses fewer parameters while delivering better results. It offers a promising solution for real-world applications in pothole detection and pavement assessment.},
  archive      = {J_PEERJCS},
  author       = {Neha Tanwar and Anil V. Turukmane},
  doi          = {10.7717/peerj-cs.2519},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2519},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Modified MobileNetV2 transfer learning model to detect road potholes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediabetes risk classification algorithm via carotid bodies and K-means clustering technique. <em>PEERJCS</em>, <em>11</em>, e2516. (<a href='https://doi.org/10.7717/peerj-cs.2516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes is a disease that affects millions of people in the world and its early screening prevents serious health problems, also providing relief in the demand for healthcare services. In the search for methods to support early diagnosis, this article introduces a novel prediabetes risk classification algorithm (PRCA) for type-2 diabetes mellitus (T2DM), utilizing the chemosensitivity of carotid bodies (CB) and K-means clustering technique from the field of machine learning. Heart rate (HR) and respiratory rate (RR) data from eight volunteers with prediabetes and 25 without prediabetes were analyzed. Data were collected in basal conditions and after stimulation of the CBs by inhalation of 100% of oxygen and after ingestion of a standardized meal. During the analysis, a greater variability of groups was observed in people with prediabetes compared to the control group, particularly after inhalation of oxygen. The algorithm developed from these results showed an accuracy of 86% in classifying for prediabetes. This approach, centered on CB chemosensitivity deregulation in early disease stages, offers a nuanced detection method beyond conventional techniques. Moreover, the adaptable algorithm and clustering methodology hold promise as risk classifications for other diseases. Future endeavors aim to validate the algorithm through longitudinal studies tracking disease development among volunteers and expand the study’s scope to include a larger participant pool.},
  archive      = {J_PEERJCS},
  author       = {Rafael F. Pinheiro and Maria P. Guarino and Marlene Lages and Rui Fonseca-Pinto},
  doi          = {10.7717/peerj-cs.2516},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2516},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Prediabetes risk classification algorithm via carotid bodies and K-means clustering technique},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EFNet: Estimation of left ventricular ejection fraction from cardiac ultrasound videos using deep learning. <em>PEERJCS</em>, <em>11</em>, e2506. (<a href='https://doi.org/10.7717/peerj-cs.2506'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ejection fraction (EF) is a vital metric for assessing cardiovascular function through cardiac ultrasound. Manual evaluation is time-consuming and exhibits high variability among observers. Deep-learning techniques offer precise and autonomous EF predictions, yet these methods often lack explainability. Accurate heart failure prediction using cardiac ultrasound is challenging due to operator dependency and inconsistent video quality, resulting in significant interobserver variability. To address this, we developed a method integrating convolutional neural networks (CNN) and transformer models for direct EF estimation from ultrasound video scans. This article introduces a Residual Transformer Module (RTM) that extends a 3D ResNet-based network to analyze (2D + t) spatiotemporal cardiac ultrasound video scans. The proposed method, EFNet, utilizes cardiac ultrasound video images for end-to-end EF value prediction. Performance evaluation on the EchoNet-Dynamic dataset yielded a mean absolute error (MAE) of 3.7 and an R2 score of 0.82. Experimental results demonstrate that EFNet outperforms state-of-the-art techniques, providing accurate EF predictions.},
  archive      = {J_PEERJCS},
  author       = {Waqas Ali and Wesam Alsabban and Muhammad Shahbaz and Ali Al-Laith and Bassam Almogadwy},
  doi          = {10.7717/peerj-cs.2506},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2506},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {EFNet: Estimation of left ventricular ejection fraction from cardiac ultrasound videos using deep learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSSA: Multi-stage semantic-aware neural network for binary code similarity detection. <em>PEERJCS</em>, <em>11</em>, e2504. (<a href='https://doi.org/10.7717/peerj-cs.2504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary code similarity detection (BCSD) aims to identify whether a pair of binary code snippets is similar, which is widely used for tasks such as malware analysis, patch analysis, and clone detection. Current state-of-the-art approaches are based on Transformer, which require substantial computation resources. Learning-based approaches remains room for optimization in learning the deeper semantics of binary code. In this paper, we propose MSSA, a multi-stage semantic-aware neural network for BCSD at the function level. It effectively integrates the semantic and structural information of assembly instructions within and between basic blocks, and across the entire function through four semantic-aware neural networks, achieving deep understanding of binary code semantics. MSSA is a lightweight model with only 0.38M parameters in its backbone network, suitable for deployment in CPU environments. Experimental results show that MSSA outperforms Gemini, Asm2Vec, SAFE, and jTrans in classification performance and ranks second only to the Transformer-based jTrans in retrieval performance.},
  archive      = {J_PEERJCS},
  author       = {Bangrui Wan and Jianjun Zhou and Ying Wang and Feng Chen and Ying Qian},
  doi          = {10.7717/peerj-cs.2504},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2504},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MSSA: Multi-stage semantic-aware neural network for binary code similarity detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random k conditional nearest neighbor for high-dimensional data. <em>PEERJCS</em>, <em>11</em>, e2497. (<a href='https://doi.org/10.7717/peerj-cs.2497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k nearest neighbor (kNN) approach is a simple and effective algorithm for classification and a number of variants have been proposed based on the kNN algorithm. One of the limitations of kNN is that the method may be less effective when data contains many noisy features due to their non-informative influence in calculating distance. Additionally, information derived from nearest neighbors may be less meaningful in high-dimensional data. To address the limitation of nearest-neighbor based approaches in high-dimensional data, we propose to extend the k conditional nearest neighbor (kCNN) method which is an effective variant of kNN. The proposed approach aggregates multiple kCNN classifiers, each constructed from a randomly sampled feature subset. We also develop a score metric to weigh individual classifiers based on the level of separation of the feature subsets. We investigate the properties of the proposed method using simulation. Moreover, the experiments on gene expression datasets show that the proposed method is promising in terms of predictive classification performance.},
  archive      = {J_PEERJCS},
  author       = {Jiaxuan Lu and Hyukjun Gweon},
  doi          = {10.7717/peerj-cs.2497},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2497},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Random k conditional nearest neighbor for high-dimensional data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Offline prompt reinforcement learning method based on feature extraction. <em>PEERJCS</em>, <em>11</em>, e2490. (<a href='https://doi.org/10.7717/peerj-cs.2490'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that combining Transformer and conditional strategies to deal with offline reinforcement learning can bring better results. However, in a conventional reinforcement learning scenario, the agent can receive a single frame of observations one by one according to its natural chronological sequence, but in Transformer, a series of observations are received at each step. Individual features cannot be extracted efficiently to make more accurate decisions, and it is still difficult to generalize effectively for data outside the distribution. We focus on the characteristic of few-shot learning in pre-trained models, and combine prompt learning to enhance the ability of real-time policy adjustment. By sampling the specific information in the offline dataset as trajectory samples, the task information is encoded to help the pre-trained model quickly understand the task characteristics and the sequence generation paradigm to quickly adapt to the downstream tasks. In order to understand the dependencies in the sequence more accurately, we also divide the fixed-size state information blocks in the input trajectory, extract the features of the segmented sub-blocks respectively, and finally encode the whole sequence into the GPT model to generate decisions more accurately. Experiments show that the proposed method achieves better performance than the baseline method in related tasks, can be generalized to new environments and tasks better, and effectively improves the stability and accuracy of agent decision making.},
  archive      = {J_PEERJCS},
  author       = {Tianlei Yao and Xiliang Chen and Yi Yao and Weiye Huang and Zhaoyang Chen},
  doi          = {10.7717/peerj-cs.2490},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2490},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Offline prompt reinforcement learning method based on feature extraction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the development of diagnostic support algorithms based on CPET biosignals data via machine learning and wavelets. <em>PEERJCS</em>, <em>11</em>, e2474. (<a href='https://doi.org/10.7717/peerj-cs.2474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For preventing health complications and reducing the strain on healthcare systems, early identification of diseases is imperative. In this context, artificial intelligence has become increasingly prominent in the field of medicine, offering essential support for disease diagnosis. This article introduces an algorithm that builds upon an earlier methodology to assess biosignals acquired through cardiopulmonary exercise testing (CPET) for identifying metabolic syndrome (MS), heart failure (HF), and healthy individuals (H). Leveraging support vector machine (SVM) technology, a well-known machine learning classification method, in combination with wavelet transforms for feature extraction, the algorithm takes an innovative approach. The model was trained on CPET data from 45 participants, including 15 with MS, 15 with HF, and 15 healthy controls. For binary classification tasks, the SVM with a polynomial kernel and 5-level wavelet transform (SVM-POL-BW5) outperformed similar methods described in the literature. Moreover, one of the main contributions of this study is the development of a multi-class classification algorithm using the SVM employing a linear kernel and 3-level wavelet transforms (SVM-LIN-MW3), reaching an average accuracy of 95%. In conclusion, the application of SVM-based algorithms combined with wavelet transforms to analyze CPET data shows promise in diagnosing various diseases, highlighting their adaptability and broader potential applications in healthcare.},
  archive      = {J_PEERJCS},
  author       = {Rafael F. Pinheiro and Rui Fonseca-Pinto},
  doi          = {10.7717/peerj-cs.2474},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2474},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {On the development of diagnostic support algorithms based on CPET biosignals data via machine learning and wavelets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge and texture aware image denoising using median noise residue U-net with hand-crafted features. <em>PEERJCS</em>, <em>11</em>, e2449. (<a href='https://doi.org/10.7717/peerj-cs.2449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a complex task that always yields an approximated version of the clean image. Unfortunately, the existing works have focussed only on the peak signal to noise ratio (PSNR) metric and have shown no attention to edge features in a reconstructed image. Although fully convolution neural networks (CNN) are capable of removing the noise using kernel filters and automatic extraction of features, it has failed to reconstruct the images for higher values of noise standard deviation. Additionally, deep learning models require a huge database to learn better from the inputs. This, in turn, increases the computational complexity and memory requirement. Therefore, we propose the Median Noise Residue U-Net (MNRU-Net) with a limited training database without involving image augmentation. In the proposed work, the learning capability of the traditional U-Net model was increased by adding hand-crafted features in the input layers of the U-Net. Further, an approximate version of the noise estimated from the median filter and the gradient information of the image were used to improve the performance of U-Net. Later, the performance of MNRU-Net was evaluated based on PSNR, structural similarity, and figure of merit for different noise standard deviations of 15, 25, and 50 respectively. It is witnessed that the results gained from the suggested work are better than the results yielded by complex denoising models such as the robust deformed denoising CNN (RDDCNN). This work emphasizes that the skip connections along with the hand-crafted features could improve the performance at higher noise levels by using this simple architecture. In addition, the model was found to be less expensive, with low computational complexity.},
  archive      = {J_PEERJCS},
  author       = {Soniya S. and Sriharipriya K. C.},
  doi          = {10.7717/peerj-cs.2449},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2449},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Edge and texture aware image denoising using median noise residue U-net with hand-crafted features},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI augmented edge and fog computing for internet of health things (IoHT). <em>PEERJCS</em>, <em>11</em>, e2431. (<a href='https://doi.org/10.7717/peerj-cs.2431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients today seek a more advanced and personalized health-care system that keeps up with the pace of modern living. Cloud computing delivers resources over the Internet and enables the deployment of an infinite number of applications to provide services to many sectors. The primary limitation of these cloud frameworks right now is their limited scalability, which results in their inability to meet needs. An edge/fog computing environment, paired with current computing techniques, is the answer to fulfill the energy efficiency and latency requirements for the real-time collection and analysis of health data. Additionally, the Internet of Things (IoT) revolution has been essential in changing contemporary healthcare systems by integrating social, economic, and technological perspectives. This requires transitioning from unadventurous healthcare systems to more adapted healthcare systems that allow patients to be identified, managed, and evaluated more easily. These techniques allow data from many sources to be integrated to effectively assess patient health status and predict potential preventive actions. A subset of the Internet of Things, the Internet of Health Things (IoHT) enables the remote exchange of data for physical processes like patient monitoring, treatment progress, observation, and consultation. Previous surveys related to healthcare mainly focused on architecture and networking, which left untouched important aspects of smart systems like optimal computing techniques such as artificial intelligence, deep learning, advanced technologies, and services that includes 5G and unified communication as a service (UCaaS). This study aims to examine future and existing fog and edge computing architectures and methods that have been augmented with artificial intelligence (AI) for use in healthcare applications, as well as defining the demands and challenges of incorporating fog and edge computing technology in IoHT, thereby helping healthcare professionals and technicians identify the relevant technologies required based on their need for developing IoHT frameworks for remote healthcare. Among the crucial elements to take into account in an IoHT framework are efficient resource management, low latency, and strong security. This review addresses several machine learning techniques for efficient resource management in the IoT, where machine learning (ML) and AI are crucial. It has been noted how the use of modern technologies, such as narrow band-IoT (NB-IoT) for wider coverage and Blockchain technology for security, is transforming IoHT. The last part of the review focuses on the future challenges posed by advanced technologies and services. This study provides prospective research suggestions for enhancing edge and fog computing services for healthcare with modern technologies in order to give patients with an improved quality of life.},
  archive      = {J_PEERJCS},
  author       = {Deepika Rajagopal and Pradeep Kumar Thimma Subramanian},
  doi          = {10.7717/peerj-cs.2431},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2431},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {AI augmented edge and fog computing for internet of health things (IoHT)},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NOMA-MIMO in 5G network: A detailed survey on enhancing data rate. <em>PEERJCS</em>, <em>11</em>, e2388. (<a href='https://doi.org/10.7717/peerj-cs.2388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-orthogonal multiple access (NOMA) is a technology that leverages user channel gains, offers higher spectral efficiency, improves user fairness, better cell-edge throughput, increased reliability, and low latency, making it a potential technology for the next generation of cellular networks. The application of NOMA in the power domain (NOMA-PD) with multiple-input multiple-output (MIMO) and other emerging technologies allows to achieve the demand for higher data rates in next-generation networks. This survey aims to funnel down NOMA MIMO resource allocation issues and different optimization problems that exist in the literature to enhance the data rate. We examine the most recent NOMA-MIMO clustering, power allocation, and joint allocation schemes and analyze various parameters used in optimization methods to design 5G systems. We finally identify a promising research problem based on the signal-to-interference-plus-noise ratio (SINR) parameter in the context of NOMA-PD with MIMO configuration.},
  archive      = {J_PEERJCS},
  author       = {Murad Halabouni and Mardeni Roslee and Sufian Mitani and Osama Abuajwa and Anwar Osman and Fatimah Zaharah binti Ali and Athar Waseem},
  doi          = {10.7717/peerj-cs.2388},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2388},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {NOMA-MIMO in 5G network: A detailed survey on enhancing data rate},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of sentiment polarity in restaurant reviews using an ordinal regression approach based on evolutionary XGBoost. <em>PEERJCS</em>, <em>11</em>, e2370. (<a href='https://doi.org/10.7717/peerj-cs.2370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the business world shifts to the web and tremendous amounts of data become available on multilingual mobile applications, new business and research challenges and opportunities have been explored. This research aims to intensify the usage of data analytics, machine learning, and sentiment analysis of textual data to classify customers’ reviews, feedback, and ratings of businesses in Jordan’s food and restaurant industry. The main methods used in this research were sentiment polarity (to address the challenges posed by businesses to automatically apply text analysis) and bio-metric techniques (to systematically identify users’ emotional states, so reviews can be thoroughly understood). The research was extended to deal with reviews in Arabic, dialectic Arabic, and English, with the main focus on the Arabic language, as the application examined (Talabat) is based in Jordan. Arabic and English reviews were collected from the application, and a new model was proposed to sentimentally analyze reviews. The proposed model has four main stages: data collection, data preparation, model building, and model evaluation. The main purpose of this research is to study the problem expressed above using a model of ordinal regression to overcome issues related to misclassification. Additionally, an automatic multi-language prediction approach for online restaurant reviews was proposed by combining the eXtreme gradient boosting (XGBoost) and particle swarm optimization (PSO) techniques for the ordinal regression of these reviews. The proposed PSO-XGB algorithm showed superior results when compared to support vector machine (SVM) and other optimization methods in terms of root mean square error (RMSE) for the English and Arabic datasets. Specifically, for the Arabic dataset, PSO-XGB achieved an RMSE value of 0.7722, whereas PSO-SVM achieved an RSME value of 0.9988.},
  archive      = {J_PEERJCS},
  author       = {Dana A. Al-Qudah and Ala’ M. Al-Zoubi and Alexandra I. Cristea and Juan J. Merelo-Guervós and Pedro A. Castillo and Hossam Faris},
  doi          = {10.7717/peerj-cs.2370},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2370},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Prediction of sentiment polarity in restaurant reviews using an ordinal regression approach based on evolutionary XGBoost},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep gradient reinforcement learning for music improvisation in cloud computing framework. <em>PEERJCS</em>, <em>11</em>, e2265. (<a href='https://doi.org/10.7717/peerj-cs.2265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) in music improvisation offers promising new avenues for developing human creativity. The difficulty of writing dynamic, flexible musical compositions in real time is discussed in this article. We explore using reinforcement learning (RL) techniques to create more interactive and responsive music creation systems. Here, the musical structures train an RL agent to navigate the complex space of musical possibilities to provide improvisations. The melodic framework in the input musical data is initially identified using bi-directional gated recurrent units. The lyrical concepts such as notes, chords, and rhythms from the recognised framework are transformed into a format suitable for RL input. The deep gradient-based reinforcement learning technique used in this research formulates a reward system that directs the agent to compose aesthetically intriguing and harmonically cohesive musical improvisations. The improvised music is further rendered in the MIDI format. The Bach Chorales dataset with six different attributes relevant to musical compositions is employed in implementing the present research. The model was set up in a containerised cloud environment and controlled for smooth load distribution. Five different parameters, such as pitch frequency (PF), standard pitch delay (SPD), average distance between peaks (ADP), note duration gradient (NDG) and pitch class gradient (PCG), are leveraged to assess the quality of the improvised music. The proposed model obtains +0.15 of PF, −0.43 of SPD, −0.07 of ADP and 0.0041 NDG, which is a better value than other improvisation methods.},
  archive      = {J_PEERJCS},
  author       = {Fadwa Alrowais and Munya A. Arasi and Saud S. Alotaibi and Mohammed Alonazi and Radwa Marzouk and Ahmed S. Salama},
  doi          = {10.7717/peerj-cs.2265},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2265},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep gradient reinforcement learning for music improvisation in cloud computing framework},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLFCNet: An ultra-lightweight and efficient strawberry feature classification network. <em>PEERJCS</em>, <em>11</em>, e2085. (<a href='https://doi.org/10.7717/peerj-cs.2085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background As modern agricultural technology advances, the automated detection, classification, and harvesting of strawberries have become an inevitable trend. Among these tasks, the classification of strawberries stands as a pivotal juncture. Nevertheless, existing object detection methods struggle with substantial computational demands, high resource utilization, and reduced detection efficiency. These challenges make deployment on edge devices difficult and lead to suboptimal user experiences. Methods In this study, we have developed a lightweight model capable of real-time detection and classification of strawberry fruit, named the Strawberry Lightweight Feature Classify Network (SLFCNet). This innovative system incorporates a lightweight encoder and a self-designed feature extraction module called the Combined Convolutional Concatenation and Sequential Convolutional (C3SC). While maintaining model compactness, this architecture significantly enhances its feature decoding capabilities. To evaluate the model’s generalization potential, we utilized a high-resolution strawberry dataset collected directly from the fields. By employing image augmentation techniques, we conducted experimental comparisons between manually counted data and the model’s inference-based detection and classification results. Results The SLFCNet model achieves an average precision of 98.9% in the mAP@0.5 metric, with a precision rate of 94.7% and a recall rate of 93.2%. Notably, SLFCNet features a streamlined design, resulting in a compact model size of only 3.57 MB. On an economical GTX 1080 Ti GPU, the processing time per image is a mere 4.1 ms. This indicates that the model can smoothly run on edge devices, ensuring real-time performance. Thus, it emerges as a novel solution for the automation and management of strawberry harvesting, providing real-time performance and presenting a new solution for the automatic management of strawberry picking.},
  archive      = {J_PEERJCS},
  author       = {Wenchao Xu and Yangxu Wang and Jiahao Yang},
  doi          = {10.7717/peerj-cs.2085},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2085},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SLFCNet: An ultra-lightweight and efficient strawberry feature classification network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
