<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PEERJCS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="peerjcs">PEERJCS - 429</h2>
<ul>
<li><details>
<summary>
(2025). SA3C-ID: A novel network intrusion detection model using feature selection and adversarial training. <em>PEERJCS</em>, <em>11</em>, e3089. (<a href='https://doi.org/10.7717/peerj-cs.3089'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous proliferation of emerging technologies such as cloud computing, 5G networks, and the Internet of Things, the field of cybersecurity is facing an increasing number of complex challenges. Network intrusion detection systems, as a fundamental part of network security, have become increasingly significant. However, traditional intrusion detection methods exhibit several limitations, including insufficient feature extraction from network data, high model complexity, and data imbalance, which result in issues like low detection efficiency, as well as frequent false positives and missed alarms. To address the above issues, this article proposed an adversarial intrusion detection model (Soft Adversarial Asynchronous Actor-Critic Intrusion Detection, SA3C-ID) based on reinforcement learning. Firstly, the raw dataset is preprocessed via one-hot encoding and standardization. Subsequently, the refined data undergoes feature selection employing an improved pigeon-inspired optimizer (PIO) algorithm. This operation eliminates redundant and irrelevant features, consequently reducing data dimensionality while maintaining critical information. Next, the network intrusion detection process is modeled as a Markov decision process and integrated with the Soft Actor-Critic (SAC) reinforcement learning algorithm, with a view to constructing agents; In the context of adversarial training, two agents, designated as the attacker and the defender, are defined to perform asynchronous adversarial training. During this training process, both agents calculate the reward value, update their respective strategies, and transfer parameters based on the classification results. Finally, to verify the robustness and generalization ability of the SA3C-ID model, ablation experiments and comparative evaluations are conducted on two benchmark datasets, NSL-KDD and CSE-CIC-IDS2018. The experimental results demonstrate that SA3C-ID exhibits superior performance in comparison to other prevalent intrusion detection models. The F1-score attained by SA3C-ID was 92.58% and 98.76% on the NSL-KDD and CSE-CIC-IDS2018 datasets, respectively.},
  archive      = {J_PEERJCS},
  author       = {Wanwei Huang and Haobin Tian and Lei Wang and Sunan Wang and Kun Wang and Songze Li},
  doi          = {10.7717/peerj-cs.3089},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3089},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SA3C-ID: A novel network intrusion detection model using feature selection and adversarial training},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Music audio emotion regression using the fusion of convolutional neural networks and bidirectional long short-term memory models. <em>PEERJCS</em>, <em>11</em>, e3086. (<a href='https://doi.org/10.7717/peerj-cs.3086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Music emotion regression (MER) is a vital field that bridges psychology and music information retrieval. Music has the powerful ability to evoke a wide range of human emotions, from joy and sadness to anger and calmness. Understanding how music influences emotional states is essential for grasping its psychological effects on individuals. This research presents an innovative model that combines convolutional neural networks (CNNs) with bidirectional long short-term memory (BiLSTM) networks to analyze and predict the emotional impact of musical audio. The model uses CNNs to detect temporal patterns and BiLSTMs to interpret sequences in both forward and backward directions, enhancing its ability to capture the complex structure of musical data. Additionally, a multi-head attention mechanism is incorporated to improve the model’s expressiveness and generalizability, making it especially effective for handling intricate sequential tasks and large datasets. The model’s performance was evaluated through sentiment prediction using extensive, publicly available datasets comprising over 9,000 musical excerpts. Results show that the proposed model significantly outperforms existing methods in MER, achieving an R-squared value of 0.845, indicating an excellent fit with the empirical data.},
  archive      = {J_PEERJCS},
  author       = {Yi Qiu and Yu Lin and Yun Lin},
  doi          = {10.7717/peerj-cs.3086},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3086},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Music audio emotion regression using the fusion of convolutional neural networks and bidirectional long short-term memory models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond buzzwords: NLP reveals common threads in sustainable and circular construction discourse. <em>PEERJCS</em>, <em>11</em>, e3085. (<a href='https://doi.org/10.7717/peerj-cs.3085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Circular economy and sustainability have both seen rapid growth in academic literature, often leading to ambiguity and the overuse of these terms. This obscures their true objectives and makes it challenging to discern their distinct intentions. Manually analyzing the vast body of recent publications to understand how these concepts connect to environmentally beneficial practices is laborious and time-consuming. This study aims to compare and analyze existing literature on sustainable and circular construction using natural language processing (NLP) techniques to elucidate the similarities and overlaps between these concepts within the construction industry. To achieve this, we employed three NLP methods: (1) TextRank, a graph-based ranking algorithm that extracts key structural relationships between terms in a document; (2) term frequency–inverse document frequency, a statistical measure that identifies the most significant terms based on their frequency and uniqueness within the corpus; and (3) semantic annotation (Wikifier), a method that links text tokens to structured knowledge bases such as Wikipedia for better contextual understanding. These methods are used to analyze a dataset of 480 academic articles focusing on sustainability and circular economy in the construction sector. Our analysis revealed that circular construction is more specific and practical, emphasizing resource efficiency, waste management, and industry-specific processes, targeting the operational aspects of recycling and resource recovery. In contrast, sustainable construction encompasses a broader and more holistic scope, including urban planning, community development, and long-term environmental impacts. This study demonstrates how NLP methods can systematically disentangle closely related frameworks in construction literature, providing a replicable methodological framework for future data-driven investigations. By clarifying the distinctions and overlaps between the terms “circular construction” and “sustainable construction”, our research offers enhanced understanding for policymakers, industry practitioners, and academics aiming to integrate sustainable and circular principles effectively within the construction sector.},
  archive      = {J_PEERJCS},
  author       = {Shakarim Aubakirov and Alexandr Pak and Iskander Akhmetov and Aidana Tleuken and Huseyin Atakan Varol and Assel Akzhalova and Ferhat Karaca},
  doi          = {10.7717/peerj-cs.3085},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3085},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Beyond buzzwords: NLP reveals common threads in sustainable and circular construction discourse},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advancing thyroid diagnosis: Integrating AI-driven CAD framework with numerical data and ultrasound images. <em>PEERJCS</em>, <em>11</em>, e3063. (<a href='https://doi.org/10.7717/peerj-cs.3063'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an advanced computer-aided diagnosis (CAD) framework for thyroid disease diagnosis that integrates numerical patient data and ultrasound images. The framework uses cutting edge technologies, including Vision Transformers (ViTs) and SHapley Additive exPlanations (SHAPs), to increase diagnostic accuracy, interpretability, and clinical applicability. The proposed CAD framework employs the sparse search algorithm (SSA) for optimized feature selection from numerical data and the tree-structured Parzen estimator for tuning the hyperparameters. ViTs are utilized for analyzing thyroid ultrasound images, whereas SHAP provides explainable AI insights into model predictions. Extensive experiments were conducted on two datasets: the thyroid disease patient dataset and the DDTI: Thyroid Ultrasound Images dataset. Performance was evaluated via five-fold and ten-fold cross-validation utilizing metrics including accuracy, precision, and recall. The framework achieved promising performance, with models trained without data augmentation consistently outperforming their augmented counterparts. For the thyroid disease patient dataset, the best-performing model reported an accuracy of 99.71%, precision of 97.05%, recall of 99.29%, and F1-score of 98.16%. For the DDTI dataset, ViTs achieved an accuracy of 95.06% without augmentation, surpassing existing methodologies. Key features such as thyroxine, thyroid surgery, and thyroid-stimulating hormone (TSH) were identified as critical predictors of thyroid conditions. This study underscores the potentiality of AI-driven approaches in healthcare, paving the way for improved diagnostic outcomes and personalized treatment strategies.},
  archive      = {J_PEERJCS},
  author       = {Saleh Ateeq Almutairi},
  doi          = {10.7717/peerj-cs.3063},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3063},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Advancing thyroid diagnosis: Integrating AI-driven CAD framework with numerical data and ultrasound images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Precise distance measurement with stereo camera: Experimental results. <em>PEERJCS</em>, <em>11</em>, e3057. (<a href='https://doi.org/10.7717/peerj-cs.3057'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, image processing is used in many areas, especially artificial intelligence. This is because images are thought to contain a lot of information. In addition, many distance measurement studies have used image processing techniques. However, no studies have reached these high sensitivity and accuracy rates that can be used in engineering. The motivation of the study is to obtain the results of the experimental application of the image processing method, which can measure distances with high sensitivity and can also be used in engineering fields. In the study, the distances of 19 different target objects were measured using Total Station, Laser Meter, and Developed Prototype (Image Meter). Total Station measurement results were used as a reference and the Laser Meter and Image Meter results were compared. As a result of the comparison, it was determined that the developed Image Meter had a smaller error rate in 11 of the 19 comparisons. Results were obtained with an average error of 1.24% as a result of 19 measurements made with the developed Image Meter. The experimental results were also compared with theoretical calculation. As a result of the comparisons, it was determined that the results with the developed Image Meter were acceptable and could be improved with mechanical arrangements.},
  archive      = {J_PEERJCS},
  author       = {Haydar Yanık and Bülent Turan},
  doi          = {10.7717/peerj-cs.3057},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3057},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Precise distance measurement with stereo camera: Experimental results},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EquiRate: Balanced rating injection approach for popularity bias mitigation in recommender systems. <em>PEERJCS</em>, <em>11</em>, e3055. (<a href='https://doi.org/10.7717/peerj-cs.3055'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems often suffer from popularity bias problem, favoring popular items and overshadowing less known or niche content, which limits recommendation diversity and content exposure. The root reason for this issue is the imbalances in the rating distribution; a few popular items receive a disproportionately large share of interactions, while the vast majority garner relatively few. In this study, we propose the EquiRate method as a pre-processing approach, addressing this problem by injecting synthetic ratings into less popular items to make the dataset regarding rating distribution more balanced. More specifically, this method utilizes several synthetic rating injection and synthetic rating generation strategies: (i) the first ones focus on determining which items to inject synthetic ratings into and calculating the total number of these ratings, while (ii) the second ones concentrate on computing the concrete values of the ratings to be included. We also introduce a holistic and highly efficient evaluation metric, i.e., the FusionIndex, concurrently measuring accuracy and several beyond-accuracy aspects of recommendations. The experiments realized on three benchmark datasets conclude that several EquiRate’s variants, with proper parameter-tuning, effectively reduce popularity bias and enhance recommendation diversity. We also observe that some prominent popularity-debiasing methods, when assessed using the FusionIndex, often fail to balance the referrals’ accuracy and beyond-accuracy factors. On the other hand, our best-performing EquiRate variants significantly outperform the existing methods regarding the FusionIndex, and their superiority is more apparent for the high-dimension data collections, which are more realistic for real-world scenarios.},
  archive      = {J_PEERJCS},
  author       = {Mert Gulsoy and Emre Yalcin and Alper Bilge},
  doi          = {10.7717/peerj-cs.3055},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3055},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {EquiRate: Balanced rating injection approach for popularity bias mitigation in recommender systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D reconstruction of toys based on adaptive scaled neural radiation field. <em>PEERJCS</em>, <em>11</em>, e3053. (<a href='https://doi.org/10.7717/peerj-cs.3053'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of computer vision technology, 3D reconstruction of toys under single-view conditions still faces significant challenges in terms of detail loss and color distortion. For this reason, this article proposes an adaptive scale neural radiance fields (AS-NeRF) model to enhance the accuracy and realism of 3D toy reconstruction. The method constructs a multi-task feature extraction network based on the Vision Transformer, which simultaneously extracts and fuses multidimensional features such as texture, shape, color, and depth through a task dynamic modulation mechanism and a dynamic adapter layer, providing a rich and accurate contextual feature representation. The NeRF model is enhanced to incorporate an adaptive scaling mechanism that dynamically optimizes rendering sampling accuracy according to the local complexity of the scene. Spectral sensing techniques are integrated to reproduce the true colors of materials accurately. Finally, the conditional diffusion model is deeply integrated with NeRF, and high-dimensional conditional vectors are used to guide the inverse diffusion process in generating unobserved images with consistent geometric structure and physical properties. Experiments on the Co3D toy dataset demonstrate that AS-NeRF significantly outperforms existing mainstream methods in terms of peak signal-to-noise ratio (PSNR), structural similarity (SSIM), loss of perceptions (LPIPS), and Chamfer distance, thereby verifying the validity and advantages of the proposed method for high-quality toy 3D reconstruction tasks.},
  archive      = {J_PEERJCS},
  author       = {Jiajun Zou and Shaojiang Liu and Feng Wang and Weichuan Ni and Shitong Ye},
  doi          = {10.7717/peerj-cs.3053},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3053},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {3D reconstruction of toys based on adaptive scaled neural radiation field},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JDroid: Android malware detection using hybrid opcode feature vector. <em>PEERJCS</em>, <em>11</em>, e3051. (<a href='https://doi.org/10.7717/peerj-cs.3051'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of devices using the Android operating system makes these devices the primary target for malware developers. Researchers are investigating different techniques to protect end users from these attackers. While many of these techniques are successful in detecting malware, they also have some limitations. Because many applications today use advanced obfuscation techniques, advanced disguise, and variant generation techniques to bypass detection tools, this creates difficulties for security experts. However, the rich semantic information hidden in opcodes offers a promising way to distinguish benign applications from malicious ones. In this study, we propose a tool called JDroid that treats opcodes (Dalvik Opcode and Java ByteCode) as features based on static analysis. The proposed tool aims to detect malicious applications with a unique ensemble model in a stacked generalised structure that uses different opcode sequences as a hybrid, and where each feature is first trained separately and then used by an ensemble decision. For this purpose, opcodes are extracted from APK files by code analysis and directly converted into vectors as 0 and 1 according to their usage cases. A subset of 461 features, obtained through filtering and feature selection processes, is then created using fewer features. This increases efficiency and performance, avoids overfitting, and reduces computational cost. The datasets Drebin, Genome, MalDroid2020, CICInvesAndMal2019, and Omer are tested with an application pool consisting of 14 thousand applications, and the classification performance is compared with different machine learning methods. Experimental results show that the proposed approach has an accuracy value of 98.6% and an area under the curve (AUC) value of 99.6% in malware detection without being affected by the obfuscation process.},
  archive      = {J_PEERJCS},
  author       = {Recep Sinan Arslan},
  doi          = {10.7717/peerj-cs.3051},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3051},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {JDroid: Android malware detection using hybrid opcode feature vector},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight graph convolutional network with multi-attention mechanisms for intelligent action recognition in online physical education. <em>PEERJCS</em>, <em>11</em>, e3050. (<a href='https://doi.org/10.7717/peerj-cs.3050'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of online physical education in higher education has improved accessibility but presents challenges in recognizing complex movements and delivering individualized feedback. Existing action recognition models are often computationally intensive and struggle to generalize across diverse skeletal patterns. To address this, we propose a lightweight graph convolutional network (GCN) that integrates an improved Ghost module with multi-attention mechanisms, including a global attention mechanism (GAM) and a channel attention mechanism (CAM), to enhance spatial and temporal feature extraction. The model is trained end-to-end on 3D skeleton sequences and optimized for real-time efficiency. The computational cost is evaluated in terms of giga floating-point operations (GFLOPs), with the proposed model requiring only 6.2 GFLOPs per inference, over 60% less than the baseline ST-GCN. Experimental results on the NTU60RGB+D dataset demonstrate that the model achieves 90.8% accuracy in cross-subject and 96.8% in cross-view settings. These findings highlight the model’s effectiveness in balancing accuracy and efficiency, with promising applications in online physical education, rehabilitation monitoring, elderly movement analysis, and VR-based interfaces.},
  archive      = {J_PEERJCS},
  author       = {Yuhao You},
  doi          = {10.7717/peerj-cs.3050},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3050},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Lightweight graph convolutional network with multi-attention mechanisms for intelligent action recognition in online physical education},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identification of core sub-team on scientific collaboration networks with shapley method. <em>PEERJCS</em>, <em>11</em>, e3048. (<a href='https://doi.org/10.7717/peerj-cs.3048'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the core sub-teams that drive productivity in scientific collaboration networks is essential for research evaluation and team management. However, existing methods typically rank individual researchers by bibliometric impact or select structurally cohesive clusters, but rarely account for both collaboration patterns and joint scientific output. To address this limitation, we propose a novel two-dimensional framework that integrates network topology with research performance to identify core sub-teams. Specifically, we measure each sub-team’s marginal structural contribution using the Shapley value and quantify its collective impact using a sub-team H-index. To efficiently identify high-contributing sub-teams, we employ the Monte Carlo Tree Search algorithm, along with an approximation strategy to estimate Shapley values under computational constraints. We evaluate our method on 61 real-world scientific collaboration teams from Web of Science and Baidu Scholar data. Experimental results validate the effectiveness of our method in identifying core sub-teams, with the highest collaborative and citation impact. The proposed method offers a valuable analytical tool for research managers and funding agencies seeking to locate high-impact collaborative clusters, and it provides a generalizable framework for studies requiring the integration of structural and performance-based indicators in network analysis.},
  archive      = {J_PEERJCS},
  author       = {Lixin Zhou and Chen Liu and Xue Song},
  doi          = {10.7717/peerj-cs.3048},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3048},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Identification of core sub-team on scientific collaboration networks with shapley method},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of psychiatry clinical notes by diagnosis: A deep learning and machine learning approach. <em>PEERJCS</em>, <em>11</em>, e3045. (<a href='https://doi.org/10.7717/peerj-cs.3045'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of clinical notes into specific diagnostic categories is critical in healthcare, especially for mental health conditions like anxiety and adjustment disorder. In this study, we compare the performance of various artificial intelligence models, including both traditional machine learning approaches (random forest, support vector machine, K-nearest neighbors, decision tree, and eXtreme Gradient Boost) and deep learning models (DistilBERT and SciBERT), to classify clinical notes into these two diagnoses. Additionally, we implemented three oversampling strategies: No Oversampling, Random Oversampling, and Synthetic Minority Over-sampling Technique (SMOTE), to assess their impact on model performance. Hyperparameter tuning was also applied to optimize model accuracy. Our results indicate that oversampling techniques had minimal impact on model performance overall. The only exception was SMOTE, which showed a positive effect specifically with Bidirectional Encoder Representations from Transformers (BERT)-based models. However, hyperparameter optimization significantly improved accuracy across the models, enhancing their ability to generalize and perform on the dataset. The decision tree and eXtreme Gradient Boost models achieved the highest accuracy among machine learning approaches, both reaching 96%, while the DistilBERT and SciBERT models also attained 96% accuracy in the deep learning category. These findings underscore the importance of hyperparameter tuning in maximizing model performance. This study contributes to the ongoing research on AI-assisted diagnostic tools in mental health by providing insights into the efficacy of different model architectures and data balancing methods.},
  archive      = {J_PEERJCS},
  author       = {Sergio Rubio-Martín and María Teresa García-Ordás and Antonio Serrano-García and Clara Margarita Franch-Pato and Arturo Crespo-Álvaro and José Alberto Benítez-Andrades},
  doi          = {10.7717/peerj-cs.3045},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3045},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Classification of psychiatry clinical notes by diagnosis: A deep learning and machine learning approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Grammatical error correction for low-resource languages: A review of challenges, strategies, computational and future directions. <em>PEERJCS</em>, <em>11</em>, e3044. (<a href='https://doi.org/10.7717/peerj-cs.3044'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grammatical error correction (GEC) is crucial for enhancing the readability and comprehension of texts, particularly in improving text quality in low-resource languages. However, challenges such as data scarcity, linguistic diversity, and limited computational resources hinder advancements in this domain. To address these challenges, researchers have developed strategies such as synthetic data generation, multilingual pre-trained models, and cross-lingual transfer learning. This review synthesizes findings from key studies to explore effective GEC methods for low-resource languages, emphasizing approaches for handling limited annotated corpora, typological complexities, and evaluation challenges. Synthetic data generation techniques, including noise injection, adversarial error generation, and translationese-based augmentation, have proven vital for overcoming data scarcity. Multilingual and transfer learning approaches demonstrate effectiveness in adapting knowledge from high-resource languages to low-resource settings, especially when combined with fine-tuning on curated datasets. Additionally, linguistic diversity has been partially addressed through methods like morphology-aware embeddings, byte-level tokenization, and contextual data preprocessing. However, limited research exists on robust evaluation metrics tailored to diverse typologies, such as agglutinative and morphologically rich languages, and the creation of gold-standard datasets remains an ongoing challenge. Recent advancements in dataset construction and the use of large language models further enrich this field, offering scalable solutions for low-resource contexts. Despite notable progress, this review identifies gaps in evaluation methodologies and typology-specific solutions, calling for future innovations in multilingual modeling, dataset creation, and computationally efficient GEC systems tailored to the unique needs of low-resource languages.},
  archive      = {J_PEERJCS},
  author       = {Syauqie Muhammad Marier and Xiangfan Chen and Linan Zhu and Xiangjie Kong},
  doi          = {10.7717/peerj-cs.3044},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3044},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Grammatical error correction for low-resource languages: A review of challenges, strategies, computational and future directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task advanced convolutional neural network for robust lymphoblastic leukemia diagnosis, classification, and segmentation. <em>PEERJCS</em>, <em>11</em>, e3043. (<a href='https://doi.org/10.7717/peerj-cs.3043'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acute lymphoblastic leukemia (ALL), a hematologic malignancy characterized by the overproduction of immature lymphocytes, a type of white blood cell. Accurate and timely diagnosis of ALL is crucial for effective management. This article introduces a novel multi-task advanced convolutional neural network (MTA-CNN) framework for ALL detection in medical imaging data by simultaneously performing, expression classification, and disease detection. The MTA-CNN is based on a deep learning architecture that can handle multiple tasks simultaneously, allowing it to learn more comprehensive and generalizable features. With, expression classification, and disease detection tasks, the MTA-CNN effectively leverages the complementary information from each task to improve overall performance. The proposed framework employs CNNs to extract informative features from medical images. These features capture the spatial and temporal characteristics of the data, which are essential for accurate ALL diagnosis. The cascaded structure of the MTA-CNN allows the model to learn features at different levels of abstraction, from low-level to high-level, enabling it to capture both fine-grained and coarse-grained information. To ensure the reliability of the detection results, non-maximum suppression is employed to eliminate redundant detections, focusing only on the most likely candidates. Additionally, the MTA-CNN’s ability to accurately localize key facial landmarks provides valuable information for further analysis, including identifying abnormal structures or changes in anatomical features associated with ALL. Experimental results on a comprehensive dataset of medical images demonstrate the superiority of the MTA-CNN over other learning methods. The proposed framework achieved an accuracy of 0.978, precision of 0.979, recall of 0.967, F1-score of 0.973, specificity of 0.991, Cohen’s kappa of 0.979, and negative predictive value (NPV) of 0.990. These metrics significantly outperform baseline models, highlighting the MTA-CNN’s ability to accurately identify and classify ALL cases. The MTA-CNN offers a promising approach for improving the efficiency and accuracy of ALL diagnosis.},
  archive      = {J_PEERJCS},
  author       = {Sercan Yalcin and Zuhal Cetin Yalcin and Muhammed Yildirim and Bilal Alatas},
  doi          = {10.7717/peerj-cs.3043},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3043},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-task advanced convolutional neural network for robust lymphoblastic leukemia diagnosis, classification, and segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective optimization for smart cities: A systematic review of algorithms, challenges, and future directions. <em>PEERJCS</em>, <em>11</em>, e3042. (<a href='https://doi.org/10.7717/peerj-cs.3042'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing complexity and interdependence of urban systems, multi-objective optimization (MOO) has become a critical tool for smart-city planning, sustainability, and real-time decision-making. This article presents a systematic literature review (SLR) of 117 peer-reviewed studies published between 2015 and 2025, assessing the evolution, classification, and performance of MOO techniques in smart-city contexts. Existing algorithms are organised into four families—bio-inspired, mathematical theory-driven, physics-inspired, and machine-learning-enhanced—and benchmarked for computational efficiency, scalability, and scenario suitability across six urban domains: infrastructure, energy, transportation, Internet of Things (IoT)/cloud systems, agriculture, and water management. While established methods such as Non-dominated Sorting Genetic Algorithm II (NSGA-II) and Multiobjective Evolutionary Algorithm based on Decomposition (MOED/D) remain prevalent, hybrid frameworks that couple deep learning with evolutionary search display superior adaptability in high-dimensional, dynamic environments. Persistent challenges include limited cross-domain generalisability, inadequate uncertainty handling, and low interpretability of artificial intelligence (AI)-assisted models. Twelve research gaps are synthesised—from privacy-preserving optimisation and sustainable trade-off resolution to integration with digital twins, large language models, and neuromorphic computing—and a roadmap towards scalable, interpretable, and resilient optimisation frameworks is outlined. Finally, a ready-to-use benchmarking toolkit and a deployment-oriented algorithm-selection matrix are provided to guide researchers, engineers, and policy-makers in real-world smart-city applications. This review targets interdisciplinary researchers, optimisation developers, and smart-city practitioners seeking to apply or advance MOO techniques in complex urban systems.},
  archive      = {J_PEERJCS},
  author       = {YiFan Chen and Weng Howe Chan and Eileen Lee Ming Su and Qi Diao},
  doi          = {10.7717/peerj-cs.3042},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3042},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-objective optimization for smart cities: A systematic review of algorithms, challenges, and future directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive filter parameter reconstruction technology for rocket inertial navigation/satellite integrated navigation system. <em>PEERJCS</em>, <em>11</em>, e3040. (<a href='https://doi.org/10.7717/peerj-cs.3040'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of micro-electro-mechanical systems (MEMS) strapdown inertial navigation systems (SINS) with global navigation satellite systems (GNSS) has emerged as a significant area of research due to its compact size, affordability, and high precision. In the context of guided rocket-borne MEMS-SINS/GNSS integrated navigation systems, the performance of navigation is characterized by the need for high overload, accuracy, and real-time capability. A variety of enhanced algorithms based on Kalman filtering are currently employed as integrated filtering methods, which comprehensively address deviations in the system model to improve navigation performance. The noise characteristics of MEMS inertial guidance devices change dramatically under long-term storage conditions, while the dynamic flight environment of rockets and the high real-time requirements of navigation solving make the design of on-board combined navigation filters challenging. To address this issue, this article introduces the Adaptive Reconfigurable Extended Kalman Filter (AREKF) method. Initially, a precise system state model is developed to reflect the unique characteristics of the rocket flight environment, facilitating rapid convergence of the filtering process. Subsequently, during the rocket alignment process, a real-time reconstruction of filter parameters is implemented to enable adaptive and precise modeling of navigation parameters. This strategy ensures lower computational costs during rocket flight, enhances the accuracy of the navigation system, and produces real-time navigation outputs that exhibit high overload and precision. The results from the Six-Degree (6D) Model simulation and car-mounted experiments demonstrate that, compared to the traditional Extended Kalman Filter (EKF) algorithm and existing improved algorithms, the AREKF method significantly enhances the real-time navigation accuracy of rockets under high overload conditions.},
  archive      = {J_PEERJCS},
  author       = {Zhijie Yang and Guoguang Chen and Mingli Niu and Xiaolong Yan and Xiaoli Tian and Guocui Zhang},
  doi          = {10.7717/peerj-cs.3040},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3040},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Adaptive filter parameter reconstruction technology for rocket inertial navigation/satellite integrated navigation system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A progressive attention-based cross-modal fusion network for cardiovascular disease detection using synchronized electrocardiogram and phonocardiogram signals. <em>PEERJCS</em>, <em>11</em>, e3038. (<a href='https://doi.org/10.7717/peerj-cs.3038'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synchronized electrocardiogram (ECG) and phonocardiogram (PCG) signals provide complementary diagnostic insights crucial for improving the accuracy of cardiovascular disease (CVD) detection. However, existing deep learning methods often utilize single-modal data or employ simplistic early or late fusion strategies, which inadequately capture the complex, hierarchical interdependencies between these modalities, thereby limiting detection performance. This study introduces PACFNet, a novel progressive attention-based cross-modal feature fusion network, for end-to-end CVD detection. PACFNet features a three-branch architecture: two modality-specific encoders for ECG and PCG, and a progressive selective attention-based cross-modal fusion encoder. A key innovation is its four-layer progressive fusion mechanism, which integrates multi-modal information from low-level morphological details to high-level semantic representations. This is achieved by selective attention-based cross-modal fusion (SACMF) modules at each progressive level, employing cascaded spatial and channel attention to dynamically emphasize salient feature contributions across modalities, thus significantly enhancing feature learning. Signals are pre-processed using a beat-to-beat segmentation approach to analyze individual cardiac cycles. Experimental validation on the public PhysioNet 2016 dataset demonstrates PACFNet’s state-of-the-art performance, with an accuracy of 97.7%, sensitivity of 98%, specificity of 97.3%, and an F1-score of 99.7%. Notably, PACFNet not only excels in multi-modal settings but also maintains robust diagnostic capabilities even with missing modalities, underscoring its practical effectiveness and reliability. The source code is publicly available on Zenodo (https://zenodo.org/records/15450169).},
  archive      = {J_PEERJCS},
  author       = {Wei Peng Li and Joon Huang Chuah and Guo Jeng Tan and Chengyu Liu and Hua-Nong Ting},
  doi          = {10.7717/peerj-cs.3038},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3038},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A progressive attention-based cross-modal fusion network for cardiovascular disease detection using synchronized electrocardiogram and phonocardiogram signals},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attention-based BiLSTM-XGBoost model for reliability assessment and lifetime prediction of digital microfluidic systems. <em>PEERJCS</em>, <em>11</em>, e3037. (<a href='https://doi.org/10.7717/peerj-cs.3037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional methods for reliability and lifetime testing of digital microfluidic systems heavily rely on real-time monitoring data. This often leads to evaluation lag and limits their application, especially for complex droplets. To address these issues, this study proposes a novel prediction model for digital microfluidic (DMF) devices. The model combines an attention-based bidirectional long short-term memory (BiLSTM) with eXtreme Gradient Boosting (XGBoost) using a Stacking approach. This integrated model efficiently identifies the health state and predicts the failure time of digital microfluidic devices. This approach overcomes the limitations of traditional methods, such as over-reliance on sensor feedback and detection hysteresis. Experimental results demonstrate high prediction accuracy. The model achieved a mean absolute percentage error (MAPE) of 1.6464, Root mean squared error (RMSE) of 0.3667, mean absolute error (MAE) of 0.2557, and a coefficient of determination (R-squared) of 0.9949. Compared to baseline methods, the proposed BiLSTM-XGBoost model achieves the highest prediction accuracy, enabling effective health monitoring, problem identification, and failure prediction. Ultimately, this improves system reliability and lifetime with greater timeliness and accuracy.},
  archive      = {J_PEERJCS},
  author       = {Lifeng He and Qili Yang and Junxi Chen and Wenjing Liu and Zhijie Luo},
  doi          = {10.7717/peerj-cs.3037},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3037},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Attention-based BiLSTM-XGBoost model for reliability assessment and lifetime prediction of digital microfluidic systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Alpha-DehazeNet: Single image dehazing via RGBA haze modeling and adaptive learning. <em>PEERJCS</em>, <em>11</em>, e3036. (<a href='https://doi.org/10.7717/peerj-cs.3036'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is a vital research area in computer vision. Many existing deep learning-based dehazing methods rely on atmospheric scattering models with manually predefined, non-trainable parameters, which limits their adaptability and transferability. We propose Alpha-DehazeNet, a novel model that leverages red green blue alpha (RGBA) haze layer effect maps by defining a grayscale transparency map in the RGBA color space as the initial haze layer. Alpha-DehazeNet employs a U-Net generator enhanced with a spatial attention mechanism to encode haze-related features. This generator is integrated into an adversarial architecture with residual connections, enabling end-to-end training. Additionally, a depth consistency loss is introduced to improve dehazing accuracy. Alpha-DehazeNet outperforms several state-of-the-art models on synthetic datasets (ITS and OTS from RESIDE), achieving 37.35 dB peak signal-to-noise ratio (PSNR) on SOTS-indoor and 37.39 dB PSNR on SOTS-outdoor, while using only 8.86 million parameters. On real-world datasets, Alpha-DehazeNet delivers competitive results, although it shows limitations in handling non-white fog and cloud conditions. The code is publicly available at: https://doi.org/10.5281/zenodo.15361810.},
  archive      = {J_PEERJCS},
  author       = {Jin He and Ruibin Li},
  doi          = {10.7717/peerj-cs.3036},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3036},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Alpha-DehazeNet: Single image dehazing via RGBA haze modeling and adaptive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local-global multi-scale attention network for medical image segmentation. <em>PEERJCS</em>, <em>11</em>, e3033. (<a href='https://doi.org/10.7717/peerj-cs.3033'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the continuous advancement of deep learning technologies, deep learning-based medical image segmentation methods have achieved remarkable results. However, existing segmentation approaches still face several key challenges, including the insufficient extraction of local and global information from images and the inaccurate selection of core features. To address these challenges, this article proposes a novel medical image segmentation architecture—local-global multi-scale attention network (LGMANet). LGMANet introduces an innovative local-global information processing block (LGIPB) to effectively facilitate the deep mining of both local and global information during the downsampling process. In addition, an efficient multi-scale reconstruction attention (EMRA) module is designed to help the model accurately extract core features and multi-scale information while effectively suppressing irrelevant content. Experiments on the ISIC2018, CVC-ClinicDB, BUSI, and GLaS datasets demonstrate that LGMANet achieves IoU scores of 85.28%, 82.67%, 70.07%, and 88.90%, respectively, showcasing its superior segmentation performance.},
  archive      = {J_PEERJCS},
  author       = {Minghui Zhu and Dapeng Cheng and Yanyan Mao and Lu Sun and Wanting Jing},
  doi          = {10.7717/peerj-cs.3033},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3033},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Local-global multi-scale attention network for medical image segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic token encryption for preventing permission leakage in serverless architectures. <em>PEERJCS</em>, <em>11</em>, e3029. (<a href='https://doi.org/10.7717/peerj-cs.3029'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Serverless architecture simplifies application development and operation, but its permission control model based on static execution roles struggles to adapt to highly dynamic runtime environments, which can easily lead to the risk of permission and key leakage. To address this challenge, this article proposes a runtime dynamic token-based access control scheme. The scheme combines function context and user-defined security rules to achieve function-level dynamic authorization and request-level identity authentication. The generated dynamic tokens possess strong randomness, unpredictability, and one-time use characteristics, effectively reducing the harm caused by token leakage. Moreover, the designed multi-factor token verification model integrates dynamic factors such as call chain features and behavior patterns, which can defend against various security threats. Through social surveys, qualitative analysis, and extensive experiments, this article confirms that the proposed scheme significantly enhances the security of serverless applications while maintaining a controllable impact on platform performance. This research enriches the theoretical knowledge in the field of serverless security and provides new ideas for development practices, which is expected to promote the expansion of serverless architecture to enterprise-level scenarios and contribute to the healthy development of its ecosystem.},
  archive      = {J_PEERJCS},
  author       = {Yu Liu and Fu Li and Chenhao Sun},
  doi          = {10.7717/peerj-cs.3029},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3029},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic token encryption for preventing permission leakage in serverless architectures},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel hybrid TCN-TE-ANN model for high-precision solar irradiance prediction. <em>PEERJCS</em>, <em>11</em>, e3026. (<a href='https://doi.org/10.7717/peerj-cs.3026'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of solar irradiance is critical for optimizing solar energy systems, enhancing grid stability, and supporting sustainable energy transitions. While numerous studies have explored various methodologies for solar radiation prediction, challenges remain in achieving high accuracy across diverse geographic locations and temporal resolutions. This study presents a novel hybrid model combining temporal convolutional networks (TCN), Transformer encoders (TE), and artificial neural networks (ANN) to predict global horizontal irradiance (GHI) with high precision. Utilizing a comprehensive dataset from three significant U.S. solar energy sites—Desert Sunlight, Copper Mountain, and Solar Star—spanning 22 years at a 30-min temporal resolution, the proposed model demonstrated superior performance metrics, with R2 ranging from 0.94768 to 0.97417, root mean square error (RMSE) between 0.04776 and 0.06543 W/m2, and mean absolute error (MAE) between 0.02510 and 0.03526 W/m2. By leveraging TCN’s temporal feature extraction, TE’s attention mechanisms, and ANN’s dense layer refinements, the model demonstrates significant advancements over existing methods.},
  archive      = {J_PEERJCS},
  author       = {Murat Isik},
  doi          = {10.7717/peerj-cs.3026},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3026},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel hybrid TCN-TE-ANN model for high-precision solar irradiance prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive review of dimensionality reduction algorithms: Challenges, limitations, and innovative solutions. <em>PEERJCS</em>, <em>11</em>, e3025. (<a href='https://doi.org/10.7717/peerj-cs.3025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction (DR) simplifies complex data from genomics, imaging, sensors, and language into interpretable forms that support visualization, clustering, and modeling. Yet widely used methods like principal component analysis, t-distributed stochastic neighbor embedding, uniform manifold approximation and projection, and autoencoders are often applied as “black boxes,” neglecting interpretability, fairness, stability, and privacy. This review introduces a unified classification—linear, nonlinear, hybrid, and ensemble approaches—and assesses them against eight core challenges: dimensionality selection, overfitting, instability, noise sensitivity, bias, scalability, privacy risks, and ethical compliance. We outline solutions such as intrinsic dimensionality estimation, robust neighborhood graphs, fairness-aware embeddings, scalable algorithms, and automated tuning. Drawing on case studies from bioinformatics, vision, language, and Internet of Things analytics, we offer a practical roadmap for deploying dimensionality reduction methods that are scalable, interpretable, and ethically sound—advancing responsible artificial intelligence in high-stakes applications.},
  archive      = {J_PEERJCS},
  author       = {Aasim Ayaz Wani},
  doi          = {10.7717/peerj-cs.3025},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3025},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comprehensive review of dimensionality reduction algorithms: Challenges, limitations, and innovative solutions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An enhanced approach for automatic annotation of error codes based on seq2edit. <em>PEERJCS</em>, <em>11</em>, e3024. (<a href='https://doi.org/10.7717/peerj-cs.3024'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep natural language translation models have been used for automatic code error correction and have demonstrated outstanding potential. However, a large and accurately annotated training dataset is essential for these models to perform well. The key to improving the performance of these models lies in automatically and accurately annotating code errors and establishing a larger training dataset. Recently, a code error automatic annotation method based on Seq2edit has been proposed to optimize the dataset. However, the accuracy of the annotation is affected because tokens in the input code from the same statement may be aligned to different statements. This article proposes a Seq2edit annotation method based on the source code’s sentence structure. By dividing the code into statements with independent meanings and introducing a cost coefficient to improve the Levenshtein algorithm, this method optimizes the calculation of edit distance and enhances the ability to align tokens. Experimental results show that this method can fully utilize the contextual information of the source code during the automatic annotation process, leading to a significant improvement in annotation accuracy.},
  archive      = {J_PEERJCS},
  author       = {Jian Wang and Tao Lin and Rongsen Zhao and Huiling Zhao},
  doi          = {10.7717/peerj-cs.3024},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3024},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An enhanced approach for automatic annotation of error codes based on seq2edit},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intent-aware knowledge graph-based model for electrical power material recommendation. <em>PEERJCS</em>, <em>11</em>, e3023. (<a href='https://doi.org/10.7717/peerj-cs.3023'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of electrical power material management, it is paramount that users receive accurate recommendations regarding the electrical power materials they require. Recently, a growing number of studies have been dedicated to graph neural network (GNN)-based recommendation systems due to their ability to seamlessly combine node information with topological structure, enhancing the effectiveness of recommendations. However, a notable drawback of current GNN-based recommendation is their inability to explicitly capture users’ intent in recommendations, which limits the performance. In fact, users’ intent is crucial in determining their actions. One example is when users first form an intent to buy a particular set of items and then choose a specific item from the set based on their preferences. To fill this gap, this article proposes an intent-aware knowledge graph-based model for electrical material recommendation, named IKG-EMR. IKG-EMR models user preferences and intent by leveraging knowledge graph and user behavior sequences, respectively. Specifically, a graph neural network is adopted to generate user intent embedding and item embedding from the tripartite graph of “User-Item-Topic”, and a multi-head attention network (Transformer) is used for extracting preference from user behavior sequences. Finally, an adaptive fusion with attention network is devised to generate comprehensive user representation by integrating user preference and intent features. Extensive experiments conducted on the real-life electric power materials show that our proposed model outperforms state-of-the-art methods.},
  archive      = {J_PEERJCS},
  author       = {Lin Zhao and Ning Luan and Weihua Cheng and Shuming Feng and Hui Wang and Yongcheng Yang and Guixiang Zhu},
  doi          = {10.7717/peerj-cs.3023},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3023},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Intent-aware knowledge graph-based model for electrical power material recommendation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new machine learning method for rainfall classification: Temporal random tree. <em>PEERJCS</em>, <em>11</em>, e3022. (<a href='https://doi.org/10.7717/peerj-cs.3022'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional classification algorithms usually assume that all samples in a dataset contribute equally to the training of a machine learning model, which is not always the case. In fact, samples in temporal data, such as precipitation data, may not have equal importance; more recent samples contain more accurate and useful information than earlier ones. To address this issue, the article proposes a novel method, named temporal random tree (TRT), in which recent training samples have a greater impact on the model’s decision-making process. It divides the dataset into temporal segments, assigns higher weights to classifiers trained on more recent data, and employs a weighted majority voting strategy. The experiments demonstrated the effectiveness of TRT on the real-world WeatherAUS precipitation dataset, achieving an accuracy of 83.54%, which represents a 5% improvement over the traditional random tree method. Additionally, our method achieved an average improvement of 9.98% compared to state-of-the-art results in the recent literature. These findings highlight TRT’s potential as a valuable method for spatiotemporal rainfall classification.},
  archive      = {J_PEERJCS},
  author       = {Kokten Ulas Birant and Bita Ghasemkhani and Özlem Varlıklar and Derya Birant},
  doi          = {10.7717/peerj-cs.3022},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3022},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A new machine learning method for rainfall classification: Temporal random tree},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fishing operation type recognition based on multi-branch convolutional neural network using trajectory data. <em>PEERJCS</em>, <em>11</em>, e3020. (<a href='https://doi.org/10.7717/peerj-cs.3020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate identification of fishing vessel operations is vital for sustainable fishery management. Existing methods inadequately exploit spatiotemporal contextual information in vessel trajectories and fail to effectively fuse multimodal data. To address this, this study proposes a novel framework integrating Geohash-based geocoding with embedding techniques inspired by natural language processing to extract spatiotemporal features from trajectory sequences. We develop a multi-branch 1D convolutional neural network (MB-1dCNN) to minimize feature engineering dependency while enhancing operational-type recognition. Comparative experiments evaluate Geohash encoding lengths and network architectures (single-branch vs. multi-branch, fully-connected vs. 1D-CNN). Results indicate optimal Geohash encoding at length 5. The multi-branch structure significantly outperforms single-branch counterparts, and MB-1dCNN demonstrates superior performance over multi-branch model with fully connected layers (MB-FCNN), achieving additional gains in accuracy and F1-score. Key findings reveal: (1) 1D-CNN processing surpasses fully-connected networks in sequential feature extraction, (2) Multi-branch architectures enhance information fusion capabilities. The proposed MB-1dCNN establishes state-of-the-art performance for trajectory-based fishing operation recognition, offering valuable insights for spatial computing applications in maritime surveillance.},
  archive      = {J_PEERJCS},
  author       = {Bohui Jiang and Weifeng Zhou},
  doi          = {10.7717/peerj-cs.3020},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3020},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fishing operation type recognition based on multi-branch convolutional neural network using trajectory data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Advanced clustering and transfer learning based approach for rice leaf disease segmentation and classification. <em>PEERJCS</em>, <em>11</em>, e3018. (<a href='https://doi.org/10.7717/peerj-cs.3018'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rice, the world’s most important food crop, requires an early and accurate identification of the diseases that infect rice panicles and leaves to increase production and reduce losses. Most conventional methods of diagnosing diseases involve the use of manual instruments, which are ineffective, imprecise, and time-consuming. In light of such drawbacks, this article introduces an improved deep learning and transfer learning method for diagnosing and categorizing rice leaf diseases proficiently. First, all input images are preprocessed; the images are resized to a fixed size before applying a sophisticated contrast enhanced adaptive histogram equalization procedure. Diseased regions are then segmented through the developed gravity weighted kernelised density clustering algorithm. In terms of feature extraction, EfficientNetB0 is fine-tuned by subtracting the last fully connected layers, and the classification is conducted with the new fully connected layers. Also, the tent chaotic particle snow ablation optimizer is added into the learning process in order to improve the learning process and shorten the time of convergence. The performance of the proposed framework was tested on two benchmark datasets and presented accuracy results of 98.87% and 97.54%, respectively. Comparisons of the proposed method with six fine-tuned models show the performance advantage and validity of the proposed method.},
  archive      = {J_PEERJCS},
  author       = {Samia Nawaz Yousafzai and Fahd N. Al-Wesabi and Hadeel Alsolai and Shouki A. Ebad and Inzamam Mashood Nasir and Emad Fadhal and Adel Thaljaoui},
  doi          = {10.7717/peerj-cs.3018},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3018},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Advanced clustering and transfer learning based approach for rice leaf disease segmentation and classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessing BERT-based models for arabic and low-resource languages in crime text classification. <em>PEERJCS</em>, <em>11</em>, e3017. (<a href='https://doi.org/10.7717/peerj-cs.3017'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bidirectional encoder representations from Transformers (BERT) has recently attracted considerable attention from researchers and practitioners, demonstrating notable effectiveness in various natural language processing (NLP) tasks, including text classification. This efficacy can be attributed to its unique architectural features, particularly its ability to process text using both left and right context, having been pre-trained on extensive datasets. In the context of the criminal domain, the classification of data is a crucial activity, and Transformers are increasingly recognized for their potential to support law enforcement efforts. BERT has been released in English and Chinese, as well as a multilingual version that accommodates over 100 languages. However, there is a pressing need to analyze the availability and performance of BERT in Arabic and other low-resource languages. This study primarily focuses on analyzing BERT-based models tailored for the Arabic language; however, due to the limited number of existing studies in this area, the research extends to include other low-resource languages. The study evaluates these models’ performance in comparison to machine learning (ML), deep learning (DL), and other Transformer models. Furthermore, it assesses the availability of relevant data and examines the effectiveness of BERT-based models in low-resource linguistic contexts. The study concludes with recommendations for future research directions, supported by empirical statistical evidence.},
  archive      = {J_PEERJCS},
  author       = {Njood K. Al-harbi and Manal Alghieth},
  doi          = {10.7717/peerj-cs.3017},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3017},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Assessing BERT-based models for arabic and low-resource languages in crime text classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Further observations on the security of speck32-like ciphers using machine learning. <em>PEERJCS</em>, <em>11</em>, e3015. (<a href='https://doi.org/10.7717/peerj-cs.3015'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread deployment of Internet of Things across various industries, the security of communications between different devices is one of the critical concerns to consider. The lightweight cryptography emerges as a specialized solution to address security requirements for resource-constrained environments. Consequently, the comprehensive security evaluation of the lightweight cryptographic primitives—from the structure of ciphers and cryptographic components—has become imperative. In this article, we focus on the security evaluation of rotation parameters in the Speck32-like lightweight cipher family. We establish a machine learning-driven security evaluation framework for the rotational parameter selection principles—the core of Speck32’s design architecture. To assess different parameters security, we develop neural-differential distinguishers with considering of two distinct input difference models: (1) the low-Hamming-weight input differences and (2) the input differences from optimal differential characteristics. Our methodology achieves the security evaluation of 256 rotation parameters using the accuracy of neural distinguishers as the evaluation criteria. Our results illustrate the parameter (7,3) has stronger ability to resist machine learning-aided distinguishing attack compared to the standard (7,2) configuration. To our knowledge, this represents the first comprehensive study applying machine learning techniques for security assessment of Speck32-like ciphers. Furthermore, we investigate the reason for the difference in the accuracy of neural distinguishers with different rotation parameters. Our experimental results demonstrate that the bit bias in output differences and truncated differences is the important factor affecting the accuracy of distinguishers.},
  archive      = {J_PEERJCS},
  author       = {Zezhou Hou and Jiongjiong Ren and Shaozhen Chen},
  doi          = {10.7717/peerj-cs.3015},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3015},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Further observations on the security of speck32-like ciphers using machine learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dual-phase deep learning framework for advanced phishing detection using the novel OptSHQCNN approach. <em>PEERJCS</em>, <em>11</em>, e3014. (<a href='https://doi.org/10.7717/peerj-cs.3014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Phishing attacks are now regarded as one of the most prevalent cyberattacks that often compromise the security of different communication and internet networks. Phishing websites are created with the goal of generating cyber threats in order to ascertain the user’s financial information. Fake websites are frequently created and circulated online, which results in the loss of essential user assets. Phishing websites can result in monetary loss, intellectual property theft, damage to one’s reputation, and disruption of regular business activities. Over the past decade, a number of anti-phishing tactics have been proposed to detect and reduce these attempts. They are still imprecise and ineffective, though. Deep Learning (DL), which can precisely learn the intrinsic features of the websites and recognize phishing websites, is one of the innovative techniques utilized to solve this issue. Methods In this study, we proposed a novel OptSHQCNN phishing detection method. Pre-deployment and post-deployment are the two phases of the proposed methodology. The dataset undergoes preprocessing in the pre-deployment phase, which includes data balancing, and handling invalid features, irrelevant features, and missing values. The convolutional block attention module (CBAM) then extracts the main characteristics from web page code and linkages. The red kite optimization algorithm (RKOA) selects the significant key attributes in the third stage. The final phase involves classifying the data using the Shallow hybrid quantum-classical convolutional neural network (SHQCNN) model. To improve the effectiveness of the classification approach, the hyperparameters present in the SHQCNN model are fine-tuned using the shuffled shepherd optimization algorithm (SSOA). Results In the post-deployment phase, the URL is encoded using Optimized Bidirectional Encoder Representations from Transformers (OptBERT), after which the features are extracted. The retrieved properties are fed into a trained classifier. Next, a prediction of “phishing” or “Legitimate” is produced by the classifier. With a maximum of above 99% accuracy, precision, recall, and F1-score, respectively, the investigation’s findings showed that the suggested technique performed better than other popular phishing detection methods. The creation of a security plugin for clients, browsers, and other instant messaging applications that operate on network edges, PCs, smartphones, and other personal terminals can be aided by these findings.},
  archive      = {J_PEERJCS},
  author       = {Srikanth Meda and Vangipuram Sesha Srinivas and Killi Chandra Bhushana Rao and Repudi Ramesh and Narasimha Rao Yamarthi},
  doi          = {10.7717/peerj-cs.3014},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3014},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A dual-phase deep learning framework for advanced phishing detection using the novel OptSHQCNN approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting sport event outcomes using deep learning. <em>PEERJCS</em>, <em>11</em>, e3011. (<a href='https://doi.org/10.7717/peerj-cs.3011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the outcomes of sports events is inherently difficult due to the unpredictable nature of gameplay and the complex interplay of numerous influencing factors. In this study, we present a deep learning framework that combines a one-dimensional convolutional neural network (1D CNN) with a Transformer architecture to improve prediction accuracy. The 1D CNN effectively captures local spatial patterns in structured match data, while the Transformer leverages self-attention mechanisms to model long-range dependencies. This hybrid design enables the model to uncover nuanced feature interactions critical to outcome prediction. We evaluate our approach on a benchmark sports dataset, where it outperforms traditional machine learning methods and standard deep learning models in both accuracy and robustness. Our results demonstrate the promise of integrating convolutional and attention-based mechanisms for enhanced performance in sports analytics and predictive modeling.},
  archive      = {J_PEERJCS},
  author       = {Jianxiong Gao and Yi Cheng and Jianwei Gao},
  doi          = {10.7717/peerj-cs.3011},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3011},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting sport event outcomes using deep learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A social recommendation model based on adaptive residual graph convolution networks. <em>PEERJCS</em>, <em>11</em>, e3010. (<a href='https://doi.org/10.7717/peerj-cs.3010'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorporating social information in the recommendation algorithm based on graph neural network (GNN) alleviates the data sparsity and cold-start problems to a certain extent, and effectively improves the recommendation performance of the model. However, there are still shortcomings in the existing studies: on the one hand, the potential effect of noise in the raw data is ignored; on the other hand, only relying on the single interaction information between the user and the item and failing to make full use of the rich multi-aided information. These factors lead to an unsatisfactory learning effect of the model. To address the above problems, we propose a social recommendation model based on adaptive residual graph convolutional networks (SocialGCNRI). Specifically, we use the idea of fast Fourier transform (FFT), a filtering algorithm in the field of signal processing, to attenuate the raw data noise in the frequency domain, followed by utilizing the user-social relations, item-association relations, and user-item-interaction relations to form a heterogeneous graph to supplement the model information, and finally using a graph convolution algorithm with an adaptive residual graph to improve the expressive power of the model. Extensive experiments on two real datasets show that SocialGCNRI outperforms state-of-the-art social recommendation methods on a variety of common evaluation metrics.},
  archive      = {J_PEERJCS},
  author       = {Rui Chen and Kangning Pang and Qingfang Liu and Lei Zhang and Hao Wu and Cundong Tang and Pu Li},
  doi          = {10.7717/peerj-cs.3010},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3010},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A social recommendation model based on adaptive residual graph convolution networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting danceability and song ratings using deep learning and auditory features. <em>PEERJCS</em>, <em>11</em>, e3009. (<a href='https://doi.org/10.7717/peerj-cs.3009'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting a song’s danceability and overall rating poses a significant challenge due to the complex interplay between musical characteristics and listener preferences. In this study, we propose a deep learning framework that jointly addresses the tasks of danceability estimation and popularity prediction. Our model integrates a Bidirectional Long Short-Term Memory (BiLSTM) network to capture sequential and contextual patterns from categorical inputs, alongside a Residual Network (ResNet) that extracts hierarchical representations from numerical auditory features. These complementary feature streams are fused using a cross-attention mechanism, enabling the model to effectively learn intricate relationships across heterogeneous data modalities. Experimental evaluations demonstrate that our approach consistently outperforms traditional machine learning baselines and recent deep learning models. The results demonstrate the effectiveness of cross-attention in structured music data modelling and highlight the framework’s potential in advancing music recommendation and audio analysis systems.},
  archive      = {J_PEERJCS},
  author       = {Wei Wu},
  doi          = {10.7717/peerj-cs.3009},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3009},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting danceability and song ratings using deep learning and auditory features},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-learning type-2 fuzzy systems with adaptive rule reduction for time series forecasting. <em>PEERJCS</em>, <em>11</em>, e3004. (<a href='https://doi.org/10.7717/peerj-cs.3004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In rapidly changing scenarios, uncertainty and chaotic oscillations often obstruct time series prediction. However, Type-1 fuzzy systems face challenges in handling high uncertainty levels, therefore, Type-2 fuzzy systems become a better solution. Nonetheless, the complexity of Type-2 fuzzy models can produce overwhelming rules, compromising interpretability and computational efficiency. We present a Self-Learning Type-2 Fuzzy System with adaptive rule reduction that optimizes the rule base as forecast accuracy begins to deteriorate after adaptation. Our model combines participatory learning (PL) and Kernel Recursive Least Squares (KRLS) for online learning, an Adaptive reduced rule strategy to eliminate repeating rules and gain computational efficiency. Our approach incorporates a compatibility measure rooted in Type-2 fuzzy sets, paving the way for an improved consideration of uncertainty. Complex datasets, including Mackey-Glass chaotic time series and Taiwan Capitalization Weighted Stock Index (TAIEX), are used to evaluate the model, which demonstrates its superior forecasting performance compared to state-of-the-art models. Experiments show that our solution, through the development of a few rules, obtains lower error measures maintaining a small rule base, thus proving to be a scalable approach amenable to on-line deployment in fast paced environments such as those appearing in the financial markets, industrial processes and others that demand highly accurate time series forecasts in the presence of uncertainty.},
  archive      = {J_PEERJCS},
  author       = {Abdulwhab Alkharashi and Gaganjot Kaur and Hadeel Alsolai and Hatim Dafaalla and Somia Asklany and Othman Alrusaini and Ali Alqazzaz and Menwa Alshammeri},
  doi          = {10.7717/peerj-cs.3004},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3004},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Self-learning type-2 fuzzy systems with adaptive rule reduction for time series forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving gaussian naive bayes classification on imbalanced data through coordinate-based minority feature mining. <em>PEERJCS</em>, <em>11</em>, e3003. (<a href='https://doi.org/10.7717/peerj-cs.3003'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a widely used classification model, the Gaussian Naive Bayes (GNB) classifier experiences a significant decline in performance when handling imbalanced data. Most traditional approaches rely on sampling techniques; however, these methods alter the quantity and distribution of the original data and are prone to issues such as class overlap and overfitting, thus presenting clear limitations. This article proposes a coordinate transformation algorithm based on radial local relative density changes (RLDC). A key feature of this algorithm is that it preserves the original dataset’s quantity and distribution. Instead of modifying the data, it enhances classification performance by generating new features that more prominently represent minority classes. The algorithm transforms the dataset from absolute coordinates to RLDC-relative coordinates, revealing latent local relative density change features. Due to the imbalanced distribution, sparse feature space, and class overlap, minority class samples can exhibit distinct patterns in these transformed features. Based on these new features, the GNB classifier can increase the conditional probability of the minority class, thereby improving its classification performance on imbalanced datasets. To validate the effectiveness of the proposed algorithm, this study conducts comprehensive comparative experiments using the GNB classifier on 20 imbalanced datasets of varying scales, dimensions, and characteristics. The evaluation includes 10 oversampling algorithms, two undersampling algorithms, and two hybrid sampling algorithms. Experimental results show that the RLDC-based coordinate transformation algorithm ranks first in the average performance across three classification evaluation metrics. Compared to the average values of the comparison algorithms, it achieves improvements of 21.84%, 33.45%, and 54.63% across the three metrics, respectively. This algorithm offers a novel approach to addressing the imbalanced data problem in GNB classification and holds significant theoretical and practical value.},
  archive      = {J_PEERJCS},
  author       = {Wei Wang and Li Yan and Fen Liu and Yanxi Li},
  doi          = {10.7717/peerj-cs.3003},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3003},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving gaussian naive bayes classification on imbalanced data through coordinate-based minority feature mining},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid deep learning framework for skin disease localization and classification using wearable sensors. <em>PEERJCS</em>, <em>11</em>, e3002. (<a href='https://doi.org/10.7717/peerj-cs.3002'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate detection of skin diseases is essential for timely intervention and treatment. This article proposes a patch-based, interpretable deep learning framework for skin disease detection using wearable sensors and clinical data. Specifically, a fully convolutional residual neural network (FCRN) is employed to extract local features from high-resolution skin images captured via wearable sensors, using a patch-level training approach. Pre-processing techniques—including image resampling, intensity normalization, and noise reduction—standardize the input data to ensure consistency across sensor variations. To enhance local feature learning, the FCRN incorporates residual modules, which mitigate gradient vanishing and improve model performance. The framework generates disease probability maps that visualize regions of high diagnostic risk, providing interpretable insights into skin anomalies. In the proposed methodology, a convolutional neural network (CNN) integrates image-derived features with clinical data such as patient demographics, symptoms, and medical history. This CNN-based multimodal fusion approach improves the model’s ability to capture spatial relationships and enhances classification performance. Experimental evaluations demonstrate that the proposed framework achieves state-of-the-art results across multiple evaluation metrics, including accuracy, sensitivity, and specificity. The interpretable disease probability maps highlight affected skin regions, enhancing model transparency and clinical usability. This approach demonstrates the potential of combining wearable sensor technology with deep learning for efficient, scalable, and explainable skin disease detection, laying the foundation for real-time clinical applications.},
  archive      = {J_PEERJCS},
  author       = {Xiaoling Zhao and Huixin Zhang and Qian Zheng and Caihong Jing},
  doi          = {10.7717/peerj-cs.3002},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3002},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid deep learning framework for skin disease localization and classification using wearable sensors},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning in time series forecasting with transformer models and RNNs. <em>PEERJCS</em>, <em>11</em>, e3001. (<a href='https://doi.org/10.7717/peerj-cs.3001'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given the increasing need for accurate weather forecasts, the use of neural networks, especially transformer and recurrent neural networks (RNNs), has been highlighted for their ability to capture complex patterns in time series. This study examined 14 neural network models applied to forecast weather variables, evaluated using metrics such as median absolute error (MedianAbsE), mean absolute error (MeanAbsE), maximum absolute error (MaxAbsE), root mean squared percent error (RMSPE), and root mean square error (RMSE). Transformer-based models such as Informer, iTransformer, Former, and patch time series transformer (PatchTST) stood out for their accuracy in capturing long-term patterns, with Informer showing the best performance. In contrast, RNN models such as auto-temporal convolutional networks (TCN) and bidirectional TCN (BiTCN) were better suited to short-term forecasting, despite being more prone to significant errors. Using iTransformer it was possible to achieve a MedianAbsE of 1.21, MeanAbsE of 1.24, MaxAbsE of 2.86, RMSPE de 0.66, and RMSE de 1.43. This study demonstrates the potential of neural networks, especially transformers, to improve accuracy, providing a practical and theoretical basis for selecting the most suitable models for predictive applications.},
  archive      = {J_PEERJCS},
  author       = {Rogerio Pereira dos Santos and João P. Matos-Carvalho and Valderi R. Q. Leithardt},
  doi          = {10.7717/peerj-cs.3001},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3001},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning in time series forecasting with transformer models and RNNs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effective classification for neonatal brain injury using EEG feature selection based on elastic net regression and improved crow search algorithm. <em>PEERJCS</em>, <em>11</em>, e3000. (<a href='https://doi.org/10.7717/peerj-cs.3000'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neonatal brain injury carries the risk of neurological sequelae such as epileptic seizures, cerebral palsy, intellectual disability, and even death. Classification methods based on electroencephalography (EEG) signals and machine learning algorithms are crucial for assessing neonatal brain injury. However, classification methods that utilise all features from the original EEG signals may result in lengthy training and classification times, thereby reducing the performance of the classification system. This article presents a novel classification system with a proposed feature selection method for assessing neonatal brain injury, in which the feature selection method is combined using elastic net (EN) regression and an improved crow search algorithm (ICSA), named EN-ICSA. In the EN-ICSA method, EN regression is used to conduct the pre-screening of features. The ICSA is utilised to select the essential figures further by introducing the dynamic perception probability for deciding whether to search locally or globally, a novel neighbor-following strategy for the local search and a global search strategy according to the crow’s search experience, resulting in accelerating the search efficiency while effectively avoiding falling into local optima. Experimental results demonstrate that the proposed system, based on support vector machine (SVM) with the EN-ICSA feature selection method, performs exceptionally well compared to other traditional machine learning and feature selection methods, achieving an accuracy of 91.94%, precision of 92.32%, recall of 89.85%, and F1-score of 90.82%.},
  archive      = {J_PEERJCS},
  author       = {Ling Li and Tao Yue and Hui Wu and Yanping Zhao and Qinmei Liu and Hairong Zhang and Wei Xu},
  doi          = {10.7717/peerj-cs.3000},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e3000},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Effective classification for neonatal brain injury using EEG feature selection based on elastic net regression and improved crow search algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of predictive maintenance efficiency with the comparison of machine learning models in machining production process in brake industry. <em>PEERJCS</em>, <em>11</em>, e2999. (<a href='https://doi.org/10.7717/peerj-cs.2999'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The utilization of technologies such as artificial intelligence (AI) and machine learning (ML) in industrial sectors has become a crucial requirement to enhance the efficiency and stability of production processes. Regular maintenance of machines and early detection of faults play a critical role in ensuring uninterrupted production and business continuity. Predictive maintenance practices, combined with sensors and data analysis methods, enable the collection, analysis, and transformation of machine-related data into meaningful insights. As a result, the anticipation of potential machine failures, the execution of planned maintenance activities, and the prevention of unexpected downtime become possible. These methods not only improve productivity in production processes but also contribute to reducing maintenance costs. Methods This study aims to predict machine faults using data analysis methods and enhance the accuracy performance of these predictions for an industrial company that produces braking components. Comprehensive examination and analysis of data were conducted to understand the symptoms and relationships of machine failures. ML classification methods were employed in the relevant study. Results Challenges such as the imbalance of class distributions in the dataset, the presence of missing and outlier values, and the high costs of necessary equipment and training pose significant barriers to implementation. Addressing these issues is critical for achieving effective predictive maintenance solutions. In order to achieve more accurate results, data splitting and k-fold cross-validation methods were applied during the learning and testing phases to overcome the imbalance problem in the dataset, undersampling techniques were applied, and outlier detection and normalization processes were used to improve data quality. The model performances, evaluated through accuracy, precision, recall, and F1-score, area under the curve (AUC), Cohen’s Matthew’s correlation coefficient (MCC) were compared. Hyperparameter optimization was also performed, resulting in significant improvements in model performance. This study contributes to the literature in terms of predictive maintenance application, classification, and data partitioning techniques. The findings highlight the importance of data preprocessing and advanced modeling techniques in predictive maintenance and emphasize how addressing data challenges can enhance the overall performance and reliability of ML models.},
  archive      = {J_PEERJCS},
  author       = {Can Aydın and Burak Evrentuğ},
  doi          = {10.7717/peerj-cs.2999},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2999},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evaluation of predictive maintenance efficiency with the comparison of machine learning models in machining production process in brake industry},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tracing truth: Dynamic temporal networks for multi-modal fake news detection. <em>PEERJCS</em>, <em>11</em>, e2998. (<a href='https://doi.org/10.7717/peerj-cs.2998'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the internet continues to evolve rapidly and social media becomes increasingly prevalent, the ways people access information has become increasingly diverse. However, the proliferation of fake news has emerged as a critical problem, presenting major challenges to the integrity of the information ecosystem. To address the complex propagation mechanisms of fake news, existing studies leverage multi-modal information and dynamic propagation social graphs for effective detection. Nonetheless, capturing the temporal relationships of propagation nodes in dynamic social networks accurately and dynamically integrating multi-modal information for improved detection accuracy remains a technical challenge. In response, This study proposes a multimodal approach to fake news detection—the dynamic temporal network (DTN) model. Firstly, this model designs a time similarity strength metric to measure the temporal similarity among nodes in propagation sequences and introduces a weighting mechanism to dynamically fuse multi-modal information. Secondly, it constructs a social propagation graph model, enhancing node representation through the dynamic variations of time similarity and graph structure, and utilizes the Transformer encoder to extract the overall semantic features of news propagation. Furthermore, the model views the news propagation process as a complex system, analyzing the temporal dynamics of news in real social networks, effectively revealing the abnormal propagation patterns of fake news. Further analysis demonstrates that the proposed DTN model exhibits high accuracy and effectiveness in multi-modal fake news detection.},
  archive      = {J_PEERJCS},
  author       = {Jiaen Hu and Juan Zhang and Zichen Li},
  doi          = {10.7717/peerj-cs.2998},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2998},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Tracing truth: Dynamic temporal networks for multi-modal fake news detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DA-FIS: A high-speed dynamic adaptive fault injection server framework for reliable FPGA-based embedded systems. <em>PEERJCS</em>, <em>11</em>, e2996. (<a href='https://doi.org/10.7717/peerj-cs.2996'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault injection is a critical technique for assessing the reliability of field programmable gate array (FPGA)-based embedded systems, particularly in radiation-prone and safety-critical applications. Conventional fault injection methods, such as bit upset fault injection testing (BUFIT), single critical fault injection testing (SCFIT), and dynamic partial reconfiguration (DPR), suffer from high resource overhead, slow injection speeds, and limited adaptability, making them inadequate for real-time fault resilience evaluation. This article introduces the dynamic adaptive fault injection server (DA-FIS), a high-speed, scalable, and resource-efficient fault injection framework designed to overcome these limitations. Unlike traditional methods, DA-FIS employs a configurable LFSR-based fault generator that enables adaptive and real-time fault injection based on workload sensitivity and system conditions. The proposed framework integrates masking logic and dynamic propagation tracking, allowing precise injection of single-event upsets (SEUs) and multiple-bit upsets (MBUs) into FPGA configuration memory and logic without disturbing non-targeted regions. DA-FIS is implemented on the Xilinx Zynq-7000 FPGA and evaluated across multiple benchmark workloads, including the Bubble Sort algorithm, 4-bit adder, 4-bit multiplier, and counter-based logic circuits. Experimental results demonstrate that DA-FIS achieves a fault injection rate of 111.1 faults per second, outperforming BUFIT (53.4 faults/s), SCFIT (27 faults/s), and DPR (18.5 faults/s), with 30% lower FPGA resource overhead compared to SCFIT. The adaptive architecture ensures seamless scalability across different FPGA platforms, making it suitable for space electronics, automotive safety systems, and high-performance computing. Additionally, DA-FIS supports real-time error model adjustments, enabling researchers to analyze fault propagation, error correction strategies, and security vulnerabilities in FPGA-based architectures. This work establishes DA-FIS as a superior fault injection framework, offering high-speed, precision-controlled fault testing while maintaining minimal FPGA overhead and enhanced scalability. Future research will explore machine learning-assisted fault modeling and self-healing FPGA architectures to further enhance FPGA fault resilience in safety-critical and autonomous systems.},
  archive      = {J_PEERJCS},
  author       = {Fatimah Alhayan and Gaganjot Kaur and Sultan Alanazi and Mohammed Burhanur Rehman and Wahida Mansouri and Da’ad Albalawneh and Ali Alqazzaz and Hanadi Alkhudhayr},
  doi          = {10.7717/peerj-cs.2996},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2996},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DA-FIS: A high-speed dynamic adaptive fault injection server framework for reliable FPGA-based embedded systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Formal modeling of a causal consistent distributed system and verification of its history via model checking using colored petri net. <em>PEERJCS</em>, <em>11</em>, e2995. (<a href='https://doi.org/10.7717/peerj-cs.2995'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various consistency models for replicated distributed systems (DSs) have been developed and are usually implemented in the middleware layer. Causal consistency (CC) is a widely used consistency model appropriate for distributed applications like discussion groups and forums. One of the known distributed algorithms for CC is based on logical time synchronization with Fidge vector clocks that use the concepts of the hold-back and delivery queues for each replica. The basics of the algorithm and its assumptions are presented in the article. Then, a novel formal hierarchical colored Petri net model of a DS with CC support and three constituting replicas is presented. The proposed model operates based on the presented distributed algorithm for CC support with potential randomness for delays in message delivery. The article tries to answer the question: is a given distributed history (DH) a valid image of a causal-consistent distributed system (CCDS)? The proposed model validates a DH via model checking. The question is answered by the execution of the proposed model and the generation of its state space graph (SSG). Required model checking functions are developed for automatically analyzing SSG for (1) extracting the existence of the answer and (2) extraction of the shortest proof scenarios that can generate the given input DH. The model was used to analyze four case study examples. The article presents three effective techniques for decreasing the state space explosion problem. Results show that the colored Petri net model of a CCDS can automatically validate a DH using model checking.},
  archive      = {J_PEERJCS},
  author       = {Khalid Amjed Mohammed Alsaegg and Saeid Pashazadeh and Mina Zolfy Lighvan},
  doi          = {10.7717/peerj-cs.2995},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2995},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Formal modeling of a causal consistent distributed system and verification of its history via model checking using colored petri net},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early breast cancer detection in CT scans using convolutional neural bidirectional feature pyramid network. <em>PEERJCS</em>, <em>11</em>, e2994. (<a href='https://doi.org/10.7717/peerj-cs.2994'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is one of the leading causes of death among women worldwide. Early detection plays a crucial role in reducing mortality rates. While mammography is a widely used diagnostic tool, computed tomography (CT) scans are increasingly being explored for detecting breast cancer due to their high-resolution imaging and ability to visualize tissue in 3D. Despite the potential of CT scans in visualizing breast tissue in 3D with high resolution, extracting meaningful patterns from these scans is difficult due to the complex and nonlinear nature of the tissue features. The challenge lies in developing computational methods that can accurately detect and localize breast cancer lesions, especially when the tumors vary in size, shape, and density. In this article, we proposed a framework called convolutional neural bidirectional feature pyramid network, which integrates multi-scale feature extraction and bidirectional feature fusion for breast cancer detection in CT scans. The proposed framework classified the images into diseased and non-diseased and then identified the infected region on breast tissue. Using convolutional neural networks, we defined several layers to classify the diseased and normal CT scan images. We collected data on breast CT scans taken from the radiology department, Ayub Teaching Hospital Abbottabad, Pakistan. We evaluated the model using a variety of classification metrics such as precision, recall, F1-measure, and average precision to determine its effectiveness in finding breast cancer lesions, and we found 96.11% accuracy. Our findings show that compared with current state-of-the-art methods, the proposed framework has satisfactory results in identifying breast cancer areas, and the proposed framework over the baselines has achieved a 1.71% improvement.},
  archive      = {J_PEERJCS},
  author       = {Tahani Jaser Alahmadi and Adeel Ahmed and Amjad Rehman and Abeer Rashad Mirdad and Bayan Al Ghofaily and Shehryar Ali},
  doi          = {10.7717/peerj-cs.2994},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2994},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Early breast cancer detection in CT scans using convolutional neural bidirectional feature pyramid network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foundational models and federated learning: Survey, taxonomy, challenges and practical insights. <em>PEERJCS</em>, <em>11</em>, e2993. (<a href='https://doi.org/10.7717/peerj-cs.2993'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning has the potential to unlock siloed data and distributed resources by enabling collaborative model training without sharing private data. As more complex foundational models gain widespread use, the need to expand training resources and integrate privately owned data grows as well. In this article, we explore the intersection of federated learning and foundational models, aiming to identify, categorize, and characterize technical methods that integrate the two paradigms. As a unified survey is currently unavailable, we present a literature survey structured around a novel taxonomy that follows the development life-cycle stages, along with a technical comparison of available methods. Additionally, we provide practical insights and guidelines for implementing and evolving these methods, with a specific focus on the healthcare domain as a case study, where the potential impact of federated learning and foundational models is considered significant. Our survey covers multiple intersecting topics, including but not limited to federated learning, self-supervised learning, fine-tuning, distillation, and transfer learning. Initially, we retrieved and reviewed a set of over 4,200 articles. This collection was narrowed to more than 250 thoroughly reviewed articles through inclusion criteria, featuring 42 unique methods. The methods were used to construct the taxonomy and enabled their comparison based on complexity, efficiency, and scalability. We present these results as a self-contained overview that not only summarizes the state of the field but also provides insights into the practical aspects of adopting, evolving, and integrating foundational models with federated learning.},
  archive      = {J_PEERJCS},
  author       = {Cosmin-Andrei Hatfaludi and Alex Serban},
  doi          = {10.7717/peerj-cs.2993},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2993},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Foundational models and federated learning: Survey, taxonomy, challenges and practical insights},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HTCNN-attn: A fine-grained hierarchical multi-label deep learning model for disaster emergency information intelligent extraction from social media. <em>PEERJCS</em>, <em>11</em>, e2992. (<a href='https://doi.org/10.7717/peerj-cs.2992'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenge of extracting fine-grained emergency information from noisy social media during disasters, we propose HTCNN-Attn, a hierarchical multi-label deep learning model. It integrates a three-level tree-structured labeling architecture, Transformer-based global feature extraction, convolutional neural network (CNN) layers for local pattern capture, and a hierarchical attention mechanism. The model employs a hierarchical loss function to enforce label consistency across three levels: binary disaster filtering (Level 1), mid-level category classification (Level 2), and fine-grained subcategory extraction (Level 3). Experiments on the Appen and HumAID datasets demonstrate superior performance, achieving an accuracy of 0.9725, a Micro-F1 of 0.7402, and a hierarchical consistency (HC) score of 0.821, outperforming state-of-the-art baselines. Ablation experiments validate the importance of hierarchical modeling, Transformer encoding, CNN layers, pre-trained embeddings, and the hierarchical attention mechanism. Cross-event generalization tests on the CrisisBench dataset demonstrate robust generalization (HC = 0.678), while its lightweight design enables efficient real-time deployment (12.0 ms latency). A case study of the 2015 Nepal earthquake validates its practical utility, where the model accurately classified tweets into hierarchical labels and routed structured information to support emergency response coordination. This demonstrates the effectiveness of the proposed model in supporting rapid, efficient, and fine-grained emergency response after disasters, thereby enhancing disaster response capabilities.},
  archive      = {J_PEERJCS},
  author       = {Shanshan Li and Qingjie Liu and Xiaoling Sun},
  doi          = {10.7717/peerj-cs.2992},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2992},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HTCNN-attn: A fine-grained hierarchical multi-label deep learning model for disaster emergency information intelligent extraction from social media},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on license plate recognition based on graphically supervised signal-assisted training. <em>PEERJCS</em>, <em>11</em>, e2989. (<a href='https://doi.org/10.7717/peerj-cs.2989'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background With the rapid growth of the number of cars and the increasing complexity of urban transportation, it is particularly important to achieve high-accuracy license plate recognition in complex scenarios. However, since license plate recognition models are mostly deployed on embedded devices with limited computational resources, designing a lightweight and accurate model has become an urgent problem in the field of license plate recognition. Methods This study proposes an improved license plate recognition algorithm. We use the License Plate Recognition Network (LPRNet) as the base model. To enhance its accuracy, we incorporate graphically supervised signals for assisted training. This approach refines the training process, yielding a model that is both lightweight and highly accurate. An auxiliary training branch is added, utilizing these graphical signals to guide the model in learning improved image features. Results Experiments show that compared with LPRNet, this study improves the accuracy in all test sets of the Chinese City Parking Dataset (CCPD) dataset, where the average accuracy is improved by 5.86%, the maximum accuracy by 10.9%, the average character precision by 2.1%, and the average recall by 6.9%, indicating that this study can achieve higher accuracy while keeping it lightweight. This study also provides new ideas for other deep learning image recognition tasks.},
  archive      = {J_PEERJCS},
  author       = {Dianwei Chi and Zehao Jia and Lizhen Liu},
  doi          = {10.7717/peerj-cs.2989},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2989},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on license plate recognition based on graphically supervised signal-assisted training},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TaGra: An open python package for easily generating graphs from data tables through manifold learning. <em>PEERJCS</em>, <em>11</em>, e2986. (<a href='https://doi.org/10.7717/peerj-cs.2986'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The challenge of analyzing high-dimensional data affects many scientific disciplines, from pharmacology to chemistry and biology. Traditional dimensionality reduction methods often oversimplify data, making it difficult to interpret individual points. This distortion can complicate the visualization of mutual distances between data points in the reduced space. Graphs provide an effective framework for representing objects and their relationships. One of their possible use is visualizing similarity patterns in tabular datasets. Here we introduce TaGra, an off-the-shelf package designed to generate a graph of similarity relations from tabular data. TaGra enables the visualization of datasets in 2D space, identification of typical data points and outliers, and assessment of the separation between items with different target variables. We describe TaGra’s functionality, options and setup. The software including examples, instructions and a guide, is openly available on PyPI at https://pypi.org/project/TaGra/ and on GitHub at https://github.com/davidetorre92/TaGra.},
  archive      = {J_PEERJCS},
  author       = {Davide Torre and Davide Chicco},
  doi          = {10.7717/peerj-cs.2986},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2986},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TaGra: An open python package for easily generating graphs from data tables through manifold learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DLProv: A suite of provenance services for deep learning workflow analyses. <em>PEERJCS</em>, <em>11</em>, e2985. (<a href='https://doi.org/10.7717/peerj-cs.2985'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) workflows consist of multiple interdependent and repetitive steps, including data preparation, model training, evaluation, and deployment. Each step involves decisions impacting the final model’s performance, interpretability, and applicability. These models rely on data, preprocessing operations, and configuration, underscoring the need for mechanisms to ease the analysis throughout the entire life cycle—from model generation and selection to deployment. Moreover, ensuring trust, reproducibility, and transparency becomes important as DL models transition into production environments. Traceability across the steps of the DL workflow is essential to address these challenges. However, existing traceability solutions often present limitations. Many fail to integrate the steps of the DL workflow, focusing on either data preparation or model training. Additionally, they frequently rely on proprietary formats to represent traceability data and rarely produce a provenance document that can accompany the model into production. To bridge these gaps, we present DLProv, a suite of provenance services designed to ensure end-to-end traceability across DL workflows. DLProv supports structured query language (SQL)-based querying during training and generates provenance graphs that capture data preparation steps, model training, and evaluation. These provenance graphs comply with the PROV de facto standard, ensuring interoperability across different environments. One of the key strengths of DLProv lies in its framework-agnostic architecture. The suite’s services can be invoked independently of the DL framework, enabling integration across several training and deployment workflows. Furthermore, DLProv includes specialized instances designed for specific DL frameworks, such as Keras and physics-informed neural networks (PINNs), offering adaptability to a wide range of applications. We evaluated DLProv using well-established datasets, including Modified National Institute of Standards and Technology (MNIST) and Canadian Institute for Advanced Research (CIFAR)-100. These datasets were chosen to illustrate the suite’s capability to capture and manage provenance data across tasks of varying complexity, from basic image classification to more complex DL workflows. Additionally, we evaluated DLProv within a handwritten transcription workflow, further showcasing its flexibility. Across all these use cases, DLProv showed its ability to ease SQL-based queries during model training while maintaining framework independence. An important aspect of our evaluation was measuring the overhead introduced by integrating DLProv into DL workflows. The results showed a maximum overhead of 1.4% in execution time, highlighting the suite’s minimal impact on DL workflow performance. For comparative analysis, we benchmarked this overhead against MLflow, further reinforcing DLProv’s suitability for real-world DL applications.},
  archive      = {J_PEERJCS},
  author       = {Débora Pina and Liliane Kunstmann and Adriane Chapman and Daniel de Oliveira and Marta Mattoso},
  doi          = {10.7717/peerj-cs.2985},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2985},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DLProv: A suite of provenance services for deep learning workflow analyses},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-based enhanced boosting algorithm for depression detection. <em>PEERJCS</em>, <em>11</em>, e2981. (<a href='https://doi.org/10.7717/peerj-cs.2981'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression is a rapidly increasing mental disorder that can interfere with a person’s ability and negatively affect functions in various aspects of life. Fortunately, machine learning and deep learning techniques have demonstrated excellent results in the early detection of depression using social media data. Most recently, researchers have utilized boosting algorithms including pre-defined boosting algorithms or built their own boosting algorithm for the detection of depression. However, both types of boosting algorithms struggle with the analysis of complex feature sets, the enhancement of weak learners, and the handling of larger datasets. Thus, this study has developed a novel feature-based enhanced boosting algorithm (F-EBA). The proposed model covers two pipelines, the feature engineering pipeline which improves the quality of features by picking up the most relevant features while the classification pipeline uses an ensemble approach designed to boost/elevate the model’s performances. The experimental results highlighted that various parameter including WordVec and BERT embeddings, attention mechanisms, and feature elimination techniques, significantly contributed to the selection of the most relevant features. This approach resulted in generating an optimized feature set that augmented both the model’s accuracy and its interpretability. In addition, utilizing over 46 million records, the F-EBA model significantly enhanced the performance of weak learners through a weight maximization strategy, achieving an impressive accuracy rate of 95%. Moreover, the integration of an adversarial layer that employs defense mechanisms against synonymous text and sarcastic phrases within the datasets has further boosted the F-EBA model’s accuracy to approximately 97%, surpassing the results reported in prior studies. Moreover, the optimized feature sets derived from the F-EBA model make a substantial contribution to boosting the performance of baseline classifiers, marking a novel advancement in the field.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Sadiq Rohei and Kasturi Dewi Varathan and Shivakumara Palaiahnakote and Nor Badrul Anuar},
  doi          = {10.7717/peerj-cs.2981},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2981},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Feature-based enhanced boosting algorithm for depression detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data driven healthcare insurance system using machine learning and blockchain technologies. <em>PEERJCS</em>, <em>11</em>, e2980. (<a href='https://doi.org/10.7717/peerj-cs.2980'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Healthcare recommendations and insurance have recently been one of the most emerging research areas in health informatics. The fraud in health insurance is becoming increasingly common day by day. To handle healthcare insurance fraud, there is an urgent need for an intelligent system that cannot only identify and monitor doctors’ and hospitals’ behavior regarding the health services they provide to patients but can also recommend doctors and hospitals to insured employees based on the quality of services they provided previously. This system creates patient and doctor profiles separately, based on their rating. The proposed system combines singular value decomposition (SVD), K-nearest neighbors based collaborative filtering (KNN-based CF), item-based collaborative filtering (Item-based CF), content-based filtering using term frequency-inverse document frequency (TF-IDF), and K-means clustering and probability distributions to recommend doctors and insurance plans. The system measures similarity scores between patients and doctors using cosine similarity, which helps to determine similarity scores and refine the recommendations. This study also uses blockchain technology to automate insurance claims reimbursement. The results are validated using real data from the employees of a local hospital. The system provides recommendations with a root mean square error (RMSE) value of 0.478 and a mean absolute error (MAE) value of 0.0422. The insurance plans developed using the proposed system have reduced the overall expenditure of the local hospital, with a reduction in total expenses. Blockchain technology further helps prevent healthcare fraud. In the proposed system, a healthcare insurance claims reimbursement system is built using smart contract technology on the Ethereum blockchain, ensuring security & transparency and lowering the number of healthcare frauds. The system includes roles for the insurance company, healthcare provider, and patients. It also provides a platform for claim submission, approval, or refusal. In Pakistan, no such system existed before recommending doctors from different hospitals based on their professional conduct or the good health services they provide.},
  archive      = {J_PEERJCS},
  author       = {Irum Matloob and Shoab Khan and Bushra Bashir and Rukaiya Rukaiya and Javed Ali Khan and Hessa Alfraihi},
  doi          = {10.7717/peerj-cs.2980},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2980},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data driven healthcare insurance system using machine learning and blockchain technologies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generating, retrieving persona and generating responses for long-term open-domain dialogue. <em>PEERJCS</em>, <em>11</em>, e2979. (<a href='https://doi.org/10.7717/peerj-cs.2979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-domain dialogue systems have shown remarkable capabilities in generating natural and consistent responses in short-term conversations. However, in long-term conversations such as multi-session chat (MSC), where the dialogue history exceeds the model’s maximum input length (i.e., 1024 tokens), existing dialogue generation systems often overlook the information from earlier dialogues, leading to the loss of context. To prevent such loss and generate natural, consistent responses, we propose a GRGPerDialogue framework, consisting of three main stages: generating persona from past dialogues, retrieving persona relevant to the current utterance, and generating responses based on both persona and recent dialogues. In the first stage, we generate the persona of each speaker in real-time with diverse expressions, leveraging Llama 2 In-Context Learning (ICL). Subsequently, we propose a new dataset called Persona-Utterance Pair (PUP) and use it to train Facebook dense passage retrieval (DPR) model for retrieving persona sentences relevant to the current utterance. Finally, we train generative models such as Generative Pre-trained Transformer 2 (GPT-2) and Bidirectional and Auto-Regressive Transformers (BART) to generate responses based on retrieved persona sentences and the recent dialogues. Experimental results on a long-term dialogue dataset demonstrate that the GRGPerDialogue framework outperforms baseline models by approximately 0.6% to 1% in terms of the Rouge-1 metric. Furthermore, human evaluation results supported the effectiveness of GRGPerDialogue. These results indicate that GRGPerDialogue can generate responses that are not only more fluent and consistent, but also more relevant to the dialogue history than baseline models.},
  archive      = {J_PEERJCS},
  author       = {Dohyun Cha and Dawon Lee and Jihie Kim},
  doi          = {10.7717/peerj-cs.2979},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2979},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Generating, retrieving persona and generating responses for long-term open-domain dialogue},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classifying reservoir facies using attention-based residual neural networks. <em>PEERJCS</em>, <em>11</em>, e2977. (<a href='https://doi.org/10.7717/peerj-cs.2977'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accurate classification of reservoir facies remains a fundamental challenge in petroleum geoscience, with significant implications for resource extraction efficiency and reservoir characterization. Traditional approaches relying on manual interpretation and conventional machine learning methods often struggle with the complexity and heterogeneity of well-log data. This architectural approach, in contrast to traditional single-stream or non-residual designs, significantly enhances the model’s ability to concentrate on key geological features while preserving hierarchical representations of the data. Consequently, it more effectively addresses data heterogeneity and contextual dependencies. The framework was trained and evaluated using measurements from eight wells that represent diverse geological settings. Comparative experiments against conventional machine learning models and state-of-the-art deep learning techniques demonstrated the superiority of our method, achieving an area under the receiver operating characteristic curve (AUROC) of 0.883 and an area under the precision-recall curve (AUPRC) of 0.502. These enhancements enable more accurate identification of subtle facies boundaries and lithological variations, particularly in complex geological formations, thereby facilitating improved reservoir delineation and reducing uncertainty in field development planning. Furthermore, reproducibility analyses confirmed consistent performance across independent trials, underscoring the model’s robustness and its viability for real-world reservoir characterization workflows.},
  archive      = {J_PEERJCS},
  author       = {An Hai Nguyen and Khang Nguyen and Nga Mai},
  doi          = {10.7717/peerj-cs.2977},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2977},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Classifying reservoir facies using attention-based residual neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Web application firewall based on machine learning models. <em>PEERJCS</em>, <em>11</em>, e2975. (<a href='https://doi.org/10.7717/peerj-cs.2975'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing reliance on web applications for storing sensitive data and financial transactions has elevated the importance of web application security. A machine learning-based web application firewall was designed to protect web applications against injection vulnerabilities. A hybrid dataset, including CISC 2010, HTTPParams 2015, and real-time Hypertext Transfer Protocol (HTTP) requests, was employed. The study evaluated five classification algorithms—K-nearest neighbors, logistic regression, naïve Bayes, support vector machine, and decision tree—for detecting cross site scripting (XSS), Structured Query Language (SQL) Injection, Operating System Command Injection, and Local File Inclusion attacks. Decision tree was identified as the algorithm with the highest precision, accuracy, recall, F1-score, receiver operating characteristic (ROC), and area under the curve (AUC) values. According to the confusion matrix analysis, the real-time tested web application firewalls (WAF) achieved a remarkably high F1 score of 93.13% and accuracy of 93.27%. The findings indicate that machine learning-based WAFs effectively protect web applications against injection threats. Future work includes expanding the WAF to cover other attack types and testing it on different datasets.},
  archive      = {J_PEERJCS},
  author       = {Muhammed Ersin Durmuşkaya and Selim Bayraklı},
  doi          = {10.7717/peerj-cs.2975},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2975},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Web application firewall based on machine learning models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of efficient generalized digital fractional order differentiators using an improved whale optimization algorithm. <em>PEERJCS</em>, <em>11</em>, e2971. (<a href='https://doi.org/10.7717/peerj-cs.2971'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new design and realization method for generalized digital fractional-order differentiator (GFOD) based on a composite structure of infinite impulse response (IIR) subfilters. The proposed method utilizes an improved whale optimization algorithm (IWOA) to compute the optimal coefficients of IIR subfilters of the realization structure. IWOA is developed by incorporating a piecewise linear chaotic mapping (PWLCM) and an adaptive inertia weight based on the hyperbolic tangent function (AIWHT) into the framework of original whale optimization algorithm (WOA). Simulation experiments are conducted to compare the performance of our method with that of well-known techniques, real-coded genetic algorithm (RCGA), particle swarm optimization (PSO), and original WOA. The results show that the new metaheuristic is superior to the other metaheuristics in terms of attaining the most accurate GFOD approximation. Moreover, the proposed IIR-based GFOD is compared with state-of-the-art GFOD, and observed to save about 50% of implementation complexity. Therefore, our method can be utilized in real-world digital signal processing applications.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Ali Mohammed Moqbel and Talal Ahmed Ali Ali and Zhu Xiao and Amani Ali Ahmed Ali},
  doi          = {10.7717/peerj-cs.2971},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2971},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of efficient generalized digital fractional order differentiators using an improved whale optimization algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of EfficientNet CNN models for multi-label chest X-ray disease diagnosis. <em>PEERJCS</em>, <em>11</em>, e2968. (<a href='https://doi.org/10.7717/peerj-cs.2968'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of chest X-ray images, which are critical for the early diagnosis of many diseases, is a difficult and time-consuming process due to the multiple labeling requirements and similar looking pathologies. In traditional methods, expert physicians analyze high-resolution chest X-ray images to diagnose these diseases using observational methods, a process that can lead to human error and hence misdiagnosis or underdiagnosis. In this study, we aim to autonomously detect 14 different diseases that significantly affect human health and some cases even lead to death using chest X-ray images in a multi-class manner using deep learning techniques. Previous studies on chest X-ray images focus on a single disease or have low success rates, and the architectures presented in previous studies have high computational costs. The novelty of this work is that it presents a hybrid lightweight, fast and attention-based architecture with high classification performance. In this study, we used the ChestX-Ray14 dataset consisting of 112,104 labeled chest X-ray images of 14 disease classes. Eight deep learning architectures (EfficientNetB0-B7) and coordinate attention mechanism are used in the training and testing processes. The proposed EfficientNetB7 architecture achieved an average overall classification performance with an AUC value of 0.8265. The EfficientNet enhanced with coordinate attention architecture achieved a classification success with an AUC value of 0.8309. Moreover, when the proposed architecture and the individual disease classes are considered separately, higher classification success is achieved for eight of the 14 diseases in the dataset. Finally, the results of this study outperformed the classification performance of other similar studies in the literature in terms of AUC score. The results obtained in our study show that the proposed deep learning based lightweight and fast architecture can support radiologists in decision making in disease diagnosis. The use of autonomous disease diagnosis systems can support the protection of human health by preventing incomplete or erroneous diagnoses.},
  archive      = {J_PEERJCS},
  author       = {Murat Ucan and Buket Kaya and Osman Aygun and Mehmet Kaya and Reda Alhajj},
  doi          = {10.7717/peerj-cs.2968},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2968},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparison of EfficientNet CNN models for multi-label chest X-ray disease diagnosis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Specx: A c++ task-based runtime system for heterogeneous distributed architectures. <em>PEERJCS</em>, <em>11</em>, e2966. (<a href='https://doi.org/10.7717/peerj-cs.2966'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallelization is needed everywhere, from laptops and mobile phones to supercomputers. Among parallel programming models, task-based programming has demonstrated a powerful potential and is widely used in high-performance scientific computing. Not only does it allow efficient parallelization across distributed heterogeneous computing nodes, but it also allows for elegant source code structuring by describing hardware-independent algorithms. In this article, we present Specx, a task-based runtime system written in modern C++. Specx supports distributed heterogeneous computing by simultaneously exploiting central processing units (CPUs) and graphics processing units (GPUs) (CUDA/HIP) and incorporating communication into the task graph. We describe the specificities of Specx and demonstrate its potential by running parallel applications.},
  archive      = {J_PEERJCS},
  author       = {Paul Cardosi and Bérenger Bramas},
  doi          = {10.7717/peerj-cs.2966},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2966},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Specx: A c++ task-based runtime system for heterogeneous distributed architectures},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Result assessment tool (RAT): Empowering search engine data analysis. <em>PEERJCS</em>, <em>11</em>, e2962. (<a href='https://doi.org/10.7717/peerj-cs.2962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Result Assessment Tool (RAT) is a Python-based software toolkit that enables researchers to analyze results from commercial search engines, social media platforms, and library search systems. RAT provides an integrated environment for designing studies, collecting results, and performing automated analysis. The software consists of two main modules: RAT Frontend and RAT Backend. RAT Frontend uses Flask to provide a researcher view for designing studies and an evaluation view for collecting ratings from study participants. RAT Backend includes modules for collecting search results, extracting source code, and adding classifiers for automated analysis. The system has been used in various studies, including search engine effectiveness studies, interactive information retrieval studies, and classification studies.},
  archive      = {J_PEERJCS},
  author       = {Sebastian Sünkler and Dirk Lewandowski and Sebastian Schultheiß and Nurce Yagci},
  doi          = {10.7717/peerj-cs.2962},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2962},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Result assessment tool (RAT): Empowering search engine data analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid extraction model for semantic knowledge discovery of water conservancy big data. <em>PEERJCS</em>, <em>11</em>, e2960. (<a href='https://doi.org/10.7717/peerj-cs.2960'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the growing demand for efficient public opinion analysis in water conservancy and related domains, as well as the inefficiencies and limited scalability of existing automated web data extraction algorithms for multi-source datasets, this research integrates advanced technologies including big data analytics, natural language processing, and deep learning. A novel, transferable web information extraction model based on deep learning (WIEM-DL) is proposed, leveraging knowledge graphs, machine learning, and ontology-based methods. This model is designed to adapt to varying website structures, enabling effective cross-website information extraction. By refining water conservancy-related online public opinion content and extracting key feature information from critical sentences, the WIEM-DL model excels in locating main content while filtering out noise. This approach not only reduces processing time but also significantly improves extraction accuracy and efficiency. Furthermore, the model establishes methods for micro-level public opinion information extraction and feature representation, creating a fusion space for data-level integration. This serves as a robust foundation for multi-granularity semantic knowledge integration in public opinion big data. Experimental results demonstrate that the WIEM-DL model substantially outperforms traditional information extraction methods, setting a new benchmark for extraction performance.},
  archive      = {J_PEERJCS},
  author       = {Yanna Feng and Feng Zhang and Yongheng Zhang and Jiangang Dong and PengJu Wang},
  doi          = {10.7717/peerj-cs.2960},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2960},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid extraction model for semantic knowledge discovery of water conservancy big data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient sepsis detection using deep learning and residual convolutional networks. <em>PEERJCS</em>, <em>11</em>, e2958. (<a href='https://doi.org/10.7717/peerj-cs.2958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sepsis is a life-threatening complication caused by infection that leads to extensive tissue damage. If not treated promptly, it can become fatal. Early identification and diagnosis of sepsis are critical to improving patient outcomes. Although recent technological advancements have aided sepsis detection, challenges remain in timely diagnosis using standard clinical practices. In this article, we present a new deep learning model to detect the occurrence of sepsis and the African vulture optimization algorithm (AVOA) to enhance the model performance. The system comprises four crucial steps: First, the enhanced convolutional learning framework (ECLF) with atrous convolutional and multi-level strategies that aim to learn high-level features from the nonlinear mapping of the medical data. Second is the spatio-channel attention network (SCAN), which has a neural architecture designed to focus on significant regions, such as spatial and channel regions, but not restricted to them. Third is the hierarchical dilated convolutional block (HDCB), which utilises a stacked dilated deep convolutional architecture for spatial feature context retrieval. Last is the residual path convolutional chain (RPCC), which uses a multi-residual convolutional approach for feature propagation, preserving important information. The sepsis detection model we bring forth involves many components, as mentioned above, and thus achieves a higher accuracy for timely intervention during sepsis. The combination of AVOA into the model ensures that it is robust and easily transferable, delivering high performance for adaptation to complicated structures inside medical datasets. The proposed model was evaluated on a clinical dataset and achieved outstanding performance, with an accuracy of 99.4%, precision of 98%, recall of 99.2%, F1-score of 99.0%, and an area under the curve (AUC) of 0.998. These results demonstrate the model’s superior ability to detect sepsis accurately and reliably, outperforming traditional clinical scoring methods and conventional machine learning approaches.},
  archive      = {J_PEERJCS},
  author       = {Ahmed S. Almasoud and Ghada Moh Samir Elhessewi and Munya A. Arasi and Abdulsamad Ebrahim Yahya and Menwa Alshammeri and Donia Badawood and Faisal Mohammed Nafie and Mohammed Assiri},
  doi          = {10.7717/peerj-cs.2958},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2958},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Efficient sepsis detection using deep learning and residual convolutional networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot cross-lingual stance detection via adversarial language adaptation. <em>PEERJCS</em>, <em>11</em>, e2955. (<a href='https://doi.org/10.7717/peerj-cs.2955'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stance detection has been widely studied as the task of determining if a social media post is positive, negative or neutral towards a specific issue, such as support towards vaccines. Research in stance detection has often been limited to a single language and, where more than one language has been studied, research has focused on few-shot settings, overlooking the challenges of developing a zero-shot cross-lingual stance detection model. This article makes the first such effort by introducing a novel approach to zero-shot cross-lingual stance detection, multilingual translation-augmented bidirectional encoder representations from Transformers (BERT) (MTAB), aiming to enhance the performance of a cross-lingual classifier in the absence of explicit training data for target languages. Our technique employs translation augmentation to improve zero-shot performance and pairs it with adversarial learning to further boost model efficacy. Through experiments on datasets labeled for stance towards vaccines in four languages—English, German, French, Italian, we demonstrate the effectiveness of our proposed approach, showcasing improved results in comparison to a strong baseline model as well as ablated versions of our model. Our experiments demonstrate the effectiveness of model components, not least the translation-augmented data as well as the adversarial learning component, to the improved performance of the model. We have made our source code accessible on GitHub: https://github.com/amcs18pd05/MTAB-cross-lingual-vaccine-stance-detection-2.},
  archive      = {J_PEERJCS},
  author       = {Bharathi A. and Arkaitz Zubiaga},
  doi          = {10.7717/peerj-cs.2955},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2955},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Zero-shot cross-lingual stance detection via adversarial language adaptation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing the knowledge of artificial intelligence chatbots in pharmacology: Examples of two groups of drugs. <em>PEERJCS</em>, <em>11</em>, e2954. (<a href='https://doi.org/10.7717/peerj-cs.2954'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objectives The study aimed to evaluate eight artificial intelligence chatbots (ChatGPT-3.5, Microsoft Copilot, Gemini, You.com, Perplexity, Character.ai, Claude 3.5, and ChatRTX) in answering questions related to two pharmacological topics taught during the basic pharmacology curriculum for medical students: antifungal drugs and hypolipidemic drugs. Methods Chatbots’ performance was assessed by answering 60 single-choice questions on antifungal and hypolipidemic drugs topics. The questions were designed to have four answers (a, b, c, and d), and the artificial intelligence (AI) role was to choose the proper one. The assessment was performed twice with a 1-year hiatus to determine if artificial intelligence chatbots’ effectiveness changed over time. All the answers were checked for being right or wrong according to up-to-date pharmacology knowledge. To improve the clarity of results, to each score, a mark was assigned based on the grading system applied in our unit. Statistica software version 13.3 and Microsoft Excel 2010 were used for statistical analysis. Results In 2023, the best results on the subject of antifungal drugs were obtained by Gemini (formerly Bard) and on the topic of hypolipidemic drugs by You.com (formerly YouChat). In 2024 Microsoft Copilot answered correctly the highest number of questions in both topics. The total results of all artificial intelligence chatbots in 2023 and 2024 were compared using t-test for dependent samples. Statistical analysis revealed that artificial intelligence chatbots improved over time in both pharmacological topics, but this change was not statistically significant (p = 0.784 for antifungal drugs subject and p = 0.056 for hypolipidemic drugs). Conclusions The accuracy of AI chatbots’ responses regarding antifungal and hypolipidemic drugs improved over one year, though not significantly. None of the tested AI systems provided correct answers to all questions within these pharmacological fields.},
  archive      = {J_PEERJCS},
  author       = {Marcin Mateusz Granat and Aleksandra Paź and Dagmara Mirowska-Guzel},
  doi          = {10.7717/peerj-cs.2954},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2954},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Testing the knowledge of artificial intelligence chatbots in pharmacology: Examples of two groups of drugs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HybridFormer: A convolutional neural network-transformer architecture for low dose computed tomography image denoising. <em>PEERJCS</em>, <em>11</em>, e2952. (<a href='https://doi.org/10.7717/peerj-cs.2952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-dose computed tomography (CT) is a potent strategy to minimize X-ray radiation and its detrimental effects on patients. However, reducing radiation significantly boosts noise in reconstructed images, causing blur and obscuring critical tissue details. This obscurity poses significant challenges for doctors in making accurate diagnoses. Traditional techniques like sinogram domain filtration and iterative reconstruction algorithms require inaccessible raw data. Thus, this article introduces HybridFormer, a revolutionary image-denoising model utilizing the Residual Convolution-Swin Transformer Network, designed to enhance images while preserving vital details. Firstly, this algorithm constructs residual convolution for local feature extraction and Swin Transformer for global feature extraction, boosting denoising efficacy. Secondly, to address texture detail errors, we introduced a combined attention transformer unit (CATU) with a cross-channel attentive fusion layer (CCAFL), integrated with residual blocks to form a residual convolution and Swin Transformer Fusion Block (RSTB). Finally, using RSTB, we developed a deep feature refinement module (DFRM) to preserve image details. To avoid smoothing, we combined multi-scale perceptual loss from ResNet-50 with Charbonnier loss into a composite loss function. Validated on the AAPM2016 Mayo dataset, HybridFormer outperformed other state-of-the-art algorithms, achieving improvements of 0.02 dB, 0.16%, and 0.28% in PSNR, SSIM, and FSIM, respectively. Compared with other advanced algorithms, the proposed algorithm achieved the best performance indicators, confirming its superiority.},
  archive      = {J_PEERJCS},
  author       = {Shanaz Sharmin Jui and Zhitao Guo and Rending Jiang and Jiale Liu and Bohua Li},
  doi          = {10.7717/peerj-cs.2952},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2952},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HybridFormer: A convolutional neural network-transformer architecture for low dose computed tomography image denoising},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study on an efficient citrus huanglong disease detection algorithm based on three-channel aggregated attention. <em>PEERJCS</em>, <em>11</em>, e2943. (<a href='https://doi.org/10.7717/peerj-cs.2943'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Aiming at the problems of complex and diverse field symptoms of citrus Huanglong disease (HLB), low efficiency and insufficient recognition accuracy of traditional detection methods, this study proposes an efficient detection algorithm based on improved You Only Look Once (YOLO)v8. Methods Firstly, a new character to float (C2f) Attention inverse residual moving block (IRMB) module is designed, which significantly enhances the model’s sensitivity to tiny disease features while reducing the number of parameters by fusing the lightweight IRMB with the adaptive attention gating mechanism, and solves the problem of losing key texture information due to downsampling in the traditional C2f module. Secondly, the three-channel aggregated attention module Powerneck is proposed in the Neck section, which realizes efficient cross-scale feature interactions, effectively suppresses background noise interference, and improves robustness in complex field scenes through SimFusion_4in feature alignment, information fusion module (IFM) global context fusion, and Power channel dynamic weighting strategy. In addition, the detection head design is optimized by structural reparameterization technique to further accelerate the inference process. Results The experimental results show that on the citrus dataset containing 12 diseases and two health states, the mAP50 of this model reaches 97% and the accuracy is 91.5%, which is 1.1% and 1.2% higher than that of the original YOLOv8, respectively, and the inference speed is improved by 14.6% to 370 frames per second (FPS). Comparison of the different models shows that the C2f Attention IRMB, through the mechanism of dual attention The comparison of different models shows that C2f Attention IRMB strengthens the feature expression ability through the dual-attention mechanism, and the Powerneck module reduces redundant computation through dynamic channel pruning, and the two synergistically optimize the model performance significantly. Compared with mainstream models such as YOLOv5m and YOLOv7x, this method is more advantageous in the balance of accuracy and speed, and can meet the demand of real-time detection in the field. Discussion The algorithm provides an efficient tool for early and accurate identification of citrus Huanglong disease, which is of great practical significance for reducing pesticide misuse and improving the efficiency of orchard management, and also provides new ideas for the design of lightweight target detection models in agricultural scenarios.},
  archive      = {J_PEERJCS},
  author       = {Yizong Wang and Zhengrong Xiao and Hong Wang and Fei Li and Jiya Tian},
  doi          = {10.7717/peerj-cs.2943},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2943},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A study on an efficient citrus huanglong disease detection algorithm based on three-channel aggregated attention},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cword2vec: A novel morphological rule-based word embedding approach for urdu text sentiment analysis. <em>PEERJCS</em>, <em>11</em>, e2937. (<a href='https://doi.org/10.7717/peerj-cs.2937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word embeddings are essential to natural language processing tasks because they contain a single word’s syntactic and semantic information. Word embeddings have been developed widely for numerous spoken languages across the globe like English. The research community needs to pay more attention to the Urdu language despite its significant number of speakers, which amounts to approximately 231.3 million individuals. Urdu is a complex language because word boundaries in Urdu are unspecified, as it does not employ delimiters between words. The compound word, a multiword expression, is a more complex word consisting of many strings or independent base words. Traditionally, compound words are identified during the word segmentation using bigram or trigram approaches. The challenge with these techniques is that they do not produce meaningful words. This study uses morphological rule-based compound words in Urdu text documents. For text representation, a self-trained morphological rule-based compound word embedding (Cword2vec) based on the word2vec model is proposed for Urdu text sentiment analysis. The performance of self-trained morphological rule-based compound word embedding was then evaluated using four well-known deep learning models, i.e., long short-term memory (LSTM), bidirectional LSTM (BiLSTM), convolutional neural networks (CNN), and convolutional LSTM (C-LSTM) for sentiment analysis. We also compare the performance of morphological rule-based compound words with traditional compound word identification techniques such as bigrams and trigrams. Regardless of the classification model, word embedding using our proposed morphological rule-based compound words outperformed in terms of precision, recall, F1 score, and accuracy than bigrams and trigrams.},
  archive      = {J_PEERJCS},
  author       = {Saquib Khushhal and Abdul Majid and Syed Ali Abass and Rabia Riaz and Mohammad Babar and Shafiq Ahmad},
  doi          = {10.7717/peerj-cs.2937},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2937},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Cword2vec: A novel morphological rule-based word embedding approach for urdu text sentiment analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fairness modeling for topics with different scales in short texts. <em>PEERJCS</em>, <em>11</em>, e2936. (<a href='https://doi.org/10.7717/peerj-cs.2936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of topic modeling to short texts is beset by challenges such as data sparsity and an absence of contextual information. Traditional research methods tend to prioritise high-attention and popular topics, frequently overlooking the identification of emerging topics. Consequently, subjects of a minor scale are prone to being overlooked during the topic identification process. Furthermore, in the context of topic modelling, information that varies in terms of the attention it receives is not treated equally. In order to address the aforementioned issues, a fairness-oriented topic discovery approach (MixTM-G) is proposed. This approach has been designed to facilitate the discovery of topics with different levels of attention. The proposed methodology involves the integration of normalized pointwise mutual information (NPMI) within a graph model to analyse text data. This approach leverages the correlation between data points to assess the semantic relationships between words, thus addressing the limitations posed by sparse data. The employment of graph algorithms facilitates the identification of semantically related clusters within the document graph, thereby enhancing the semantic associations between sparse data. Finally, a mixed topic modeling approach (MixTM), based on bi-grams and tri-grams combinations, is proposed to further improve topic discovery by strengthening the contextual relationships between words. The experimental results demonstrate the efficacy of the proposed method in topic modelling. In comparison to conventional methods, the proposed approach exhibits superior performance in detecting small-scale topics under equivalent conditions.},
  archive      = {J_PEERJCS},
  author       = {Chuangying Zhu and Yongyu Liang and Xinyuan Liang and Limiao Zhong and Fei Xie},
  doi          = {10.7717/peerj-cs.2936},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2936},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fairness modeling for topics with different scales in short texts},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Opposition-based learning techniques in metaheuristics: Classification, comparison, and convergence analysis. <em>PEERJCS</em>, <em>11</em>, e2935. (<a href='https://doi.org/10.7717/peerj-cs.2935'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, opposition-based learning (OBL) has emerged as a powerful enhancement strategy in metaheuristic algorithms (MAs), gaining significant attention for its potential to accelerate convergence and improve solution quality. Existing research lacks a structured analysis of how different OBL variants influence optimization performance when integrated into various MAs. This study categorizes and analyzes nine distinct OBL techniques: basic opposition-based learning, quasi-opposition-based learning, generalized opposition-based learning, current optimum opposition-based learning, quasi-reflection opposition-based learning, centroid opposition-based learning, random opposition-based learning, super opposition-based learning, and stochastic opposition-based learning. To systematically assess the effectiveness of these techniques, five widely used OBL variants—basic opposition-based learning, quasi-opposition-based learning, generalized opposition-based learning, current optimum opposition-based learning, quasi-reflection opposition-based learning—were selected for implementation within five well-established MAs: differential evolution, genetic algorithm, particle swarm optimization, artificial bee colony, and harmony search. These hybridized algorithms were evaluated across different integration phases, including the initialization passes and generation updates phase, and in both phases. To experimentally demonstrate the capability of OBL strategies to enhance MAs that face common issues such as slow convergence, limited exploration, and imbalanced exploration-exploitation, we have used 12 benchmark functions from CEC2022 suite. Key performance metrics—including maximum, minimum, mean, standard deviation, and convergence curves—were rigorously analyzed to quantify the improvements introduced by each OBL-enhanced MA. Additionally, a Friedman test was conducted to statistically validate the performance differences among the variants. The results indicate that quasi-reflection opposition-based learning consistently outperforms other OBL variants, demonstrating superior convergence speed and solution quality across most benchmark functions.},
  archive      = {J_PEERJCS},
  author       = {Rihab Lakbichi and Farouq Zitouni and Saad Harous and Aridj Ferhat and Abdelhadi Limane and Abdulaziz S. Almazyad and Guojiang Xiong and Ali Wagdy Mohamed},
  doi          = {10.7717/peerj-cs.2935},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2935},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Opposition-based learning techniques in metaheuristics: Classification, comparison, and convergence analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimising AI writing assessment using feedback and knowledge graph integration. <em>PEERJCS</em>, <em>11</em>, e2893. (<a href='https://doi.org/10.7717/peerj-cs.2893'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the authors provide a novel framework for the effectiveness of AI writing assessment systems by embedding state-of-the-art deep learning networks, user feedback mechanisms, and knowledge graph frameworks. Most writing assessment tools cannot give personalized, detailed feedback. To tackle this problem, we employ writing assessment transformer models BERT and GPT-3, which allow exploring and scoring the writing on various features, including phrase structure, semantics, vocabulary usage, etc. In our system, we propose a dynamic relational knowledge graph that incorporates writing concepts and their relations, making it easier for the system to devise contextualized thesaurus-wise suggestions. The addition of graph neural networks (GNNs) empowers the model by boosting the GNN’s learning ability regarding the knowledge graph and improving comprehension of complex semantics. Additionally, we have included an iterative design whereby user feedback is collected, and the system adjusts the feedback given in light of historical feedback and changes in a user’s writing behavior over time. The system reconceptualizes the problem of user AI interaction by incorporating its dynamic nature and movement towards the known user and not vice-versa, achieving higher efficiency. To assess user satisfaction and improvements in the quality of the prepared texts, the authors conduct a series of user studies evaluating the efficiency of this integrated system. However, the preliminary data obtained from the task performance analysis show that the results of the proposed framework are far better than those of traditional methods, achieving a better level of engagement and feedback while performing the assessment. This study underscores the potential of deep learning, feedback, and knowledge graph integration in leveraging writing education. It can potentially reform learners’ capabilities, enabling them to write better and more effectively.},
  archive      = {J_PEERJCS},
  author       = {Ci Zhang},
  doi          = {10.7717/peerj-cs.2893},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2893},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimising AI writing assessment using feedback and knowledge graph integration},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Charting new territories: Fuzzy systems in english language teaching and learning. <em>PEERJCS</em>, <em>11</em>, e2887. (<a href='https://doi.org/10.7717/peerj-cs.2887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background In reality, the English language is a mystery; despite its inherent worth and the advantages of fluency, there is a pervasive impression that English instruction in secondary schools is of low quality, contributing to students’ lack of proficiency in the language in higher education and beyond. Pedagogical approaches persist in the classroom, topic after subject, including English. Analyzing texts in great depth via translation and emphasizing vocabulary are joint exercises in English classes. Students waste a lot of time copying things off the board in English classes despite the growing recognition of the significance of both listening and speaking effectively. Methods The Fuzzy Bayesian Intelligent Tutoring System (FB-ITS) is an artificial intelligence (AI) system that adaptively supports students in English teaching and learning settings. It is built in this experimental research employing AI methodologies based on fuzzy logic and the Bayesian network methodology. Using conventional approaches that rely primarily on numerical scores to evaluate academics’ teaching and research activities at various levels is becoming increasingly challenging. Expert systems based on fuzzy logic, suggested in this study, can handle teacher and student evaluations even when faced with imprecise information and uncertainty; this is necessary since academic performance is being indexed in multiple international databases using impact indices at different scales. Results The results showed that, on average, students using the FB-ITS took less time to complete the post-test than students using the conventional e-learning system. This research proposes an English teaching and learning approach that has been very successful based on experimental findings of related big data clustering algorithms. The assessment accuracy has risen by 4%, and the teaching resource utilization rate has been increased by 5%.},
  archive      = {J_PEERJCS},
  author       = {Xiaomei Wen and Deng Pan},
  doi          = {10.7717/peerj-cs.2887},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2887},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Charting new territories: Fuzzy systems in english language teaching and learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PSLDV-hop: A robust localization algorithm for WSN using PSO and refinement process. <em>PEERJCS</em>, <em>11</em>, e2770. (<a href='https://doi.org/10.7717/peerj-cs.2770'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various areas, wireless sensor networks (WSNs) are popular for achieving goals related to security in buildings when there is fire, in military areas to know the position of terrorists in moles and to observe the behavior of animals in forest areas. All these objectives can be achieved only when the position of the sensor is known to the base station, which helps to achieve the appropriate action in unwanted situations. The controlling point is the base station, which would be able to take action only in case the correct position of the unwanted event is known to the base station. Researches have designed various localization/positioning approaches but still have some challenges related to the accuracy of sensor nodes in localization. Distance vector hop is a popular localization algorithm. Its dependence on the estimated average size of a hop results in a significant localization error. This work suggests an improved algorithm combining a refinement procedure with particle swarm optimization, called DVHOP-PSO. This improved algorithm, called PSLDV-Hop, uses exact anchor sensor node coordinates and fractional hop count information to correct estimated distances. By utilizing an improved iterative evolution algorithm, the PSLDV-Hop algorithm reduces localization errors by achieving a higher degree of accuracy in node localization. Simulation results demonstrate their superiority over other classical improved algorithms and the original distance vector hop. The simulation of this approach is done using the MATLAB tool by considering different parameters such as the number of anchor nodes, number of sensor nodes, area, and range of sensor nodes. Integrating particle swarm optimization with distance vector hop, the proposed localization algorithm consistently outperforms conventional methods, showcasing significant percentage improvements . The suggested algorithm consistently performs better than all other approaches at ranges 20 and 40. Overall, the suggested method performs noticeably better than distance vector hop at range 40, especially when range grows by up to 65%. Additionally, across communication ranges of 20, 30, and 40 units, the proposed algorithm consistently outshines PSO-DV-Hop and GA-DV-Hop, exhibiting notable percentage improvements in localization accuracy.},
  archive      = {J_PEERJCS},
  author       = {Bhupinder Kaur and Deepak Prashar and Arfat Ahmad Khan and Seifedine Kadry and Jungeun Kim},
  doi          = {10.7717/peerj-cs.2770},
  journal      = {PeerJ Computer Science},
  month        = {7},
  pages        = {e2770},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {PSLDV-hop: A robust localization algorithm for WSN using PSO and refinement process},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable credit risk assessment model with boundary sample identification. <em>PEERJCS</em>, <em>11</em>, e2988. (<a href='https://doi.org/10.7717/peerj-cs.2988'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Interpretability is a key requirement for ensuring that credit risk assessment models are trustworthy and compliant with regulatory standards. Simultaneously, effectively distinguishing between noise samples and boundary samples is crucial for improving the accuracy of credit risk predictions. Methods This article introduces a novel credit risk assessment model, Interpretable Credit Risk Assessment Model with Identifying Boundary Samples (IAIBS). The model begins with a logistic regression sub-model that offers strong self-interpretable features. For samples that are not correctly classified, the Attribute Recognition and Perception based on the Distribution of neighboring sample features (ARPD) algorithm is applied to filter out noisy samples and identify boundary samples. A deep learning sub-model is then trained to deeply learn the risk features of these boundary samples. Finally, representative features of all samples are extracted using agglomerative clustering, and the most suitable sub-model is selected for prediction based on the similarity between each sample and the cluster centers. Results Experimental results on four public datasets demonstrate that the IAIBS model significantly outperforms 11 baseline models, as confirmed by the Nemenyi test. The model achieved area under the curve (AUC) scores of 89.17, 79.86, 97.48, and 66.03 on the PCL, FICO, CCF, and VL datasets, respectively. With appropriate parameter tuning, the IAIBS model maintains strong generalization ability, and each module contributes positively to overall performance. Additionally, the IAIBS model effectively interprets key predictors and prediction outcomes.},
  archive      = {J_PEERJCS},
  author       = {Runchi Zhang and Iris Li and Zhiyuan Ding},
  doi          = {10.7717/peerj-cs.2988},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2988},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An interpretable credit risk assessment model with boundary sample identification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep context-attentive transformer transfer learning for financial forecasting. <em>PEERJCS</em>, <em>11</em>, e2983. (<a href='https://doi.org/10.7717/peerj-cs.2983'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents 2CAT (CNN-Correlation-based Attention Transformer), a deep learning model for financial time-series forecasting. The model integrates signal decomposition, convolutional layers, and correlation-based attention mechanisms to capture temporal patterns. A transfer learning framework is incorporated to enhance generalization across markets through pretraining, encoder freezing, and fine-tuning. Evaluation on six stock indices—Dow Jones Industrial Average (DJIA), Nikkei 225 (N225), Hang Seng Index (HSI), Shanghai Stock Exchange (SSE), Bombay Stock Exchange (BSE), and the Stock Exchange of Thailand (SET)—demonstrates strong predictive accuracy. On DJIA, 2CAT records an MSE of 0.0655, MAE of 0.2023, and R2 of 0.9169, outperforming Deep-Transformer, which yields an MSE of 0.1360 and R2 of 0.8274. The SET index, which posed challenges for previous models, demonstrates notable improvement with 2CAT, achieving an R2 of 0.9094. Wilcoxon signed-rank test confirms statistically significant gains in non-transfer learning scenarios at the 0.05 level. Transfer learning experiments reveal statistically significant improvements, reinforcing the feasibility of cross-market knowledge transfer. An ablation study highlights the impact of architectural refinements and rotary positional encoding, while prediction horizon analysis confirms stable forecasting performance. These results establish 2CAT as a robust financial forecasting framework adaptable to diverse market conditions.},
  archive      = {J_PEERJCS},
  author       = {Ling Feng and Ananta Sinchai},
  doi          = {10.7717/peerj-cs.2983},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2983},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep context-attentive transformer transfer learning for financial forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMAD-LDS: Enhanced secure message authentication and dissemination with lightweight digital signature in the internet of vehicles. <em>PEERJCS</em>, <em>11</em>, e2982. (<a href='https://doi.org/10.7717/peerj-cs.2982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Vehicles (IoV) plays a crucial role in enhancing driver experience and road safety by enabling vehicles to share real-time traffic information. However, the nature of the IoV communication exposes vehicles to potential security risks. Therefore, robust authentication mechanisms are essential to prevent malicious vehicles from disseminating misleading information. Ensuring secure communication in IoV necessitates addressing critical security and privacy requirements, including entity and data authentication, privacy preservation, and confidentiality. Recent research in this area frequently encounters challenges related to high computation and communication overheads, as well as vulnerability to diverse security threats. Consequently, this article proposes secure message authentication and dissemination with lightweight digital signature (SMAD-LDS), a robust message authentication and dissemination scheme incorporating a novel lightweight digital signature technique designed to minimize these overheads. There are four main phases in the proposed SMAD-LDS scheme such as the initiation phase, registration phase, send/receive message phase, and key updating phase. For the registration phase, our results demonstrate that the computation cost in SMAD-LDS is reduced by at least 46.8% compared to other works in the literature. Moreover, the computation overhead in the send/receive messages phase in SMAD-LDS is reduced by at least 94%. Additionally, the overall cost of communications in the proposed article has improved. The cost of communications in the registration phase and send/receive message phase has improved by 28% and 70%, respectively, compared to other works. Furthermore, SMAD-LDS is resistant to known attacks such as modification attacks, impersonation attacks, eavesdropping attacks, fake roadside unit (RSU) attacks, traceability attacks, and replay attacks.},
  archive      = {J_PEERJCS},
  author       = {Islam Z. Ahmed and Yasser Hifny and Rowayda A. Sadek},
  doi          = {10.7717/peerj-cs.2982},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2982},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SMAD-LDS: Enhanced secure message authentication and dissemination with lightweight digital signature in the internet of vehicles},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anomaly detection method for power system information based on multimodal data. <em>PEERJCS</em>, <em>11</em>, e2976. (<a href='https://doi.org/10.7717/peerj-cs.2976'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing complexity of modern power systems, effective anomaly detection is essential to ensure operational security. Conventional methods often depend on single-domain data, which limits their ability to fully capture the dynamic behavior of power systems. This study introduces a novel multimodal approach that integrates time-domain and frequency-domain data to improve anomaly detection accuracy and robustness. By leveraging this integration, our method captures both temporal patterns and spectral signatures, offering a more comprehensive analysis of system behavior—an advancement that significantly enhances detection performance compared to traditional techniques. Experimental results show that our approach achieves a detection accuracy of 97.6%, outperforming baseline methods. Beyond its technical merits, this method has practical implications for real-world power systems, enabling early identification of security threats, improving system reliability, and reducing the risk of operational failures. These findings contribute to the field of power system security and provide a versatile framework for anomaly detection in critical infrastructures.},
  archive      = {J_PEERJCS},
  author       = {Liyue Chen and XuXiang Zhou and Peng Zhou and Xin Sun and SenSen Zheng},
  doi          = {10.7717/peerj-cs.2976},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2976},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Anomaly detection method for power system information based on multimodal data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IDILI-MT: Identifying drug-induced liver injury compounds with a multi-head transformer. <em>PEERJCS</em>, <em>11</em>, e2973. (<a href='https://doi.org/10.7717/peerj-cs.2973'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drug-induced liver injury (DILI) is a leading cause of late-stage drug attrition and post-approval withdrawals, making early in silico risk assessment essential for drug safety. We present iDILI-MT (identifying drug-induced liver injury compounds with a multi-head Transformer), a self-contained computational framework that integrates a feed-forward network for sequential feature extraction, a multi-head Transformer encoder for contextual representation learning, and a squeeze-and-excitation attention module for channel-wise feature recalibration. Evaluated on a curated set of 1,919 small-molecule compounds, iDILI-MT outperformed traditional machine-learning classifiers and state-of-the-art graph neural networks, achieving a mean area under the receiver-operating-characteristic curve (AUC-ROC) of 0.8499, area under the precision-recall curve (AUC-PR) of 0.8905, and F1 score of 0.8173 across ten trials. Attention-weight analysis reveals that the combined multi-head and squeeze-and-excitation attention mechanisms effectively highlight key substructural and chemical motifs associated with hepatotoxicity. These findings indicate that iDILI-MT provides an useful method for detecting compounds at risk of DILI, potentially accelerating safety assessments in drug development.},
  archive      = {J_PEERJCS},
  author       = {Wanrong Zheng and Fobao Lai},
  doi          = {10.7717/peerj-cs.2973},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2973},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IDILI-MT: Identifying drug-induced liver injury compounds with a multi-head transformer},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gated attention based generative adversarial networks for imbalanced credit card fraud detection. <em>PEERJCS</em>, <em>11</em>, e2972. (<a href='https://doi.org/10.7717/peerj-cs.2972'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit card fraud detection is highly important to maintain financial security. However, it is challenging to train suitable models due to the class imbalance in credit card transaction data. To address this issue, this work proposes a novel deep learning framework, gated attention-based generative adversarial networks (GA-GAN) for credit card fraud detection in class-imbalanced data. GA-GAN integrates GAN and the gated attention mechanism to generate high-quality synthetic data that realistically simulates fraudulent behaviors. Experimental results on two public credit card datasets demonstrate that GA-GAN outperforms state-of-the-art methods on credit card fraud detection tasks in class-imbalanced data, indicating the advantage of GA-GAN. The code is publicly available at https://github.com/Gejiangmeng/gagan/tree/main.},
  archive      = {J_PEERJCS},
  author       = {Jiangmeng Ge and Lanxiang Yin and Shiqing Zhang and Xiaoming Zhao},
  doi          = {10.7717/peerj-cs.2972},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2972},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Gated attention based generative adversarial networks for imbalanced credit card fraud detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Linear B-cell epitope prediction for SARS and COVID-19 vaccine design: Integrating balanced ensemble learning models and resampling strategies. <em>PEERJCS</em>, <em>11</em>, e2970. (<a href='https://doi.org/10.7717/peerj-cs.2970'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a comprehensive machine learning framework to enhance the prediction accuracy of B-cell epitopes and antibody recognition related to Severe Acute Respiratory Syndrome (SARS) and Coronavirus Disease 2019 (COVID-19). To address the issue of data imbalance, various resampling techniques were applied using three types of strategies: over-sampling, under-sampling, and hybrid-sampling. The implemented resampling methods were designed to improve class balance and enhance model training. The rebalanced datasets were then used in model building with ensemble classifiers employing Boosting, Bagging, and Balancing strategies. Hyperparameter optimization for the classifiers was conducted using GridSearchCV, while feature selection was performed with the recursive feature elimination (RFE) algorithm. Model performance was evaluated using seven different metrics: Accuracy, Precision, Recall, F1-score, receiver operating characteristic area under the curve (ROC AUC), precision recall area under the curve (PR AUC), and Matthews correlation coefficient (MCC). Furthermore, statistical significance analyses including paired t-test, Wilcoxon, and permutation tests confirmed the reliability of the model improvements. To interpret the model’s predictive behavior, peptides with the highest confidence among correctly classified instances were identified as potential epitope candidates. The results indicate that the combination of Synthetic Minority Over-Sampling Technique—Edited Nearest Neighbors (SMOTE-ENN), and ExtraTrees yielded the best performance, achieving an ROC AUC score of 0.9899. The combination of Instance Hardness Threshold (IHT) and ExtraTrees followed closely with a score of 0.9799. These findings emphasize the effectiveness of integrating resampling models and balancing ensemble classifiers in improving the accuracy of B-cell epitope prediction and antibody recognition for SARS and COVID-19 infections. This study contributes to vaccine development efforts and the advancement of immunoinformatics research by identifying promising epitope candidates.},
  archive      = {J_PEERJCS},
  author       = {Fatih Gurcan},
  doi          = {10.7717/peerj-cs.2970},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2970},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Linear B-cell epitope prediction for SARS and COVID-19 vaccine design: Integrating balanced ensemble learning models and resampling strategies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSKT: Multimodal data fusion for improved nursing management in hemorrhagic stroke. <em>PEERJCS</em>, <em>11</em>, e2969. (<a href='https://doi.org/10.7717/peerj-cs.2969'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The study aims to address the challenges of nursing decision-making and the optimization of personalized nursing plans in the management of hemorrhagic stroke. Due to the rapid progression and high complexity of hemorrhagic stroke, traditional nursing methods struggle to cope with the challenges posed by its high incidence and high disability rate. Methods To address this, we propose an innovative approach based on multimodal data fusion and a non-stationary Gaussian process model. Utilizing multidimensional data from the MIMIC-IV database (including patient medical history, nursing records, laboratory test results, etc.), we developed a hybrid predictive model with a multiscale kernel transformer non-stationary Gaussian process (MSKT-NSGP) architecture to handle non-stationary time-series data and capture the dynamic changes in a patient’s condition. Results The proposed MSKT-NSGP model outperformed traditional algorithms in prediction accuracy, computational efficiency, and uncertainty handling. For hematoma expansion prediction, it achieved 85.5% accuracy, an area under the curve (AUC) of 0.87, and reduced mean squared error (MSE) by 18% compared to the sparse variational Gaussian process (SVGP). With an inference speed of 55 milliseconds per sample, it supports real-time predictions. The model maintained a confidence interval coverage near 95% with narrower widths, indicating precise uncertainty estimation. These results highlight its potential to enhance nursing decision-making, optimize personalized plans, and improve patient outcomes.},
  archive      = {J_PEERJCS},
  author       = {Ting Zhou and Dandan Li and Jingfang Zuo and Aihua Gu and Li Zhao},
  doi          = {10.7717/peerj-cs.2969},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2969},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MSKT: Multimodal data fusion for improved nursing management in hemorrhagic stroke},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evolutionary bi-LSTM-DQN framework for enhanced recognition and classification in rural information management. <em>PEERJCS</em>, <em>11</em>, e2967. (<a href='https://doi.org/10.7717/peerj-cs.2967'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As deep learning and reinforcement learning technologies advance, intelligent rural information management is transforming substantially. This article presents an innovative framework, the evolutionary bidirectional long short-term memory deep Q-network (EBLM-DQN), which integrates evolutionary algorithms, reinforcement learning, and bidirectional long short-term memory (Bi-LSTM) networks to significantly improve the accuracy and efficiency of rural information management, particularly for recognizing and classifying information relevant to farmers. The proposed framework begins with data preprocessing using disambiguation techniques and data complementation, followed by temporal feature extraction via a Bi-LSTM layer. It then employs a deep Q-network (DQN) to adjust and optimize weights dynamically. After feature extraction and weight optimization, evolutionary algorithms are used to select the optimal weights, enabling precise recognition and classification of conditions encountered by farmers seeking assistance. Experimental results indicate that the EBLM-DQN framework outperforms existing frameworks on public datasets and real-world applications, providing higher classification accuracy. This framework offers valuable technical support and a reference for future optimization and development of rural information management systems.},
  archive      = {J_PEERJCS},
  author       = {Taiping Deng and Xi He and Jiao Li and Feifei Ye and Jingyang Tang},
  doi          = {10.7717/peerj-cs.2967},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2967},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An evolutionary bi-LSTM-DQN framework for enhanced recognition and classification in rural information management},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A methodological approach for inferring causal relationships from opinions and news-derived events with an application to climate change. <em>PEERJCS</em>, <em>11</em>, e2964. (<a href='https://doi.org/10.7717/peerj-cs.2964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms like Twitter (now X) provide a global forum for discussing ideas. In this work, we propose a novel methodology for detecting causal relationships in online discourse. Our approach integrates multiple causal inference techniques to analyze how public sentiment and discourse evolve in response to key events and influential figures, using five causal detection methods: Direct-LiNGAM, PC, PCMCI, VAR, and stochastic causality. The datasets contain variables, such as different topics, sentiments, and real-world events, among which we seek to detect causal relationships at different frequencies. The proposed methodology is applied to climate change opinions and data, offering insights into the causal relationships among public sentiment, specific topics, and natural disasters. This approach provides a framework for analyzing various causal questions. In the specific case of climate change, we can hypothesize that a surge in discussions on a specific topic consistently precedes a change in overall sentiment, level of aggressiveness, or the proportion of users expressing certain stances. We can also conjecture that real-world events, like natural disasters and the rise to power of politicians leaning towards climate change denial, may have a noticeable impact on the discussion on social media. We illustrate how the proposed methodology can be applied to examine these questions by combining datasets on tweets and climate disasters.},
  archive      = {J_PEERJCS},
  author       = {Juan Marten and Fernando Delbianco and Fernando Tohme and Ana G. Maguitman},
  doi          = {10.7717/peerj-cs.2964},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2964},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A methodological approach for inferring causal relationships from opinions and news-derived events with an application to climate change},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tencent meeting forensics based on memory reverse analysis. <em>PEERJCS</em>, <em>11</em>, e2963. (<a href='https://doi.org/10.7717/peerj-cs.2963'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tencent Meeting, an instant meeting software, is widely used at present, but no research has been conducted on its forensics. Since the real-time data generated by such software during meetings will not be stored in the computer disk, the traditional disk forensics method against such software is no longer applicable and needs to obtain evidence through memory analysis. To extract meeting data transmitted during meetings, this article proposes a method for Tencent Meeting forensics based on memory reverse analysis. First, by analyzing the process storage and metadata format of Tencent Meeting in memory, an inverse metadata extraction algorithm is designed. Then, by analyzing the data structure of Tencent Meeting in memory, a meeting data stream engraving algorithm is developed. Finally, the experimental results indicate that the proposed method can effectively extract metadata information such as meeting time, meeting number, topic, and data flow information such as participants, message records, as well as transmitted files from the memory of Tencent Meeting, providing crucial digital evidence for digital crime investigation. Compared with other forensic analysis methods for instant meeting software, our proposed forensic method for Tencent Meeting conducts memory reverse analysis with the entire memory file, enabling the extraction of more comprehensive and abundant forensic data.},
  archive      = {J_PEERJCS},
  author       = {Shilong Yu and Binglong Li and Lin Zhu and Heyu Zhang and Sen Yang and Zhangxiao Li and Wenzheng Feng},
  doi          = {10.7717/peerj-cs.2963},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2963},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Tencent meeting forensics based on memory reverse analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The geometry of meaning: Evaluating sentence embeddings from diverse transformer-based models for natural language inference. <em>PEERJCS</em>, <em>11</em>, e2957. (<a href='https://doi.org/10.7717/peerj-cs.2957'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language inference (NLI) is a fundamental task in natural language processing that focuses on determining the relationship between pairs of sentences. In this article, we present a simple and straightforward approach to evaluate the effectiveness of various transformer-based models such as bidirectional encoder representations from transformers (BERT), Generative Pre-trained Transformer (GPT), robustly optimized BERT approach (RoBERTa), and XLNet in generating sentence embeddings for NLI. We conduct comprehensive experiments with different pooling techniques and evaluate the embeddings using different norms across multiple layers of each model. Our results demonstrate that the choice of pooling strategy, norm, and model layer significantly impacts the performance of NLI, with the best results achieved using max pooling and the L2 norm across specific model layers. On the Stanford Natural Language Inference (SNLI) dataset, the model reached 90% accuracy and 86% F1-score, while on the MedNLI dataset, the highest F1-score recorded was 84%. This article provides insights into how different models and evaluation strategies can be effectively combined to improve the understanding and classification of sentence relationships in NLI tasks.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Alsuhaibani},
  doi          = {10.7717/peerj-cs.2957},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2957},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The geometry of meaning: Evaluating sentence embeddings from diverse transformer-based models for natural language inference},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The accuracy-bias trade-offs in AI text detection tools and their impact on fairness in scholarly publication. <em>PEERJCS</em>, <em>11</em>, e2953. (<a href='https://doi.org/10.7717/peerj-cs.2953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) text detection tools are considered a means of preserving the integrity of scholarly publication by identifying whether a text is written by humans or generated by AI. This study evaluates three popular tools (GPTZero, ZeroGPT, and DetectGPT) through two experiments: first, distinguishing human-written abstracts from those generated by ChatGPT o1 and Gemini 2.0 Pro Experimental; second, evaluating AI-assisted abstracts where the original text has been enhanced by these large language models (LLMs) to improve readability. Results reveal notable trade-offs in accuracy and bias, disproportionately affecting non-native speakers and certain disciplines. This study highlights the limitations of detection-focused approaches and advocates a shift toward ethical, responsible, and transparent use of LLMs in scholarly publication.},
  archive      = {J_PEERJCS},
  author       = {Ahmad R. Pratama},
  doi          = {10.7717/peerj-cs.2953},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2953},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The accuracy-bias trade-offs in AI text detection tools and their impact on fairness in scholarly publication},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced recurrent attention-deep q learning with optimal node constrains and effective penalty based model for data transmission scheduling on wireless sensor networks. <em>PEERJCS</em>, <em>11</em>, e2950. (<a href='https://doi.org/10.7717/peerj-cs.2950'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective scheduling of data transmission is critical to maximizing network performance and resource usage in the context of wireless sensor networks (WSNs). In order to improve the effectiveness of data transmission scheduling in wireless sensor networks (WSNs), this paper proposes a unique method called Recurrent Attention-Deep Q Learning with Optimal Node Constraints and Effective Penalty based WSN Scheduling (RA-DQL-ONC&EP). This technique performs dynamic scheduling of data transmission tasks considering energy consumption and network interference by combining a penalty-based model, optimal node limitations, and recurrent attention techniques. Simulation results show that the proposed approach performs remarkably well. With a 91.21% success rate, it also guarantees dependable data transport throughout the network. Additionally, the delay rate is reduced to 1.99%, demonstrating effective data transfer with low latency. It is an effective model, yet it uses 70% less energy than other models since it is energy-efficient. The algorithm’s performance is further demonstrated by throughput analysis, which shows a 72% throughput over 1,000 time steps. Based on enhanced reliability, efficiency, and energy conservation in network operations, our results highlight the potential of RA-DQL-ONC&EP as a promising approach for improving data transmission scheduling in WSNs. This optimized scheduling enhances network reliability, ensuring timely and accurate data delivery, which can support various applications such as environmental monitoring, healthcare systems, and smart city infrastructure, ultimately fostering societal well-being and progress. Additionally, the algorithm’s efficiency contributes to cost savings and resource conservation, making it a socially responsible choice for managing wireless sensor networks.},
  archive      = {J_PEERJCS},
  author       = {D.R. Anita Sofia Liz and Yesubai Rubavathi C},
  doi          = {10.7717/peerj-cs.2950},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2950},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced recurrent attention-deep q learning with optimal node constrains and effective penalty based model for data transmission scheduling on wireless sensor networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized customer churn prediction using tabular generative adversarial network (GAN)-based hybrid sampling method and cost-sensitive learning. <em>PEERJCS</em>, <em>11</em>, e2949. (<a href='https://doi.org/10.7717/peerj-cs.2949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Imbalanced and overlapped data in customer churn prediction significantly impact classification results. Various sampling and hybrid sampling methods have demonstrated effectiveness in addressing these issues. However, these methods have not performed well with classical machine learning algorithms. Methods To optimize the performance of classical machine learning on customer churn prediction tasks, this study introduces an extension framework called CostLearnGAN, a tabular generative adversarial network (GAN)-hybrid sampling method, and cost-sensitive Learning. Utilizing a cost-sensitive learning perspective, this research aims to enhance the performance of several classical machine learning algorithms in customer churn prediction tasks. Based on the experimental results classical machine learning algorithms exhibit shorter execution times, making them suitable for predicting churn in large customer bases. Results This study conducted an experiment with six comparative sampling methods, six datasets, and three machine learning algorithms. The results show that CostLearnGAN achieved a satisfying result across all evaluation metrics with a 1.44 average mean rank score. Additionally, this study provided a robustness measurement for algorithms, demonstrating that CostLearnGAN outperforms other sampling methods in improving the performance of classical machine learning models with a 5.68 robustness value on average.},
  archive      = {J_PEERJCS},
  author       = {I Nyoman Mahayasa Adiputra and Paweena Wanchai and Pei-Chun Lin},
  doi          = {10.7717/peerj-cs.2949},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2949},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimized customer churn prediction using tabular generative adversarial network (GAN)-based hybrid sampling method and cost-sensitive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IAESR: IoT-oriented authenticated encryption based on iShadow round function. <em>PEERJCS</em>, <em>11</em>, e2947. (<a href='https://doi.org/10.7717/peerj-cs.2947'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing popularity of the Internet of Things (IoT) devices and the widespread application of embedded systems, the demand for security and resource efficiency in these devices is also increasing. Traditional authenticated encryption (AE) algorithms are often unsuitable for lightweight devices due to their complexity and resource consumption, creating a need for lightweight AE algorithms. Lightweight devices typically have limited processing power, storage capacity, and energy resources, which necessitates the design of simple and efficient encryption algorithms that can function within these constraints. Despite these resource limitations, security remains of paramount importance. Therefore, lightweight AE algorithms must minimize resource consumption while ensuring adequate security. This article presents a theoretical lightweight AE scheme based on Shadow, a lightweight block encryption algorithm, to address the requirements for secure communication in resource-constrained environments. The scheme first enhances the Shadow algorithm by introducing the improved Shadow (iShadow) algorithm. It then combines this with the duplex sponge structure to propose the IoT-oriented authenticated encryption based on the iShadow round function (IAESR). The integration of iShadow with the duplex sponge structure achieves a balance between security and efficiency through three key mechanisms: (1) The sponge’s capacity (64/128-b for IAESR-32/64) provides provable indistinguishability under chosen-plaintext attack (IND-CPA) and chosen-ciphertext attack (IND-CCA) security bounds, effectively resisting generic attacks with an adversarial advantage limited to O(q2/2c); (2) the duplex mode’s single-pass processing reduces memory overhead by reusing the permutation state; and (3) iShadow’s ARX operations reduce energy consumption to 0.4–0.5 µJ/byte on 32-b microcontrollers, outperforming AES-GCM by 20–30%. Empirical tests on an Intel i5-1035G1 CPU demonstrate stable execution times. This design ensures the security and integrity of communication while balancing efficiency, and resource utilization. This design ensures IND-CCA secure confidentiality and integrity against plaintext (INT-PTXT), as demonstrated by the security bounds of the sponge construction. Specifically, IAESR guarantees both confidentiality and authenticity. Additionally, it is particularly well-suited for scenarios with lightweight requirements, such as those found in the IoT.},
  archive      = {J_PEERJCS},
  author       = {Yanshuo Zhang and Liqiu Li and Hengyu Bao and Xiaohong Qin and Zhiyuan Zhang and Xiaoyi Duan},
  doi          = {10.7717/peerj-cs.2947},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2947},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IAESR: IoT-oriented authenticated encryption based on iShadow round function},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A malware detection method with function parameters encoding and function dependency modeling. <em>PEERJCS</em>, <em>11</em>, e2946. (<a href='https://doi.org/10.7717/peerj-cs.2946'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As computers are widely used in people’s work and daily lives, malware has become an increasing threat to network security. Although researchers have introduced traditional machine learning and deep learning methods to conduct extensive research on functions in malware detection, these methods have largely ignored the analysis of function parameters and functional dependencies. To address these limitations, we propose a new malware detection method. Specifically, we first design a parameter encoder to convert various types of function parameters into feature vectors, and then discretize various parameter features through clustering methods to enhance the representation of API encoding. Additionally, we design a deep neural network to capture functional dependencies, enabling the generation of robust semantic representations of function sequences. Experiments on a large-scale malware detection dataset demonstrate that our method outperforms other techniques, achieving 98.62% accuracy and a 98.40% F1-score. Furthermore, the results of ablation experiments show the important role of function parameters and functional dependencies in malware detection.},
  archive      = {J_PEERJCS},
  author       = {Ronghao Hou and Dongjie Liu and Xiaobo Jin and Jian Weng and Guanggang Geng},
  doi          = {10.7717/peerj-cs.2946},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2946},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A malware detection method with function parameters encoding and function dependency modeling},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An interpretable multi-transformer ensemble for text-based movie genre classification. <em>PEERJCS</em>, <em>11</em>, e2945. (<a href='https://doi.org/10.7717/peerj-cs.2945'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label movie genre classification is challenging due to the inherent ambiguity and overlap between different genres. Most of the existing works in genre classification use audio-visual modalities. The potential of text-based modalities in movie genre classification is still underexplored. This paper proposes an ensemble deep-learning model that uses movie plots to predict movie genres. After pre-processing the text plots, three transformer-based models, Bidirectional Encoder Representations from Transformers (BERT), DistilBERT, and Robustly Optimized BERT Pre-training Approach (ROBERTa), are used to generate genre predictions, combined through a weighted soft-voting method. The proposed ensemble architecture achieves state-of-the-art performance on two benchmark datasets, Trailers12K and LMTD9, with a micro-average precision of 80.10% and 80.37%, respectively, significantly outperforming both traditional machine learning approaches and advanced deep learning models. The ensemble’s superior performance is attributed to its ability to combine the diverse strengths of individual models and capture nuanced genre-specific information from textual features. The lack of interpretability in deep learning models for genre classification is addressed using Local Interpretable Model-Agnostic Explanations (LIME), which provides both local and global explanations for the model’s predictions. The findings of the study highlight the potential of textual data in automated genre classification and emphasize the importance of interpretability methods in multi-label genre classification.},
  archive      = {J_PEERJCS},
  author       = {Faheem Shaukat and Naveed Ejaz and Zeeshan Ashraf and Mrim M. Alnfiai and Nouf Nawar Alotaibi and Salma Mohsen M. Alnefaie},
  doi          = {10.7717/peerj-cs.2945},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2945},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An interpretable multi-transformer ensemble for text-based movie genre classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new approach of anomaly detection in shopping center surveillance videos for theft prevention based on RLCNN model. <em>PEERJCS</em>, <em>11</em>, e2944. (<a href='https://doi.org/10.7717/peerj-cs.2944'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of video data produced daily by today’s surveillance systems is enormous, making analysis difficult for computer vision specialists. It is challenging to continuously search these massive video streams for unexpected accidents because they occur seldom and have little chance of being observed. Contrarily, deep learning-based anomaly detection decreases the need for human labor and has comparably trustworthy decision-making capabilities, hence promoting public safety. In this article, we introduce a system for efficient anomaly detection that can function in surveillance networks with a modest level of complexity. The proposed method starts by obtaining spatiotemporal features from a group of frames. The multi-layer extended short-term memory model can precisely identify continuing unusual activity in complicated video scenarios of a busy shopping mall once we transmit the in-depth features extracted. We conducted in-depth tests on numerous benchmark datasets for anomaly detection to confirm the proposed framework’s functionality in challenging surveillance scenarios. Compared to state-of-the-art techniques, our datasets, UCF50, UCF101, UCFYouTube, and UCFCustomized, provided better training and increased accuracy. Our model was trained for more classes than usual, and when the proposed model, RLCNN, was tested for those classes, the results were encouraging. All of our datasets worked admirably. However, when we used the UCFCustomized and UCFYouTube datasets compared to other UCF datasets, we achieved greater accuracy of 96 and 97, respectively.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Sajid and Ali Haider Khan and Kaleem Razzaq Malik and Javed Ali Khan and Ayed Alwadain},
  doi          = {10.7717/peerj-cs.2944},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2944},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A new approach of anomaly detection in shopping center surveillance videos for theft prevention based on RLCNN model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topic adversarial neural network for cross-topic cyberbullying detection. <em>PEERJCS</em>, <em>11</em>, e2942. (<a href='https://doi.org/10.7717/peerj-cs.2942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the proliferation of social media, cyberbullying has emerged as a pervasive threat, causing significant psychological harm to individuals and undermining social cohesion. Its linguistic expressions vary widely across topics, complicating automatic detection efforts. Most existing methods struggle to generalize across diverse online contexts due to their reliance on topic-specific features. To address this issue, we propose the Topic Adversarial Neural Network (TANN), a novel end-to-end framework for topic-invariant cyberbullying detection. TANN integrates a multi-level feature extractor with a topic discriminator and a cyberbullying detector. It leverages adversarial training to disentangle topic-related information while retaining universal linguistic cues relevant to harmful content. We construct a multi-topic dataset from major Chinese social media platforms, such as Weibo and Tieba, to evaluate the generalization performance of TANN in real-world scenarios. Experimental results demonstrate that TANN outperforms existing methods in cross-topic detection tasks, significantly improving robustness and accuracy. This work advances cross-topic cyberbullying detection by introducing a scalable solution that mitigates topic interference and enables reliable performance across dynamic online environments.},
  archive      = {J_PEERJCS},
  author       = {Shufeng Xiong and Wenzhuo Liu and Bingkun Wang and Yinchao Che and Lei Shi},
  doi          = {10.7717/peerj-cs.2942},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2942},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Topic adversarial neural network for cross-topic cyberbullying detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAPNet: Single and multiplant leaf disease classification method based on simplified SqueezeNet for grape, apple and potato plants. <em>PEERJCS</em>, <em>11</em>, e2941. (<a href='https://doi.org/10.7717/peerj-cs.2941'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans need food to sustain their lives. Therefore, agriculture is one of the most important issues in nations. Agriculture also plays a major role in the economic development of countries by increasing economic income. Early diagnosis of plant diseases is crucial for agricultural productivity and continuity. Early disease detection directly impacts the quality and quantity of crops. For this reason, many studies have been carried out on plant leaf disease classification. In this study, a simple and effective leaf disease classification method was developed. Disease classification was performed using seven state-of-the-art pretrained convolutional neural network architectures: VGG16, ResNet50, SqueezeNet, Xception, ShuffleNet, DenseNet121 and MobileNetV2. A simplified SqueezeNet model, GAPNet, was subsequently proposed for grape, apple and potato leaf disease classification. GAPNet was designed to be a lightweight and fast model with 337.872 parameters. To address the data imbalance between classes, oversampling was carried out using the synthetic minority oversampling technique. The proposed model achieves accuracy rates of 99.72%, 99.53%, and 99.83% for grape, apple and potato leaf disease classification, respectively. A success rate of 99.64% was achieved in multiplant leaf disease classification when the grape, apple and potato datasets were combined. Compared with the state-of-the-art methods, the lightweight GAPNet model produces promising results for various plant species.},
  archive      = {J_PEERJCS},
  author       = {Özge Nur Özaras and Asuman Günay Yılmaz},
  doi          = {10.7717/peerj-cs.2941},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2941},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GAPNet: Single and multiplant leaf disease classification method based on simplified SqueezeNet for grape, apple and potato plants},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fusing transformer-XL with bi-directional recurrent networks for cyberbullying detection. <em>PEERJCS</em>, <em>11</em>, e2940. (<a href='https://doi.org/10.7717/peerj-cs.2940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying cyberbullying in languages other than English presents distinct difficulties owing to linguistic subtleties and scarcity of annotated datasets. This article presents a new method for identifying cyberbullying in Bengali text data using the Kaggle dataset. This strategy combines Transformer-Extra Large (XL) with bi-directional recurrent neural networks (BiGRU-BiLSTM). Extensive data preparation was performed, including data cleaning, data analysis, and label encoding. Upsampling methods were used to handle imbalanced classes, and data augmentation enhanced the training dataset. We carried out tokenization of the text using a pre-trained tokenizer to capture semantic representations accurately. The model we presented, Transformer-XL-bidirectional gated recurrent units (BiGRU)-bidirectional long short-term memory (BiLSTM), which is called Fusion Transformer-XL, surpassed the performance of the baseline models, attaining an accuracy of 98.17% and an F1-score of 98.18%. Local interpretable model-agnostic explanation (LIME) text explanations were used to understand the reasoning behind the model’s choices and performed the cross-dataset evaluation of the model using the English dataset. This helped improve the clarity and reliability of the proposed method. Furthermore, implementing k-fold cross-validation ensures our model’s robustness and adaptability across diverse data categories. The results of our study demonstrate the effectiveness of combining Transformer-XL with bi-directional recurrent networks for detecting cyberbullying in Bengali. This emphasizes the significance of using hybrid architectures to address intricate natural language processing problems in languages with limited resources. This study enhances the development of methods for detecting cyberbullying and opens up opportunities for additional investigation into language diversity and social media analytics.},
  archive      = {J_PEERJCS},
  author       = {Md. Mithun Hossain and Md. Shakil Hossain and Md. Shakhawat Hossain and M. Firoz Mridha and Mejdl Safran and Sultan Alfarhood and Dunren Che},
  doi          = {10.7717/peerj-cs.2940},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2940},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fusing transformer-XL with bi-directional recurrent networks for cyberbullying detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Systematic review: Progress in EEG-based speech imagery brain-computer interface decoding and encoding research. <em>PEERJCS</em>, <em>11</em>, e2938. (<a href='https://doi.org/10.7717/peerj-cs.2938'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article systematically reviews the latest developments in electroencephalogram (EEG)-based speech imagery brain-computer interface (SI-BCI). It explores the brain connectivity of SI-BCI and reveals its key role in neural encoding and decoding. It analyzes the research progress on vowel-vowel and vowel-consonant combinations, as well as Chinese characters, words, and long-words speech imagery paradigms. In the neural encoding section, the preprocessing and feature extraction techniques for EEG signals are discussed in detail. The neural decoding section offers an in-depth analysis of the applications and performance of machine learning and deep learning algorithms. Finally, the challenges faced by current research are summarized, and future directions are outlined. The review highlights that future research should focus on brain region mechanisms, paradigms innovation, and the optimization of decoding algorithms to promote the practical application of SI-BCI technology.},
  archive      = {J_PEERJCS},
  author       = {Ke Su and Liang Tian},
  doi          = {10.7717/peerj-cs.2938},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2938},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Systematic review: Progress in EEG-based speech imagery brain-computer interface decoding and encoding research},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing book genre classification with BERT and InceptionV3: A deep learning approach for libraries. <em>PEERJCS</em>, <em>11</em>, e2934. (<a href='https://doi.org/10.7717/peerj-cs.2934'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate book genre classification is essential for library organization, information retrieval, and personalized recommendations. Traditional classification methods, often reliant on manual categorization and metadata-based approaches, struggle with the complexities of hybrid genres and evolving literary trends. To address these limitations, this study proposes a hybrid deep learning model that integrates visual and textual features for enhanced genre classification. Specifically, we employ InceptionV3, an advanced convolutional neural network architecture, to extract visual features from book cover images and bidirectional encoder representations from transformers (BERT) to analyze textual data from book titles. A scaled dot-product attention mechanism is used to effectively fuse these multimodal features, dynamically weighting their contributions based on contextual relevance. Experimental results on the BookCover30 dataset demonstrate that our proposed model outperforms baseline approaches, achieving a balanced accuracy of 0.7951 and an F1-score of 0.7920, surpassing both standalone image- and text-based classifiers. This study highlights the potential of deep learning in improving automated genre classification, offering a scalable and adaptable solution for libraries and digital platforms. Future research may focus on expanding dataset diversity, optimizing computational efficiency, and addressing biases in classification models.},
  archive      = {J_PEERJCS},
  author       = {Xinting Yang and Zehua Zhang},
  doi          = {10.7717/peerj-cs.2934},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2934},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing book genre classification with BERT and InceptionV3: A deep learning approach for libraries},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VBM-YOLO: An enhanced YOLO model with reduced information loss for vehicle body markers detection. <em>PEERJCS</em>, <em>11</em>, e2932. (<a href='https://doi.org/10.7717/peerj-cs.2932'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In vehicle safety detection, the accurate identification of body markers on medium and large vehicles plays a critical role in ensuring safe road travel. To address the issues of the feature and gradient information loss in previous You Only Look Once (YOLO) series models, a novel Vehicle Body Markers YOLO (VBM-YOLO) model has been designed. Firstly, the model integrates the cross-spatial-channel attention (CSCA) mechanism proposed in this study. The CSCA uses cross-dimensional information to address interaction issues during the fusion of spatial and channel dimensions, significantly enhancing the model’s representational capacity. Secondly, we propose a multi-scale selective feature pyramid network (MSSFPN). By a progressive fusion approach and multi-scale feature selection learning, MSSFPN alleviates the issues of feature loss and target layer information confusion caused by traditional top-down and bottom-up feature pyramids. Finally, an auxiliary gradient branch (AGB) is proposed. During training, AGB incorporates feature information from different target layers to help the current layer retain complete gradient information. Additionally, the AGB branch does not participate in model inference, thereby reducing additional overhead. Experimental results demonstrate that VBM-YOLO improves mean average precision (mAP) by 2.3% and 4.3% at intersection over union (IoU) thresholds of 0.5 and 0.5:0.95, respectively, compared to YOLOv8s on the vehicle body markers dataset. VBM-YOLO also achieves a better balance between accuracy and computational resources than other mainstream models, exhibiting good generalization performance on public datasets like PASCAL VOC and D-Fire.},
  archive      = {J_PEERJCS},
  author       = {Bin Wang and Chao Li and Chao Zhou and Jun Sun},
  doi          = {10.7717/peerj-cs.2932},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2932},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {VBM-YOLO: An enhanced YOLO model with reduced information loss for vehicle body markers detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CST-net: Community-guided structural-temporal convolutional networks for popularity prediction. <em>PEERJCS</em>, <em>11</em>, e2931. (<a href='https://doi.org/10.7717/peerj-cs.2931'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to predict the popularity of online contents has important implications in a wide range of areas. The challenge of this problem comes from the inequality of the popularity of content and the numerous complex factors. Existing works fall into three main paradigms: feature-driven approaches, generative models, and methods based on deep learning, each with known strengths and limitations. In this article, we propose an end-to-end deep learning framework, called CST-Net, to combat the defects of existing methods. We first learn a low-dimensional embedding for each user based on historic interactions. Then, users are clustered into communities based on the learned user embeddings, and information cascades are represented as a series of episodes in the form of community interaction matrix. Afterwards, a convolutional architecture is applied to learn the representation of the entire information cascade. Finally, the extracted structural and temporal features are further combined to predict the incremental popularity. We validate the effectiveness of the proposed CST-Net by applying it on two different types of population-scale datasets, i.e., a microblogging dataset and an academic citation dataset. Experimental results demonstrate that the proposed CST-Net model consistently outperforms the existing competitive popularity prediction methods.},
  archive      = {J_PEERJCS},
  author       = {Xuxu Zheng and Peng Bao and Lin Qi and Chen Tian and Huawei Shen},
  doi          = {10.7717/peerj-cs.2931},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2931},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {CST-net: Community-guided structural-temporal convolutional networks for popularity prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel dilated weighted recurrent neural network (RNN)-based smart contract for secure sharing of big data in ethereum blockchain using hybrid encryption schemes. <em>PEERJCS</em>, <em>11</em>, e2930. (<a href='https://doi.org/10.7717/peerj-cs.2930'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background With the enhanced data amount being created, it is significant to various organizations and their processing, and managing big data becomes a significant challenge for the managers of the data. The development of inexpensive and new computing systems and cloud computing sectors gave qualified industries to gather and retrieve the data very precisely however securely delivering data across the network with fewer overheads is a demanding work. In the decentralized framework, the big data sharing puts a burden on the internal nodes among the receiver and sender and also creates the congestion in network. The internal nodes that exist to redirect information may have inadequate buffer ability to momentarily take the information and again deliver it to the upcoming nodes that may create the occasional fault in the transmission of data and defeat frequently. Hence, the next node selection to deliver the data is tiresome work, thereby resulting in an enhancement in the total receiving period to allocate the information. Methods Blockchain is the primary distributed device with its own approach to trust. It constructs a reliable framework for decentralized control via multi-node data repetition. Blockchain is involved in offering a transparency to the application of transmission. A simultaneous multi-threading framework confirms quick data channeling to various network receivers in a very short time. Therefore, an advanced method to securely store and transfer the big data in a timely manner is developed in this work. A deep learning-based smart contract is initially designed. The dilated weighted recurrent neural network (DW-RNN) is used to design the smart contract for the Ethereum blockchain. With the aid of the DW-RNN model, the authentication of the user is verified before accessing the data in the Ethereum blockchain. If the authentication of the user is verified, then the smart contracts are assigned to the authorized user. The model uses elliptic Curve ElGamal cryptography (EC-EC), which is a combination of elliptic curve cryptography (ECC) and ElGamal encryption for better security, to make sure that big data transfers on the Ethereum blockchain are safe. The modified Al-Biruni earth radius search optimization (MBERSO) algorithm is used to make the best keys for this EC-EC encryption scheme. This algorithm manages keys efficiently and securely, which improves data security during blockchain operations. Results The processes of encryption facilitate the secure transmission of big data over the Ethereum blockchain. Experimental analysis is carried out to prove the efficacy and security offered by the suggested model in transferring big data over blockchain via smart contracts.},
  archive      = {J_PEERJCS},
  author       = {Swetha S and Joe Prathap P M},
  doi          = {10.7717/peerj-cs.2930},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2930},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel dilated weighted recurrent neural network (RNN)-based smart contract for secure sharing of big data in ethereum blockchain using hybrid encryption schemes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RA-QoS: A robust autoencoder-based QoS predictor for highly accurate web service QoS prediction. <em>PEERJCS</em>, <em>11</em>, e2928. (<a href='https://doi.org/10.7717/peerj-cs.2928'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Web services are fundamental for online service-oriented applications, where accurately predicting quality of service (QoS) is critical for recommending optimal services among multiple candidates. Since QoS data often contains noise—stemming from factors like remote user or service locations—current deep neural network (DNN)-based QoS predictors, which generally rely on L2-norm loss functions, face limitations in robustness due to sensitivity to outliers. To address this issue, we propose a novel robust autoencoder-based QoS predictor (RA-QoS) that leverages a hybrid loss function combining bias, training bias, L1-norm and L2-norm to build a robust Autoencoder. This hybrid approach allows RA-QoS to better handle noisy data, minimizing the impact of outliers and biases on prediction accuracy. The RA-QoS model further incorporates preprocessing and training biases, improving its adaptability to real-world QoS data. To evaluate the proposed RA-QoS predictor, extensive experiments are conducted on two real-world QoS datasets. The results demonstrate that our RA-QoS predictor exhibits superior robustness to outliers and higher accuracy in QoS prediction compared to the related state-of-the-art models.},
  archive      = {J_PEERJCS},
  author       = {Shun Fu and Junnan Li and Lufeng Wang},
  doi          = {10.7717/peerj-cs.2928},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2928},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RA-QoS: A robust autoencoder-based QoS predictor for highly accurate web service QoS prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid decision support system disaster management: Application of lattice ordered q-rung linear diophantine fuzzy hypersoft sets. <em>PEERJCS</em>, <em>11</em>, e2927. (<a href='https://doi.org/10.7717/peerj-cs.2927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of the lattice-ordered q-rung linear Diophantine fuzzy hypersoft set is a significant extension of fuzzy set theory. This study describes many of its fundamental algebraic operations, such as restricted union, extended union, restricted intersection, OR operation, and AND operation, along with examples. Further, an algorithm based on the proposed operations is presented in this study to handle multi-attributed decision-making problems extremely well, along with an illustrative multi-attribute decision-making example in the area of disaster management, which helps in choosing the most appropriate plan to tackle the known natural disaster by considering a greater number of attributes together. Further, the contribution of the method in the disaster management field is presented in the comparative analysis along with computational efficiency and scalability and an analysis of the comparison between the existing decision-making methods and the proposed one to express the superiority and advantages of the suggested approach over the existing methods.},
  archive      = {J_PEERJCS},
  author       = {J. Vimala and A. N. Surya and Nasreen Kausar and Dragan Pamucar and Seifedine Kadry and Jungeun Kim},
  doi          = {10.7717/peerj-cs.2927},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2927},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hybrid decision support system disaster management: Application of lattice ordered q-rung linear diophantine fuzzy hypersoft sets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RiceChain-plus: An enhanced framework for blockchain-based rice supply chain systems-ensuring security, privacy, and efficiency. <em>PEERJCS</em>, <em>11</em>, e2926. (<a href='https://doi.org/10.7717/peerj-cs.2926'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rice supply chain is a complex system that demands effective management to ensure reliability and efficiency, given the involvement of multiple stakeholders. Blockchain technology, with its decentralized and tamper-resistant nature, offers a promising solution for improving transparency, traceability, and credibility in agricultural supply chains. However, existing blockchain systems face several technological challenges, including security vulnerabilities, privacy concerns, and performance limitations. To address these issues, this article presents RiceChain-Plus, an enhanced architecture that incorporates a private Ethereum blockchain, proof of authority (PoA) consensus mechanism, mutual authentication, zero-knowledge proofs (ZKPs), a hybrid role-based access control (RBAC) and attribute-based access control (ABAC) system, and one-way hash functions. This approach enhances the rice supply chain’s security, privacy, and efficiency by safeguarding sensitive data and ensuring confidentiality. Performance assessments show that RiceChain-Plus surpasses existing benchmark models, achieving the lowest average execution costs (44,634 gas), reduced energy consumption (9.38828E−05 J), higher throughput (0.071201 transactions/s), faster execution (44.5 ms), and quicker transaction times (14.045 s), while also improving scalability. A comprehensive security analysis further confirms the framework’s resilience against various cyberattacks. These results highlight RiceChain-Plus as a secure, efficient, and effective solution for optimizing rice supply chain operations.},
  archive      = {J_PEERJCS},
  author       = {Bello Musa Yakubu and Abdullah Abdulrahman Alabdulatif and Pattarasinee Bhattarakosol},
  doi          = {10.7717/peerj-cs.2926},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2926},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RiceChain-plus: An enhanced framework for blockchain-based rice supply chain systems-ensuring security, privacy, and efficiency},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-objective federated learning traffic prediction in vehicular network for intelligent transportation system. <em>PEERJCS</em>, <em>11</em>, e2922. (<a href='https://doi.org/10.7717/peerj-cs.2922'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatial-temporal data of future freight traffic speed in the metropolitan region must be properly understood to develop freight-related traffic management strategies. This work introduces a new approach to traffic prediction using multi-objective federated learning. Instead of relying on a centralized cloud server for data processing, collaborative training is implemented among several participants. The proposed method utilizes the advantages of reinforcement learning in dynamic decision-making scenarios and the expressive capabilities of graphical models to identify traffic intensity. Furthermore, a new methodology integrates federated learning concepts with multi-objective optimization to forecast traffic patterns accurately. The proposed approach exhibits a higher level of performance than existing methods for estimating traffic speed. It achieves a communication delay of 23.4%, packet delivery ratio (PDR) of 92.45%, packet loss rate of 12.34%, prediction accuracy of 97.45%, and resource utilization of 89.56%. The visualisation findings demonstrate that this new approach is able to successfully capture interconnections of metropolitan areas in different neighboring cities.},
  archive      = {J_PEERJCS},
  author       = {Arulmurgan Aalavanthar and Famila S. and Shanmugam Sundaramurthy and Stefano Cirillo and Giandomenico Solimando and Giuseppe Polese},
  doi          = {10.7717/peerj-cs.2922},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2922},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-objective federated learning traffic prediction in vehicular network for intelligent transportation system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep vision-based real-time hand gesture recognition: A review. <em>PEERJCS</em>, <em>11</em>, e2921. (<a href='https://doi.org/10.7717/peerj-cs.2921'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand gesture recognition is an approach to comprehending human body language, applied in various fields such as human-computer interaction. However, some issues remain in edge blurring generated by complex backgrounds, rotation inaccuracy induced by fast movement, and delay caused by computing cost. Recently, the emergence of deep learning has ameliorated these issues, convolution neural network (CNN) enhanced edge clarity, long-short term memory (LSTM) improved rotation accuracy, and attention mechanism optimized response time. In this context, this review starts with the deep learning models, specifically CNN, LSTM, and attention mechanisms, which are compared and discussed from the utilization rate of each, their contribution to improving accuracy or efficiency, and their role in the recognition stage, like feature extraction. Furthermore, to evaluate the performance of these deep learning models, the evaluation metrics, datasets, and ablation studies are analyzed and discussed. The choice of evaluation metrics and dataset is critical since different tasks require different evaluation parameters, and the model learns more patterns and features from diverse data. Therefore, the evaluation metrics are categorized into accuracy and efficiency. The datasets are analyzed from self-created to public datasets. The ablation study is summarized in four aspects: similar underlying models, integrating specific models, pre-processing, others. Finally, the existing research gaps and further research on accuracy, efficiency, application range, and environmental adaptation are discussed.},
  archive      = {J_PEERJCS},
  author       = {Cui Cui and Mohd Shahrizal Sunar and Goh Eg Su},
  doi          = {10.7717/peerj-cs.2921},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2921},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep vision-based real-time hand gesture recognition: A review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel brain tumor magnetic resonance imaging dataset (Gazi brains 2020): Initial benchmark results and comprehensive analysis. <em>PEERJCS</em>, <em>11</em>, e2920. (<a href='https://doi.org/10.7717/peerj-cs.2920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new benchmark MRI dataset called the Gazi Brains Dataset 2020, containing MRI images of 100 patients, and introduces initial experimental results performed on this dataset in comparison with available brain MRI datasets. Furthermore, the dataset is analyzed using eight different deep learning models for high-grade glioma tumor prediction, classification, and detection tasks. Additionally, this study demonstrates the results of an explainable Artificial Intelligence (XAI) approach applied to the trained models. To demonstrate the utility of the proposed dataset, different deep learning models were applied to the problem, and these models were tested on various data and models applied for various tasks such as region of interest extraction, whole tumor segmentation, prediction, detection, and classification with accuracy, precision, recall, and F1-score. The experimental results indicate that the dataset is highly effective for multiple purposes, and the models reached significant results with successful F1-scores ranging between 93.2% and 96.4%. ROI and whole tumor segmentations were successfully performed and compared with seven algorithms with accuracies of 87.61% and 97.18%. The Grad-CAM model also demonstrated satisfactory accuracy across the tests that were conducted. Moreover, this study explores the application of XAI to the trained models, providing interpretability and insights into the decision-making processes. The findings signify that this dataset holds significant potential for various future research directions, including age estimation, gender detection, causal inference with XAI, and disease-related survival analysis.},
  archive      = {J_PEERJCS},
  author       = {Seref Sagiroglu and Ramazan Terzi and Emrah Celtikci and Alp Özgün Börcek and Yilmaz Atay and Bilgehan Arslan and Mustafa Caglar Sahin and Kerem Nernekli and Umut Demirezen and Okan Bilge Ozdemir and Kevser Özdem Karaca and Nuh Azgınoğlu},
  doi          = {10.7717/peerj-cs.2920},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2920},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel brain tumor magnetic resonance imaging dataset (Gazi brains 2020): Initial benchmark results and comprehensive analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sporting a virtual future: Exploring sports and virtual reality patents using deep learning-based analysis. <em>PEERJCS</em>, <em>11</em>, e2919. (<a href='https://doi.org/10.7717/peerj-cs.2919'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the convergence of sports and emerging technologies from the Fourth Industrial Revolution, with a focus on virtual reality (VR) applications. Using patent big data, we introduce SportsBERT, a bidirectional encoder representation from transformers (BERT)-based algorithm tailored for enhanced natural language processing in sports-related knowledge-based documents. Through topic modeling, we extract key themes and clusters from sports-related VR patents, providing insights into the knowledge structure and technological trends in VR applications for sports. Our analysis identifies key drivers of technological advancement, including spatial hardware, tactile human–computer interactions, aerobic exercise, rehabilitation, and swing sports. Additionally, we highlight challenges such as the high cost and usability limitations of current VR devices. This study presents the first deep learning-based topic modeling approach specialized for sports patents and offers a comprehensive roadmap for current developments and future trajectories in VR sports technologies.},
  archive      = {J_PEERJCS},
  author       = {Jea Woog Lee and Sangmin Song and JungMin Yun and Doug Hyun Han and YoungBin Kim},
  doi          = {10.7717/peerj-cs.2919},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2919},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Sporting a virtual future: Exploring sports and virtual reality patents using deep learning-based analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aerial image segmentation of embankment dams based on multispectral remote sensing: A case study in the belo monte hydroelectric complex, pará, brazil. <em>PEERJCS</em>, <em>11</em>, e2917. (<a href='https://doi.org/10.7717/peerj-cs.2917'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual inspection is essential to ensure the stability of earth-rock dams. Periodic visual assessment of this type of structure through vegetation cover analysis is an effective monitoring method. Recently, multispectral remote sensing data and machine learning techniques have been applied to develop methodologies that enable automatic vegetation analysis and anomaly detection based on computer vision. As a first step toward this automation, this study introduces a methodology for land cover segmentation of earth-rock embankment dam structures within the Belo Monte Hydroelectric Complex, located in the state of Pará, northern Brazil. Random forest (RF) ensemble models were trained on manually annotated data captured by a multispectral sensor embedded in an uncrewed aerial vehicle (UAV). The main objectives of this study are to assess the classification performance of the algorithm in segmenting earth-rock dams and the contribution of non-visible band reflectance data to the overall model performance. A comprehensive feature engineering and ranking approach is presented to select the most descriptive features that represent the four dataset classes. Model performance was assessed using classical performance metrics derived from the confusion matrix, such as accuracy, Kappa coefficient, precision, recall, F1-score, and intersection over union (IoU). The final RF model achieved 90.9% mean IoU for binary segmentation and 91.1% mean IoU for multiclass segmentation. Post-processing techniques were applied to refine the predicted masks, enhancing the mean IoU to 93.2% and 91.9%, respectively. The flexible methodology presented in this work can be applied to different scenarios when treated as a framework for pixel-wise land cover classification, serving as a crucial step toward automating visual inspection processes. The implementation of automated monitoring solutions improves the visual inspection process and mitigates the catastrophic consequences resulting from dam failures.},
  archive      = {J_PEERJCS},
  author       = {Carlos André de Mattos Teixeira and Thabatta Moreira Alves de Araujo and Evelin Cardoso and Marcos Antonio Costantin Filho and João Weyl Costa and Carlos Renato Lisboa Frances},
  doi          = {10.7717/peerj-cs.2917},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2917},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Aerial image segmentation of embankment dams based on multispectral remote sensing: A case study in the belo monte hydroelectric complex, pará, brazil},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated guided vehicle (AGV) path optimization method based on improved rapidly-exploring random trees. <em>PEERJCS</em>, <em>11</em>, e2915. (<a href='https://doi.org/10.7717/peerj-cs.2915'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In response to the issues of low computational efficiency, slow convergence speed, curvy paths, and the tendency to fall into local optima in rapidly-exploring random tree (RRT) algorithms for automated guided vehicle (AGV) path planning, this article proposes an improved RRT algorithm that combines adaptive step-size optimization with K-dimensional tree (KD-Tree) based fast nearest neighbor search. Firstly, an adaptive step-size optimization strategy is introduced to dynamically adjust the step size during node searches, improving both the planning quality and computational efficiency of the algorithm. Secondly, the KD-Tree nearest neighbor search method is employed to accelerate node searching and reduce the time cost of path planning. Finally, a cubic spline interpolation function is applied to smooth the optimal path, further enhancing the planning quality. Experimental results show that the improved RRT algorithm significantly outperforms traditional RRT, RRT*, and Informed-RRT* in terms of path length, planning time, and path smoothness. Specifically, the average path length is reduced by 164.33 m, and the average search time is shortened by 3.3 s, making it more suitable for AGV path planning in practical applications.},
  archive      = {J_PEERJCS},
  author       = {Zhigang Ren and Anjiang Cai and Feilong Xu},
  doi          = {10.7717/peerj-cs.2915},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2915},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automated guided vehicle (AGV) path optimization method based on improved rapidly-exploring random trees},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Siamese meta-learning network for social disputes based on multi-head attention. <em>PEERJCS</em>, <em>11</em>, e2910. (<a href='https://doi.org/10.7717/peerj-cs.2910'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning has been widely used in scenarios where labeled data is scarce, where meta-learning based few-shot classification is widely used, such as the Siamese network. Although the Siamese network has achieved good results in some applications, there are still some problems: (1) When computing prototype vectors with external knowledge of class labels, it depends on the quality and correctness of class labels. (2) When processing data, the Siamese network is not sufficient to capture dependencies between long distance. (3) When the data is complex or the samples are unbalanced, the Siamese network does not achieve the best performance. Therefore, this article proposes a multi-head attention siamese meta-learning network (MASM). Specifically, this article uses synonym substitution to solve the problem that the computation of prototype vectors will be transitionally dependent on class label. In addition, we use the multi-head attention mechanism to capture long-distance dependence by exploiting its global perception capability, which further improves the model performance. We conducted experiments on four benchmark datasets, all of which achieved good performance, and also applied the model for the first time in the field of social disputes, and experimented on a homemade private dispute dataset, which also achieved good results.},
  archive      = {J_PEERJCS},
  author       = {Jing Wang and Rui Zhang and Huijian Han and Yuxiang Liu and Zhaoxing Peng},
  doi          = {10.7717/peerj-cs.2910},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2910},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Siamese meta-learning network for social disputes based on multi-head attention},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdvFaceGAN: A face dual-identity impersonation attack method based on generative adversarial networks. <em>PEERJCS</em>, <em>11</em>, e2904. (<a href='https://doi.org/10.7717/peerj-cs.2904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article aims to reveal security vulnerabilities in current commercial facial recognition systems and promote advancements in facial recognition technology security. Previous research on both digital-domain and physical-domain attacks has lacked consideration of real-world attack scenarios: Digital-domain attacks with good stealthiness often fail to achieve physical implementation, while wearable-based physical-domain attacks typically appear unnatural and cannot evade human visual inspection. We propose AdvFaceGAN, a generative adversarial network (GAN)-based impersonation attack method that generates dual-identity adversarial faces capable of bypassing defenses and being uploaded to facial recognition system databases in our proposed attack scenario, thereby achieving dual-identity impersonation attacks. To enhance visual quality, AdvFaceGAN introduces a structural similarity loss in addition to conventional generative loss and perturbation loss, optimizing the generation pattern of adversarial perturbations. Under the combined effect of these three losses, our method produces adversarial faces with excellent stealthiness that can pass administrator’s human review. To improve attack effectiveness, AdvFaceGAN employs an ensemble of facial recognition models with maximum model diversity to calculate identity loss, thereby enhancing similarity to target identities. Innovatively, we incorporate source identity loss into the identity loss calculation, discovering that minor reductions in target identity similarity can be traded for significant improvements in source identity similarity, thus making the adversarial faces generated by our method highly similar to both the source identity and the target identity, addressing limitations in existing impersonation attack methods. Experimental results demonstrate that in black-box attack scenarios, AdvFaceGAN-generated adversarial faces exhibit better stealthiness and stronger transferability compared to existing methods, achieving superior traditional and dual-identity impersonation attack success rates across multiple black-box facial recognition models and three commercial facial recognition application programming interfaces (APIs).},
  archive      = {J_PEERJCS},
  author       = {Hong Huang and Yang Yang and Yunfei Wang},
  doi          = {10.7717/peerj-cs.2904},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2904},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {AdvFaceGAN: A face dual-identity impersonation attack method based on generative adversarial networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent reflecting surface backscatter-enabled physical layer security enhancement via deep reinforcement learning. <em>PEERJCS</em>, <em>11</em>, e2902. (<a href='https://doi.org/10.7717/peerj-cs.2902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a novel strategy for wireless communication security utilizing intelligent reflecting surfaces (IRS). The IRS is strategically deployed to mitigate jamming attacks and eavesdropper threats while improving signal reception for legitimate users (LUs) by redirecting jamming signals toward desired communication signals leveraging physical layer security (PLS). By integrating the IRS into the backscatter communication system, we enhance the overall secrecy rate of LU, by dynamically adjusting IRS reflection coefficients and active beamforming at the base station (BS). A design problem is formulated to jointly optimize IRS reflecting beamforming and BS active beamforming, considering time-varying channel conditions and desired secrecy rate requirements. We propose a novel approach based on deep reinforcement learning (DRL) named Deep-PLS. This approach aims to determine an optimal beamforming policy capable of thwarting eavesdroppers in evolving environmental conditions. Extensive simulation studies validate the efficacy of our proposed strategy, demonstrating superior performance compared to traditional IRS approaches, IRS backscattering-based anti-eavesdropping methods, and other benchmark strategies in terms of secrecy performance.},
  archive      = {J_PEERJCS},
  author       = {Manzoor Ahmed and Touseef Hussain and Muhammad Shahwar and Feroz Khan and Muhammad Sheraz and Wali Ullah Khan and Teong Chee Chuah and It Ee Lee},
  doi          = {10.7717/peerj-cs.2902},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2902},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Intelligent reflecting surface backscatter-enabled physical layer security enhancement via deep reinforcement learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-based real-time enhancement for disease prediction using confluent cloud, apache kafka, feature optimization, and explainable artificial intelligence. <em>PEERJCS</em>, <em>11</em>, e2899. (<a href='https://doi.org/10.7717/peerj-cs.2899'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Internet of Things (IoT)-based technologies have advanced healthcare by facilitating the development of monitoring systems, subsequently generating an exponential amount of streaming data. This streaming data can be preprocessed and analyzed using technologies that integrate ensemble models, Explainable Artificial Intelligence (XAI), feature selection (FS) method and big data streaming processing platforms to develop predictive real-time systems. This integration adds new value to healthcare that helps organizations enhance clinical decision-making, improve patient care, and elevate the overall quality of healthcare. This article presents a real-time system for the early detection and treatment of chronic kidney disease (CKD) using a real-world simulation application. The real-time system is developed in two phases. The first phase aims to propose a stacking model, apply a genetic algorithm (GA) and Particle swarm optimization (PSO) as feature selection, and explore a stacking model with the best features with explainable artificial intelligence (XAI). The best model with the best-optimized features is used to develop the second phase. The results showed that stacking model with GA is achieved the hightest performance with 100 accuracy, 100 precision, 100 recall, and 100 F1-score. The second phase is designed based on Confluent Cloud, which offers several benefits for creating a real-time streaming system based on Apache Kafka, providing multiple APIs—the Producer API and Consumer API—for data producers and consumers, respectively. Python scripts are developed to pipeline streaming data. The first Python script to generate streaming health attributes that are pushed into a Kafka topic. A second Python script to consume health attributes from a Kafka topic and apply a stacking model to predict CKD in real-time. The results showed that the stacking model with features selected by GA recorded the best performance with 100 accuracy. The pipeline’s streaming steps have validated our approach’s effectiveness in real-time, leveraging Confluent Cloud and Apache Kafka.},
  archive      = {J_PEERJCS},
  author       = {Abdulaziz AlMohimeed},
  doi          = {10.7717/peerj-cs.2899},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2899},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Cloud-based real-time enhancement for disease prediction using confluent cloud, apache kafka, feature optimization, and explainable artificial intelligence},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel conditional tabular generative adversarial network based image augmentation for railway track fault detection. <em>PEERJCS</em>, <em>11</em>, e2898. (<a href='https://doi.org/10.7717/peerj-cs.2898'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Railway track fault recognition is a critical aspect of railway maintenance, aiming to identify and rectify defects such as cracks, misalignments, and wear on tracks to ensure safe and efficient train operations. Classical methods for fault detection, including manual inspections and simple sensor-based systems, face significant challenges, such as high labour costs, human error, and limited detection accuracy under varying environmental conditions. These methods are often time-consuming and unable to provide real-time monitoring, leading to potential safety risks and operational inefficiencies. To address these challenges, efficient artificial intelligence-based image classification is being explored to enhance railway track fault detection accuracy, efficiency, and reliability. This research aims to develop an advanced generative neural network for efficient railway track fault detection. We propose a novel conditional tabular generative adversarial network (CTGAN)-based image augmentation approach to producing realistic synthetic image data using railway track images. We developed five advanced neural network techniques for comparison with railway track image classification. The random forest approach surpasses state-of-the-art studies with a high accuracy score of 0.99 for railway track fault detection. Hyperparameter optimization is applied to achieve optimal performance, and the performance is evaluated using the k-fold cross-validation approach. The proposed research enhances operational efficiency, reduces maintenance costs, and significantly improves the safety and reliability of rail transportation.},
  archive      = {J_PEERJCS},
  author       = {Ali Raza and Rukhshanda Sehar and Abdul Moiz and Ala Saleh Alluhaidan and Sahar A. El-Rahman and Diaa Salama AbdElminaam},
  doi          = {10.7717/peerj-cs.2898},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2898},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Novel conditional tabular generative adversarial network based image augmentation for railway track fault detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of an improved graph-based model integrating LSTM, LoRaWAN, and blockchain for smart agriculture. <em>PEERJCS</em>, <em>11</em>, e2896. (<a href='https://doi.org/10.7717/peerj-cs.2896'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research is anchored on the burning need for irrigation optimization and crop water use efficiency improvement, which remains a challenge in smart agriculture processes. Traditional irrigation methods normally lead to inefficiency, resulting in wasted water and non-maximum crops. These traditional ways normally lack attributes of real-time adaptability and secure data management—things that are very key to modernizing agricultural practices. In this work, artificial intelligence (AI), Internet of Things (IoT), and blockchain techniques will be integrated to design a comprehensive system for monitoring and predicting soil moisture levels. In the proposed model, long short-term memory (LSTM) networks are considered for soil moisture level prediction, taking into consideration past data, weather, and crop type. LSTM networks are chosen here for their high performance in timestamp series prediction tasks with an mean average error (MAE) of 0.02 m3/m3 over a 7-day forecast horizon. For real-time monitoring, IoT sensors based on long range wide area network (LoRaWAN) technology are field-deployed for conducting long-range communications while consuming very limited energy to extend the sensor battery life over 5 years and bring down the data transmission latency below 5 s. It has an inbuilt permissioned blockchain framework—Hyperledger Fabric—which offers a secure and transparent system for data management and maintaining a record of soil moisture data, irrigation events, and metadata from sensors. This ensures the immutability and integrity of sets of data. Smart contracts automate irrigation upon reaching preconfigured soil moisture thresholds, and hence zero data integrity breaches occur with a transaction throughput of 1,000 transactions per second, taken into view with smart contract execution latency of less than 2 s. Moreover, it utilizes reinforcement learning with Deep Q-Learning to derive an optimized irrigation schedule. In this regard, it enables learning optimal irrigation policies and implements them to improve efficiency in the usage of water by 25% and increases crop yield by 15% compared to the traditional methods. Clearly from field trials, results indicate evident efficiency of the integrated system: a 20% water usage reduction and a 12% increase in crop yield within one growing season. This is rather an innovative take on irrigation practices, increasing a great deal of accuracy and sustainability for such and providing a really strong solution toward better agricultural productivity and resource management.},
  archive      = {J_PEERJCS},
  author       = {Ravi Kumar Munaganuri and Narasimha Rao Yamarthi and Sai Chandana Bolem},
  doi          = {10.7717/peerj-cs.2896},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2896},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of an improved graph-based model integrating LSTM, LoRaWAN, and blockchain for smart agriculture},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recent advances in the inverse design of silicon photonic devices and related platforms using deep generative models. <em>PEERJCS</em>, <em>11</em>, e2895. (<a href='https://doi.org/10.7717/peerj-cs.2895'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents an overview of recent research on the inverse design of optical devices using deep generative models. The increasing complexity of modern optical devices necessitates advanced design methodologies that can efficiently navigate vast parameter spaces and generate novel, high-performance structures. Established optimization methods, such as adjoint and topology optimization, have successfully addressed many design challenges. However, the increasing complexity of modern optical devices creates opportunities for complementary approaches. Deep generative models offer additional capabilities by leveraging their ability to learn complex patterns and generate novel designs. This review examines various deep learning methodologies, including multi-layer perceptrons (MLP), convolutional neural networks (CNN), auto-encoders (AE), Generative Adversarial Networks (GAN), and reinforcement learning (RL) approaches. We analyze their applications in the inverse design of photonic devices, comparing their effectiveness and integration in the design process. Our findings indicate that while MLP-based methods were commonly used in early research, recent studies have increasingly employed CNN, GAN, AE, and RL methods, as well as advanced MLP models. Each of these methods offers unique advantages and presents specific challenges in the context of optical device inverse design. This review critically evaluates these deep learning-based inverse design technologies, highlighting their strengths and limitations in the context of optical device design. By synthesizing current research and identifying key trends, this article aims to guide future developments in the application of deep generative models for optical device inverse design.},
  archive      = {J_PEERJCS},
  author       = {Sun Jae Baek and Minhyeok Lee},
  doi          = {10.7717/peerj-cs.2895},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2895},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Recent advances in the inverse design of silicon photonic devices and related platforms using deep generative models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Majority clustering for imbalanced image classification. <em>PEERJCS</em>, <em>11</em>, e2891. (<a href='https://doi.org/10.7717/peerj-cs.2891'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class imbalance is a prevalent challenge in image classification tasks, where certain classes are significantly underrepresented compared to others. This imbalance often leads to biased models that perform poorly in predicting minority classes, affecting the overall performance and reliability of image classification systems. In this article, an under-sampling approach based on reducing the samples of majority class is used along with the unsupervised clustering approach for partitioning the majority class into clusters within the datasets. The proposed technique, Majority Clustering for Imbalanced Image Classification (MCIIC) improves the traditional binary classification problems by converting it into multi-class problem, thereby creating the more balanced classification solution to the problems where one need to detect rare samples present in the dataset. By utilizing the elbow method, we determine the optimal number of clusters for the majority class and assign each cluster a new class label. This complete process ensures a balanced and symmetrical class distribution, effectively addressing imbalances both between and within classes and helps to perform imbalanced classification. The effectiveness of the proposed model is evaluated on various benchmark datasets, demonstrating their ability to improve the predictive performance of the proposed MCIIC on imbalanced image datasets. Through empirical evaluation, we showcase the impact of proposed technique on model accuracy, precision, recall, and F1-score, highlighting its importance as a pre-processing step in handling imbalanced image datasets. The results highlight the significance of proposed model as a practical approach to address the challenges posed by imbalanced data distributions in machine learning tasks.},
  archive      = {J_PEERJCS},
  author       = {Keshav Sharma and Jyoti Arora and Pooja Kherwa and Zainab Alansari},
  doi          = {10.7717/peerj-cs.2891},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2891},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Majority clustering for imbalanced image classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging large language models for spelling correction in turkish. <em>PEERJCS</em>, <em>11</em>, e2889. (<a href='https://doi.org/10.7717/peerj-cs.2889'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of natural language processing (NLP) has rapidly progressed, particularly with the rise of large language models (LLMs), which enhance our understanding of the intrinsic structures of languages in a cross-linguistic manner for complex NLP tasks. However, commonly encountered misspellings in human-written texts adversely affect language understanding for LLMs for various NLP tasks as well as misspelling applications such as auto-proofreading and chatbots. Therefore, this study focuses on the task of spelling correction in the agglutinative language Turkish, where its nature makes spell correction significantly more challenging. To address this, the research introduces a novel dataset, referred to as NoisyWikiTr, to explore encoder-only models based on bidirectional encoder representations from transformers (BERT) and existing auto-correction tools. For the first time in this study, as far as is known, encoder-only models based on BERT are presented as subword prediction models, and encoder-decoder models based on text-cleaning (Text-to-Text Transfer Transformer) architecture are fine-tuned for this task in Turkish. A comprehensive comparison of these models highlights the advantages of context-based approaches over traditional, context-free auto-correction tools. The findings also reveal that among LLMs, a language-specific sequence-to-sequence model outperforms both cross-lingual sequence-to-sequence models and encoder-only models in handling realistic misspellings.},
  archive      = {J_PEERJCS},
  author       = {Ceren Guzel Turhan},
  doi          = {10.7717/peerj-cs.2889},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2889},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging large language models for spelling correction in turkish},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic dependent surveillance-broadcast (ADS-b) anomalous messages and attack type detection: Deep learning-based architecture. <em>PEERJCS</em>, <em>11</em>, e2886. (<a href='https://doi.org/10.7717/peerj-cs.2886'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Dependent Surveillance-Broadcast (ADS-B) is a vital communication protocol within air traffic control (ATC) systems. Unlike traditional technologies, ADS-B utilizes the Global Positioning System (GPS) to deliver more accurate and precise location data while reducing operational and deployment costs. It enhances radar coverage and serves as a standalone solution in areas lacking radar services. Despite these advantages, ADS-B faces significant security vulnerabilities due to its open design and the absence of built-in security features. Given its critical role, developing an advanced security framework to classify ADS-B messages and identify various attack types is essential to safeguard the system. This research makes several key contributions to address these challenges. First, it presents a comprehensive review of state-of-the-art machine learning and deep learning techniques, critically analyzing existing methodologies for ADS-B intrusion detection. Second, a detailed attack model is developed, categorizing potential threats and aligning them with key security requirements, including confidentiality, integrity, availability, and authentication. Third, the study proposes a robust and accurate Intrusion Detection System (IDS) using three advanced deep learning models—TabNet, Neural Oblivious Decision Ensembles (NODE), and DeepGBM—to classify ADS-B messages and detect specific attack types. The models are evaluated using standard metrics, including accuracy, precision, recall, and F1-score. Among the tested models, DeepGBM achieves the highest accuracy at 98%, outperforming TabNet (92%) and NODE (96%). The findings offer valuable insights into ADS-B security and define essential requirements for a future security framework, contributing actionable recommendations for mitigating threats in this critical communication protocol.},
  archive      = {J_PEERJCS},
  author       = {Waqas Ahmed and Ammar Masood and Jawad Manzoor and Sedat Akleylek},
  doi          = {10.7717/peerj-cs.2886},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2886},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automatic dependent surveillance-broadcast (ADS-b) anomalous messages and attack type detection: Deep learning-based architecture},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of multi-objective feature regression models for designing performance assessment methods in college and university educational reform. <em>PEERJCS</em>, <em>11</em>, e2883. (<a href='https://doi.org/10.7717/peerj-cs.2883'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evaluation of teacher performance in higher education is a critical component of educational reform, requiring robust and accurate assessment methodologies. Multi-objective regression offers a promising approach to optimizing the construction of performance evaluation index systems. However, conventional regression models often rely on a shared input space for all targets, neglecting the fact that distinct and complex feature sets may influence each target. This study introduces a novel Multi-Objective Feature Regression model under Label-Specific Features (MOFR-LSF), which integrates target-specific features and inter-target correlations to address this limitation. By extending the single-objective stacking framework, the proposed method learns label-specific features for each target and employs cluster analysis on binned samples to uncover underlying correlations among objectives. Experimental evaluations on three datasets—Education Reform (EDU-REFORM), Programme for International Student Assessment (PISA), and National Assessment of Educational Progress (NAEP)—demonstrate the superior performance of MOFR-LSF, achieving relative root mean square error (RRMSE) values of 0.634, 0.332, and 0.925, respectively, outperforming existing multi-objective regression algorithms. The proposed model not only enhances predictive accuracy but also strengthens the scientific validity and fairness of performance evaluations, offering meaningful contributions to educational reform in colleges and universities. Moreover, its adaptable framework suggests potential applicability across a range of other domains.},
  archive      = {J_PEERJCS},
  author       = {Fengjun Qi and Zhenping Liu and Wenzheng Zhang and Zhenjie Sun},
  doi          = {10.7717/peerj-cs.2883},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2883},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimization of multi-objective feature regression models for designing performance assessment methods in college and university educational reform},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced futures price-spread forecasting based on an attention-driven optimized LSTM network: Integrating an improved grey wolf optimizer algorithm for enhanced accuracy. <em>PEERJCS</em>, <em>11</em>, e2865. (<a href='https://doi.org/10.7717/peerj-cs.2865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial market prediction faces significant challenges due to the complex temporal dependencies and heterogeneous data relationships inherent in futures price-spread data. Traditional machine learning methods struggle to effectively mine these patterns, while conventional long short-term memory (LSTM) models lack focused feature prioritization and suffer from suboptimal hyperparameter selection. This article proposes the Improved Grey Wolf Optimizer with Multi-headed Self-attention and LSTM (IGML) model, which integrates a multi-head self-attention mechanism to enhance feature interaction and introduces an improved grey wolf optimizer (IGWO) with four strategic enhancements for automated hyperparameter tuning. Benchmark tests on optimization problems validate IGWO’s superior convergence efficiency. Evaluated on real futures price-spread datasets, the IGML reduces mean square error (RMSE) and mean absolute error (MAE) by up to 88% and 85%, respectively, compared to baseline models, demonstrating its practical efficacy in capturing intricate financial market dynamics.},
  archive      = {J_PEERJCS},
  author       = {Yongli Tang and Zhenlun Gao and Zhongqi Cai and Jinxia Yu and Panke Qin},
  doi          = {10.7717/peerj-cs.2865},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2865},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced futures price-spread forecasting based on an attention-driven optimized LSTM network: Integrating an improved grey wolf optimizer algorithm for enhanced accuracy},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of different initial solutions on the metaheuristic algorithms for the single allocation p-hub center and routing problem. <em>PEERJCS</em>, <em>11</em>, e2840. (<a href='https://doi.org/10.7717/peerj-cs.2840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces methods for initializing a single-trajectory-based metaheuristic, specifically a simulated annealing (SA) algorithm, using constructive heuristics. These methods are designed to target promising regions within the search space of an nondeterministic polynomial time (NP)-hard problem, namely the single allocation p-hub center and routing problem. The objective of this problem is to allocate demand centers to hubs and design vehicle routes such that the maximum distance between all origin-destination pairs is minimized. To analyze the impact of different initial solutions, various constructive heuristics, including greedy and hybrid strategies, have been proposed. Additionally, a problem decomposition approach leveraging domain-specific knowledge has been incorporated through a matheuristic initial solution strategy to enhance the efficiency of the simulated annealing algorithm. This approach generates high-quality initial solutions by first solving the p-hub center problem and then using the obtained hubs and their assignments as inputs to the min-max multiple traveling salesman problem. In this problem, the objective function is formulated differently from the literature by minimizing the longest distance between the two nodes. Several experiments have been conducted on the Turkish network, and upon examining the results, it has been observed that each initial solution generation strategy provides improvements in problem instances with specific characteristics, such as the number of vehicles and nodes. We also observed lower objective function values for all medium- and large-sized test problems taken from the literature, highlighting the effectiveness of the proposed strategies.},
  archive      = {J_PEERJCS},
  author       = {Abdul Kader Kassoumeh and Zühal Kartal and Ahmet Arslan},
  doi          = {10.7717/peerj-cs.2840},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2840},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The effect of different initial solutions on the metaheuristic algorithms for the single allocation p-hub center and routing problem},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating cyber-physical systems with embedding technology for controlling autonomous vehicle driving. <em>PEERJCS</em>, <em>11</em>, e2823. (<a href='https://doi.org/10.7717/peerj-cs.2823'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber-physical systems (CPSs) in autonomous vehicles must handle highly dynamic and uncertain settings, where unanticipated impediments, shifting traffic conditions, and environmental changes all provide substantial decision-making issues. Deep reinforcement learning (DRL) has emerged as a strong tool for dealing with such uncertainty, yet current DRL models struggle to ensure safety and optimal behaviour in indeterminate settings due to the difficulties of understanding dynamic reward systems. To address these constraints, this study incorporates double deep Q networks (DDQN) to improve the agent’s adaptability under uncertain driving conditions. A structured reward system is established to accommodate real-time fluctuations, resulting in safer and more efficient decision-making. The study acknowledges the technological limitations of automobile CPSs and investigates hardware acceleration as a potential remedy in addition to algorithmic enhancements. Because of their post-manufacturing adaptability, parallel processing capabilities, and reconfigurability, field programmable gate arrays (FPGAs) are used to execute reinforcement learning in real-time. Using essential parameters, including collision rate, behaviour similarity, travel distance, speed control, total rewards, and timesteps, the suggested method is thoroughly tested in the TORCS Racing Simulator. The findings show that combining FPGA-based hardware acceleration with DDQN successfully improves computational efficiency and decision-making reliability, tackling significant issues brought on by uncertainty in autonomous driving CPSs. In addition to advancing reinforcement learning applications in CPSs, this work opens up possibilities for future investigations into real-world generalisation, adaptive reward mechanisms, and scalable hardware implementations to further reduce uncertainty in autonomous systems.},
  archive      = {J_PEERJCS},
  author       = {Manal Abdullah Alohali and Hamed Alqahtani and Abdulbasit Darem and Monir Abdullah and Yunyoung Nam and Mohamed Abouhawwash},
  doi          = {10.7717/peerj-cs.2823},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2823},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Integrating cyber-physical systems with embedding technology for controlling autonomous vehicle driving},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing analogy-based software cost estimation using grey wolf optimization algorithm. <em>PEERJCS</em>, <em>11</em>, e2794. (<a href='https://doi.org/10.7717/peerj-cs.2794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate software cost estimation (SCE) is a critical factor in the successful delivery of software projects, as highlighted by industry statistics indicating that only some of the projects comply with the predicted budget. Among the software estimation methods, analogy-based estimation (ABE) is one of the most popular ones. Although this method has been customized in recent years with the help of optimization algorithms to achieve better results, the use of more powerful optimization algorithms can be effective in achieving better results in software size estimation. This study presents an innovative approach to SCE that integrates the grey wolf optimization (GWO) algorithm to enhance the precision of ABE. The GWO algorithm, inspired by the hunting behavior and social hierarchy of grey wolves, is mathematically modeled and incorporated into the ABE approach. The key focus of this research is the optimization of the similarity function, a crucial component of the ABE, using both Euclidean and Manhattan distance measures. The article addresses the challenges in selecting an optimal similarity function and emphasizes the importance of proper feature weighting to improve estimation accuracy. The proposed GWO-based ABE method is rigorously evaluated on multiple software project datasets using cross-validation techniques. The performance of the GWO-based ABE is compared to other evolutionary algorithms based on widely accepted evaluation metrics. The results confirm that the integration of the GWO algorithm into ABE enhances estimation accuracy and model robustness. By optimizing feature weights in the similarity function, GWO-ABE effectively addresses key limitations of traditional analogy-based methods. The proposed approach demonstrates superior performance across multiple datasets, particularly under the Euclidean distance function, making it a reliable solution for software project cost estimation. Experimental evaluations show that GWO-ABE achieves notable improvements in key performance metrics, leading to reduced mean magnitude of relative error (MMRE), median magnitude of relative error (MdMRE), and higher percentage of prediction (PRED) compared to other ABE-customized methods. These findings highlight the role of metaheuristic optimization in improving software estimation techniques, contributing to more precise and efficient project planning and management.},
  archive      = {J_PEERJCS},
  author       = {Taghi Javdani Gandomani and Maedeh Dashti and Sadegh Ansaripour and Hazura Zulzalil},
  doi          = {10.7717/peerj-cs.2794},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2794},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing analogy-based software cost estimation using grey wolf optimization algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Informed decision-making in prioritising product variants. <em>PEERJCS</em>, <em>11</em>, e2778. (<a href='https://doi.org/10.7717/peerj-cs.2778'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature models (FMs) play a crucial role in software product lines (SPLs) by representing variability and enabling the generation of diverse product configurations. However, the vast number of possible configurations often makes it challenging to identify the most suitable variant, especially when multiple criteria must be considered. Multi-criteria decision-making (MCDM) methods, such as analytic hierarchy process (AHP), technique for order of preference by similarity to ideal solution (TOPSIS), and VIseKriterijumska Optimizacija I Kompromisno Resenje (“multicriteria optimization and compromise solution”) (VIKOR), are effective for ranking configurations based on user-defined preferences. However, the application of disparate MCDM techniques to the same feature model with identical criteria can yield conflicting rankings, thereby complicating the decision-making process. To address this issue, we propose a novel framework that systematically integrates multiple MCDM methods to prioritise product configurations and provides informed decision support to reconcile ranking discrepancies. The framework automates the prioritisation process and offers a structured approach to explain differences between rankings, enhancing transparency and user confidence in the final selection. The framework’s effectiveness has been validated through real-world case studies, demonstrating its ability to streamline configuration prioritisation and support consistent, preference-driven decision-making in complex SPL environments.},
  archive      = {J_PEERJCS},
  author       = {Diana Borrego and Ángel Jesús Varela-Vaca and María Teresa Gómez-López and Rafael M. Gasca},
  doi          = {10.7717/peerj-cs.2778},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2778},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Informed decision-making in prioritising product variants},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intelligent reflective surfaces in 5G and beyond: Optimizing uplink satellite connectivity for IoT. <em>PEERJCS</em>, <em>11</em>, e2726. (<a href='https://doi.org/10.7717/peerj-cs.2726'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolving landscape of communication technologies, the integration of intelligent reflective surfaces (IRS) into uplink satellite communication for Internet of Things (IoT) ecosystems presents a promising solution to overcome traditional communication challenges. The purpose of this study is to explore the impact of IRS on enhancing signal quality and communication efficiency in satellite-supported IoT environments. This article adopts a simulation-based approach, using MATLAB and Simulink to model the uplink transmission of IoT devices to satellites with and without IRS assistance. The methodology focuses on analysing key performance metrics, including signal-to-noise ratio (SNR), spectral efficiency, signal strength, and interference mitigation. A reinforcement learning algorithm was employed to optimise IRS phase shifts and beamforming to maximise communication performance. The findings reveal that the integration of IRS leads to significant improvements in SNR, spectral efficiency, and overall signal quality, with a 2 dB increase in SNR and enhanced data transmission rates compared to non-IRS systems. IRS also mitigates interference and extends the coverage area of satellite networks. These results demonstrate the practical implications of IRS technology, which can be applied in scenarios such as smart cities, remote sensing, and disaster recovery, where reliable satellite communication is crucial. The study highlights the strategic importance of IRS in revolutionising IoT-satellite communication systems and sets the foundation for future work on scaling IRS technology for broader applications.},
  archive      = {J_PEERJCS},
  author       = {Callistus Odinaka Obidiozor and Adeeb Sait and Tawfik Al-Hadhrami and Eman H. Alkhammash and Faisal Saeed},
  doi          = {10.7717/peerj-cs.2726},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2726},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Intelligent reflective surfaces in 5G and beyond: Optimizing uplink satellite connectivity for IoT},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anime popularity prediction before huge investments: A multimodal approach using deep learning. <em>PEERJCS</em>, <em>11</em>, e2715. (<a href='https://doi.org/10.7717/peerj-cs.2715'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Japanese anime industry, predicting whether an upcoming product will be popular is crucial. This article introduces one of the most comprehensive free datasets for predicting anime popularity using only features accessible before huge investments, relying solely on freely available internet data and adhering to rigorous standards based on real-life experiences. To explore this dataset and its potential, a deep neural network architecture incorporating GPT-2 and ResNet-50 is proposed. The model achieved a best mean squared error (MSE) of 0.012, significantly surpassing a benchmark with traditional methods of 0.415, and a best R-square (R2) score of 0.142, outperforming the benchmark of −37.591. The aim of this study is to explore the scope and impact of features available before huge investments in relation to anime popularity. For that reason, and complementing the MSE and R2 metrics, Pearson and Spearman correlation coefficients are used. The best results, with Pearson at 0.382 and Spearman at 0.362, along with a well-fitted learning curves, suggests that while these features are relevant, they are not decisive for determining anime popularity and they likely interacts with additional features accessible after further investments. This is one of the first multimodal approaches to address this kind of tasks, aiming to support an entertainment industry by helping to avoid financial failures and guide successful production strategies.},
  archive      = {J_PEERJCS},
  author       = {Jesús Armenta-Segura and Grigori Sidorov},
  doi          = {10.7717/peerj-cs.2715},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2715},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Anime popularity prediction before huge investments: A multimodal approach using deep learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Iterative segmentation and classification for enhanced crop disease diagnosis using optimized hybrid U-nets model. <em>PEERJCS</em>, <em>11</em>, e2543. (<a href='https://doi.org/10.7717/peerj-cs.2543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The major challenges that the agricultural sector faces are that with the kind of methodologies that exist, gross limitations may occur to the exact diagnosis of crop diseases. They are unable to achieve correct precision in disease classification, relatively lower accuracy, and delayed response time—all these obstacles result in a deficiency in effectual disease management and control. Our research proposes a new framework instigated and developed to improve crop disease detection and classification by multifaceted analysis. In the core of our methodology is the implementation of adaptive anisotropic diffusion for the denoising of obtained agro images, therefore making it a step towards assurance in data quality. Along with this is the use of a Fuzzy U-Net++ model for image segmentation, whereby fuzzy decisions in generously instill an increase in performance for image segmentation. Feature selection itself is innovated by the introduction of the Moving Gorilla Remora Algorithm (MGRA) combined with convolutional operations, setting a new benchmark in the selection of optimal features pertaining to disease identification operations. To further refine this model, classification is adeptly handled by a process inspired by the LeNet architecture, significantly improving identification against various diseases. Our approach’s performance is therefore strongly assessed through a number of renowned datasets, such as PlantVillage and PlantDoc, on which test metrics show superior performance: 8.5% improvement in disease classification precision, 8.3% higher accuracy, 9.4% improved recall, with a reduction in time delay by 4.5%, area under the curve (AUC) increasing by 5.9%, a 6.5% improvement in specificity, far ahead of other methods. This work not only sets new standards in crop disease analysis but also opens possibilities for the preemptive measures to come in agricultural health, promising a future where crop management is more effective and efficient. Our results thus have implications that reach beyond the immediate benefits accruable from improved diagnosis of diseases. It is a harbinger of a new era in agricultural technology where precision, accuracy, and timeliness will meet to enhance crop resilience and yield.},
  archive      = {J_PEERJCS},
  author       = {Malathi Chilakalapudi and Sheela Jayachandran},
  doi          = {10.7717/peerj-cs.2543},
  journal      = {PeerJ Computer Science},
  month        = {6},
  pages        = {e2543},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Iterative segmentation and classification for enhanced crop disease diagnosis using optimized hybrid U-nets model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic detection of teacher behavior in classroom videos using AlphaPose and faster R-CNN algorithms. <em>PEERJCS</em>, <em>11</em>, e2933. (<a href='https://doi.org/10.7717/peerj-cs.2933'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an automated classification framework for evaluating teacher behavior in classroom settings by integrating AlphaPose and Faster region-based convolutional neural networks (R-CNN) algorithms. The method begins by applying AlphaPose to classroom video footage to extract detailed skeletal pose information of both teachers and students across individual frames. These pose-based features are subsequently processed by a Faster R-CNN model, which classifies teacher behavior into appropriate or inappropriate categories. The approach is validated on the Classroom Behavior (PCB) dataset, comprising 74 video clips and 51,800 annotated frames. Experimental results indicate that the proposed system achieves an accuracy of 74.89% in identifying inappropriate behaviors while also reducing manual behavior logging time by 47% and contributing to a 63% decrease in such behaviors. The findings highlight the potential of computer vision techniques for scalable, objective, and real-time classroom behavior analysis, offering a viable tool for enhancing educational quality and teacher performance monitoring.},
  archive      = {J_PEERJCS},
  author       = {Jing Huang and Harwati Hashim and Helmi Norman and Mohammad Hafiz Zaini and Xiaojun Zhang},
  doi          = {10.7717/peerj-cs.2933},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2933},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automatic detection of teacher behavior in classroom videos using AlphaPose and faster R-CNN algorithms},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The application of blockchain technology in data trading: A systematic review. <em>PEERJCS</em>, <em>11</em>, e2925. (<a href='https://doi.org/10.7717/peerj-cs.2925'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of exponential data growth, the imperative to establish secure and efficient data trading mechanisms has become paramount. While traditional centralized architectures present critical limitations in security and operational efficiency, the emergence of blockchain technology offers transformative potential for decentralized solutions. This study conducts a systematic literature review to critically examine blockchain’s evolving role in data trading ecosystems. Adhering to the PRISMA 2020 framework, we analyzed 18 rigorously selected studies from an initial pool of 164 Web of Science publications identified through “data trading” and “blockchain” keyword searches. Our analysis reveals three principal findings: first, current blockchain implementations predominantly cluster within computer science applications, indicating disciplinary concentration. Second, technical development emphasizes solution-oriented systems over theoretical model construction, suggesting an application-prioritized research paradigm. Third, we identify persistent challenges across three critical dimensions: (i) security-efficiency paradox in decentralized architectures, (ii) transparency-privacy equilibrium maintenance, and (iii) scalability constraints under high-concurrency scenarios. This study aims to offer in-depth insights into blockchain’s potential applications in data trading and future research directions.},
  archive      = {J_PEERJCS},
  author       = {Wei Xiong and Huaibin Shao and Hong Ge},
  doi          = {10.7717/peerj-cs.2925},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2925},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The application of blockchain technology in data trading: A systematic review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning model for gastrointestinal polyp segmentation. <em>PEERJCS</em>, <em>11</em>, e2924. (<a href='https://doi.org/10.7717/peerj-cs.2924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the biggest hazards to cancer-related mortality globally is colorectal cancer, and improved patient outcomes are greatly influenced by early identification. Colonoscopy is a highly effective screening method, yet segmentation and detection remain challenging aspects due to the heterogeneity and variability of readers’ interpretations of polyps. In this work, we introduce a novel deep learning architecture for gastrointestinal polyp segmentation in the Kvasir-SEG dataset. Our method employs an encoder-decoder structure with a pre-trained ConvNeXt model as the encoder to learn multi-scale feature representations. The feature maps are passed through a ConvNeXt Block and then through a decoder network consisting of three decoder blocks. Our key contribution is the employment of a cross-attention mechanism that creates shortcut connections between the decoder and encoder to maximize feature retention and reduce information loss. In addition, we introduce a Residual Transformer Block in the decoder that learns long-term dependency by using self-attention mechanisms and enhance feature representations. We evaluate our model on the Kvasir-SEG dataset, achieving a Dice coefficient of 0.8715 and mean intersection over union (mIoU) of 0.8021. Our methodology demonstrates state-of-the-art performance in gastrointestinal polyp segmentation and its feasibility of being used as part of clinical pipelines to assist with automated detection and diagnosis of polyps.},
  archive      = {J_PEERJCS},
  author       = {Zitong Wang and Zeyi Wang and Pengyu Sun},
  doi          = {10.7717/peerj-cs.2924},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2924},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning model for gastrointestinal polyp segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining convolutional neural network with transformer to improve YOLOv7 for gas plume detection and segmentation in multibeam water column images. <em>PEERJCS</em>, <em>11</em>, e2923. (<a href='https://doi.org/10.7717/peerj-cs.2923'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multibeam bathymetry has become an effective underwater target detection method by using echo signals to generate a high-resolution water column image (WCI). However, the gas plume in the image is often affected by the seafloor environment and exhibits sparse texture and changing motion, making traditional detection and segmentation methods more time-consuming and labor-intensive. The emergence of convolutional neural networks (CNNs) alleviates this problem, but the local feature extraction of the convolutional operations, while capturing detailed information well, cannot adapt to the elongated morphology of the gas plume target, limiting the improvement of the detection and segmentation accuracy. Inspired by the transformer’s ability to achieve global modeling through self-attention, we combine CNN with the transformer to improve the existing YOLOv7 (You Only Look Once version 7) model. First, we sequentially reduce the ELAN (Efficient Layer Aggregation Networks) structure in the backbone network and verify that using the enhanced feature extraction module only in the deep network is more effective in recognising the gas plume targets. Then, the C-BiFormer module is proposed, which can achieve effective collaboration between local feature extraction and global semantic modeling while reducing computing resources, and enhance the multi-scale feature extraction capability of the model. Finally, two different depths of networks are designed by stacking C-BiFormer modules with different numbers of layers. This improves the receptive field so that the model’s detection and segmentation accuracy achieve different levels of improvement. Experimental results show that the improved model is smaller in size and more accurate compared to the baseline.},
  archive      = {J_PEERJCS},
  author       = {Wenguang Chen and Xiao Wang and Junjie Chen and Jialong Sun and Guozhen Zha},
  doi          = {10.7717/peerj-cs.2923},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2923},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Combining convolutional neural network with transformer to improve YOLOv7 for gas plume detection and segmentation in multibeam water column images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BlockDroid: Detection of android malware from images using lightweight convolutional neural network models with ensemble learning and blockchain for mobile devices. <em>PEERJCS</em>, <em>11</em>, e2918. (<a href='https://doi.org/10.7717/peerj-cs.2918'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the increase in the volume and diversity of malware targeting Android systems, research on detecting this harmful software is steadily growing. Traditional malware detection studies require significant human intervention and resource consumption to analyze all malware files. Moreover, malware developers have developed polymorphism and code obfuscation techniques to evade traditional signature-based detection approaches used by antivirus companies. Consequently, traditional methods have become increasingly inadequate for malware detection. So far, many machine learning methods have been successfully applied to address the issue of malware detection. Recent efforts in this area have turned to deep learning methods. Because these methods can automatically extract meaningful features from data and efficiently learn complex relationships, they can achieve better performance in malware detection as well as in solving many other problems. This article presents BlockDroid, an approach that combines convolutional neural network (CNN) models, ensemble learning, and blockchain technology to increase the accuracy and efficiency of malware detection for mobile devices. By converting Android DEX files into image data, BlockDroid leverages the superior image analysis capabilities of CNN models to discern patterns indicative of malware. The CICMalDroid 2020 dataset, comprising 13,077 applications, was utilized to create a balanced dataset of 3,590 images, with an equal number of benign and malware instances. The proposed detection system was developed using lightweight models, including EfficientNetB0, MobileNetV2, and a custom model as CNN models. Experimental studies were conducted by applying both individual models and the proposed BlockDroid system to our dataset. The empirical results illustrate that BlockDroid surpasses the performance of the individual models, demonstrating a substantial accuracy rate of 97.38%. Uniquely, BlockDroid integrates blockchain technology to record the predictions made by the malware detection model, thereby eliminating the need for re-analysis of previously evaluated applications and ensuring more efficient resource utilization. Our approach offers a promising and innovative strategy for effective and efficient Android malware detection.},
  archive      = {J_PEERJCS},
  author       = {Emre Şafak and İbrahim Alper Doğru and Necaattin Barışçı and İsmail Atacak},
  doi          = {10.7717/peerj-cs.2918},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2918},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {BlockDroid: Detection of android malware from images using lightweight convolutional neural network models with ensemble learning and blockchain for mobile devices},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative performance of twelve machine learning models in predicting COVID-19 mortality risk in children: A population-based retrospective cohort study in brazil. <em>PEERJCS</em>, <em>11</em>, e2916. (<a href='https://doi.org/10.7717/peerj-cs.2916'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has catalyzed the application of advanced digital technologies such as artificial intelligence (AI) to predict mortality in adult patients. However, the development of machine learning (ML) models for predicting outcomes in children and adolescents with COVID-19 remains limited. This study aimed to evaluate the performance of multiple machine learning models in forecasting mortality among hospitalized pediatric COVID-19 patients. In this cohort study, we used the SIVEP-Gripe dataset, a public resource maintained by the Ministry of Health, to track severe acute respiratory syndrome (SARS) in Brazil. To create subsets for training and testing the machine learning (ML) models, we divided the primary dataset into three parts. Using these subsets, we developed and trained 12 ML algorithms to predict the outcomes. We assessed the performance of these models using various metrics such as accuracy, precision, sensitivity, recall, and area under the receiver operating characteristic curve (AUC). Among the 37 variables examined, 24 were found to be potential indicators of mortality, as determined by the chi-square test of independence. The Logistic Regression (LR) algorithm achieved the highest performance, with an accuracy of 92.5% and an AUC of 80.1%, on the optimized dataset. Gradient boosting classifier (GBC) and AdaBoost (ADA), closely followed the LR algorithm, producing similar results. Our study also revealed that baseline reduced oxygen saturation, presence of comorbidities, and older age were the most relevant factors in predicting mortality in children and adolescents hospitalized with SARS-CoV-2 infection. The use of ML models can be an asset in making clinical decisions and implementing evidence-based patient management strategies, which can enhance patient outcomes and overall quality of medical care. LR, GBC, and ADA models have demonstrated efficiency in accurately predicting mortality in COVID-19 pediatric patients.},
  archive      = {J_PEERJCS},
  author       = {Adriano Lages dos Santos and Maria Christina L. Oliveira and Enrico A. Colosimo and Robert H. Mak and Clara C. Pinhati and Stella C. Gallante and Hercílio Martelli-Júnior and Ana Cristina Simões e Silva and Eduardo A. Oliveira},
  doi          = {10.7717/peerj-cs.2916},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2916},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparative performance of twelve machine learning models in predicting COVID-19 mortality risk in children: A population-based retrospective cohort study in brazil},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing east-west interface security in heterogeneous SDN via blockchain. <em>PEERJCS</em>, <em>11</em>, e2914. (<a href='https://doi.org/10.7717/peerj-cs.2914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defined networking (SDN) increasingly integrates multiple controllers from diverse vendors to enhance network scalability, flexibility, and reliability. However, such heterogeneous deployments pose significant security threats, especially at the east-west interface which is connecting these controllers. Existing solutions are inadequate for ensuring robust protection across multi-vendor SDN environments as most of them are meant to a specific type of attacks, use centralized solution, or designed for homogeneous SDN environments. This study proposes a blockchain-based security framework to address existing security gaps within heterogeneous SDN environments. The framework establishes a decentralized, robust, and interoperable security layer for distributed SDN controllers. By utilizing the Ethereum blockchain with customized smart contract-based checks, the proposed approach enables mutual authentication among controllers, secures data exchange, and controls network access. The framework effectively mitigates common SDN threats such as distributed denial-of-service (DDoS), man-in-the-middle (MitM), false data injection, and unauthorized access. Experimental results highlight the practicality of the solution, achieving a stable throughput of approximately 20 transactions per second with an average authentication latency of 28–40 ms. These results demonstrate that the proposed framework not only enhances inter-controller communication security but also maintains the network performance, making it a reliable and scalable solution for real-world SDN deployments.},
  archive      = {J_PEERJCS},
  author       = {Hamad Alrashede and Fathy Eassa and Abdullah Marish Ali and Hosam Aljihani and Faisal Albalwy},
  doi          = {10.7717/peerj-cs.2914},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2914},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing east-west interface security in heterogeneous SDN via blockchain},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TARGE: Large language model-powered explainable hate speech detection. <em>PEERJCS</em>, <em>11</em>, e2911. (<a href='https://doi.org/10.7717/peerj-cs.2911'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of user-generated content on social networking sites has intensified the challenge of accurately and efficiently detecting inflammatory and discriminatory speech at scale. Traditional manual moderation methods are impractical due to the sheer volume and complexity of online discourse, necessitating automated solutions. However, existing deep learning models for hate speech detection typically function as black-box systems, providing binary classifications without interpretable insights into their decision-making processes. This opacity significantly limits their practical utility, particularly in nuanced content moderation tasks. To address this challenge, our research explores leveraging the advanced reasoning and knowledge integration capabilities of state-of-the-art language models, specifically Mistral-7B, to develop transparent hate speech detection systems. We introduce a novel framework wherein large language models (LLMs) generate explicit rationales by identifying and analyzing critical textual features indicative of hate speech. These rationales are subsequently integrated into specialized classifiers designed to perform explainable content moderation. We rigorously evaluate our methodology on multiple benchmark English-language social media datasets. Results demonstrate that incorporating LLM-generated explanations significantly enhances both the interpretability and accuracy of hate speech detection. This approach not only identifies problematic content effectively but also clearly articulates the analytical rationale behind each decision, fulfilling the critical demand for transparency in automated content moderation.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Haseeb Hashir and Memoona and Sung Won Kim},
  doi          = {10.7717/peerj-cs.2911},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2911},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TARGE: Large language model-powered explainable hate speech detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Native language identification from text using a fine-tuned GPT-2 model. <em>PEERJCS</em>, <em>11</em>, e2909. (<a href='https://doi.org/10.7717/peerj-cs.2909'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Native language identification (NLI) is a critical task in computational linguistics, supporting applications such as personalized language learning, forensic analysis, and machine translation. This study investigates the use of a fine-tuned GPT-2 model to enhance NLI accuracy. Using the NLI-PT dataset, we preprocess and fine-tune GPT-2 to classify the native language of learners based on their Portuguese-written texts. Our approach leverages deep learning techniques, including tokenization, embedding extraction, and multi-layer transformer-based classification. Experimental results show that our fine-tuned GPT-2 model significantly outperforms traditional machine learning methods (e.g., SVM, Random Forest) and other pre-trained language models (e.g., BERT, RoBERTa, BioBERT), achieving a weighted F1 score of 0.9419 and an accuracy of 94.65%. These results show that large transformer models work well for native language identification and can help guide future research in personalized language tools and artificial intelligence (AI)-based education.},
  archive      = {J_PEERJCS},
  author       = {Yuzhe Nie},
  doi          = {10.7717/peerj-cs.2909},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2909},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Native language identification from text using a fine-tuned GPT-2 model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proto-caps: Interpretable medical image classification using prototype learning and privileged information. <em>PEERJCS</em>, <em>11</em>, e2908. (<a href='https://doi.org/10.7717/peerj-cs.2908'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explainable artificial intelligence (xAI) is becoming increasingly important as the need for understanding the model’s reasoning grows when applying them in high-risk areas. This is especially crucial in the field of medicine, where decision support systems are utilised to make diagnoses or to determine appropriate therapies. Here it is essential to provide intuitive and comprehensive explanations to evaluate the system’s correctness. To meet this need, we have developed Proto-Caps, an intrinsically explainable model for image classification. It explains its decisions by providing visual prototypes that resemble specific appearance features. These characteristics are predefined by humans, which on the one hand makes them understandable and on the other hand leads to the model basing its decision on the same features as the human expert. On two public datasets, this method shows better performance compared to existing explainable approaches, despite the additive explainability modality through the visual prototypes. In addition to the performance evaluations, we conducted an analysis of truthfulness by examining the joint information between the target prediction and its explanation output. This was done in order to ensure that the explanation actually reasons the target classification. Through extensive hyperparameter studies, we also found optimal model settings, providing a starting point for further research. Our work emphasises the prospects of combining xAI approaches for greater explainability and demonstrates that incorporating explainability does not necessarily lead to a loss of performance.},
  archive      = {J_PEERJCS},
  author       = {Luisa Gallée and Catharina Silvia Lisson and Timo Ropinski and Meinrad Beer and Michael Götz},
  doi          = {10.7717/peerj-cs.2908},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2908},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Proto-caps: Interpretable medical image classification using prototype learning and privileged information},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Yoga pose recognition using dual structure convolutional neural network. <em>PEERJCS</em>, <em>11</em>, e2907. (<a href='https://doi.org/10.7717/peerj-cs.2907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular form of physical and mental exercise, the correct execution of yoga movements is crucial. With the development of deep learning technologies, automatic recognition of yoga postures has become popular. To recognize five different yoga postures, this article proposed a dual structure convolutional neural network with a feature fusion function, which consists of the convolutional neural network A (CNN A) and convolutional neural network B (CNN B). Among them, the structure CNN A observes different channels finding the global feature of yoga images, and the structure CNN B calculates the depth information in each pixel of the yoga images. Following that, the extracted global feature and local feature are fused by a feature fusion function of taking a matrix dot multiplication. Finally, the softmax layer accurately recognizes yoga postures based on the fused features. Experimental results show that the proposed model achieves 97.23% accuracy with 96.08% precision and defeats against the competitors in the recognition of yoga postures. Moreover, the feature fusion function is proved to be successful in terms of the recognition to yoga postures. We also find that the feature fusion with a matrix dot multiplication operation can significantly improve the recognition accuracy of yoga postures than that with a direct connection operation.},
  archive      = {J_PEERJCS},
  author       = {Xiang Meng and Zhaobing Liu},
  doi          = {10.7717/peerj-cs.2907},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2907},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Yoga pose recognition using dual structure convolutional neural network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive regularized spectral reduction for stabilizing ill-conditioned bone-conducted speech signals. <em>PEERJCS</em>, <em>11</em>, e2906. (<a href='https://doi.org/10.7717/peerj-cs.2906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bone-conducted (BC) speech signals are inherently challenging to analyze due to their wide frequency range, which leads to ill-conditioning in numerical analysis and linear prediction (LP) techniques. This ill-conditioning is primarily caused by the expansion of eigenvalues, which complicates the stability and accuracy of traditional methods. To address this issue, we propose a novel regularized spectral reduction (RSR) method, built upon the regularized least squares (RLS) framework. The RSR method compresses the frequency range of BC speech signals, effectively reducing eigenvalue spread and enhancing the robustness of LP analysis. Key to the RSR approach is a regularization parameter, fine-tuned iteratively to achieve optimal performance. Experimental results demonstrate that RSR significantly outperforms existing techniques in eigenvalue compression, resulting in more accurate LP analysis for both synthetic and real BC speech datasets. These improvements hold promise for applications in hearing aids, voice recognition systems, and speaker identification in noisy environments, where reliable BC speech analysis is critical.},
  archive      = {J_PEERJCS},
  author       = {Kanwar Muhammad Afaq and Ammar Amjad and Li-Chia Tai and Hsien-Tsung Chang},
  doi          = {10.7717/peerj-cs.2906},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2906},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Adaptive regularized spectral reduction for stabilizing ill-conditioned bone-conducted speech signals},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient deep learning model for classifying lung cancer images using normalized stain agnostic feature method and FastAI-2. <em>PEERJCS</em>, <em>11</em>, e2903. (<a href='https://doi.org/10.7717/peerj-cs.2903'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Lung cancer has the highest global fatality rate, with diagnosis primarily relying on histological tissue sample analysis. Accurate classification is critical for treatment planning and patient outcomes. Methods This study develops a computer-assisted diagnosis system for non-small cell lung cancer histology classification, utilizing the FastAI-2 framework with a modified ResNet-34 architecture. The methodology includes stain normalization using LAB colour space for colour consistency, followed by deep learning-based classification. The proposed model is trained on the LC25000 dataset and compared with VGG11 and SqueezeNet1_1, demonstrating modified ResNet-34’s optimal balance between depth and performance. FastAI-2 enhances computational efficiency, enabling rapid convergence with minimal training time. Results The proposed system achieved 99.78% accuracy, confirming the effectiveness of automated lung cancer histopathology classification. This study highlights the potential of artificial intelligence (AI)-driven diagnostic tools to assist pathologists by improving accuracy, reducing workload, and enhancing decision-making in clinical settings.},
  archive      = {J_PEERJCS},
  author       = {Pranshu Saxena and Sanjay Kumar Singh and Mamoon Rashid and Sultan S. Alshamrani and Mrim M. Alnfiai},
  doi          = {10.7717/peerj-cs.2903},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2903},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Efficient deep learning model for classifying lung cancer images using normalized stain agnostic feature method and FastAI-2},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved hippopotamus optimization algorithm based on adaptive development and solution diversity enhancement. <em>PEERJCS</em>, <em>11</em>, e2901. (<a href='https://doi.org/10.7717/peerj-cs.2901'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes an improved hippopotamus optimization algorithm to address the limitations of the traditional hippopotamus optimization algorithm in terms of convergence performance and solution diversity in complex high-dimensional problems. Inspired by the natural behavior of hippopotamuses, this article introduces chaotic map initialization, an adaptive exploitation mechanism, and a solution diversity enhancement strategy based on the original algorithm. The chaotic map is employed to optimize the initial population distribution, thereby enhancing the global search capability. The adaptive exploitation mechanism dynamically adjusts the weights between the exploration and exploitation phases to balance global and local searches. The solution diversity enhancement is achieved through the introduction of nonlinear perturbations, which help the algorithm avoid being trapped in local optima. The proposed algorithm is validated on several standard benchmark functions (CEC17, CEC22), and the results demonstrate that the improved algorithm significantly outperforms the original hippopotamus optimization algorithm and other mainstream optimization algorithms in terms of convergence speed, solution accuracy, and global search ability. Moreover, statistical analysis further confirms the superiority of the improved algorithm in balancing exploration and exploitation, particularly when dealing with high-dimensional multimodal functions. This study provides new insights and enhancement strategies for the application of the hippopotamus optimization algorithm in solving complex optimization problems.},
  archive      = {J_PEERJCS},
  author       = {Shengyu Pei and Gang Sun and Lang Tong},
  doi          = {10.7717/peerj-cs.2901},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2901},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An improved hippopotamus optimization algorithm based on adaptive development and solution diversity enhancement},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). High-fidelity steganography in EEG signals using advanced transform-based methods. <em>PEERJCS</em>, <em>11</em>, e2900. (<a href='https://doi.org/10.7717/peerj-cs.2900'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing prevalence of digital health solutions and smart health devices (SHDs) ensures the continuity of personal biometric data while simultaneously raising concerns about their security and privacy. Consequently, the development of novel encryption techniques and data protection policies is crucial to comply with regulations such as The Health Insurance Portability and Accountability Act (HIPAA) and to safeguard against cyber threats. This study introduces a robust and efficient method for embedding private information into electroencephalogram (EEG) signals by employing the stationary wavelet transform (SWT), singular value decomposition (SVD), and tent map techniques. The proposed approach aims to increase embedding capacity while maintaining signal integrity, ensuring resilience against various forms of distortion, and achieving computational efficiency. Experiments were conducted on three publicly available EEG datasets (Graz A, DEAP, and Bonn), and performance was evaluated using widely recognized metrics, including peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), percentage root mean square difference (PRD), normalized cross-correlation (NCC), bit error rate (BER), and Euclidean distance (ED). The results indicate that the method preserves perceptual quality, achieving PSNR values above 60 dB and demonstrating minimal signal distortion. Robustness tests involving noise addition, random cropping, and low-pass filtering confirm the method’s high resilience, with BER approaching zero and NCC near unity. Moreover, the proposed method demonstrates significantly reduced hiding and extraction times compared to conventional approaches, enhancing its suitability for real-time, secure biomedical data transmission.},
  archive      = {J_PEERJCS},
  author       = {Enes Efe},
  doi          = {10.7717/peerj-cs.2900},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2900},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {High-fidelity steganography in EEG signals using advanced transform-based methods},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of the stages of alzheimer’s disease based on three-dimensional lightweight neural networks. <em>PEERJCS</em>, <em>11</em>, e2897. (<a href='https://doi.org/10.7717/peerj-cs.2897'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease is a neurodegenerative disease that seriously threatens the life and health of the elderly. This study used three-dimensional lightweight neural networks to classify the stages of Alzheimer’s disease and explore the relationship between the stages and the variations of brain tissue. The study used CAT12 to preprocess magnetic resonance images of the brain and got three kinds of preprocessed images: standardized images, segmented standardized gray matter images, and segmented standardized white matter images. The three kinds of images were used to train four kinds of three-dimensional lightweight neural networks respectively, and the evaluation metrics of the neural networks are calculated. The accuracies of the neural networks for classifying the stages of Alzheimer’s disease (cognitively normal, mild cognitive impairment, Alzheimer’s disease) in the study are above 96%, and the precisions and recalls of classifying the three stages are above 94%. The study found that for the classification of cognitively normal, the best classification results can be obtained by training with the segmented standardized gray matter images, and for mild cognitive impairment and Alzheimer’s disease, the best classification results can be obtained by training with the standardized images. The study analyzed that in the process of cognitively normal to mild cognitive impairment, variations in the segmented standardized gray matter images are more obvious at the beginning, while variations in the segmented standardized white matter images are not obvious. As the disease progresses, variations in the segmented standardized white matter images tend to become more significant, and variations in the segmented standardized gray matter images and white matter images are both significant in the development of Alzheimer’s disease.},
  archive      = {J_PEERJCS},
  author       = {Jun Li and Juntong Liu and Yang Su and Jie Chang and Mingquan Ye},
  doi          = {10.7717/peerj-cs.2897},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2897},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Classification of the stages of alzheimer’s disease based on three-dimensional lightweight neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling and implementation of a real-time digital twin for the stewart platform with real-time trajectory computation. <em>PEERJCS</em>, <em>11</em>, e2892. (<a href='https://doi.org/10.7717/peerj-cs.2892'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concept of a digital twin is increasingly acknowledged as an innovative and promising tool with significant potential in various end-use applications. At the heart of digital twin technology is the acquisition of real-time data from physical entities. However, the occurrence of disturbances necessitates the incorporation of resilience features within the digital twin architecture. The primary objective of this article is to develop resilient digital twins specifically for the Stewart platform. This work focuses on constructing the virtual component of the digital twin using MATLAB/Simulink and subsequently integrating this virtual model with its physical counterpart to establish a comprehensive digital twin system. Unlike other models, this system includes a motion trajectory computation module. This module is designed to receive signals from physical entities and convert them into motion trajectory data for input into the model, thereby aiming to accurately reflect the state of the physical entities under disruptive conditions. This functionality significantly enhances the reliability of the system beyond that of traditional digital twin systems. Furthermore, the article explores novel strategies and a framework for enhancing the resilience of the Stewart platform to disturbances.},
  archive      = {J_PEERJCS},
  author       = {Xiurui Ding and Jinsheng Xing},
  doi          = {10.7717/peerj-cs.2892},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2892},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Modeling and implementation of a real-time digital twin for the stewart platform with real-time trajectory computation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analysis of the fusion of multimodal sentiment perception and physiological signals in chinese-english cross-cultural communication: Transformer approach incorporating self-attention enhancement. <em>PEERJCS</em>, <em>11</em>, e2890. (<a href='https://doi.org/10.7717/peerj-cs.2890'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the acceleration of globalization, cross-cultural communication has become a crucial issue in various fields. Emotion, as an essential component of communication, plays a key role in improving understanding and interaction efficiency across different cultures. However, accurately recognizing emotions across cultural backgrounds remains a major challenge in affective computing, particularly due to limitations in multimodal feature fusion and temporal dependency modeling in traditional approaches. To address this, we propose the TAF-ATRM framework, which integrates Transformer and multi-head attention mechanisms for cross-cultural emotion recognition. Specifically, the framework employs bidirectional encoder representations from transformers (BERT) for semantic feature extraction from text, Mel-frequency Cepstral Coefficients (MFCC) and Residual Neural Network (ResNet) for capturing critical features from speech and facial expressions, respectively, thereby enhancing multimodal emotion recognition capability. To improve the fusion of multimodal data, the Transformer is utilized for temporal feature modeling, while multi-head attention reinforces feature representation by capturing complex inter-modal dependencies. The framework is evaluated on the MOSI and MOSEI datasets, where experimental results demonstrate that TAF-ATRM outperforms traditional methods in emotion classification accuracy and robustness, particularly in cross-cultural emotion recognition tasks. This study provides a strong technical foundation for future advancements in multimodal emotion analysis and cross-cultural affective computing.},
  archive      = {J_PEERJCS},
  author       = {Xin Bi and Tian Zhang},
  doi          = {10.7717/peerj-cs.2890},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2890},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Analysis of the fusion of multimodal sentiment perception and physiological signals in chinese-english cross-cultural communication: Transformer approach incorporating self-attention enhancement},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimation of lower limb torque: A novel hybrid method based on continuous wavelet transform and deep learning approach. <em>PEERJCS</em>, <em>11</em>, e2888. (<a href='https://doi.org/10.7717/peerj-cs.2888'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomechanical analysis of the human lower limbs plays a critical role in movement assessment, injury prevention, and rehabilitation guidance. Traditional gait analysis techniques, such as optical motion capture systems and biomechanical force platforms, are limited by high costs, operational complexity, and restricted applicability. In view of this, this study proposes a cost-effective and user-friendly approach that integrates inertial measurement units (IMUs) with a novel deep learning framework for real-time lower limb joint torque estimation. The proposed method combines time-frequency domain analysis through continuous wavelet transform (CWT) with a hybrid architecture comprising multi-head self-attention (MHSA), bidirectional long short-term memory (Bi-LSTM), and a one-dimensional convolutional residual network (1D Conv ResNet). This integration enhances feature extraction, noise suppression, and temporal dependency modeling, particularly for non-stationary and nonlinear signals in dynamic environments. Experimental validation on public datasets demonstrates high accuracy, with a root mean square error (RMSE) of 0.16 N·m/kg, Coefficient of Determination (R2) of 0.91, and Pearson correlation coefficient of 0.95. Furthermore, the framework outperforms existing models in computational efficiency and real-time applicability, achieving a single-cycle inference time of 152.6 ms, suitable for portable biomechanical monitoring systems.},
  archive      = {J_PEERJCS},
  author       = {Shu Xu and Tao Wang and Zenghui Ding and Yu Wang and Tongsheng Wan and Dezhang Xu and Xianjun Yang and Ting Sun and Meng Li},
  doi          = {10.7717/peerj-cs.2888},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2888},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Estimation of lower limb torque: A novel hybrid method based on continuous wavelet transform and deep learning approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperdimensional computing in biomedical sciences: A brief review. <em>PEERJCS</em>, <em>11</em>, e2885. (<a href='https://doi.org/10.7717/peerj-cs.2885'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperdimensional computing (HDC, also known as vector-symbolic architectures—VSA) is an emerging computational paradigm that relies on dealing with vectors in a high-dimensional space to represent and combine every kind of information. It finds applications in a wide array of fields including bioinformatics, natural language processing, machine learning, artificial intelligence, and many other scientific disciplines. Here we introduced the basic foundations of the HDC, focusing on its application to biomedical sciences, with a particular emphasis to bioinformatics, cheminformatics, and medical informatics, providing a critical and comprehensive review of the current HDC landscape, highlighting pros and cons of applying this computational paradigm in these specific scientific domains. In this study, we first selected around forty scientific articles on hyperdimensional computing applied to biomedical data existing in the literature, and then analyzed key aspects of their studies, such as vector construction, data encoding, programming language employed, and other features. We also counted how many of these scientific articles are open access, how many have public software code available, how many groups of authors, journals, and conferences are most present among them. Finally, we discussed the advantages and limitations of the HDC approach, outlining potential future directions and open challenges for the adoption of HDC in biomedical sciences. To the best of our knowledge, our review is the first open brief survey on this topic among the biomedical sciences, and therefore we believe it can be of interest and useful for the readership.},
  archive      = {J_PEERJCS},
  author       = {Fabio Cumbo and Davide Chicco},
  doi          = {10.7717/peerj-cs.2885},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2885},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hyperdimensional computing in biomedical sciences: A brief review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RMIS-net: A fast medical image segmentation network based on multilayer perceptron. <em>PEERJCS</em>, <em>11</em>, e2882. (<a href='https://doi.org/10.7717/peerj-cs.2882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image segmentation, a pivotal component in diagnostic workflows and therapeutic decision-making, plays a critical role in clinical applications ranging from pathological diagnosis to surgical navigation and treatment evaluation. To address the persistent challenges of computational complexity and efficiency limitations in existing methods, we propose RMIS-Net—an innovative lightweight segmentation network with three core components: a convolutional layer for preliminary feature extraction, a shift-based fully connected layer for parameter-efficient spatial modeling, and a tokenized multilayer perceptron for global context capture. This architecture achieves significant parameter reduction while enhancing local feature representation through optimized shift operations. The network incorporates layer normalization and dropout regularization to ensure training stability, complemented by Gaussian error linear unit (GELU) activation functions for improved non-linear modeling. To further refine segmentation precision, we integrate residual connections for gradient flow optimization, a Dice loss function for class imbalance mitigation, and bilinear interpolation for accurate mask reconstruction. Comprehensive evaluations on two benchmark datasets (2018 Data Science Bowl for cellular structure segmentation and ISIC-2018 for lesion boundary delineation) demonstrate RMIS-Net’s superior performance, achieving state-of-the-art metrics including an average F1-score of 0.91 and mean intersection-over-union of 0.82. Remarkably, the proposed architecture requires only 0.03 s per image inference while achieving 27× parameter compression, 10× acceleration in inference speed, and 53× reduction in computational complexity compared to conventional approaches, establishing new benchmarks for efficient yet accurate medical image analysis.},
  archive      = {J_PEERJCS},
  author       = {Binbin Zhang and Guoliang Xu and Yiying Xing and Nanjie Li and Deguang Li},
  doi          = {10.7717/peerj-cs.2882},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2882},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RMIS-net: A fast medical image segmentation network based on multilayer perceptron},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel attention-based deep learning model for improving sentiment classification after the case of the 2023 Kahramanmaras/Turkey earthquake on twitter. <em>PEERJCS</em>, <em>11</em>, e2881. (<a href='https://doi.org/10.7717/peerj-cs.2881'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter has emerged as one of the most widely used platforms for sharing information and updates. As users freely express their thoughts and emotions, a vast amount of data is generated, particularly in the aftermath of disasters, which can be collected quickly and directly from individuals. Traditionally, earthquake impact assessments have been conducted through field studies by non-governmental organizations (NGOs), a process that is often time-consuming and costly. Sentiment analysis (SA) on Twitter presents a valuable research area, enabling the extraction and interpretation of real-time public perceptions. In recent years, attention-based methods in deep learning networks have gained significant attention among researchers. This study proposes a novel sentiment classification model, MConv-BiLSTM-GAM, which leverages an attention mechanism to analyze public sentiment following the 7.8 and 7.5 Mw earthquakes that struck Kahramanmaraş, Turkey. The model employs the FastText word embedding technique to convert tweets into vector representations. These vectorized inputs are then processed by a hybrid model integrating convolutional neural networks (CNNs) and recurrent neural networks (RNNs) with a global attention mechanism. This ensures careful consideration of semantic dependencies in sentiment classification. The proposed model operates in three stages: (i) MConv—Local Contextual Feature Extraction, (ii) bidirectional long short-term memory (BiLSTM)—sequence learning, and (iii) Global Attention Mechanism (GAM)—Attention Mechanism. Experimental results demonstrate that the model achieves an accuracy of 93.32%, surpassing traditional deep learning models in the literature by approximately 3%. This research aims to provide objective insights to policymakers and decision-makers, facilitating adequate support for individuals and communities affected by disasters. Moreover, analyzing public sentiment during earthquakes contributes to understanding societal responses and emotional trends in disaster scenarios.},
  archive      = {J_PEERJCS},
  author       = {Serpil Aslan and Muhammed Yildirim},
  doi          = {10.7717/peerj-cs.2881},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2881},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel attention-based deep learning model for improving sentiment classification after the case of the 2023 Kahramanmaras/Turkey earthquake on twitter},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RBI: A novel algorithm for regulatory-metabolic network model in designing the optimal mutant strain. <em>PEERJCS</em>, <em>11</em>, e2880. (<a href='https://doi.org/10.7717/peerj-cs.2880'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the last 20 years, researchers have proposed regulatory-metabolic network models to integrate gene regulatory networks (GRNs) and metabolic networks in in silico metabolic engineering, aiming to enhance the production rate of desired metabolites. However, the proposed models are unable to comprehensively include the Boolean rules in the empirical gene regulatory networks (GRNs) and gene-protein-reaction (GPR) interactions. Thus, the types of gene interactions, such as inhibition and activation, are disregarded from the analysis. This may result in sub-optimal model performance. Hence, this article presented a novel model using reliability theory to include Boolean rules in empirical GRNs and GPR rules in the integrating process. The proposed algorithm of this model is termed as a reliability-based integrating (RBI) algorithm. The suggested algorithm had three variants: RBI-T1, RBI-T2, and RBI-T3. The performance of the RBI algorithms was assessed by comparing them with the existing algorithms, using empirical results and validated transcription factors (TF) knockout schemes, and their complexity time was identified. Also, the RBI method was implemented in the design of optimal mutant strains of Escherichia coli and Saccharomyces cerevisiae. The simulation results indicated that the effectiveness and efficiency of the RBI algorithms are adequately strong and competitive relative to the existing algorithms. Furthermore, the RBI algorithm effectively identified eight schemes capable of enhancing succinate and ethanol production rates by maintaining the survival of microbial strains. Those results demonstrated that the RBI algorithms are recommended for the construction of optimum mutant strains in in silico metabolic engineering.},
  archive      = {J_PEERJCS},
  author       = {Ridho Ananda and Kauthar Mohd Daud and Suhaila Zainudin},
  doi          = {10.7717/peerj-cs.2880},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2880},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RBI: A novel algorithm for regulatory-metabolic network model in designing the optimal mutant strain},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GFADE: Generalized feature adaptation and discrimination enhancement for deepfake detection. <em>PEERJCS</em>, <em>11</em>, e2879. (<a href='https://doi.org/10.7717/peerj-cs.2879'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of deep generative techniques, such as generative adversarial networks (GANs), the creation of realistic fake images and videos has become increasingly accessible, raising significant security and privacy concerns. Although existing deepfake detection methods perform well within a single dataset, they often experience substantial performance degradation when applied across datasets or manipulation types. To address this challenge, we propose a novel deepfake detection framework that combines multiple loss functions and the MixStyle technique. By integrating Cross-Entropy Loss, ArcFace loss, and Focal Loss, our model enhances its discriminative power to better handle complex forgery characteristics and effectively mitigate data imbalance. Additionally, the MixStyle technique introduces diverse visual styles during training, further improving the model’s generalization across different datasets and manipulation scenarios. Experimental results demonstrate that our method achieves superior detection accuracy across a range of cross-dataset and cross-manipulation tests, significantly improving model robustness and generalizability.},
  archive      = {J_PEERJCS},
  author       = {ZhiYong Tian and Junkai Yi},
  doi          = {10.7717/peerj-cs.2879},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2879},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GFADE: Generalized feature adaptation and discrimination enhancement for deepfake detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluation of unsupervised static topic models’ emergence detection ability. <em>PEERJCS</em>, <em>11</em>, e2875. (<a href='https://doi.org/10.7717/peerj-cs.2875'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting emerging topics is crucial for understanding research trends, technological advancements, and shifts in public discourse. While unsupervised topic modeling techniques such as Latent Dirichlet allocation (LDA), BERTopic, and CoWords clustering are widely used for topic extraction, their ability to retrospectively detect emerging topics without relying on ground truth labels has not been systematically compared. This gap largely stems from the lack of a dedicated evaluation metric for measuring emergence detection. In this study, we introduce a quantitative evaluation metric to assess the effectiveness of topic models in detecting emerging topics. We evaluate three topic modeling approaches using both qualitative analysis and our proposed emergence detection metric. Our results indicate that, qualitatively, CoWords identifies emerging topics earlier than LDA and BERTopics. Quantitatively, our evaluation metric demonstrates that LDA achieves an average F1 score of 80.6% in emergence detection, outperforming BERTopic by 24.0%. These findings highlight the strengths and limitations of different topic models for emergence detection, while our proposed metric provides a robust framework for future benchmarking in this area.},
  archive      = {J_PEERJCS},
  author       = {Xue Li and Ciro D. Esposito and Paul Groth and Jonathan Sitruk and Balazs Szatmari and Nachoem Wijnberg},
  doi          = {10.7717/peerj-cs.2875},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2875},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evaluation of unsupervised static topic models’ emergence detection ability},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Technological trends in epidemic intelligence for infectious disease surveillance: A systematic literature review. <em>PEERJCS</em>, <em>11</em>, e2874. (<a href='https://doi.org/10.7717/peerj-cs.2874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background This research focuses on improving epidemic monitoring systems by incorporating advanced technologies to enhance the surveillance of diseases more effectively than before. Considering the drawbacks associated with surveillance methods in terms of time consumption and efficiency, issues highlighted in this study includes the integration of Artificial Intelligence (AI) in early detection, decision support and predictive modeling, big data analytics in data sharing, contact tracing and countering misinformation, Internet of Things (IoT) devices in real time disease monitoring and Geographic Information Systems (GIS) for geospatial artificial intelligence (GeoAI) applications and disease mapping. The increasing intricacy and regular occurrence of disease outbreaks underscore the pressing necessity for improvements in public health monitoring systems. This research delves into the developments and their utilization in detecting and handling infectious diseases while exploring how these progressions contribute to decision making and policy development, in public healthcare. Methodology This review systematically analyzes how technological tools are being used in epidemic monitoring by conducting a structured search across online literature databases and applying eligibility criteria to identify relevant studies on current technological trends in public health surveillance. Results The research reviewed 69 articles from 2019 to 2023 focusing on emerging trends in epidemic intelligence. Most of the studies emphasized the integration of artificial intelligence with technologies like big data analytics, geographic information systems, and the Internet of Things for monitoring infectious diseases. Conclusions The expansion of publicly accessible information on the internet has opened a new pathway for epidemic intelligence. This study emphasizes the importance of integrating information technology tools such as AI, big data analytics, GIS, and the IoT in epidemic intelligence surveillance to effectively track infectious diseases. Combining these technologies helps public health agencies in detecting and responding to health threats.},
  archive      = {J_PEERJCS},
  author       = {Hazeeqah Amny Kamarul Aryffin and Murtadha Arif Bin Sahbudin and Sakinah Ali Pitchay and Azni Haslizan Abhalim and Ilfita Sahbudin},
  doi          = {10.7717/peerj-cs.2874},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2874},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Technological trends in epidemic intelligence for infectious disease surveillance: A systematic literature review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning for internet of things (IoT) device identification: A comparative study. <em>PEERJCS</em>, <em>11</em>, e2873. (<a href='https://doi.org/10.7717/peerj-cs.2873'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid deployment of millions of connected devices brings significant security challenges to the Internet of Things (IoT). IoT devices are typically resource-constrained and designed for specific tasks, from which new security challenges are introduced. As such, IoT device identification has garnered substantial attention and is regarded as an initial layer of cybersecurity. One of the major steps in distinguishing IoT devices involves leveraging machine learning (ML) techniques on device network flows known as device fingerprinting. Numerous studies have proposed various solutions that incorporate ML and feature selection (FS) algorithms with different degrees of accuracy. Yet, the domain needs a comparative analysis of the accuracy of different classifiers and FS algorithms to comprehend their true capabilities in various datasets. This article provides a comprehensive performance evaluation of several reputable classifiers being used in the literature. The study evaluates the efficacy of filter-and wrapper-based FS methods across various ML classifiers. Additionally, we implemented a Binary Green Wolf Optimizer (BGWO) and compared its performance with that of traditional ML classifiers to assess the potential of this binary meta-heuristic algorithm. To ensure the robustness of our findings, we evaluated the effectiveness of each classifier and FS method using two widely utilized datasets. Our experiments demonstrated that BGWO effectively reduced the feature set by 85.11% and 73.33% for datasets 1 and 2, respectively, while achieving classification accuracies of 98.51% and 99.8%, respectively. The findings of this study highlight the strong capabilities of BGWO in reducing both the feature dimensionality and accuracy gained through classification. Furthermore, it demonstrates the effectiveness of wrapper methods in the reduction of feature sets.},
  archive      = {J_PEERJCS},
  author       = {Hamid Tahaei and Anqi Liu and Hamid Forooghikian and Mehdi Gheisari and Faiz Zaki and Nor Badrul Anuar and Zhaoxi Fang and Longjun Huang},
  doi          = {10.7717/peerj-cs.2873},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2873},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Machine learning for internet of things (IoT) device identification: A comparative study},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gene selection based on adaptive neighborhood-preserving multi-objective particle swarm optimization. <em>PEERJCS</em>, <em>11</em>, e2872. (<a href='https://doi.org/10.7717/peerj-cs.2872'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of high-dimensional microarray gene expression data presents critical challenges, including excessive dimensionality, increased computational burden, and sensitivity to random initialization. Traditional optimization algorithms often produce inconsistent and suboptimal results, while failing to preserve local data structures limiting both predictive accuracy and biological interpretability. To address these limitations, this study proposes an adaptive neighborhood-preserving multi-objective particle swarm optimization (ANPMOPSO) framework for gene selection. ANPMOPSO introduces four key innovations: (1) a weighted neighborhood-preserving ensemble embedding (WNPEE) technique for dimensionality reduction that retains local structure; (2) Sobol sequence (SS) initialization to enhance population diversity and convergence stability; (3) a differential evolution (DE)-based adaptive velocity update to dynamically balance exploration and exploitation; and (4) a novel ranking strategy that combines Pareto dominance with neighborhood preservation quality to prioritize biologically meaningful gene subsets. Experimental evaluations on six benchmark microarray datasets and eleven multi-modal test functions (MMFs) demonstrate that ANPMOPSO consistently outperforms state-of-the-art methods. For example, it achieves 100% classification accuracy on Leukemia and Small-Round-Blue-Cell Tumor (SRBCT) using only 3–5 genes, improving accuracy by 5–15% over competitors while reducing gene subsets by 40–60%. Additionally, on MMFs, ANPMOPSO attains superior hypervolume values (e.g., 1.0617 ± 0.2225 on MMF1, approximately 10–20% higher than competitors), confirming its robustness in balancing convergence and diversity. Although the method incurs higher training time due to its structural and adaptive components, it achieves a strong trade-off between computational cost and biological relevance, making it a promising tool for high-dimensional gene selection in bioinformatics.},
  archive      = {J_PEERJCS},
  author       = {Sumet Mehta and Fei Han and Muhammad Sohail and Bhekisipho Twala and Asad Ullah and Fasee Ullah and Arfat Ahmad Khan and Qinghua Ling},
  doi          = {10.7717/peerj-cs.2872},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2872},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Gene selection based on adaptive neighborhood-preserving multi-objective particle swarm optimization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient parallel DCNN algorithm in big data environment. <em>PEERJCS</em>, <em>11</em>, e2871. (<a href='https://doi.org/10.7717/peerj-cs.2871'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data plays a vital role in developing remote sensing, landslide prediction, and enabling applications, the integration of deep convolutional neural networks (DCNN) has significantly improved its prediction accuracy. However, several challenges remain in processing vast satellite imagery and other geospatial data. These challenges include excessive redundant features, slow convolution operation, and poor loss function convergence. An efficient parallel DCNN algorithm (PDCNN-MI), combined with MapReduce and Im2col algorithms, is introduced to address these challenges. First, a parallel feature extraction strategy based on the Marr-Hildreth operator (PFE-MHO) is proposed to extract target features from data as inputs to the network, effectively solving the problem of high data redundancy. Next, a parallel model training strategy based on Im2col method (PMT-IM) is designed to remove the redundant convolutional kernels by designing the center value of distance, improving convolution operation speed. Finally, a small batch gradient descent strategy (IMBGD) is presented to exclude the influence of training data of anomalous nodes on the batch gradient and solve the problem of poor convergence of the loss function. By utilizing these enhancements, the experimental results indicate that PDCNN-MI outperforms existing algorithms in classification accuracy and is well-suited for fast and large-scale image dataset processing.},
  archive      = {J_PEERJCS},
  author       = {Yimin Mao and Yaser Ahangari Nanehkaran and Neelakandan Chandrasekaran and Ying Huo and Zhan Qing Wen and ke Gong and Miao Decheng},
  doi          = {10.7717/peerj-cs.2871},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2871},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An efficient parallel DCNN algorithm in big data environment},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing healthcare data privacy and interoperability with federated learning. <em>PEERJCS</em>, <em>11</em>, e2870. (<a href='https://doi.org/10.7717/peerj-cs.2870'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article explores the application of federated learning (FL) with the Fast Healthcare Interoperability Resources (FHIR) protocol to address the underutilization of the huge volumes of healthcare data generated by the digital health revolution, especially those from wearable sensors, due to privacy concerns and interoperability challenges. Despite advances in electronic medical records, mobile health applications, and wearable sensors, current digital health cannot fully exploit these data due to the lack of data analysis and exchange between heterogeneous systems. To address this gap, we present a novel converged platform combining FL and FHIR, which enables collaborative model training that preserves the privacy of wearable sensor data while promoting data standardization and interoperability. Unlike traditional centralized learning (CL) solutions that require data centralization, our platform uses local model learning, which naturally improves data privacy. Our empirical evaluation demonstrates that federated learning models perform as well as, or even numerically better than, centralized learning models in terms of classification accuracy, while also performing equally well in regression, as indicated by metrics such as accuracy, area under the curve (AUC), recall, and precision, among others, for classification, and mean absolute error (MAE), mean squared error (MSE), and root mean square error (RMSE) for regression. In addition, we developed an intuitive AutoML-powered web application that is FL and CL compatible to illustrate the feasibility of our platform for predictive modeling of physical activity and energy expenditure, while complying with FHIR data reporting standards. These results highlight the immense potential of our FHIR-integrated federated learning platform as a practical framework for future interoperable and privacy-preserving digital health ecosystems to optimize the use of connected health data.},
  archive      = {J_PEERJCS},
  author       = {Adil Akhmetov and Zohaib Latif and Benjamin Tyler and Adnan Yazici},
  doi          = {10.7717/peerj-cs.2870},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2870},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing healthcare data privacy and interoperability with federated learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling LTE-advanced cell capacity estimation using packet bundling and carrier aggregation. <em>PEERJCS</em>, <em>11</em>, e2868. (<a href='https://doi.org/10.7717/peerj-cs.2868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increasing demand for mobile usage raises many challenges for service providers. Satisfying subscribers by providing a better quality of service (QoS) is one of the major concerns of an operator. One way to solve this problem is to adopt an efficient radio resource use mechanism. In this context, smart network planning management requires the estimation and enhancement of capacity in terms of the number of subscribers within a cell for better QoS provisioning through radio resource usage optimization. The model proposed in this article explicitly uses some long term evolution (LTE)-Advanced (LTE-A)-specific enhancements such as carrier aggregation (CA) and channel quality indication (CQI)-based resource allocation. Further, this study proposes a CQI-based clustering approach with packet bundling and CA to optimize radio resource utilization and enhance LTE-A cell capacity. LTE-A cell is logically divided into clusters such as the Silver class, Platinum class, Gold class, and Diamond class based on the CQI from the user end to eNodeB (eNB). Further, cell capacity (CCa) estimation algorithms are proposed in a simplistic scenario as well as in each cluster using packet bundling factor ω considering CA. From the result analysis, it is found that appropriate modulation and voice codecs can be used in appropriate clusters to enhance the cell capacity. Furthermore, it is observed that the packet bundling factor helps in improving the radio resource usage and thereby improving the capacity of a cell. The research work proposed in this article can be extended further to estimate the user capacity in the context of the 5th generation cellular network.},
  archive      = {J_PEERJCS},
  author       = {Rajiv Senapati},
  doi          = {10.7717/peerj-cs.2868},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2868},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Modeling LTE-advanced cell capacity estimation using packet bundling and carrier aggregation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effectiveness and optimization of bidirectional long short-term memory (BiLSTM) based fast detection of deep fake face videos for real-time applications. <em>PEERJCS</em>, <em>11</em>, e2867. (<a href='https://doi.org/10.7717/peerj-cs.2867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a rapid detection method for deepfake face videos designed for real-time applications using bidirectional long short-term memory (BiLSTM) networks. The aim is to overcome the limitations of current technologies in terms of efficiency and accuracy. An optimized BiLSTM architecture and training strategy are employed, enhancing recognition capabilities through data preprocessing and feature enhancement while also minimizing computational complexity and resource consumption during detection. Experiments were conducted on the FaceForensics++ dataset, which includes both authentic and four types of manipulated videos. The results show that the proposed BiLSTM-based approach outperforms existing methods in real-time detection. Specifically, the integration of temporal analysis and conditional random fields (CRF) resulted in significant accuracy improvements: a 1.6% increase in checking accuracy, a 2.0% improvement in checking completeness, and a 2.5% increase in the F1-score. The BiLSTM-based rapid detection approach demonstrated high efficiency and accuracy across multiple standard datasets, achieving notable performance gains over current technologies. These findings highlight the method’s potential and value for real-time deepfake detection applications.},
  archive      = {J_PEERJCS},
  author       = {Haoxiang Wang},
  doi          = {10.7717/peerj-cs.2867},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2867},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Effectiveness and optimization of bidirectional long short-term memory (BiLSTM) based fast detection of deep fake face videos for real-time applications},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel facial expression recognition framework using deep learning based dynamic cross-domain dual attention network. <em>PEERJCS</em>, <em>11</em>, e2866. (<a href='https://doi.org/10.7717/peerj-cs.2866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variations in domain targets have recently posed significant challenges for facial expression recognition tasks, primarily due to domain shifts. Current methods focus largely on global feature adoption to achieve domain-invariant learning; however, transferring local features across diverse domains remains an ongoing challenge. Additionally, during training on target datasets, these methods often suffer from reduced feature representation in the target domain due to insufficient discriminative supervision. To tackle these challenges, we propose a dynamic cross-domain dual attention network for facial expression recognition. Our model is specifically designed to learn domain-invariant features through separate modules for global and local adversarial learning. We also introduce a semantic-aware module to generate pseudo-labels, which computes semantic labels from both global and local features. We assess our model’s effectiveness through extensive experiments on the Real-world Affective Faces Database (RAF-DB), FER-PLUS, AffectNet, Expression in the Wild (ExpW), SFEW 2.0, and Japanese Female Facial Expression (JAFFE) datasets. The results demonstrate that our scheme outperforms the existing state-of-the-art methods by attaining recognition accuracies 93.18, 92.35, 82.13, 78.37, 72.47, 70.68 respectively.},
  archive      = {J_PEERJCS},
  author       = {Ahmed Omar Alzahrani and Ahmed Mohammed Alghamdi and M. Usman Ashraf and Iqra Ilyas and Nadeem Sarwar and Abdulrahman Alzahrani and Alaa Abdul Salam Alarood},
  doi          = {10.7717/peerj-cs.2866},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2866},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel facial expression recognition framework using deep learning based dynamic cross-domain dual attention network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of domain-specific modeling in kinetography and bipedal humanoid robot control. <em>PEERJCS</em>, <em>11</em>, e2864. (<a href='https://doi.org/10.7717/peerj-cs.2864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article presents a new approach in the development of software for bipedal humanoid robot controllers, based on the construction and application of graphic domain-specific languages (DSLs). The notations used to describe dance movements and gestures are typical examples of DSLs. With certain extensions, related to the description of foot topology, sensors and actuators, such DSLs are applicable for modeling dance movements that would be performed by a robot. The existing software development methodologies in robotics have a purely mechanistic approach to understanding and implementing robotic tasks. Such an approach in humanoid robotics complicates the understanding of the problem, as well as the specification and implementation of solutions. Our approach, which uses DSLs, adopts complex movements and gestures performed by the feet of dancers using professional dancers, people with above-average motor skills, as reference. We believe that the developed software can also be successfully applied to assistive robots that would help people with special needs whose mobility is significantly lower than average.},
  archive      = {J_PEERJCS},
  author       = {Verislav Djukić and Dragana Oros and Marko Penčić and Zhenli Lu},
  doi          = {10.7717/peerj-cs.2864},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2864},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Application of domain-specific modeling in kinetography and bipedal humanoid robot control},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic bandwidth allocation in time division multiplexed passive optical networks: A dual-standard analysis of ITU-T and IEEE standard algorithms. <em>PEERJCS</em>, <em>11</em>, e2863. (<a href='https://doi.org/10.7717/peerj-cs.2863'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last 25 years, operators have effectively established passive optical networks (PONs), catering to around 1 billion users and earning income surpassing 8.5 billion Euros. Major standardization bodies like IEEE and ITU-T have introduced several PON solutions to mitigate last-mile broadband access and bandwidth allocation problems for end users. In this case, a compelling dynamic bandwidth allocation (DBA) algorithm can provide contention-free access (fairness) to the end user for the upstream channel with high bandwidth efficiency, minimal upstream delays, and scalability. This, in turn, boosts network quality of service (QoS) and allows operators to accommodate more users (revenue). This article examines the evolution of time-division multiplexed PON solutions such as A/BPON, EPON, GPON, XGPON, 10G-EPON, and NG-PON2 under both IEEE and ITU-T standards, addressing their approaches to DBA challenges. We analyze the bottlenecks and compare reported works based on their key strengths/applications, weaknesses, and operational mechanisms, as well as highlight their quantitative insights. We also discuss next-generation PONs (NG-PONs) and their emerging applications, such as 5G/6G fronthaul architecture in the cloud radio access network (CRAN) environment, fiber to the room (FTTR), and industrial PON, with a focus on DBA designs. Finally, the article summarizes current progress, highlights challenges, and proposes future research directions for developing more efficient DBA algorithms for these new applications.},
  archive      = {J_PEERJCS},
  author       = {Kamran Ali Memon and Syed Saeed Jaffer and Muhammad Ali Qureshi and Khurram Karim Qureshi},
  doi          = {10.7717/peerj-cs.2863},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2863},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic bandwidth allocation in time division multiplexed passive optical networks: A dual-standard analysis of ITU-T and IEEE standard algorithms},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast2Vec, a modified model of FastText that enhances semantic analysis in topic evolution. <em>PEERJCS</em>, <em>11</em>, e2862. (<a href='https://doi.org/10.7717/peerj-cs.2862'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Topic modeling approaches, such as latent Dirichlet allocation (LDA) and its successor, the dynamic topic model (DTM), are widely used to identify specific topics by extracting words with similar frequencies from documents. However, these topics often require manual interpretation, which poses challenges in constructing semantics topic evolution, mainly when topics contain negations, synonyms, or rare terms. Neural network-based word embeddings, such as Word2vec and FastText, have advanced semantic understanding but have their limitations. Word2Vec struggles with out-of-vocabulary (OOV) words, and FastText generates suboptimal embeddings for infrequent terms. Methods This study introduces Fast2Vec, a novel model that integrates the semantic capabilities of Word2Vec with the subword analysis strength of FastText to enhance semantic analysis in topic modeling. The model was evaluated using research abstracts from the Science and Technology Index (SINTA) journal database and validated using twelve public word similarity benchmarks, covering diverse semantic and syntactic dimensions. Evaluation metrics include Spearman and Pearson correlation coefficients to assess the alignment with human judgments. Results Experimental findings demonstrated that Fast2Vec outperforms or closely matches Word2Vec and FastText across most benchmark datasets, particularly in task requiring fine-grained semantic similarity. In OOV scenarios, Fast2Vec improved semantic similarity by 39.64% compared to Word2Vec, and 6.18% compared to FastText. Even in scenarios without OOV terms, Fast2Vec achieved a 7.82% improvement over FastText and a marginal 0.087% improvement over Word2Vec. Additionally, the model effectively categorized topics into four distinct evolution patterns (diffusion, shifting, moderate fluctuations, and stability), enabling a deeper understanding of evolution topic interests and their dynamic characteristics. Conclusion Fast2Vec presents a robust and generalizable word embedding framework for semantic-based topic modeling. By combining the contextual sensitivity of Word2Vec with the subword flexibility of FastText, Fast2Vec effectively addresses prior limitations in handling OOV terms and semantic variation and demonstrates strong potential for boarder applications in natural language processing tasks.},
  archive      = {J_PEERJCS},
  author       = {Ayu Pertiwi and Azhari Azhari and Sri Mulyana},
  doi          = {10.7717/peerj-cs.2862},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2862},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fast2Vec, a modified model of FastText that enhances semantic analysis in topic evolution},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMEDNet: A multimodal approach for emotion detection in the urdu language. <em>PEERJCS</em>, <em>11</em>, e2861. (<a href='https://doi.org/10.7717/peerj-cs.2861'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion detection is a critical component of interaction between human and computer systems, more especially affective computing, and health screening. Integrating video, speech, and text information provides better coverage of the basic and derived affective states with improved estimation of verbal and non-verbal behavior. However, there is a lack of systematic preferences and models for the detection of emotions in low-resource languages such as Urdu. To this effect, we propose Urdu Multimodal Emotion Detection Network (UMEDNet), a new emotion detection model for Urdu that works with video, speech, and text inputs for a better understanding of emotion. To support our proposed UMEDNet, we created the Urdu Multimodal Emotion Detection (UMED) corpus, which is a seventeen-hour annotated corpus of five basic emotions. To the best of our knowledge, the current study provides the first corpus for detecting emotion in the context of multimodal emotion detection for the Urdu language and is extensible for extended research. UMEDNet leverages state-of-the-art techniques for feature extraction across modalities; for extracting facial features from video, both Multi-task Cascaded Convolutional Networks (MTCNN) and FaceNet were used with fine-tuned Wav2Vec2 for speech features and XLM-Roberta for text. These features are then projected into common latent spaces to enable the effective fusion of multimodal data and to enhance the accuracy of emotion prediction. The model demonstrates strong performance, achieving an overall accuracy of 85.27%, while precision, recall, and F1 scores, are all approximately equivalent. In the end, we analyzed the impact of UMEDNet and found that our model integrates data on different modalities and leads to better performance.},
  archive      = {J_PEERJCS},
  author       = {Adil Majeed and Hasan Mujtaba},
  doi          = {10.7717/peerj-cs.2861},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2861},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {UMEDNet: A multimodal approach for emotion detection in the urdu language},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal scene recognition using semantic segmentation and deep learning integration. <em>PEERJCS</em>, <em>11</em>, e2858. (<a href='https://doi.org/10.7717/peerj-cs.2858'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic modeling and recognition of indoor scenes present a significant challenge due to the complex composition of generic scenes, which contain a variety of features including themes and objects, makes semantic modeling and indoor scene recognition difficult. The gap between high-level scene interpretation and low-level visual features increases the complexity of scene recognition. In order to overcome these obstacles, this study presents a novel multimodal deep learning technique that enhances scene recognition accuracy and robustness by combining depth information with conventional red-green-blue (RGB) image data. Convolutional neural networks (CNNs) and spatial pyramid pooling (SPP) are used for analysis after a depth-aware segmentation methodology is used to identify several objects in an image. This allows for more precise image classification. The effectiveness of this method is demonstrated by experimental findings, which show 91.73% accuracy on the RGB-D scene dataset and 90.53% accuracy on the NYU Depth v2 dataset. These results demonstrate how the multimodal approach can improve scene detection and classification, with potential uses in fields including robotics, sports analysis, and security systems.},
  archive      = {J_PEERJCS},
  author       = {Aysha Naseer and Mohammed Alnusayri and Haifa F. Alhasson and Mohammed Alatiyyah and Dina Abdulaziz AlHammadi and Ahmad Jalal and Jeongmin Park},
  doi          = {10.7717/peerj-cs.2858},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2858},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multimodal scene recognition using semantic segmentation and deep learning integration},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vision-based approach to knee osteoarthritis and parkinson’s disease detection utilizing human gait patterns. <em>PEERJCS</em>, <em>11</em>, e2857. (<a href='https://doi.org/10.7717/peerj-cs.2857'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the number of cases of musculoskeletal and neurological disorders, such as knee osteoarthritis (KOA) and Parkinson’s disease (PD), has significantly increased. Numerous clinical methods have been proposed in research to diagnose these disorders; however, a current trend in diagnosis is through human gait patterns. Several researchers proposed different methods in this area, including gait detection utilizing sensor-based data and vision-based systems that include both marker-based and marker-free techniques. The majority of current studies are concerned with the classification of Parkinson’s disease. Furthermore, many vision-based algorithms rely on human gait silhouettes or gait representations and employ traditional similarity-based methodologies. However, in this study, a novel approach is proposed in which spatiotemporal features are extracted via deep learning methods with a transfer learning paradigm. Following that, advanced deep learning approaches, including sequential models like gated recurrent unit (GRU), are used for additional analysis. The experimentation is performed on the publicly available KOA–PD–normal dataset comprising gait videos with various abnormalities, and the proposed model has the highest accuracy of approximately 94.81%.},
  archive      = {J_PEERJCS},
  author       = {Zeeshan Ali and Jihoon Moon and Saira Gillani and Sitara Afzal and Muazzam Maqsood and Seungmin Rho},
  doi          = {10.7717/peerj-cs.2857},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2857},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Vision-based approach to knee osteoarthritis and parkinson’s disease detection utilizing human gait patterns},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mining autonomous student patterns score on LMS within online higher education. <em>PEERJCS</em>, <em>11</em>, e2855. (<a href='https://doi.org/10.7717/peerj-cs.2855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher education institutions actively integrate information and communication technologies through learning management systems (LMS), which are crucial for online education. This study used data mining techniques to predict the autonomous scores of students in the online Law and Psychology programs at the Technical University of Manabi. The process involved data integration and selection of more than 16,000 records, preprocessing, transformation with RobustScaler, predictive modelling that included recursive feature elimination with cross-validation to select features (RFEcv), and hyperparameter fitting to achieve the best fit, and finally, evaluation of the models using metrics of root mean square error (RMSE), mean absolute error (MAE), and the coefficient of determination (R2). The feature selection framework suggested by RFEcv contributed to the performance of the models. The variables analyzed focused on download rate, homework submission rate, test performance rate, median daily accesses, median days of access per month, observation of comments on teacher-reviewed assignments, length of final exam, and not requiring the supplemental exam. Hyperparameter adjustment improved the performance of the models after applying RFEcv. The models evaluated showed minimal differences in RMSE ([0.5411 .. 0.6025]). The gradient boosting model achieved the best performance of R2 = 0.6693, MAE = 0.4041 and RMSE = 0.5411 with the Law online program data, as with the Psychology online program data, with an R2 = 0.6418, MAE = 0.4232 and RMSE = 0.6025, while the combination of both data sets reflected the best performance with the extreme gradient boosting (XGBoost) model with the values of R2 = 0.6294, MAE = 0.4295 and RMSE = 0.5985. Future research and implementations could include autonomous score data through plugins and reports integrated into LMSs. This approach may provide indicators of interest for understanding and improving online learning from a personalized, real-time perspective.},
  archive      = {J_PEERJCS},
  author       = {Ricardo Ordoñez-Avila and Jaime Meza and Sebastian Ventura},
  doi          = {10.7717/peerj-cs.2855},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2855},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Mining autonomous student patterns score on LMS within online higher education},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized deep learning approach for lung cancer detection using flying fox optimization and bidirectional generative adversarial networks. <em>PEERJCS</em>, <em>11</em>, e2853. (<a href='https://doi.org/10.7717/peerj-cs.2853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lung cancer remains one of the most prevalent and life-threatening diseases, often diagnosed at an advanced stage due to the challenges in early detection. Contributory factors include genetic mutations, smoking, alcohol consumption, and exposure to hazardous environmental conditions. Computer-aided diagnosis (CAD) systems have significantly improved early cancer detection, but limitations such as high-dimensional feature sets and overfitting issues persist. This study presents an optimised deep learning approach for lung cancer classification, integrating flying fox optimization (FFXO) for feature selection and bidirectional generative adversarial networks (Bi-GAN) for classification. The methodology consists of three key phases: (1) Data preprocessing, where missing values are handled using the multiple imputations by chain equation (MICE) technique and feature scaling is applied using standard and min-max scalers; (2) Feature selection, where the FFXO algorithm reduces feature dimensionality to enhance classification efficiency; and (3) Lung tumor classification, utilizing Bi-GAN to improve predictive accuracy. The proposed system was evaluated using key performance metrics—accuracy, precision, recall, and F1-score—and demonstrated superior performance to conventional models. Experimental results on a publicly available lung cancer dataset showed an accuracy of 98.7% highlighting the approach’s robustness in precise lung tumor classification. This study provides a novel framework for improving the reliability and efficiency of lung cancer detection, offering significant potential for clinical applications.},
  archive      = {J_PEERJCS},
  author       = {Manal Abdullah Alohali and Hamed Alqahtani and Shouki A. Ebad and Faiz Abdullah Alotaibi and Venkatachalam K. and Jaehyuk Cho},
  doi          = {10.7717/peerj-cs.2853},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2853},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimized deep learning approach for lung cancer detection using flying fox optimization and bidirectional generative adversarial networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on channel estimation based on joint perception and deep enhancement learning in complex communication scenarios. <em>PEERJCS</em>, <em>11</em>, e2852. (<a href='https://doi.org/10.7717/peerj-cs.2852'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary wireless communication systems, channel estimation and optimization have become increasingly pivotal with the growing number and complexity of devices. Communication systems frequently encounter multiple challenges, such as multipath propagation, signal fading, and interference, which may result in the degradation of communication quality, a reduction in data transmission rates, and even communication interruptions. Therefore, effective estimation and optimization of channels in complex communication environments are of paramount importance to ensure communication quality and enhance system performance. In this article, we address the intelligent, reflective surface (IRS)-assisted channel estimation problem and propose an intelligent channel estimation model based on the fusion of convolutional neural network (CNN) and gated recurrent unit (GRU) row features, utilizing the reinforcement learning Deep Deterministic Policy Gradient (DDPG) strategy for Channel Reconstruction Prediction and Generation Network (CRPG-Net). The framework initially acquires the received signal by converting the guide-frequency symbols at the transmitter into time-domain sequences to be transmitted, and after propagating through the direct channel and the IRS reflection channel, processes the data at the receiver. Subsequently, the spatial and temporal features in the received signal are extracted using the CRPG-Net model, with the adaptive optimization capability of the model enhanced by deep reinforcement learning. The introduction of reinforcement learning enables the model to continuously optimize decisions in dynamic channel environments, improve the robustness of channel estimation, and quickly adjust the IRS reflection parameters when the channel state changes to adapt to complex communication conditions. Experimental results demonstrate that the framework achieves significant channel estimation accuracy and robustness across several public datasets and real test scenarios, with the channel estimation error markedly smaller than that of traditional least squares (LS) and linear minimum mean square error (LMMSE) methods. This method introduces innovative techniques for channel estimation in intelligent communication systems, playing a crucial role in enhancing communication quality and overall system performance.},
  archive      = {J_PEERJCS},
  author       = {Xin Liu and Shanghong Zhao and Yanxia Liang and Shahid Karim},
  doi          = {10.7717/peerj-cs.2852},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2852},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on channel estimation based on joint perception and deep enhancement learning in complex communication scenarios},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative data storage with incentive mechanism for blockchain-based IoV. <em>PEERJCS</em>, <em>11</em>, e2849. (<a href='https://doi.org/10.7717/peerj-cs.2849'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the volume of data in the Internet of Vehicles (IoV) continues to grow, challenges such as insufficient storage capacity and potential privacy breaches become more pronounced. To address these issues, this article proposes a novel collaborative data storage scheme with an incentivization mechanism, termed Blockchain-Based Collaborative Data Storage with Incentive Mechanism for IoV (CDS-BIoV). The CDS-BIoV framework consists of vehicles, roadside units (RSUs), and cloud infrastructure. In the first phase, vehicles collect and transmit data to their nearest RSU nodes. To encourage active participation in data reception and storage, an incentive mechanism is introduced to motivate RSU nodes. Two algorithms are developed: the Incentive Mechanism Collaborative Data Storage Algorithm (I-CDSA) and the Data Offloading Algorithm (DOA). The I-CDSA uses a competitiveness matrix to incentivize RSU nodes to minimize storage consumption, while the DOA employs incentives to secure additional cloud storage for offloading data. Experimental results show that the CDS-BIoV scheme reduces storage consumption by up to 93% compared to the Generic Parallel Database (GPDB), particularly as the number of blocks increases, effectively alleviating storage capacity limitations.},
  archive      = {J_PEERJCS},
  author       = {Quan Shi and Lankai Wang and Chen Chen},
  doi          = {10.7717/peerj-cs.2849},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2849},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A collaborative data storage with incentive mechanism for blockchain-based IoV},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intersection collision prediction and prevention based on vehicle-to-vehicle (V2V) and cloud computing communication. <em>PEERJCS</em>, <em>11</em>, e2846. (<a href='https://doi.org/10.7717/peerj-cs.2846'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern transportation systems, the management of traffic safety has become increasingly critical as both the number and complexity of vehicles continue to rise. These systems frequently encounter multiple challenges. Consequently, the effective assessment and management of collision risks in various scenarios within transportation systems are paramount to ensuring traffic safety and enhancing road utilization efficiency. In this paper, we tackle the issue of intelligent traffic collision prediction and propose a vehicle collision risk prediction model based on vehicle-to-vehicle (V2V) communication and the graph attention network (GAT). Initially, the framework gathers vehicle trajectory, speed, acceleration, and relative position information via V2V communication technology to construct a graph representation of the traffic environment. Subsequently, the GAT model extracts interaction features between vehicles and optimizes the vehicle driving strategy through deep reinforcement learning (DRL), thereby augmenting the model’s decision-making capabilities. Experimental results demonstrate that the framework achieves over 80% collision recognition accuracy concerning true warning rate on both public and real-world datasets. The metrics for false detection are thoroughly analyzed, revealing the efficacy and robustness of the proposed framework. This method introduces a novel technological approach to collision prediction in intelligent transportation systems and holds significant implications for enhancing traffic safety and decision-making efficiency.},
  archive      = {J_PEERJCS},
  author       = {Min Zeng and Mohd Sani Mohamad Hashim and Mohd Nasir Ayob and Abdul Halim Ismail and Qiling Zang},
  doi          = {10.7717/peerj-cs.2846},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2846},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Intersection collision prediction and prevention based on vehicle-to-vehicle (V2V) and cloud computing communication},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ROM-pose: Restoring occluded mask image for 2D human pose estimation. <em>PEERJCS</em>, <em>11</em>, e2843. (<a href='https://doi.org/10.7717/peerj-cs.2843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) is a field focused on estimating human poses by detecting key points in images. HPE includes methods like top-down and bottom-up approaches. The top-down approach uses a two-stage process, first locating and then detecting key points on humans with bounding boxes, whereas the bottom-up approach directly detects individual key points and integrates them to estimate the overall pose. In this article, we address the problem of bounding box detection inaccuracies in certain situations using the top-down method. The detected bounding boxes, which serve as input for the model, impact the accuracy of pose estimation. Occlusions occur when a part of the target’s body is obscured by a person or object and hinder the model’s ability to detect complete bounding boxes. Consequently, the model produces bounding boxes that do not recognize occluded parts, resulting in their exclusion from the input used by the HPE model. To mitigate this issue, we introduce the Restoring Occluded Mask Image for 2D Human Pose Estimation (ROM-Pose), comprising a restoration model and an HPE model. The restoration model is designed to delineate the boundary between the target’s grayscale mask (occluded image) and the blocker’s grayscale mask (occludee image) using the specially created Whole Common Objects in Context (COCO) dataset. Upon identifying the boundary, the restoration model restores the occluded image. This restored image is subsequently overlaid onto the RGB image for use in the HPE model. By integrating occluded parts’ information into the input, the bounding box includes these areas during detection, thus enhancing the HPE model’s ability to recognize them. ROM-Pose achieved a 1.6% improvement in average precision (AP) compared to the baseline.},
  archive      = {J_PEERJCS},
  author       = {Yunju Lee and Jihie Kim},
  doi          = {10.7717/peerj-cs.2843},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2843},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {ROM-pose: Restoring occluded mask image for 2D human pose estimation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autonomous vehicle surveillance through fuzzy C-means segmentation and DeepSORT on aerial images. <em>PEERJCS</em>, <em>11</em>, e2835. (<a href='https://doi.org/10.7717/peerj-cs.2835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high mobility of uncrewed aerial vehicles (UAVs) has led to their usage in various computer vision applications, notably in intelligent traffic surveillance, where it enhances productivity and simplifies the process. Yet, there are still several challenges that must be resolved to automate these systems. One significant challenge is the accurate extraction of vehicle foregrounds in complex traffic scenarios. As a result, this article proposes a novel vehicle detection and tracking system for autonomous vehicle surveillance, which employs Fuzzy C-mean clustering to segment the aerial images. After segmentation, we employed the YOLOv4 deep learning algorithm, which is efficient in detecting small-sized objects in vehicle detection. Furthermore, an ID assignment and recovery algorithm based on Speed-Up Robust Feature (SURF) is used for multi-vehicle tracking across image frames. Vehicles are determined by counting in each image to estimate the traffic density at different time intervals. Finally, these vehicles were tracked using DeepSORT, which combines the Kalman filter with deep learning to produce accurate results. Furthermore, to understand the traffic flow direction, the path trajectories of each tracked vehicle is projected. Our proposed model demonstrates a noteworthy vehicle detection and tracking rate during experimental validation, attaining precision scores of 0.82 and 0.80 over UAVDT and KIT-AIS datasets for vehicle detection. For vehicle tracking, the precision is 0.87 over the UAVDT dataset and 0.83 for the KIT-AIS dataset.},
  archive      = {J_PEERJCS},
  author       = {Asifa Mehmood Qureshi and Moneerah Alotaibi and Sultan Refa Alotaibi and Dina Abdulaziz AlHammadi and Muhammad Asif Jamal and Ahmad Jalal and Bumshik Lee},
  doi          = {10.7717/peerj-cs.2835},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2835},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Autonomous vehicle surveillance through fuzzy C-means segmentation and DeepSORT on aerial images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Anticancer drug synergy prediction based on CatBoost. <em>PEERJCS</em>, <em>11</em>, e2829. (<a href='https://doi.org/10.7717/peerj-cs.2829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The research of cancer treatments has always been a hot topic in the medical field. Multi-targeted combination drugs have been considered as an ideal option for cancer treatment. Since it is not feasible to use clinical experience or high-throughput screening to identify the complete combinatorial space, methods such as machine learning models offer the possibility to explore the combinatorial space effectively. Methods In this work, we proposed a machine learning method based on CatBoost to predict the synergy scores of anticancer drug combinations on cancer cell lines, which utilized oblivious trees and ordered boosting technique to avoid overfitting and bias. The model was trained and tested using the data screened from NCI-ALMANAC dataset. The drugs were characterized with morgan fingerprints, drug target information, monotherapy information, and the cell lines were described with gene expression profiles. Results In the stratified 5-fold cross-validation, our method obtained excellent results, where, the receiver operating characteristic area under the curve (ROC AUC) is 0.9217, precision-recall area under the curve (PR AUC) is 0.4651, mean squared error (MSE) is 0.1365, and Pearson correlation coefficient is 0.5335. The performance is significantly better than three other advanced models. Additionally, when using SHapley Additive exPlanations (SHAP) to interpret the biological significance of the prediction results, we found that drug features played more prominent roles than cell line features, and genes associated with cancer development, such as PTK2, CCND1, and GNA11, played an important part in drug synergy prediction. Combining the experimental results, the model proposed in this study has a good prediction effect and can be used as an alternative method for predicting anticancer drug combinations.},
  archive      = {J_PEERJCS},
  author       = {Changheng Li and Nana Guan and Hongyi Zhang},
  doi          = {10.7717/peerj-cs.2829},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2829},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Anticancer drug synergy prediction based on CatBoost},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved salp swarm algorithm based optimization of mobile task offloading. <em>PEERJCS</em>, <em>11</em>, e2818. (<a href='https://doi.org/10.7717/peerj-cs.2818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The realization of computation-intensive applications such as real-time video processing, virtual/augmented reality, and face recognition becomes possible for mobile devices with the latest advances in communication technologies. This application requires complex computation for better user experience and real-time decision-making. However, the Internet of Things (IoT) and mobile devices have computational power and limited energy. Executing these computational-intensive tasks on edge devices may result in high energy consumption or high computation latency. In recent times, mobile edge computing (MEC) has been used and modernized for offloading this complex task. In MEC, IoT devices transmit their tasks to edge servers, which consecutively carry out faster computation. Methods However, several IoT devices and edge servers put an upper limit on executing concurrent tasks. Furthermore, implementing a smaller size task (1 KB) over an edge server leads to improved energy consumption. Thus, there is a need to have an optimum range for task offloading so that the energy consumption and response time will be minimal. The evolutionary algorithm is the best for resolving the multiobjective task. Energy, memory, and delay reduction together with the detection of the offloading task is the multiobjective to achieve. Therefore, this study presents an improved salp swarm algorithm-based Mobile Application Offloading Algorithm (ISSA-MAOA) technique for MEC. Results This technique harnesses the optimization capabilities of the improved salp swarm algorithm (ISSA) to intelligently allocate computing tasks between mobile devices and the cloud, aiming to concurrently minimize energy consumption, and memory usage, and reduce task completion delays. Through the proposed ISSA-MAOA, the study endeavors to contribute to the enhancement of mobile cloud computing (MCC) frameworks, providing a more efficient and sustainable solution for offloading tasks in mobile applications. The results of this research contribute to better resource management, improved user interactions, and enhanced efficiency in MCC environments.},
  archive      = {J_PEERJCS},
  author       = {Aishwarya R. and Mathivanan G.},
  doi          = {10.7717/peerj-cs.2818},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2818},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improved salp swarm algorithm based optimization of mobile task offloading},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG-based schizophrenia diagnosis using deep learning with multi-scale and adaptive feature selection. <em>PEERJCS</em>, <em>11</em>, e2811. (<a href='https://doi.org/10.7717/peerj-cs.2811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schizophrenia is a chronic and severe mental illness that significantly impacts the daily lives and work of those affected. Unfortunately, schizophrenia with negative symptoms often gets misdiagnosed, relying heavily on the clinician’s experience. There is a pressing need to develop an objective and effective diagnostic method for this specific type of schizophrenia. This paper proposes a new deep-learning method called Cascaded Atrous Convolutional Network with Adaptive Weight Fusion (CA-AWFM) for classifying schizophrenia from electroencephalogram (EEG) data that combines cascaded networks with atrous convolutions and an adaptive weight fusion module (AWFM). This is because schizophrenia involves intricate and subtle brain wave patterns that make it difficult to detect the disorder from EEG signals. As such, our model uses an “atrous” convolution operation to extract multi-scale temporal information and a cascade network structure that progressively improves the attribute representations across layers. For classification purposes, AWFM enables our model to modify the importance of features dynamically. We evaluated our technique using a publicly available dataset of EEG recordings acquired from patients who have schizophrenia and everyday individuals. The proposed model has significantly outperformed existing methods with a 99.5% accuracy rate. With the help of atrous convolutions, local and global dependencies within the EEGs can be effectively modeled in this way. At the same time, AWFM makes flexible prioritization of characteristics possible for improved classification performance. With such impressive figures achieved, it can be concluded that our approach should be considered as accurate enough for routine clinical use in identifying schizophrenic patients early on so they can receive intervention measures on time or when diagnosed late, then dealt with appropriately.},
  archive      = {J_PEERJCS},
  author       = {Alanoud Al Mazroa and Majdy M. Eltahir and Shouki A. Ebad and Faiz Abdullah Alotaibi and Venkatachalam K and Jaehyuk Cho},
  doi          = {10.7717/peerj-cs.2811},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2811},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {EEG-based schizophrenia diagnosis using deep learning with multi-scale and adaptive feature selection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class-weighted Dempster–Shafer in dual-level fusion for multimodal fake real estate listings detection. <em>PEERJCS</em>, <em>11</em>, e2797. (<a href='https://doi.org/10.7717/peerj-cs.2797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Detecting fake multimodal property listings is a significant challenge in online real estate platforms due to the increasing sophistication of fraudulent activities. The existing multimodal data fusion methods have several limitations and strengths in identifying fraudulent listings. Single-level fusion models whether at the feature, decision, or intermediate level struggle with balancing the contributions of different modalities leading to suboptimal decision-making. To address these problems, a dual-level fusion from multimodal for fake real estate listings detection is proposed. The dual-level fusion allows the integration of detailed features from text and image data to be performed at an early stage, followed by the metadata fusion at the decision stage in order to obtain a more comprehensive final classification. Furthermore, a new weighting scheme is introduced to optimize Dempster–Shafer in decision fusion to help the model achieve optimal performance and as a result, our method improves the classification. The Dempster–Shafer without class weightage lacks the flexibility to adapt to varying levels of uncertainty or importance across different classes. Methods In Class Weighted Dempster–Shafer in Dual Level Fusion (CWDS-DLF), we employ advanced models (XLNet for text and ResNet101 for images) for feature extraction and use the Dempster–Shafer theory for decision fusion. A new weighting scheme, based on Bayesian optimization, was used to assign optimal weights to the ‘fake’ and ‘not fake’ classes, thereby enhancing the Dempster–Shafer theory in the decision fusion process. Results The CWDS-DLF was evaluated on the property listing website dataset and achieved an F1 score of 96% and an accuracy of 93%. A t-test confirms the significance of these improvements (p < 0.05), demonstrating the effectiveness of our method in detecting fake property listings. Compared to other models, including 2D-convolutional neural network (CNN), XGBoost, and various multimodal approaches, our model consistently outperforms in precision, recall, and F1-score. This underscores the potential of integrating multimodal analysis with sophisticated fusion techniques to enhance the detection of fake property listings, ultimately improving consumer protection and operational efficiency in online real estate platforms.},
  archive      = {J_PEERJCS},
  author       = {Maifuza Mohd Amin and Nor Samsiah Sani and Mohammad Faidzul Nasrudin},
  doi          = {10.7717/peerj-cs.2797},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2797},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Class-weighted Dempster–Shafer in dual-level fusion for multimodal fake real estate listings detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAPE-ViT: Multimodal scene understanding with novel wavelet-augmented vision transformer. <em>PEERJCS</em>, <em>11</em>, e2796. (<a href='https://doi.org/10.7717/peerj-cs.2796'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces Multimodal Adaptive Patch Embedding with Vision Transformer (MAPE-ViT), a novel approach for RGB-D scene classification that effectively addresses fundamental challenges of sensor misalignment, depth noise, and object boundary preservation. Our framework integrates maximally stable extremal regions (MSER) with wavelet coefficients to create comprehensive patch embedding that capture both local and global image features. These MSER-guided patches, incorporating original pixels and multi-scale wavelet information, serve as input to a Vision Transformer, which leverages its attention mechanisms to extract high-level semantic features. The feature discrimination capability is further enhanced through optimization using the Gray Wolf algorithm. The processed features then flow into a dual-stream architecture, where an extreme learning machine handles multi-object classification, while conditional random fields (CRF) manage scene-level categorization. Extensive experimental results demonstrate the effectiveness of our approach, showing significant improvements in classification accuracy compared to existing methods. Our system provides a robust solution for RGB-D scene understanding, particularly in challenging conditions where traditional approaches struggle with sensor artifacts and noise.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Waqas Ahmed and Touseef Sadiq and Hameedur Rahman and Sulaiman Abdullah Alateyah and Mohammed Alnusayri and Mohammed Alatiyyah and Dina Abdulaziz AlHammadi},
  doi          = {10.7717/peerj-cs.2796},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2796},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MAPE-ViT: Multimodal scene understanding with novel wavelet-augmented vision transformer},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient gradient-based algorithm with descent direction for unconstrained optimization with applications to image restoration and robotic motion control. <em>PEERJCS</em>, <em>11</em>, e2783. (<a href='https://doi.org/10.7717/peerj-cs.2783'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel gradient-based algorithm designed to enhance the performance of optimization models, particularly in computer science applications such as image restoration and robotic motion control. The proposed algorithm introduces a modified conjugate gradient (CG) method, ensuring the CG coefficient, β κ, remains integral to the search direction, thereby maintaining the descent property under appropriate line search conditions. Leveraging the strong Wolfe conditions and assuming Lipschitz continuity, we establish the global convergence of the algorithm. Computational experiments demonstrate the algorithm’s superior performance across a range of test problems, including its ability to restore corrupted images with high precision and effectively manage motion control in a 3DOF robotic arm model. These results underscore the algorithm’s potential in addressing key challenges in image processing and robotics.},
  archive      = {J_PEERJCS},
  author       = {Sulaiman Mohammed Ibrahim and Aliyu M. Awwal and Maulana Malik and Ruzelan Khalid and Aida Mauziah Benjamin and Mohd Kamal Mohd Nawawi and Elissa Nadia Madi},
  doi          = {10.7717/peerj-cs.2783},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2783},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An efficient gradient-based algorithm with descent direction for unconstrained optimization with applications to image restoration and robotic motion control},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Using transformers and bi-LSTM with sentence embeddings for prediction of openness human personality trait. <em>PEERJCS</em>, <em>11</em>, e2781. (<a href='https://doi.org/10.7717/peerj-cs.2781'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding human personality traits is significant as it helps in decision making related to consumers’ behavior, career counselling, team building and top candidates’ selection for recruitment. Among various traits, openness is essential as it shows both diverse aspects of sensitive nature or intuitive nature. The individuals having a sensing nature tends to be more practical and prefer to focus on concrete information whereas the users having intuitive trait type is characterized by a focus on abstract ideas, creative thinking and future-oriented perspectives. In this research work, we aim to explore diverse natural language processing (NLP) based features and apply state of the art deep learning algorithms for openness trait prediction. Using standard Myers-Briggs Type Indicator (MBTI) dataset, we propose the use of the latest deep features of sentence embeddings which captures contextual semantics of the content to be used with deep learning models. For comparison, we explore textual features of Frequency-Inverse Document (TF-IDF) and parts of speech (POS) tagging with machine learning models and deep features of word2vec and global vectors for word representation (GloVe) with deep learning models. The comprehensive empirical analysis reveals that TF-IDF used with gradient boosting achieves high accuracy of 90% whereas, the deep feature of sentence embeddings when used and with deep model bidirectional long short-term memory (Bi-LSTM) achieves 90.5% accuracy. The best results have been achieved using the latest Transformer-based DistilBERT, which achieves the highest accuracy of 92% outperforming the existing studies in relevant literature.},
  archive      = {J_PEERJCS},
  author       = {Anam Naz and Hikmat Ullah Khan and Tariq Alsahfi and Mousa Alhajlah and Bader Alshemaimri and Ali Daud},
  doi          = {10.7717/peerj-cs.2781},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2781},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Using transformers and bi-LSTM with sentence embeddings for prediction of openness human personality trait},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HFSA: Hybrid feature selection approach to improve medical diagnostic system. <em>PEERJCS</em>, <em>11</em>, e2764. (<a href='https://doi.org/10.7717/peerj-cs.2764'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the presence of artificial intelligence methods, the diagnosis of patients can be done quickly and accurately. This article introduces a new diagnostic system (DS) that includes three main layers called the rejection layer (RL), selection layer (SL), and diagnostic layer (DL) to accurately diagnose cases suffering from various diseases. In RL, outliers can be removed using the genetic algorithm (GA). At the same time, the best features can be selected by using a new feature selection method called the hybrid feature selection approach (HFSA) in SL. In the next step, the filtered data is passed to the naive Bayes (NB) classifier in DL to give accurate diagnoses. In this work, the main contribution is represented in introducing HFSA as a new selection approach that is composed of two main stages; fast stage (FS) and accurate stage (AS). In FS, chi-square, as a filtering methodology, is applied to quickly select the best features while Hybrid Optimization Algorithm (HOA), as a wrapper methodology, is applied in AS to accurately select features. It is concluded that HFSA is better than other selection methods based on experimental results because HFSA can enable three different classifiers called NB, K-nearest neighbors (KNN), and artificial neural network (ANN) to provide the maximum accuracy, precision, and recall values and the minimum error value. Additionally, experimental results proved that DS, including GA as an outlier rejection method, HFSA as feature selection, and NB as diagnostic mode, outperformed other diagnosis models.},
  archive      = {J_PEERJCS},
  author       = {Asmaa H. Rabie and Mohammed Aldawsari and Ahmed I. Saleh and M. S. Saraya and Metwally Rashad},
  doi          = {10.7717/peerj-cs.2764},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2764},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {HFSA: Hybrid feature selection approach to improve medical diagnostic system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TASCI: Transformers for aspect-based sentiment analysis with contextual intent integration. <em>PEERJCS</em>, <em>11</em>, e2760. (<a href='https://doi.org/10.7717/peerj-cs.2760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we present a novel Transformer-Based Aspect-Level Sentiment Classification with Intent (TASCI) model, designed to enhance sentiment analysis by integrating aspect-level sentiment classification with intent analysis. Traditional sentiment analysis methods often overlook the nuanced relationship between the intent behind a statement and the sentiment expressed toward specific aspects of an entity. TASCI addresses this gap by first extracting aspects using a self-attention mechanism and then employing a Transformer-based model to infer the speaker’s intent from preceding sentences. This dual approach allows TASCI to contextualize sentiment analysis, providing a more accurate reflection of user opinions. We validate TASCI’s performance on three benchmark datasets: Restaurant, Laptop, and Twitter, achieving state-of-the-art results with an accuracy of 89.10% and a macro-F1 score of 83.38% on the Restaurant dataset, 84.81% accuracy and 78.63% macro-F1 score on the Laptop dataset, and 79.08% accuracy and 77.27% macro-F1 score on the Twitter dataset. These results demonstrate that incorporating intent analysis significantly enhances the model’s ability to capture complex sentiment expressions across different domains, thereby setting a new standard for aspect-level sentiment classification.},
  archive      = {J_PEERJCS},
  author       = {Hassan Nazeer Chaudhry and Farzana Kulsoom and Zahid Ullah Khan and Muhammad Aman and Sajid Ullah Khan and Abdullah Albanyan},
  doi          = {10.7717/peerj-cs.2760},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2760},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TASCI: Transformers for aspect-based sentiment analysis with contextual intent integration},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eternal-MAML: A meta-learning framework for cross-domain defect recognition. <em>PEERJCS</em>, <em>11</em>, e2757. (<a href='https://doi.org/10.7717/peerj-cs.2757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defect recognition tasks for industrial product suffer from a serious lack of samples, greatly limiting the generalizability of deep learning models. Addressing the imbalance of defective samples often involves leveraging pre-trained models for transfer learning. However, when these models, pre-trained on natural image datasets, are transferred to pixel-level defect recognition tasks, they frequently suffer from overfitting due to data scarcity. Furthermore, significant variations in the morphology, texture, and underlying causes of defects across different industrial products often lead to a degradation in performance, or even complete failure, when directly transferring a defect classification model trained on one type of product to another. The Model-Agnostic Meta-Learning (MAML) framework can learn a general representation of defects from multiple industrial defect recognition tasks and build a foundational model. Despite lacking sufficient training data, the MAML framework can still achieve effective knowledge transfer among cross-domain tasks. We noticed there exists serious label arrangement issues in MAML because of the random selection of recognition tasks, which seriously affects the performance of MAML model during both training and testing phase. This article proposes a novel MAML framework, termed as Eternal-MAML, which guides the update of the classifier module by learning a meta-vector that shares commonality across batch tasks in the inner loop, and addresses the overfitting phenomenon caused by label arrangement issues in testing phase for vanilla MAML. Additionally, the feature extractor in this framework combines the advantages of the Squeeze-and-Excitation module and Residual block to enhance training stability and improve the generalization accuracy of model transfer with the learned initialization parameters. In the simulation experiments, several datasets are applied to verified the cross-domain meta-learning performance of the proposed Eternal-MAML framework. The experimental results show that the proposed framework outperforms the state-of-the-art baselines in terms of average normalized accuracy. Finally, the ablation studies are conducted to examine how the primary components of the framework affect its overall performance. Code is available at https://github.com/zhg-SZPT/Eternal-MAML.},
  archive      = {J_PEERJCS},
  author       = {Jipeng Feng and Haigang Zhang and Zhifeng Wang},
  doi          = {10.7717/peerj-cs.2757},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2757},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Eternal-MAML: A meta-learning framework for cross-domain defect recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing phishing detection with dynamic optimization and character-level deep learning in cloud environments. <em>PEERJCS</em>, <em>11</em>, e2640. (<a href='https://doi.org/10.7717/peerj-cs.2640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As cloud computing becomes increasingly prevalent, the detection and prevention of phishing URL attacks are essential, particularly in the Internet of Vehicles (IoV) environment, to maintain service reliability. In such a scenario, an attacker could send misleading phishing links, potentially compromising the system’s functionality or, at worst, leading to a complete shutdown. To address these emerging threats, this study introduces a novel Dynamic Arithmetic Optimization Algorithm with Deep Learning-Driven Phishing URL Classification (DAOA-DLPC) model for cloud-enabled IoV infrastructure. The candidate’s research utilizes character-level embeddings instead of word embeddings, as the former can capture intricate URL patterns more effectively. These embeddings are integrated with a deep learning model, the Multi-Head Attention and Bidirectional Gated Recurrent Units (MHA-BiGRU). To improve precision, hyperparameter tuning has been done using DAOA. The proposed method offers a feasible solution for identifying the phishing URLs, and the method achieves computational efficiency through the attention mechanism and dynamic hyperparameter optimization. The need for this work comes from the observation that the traditional machine learning approaches are not effective in dynamic environments like phishing threat landscapes in a dynamic environment such as the one of phishing threats. The presented DLPC approach is capable of learning new forms of phishing attacks in real time and reduce false positives. The experimental results show that the proposed DAOA-DLPC model outperforms the other models with an accuracy of 98.85%, recall of 98.49%, and F1-score of 98.38% and can effectively detect safe and phishing URLs in dynamic environments. These results imply that the proposed model is useful in distinguishing between safe and unsafe URLs than the conventional models.},
  archive      = {J_PEERJCS},
  author       = {Vishnukumar Ravula and Mangayarkarasi Ramaiah},
  doi          = {10.7717/peerj-cs.2640},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2640},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing phishing detection with dynamic optimization and character-level deep learning in cloud environments},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SODU2-NET: A novel deep learning-based approach for salient object detection utilizing U-NET. <em>PEERJCS</em>, <em>11</em>, e2623. (<a href='https://doi.org/10.7717/peerj-cs.2623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. To address this challenge posed by complex backgrounds in salient object detection is crucial for advancing the field. This article proposes a novel deep learning-based architecture called SODU2-NET (Salient object detection U2-Net) for salient object detection that utilizes the U-NET base structure. This model addresses a gap in previous work that focused primarily on complex backgrounds by employing a densely supervised encoder-decoder network. The proposed SODU2-NET employs sophisticated background subtraction techniques and utilizes advanced deep learning architectures that can discern relevant foreground information when dealing with complex backgrounds. Firstly, an enriched encoder block with full feature fusion (FFF) with atrous spatial pyramid pooling (ASPP) varying dilation rates to efficiently capture multi-scale contextual information, improving salient object detection in complex backgrounds and reducing the loss of information during down-sampling. Secondly the block includes an attention module that refines the decoder, is constructed to enhances the detection of salient objects in complex backgrounds by selectively focusing attention on relevant features. This allows the model to reconstruct detailed and contextually relevant information, which is essential to determining salient objects accurately. Finally, the architecture has been improved by adding a residual block at the encoder end, which is responsible for both saliency prediction and map refinement. The proposed network is designed to learn the transformation between input images and ground truth, enabling accurate segmentation of salient object regions with clear borders and accurate prediction of fine structures. SODU2-NET is demonstrated to have superior performance in five public datasets, including DUTS, SOD, DUT OMRON, HKU-IS, PASCAL-S, and a new real world dataset, the Changsha dataset. Based on a comparative assessment of the model FCN, Squeeze-net, Deep Lab, Mask R-CNN the proposed SODU2-NET is found and achieve an improvement of precision (6%), recall (5%) and accuracy (3%). Overall, approach shows promise for improving the accuracy and efficiency of salient object detection in a variety of settings.},
  archive      = {J_PEERJCS},
  author       = {Hyder Abbas and Shen Bing Ren and Muhammad Asim and Syeda Iqra Hassan and Ahmed A. Abd El-Latif},
  doi          = {10.7717/peerj-cs.2623},
  journal      = {PeerJ Computer Science},
  month        = {5},
  pages        = {e2623},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SODU2-NET: A novel deep learning-based approach for salient object detection utilizing U-NET},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DMSA-net: A deformable multiscale adaptive classroom behavior recognition network. <em>PEERJCS</em>, <em>11</em>, e2876. (<a href='https://doi.org/10.7717/peerj-cs.2876'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the intelligent transformation of education, accurate recognition of students’ classroom behavior has become one of the key technologies for enhancing the quality of instruction and the efficacy of learning. However, in the recognition of target behavior in real classroom scenarios, due to the use of wide-angle or panoramic images for image acquisition, students in the back row are far away from monitoring devices, and their subtle body movements such as the small opening and closing of the mouth (to determine whether they are speaking), fine finger operations (to distinguish between reading books or operating mobile phones) are difficult to recognize. Moreover, there are occlusions and scale differences in the front and back rankings, which can easily cause confusion and interference with target features in the detection process, greatly limiting the accurate recognition ability of existing visual algorithms for classroom behavior. This article proposes a deformable multiscale adaptive classroom behavior recognition network. To improve the network’s capacity to model minute behavioral phenomena, the backbone section introduces a deformable self-attention dattention module, dynamically modifying the receptive field’s geometry to enhance the model’s concentration on the region of interest. To improve the network’s capacity for feature extraction and integration of behavior occlusion and classroom behavior at different scales, a proposal has been put forward the Multiscale Attention Feature Pyramid Structure (MSAFPS), to achieve multi-level feature aggregation after multiscale feature fusion, reducing the impact of mutual occlusion and scale differences in classroom behavior between front and back rows. In the detect section, we adopt the Wise Intersection Over Union (Wise-IoU) loss as our loss criterion, augmenting the evaluation framework with richer contextual cues to broaden its scope and elevate the network’s detection prowess. Extensive experimentation reveals that our proposed method outperforms rival algorithms on two widely adopted benchmark datasets: SCB-Dataset3-S (the Student Classroom Behavior Dataset–https://github.com/Whiffe/SCB-dataset) and we created object detection dataset DataMountainSCB (https://github.com/Chunyu-Dong/DataFountainSCB1) containing six types of behaviors.},
  archive      = {J_PEERJCS},
  author       = {Chunyu Dong and Jing Liu and Shenglong Xie},
  doi          = {10.7717/peerj-cs.2876},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2876},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DMSA-net: A deformable multiscale adaptive classroom behavior recognition network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Employing SAE-GRU deep learning for scalable botnet detection in smart city infrastructure. <em>PEERJCS</em>, <em>11</em>, e2869. (<a href='https://doi.org/10.7717/peerj-cs.2869'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of Internet of Things (IoT) devices in smart cities has revolutionized urban infrastructure while escalating the risk of botnet attacks that threaten essential services and public safety. This research addresses the critical need for intrusion detection and mitigation systems by introducing a novel hybrid deep learning model, Stacked Autoencoder–Gated Recurrent Unit (SAE-GRU), specifically designed for IoT networks in smart cities. The study targets the dual challenges of processing high-dimensional data and recognizing temporal patterns to identify and mitigate botnet activities in real time. The methodology integrates Stacked Autoencoders for reducing dimensionality and gated recurrent units for analyzing sequential data to ensure both accuracy and efficiency. An emulated smart city environment with diverse IoT devices and communication protocols provided a realistic testbed for evaluating the model. Results demonstrate significant improvements in detection performance with an average accuracy of 98.65 percent and consistently high precision and recall values. These findings enhance the understanding of IoT security by offering a scalable and resource-efficient solution for botnet detection. The functional investigation establishes a foundation for future research into adaptive security mechanisms that address emerging threats and highlights the practical potential of advanced deep learning techniques in safeguarding next-generation smart city ecosystems.},
  archive      = {J_PEERJCS},
  author       = {Usman Tariq and Tariq Ahamed Ahanger},
  doi          = {10.7717/peerj-cs.2869},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2869},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Employing SAE-GRU deep learning for scalable botnet detection in smart city infrastructure},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fish species identification on low resolution—a study with enhanced super-resolution generative adversarial network (ESRGAN), YOLO and VGG-16. <em>PEERJCS</em>, <em>11</em>, e2860. (<a href='https://doi.org/10.7717/peerj-cs.2860'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An intelligent detection and recognition model for the fish species from camera footage is urgently required as fishery contributes to a large portion of the world economy, and these kinds of advanced models can aid fishermen on a large scale. Such models incorporating a pick-and-place machine can be beneficial to sorting different fish species in bulk without human intervention, significantly reducing costs for large-scale fishing industries. Existing methods for detecting and recognizing fish species have many limitations, such as limited scalability, detection accuracy, failure to detect multiple species, degraded performance at a lower resolution, or pinpointing the exact location of the fish. Modifying the head of a compelling deep learning model, namely VGG-16, with pre-trained weights, can be used to detect both the species of the fish and find the exact location of the fish in an image by implementing a modified You Only Look Once (YOLO) to incorporate the bounding box regression head. We have proposed using the Enhanced Super Resolution Generative Adversarial Network (ESRGAN) algorithm and the proposed neural network to amplify the image resolution by a factor of 4. With this method, an overall detection accuracy of 96.5% has been obtained. The experiment has been conducted based on a total of 9,460 images spread across nine species. After further improving the model, a pick-and-place machine could be integrated to quickly sort the fish according to their species in different large-scale fish industries.},
  archive      = {J_PEERJCS},
  author       = {Subhrangshu Adhikary and Saikat Banerjee and Rajani Singh and Ashutosh Dhar Dwivedi},
  doi          = {10.7717/peerj-cs.2860},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2860},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fish species identification on low resolution—a study with enhanced super-resolution generative adversarial network (ESRGAN), YOLO and VGG-16},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scaling laws for haralick texture features of linear gradients. <em>PEERJCS</em>, <em>11</em>, e2856. (<a href='https://doi.org/10.7717/peerj-cs.2856'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel analytical framework for understanding the relationship between the image gradients and the symmetries of the Gray Level Co-occurrence Matrix (GLCM). Analytical expression for four key features–sum average (SA), sum variance (SV), difference variance (DV), and entropy–were derived to capture their dependence on image’s gray-level quantization (Ng), the gradient magnitude (∇), and the displacement vector (d) through the corresponding GLCM. Scaling laws obtained from the exact analytical dependencies of Haralick features on Ng, ∇ and |d| show that SA and DV scale linearly with Ng, SV scales quadratically, and entropy follows a logarithmic trend. The scaling laws allow a consistent derivation of normalization factors that make Haralick features independent of the quantization scheme Ng. Numerical simulations using synthetic one-dimensional gradients validated our theoretical predictions. This theoretical framework establishes a foundation for consistent derivation of analytic expressions and scaling laws for Haralick features. Such an approach would streamline texture analysis across datasets and imaging modalities, enhancing the portability and interpretability of Haralick features in machine learning and medical imaging applications.},
  archive      = {J_PEERJCS},
  author       = {Sorinel A. Oprisan and Ana Oprisan},
  doi          = {10.7717/peerj-cs.2856},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2856},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Scaling laws for haralick texture features of linear gradients},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Human pose estimation in physiotherapy fitness exercise correction using novel transfer learning approach. <em>PEERJCS</em>, <em>11</em>, e2854. (<a href='https://doi.org/10.7717/peerj-cs.2854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective To introduce and evaluate an efficient neural network approach for human pose estimation and correction during physical therapy exercises using wearable sensor data. Methods We leveraged benchmark data consisting of 276,625 records from wearable inertial and magnetic sensors. A novel method termed Random Forest Long Short-Term Memory (RFL), which integrates long short-term memory and Random Forest neural networks, was implemented for transfer feature engineering. The smartphone sensor data was used to generate new temporal and probabilistic features. These features were then utilized in machine learning methods to classify physical therapy exercises. Rigorous experiments, including k-fold validation and hyperparameter optimization, were conducted to validate the performance of the RFL approach. Results The RFL approach demonstrated superior performance, achieving a remarkable 99% accuracy with the Random Forest method. The rigorous experiments confirmed the efficacy and reliability of the method in classifying physical therapy exercises. Conclusions The proposed RFL method introduces a novel feature generation approach enhancing the accuracy of physical therapy exercise classification and correction. This innovative integration not only improves rehabilitation monitoring but also paves the way for more adaptive and intelligent physiotherapy assistance systems. By leveraging sensor data and advanced machine learning techniques, it has the potential to mitigate risks associated with disabilities and major diseases, thereby offering a feasible alternative to frequent clinic visits for consistent therapist guidance.},
  archive      = {J_PEERJCS},
  author       = {Aisha Naseer and Ali Raza and Hadeeqa Afzal and Aseel Smerat and Norma Latif Fitriyani and Yeonghyeon Gu and Muhammad Syafrudin},
  doi          = {10.7717/peerj-cs.2854},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2854},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Human pose estimation in physiotherapy fitness exercise correction using novel transfer learning approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging resource gaps in cross-lingual sentiment analysis: Adaptive self-alignment with data augmentation and transfer learning. <em>PEERJCS</em>, <em>11</em>, e2851. (<a href='https://doi.org/10.7717/peerj-cs.2851'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-lingual sentiment analysis plays a crucial role in accurately interpreting emotions across diverse linguistic contexts. However, performance disparities remain a major challenge, particularly in fewer-resource (including medium-resource and low-resource) languages. This study proposes an adaptive self-alignment framework for large language models, incorporating novel data augmentation techniques and transfer learning strategies to mitigate resource imbalances. Comprehensive experiments conducted on 11 languages demonstrate that our approach consistently surpasses state-of-the-art baselines, achieving an average F1-score improvement of 7.35 points. Notably, our method exhibits exceptional effectiveness in fewer-resource languages, significantly narrowing the performance gap between fewer- and high-resource settings. With robust domain adaptation capabilities and strong potential for real-world industrial applications, this research establishes a new benchmark for multilingual sentiment analysis, advancing the development of more inclusive and equitable natural language processing solutions.},
  archive      = {J_PEERJCS},
  author       = {Li Chen and Shifeng Shang and Yawen Wang},
  doi          = {10.7717/peerj-cs.2851},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2851},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Bridging resource gaps in cross-lingual sentiment analysis: Adaptive self-alignment with data augmentation and transfer learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can executives’ digital background develop the level of AI utilization in enterprises. <em>PEERJCS</em>, <em>11</em>, e2848. (<a href='https://doi.org/10.7717/peerj-cs.2848'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital talent has emerged as a pivotal resource for advancing artificial intelligence (AI) within enterprises, and it constitutes a critical digital asset that firms prioritize in their AI development strategies, aiming to attract and retain such talent. To elucidate the interconnections and mechanisms linking the digital backgrounds of executives with the level of AI utilization in enterprises, this study undertakes an examination of the relationship between these two variables, drawing on a sample of Chinese A-share listed companies spanning from 2012 to 2022. The research findings are as follows: (1) The presence of a substantial number of executives with digital backgrounds within a company significantly enhances the development of AI utilization within the enterprise. (2) The influence of executives’ digital backgrounds on the level of AI utilization in enterprises is modulated through the enhancement of the enterprise’s total factor productivity. (3) Executives’ digital backgrounds elevate the level of AI utilization by bolstering their proficiency in applying digital technology. (4) Heterogeneity analysis, conducted based on the nature of enterprise property rights, geographical regions, and technological proficiency, reveals that the positive effect of executives’ digital backgrounds on AI utilization is more pronounced in private enterprises compared to state-owned enterprises. Regionally, the impact of executives’ digital backgrounds on AI utilization diminishes progressively from the eastern to the central and western regions. Robustness checks, including the substitution of key variables, model alteration, the application of the instrumental variable method, and the Heckman two-stage method, confirm the persistence of these findings. This article contributes to the understanding of the impact and pathways through which executives’ digital backgrounds influence AI utilization in enterprises against the backdrop of high-quality economic development. It enriches the scholarly discourse on executives’ digital backgrounds and AI utilization in enterprises, offering both theoretical support and practical insights for the advancement of AI within enterprises.},
  archive      = {J_PEERJCS},
  author       = {Ruichao Yu and Linrong Wu and Guiying Li and Zixin Wang},
  doi          = {10.7717/peerj-cs.2848},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2848},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Can executives’ digital background develop the level of AI utilization in enterprises},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards interpretable drug interaction prediction via dual-stage attention and bayesian calibration with active learning. <em>PEERJCS</em>, <em>11</em>, e2847. (<a href='https://doi.org/10.7717/peerj-cs.2847'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Drug-drug interactions (DDIs) account for 17–23% of adverse drug reactions leading to hospitalization, with over 74,000 DDI-related events reported in the FDA Adverse Event Reporting System (FAERS) during 2023. While recent computational methods focus on improving prediction accuracy, they suffer from high false-positive rates (>45%) and often function as black-box models without biological interpretability. Methods We propose Dual-stage attention and Bayesian calibration with active learning Drug-Drug Interaction (DABI-DDI), a novel framework integrating: (1) A dual-stage attention mechanism with LSTM networks for capturing temporal dependencies in drug interactions, (2) a Bayesian calibration approach with beta-binomial modeling for refining interaction signals and reducing false positives, (3) an active learning strategy for efficient sample selection, and (4) a network pharmacology component linking drug interactions to underlying biological mechanisms. The model was validated using data from FAERS, DrugBank, and STRING databases, with comprehensive evaluation on both computational performance and biological interpretability. Results DABI-DDI achieved superior performance (AUC = 0.947, PR_AUC = 0.944). Bayesian calibration improved adverse event detection accuracy (94% vs. 54% AUC), while network pharmacology revealed key molecular mechanisms through enzyme-transporter interactions. Ablation studies demonstrated each component’s significance, with active learning maintaining performance while reducing training data requirements. Conclusion We present DABI-DDI, an integrated feature extraction framework that successfully addresses key challenges in DDIs prediction through three major innovations: Temporal pattern recognition, reducing false positives, and biological interpretability. Most importantly, the framework demonstrates strong clinical applicability by efficiently identifying high-risk drug combinations while providing mechanistic insights through enzyme-transporter pathway analysis. This approach bridges the gap between computational prediction and clinical understanding, offering a promising tool for safer drug combination therapy.},
  archive      = {J_PEERJCS},
  author       = {Rongpei Li and Yufang Zhang and Heqi Sun and Shenggeng Lin and Guihua Jia and Yitian Fang and Chen Zhang and Xiaotong Song and Jianwei Zhao and Lyubin Hu and Yajing Yuan and Xueying Mao and Jiayi Li and Aman Kaushik and Dandan An and Dongqing Wei},
  doi          = {10.7717/peerj-cs.2847},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2847},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Towards interpretable drug interaction prediction via dual-stage attention and bayesian calibration with active learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive fusion-based data augmentation method for abstract dialogue summarization. <em>PEERJCS</em>, <em>11</em>, e2845. (<a href='https://doi.org/10.7717/peerj-cs.2845'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dialogue summarization is necessary for information retrieval, and the training of abstract dialogue summarization models heavily rely on large amounts of labeled data. However, manual summarization of long dialogue is labor-costing and time-consuming. To solve this problem, this article proposes a data augmentation method for dialogue summary based on adaptive augmentation fusion (AAF), integrating the strengths of both Minor Perturbation Augmentation (MPA) and Semantic Reconstructive Augmentation (SRA) to balance model learning effectiveness and generalization capabilities. We first integrated existing enhancement methods to address the problem of insufficient annotated data for dialogue summarization. The experimental results on both the DialogSum and SAMSum datasets demonstrate that the AAF method achieves significant improvements in ROUGE scores under resource-constrained conditions, outperforming baseline approaches. Furthermore, it was validated that the selection of the amount of augmented data has a significant impact on model training results under resource-constrained conditions. We have publically released our code at https://github.com/alolke/AAF.},
  archive      = {J_PEERJCS},
  author       = {Weihao Li and Dan Jiang and Han Zhang and Kejing Xiao and Shaozhong Cao},
  doi          = {10.7717/peerj-cs.2845},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2845},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An adaptive fusion-based data augmentation method for abstract dialogue summarization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards efficient glaucoma screening with modular convolution-involution cascade architecture. <em>PEERJCS</em>, <em>11</em>, e2844. (<a href='https://doi.org/10.7717/peerj-cs.2844'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated glaucoma detection from retinal fundus images plays a crucial role in facilitating early intervention and improving the management of this progressive ocular condition. Although convolutional neural networks (CNNs) have significantly advanced image analysis, current CNN-based models encounter two major limitations. First, they rely primarily on convolutional operations, which restrict the ability to capture cross-channel correlations effectively due to the channel-specific focus of these operations. Second, they often depend on fully-connected (FC) layers for classification, which can introduce unnecessary complexity and limit adaptability, potentially impacting overall classification performance. This study introduces the Modular Convolution-Involution Cascade Network (MCICNet), an innovative CNN architecture designed to address these challenges in the context of glaucoma detection. The model employs a combination of convolution and involution operations in a cascade structure, allowing for the effective capture of inter-channel dependencies within the feature extraction process. Furthermore, the classification phase integrates light gradient boosting machine (LightGBM) as a replacement for traditional FC layers, offering enhanced precision and generalization while reducing model complexity. Extensive experiments conducted on the LAG and ACRIMA datasets demonstrate that MCICNet achieves significant improvements compared to existing CNN and transformer-based models. The model attained a classification accuracy of 95.6% on the LAG dataset and 96.2% on ACRIMA, outperforming nine widely used CNN architectures (AlexNet, MobileNetV2, SqueezeNet, ResNet18, GoogLeNet, DenseNet121, EfficientNetB0, ShuffleNet, and VGG16), as well as three transformer-based models (ViT, MaxViT, and SwinT). Additionally, MCICNet showed superior performance over its variant without involution (MCICNet-NoInvolution). With only 0.9 million parameters, MCICNet demonstrates substantial efficiency in resource utilization alongside its high learning capability, establishing it as an advanced and computationally efficient solution for glaucoma detection.},
  archive      = {J_PEERJCS},
  author       = {Mohamed Mouhafid and Yatong Zhou and Chunyan Shan and Zhitao Xiao},
  doi          = {10.7717/peerj-cs.2844},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2844},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Towards efficient glaucoma screening with modular convolution-involution cascade architecture},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social-aware trajectory prediction using goal-directed attention networks with egocentric vision. <em>PEERJCS</em>, <em>11</em>, e2842. (<a href='https://doi.org/10.7717/peerj-cs.2842'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel social-goal attention networks (SGANet) model that employs a vision-based multi-stacked neural network framework to predict multiple future trajectories for both homogeneous and heterogeneous road users. Unlike existing methods that focus solely on one dataset type and treat social interactions, temporal dynamics, destination point, and uncertainty behaviors independently, SGANet integrates these components into a unified multimodal prediction framework. A graph attention network (GAT) captures socially-aware interaction correlation, a long short-term memory (LSTM) network encodes temporal dependencies, a goal-directed forecaster (GDF) estimates coarse future goals, and a conditional variational autoencoder (CVAE) generates multiple plausible trajectories, with multi-head attention (MHA) and feed-forward networks (FFN) refining the final multimodal trajectory prediction. Evaluations on homogeneous datasets (JAAD and PIE) and the heterogeneous TITAN dataset demonstrate that SGANet consistently outperforms previous benchmarks across varying prediction horizons. Extensive experiments highlight the critical role of socially-aware interaction weighting in capturing road users’ influence on ego-vehicle maneuvers while validating the effectiveness of each network component, thereby demonstrating the advantages of multi-stacked neural network integration for trajectory prediction. The dataset is available at https://usa.honda-ri.com/titan.},
  archive      = {J_PEERJCS},
  author       = {Lia Astuti and Chui-Hong Chiu and Yu-Chen Lin and Ming-Chih Lin},
  doi          = {10.7717/peerj-cs.2842},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2842},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Social-aware trajectory prediction using goal-directed attention networks with egocentric vision},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reconstructing unreadable QR codes: A deep learning based super resolution strategy. <em>PEERJCS</em>, <em>11</em>, e2841. (<a href='https://doi.org/10.7717/peerj-cs.2841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quick-response (QR) codes have become an integral component of the digital transformation process, facilitating fast and secure information sharing across various sectors. However, factors such as low resolution, misalignment, panning and rotation, often caused by the limitations of scanning devices, can significantly impact their readability. These distortions prevent reliable extraction of embedded data, increase processing times and pose potential security risks. In this study, four super-resolution models Enhanced Deep Super Resolution (ESDR) network, Very Deep Super Resolution (VDSR) network, Efficient Sub-Pixel Convolutional Network (ESPCN) and Super Resolution Convolutional Neural Network (SRCNN) are used to mitigate resolution loss, rotation errors and misalignment issues. To simulate scanner-induced distortions, a dataset of 16,000 computer-generated QR codes with various filters was used. In addition, super-resolution models were applied to 4,593 QR codes that OpenCV’s QRCodeDetector function could not decode in real-world scans. The results showed that EDSR, VDSR, ESPCN and SRCNN successfully read 4,261, 4,229, 4,255 and 4,042 of these QR codes, respectively. Furthermore, the EDSR, VDSR, ESPCN and SRCNN models trained by OpenCV’s deep learning-based WeChat QR Code Detector function to read 2,899 QR codes that were initially unreadable and simulated on the computer were able to successfully read 2,891, 2,884, 2,433 and 2,560 of them, respectively. These findings show that super-resolution models can effectively improve the readability of degraded or low-resolution QR codes.},
  archive      = {J_PEERJCS},
  author       = {Yasin Sancar},
  doi          = {10.7717/peerj-cs.2841},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2841},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Reconstructing unreadable QR codes: A deep learning based super resolution strategy},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMPACT: An interactive multi-disease prevention and counterfactual treatment system using explainable AI and a multimodal LLM. <em>PEERJCS</em>, <em>11</em>, e2839. (<a href='https://doi.org/10.7717/peerj-cs.2839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-disease conditions strain the body’s defenses, complicating recovery and increasing mortality risk. Therefore, effective concurrent prevention of multiple diseases is essential for mitigating complications and improving overall well-being. Explainable artificial intelligence (XAI) with an advanced multimodal large language model (LLM) can create an interactive system enabling the general public to engage in natural language without any specialized knowledge prerequisites. Counterfactual explanation, an XAI method, offers valuable insights by suggesting adjustments to patient features to minimize disease risks. However, addressing multiple diseases simultaneously poses challenging barriers. This article proposes an interactive multi-disease prevention system that uses Google Gemini Pro, a multimodal LLM, and a non-dominated sorting genetic algorithm, namely NSGA-II, to overcome such problems. This system recommends changes in feature values to concurrently minimize the risk of diseases such as heart attacks and diabetes. The system facilitates personalized feature value selection, significantly reducing disease attack probabilities to as low as possible. Such an approach holds the potential to simultaneously address the unresolved issue of preventing and managing multiple diseases for the general public.},
  archive      = {J_PEERJCS},
  author       = {Prasant Kumar Mohanty and Sharmila Anand John Francis and Rabindra Kumar Barik and K. Hemant Kumar Reddy and Diptendu Sinha Roy and Manob Jyoti Saikia},
  doi          = {10.7717/peerj-cs.2839},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2839},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IMPACT: An interactive multi-disease prevention and counterfactual treatment system using explainable AI and a multimodal LLM},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multipath subflow transmission scheduling optimization algorithm based on cost-performance balance. <em>PEERJCS</em>, <em>11</em>, e2838. (<a href='https://doi.org/10.7717/peerj-cs.2838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a cost-performance balance algorithm for multipath data transmission within an Software Defined Network (SDN)-5G-Multipath Transmission Control Protocol (MPTCP) network framework. A unified communication interface is designed to schedule transmission paths, optimizing data flow allocation dynamically. To achieve a balance between network throughput and transmission costs, traffic volume and associated expenses are mapped into physical and virtual buffer queues for real-time substream updates. The relationship between unreceived subflows and consumption costs is mathematically modelled, with both parameters represented using vector matrices. Lyapunov stability theory is applied to determine the optimal cost-performance balance. In contrast, key evaluation metrics—including substream transmission efficiency, consumption expenditure, and overall balance control—are introduced to assess performance. Comparative analysis against classical price balance control algorithms demonstrates that the proposed strategy significantly enhances data transmission efficiency while reducing costs. Experimental results validate the effectiveness of the model in achieving cost-performance balance control for multi-channel transmission of large-scale files (ranging from 2 to 20 GB) under varying network conditions. The findings highlight the potential of this approach for optimizing high-performance, cost-efficient data transmission in next-generation communication networks.},
  archive      = {J_PEERJCS},
  author       = {Xinyu Sun},
  doi          = {10.7717/peerj-cs.2838},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2838},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multipath subflow transmission scheduling optimization algorithm based on cost-performance balance},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel user-centric happiness model for personalized tour recommendations. <em>PEERJCS</em>, <em>11</em>, e2837. (<a href='https://doi.org/10.7717/peerj-cs.2837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel personalized tour recommendation model, the Happiness Model (HM), is presented. The HM optimizes itineraries by considering traveler satisfaction as a function of time and maximizing it over the trip duration. The model integrates the Item Constraints Data Model (ICDM) to reduce data dimensionality and search space. By considering various activities within different points of interest (POIs) and minimizing wasted time, the HM overcomes the limitations of existing methods. Unlike existing POI-centric models, the HM is time-centric, creating tour recommendations that maximize user satisfaction throughout the trip. Experimental results demonstrate the model’s effectiveness in generating personalized tour recommendations aligned with user preferences. The HM achieves an average satisfaction score of 0.85 across multiple datasets, outperforming traditional models such as the Time-Dependent Orienteering Problem with Time Windows (TOPTW), which achieves an average score of 0.72. Additionally, the HM reduces waiting times by 30% and increases the number of recommended POIs by 20% compared to existing methods. These results highlight the HM’s ability to provide more efficient and enjoyable travel experiences.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Alatiyyah},
  doi          = {10.7717/peerj-cs.2837},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2837},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel user-centric happiness model for personalized tour recommendations},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Numerical dispersed flow simulation of fire-flake particle dynamics and its learning representation. <em>PEERJCS</em>, <em>11</em>, e2836. (<a href='https://doi.org/10.7717/peerj-cs.2836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose methods for simulating the detailed flow of dispersed fire-flake particles in response to the movement of a flame, using chaotic advection and various buoyant flow techniques. Furthermore, we utilize these techniques to gather a synthetic dataset of detailed fire-flake particles and extend the solver to represent the movement of fire-flake particles based on learning-based approaches. Fire-flake particles not only exhibit unique and complex movements on their own, but they are also significantly influenced by the movement of the flame and the surrounding airflow. Modeling the flow of fire-flake particles realistically is challenging due to their chaotic and constantly changing nature. Instead of explicitly modeling the complex fire-flake particles in the flame based on fluid mechanics, this article efficiently approximates the chaotic motion of fire-flake particles using two approaches: 1) chaotic advection to simulate the flow and 2) controlled buoyant flow, which varies based on the temperature and lifespan of the fire-flake particles. Additionally, we collect a fire-flake dataset through this simulation and extends the solver to learn the representation of fire-flake motion using neural networks. During the advection process of fire-flake particles, a new stochastic solver is used to calculate the subgrid interactions between them. In this article, not only we propose algorithms that can express these techniques through numerical simulation, but we also extend this solver using artificial intelligence techniques to enable learning representation. By using the proposed technique, it is possible to efficiently simulate fire-flake particles with various movements in chaotic regions, and it allows for more detailed representation of fire-flake particles compared to existing methods. Unlike the typical random walk approach that adds noise randomly to the movement, our method considers the size and direction of the flame. This allows us to express fire-flake particles stably in most scenes without the need for parameter adjustments.},
  archive      = {J_PEERJCS},
  author       = {Jong-Hyun Kim and Jung Lee},
  doi          = {10.7717/peerj-cs.2836},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2836},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Numerical dispersed flow simulation of fire-flake particle dynamics and its learning representation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A feature selection method utilizing path accumulation cost, redundancy minimization, and interaction maximization for the diagnosis of coronary heart disease. <em>PEERJCS</em>, <em>11</em>, e2834. (<a href='https://doi.org/10.7717/peerj-cs.2834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Coronary heart disease (CHD) is a major cause of mortality worldwide, with an increasing trend of affecting younger populations. The asymptomatic early stages and rapid progression of CHD make diagnosis challenging, necessitating efficient diagnostic approaches. Methods We propose a novel algorithm that focuses on accumulating soft path costs to discern crucial indicators from extensive diagnostic tests, aiming to improve early CHD identification. Our approach emphasizes feature interaction using an interaction accumulation evaluation function to identify features with maximal interaction and minimal redundancy. A new stopping criterion based on information gain ratio is also introduced. Results Experimental outcomes demonstrate that our algorithm outperforms several classical algorithms in terms of classification accuracy and feature dimension reduction, while also identifying highly correlated feature subsets. Conclusion The proposed approach offers an efficient solution for early detection of CHD by identifying critical indicators, reducing diagnostic complexity, and improving predictive accuracy, thus potentially leading to more effective CHD management.},
  archive      = {J_PEERJCS},
  author       = {Jiayao Jiang and Zheng Yue and Hongling Zhu and Yan Wang and Hongsen Cai and Wenguang Hou},
  doi          = {10.7717/peerj-cs.2834},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2834},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A feature selection method utilizing path accumulation cost, redundancy minimization, and interaction maximization for the diagnosis of coronary heart disease},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCA-YOLOv8n: A real-time and efficient fruit chunks detection algorithm for meal-assistance robot. <em>PEERJCS</em>, <em>11</em>, e2832. (<a href='https://doi.org/10.7717/peerj-cs.2832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The advancement of assistive technologies for individuals with disabilities has increased the demand for efficient and accurate object detection algorithms, particularly in meal-assistance robots designed to identify and handle food items such as fruit chunks. However, existing algorithms for fruit chunk detection often suffer from prolonged inference times and insufficient accuracy. Methods We propose an improved YOLOv8n algorithm optimized for real-time, high-accuracy fruit chunk detection. The Universal Inverted Bottleneck (UIB) module has been integrated into the original C2f structure, significantly reducing the model’s parameter count while preserving detection accuracy. Furthermore, the coordinate attention (CA) mechanism has been incorporated into the detection head to enhance the focus on fruit chunk regions within complex backgrounds while suppressing irrelevant features, thus improving detection performance. Additionally, the ADown module from YOLOv9 has been embedded into the YOLOv8 backbone network, further increasing accuracy and reducing the number of parameters. Results Experimental results indicate that these enhancements substantially improve detection accuracy while reducing model size. Specifically, the optimized model achieves a 1.9 MB reduction in size, a decrease of 2.5 GFLOPs in parameter count, and an increase in mAP50 and mAP50-95 by 2.1% and 3.3%, respectively. The improved algorithm (UCA-YOLOv8n) enables real-time, accurate detection of various fruit chunks. Comparative analyses with other mainstream object detection algorithms further demonstrate the superiority and effectiveness of the proposed method.},
  archive      = {J_PEERJCS},
  author       = {Fei Liu and Mingyue Hu},
  doi          = {10.7717/peerj-cs.2832},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2832},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {UCA-YOLOv8n: A real-time and efficient fruit chunks detection algorithm for meal-assistance robot},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid model based on CNN-LSTM for assessing the risk of increasing claims in insurance companies. <em>PEERJCS</em>, <em>11</em>, e2830. (<a href='https://doi.org/10.7717/peerj-cs.2830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a hybrid model to assist insurance companies accurately assess the risk of increasing claims for their premiums. The model integrates long short-term memory (LSTM) networks and convolutional neural networks (CNN) to analyze historical claim data and identify emerging risk trends. We analyzed data obtained from insurance companies and found that the hybrid CNN-LSTM model outperforms standalone models in accurately assessing and categorizing risk levels. The proposed CNN-LSTM model achieved an accuracy of 98.5%, outperforming the standalone CNN (95.8%) and LSTM (92.6%). We implemented 10-fold cross-validation to ensure robustness, confirming consistent performance across different data splits. Furthermore, we validated the model on an external dataset to assess its generalizability. The results demonstrate that the model effectively classifies insurance risks in different market environments, highlighting its potential for real-world applications. Our study contributes to the insurance industry by providing valuable insights for effective risk management strategies and highlights the model’s broader applicability in global insurance markets.},
  archive      = {J_PEERJCS},
  author       = {Walaa Gamaleldin and Osama Attayyib and Mrim M. Alnfiai and Faiz Abdullah Alotaibi and Ruixing Ming},
  doi          = {10.7717/peerj-cs.2830},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2830},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid model based on CNN-LSTM for assessing the risk of increasing claims in insurance companies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI framework for DRIVE model based mental health detection in text: A case study on how coping strategies are expressed during COVID-19. <em>PEERJCS</em>, <em>11</em>, e2828. (<a href='https://doi.org/10.7717/peerj-cs.2828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background This article defines an artificial intelligence framework to detect individual’s mental health (MH) status on social networks. The proposed framework, which consists of four main modules, aims to analyze the emotions that are expressed by social network users in their text posts and identify their mental coping strategies, resources, and demands based on The Demands-Resources-Individual Effects (DRIVE) model. Although sentiment analysis (SA) is effective in analyzing the polarity of the text, it is limited in detecting the mental health status in terms of the coping strategies, available resources, or encountered stressors. This study illustrates such limitations in detecting the coping strategies and shows the effectiveness of the coping-based analysis. The work also reveals the phrases and topics that were used by individuals to express their coping strategies which provides a novel outlook of the individuals’ psychological coping within their environment. Methods The social network X is used to collect the coping strategies expressed by people who experienced stress during COVID-19 from November 2019 to May 2022. Text was processed using natural language processing (NLP). A sample of posts was coded into a positive or negative coping category and one of eight subtypes. SA and statistical analysis were performed to compare SA results with coded coping strategies. Latent Dirichlet Allocation and bigram NLP were applied to identify main themes and terminologies. Coping classification models were created and tested. Results The findings reveal that 70% of posts show positive coping strategies. The main positive coping themes included self-care, seeking help, positive reframing, engaging in prayers and meditation, employing humor through sarcasm, and implementing a practical mindset. Conversely, the remaining 30% of posts expressed negative coping themes, such as conspiracy thoughts, wishful or hopeless thinking, and negative perceptions. The coping classification models achieved a reliable predictive level with an average accuracy of 74.8%. Categorizing coping strategies using SA methods, particularly TextBlob and VADER, revealed high miscategorization rates, especially for negative coping strategies. Bigrams and LDA analysis identified distinct word patterns in positive and negative coping strategies, with emojis playing a significant role in emotional expression across both categories. Conclusion The article defined a framework for a MH detector based on the DRIVE model. It highlighted the resilience and adaptive responses of individuals in times of crisis. It also focused on coping and identified physical, emotional, and social support and positive reframing as major positive strategies; and the spread of false information and loss of social support as negative coping strategies. The applied coping classification models showed reliable performance in distinguishing between positive and negative coping categories.},
  archive      = {J_PEERJCS},
  author       = {Loulwah AlSumait and Altaf AlFarhan and Hasah AlHeneidi},
  doi          = {10.7717/peerj-cs.2828},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2828},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {AI framework for DRIVE model based mental health detection in text: A case study on how coping strategies are expressed during COVID-19},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic prediction of carbon prices based on the multi-frequency combined model. <em>PEERJCS</em>, <em>11</em>, e2827. (<a href='https://doi.org/10.7717/peerj-cs.2827'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a central participant and important leader in the global climate governance system, China is facing the urgent need to predict and regulate the price of carbon emissions to promote the sound development of its carbon market. In this article, a rolling prediction model based on Least Absolute Shrinkage and Selection Operator-cheetah optimization algorithm-extreme gradient boosting (Lasso-COA-XGBoost) carbon price decomposition integration is proposed to address the defects of low prediction accuracy and insufficient model stability of a single machine learning model in the carbon price prediction problem. During the modeling process, the adaptive Lasso method is first employed to select factors from 15 primary indicators of carbon prices, identifying the most important influencing factors. Next, the COA-XGBoost model is built and the parameters of the XGBoost model are optimized using the COA algorithm. Finally, the complete ensemble empirical Mode Decomposition with adaptive noise (CEEMDAM) method is utilized to decompose the residual sequence of the COA-XGBoost model and reconstruct it into high-frequency and low-frequency components. Appropriate frequency models are applied to achieve error correction, thereby constructing the combined Lasso-COA-XGBoost-CEEMDAN model. To further enhance the predictive accuracy and practicality of the model, a rolling time window is introduced for forecasting in the Hubei and Guangzhou carbon emission trading markets, ensuring that the forecasting model can adapt to market changes in real-time. The experimental results show that, taking the carbon price prediction in Hubei as an example, the proposed hybrid model has a significant improvement in prediction accuracy compared with the comparison model (XGBoost model): the RMSE is improved by 99.9987%, the MAE is improved by 99.9039%, the MAPE is improved by 99.9960%, and the R2 is improved by 0.2004%, and the advantages of this hybrid model are also verified in other experiments. The results provide an effective experimental method for future carbon price prediction.},
  archive      = {J_PEERJCS},
  author       = {Yonghui Duan and Yingying Fan and Xiang Wang and Kaige Liu and Xiaotong Zhang},
  doi          = {10.7717/peerj-cs.2827},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2827},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic prediction of carbon prices based on the multi-frequency combined model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building extraction from remote sensing images based on multi-scale attention gate and enhanced positional information. <em>PEERJCS</em>, <em>11</em>, e2826. (<a href='https://doi.org/10.7717/peerj-cs.2826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting buildings from high-resolution remote sensing images is currently a research hotspot in the field of remote sensing applications. Deep learning methods have significantly improved the accuracy of building extraction, but there are still deficiencies such as blurred edges, incomplete structures and loss of details in the extraction results. To obtain accurate contours and clear boundaries of buildings, this article proposes a novel building extraction method utilizing multi-scale attention gate and enhanced positional information. By employing U-Net as the main framework, this article introduces a multi-scale attention gate module in the encoder, which effectively improves the ability to capture multi-scale information, and designs a module in the decoder to enhance the positional information of the features, allowing for more precise localization and extraction of the shape and edge information of buildings. To validate the effectiveness of the proposed method, comprehensive evaluations were conducted on three benchmark datasets, Massachusetts, WHU, and Inria. The comparative analysis with six state-of-the-art models (SegNet, DeepLabv3+, U-Net, DSATNet, SDSC-Unet, and BuildFormer) demonstrates consistent performance improvements in intersection over union (IoU) metrics. Specifically, the proposed method achieves IoU increments of 2.19%, 3.31%, 3.10%, 2.00%, 3.35%, and 3.48% respectively on Massachusetts dataset, 1.26%, 4.18%, 1.18%, 2.01%, 2.03%, and 2.29% on WHU dataset, and 0.87%, 5.25%, 2.02%, 5.55%, 4.39%, and 1.18% on Inria dataset. The experimental results indicate that the proposed method can effectively integrate multi-scale features and optimize the extracted building edges, achieving superior performance compared to existing methodologies in building extraction tasks.},
  archive      = {J_PEERJCS},
  author       = {Rui Xu and Renzhong Mao and Zhenxing Zhuang and Fenghua Huang and Yihui Yang},
  doi          = {10.7717/peerj-cs.2826},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2826},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Building extraction from remote sensing images based on multi-scale attention gate and enhanced positional information},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A genetic programming-based ensemble method for long-term electricity demand forecasting. <em>PEERJCS</em>, <em>11</em>, e2825. (<a href='https://doi.org/10.7717/peerj-cs.2825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study introduces a novel genetic programming-based ensemble method for forecasting long-term electricity consumption in Ethiopia. The technique utilizes a two-stage ensemble approach to project Ethiopia’s electricity consumption through 2031. In the initial stage, genetic algorithms, particle swarm optimization, and simulated annealing methods are applied to various regression models (linear, quadratic, and exponential). The preliminary forecast values generated in this stage were further refined in the second stage. Here, the genetic programming method was utilized to develop a formula based on the initial forecast values, which then provided the final forecast results. The most accurate predictions in the first stage were obtained using the GA_Quadratic, PSO_Quadratic, and SA_Quadratic methods, resulting in mean absolute percentage error (MAPE) values of 3.61, 3.63, and 4.68, respectively. In the second stage, the GP-based prediction achieved an even lower MAPE value of 2.83. Other error metrics, including MSE, root mean square error (RMSE), and R2, were also evaluated, with the proposed model outperforming all methods from the first stage on these metrics. The study projected Ethiopia’s total annual electricity consumption through 2031 under two different scenarios. Both scenarios indicate that by 2031, electricity consumption will have tripled compared to 2021 levels.},
  archive      = {J_PEERJCS},
  author       = {Hayat Ahmed Issa and Hasan Hüseyin Çevik and Ahmet Yilmaz and Mehmet Cunkas},
  doi          = {10.7717/peerj-cs.2825},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2825},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A genetic programming-based ensemble method for long-term electricity demand forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversified caching algorithm with cooperation between edge servers. <em>PEERJCS</em>, <em>11</em>, e2824. (<a href='https://doi.org/10.7717/peerj-cs.2824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing makes up for the high latency of the central cloud network by deploying server resources in close proximity to users. The storage and other resources configured by edge servers are limited, and a reasonable cache replacement strategy is conducive to improving the cache hit ratio of edge services, thereby reducing service latency and enhancing service quality. The spatiotemporal correlation of user service request distribution brings opportunities and challenges to edge service caching. The collaboration between edge servers is often ignored in the existing research work for caching decisions, which can easily lead to a low edge cache hit rate, thereby reducing the efficiency of edge resource use and service quality. Therefore, this article proposes a diversified caching method to ensure the diversity of edge cache services, utilizing inter-server collaboration to enhance the cache hit rate. After the service request reaches the server, if it misses, the proposed algorithm will judge whether the neighbor node can provide services through the cache information of the neighbor node, and then the server and the neighbor node jointly decide how to cache the service. At the same time, the performance of the proposed diversified caching method is evaluated through a large number of simulation experiments, and the experimental results show that the proposed method can improve the cache hit rate by 27.01–37.43%, reduce the average service delay by 25.57–30.68%, and with the change of the scale of the edge computing platform, the proposed method can maintain good performance.},
  archive      = {J_PEERJCS},
  author       = {Yongxuan Sang and Yukang Guo and Bo Wang and Ying Song},
  doi          = {10.7717/peerj-cs.2824},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2824},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Diversified caching algorithm with cooperation between edge servers},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Validation of automated paper screening for esophagectomy systematic review using large language models. <em>PEERJCS</em>, <em>11</em>, e2822. (<a href='https://doi.org/10.7717/peerj-cs.2822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Large language models (LLMs) offer a potential solution to the labor-intensive nature of systematic reviews. This study evaluated the ability of the GPT model to identify articles that discuss perioperative risk factors for esophagectomy complications. To test the performance of the model, we tested GPT-4 on narrower inclusion criterion and by assessing its ability to discriminate relevant articles that solely identified preoperative risk factors for esophagectomy. Methods A literature search was run by a trained librarian to identify studies (n = 1,967) discussing risk factors to esophagectomy complications. The articles underwent title and abstract screening by three independent human reviewers and GPT-4. The Python script used for the analysis made Application Programming Interface (API) calls to GPT-4 with screening criteria in natural language. GPT-4’s inclusion and exclusion decision were compared to those decided human reviewers. Results The agreement between the GPT model and human decision was 85.58% for perioperative factors and 78.75% for preoperative factors. The AUC value was 0.87 and 0.75 for the perioperative and preoperative risk factors query, respectively. In the evaluation of perioperative risk factors, the GPT model demonstrated a high recall for included studies at 89%, a positive predictive value of 74%, and a negative predictive value of 84%, with a low false positive rate of 6% and a macro-F1 score of 0.81. For preoperative risk factors, the model showed a recall of 67% for included studies, a positive predictive value of 65%, and a negative predictive value of 85%, with a false positive rate of 15% and a macro-F1 score of 0.66. The interobserver reliability was substantial, with a kappa score of 0.69 for perioperative factors and 0.61 for preoperative factors. Despite lower accuracy under more stringent criteria, the GPT model proved valuable in streamlining the systematic review workflow. Preliminary evaluation of inclusion and exclusion justification provided by the GPT model were reported to have been useful by study screeners, especially in resolving discrepancies during title and abstract screening. Conclusion This study demonstrates promising use of LLMs to streamline the workflow of systematic reviews. The integration of LLMs in systematic reviews could lead to significant time and cost savings, however caution must be taken for reviews involving stringent a narrower and exclusion criterion. Future research is needed and should explore integrating LLMs in other steps of the systematic review, such as full text screening or data extraction, and compare different LLMs for their effectiveness in various types of systematic reviews.},
  archive      = {J_PEERJCS},
  author       = {Rashi Ramchandani and Eddie Guo and Esra Rakab and Jharna Rathod and Jamie Strain and William Klement and Risa Shorr and Erin Williams and Daniel Jones and Sebastien Gilbert},
  doi          = {10.7717/peerj-cs.2822},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2822},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Validation of automated paper screening for esophagectomy systematic review using large language models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparative evaluation of approaches & tools for effective security testing of web applications. <em>PEERJCS</em>, <em>11</em>, e2821. (<a href='https://doi.org/10.7717/peerj-cs.2821'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is generally accepted that adopting both static application security testing (SAST) and dynamic application security testing (DAST) approaches is vital for thorough and effective security testing. However, this suggestion has not been comprehensively evaluated, especially with regard to the individual risk categories mentioned in Open Web Application Security Project (OWASP) Top 10:2021 and common weakness enumeration (CWE) Top 25:2023 lists. Also, it is rare to find any evidence-based recommendations for effective tools for detecting vulnerabilities from a specific risk category or severity level. These shortcomings increase both the time and cost of systematic security testing when its need is heightened by increasingly frequent and preventable incidents. This study aims to fill these gaps by empirically testing seventy-five real-world Web applications using four SAST and five DAST tools. Only popular, free, and open-source tools were selected and each Web application was scanned using these nine tools. From the report generated by these tools, we considered two parameters to measure effectiveness: count and severity of the vulnerability found. We also mapped the vulnerabilities to OWASP Top 10:2021 and CWE Top 25:2023 lists. Our results show that using only DAST tools is the preferred option for four OWASP Top 10:2021 risk categories while using only SAST tools is preferred for only three risk categories. Either approach is effective for two of the OWASP Top 10:2021 risk categories. For CWE Top 25:2023 list, all three approaches were equally effective and found vulnerabilities belonging to three risk categories each. We also found that none of the tools were able to detect any vulnerability in one OWASP Top 10:2021 risk category and in eight CWE Top 25:2023 categories. This highlights a critical limitation of popular tools. The most effective DAST tool was OWASP Zed Attack Proxy (ZAP), especially for detecting vulnerabilities in broken access control, insecure design, and security misconfiguration risk categories. Yasca was the best-performing SAST tool, and outperformed all other tools at finding high-severity vulnerabilities. For medium-severity and low-severity levels, the DAST tools Iron Web application Advanced Security testing Platform (WASP) and Vega performed better than all the other tools. These findings reveal key insights, such as, the superiority of DAST tools for detecting certain types of vulnerabilities and the indispensability of SAST tools for detecting high-severity issues (due to detailed static code analysis). This study also addresses significant limitations in previous research by testing multiple real-world Web applications across diverse domains (technology, health, and education), enhancing generalization of the findings. Unlike studies that rely primarily on proprietary tools, our use of open-source SAST and DAST tools ensures better reproducibility and accessibility for organizations with limited budget.},
  archive      = {J_PEERJCS},
  author       = {Sana Qadir and Eman Waheed and Aisha Khanum and Seema Jehan},
  doi          = {10.7717/peerj-cs.2821},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2821},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparative evaluation of approaches & tools for effective security testing of web applications},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv8-POS: A lightweight model for coal-rock image recognition. <em>PEERJCS</em>, <em>11</em>, e2820. (<a href='https://doi.org/10.7717/peerj-cs.2820'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach, designated YOLOv8-POS, is introduced to address the issue of false detections in coal-rock image recognition tasks, frequently caused by factors such as image defocus, dim lighting, and worker occlusion, and to further enhance the model’s accuracy and reduce its complexity. The methodology introduces a C2f-PConv module, which ingeniously combines the strengths of C2f and partial convolution (PConv) to selectively process channels. This reduces unnecessary computational overhead while preserving the integrity of critical feature information, thus significantly cutting down on the model’s parameters and computational demands. Additionally, an Overlapping Spatial Reduction Attention module is incorporated into the model’s architecture to optimize the fusion of spatial features, substantially improving the handling of complex scenarios. The adoption of a slim-neck design further streamlines the computational and storage requirements, leveraging meticulously engineered lightweight modules to enhance the model’s practical applicability. Empirical results demonstrate that YOLOv8-POS markedly improves performance on coal-rock image datasets, achieving an AP50 of 77.1% and an AP50:95 of 63.6%, while concurrently reducing the model’s parameters to 2.60 M and the floating point operations (FLOPS) to 6.4 G. Comparative evaluations with other prominent algorithms confirm the superior performance of this refined approach, solidifying its advantage in practical deployments.},
  archive      = {J_PEERJCS},
  author       = {Yanqin Zhao and Wenyu Wang},
  doi          = {10.7717/peerj-cs.2820},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2820},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {YOLOv8-POS: A lightweight model for coal-rock image recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REDf: A deep learning model for short-term load forecasting to facilitate renewable integration and attaining the SDGs 7, 9, and 13. <em>PEERJCS</em>, <em>11</em>, e2819. (<a href='https://doi.org/10.7717/peerj-cs.2819'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future in line with the United Nations (UN) Sustainable Development Goal (SDG) 7 (Affordable and Clean Energy). However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity, which is crucial for achieving SDG 9 (Industry, Innovation and Infrastructure). In this article, we propose a deep learning model for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. Our approach aligns with SDG 13 (Climate Action) on climate action, enabling more efficient management of renewable energy resources. We use long short-term memory networks, well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four historical short-term energy demand data datasets from different energy distribution companies, including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is compared with three other state-of-the-art forecasting algorithms: Facebook Prophet, support vector regression, and random forest regression. The experimental results show that the proposed REDf model can accurately predict energy demand with a mean absolute error of 1.4%, indicating its potential to enhance the stability and efficiency of the power grid and contribute to achieving SDGs 7, 9, and 13. The proposed model also has the potential to manage the integration of renewable energy sources effectively.},
  archive      = {J_PEERJCS},
  author       = {Md Saef Ullah Miah and Junaida Sulaiman and Md Imamul Islam and Md Masuduzzaman and Molla Shahadat Hossain Lipu and Ramdhan Nugraha},
  doi          = {10.7717/peerj-cs.2819},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2819},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {REDf: A deep learning model for short-term load forecasting to facilitate renewable integration and attaining the SDGs 7, 9, and 13},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting sarcasm in user-generated content integrating transformers and gated graph neural networks. <em>PEERJCS</em>, <em>11</em>, e2817. (<a href='https://doi.org/10.7717/peerj-cs.2817'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread use of the Internet and social media has posed significant challenges to automated sentiment analysis, particularly in relation to detecting sarcasm in user-generated content. Sarcasm often expresses negative emotions through seemingly positive or exaggerated language, making its detection a complex task in natural language processing. To address this issue, the present study proposes a novel sarcasm detection model that combines bidirectional encoder representations from transformers (BERT) with gated graph neural networks (GGNN), further enhanced by a self-attention mechanism to more effectively capture ironic cues. BERT is utilized to extract deep contextual information from the text, while GGNN is employed to learn global semantic structures by incorporating dependency and emotion graphs. Experiments were conducted on two benchmark sarcasm detection datasets, namely Headlines and Riloff. The experimental results demonstrate that the proposed BERT-GGNN model achieves an accuracy of 92.00% and an F1 score of 91.51% on the Headlines dataset, as well as an accuracy of 86.49% and an F1 score of 86.59% on the Riloff dataset, significantly outperforming the conventional BERT-GCN models. The results of ablation studies further corroborate the efficacy of integrating GGNN, particularly for handling complex ironic expressions frequently encountered in social media contexts.},
  archive      = {J_PEERJCS},
  author       = {Zhenkai Qin and Qining Luo and Zhidong Zang and Hongpeng Fu},
  doi          = {10.7717/peerj-cs.2817},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2817},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Detecting sarcasm in user-generated content integrating transformers and gated graph neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IoT in urban development: Insight into smart city applications, case studies, challenges, and future prospects. <em>PEERJCS</em>, <em>11</em>, e2816. (<a href='https://doi.org/10.7717/peerj-cs.2816'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the integration of Internet of Things (IoT) technology, smart cities possess the capability to advance their public transportation modalities, address prevalent traffic congestion challenges, refine infrastructure, and optimize communication frameworks, thereby augmenting their progression towards heightened urbanization. Through the integration of sensors, cell phones, artificial intelligence (AI), data analytics, and cloud computing, smart cities worldwide are evolving to be more efficient, productive, and responsive to their residents’ needs. While the promise of smart cities has been marked over the past decade, notable challenges, especially in the realm of security, threaten their optimal realization. This research provides a comprehensive survey on IoT in smart cities. It focuses on the IoT-based smart city components. Moreover, it provides explanation for integrating different technologies with IoT for smart cities such as AI, sensing technologies, and networking technologies. Additionally, this study provides several case studies for smart cities. In addition, this study investigates the challenges of adopting IoT in smart cities and provides prevention methods for each challenge. Moreover, this study provides future directions for the upcoming researchers. It serves as a foundational guide for stakeholders and emphasizes the pressing need for a balanced integration of innovation and safety in the smart city landscape.},
  archive      = {J_PEERJCS},
  author       = {Sayeed Salih and Abdelzahir Abdelmaboud and Omayma Husain and Abdelwahed Motwakel and Hashim Elshafie and Mahir Sharif and Mosab Hamdan},
  doi          = {10.7717/peerj-cs.2816},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2816},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IoT in urban development: Insight into smart city applications, case studies, challenges, and future prospects},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction. <em>PEERJCS</em>, <em>11</em>, e2815. (<a href='https://doi.org/10.7717/peerj-cs.2815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defect detection is a critical research topic in the field of software engineering, aiming to identify potential defects during the development process to improve software quality and reduce maintenance costs. This study proposes a novel feature selection and defect prediction classification algorithm based on a multi-strategy enhanced Parrot Optimization (PO) algorithm. Firstly, to address the limitations of the original Parrot Optimization algorithm, such as strong dependence on the initial population, premature convergence, and insufficient global search capability, this article develops a multi-strategy enhanced Parrot Optimization algorithm (MEPO). Experiments conducted on eight benchmark test functions validate the superior performance of MEPO in terms of convergence speed and solution accuracy. Secondly, to mitigate the adverse impact of irrelevant features on model performance in traditional software defect prediction methods, this study introduces a binary multi-strategy enhanced Parrot Optimization algorithm (BMEPO) for optimizing feature selection. Comparative experiments demonstrate that BMEPO exhibits stronger competitiveness in feature selection quality and classification performance compared to advanced feature selection algorithms. Finally, to further enhance the classification performance of defect prediction, a heterogeneous data stacking ensemble learning algorithm (HEDSE) based on feature selection is proposed. Experimental evaluations on 16 open-source software defect datasets indicate that the proposed HEDSE outperforms existing methods, providing a novel and effective solution for software defect prediction. The proposed approaches in this study hold significant practical value, particularly in improving software quality, optimizing testing resource allocation, and reducing maintenance costs, offering broad potential for application in real-world software engineering scenarios.},
  archive      = {J_PEERJCS},
  author       = {Qi Fei and Guisheng Yin and Zhian Sun},
  doi          = {10.7717/peerj-cs.2815},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2815},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Feature selection using a multi-strategy improved parrot optimization algorithm in software defect prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The pivotal role of software defined networks to safeguard against cyber attacks: A comprehensive review. <em>PEERJCS</em>, <em>11</em>, e2814. (<a href='https://doi.org/10.7717/peerj-cs.2814'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software defined networks (SDNs) offer novel approaches to managing networks by separating the control plane from the data plane to enable programmable control over network resources effectively and dynamically. This framework supports monitoring of traffic flow and detection of threats while also enabling easy adaptation of network configurations, which is critical in safeguarding against cyber threats. However, this separation also brings forth security risks that cyber attackers may exploit. In this examination, the basic concepts of SDN are explained, pointing out their benefits compared to conventional networks and exploring the security issues that are part of SDN architectures. Different types of threats that focus on SDN layers are categorized and how they impact network security while suggesting different ways to address them. Furthermore, the review highlights issues and suggests potential research paths to enhance SDN security measures and ensure their effectiveness against ever-changing cyber dangers.},
  archive      = {J_PEERJCS},
  author       = {Ahmed Aljughaiman and Seetah Almarri},
  doi          = {10.7717/peerj-cs.2814},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2814},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The pivotal role of software defined networks to safeguard against cyber attacks: A comprehensive review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLPDBO-BP: An efficient valuation model for data asset value. <em>PEERJCS</em>, <em>11</em>, e2813. (<a href='https://doi.org/10.7717/peerj-cs.2813'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data asset value assessment is of strategic significance to the development of data factorization, in order to solve the problems of strong assessment subjectivity and low assessment efficiency and accuracy in traditional assessment methods. This article introduces the SLPDBO-BP data asset assessment model for data asset value assessment. Firstly, the sinusoidal chaos mapping strategy, the Levy flight strategy and the fusion of adaptive weight variation operators are integrated to increase the population diversity of the algorithm, broaden the search range, and augment the global optimization capability of the algorithm. Secondly, in an attempt to comprehensively evaluate the optimization performance of SLPDBO, a series of numerical optimization experiments are carried out with 20 test functions and with popular optimization algorithms and dung beetle optimizer (DBO) algorithms with different improvement strategies. Finally, in order to verify the effectiveness of the proposed algorithm in data asset value assessment, the SLPDBO algorithm is combined with backpropagation (BP) to establish the SLPDBO-BP model for data asset value assessment, and the acquired data sets are used in the proposed model for data asset value assessment. The experimental results show that the SLPDBO-BP model performs well in assessment accuracy, and its assessment indexes mean absolute error (MAE), root mean square error (RMSE) and mean absolute percentage error (MAPE) are reduced by 35.1%, 37.6% and 38.7%, respectively, compared with the dung beetle optimizer backpropagation (DBO-BP) model, and its evaluation efficiency is improved, and the proposed model demonstrates better evaluation simulation effects by remarkably outperforming other models in terms of evaluation accuracy and error level.},
  archive      = {J_PEERJCS},
  author       = {Cuiping Zhou and Shaobo Li and Cankun Xie and Panliang Yuan and Zihao Liao},
  doi          = {10.7717/peerj-cs.2813},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2813},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SLPDBO-BP: An efficient valuation model for data asset value},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization model for enterprise financial management utilizing genetic algorithms and fuzzy logic. <em>PEERJCS</em>, <em>11</em>, e2812. (<a href='https://doi.org/10.7717/peerj-cs.2812'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores the complexities of enterprise financial management by optimizing financial models with a particular focus on enhancing risk prediction performance. A multi-objective mathematical model is first developed to establish key optimization goals, including cost reduction, improved capital utilization, and increased economic benefits. This model systematically defines decision variables and optimization objectives, providing a comprehensive framework for enterprise financial management. To improve predictive accuracy, the study integrates genetic algorithms with back-propagation (BP) neural networks, leveraging genetic algorithms to optimize the neural network’s parameters and structure. Additionally, a hierarchical reinforcement learning model based on fuzzy reasoning (HRL-FR) is proposed to enhance decision-making capabilities. This model employs hierarchical decision-making and policy optimization, incorporating fuzzy reasoning to address uncertainties in complex and dynamic financial environments. Experimental validation using the Compustat dataset confirms the effectiveness of the proposed model. Key financial variables, including the working capital asset ratio and debt-to-equity ratio, are identified as significant influencers of prediction accuracy, reinforcing the model’s robustness. The genetic algorithm’s search and optimization process identifies parameter combinations that maximize neural network performance, further improving predictive capabilities. Comprehensive evaluations conducted on the Center for Research in Security Prices (CRSP) and Compustat datasets for 2022 confirm the HRL-FR model’s superior ability to predict and analyze enterprise financial management information accurately. The model demonstrates higher profitability, enhanced efficiency, and predictive curves that closely align with optimal financial models. These findings highlight the HRL-FR model’s potential as a powerful tool for enterprise financial management optimization, offering valuable insights for risk mitigation and strategic decision-making.},
  archive      = {J_PEERJCS},
  author       = {Sujuan Wang and Musadaq Mansoor},
  doi          = {10.7717/peerj-cs.2812},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2812},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimization model for enterprise financial management utilizing genetic algorithms and fuzzy logic},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep ensemble learning for gastrointestinal diagnosis using endoscopic image classification. <em>PEERJCS</em>, <em>11</em>, e2809. (<a href='https://doi.org/10.7717/peerj-cs.2809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transfer learning is a valuable tool for the effective assistance of gastroenterologists in the powerful diagnosis of medical images with fast convergence. It also intends to minimize the time and estimated effort required for improved gastrointestinal tract (GIT) diagnosis. GIT abnormalities are widely known to be fatal disorders leading to significant mortalities. It includes both upper and lower GIT disorders. The challenges of addressing GIT issues are complex and need significant study. Multiple challenges exist regarding computer-aided diagnosis (CAD) and endoscopy including a lack of annotated images, dark backgrounds, less contrast, noisy backgrounds, and irregular patterns. Deep learning and transfer learning have assisted gastroenterologists in effective diagnosis in various ways. The goal of proposed framework is the effective classification of endoscopic GIT images with enhanced accuracy. The proposed research aims to formulate a transfer learning-based deep ensemble model, accurately classifying GIT disorders for therapeutic purposes. The proposed model is based on weighted voting ensemble of the two state-of-the-art (STA) base models, NasNet-Mobile and EfficientNet. The extraction of regions of interest, specifically the sick portions, have been performed using images captured from endoscopic procedure. Performance evaluation of the proposed model is performed with cross-dataset evaluation. The datasets utilized include the training dataset HyperKvasir and two test datasets, Kvasir v1 and Kvasir v2. However, the dataset alone cannot create a robust model due to the unequal distribution of images across categories, making transfer learning a promising approach for model development. The evaluation of the proposed framework has been conducted by cross-dataset evaluation utilizing accuracy, precision, recall, Area under curve (AUC) score and F1 score performance metrics. The proposed work outperforms much of the existing transfer learning-based models giving 97.83% on Kvasir v1 and 98.45% accuracy on Kvasir v2.},
  archive      = {J_PEERJCS},
  author       = {Samra Siddiqui and Junaid Ali Khan and Shabbab Algamdi},
  doi          = {10.7717/peerj-cs.2809},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2809},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep ensemble learning for gastrointestinal diagnosis using endoscopic image classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). JGURD: Joint gradient update relational direction-enhanced method for knowledge graph completion. <em>PEERJCS</em>, <em>11</em>, e2808. (<a href='https://doi.org/10.7717/peerj-cs.2808'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relational direction plays an important role in multi-relational knowledge graphs (KGs). Current knowledge graph completion (KGC) methods suffer from insufficient utilization of relation correlation information. To address this issue, this article proposes a novel KGC framework, namely JGURD, which uses the encoder-decoder structure to achieve Joint Gradient Update with Relational Direction information. It combines graph convolutional networks (GCNs) with KG embedding methods, defining a update mechanism for entities and relationships to joint gradient updates. To incorporate entity information into the update of relationships, the forward propagation gradients of the triple score function are recorded, and entity gradient information is fused into relationship updates. To fully utilize relational direction information, a relation correlation graph (RCG) is constructed based on the topological patterns of relationship pairs. We design a multi-relation encoder combining GCN and multi-layer attention mechanism on RCG to comprehensively capture local and global structures of the RCG. To enhance the interpretability and adaptability of JGURD, three different decoders are employed. Experimental results show that JGURD outperforms the second-place HHAN-KGC, and the Hits@3 and MRR metrics on the FB15k dataset increased by 6.8% and 8.9%, respectively.},
  archive      = {J_PEERJCS},
  author       = {Lianhong Ding and Mengxiao Li and Shengchang Gao and Juntao Li and Ruiping Yuan and Jianye Yu},
  doi          = {10.7717/peerj-cs.2808},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2808},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {JGURD: Joint gradient update relational direction-enhanced method for knowledge graph completion},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing a novel technique for evaluation of tourism informatization in scenic spots from a big data perspective. <em>PEERJCS</em>, <em>11</em>, e2807. (<a href='https://doi.org/10.7717/peerj-cs.2807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, tourism has become a significant driver of many countries’ economies. To maximize revenue from tourism, it is crucial to prioritize the effective management of scenic spots and tourist attractions, and also raise awareness about these places. Social media platforms have played a pivotal role in promoting tourism, as users frequently share videos and reviews related to tourism. Analyzing and managing these reviews is essential for understanding tourists’ opinions about specific destinations. In this study, we evaluated a scenic spot by analyzing tourists’ sentiments. Data was collected from popular social media sites such as TripAdvisor and Twitter using web scraping and the Twitter API. The raw data was preprocessed to remove irrelevant information and redundancies and was properly annotated for further processing. We applied two approaches to analyze the sentiments of tourists. First, we vectorized the text representing the sentiment using the term frequency-inverse document frequency (TF-IDF) and utilized big data analytics to extract meaningful insights. Secondly, we employed a pre-trained large language model, bidirectional encoder representations from transformers (BERT), with a linear classifier to classify tourists’ sentiments. The results of the big data analytics approaches were compared with those of BERT and previously proposed methods. BERT outperformed other machine learning models, achieving an average accuracy of 83.5% on the test set. These insights are valuable for evaluating the informatization of tourist spots, destination management, hospitality, and overall tourist attractions.},
  archive      = {J_PEERJCS},
  author       = {Li Fu and Yao Yi and Lina Liu and Ran Chen},
  doi          = {10.7717/peerj-cs.2807},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2807},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Designing a novel technique for evaluation of tourism informatization in scenic spots from a big data perspective},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Demystifying diagnosis: An efficient deep learning technique with explainable AI to improve breast cancer detection. <em>PEERJCS</em>, <em>11</em>, e2806. (<a href='https://doi.org/10.7717/peerj-cs.2806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As per a WHO survey conducted in 2023, more than 2.3 million breast cancer (BC) cases are reported every year. In nearly 95% of countries, the second leading cause of death for females is BC. Breast and cervical cancers cause 80% of reported deaths in middle-income countries. Early detection of breast cancer can help patients better manage their condition and increase their chances of survival. However, traditional AI models frequently conceal their decision-making processes and are mainly tailored for classification tasks. Our approach combines composite deep learning techniques with explainable artificial intelligence (XAI) to enhance interpretability and predictive accuracy. By utilizing XAI to examine features and provide insights into its classifications, the model clarifies the rationale behind its decisions, resulting in an understanding of concealed patterns linked to breast cancer detection. The XAI strengthens practitioners’ and health researchers’ confidence and understanding of artificial intelligence (AI)-based models. In this work, we introduce a hybrid deep learning bi-directional long short-term memory-convolutional neural network (BiLSTM-CNN) model to identify breast cancer using patient data effectively. We first balanced the dataset before using the BiLSTM-CNN model. The hybrid deep learning (DL) model presented here performed well in comparison to other studies, with 0.993 accuracy, precision 0.99, recall 0.99, and F1-score 0.99.},
  archive      = {J_PEERJCS},
  author       = {Ahmed Alzahrani and Muhammad Ali Raza and Muhammad Zubair Asghar},
  doi          = {10.7717/peerj-cs.2806},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2806},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Demystifying diagnosis: An efficient deep learning technique with explainable AI to improve breast cancer detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improvement strategies for heuristic algorithms based on machine learning and information concepts: A review of the seahorse optimization algorithm. <em>PEERJCS</em>, <em>11</em>, e2805. (<a href='https://doi.org/10.7717/peerj-cs.2805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To overcome the mechanical limitations of traditional inertia weight optimization methods, this study draws inspiration from machine learning models and proposes an inertia weight optimization strategy based on the K-nearest neighbors (KNN) principle with dynamic adjustment properties. Unlike conventional approaches that determine inertia weight solely based on the number of iterations, the proposed strategy allows inertia weight to more accurately reflect the relative distance between individuals and the target value. Consequently, it transforms the discrete “iteration-weight” mapping ( $t\rightarrow w$t→w ) into a continuous “distance-weight” mapping ( $d\rightarrow w$d→w ), thereby enhancing the adaptability and optimization capability of the algorithm. Furthermore, inspired by the entropy weight method, this study introduces an entropy-based weight allocation mechanism in the crossover and mutation process to improve the efficiency of high-quality information inheritance. To validate its effectiveness, the proposed strategy is incorporated into the Seahorse Optimization Algorithm (SHO) and systematically evaluated using 31 benchmark functions from CEC2005 and CEC2021 test suites. Experimental results demonstrate that the improved SHO algorithm, integrating the logistic-KNN inertia weight optimization strategy and the entropy-based crossover-mutation mechanism, exhibits significant advantages in terms of convergence speed, solution accuracy, and algorithm stability. To further investigate the performance of the proposed improvements, this study conducts ablation experiments to analyze each modification separately. The results confirm that each individual strategy significantly enhances the overall performance of the SHO algorithm.},
  archive      = {J_PEERJCS},
  author       = {Shixing Zheng},
  doi          = {10.7717/peerj-cs.2805},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2805},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improvement strategies for heuristic algorithms based on machine learning and information concepts: A review of the seahorse optimization algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased machine learning-assisted approach for conditional discretization of human performances. <em>PEERJCS</em>, <em>11</em>, e2804. (<a href='https://doi.org/10.7717/peerj-cs.2804'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance discretization maps numerical performance values to ordinal categories or performance ranking labels. Norm-referenced performance discretization is extensively applied in human performance evaluation such as grading academic achievements and determining salary increases for employees. These tasks stipulate a common condition that certain performance ranking labels might have no associated performance values and are referred to as conditional discretization. Currently, the only statistical method available for norm-referenced performance discretization is Z score, which merely addresses partial conditions. To achieve a fully conditionally norm-referenced performance discretization, this article proposes four novel approaches enlisting a multi-modal technique that incorporates unsupervised machine-learning algorithms and a heuristic method as well as a novel decision function ensuring conditional unbiasedness. The machine-learning-based methods demonstrate superiority over the heuristic one across most testing data sets, achieving a conditional unbiasedness degree ranging from 0.11 to 0.82. On the other hand, the heuristic method notably outperforms for a specific data set, exhibiting a conditional unbiasedness degree up to 0.76. Leveraging the strengths of these constituent methods enable the effectiveness of the proposed multi-modal approach for conditionally norm-referenced performance discretization.},
  archive      = {J_PEERJCS},
  author       = {Thepparit Banditwattanawong and Masawee Masdisornchote},
  doi          = {10.7717/peerj-cs.2804},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2804},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Unbiased machine learning-assisted approach for conditional discretization of human performances},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SFTA-net: A self-supervised approach to detect copy-move and splicing forgery to leverage triplet loss, auxiliary loss, and spatial attention. <em>PEERJCS</em>, <em>11</em>, e2803. (<a href='https://doi.org/10.7717/peerj-cs.2803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image forgery is an increasing threat, fueling misinformation and potentially impacting legal decisions and everyday life. Detecting forged media, including images and videos, is crucial for preserving trust and integrity across various platforms. Common forgery techniques like copy-move and splicing require robust detection methods to identify tampered areas without explicit guidance. The previously proposed studies focused on a single type of forgery detection utilizing block-based and key-point feature selection-based classical machine learning (ML) approaches. Furthermore, applied deep learning (DL) methods only focus on deep feature extraction without considering the focus on tampered regions detection or any domain-specific loss. Therefore, this study addresses the aforementioned challenges by proposing a lightweight DL approach, a self-supervised, triplet and auxiliary losses-based forgery detection network (SFTA-Net), featuring a self-guidance mechanism for detecting tampered regions with a commutative loss within images. The SFTA-Net method is proposed to classify forged and original photos belonging to copy-move and splicing forgeries. To effectively analyze the added components in the proposed model, three experiments were conducted, one with a self-guided (SG) head-based convolutional neural network (CNN), a second with SG-head and auxiliary loss, and a third one with SG-head auxiliary loss and triplet losses-based CNN. For experimentation, CASIA 1.0 and CASIA 2.0 datasets were used with 80-10-10% train-validation and test ratios. The testing results achieved on CASIA 1.0 were 95% accuracy and 97% accuracy on the CASIA 2.0 dataset. To prove the approach’s robustness and generalization, the CASIA 2.0-trained weights were used to test on the MICC-FC2000 dataset and yielded limited results. To improve the results, fine-tuning was performed on CASIA 2.0 weights utilizing the MICC-FC2000 dataset which achieved 98% accurate results. Our findings demonstrate that the SFTA-Net surpasses the baseline ResNet18 model and previous state-of-the-art (SOTA) methods. Overall, our SG approach offers a promising solution for detecting forged images across diverse real-world scenarios, contributing to the mitigation of image forgery and preservation of trust in digital content.},
  archive      = {J_PEERJCS},
  author       = {Amerah Alabrah},
  doi          = {10.7717/peerj-cs.2803},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2803},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SFTA-net: A self-supervised approach to detect copy-move and splicing forgery to leverage triplet loss, auxiliary loss, and spatial attention},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A method for explaining individual predictions in neural networks. <em>PEERJCS</em>, <em>11</em>, e2802. (<a href='https://doi.org/10.7717/peerj-cs.2802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Recently, the explainability of the prediction results of machine learning models has attracted attention. Most high-performance prediction models are black boxes that cannot be explained. Artificial neural networks are also considered black box models. Although they can explain image classification results to some extent, they still struggle to explain the classification and regression results for tabular data. In this study, we explain the individual prediction results derived from a neural network-based prediction model. Methods The output of a neural network is fundamentally determined by multiplying the input values by the network weights. In other words, the output is a weighted sum of the input values. The weights control how much each input value contributes to the output. The degree of influence of an input value xi on the output can be evaluated as (xi · weight value wi)/weighted sum. From this insight, we can calculate the contribution of each input value to the output as it flows through the neural network. Results With the proposed method, the neural network is no longer a black box. The proposed method effectively explains the predictions made by the neural network and is independent of the depth of the hidden layers and the number of nodes in each hidden layer. This provides a clear rationale for this interpretation. It can be applied to both regression and classification models. The proposed method is implemented as a Python library, making it easy to use.},
  archive      = {J_PEERJCS},
  author       = {Sejong Oh},
  doi          = {10.7717/peerj-cs.2802},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2802},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A method for explaining individual predictions in neural networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal hate speech detection: A novel deep learning framework for multilingual text and images. <em>PEERJCS</em>, <em>11</em>, e2801. (<a href='https://doi.org/10.7717/peerj-cs.2801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid proliferation of social media platforms has facilitated the expression of opinions but also enabled the spread of hate speech. Detecting multimodal hate speech in low-resource multilingual contexts poses significant challenges. This study presents a deep learning framework that integrates bidirectional long short-term memory (BiLSTM) and EfficientNetB1 to classify hate speech in Urdu-English tweets, leveraging both text and image modalities. We introduce multimodal multilingual hate speech (MMHS11K), a manually annotated dataset comprising 11,000 multimodal tweets. Using an early fusion strategy, text and image features were combined for classification. Experimental results demonstrate that the BiLSTM+EfficientNetB1 model outperforms unimodal and baseline multimodal approaches, achieving an F1-score of 81.2% for Urdu tweets and 75.5% for English tweets. This research addresses critical gaps in multilingual and multimodal hate speech detection, offering a foundation for future advancements.},
  archive      = {J_PEERJCS},
  author       = {Furqan Khan Saddozai and Sahar K. Badri and Daniyal Alghazzawi and Asad Khattak and Muhammad Zubair Asghar},
  doi          = {10.7717/peerj-cs.2801},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2801},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multimodal hate speech detection: A novel deep learning framework for multilingual text and images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fine-tuned deep transfer learning: An effective strategy for the accurate chronic kidney disease classification. <em>PEERJCS</em>, <em>11</em>, e2800. (<a href='https://doi.org/10.7717/peerj-cs.2800'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kidney diseases are becoming an alarming concern around the globe. Premature diagnosis of kidney disease can save precious human lives by taking preventive measures. Deep learning demonstrates a substantial performance in various medical disciplines. Numerous deep learning approaches are suggested in the literature for accurate chronic kidney disease classification by compromising on architectural complexity, classification speed, and resource constraints. In this study, deep transfer learning is exploited by incorporating unexplored yet effective variants of ConvNeXt and EfficientNetV2 for accurate and efficient classification of chronic kidney diseases. The benchmark computed tomography (CT)-based kidney database containing 12,446 CT scans of kidney tumor, stone cysts, and normal patients is utilized to train the designed fine-tuned networks. However, due to the highly imbalanced distribution of images among classes, the operation of data trimming is exploited for balancing the number of CT scans in each class, which is essential for designing an unbiased predictive network. By utilizing fine-tuned pre-trained models for our specific task, the training time is reduced leading to a computationally inexpensive solution. After the comprehensive hyperparameters tuning with respect to changes in learning rates, batch sizes, and optimizers, it is depicted that the designed fine-tuned EfficientNetV2B0 network of 23.8 MB in size with only 6.2 million architectural parameters shows substantial diagnostic performance by achieving a generalized test accuracy of 99.75% on balanced CT kidney database. Furthermore, the designed fine-tuned EfficientNetV2B0 attains high precision, recall, and F1-score of 99.75%, 99.63%, and 99.75%, respectively. Moreover, the final fine-tuned EfficientNetV2B0 ensures its scalability by achieving an impressive diagnostic accuracy of 99.73% on the test set of the original CT kidney dataset as well. Through the extensive evaluation of the proposed transfer learning strategy, it is concluded that the proposed design of fine-tuned EfficientNetV2B0 outperforms its counterparts in terms of accuracy and computational efficiency for chronic kidney disease diagnosis tasks. The final fine-tuned EfficientNetV2B0 serves as an accurate, efficient, and computationally inexpensive solution tailored for real-time deployment on medical or mobile edge devices.},
  archive      = {J_PEERJCS},
  author       = {Zeshan Aslam Khan and Muhammad Waqar and Hashir Ullah Khan and Naveed Ishtiaq Chaudhary and Abeer TMA Khan and Iqra Ishtiaq and Farrukh Aslam Khan and Muhammad Asif Zahoor Raja},
  doi          = {10.7717/peerj-cs.2800},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2800},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fine-tuned deep transfer learning: An effective strategy for the accurate chronic kidney disease classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-stage object detection in low-light environments using deep learning image enhancement. <em>PEERJCS</em>, <em>11</em>, e2799. (<a href='https://doi.org/10.7717/peerj-cs.2799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a two-stage object detection system specifically tailored for low-light conditions. In the initial stage, supervised deep learning image enhancement techniques are utilized to improve image quality and enhance features. The second stage employs a computer vision algorithm for object detection. Three image enhancement algorithms—ZeroDCE++, Gladnet, and two-branch exposure-fusion network for low-light image enhancement (TBEFN)—were assessed in the first stage to enhance image quality. YOLOv7 was utilized in the object detection phase. The ExDark dataset, recognized for its extensive collection of low-light images, served as the basis for training and evaluation. No-reference image quality evaluators were applied to measure improvements in image quality, while object detection performance was assessed using metrics such as recall and mean average precision (mAP). The results indicated that the two-stage system incorporating TBEFN significantly improved detection performance, achieving a mAP of 0.574, compared to 0.49 for YOLOv7 without the enhancement stage. Furthermore, this study investigated the relationship between object detection performance and image quality evaluation metrics, revealing that the image quality evaluator NIQE exhibited a strong correlation with mAP for object detection. This correlation aids in identifying the features that influence computer vision performance, thereby facilitating its enhancement.},
  archive      = {J_PEERJCS},
  author       = {Ghaith Al-refai and Hisham Elmoaqet and Abdullah Al-Refai and Ahmad Alzu’bi and Tawfik Al-Hadhrami and Abedalrhman Alkhateeb},
  doi          = {10.7717/peerj-cs.2799},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2799},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Two-stage object detection in low-light environments using deep learning image enhancement},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLOv8n-DDSW: An efficient fish target detection network for dense underwater scenes. <em>PEERJCS</em>, <em>11</em>, e2798. (<a href='https://doi.org/10.7717/peerj-cs.2798'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aquaculture is of great significance to economic development. It is assessed by manual periodic sampling traditionally, consumes workforce and material resources, and quickly leads to inadequate supervision, which results in substantial property losses. Fish target detection technology can effectively solve the issue of manual monitoring. However, a majority of current studies are based on ideal underwater environments and are inapplicable to complex underwater aquaculture scenarios. Therefore, the YOLOv8n-DDSW fish target detection algorithm was proposed in this article to resolve the detection difficulties resulting from fish occlusion, deformation and detail loss in complex intensive aquaculture scenarios. (1) The C2f-deformable convolutional network (DCN) module is proposed to take the place of the C2f module in the YOLOv8n backbone to raise the detection accuracy of irregular fish targets. (2) The dual-pooling squeeze-and-excitation (DPSE) attention mechanism is put forward and integrated into the YOLOv8n neck network to reinforce the features of the visible parts of the occluded fish target. (3) Small detection is introduced to make the network more capable of sensing small targets and improving recall. (4) Wise intersection over union (IOU) rather than the original loss function is used for improving the bounding box regression performance of the network. Training and testing are based on the publicly available Kaggle dataset. According to the experimental results, the mAP50, precision (P), recall (R) and mAP50-95 values of the improved algorithm are 3.9%, 3.7%, 6.1%, and 7.7% higher than those of the original YOLOv8n algorithm, respectively. Thus, the algorithm is effective in solving low detection accuracy in intensive aquaculture scenarios and theoretically supports the intelligent and modern development of fisheries.},
  archive      = {J_PEERJCS},
  author       = {Jinwang Yi and Wei Han and Fangfei Lai},
  doi          = {10.7717/peerj-cs.2798},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2798},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {YOLOv8n-DDSW: An efficient fish target detection network for dense underwater scenes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing skin lesion classification: A CNN approach with human baseline comparison. <em>PEERJCS</em>, <em>11</em>, e2795. (<a href='https://doi.org/10.7717/peerj-cs.2795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents an augmented hybrid approach for improving the diagnosis of malignant skin lesions by combining convolutional neural network (CNN) predictions with selective human interventions based on prediction confidence. The algorithm retains high-confidence CNN predictions while replacing low-confidence outputs with expert human assessments to enhance diagnostic accuracy. A CNN model utilizing the EfficientNetB3 backbone is trained on datasets from the ISIC-2019 and ISIC-2020 SIIM-ISIC melanoma classification challenges and evaluated on a 150-image test set. The model’s predictions are compared against assessments from 69 experienced medical professionals. Performance is assessed using receiver operating characteristic (ROC) curves and area under curve (AUC) metrics, alongside an analysis of human resource costs. The baseline CNN achieves an AUC of 0.822, slightly below the performance of human experts. However, the augmented hybrid approach improves the true positive rate to 0.782 and reduces the false positive rate to 0.182, delivering better diagnostic performance with minimal human involvement. This approach offers a scalable, resource-efficient solution to address variability in medical image analysis, effectively harnessing the complementary strengths of expert humans and CNNs.},
  archive      = {J_PEERJCS},
  author       = {Deep Ajabani and Zaffar Ahmed Shaikh and Amr Yousef and Karar Ali and Marwan A. Albahar},
  doi          = {10.7717/peerj-cs.2795},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2795},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing skin lesion classification: A CNN approach with human baseline comparison},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effects of mismatched train and test data cleaning pipelines on regression models: Lessons for practice. <em>PEERJCS</em>, <em>11</em>, e2793. (<a href='https://doi.org/10.7717/peerj-cs.2793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data quality problems are present in all real-world, large-scale datasets. Each of these potential problems can be addressed in multiple ways through data cleaning. However, there is no single best data cleaning approach that always produces a perfect result, meaning that a choice needs to be made about which approach to use. At the same time, machine learning (ML) models are being trained and tested on these cleaned datasets, usually with one single data cleaning pipeline applied. In practice, however, data cleaning pipelines are updated regularly, often without retraining of production models. It is therefore common to apply different test (or production) data than the data on which the models were originally trained. The changes in these new test data and the data cleaning process applied can have potential ramifications for model performance. In this article, we show the impact that altering a data cleaning pipeline between the training and testing steps of an ML workflow can have. Through the fitting and evaluation of over 6,000 models, we find that mismatches between cleaning pipelines on training and test data can have a meaningful impact on regression model performance. Counter-intuitively, such mismatches can improve test set performance and potentially alter model selection choices.},
  archive      = {J_PEERJCS},
  author       = {James Nevin and Michael Lees and Paul Groth},
  doi          = {10.7717/peerj-cs.2793},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2793},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The effects of mismatched train and test data cleaning pipelines on regression models: Lessons for practice},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FLASC: A flare-sensitive clustering algorithm. <em>PEERJCS</em>, <em>11</em>, e2792. (<a href='https://doi.org/10.7717/peerj-cs.2792'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploratory data analysis workflows often use clustering algorithms to find groups of similar data points. The shape of these clusters can provide meaningful information about the data. For example, a Y-shaped cluster might represent an evolving process with two distinct outcomes. This article presents flare-sensitive clustering (FLASC), an algorithm that detects branches within clusters to identify such shape-based subgroups. FLASC builds upon HDBSCAN*—a state-of-the-art density-based clustering algorithm—and detects branches in a post-processing step using within-cluster connectivity. Two algorithm variants are presented, which trade computational cost for noise robustness. We show that both variants scale similarly to HDBSCAN* regarding computational cost and provide similar outputs across repeated runs. In addition, we demonstrate the benefit of branch detection on two real-world data sets. Our implementation is included in the hdbscan Python package and available as a standalone package at https://github.com/vda-lab/pyflasc.},
  archive      = {J_PEERJCS},
  author       = {Daniël M. Bot and Jannes Peeters and Jori Liesenborgs and Jan Aerts},
  doi          = {10.7717/peerj-cs.2792},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2792},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {FLASC: A flare-sensitive clustering algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RARE: Right algorithm for the right errand; a multi-model machine learning-based approach for tourism routes and spots recommendation. <em>PEERJCS</em>, <em>11</em>, e2791. (<a href='https://doi.org/10.7717/peerj-cs.2791'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the globalization of the economy, tourism has emerged as a significant sector of entertainment and economic growth. Optimizing tourist attractions and routes has become crucial in modern travel planning, driven by the increasing demand for personalized recommendations. However, traditional static route-based algorithms struggle to adapt to the rapid expansion of the tourism industry, necessitating the development of dynamic, machine-learning-driven solutions. This study introduces a novel tourism recommendation system integrating multiple machine learning algorithms to provide personalized tourist spot and route recommendations. The proposed approach models the tourist map as a 2D grid of interconnected nodes, allowing for dynamic and adaptive recommendations. The framework employs long short-term memory (LSTM) for spot relevance prediction, support vector machine (SVM) for spot name classification, and depth first search (DFS) for optimal route generation. A k-means clustering approach is also utilized to designate a cluster leader (CL) responsible for managing node information within a specific zone. By inputting a simple textual query, tourists receive optimized travel routes tailored to their preferences, incorporating relevant attractions. The model is implemented in a Python-based environment and evaluated using an augmented Travel Recommendation dataset from Kaggle. Experimental results demonstrate the model’s effectiveness in enhancing tourism planning and user experience, showcasing its potential for advancing intelligent tourism solutions.},
  archive      = {J_PEERJCS},
  author       = {Ling Luo},
  doi          = {10.7717/peerj-cs.2791},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2791},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RARE: Right algorithm for the right errand; a multi-model machine learning-based approach for tourism routes and spots recommendation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Variational quantum classifier-based early identification and classification of chronic kidney disease using sparse autoencoder and LASSO shrinkage. <em>PEERJCS</em>, <em>11</em>, e2789. (<a href='https://doi.org/10.7717/peerj-cs.2789'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two leading causes of chronic kidney disease (CKD) are excessive blood pressure and diabetes. Researchers worldwide utilize the rate of globular filtration and kidney inflammation biomarkers to identify chronic kidney disease that gradually reduces renal function. The mortality rate for CKD is high, and thus, a person with this illness is more likely to pass away at a younger age. Healthcare professionals must diagnose the various illnesses connected to this deadly disease as promptly as possible to lighten the impact of CKD. A quantum machine learning (QML) based technique is presented in this research to help with the early diagnosis and prognosis of CKD. The proposed research comprises four phases: data pre-processing, data augmentation, feature selection, and classification. In the first phase, Kalman filter and data normalization techniques are applied to handle the missing and noisy data. In the second phase, data augmentation uses sparse autoencoders to balance the data for smaller classes. In the third phase, LASSO shrinkage is used to select the significant features in the dataset. Variational Quantum classifiers, a supervised QML technique, are employed in the classification phase to classify chronic kidney diseases. The proposed system has been evaluated on the UCI dataset, which comprises 400 CKD patients in the early stages with 25 attributes. The suggested system was assessed using F1-score, precision, recall, and accuracy as evaluation metrics. With a 99.2% classification accuracy, it was found that this model performed better than the other traditional classifiers used for chronic kidney disease classification.},
  archive      = {J_PEERJCS},
  author       = {P. Parthasarathi and Haya Mesfer Alshahrani and K. Venkatachalam and Jaehyuk Cho},
  doi          = {10.7717/peerj-cs.2789},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2789},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Variational quantum classifier-based early identification and classification of chronic kidney disease using sparse autoencoder and LASSO shrinkage},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic trajectory index method based on large-scale real-time trajectory data. <em>PEERJCS</em>, <em>11</em>, e2785. (<a href='https://doi.org/10.7717/peerj-cs.2785'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constructing a trajectory index can efficiently improve the performances of trajectory data processing, provide basic supports for trajectory data mining. With the constantly growing of trajectory data scale and increasing demands for trajectory retrieval efficiency and accuracy, the indexing methods have become more and more crucial. The indexing method faces significant challenges in terms of spatiotemporal trajectory locality, imbalanced trajectory distribution and low trajectory data value density. To address these, we proposed an indexing method based on large-scale real-time trajectory data, it extends the vertical storage mode of HBase, designs the core index, and optimizes the design of the row key, refines the data retrieval process and provides specific mappings for each independent part of the dataset. Besides, it designs the primary index, implements a dynamic indexing mechanism, dynamically load relevant index based on query strategies to flexibly meet the complex query requirements. Comparative experiments demonstrate that the proposed index method is superior in range retrievals and trajectory retrievals, the responding speed is faster.},
  archive      = {J_PEERJCS},
  author       = {Huawei Zhai and Licheng Cui and Kemal Polat and Fayadh Alenezi},
  doi          = {10.7717/peerj-cs.2785},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2785},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic trajectory index method based on large-scale real-time trajectory data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Early detection and analysis of accurate breast cancer for improved diagnosis using deep supervised learning for enhanced patient outcomes. <em>PEERJCS</em>, <em>11</em>, e2784. (<a href='https://doi.org/10.7717/peerj-cs.2784'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early detection of breast cancer (BC) is essential for effective treatment and improved prognosis. This study compares the performance of various machine learning (ML) algorithms, including convolutional neural networks (CNNs), logistic regression (LR), support vector machines (SVMs), and Gaussian naive Bayes (GNB), on two key datasets, Wisconsin Diagnostic Breast Cancer (WDBC) and Breast Cancer Histopathological Image Classification (BreaKHis). For the BreaKHis dataset, the CNN achieved an impressive accuracy of 92%, with precision, recall, and F1 score values of 91%, 93%, and 91%, respectively. In contrast, LR achieved 88% accuracy, with corresponding precision, recall, and F1 score values of 86%, 87%, and 89%, respectively. SVM and GNB demonstrated 90% and 84% accuracy, respectively, with similar precision, recall, and F1-score metric performances. In the WDBC dataset, LR achieved the highest accuracy of 97.5%, with nearly 97% values for precision, recall, and F1 score. In contrast, CNN attained 96% accuracy with equal recall, precision, and F1 score values of 96%. SVM and GNB followed closely with 95% and 94% accuracy, respectively. Minimising the false negative rate (FNR) and false omission rate (FOR) is vital for improving model reliability, with the LR excelling in the WDBC dataset (FNR: 5.9%, FOR: 4.8%) and the CNN performing best in the BreaKHis dataset (FNR: 8.3%, FOR: 7.0%). The results demonstrate that CNN outperforms traditional models across both datasets, highlighting its potential for early and accurate BC detection.},
  archive      = {J_PEERJCS},
  author       = {Mandika Chetry and Ruiling Feng and Samra Babar and Hao Sun and Imran Zafar and Mohamed Mohany and Hassan Imran Afridi and Najeeb Ullah Khan and Ijaz Ali and Muhammad Shafiq and Sabir Khan},
  doi          = {10.7717/peerj-cs.2784},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2784},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Early detection and analysis of accurate breast cancer for improved diagnosis using deep supervised learning for enhanced patient outcomes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSGRec: Dual-path selection graph for multimodal recommendation. <em>PEERJCS</em>, <em>11</em>, e2779. (<a href='https://doi.org/10.7717/peerj-cs.2779'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advancement of digital streaming technology, multi-modal recommendation systems have gained significant attention. Current graph-based multi-modal recommendation approaches typically model user interests using either user interaction signals or multi-modal item information derived from heterogeneous graphs. Although methods based on graph convolutional networks (GCNs) have achieved notable success, they still face two key limitations: (1) the narrow interpretation of interaction information, leading to incomplete modeling of user behavior, and (2) a lack of fine-grained collaboration between user behavior and multi-modal information. To address these issues, we propose a novel method by decomposing interaction information into two distinct signal pathways, referred to as a dual-path selection architecture, named Dual-path Selective Graph Recommender (DSGRec). DSGRec is designed to deliver more accurate and personalized recommendations by facilitating the positive collaboration of interactive data and multi-modal information. To further enhance the represetation of these signals, we introduce two key components: (1) behavior-aware multimodal signal augmentation, which extract rich multimodal semantic information; and (b) hypergraph-guided cooperative signal enhancement, which captures hybrid global information. Our model learns dual-path selection signals via a primary module and introduces two auxiliary modules to adjust these signals. We introduce independent contrastive learning tasks for the auxiliary signals, enabling DSGRec to explore the mechanisms behind feature embeddings from different perspectives. This approach ensures that each auxiliary module aligns with the user-item interaction view independently, calibrating its contribution based on historical interactions. Extensive experiments conducted on three benchmark datasets demonstrate the superiority of DSGRec over several state-of-the-art recommendation baselines, highlighting the effectiveness of our method.},
  archive      = {J_PEERJCS},
  author       = {Zihao Liu and Wen Qu},
  doi          = {10.7717/peerj-cs.2779},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2779},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DSGRec: Dual-path selection graph for multimodal recommendation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Smart waste management and classification system using advanced IoT and AI technologies. <em>PEERJCS</em>, <em>11</em>, e2777. (<a href='https://doi.org/10.7717/peerj-cs.2777'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effective management of municipal solid waste is a critical global issue, affecting both urban and rural areas. To address the growing volume of solid waste, proactive planning is essential. Traditionally, solid waste is often disposed of without segregation, preventing recycling and the recovery of raw materials. Proper waste segregation is a fundamental requirement for effective solid waste management, allowing materials to be recycled efficiently. Emerging technologies such as artificial intelligence (AI), machine learning (ML), and the Internet of Things (IoT) offer powerful tools for identifying recyclable materials like glass, plastic, and metal within solid waste. The primary goal of this research is to contribute to a cleaner environment, reduce infant mortality, improve maternal health, and support efforts to combat HIV/AIDS, malaria, and other diseases. This study introduces an intelligent and smart solid waste management system (iSSWMs) designed to smartly collect and segregate solid waste. The proposed system focuses on three types of materials: plastic, glass, and metal. The first phase involves waste collection using smart bins connected to a mobile application, which sends notifications when the bins are full. In the second phase, we develop a deep learning-based mechanical model to segregate the waste, using the VGG-19 model, which achieved a performance accuracy of 99.7% during training. To the best of our knowledge, iSSWMs is a promising framework that integrates both waste collection and segregation through the use of cutting-edge technologies, delivering high accuracy and efficiency.},
  archive      = {J_PEERJCS},
  author       = {Abdullah Alourani and M. Usman Ashraf and Mohammed Aloraini},
  doi          = {10.7717/peerj-cs.2777},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2777},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Smart waste management and classification system using advanced IoT and AI technologies},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Photovoltaic panel defect detection algorithm based on infrared imaging and improved YOLOv8. <em>PEERJCS</em>, <em>11</em>, e2776. (<a href='https://doi.org/10.7717/peerj-cs.2776'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the challenges of high missed detection rates, complex backgrounds, unclear defect features, and uneven difficulty levels in target detection during the industrial process of photovoltaic panel defect detection, this article proposes an infrared detection method based on computer vision, with enhancements built upon the YOLOv8 model. First, a multi-channel squeeze-and-excitation network is introduced to improve feature extraction capabilities and is integrated into the neck network. Second, GhostConv and BoTNet are incorporated into the backbone network to reduce model parameters while enhancing defect detection performance. Finally, the Focaler-Complete Intersection over Union (Focaler-CIoU) loss function is employed to tackle the issue of imbalanced difficulty in target detection tasks. The method is evaluated on the PV-Multi-Defect-main dataset and further validated through a generalization test on the PVEL-AD dataset. Results demonstrate that, compared with the baseline YOLOv8 model, the proposed approach achieves significant improvements in precision (3.6%), recall (10.4%), mAP50 (4.8%), and mAP50-95 (4.5%) while maintaining nearly the same parameter count. On the PVEL-AD dataset, the method effectively addresses the challenge of feature extraction failure for dislocation-type defects, achieving substantial gains in precision (7.8%), recall (17.1%), mAP50 (19.5%), and mAP50-95 (13.2%). Furthermore, comparisons with several state-of-the-art detection algorithms reveal that the proposed method consistently delivers improved detection performance, validating its effectiveness as a robust solution for photovoltaic panel defect detection.},
  archive      = {J_PEERJCS},
  author       = {Jingdong Wang and Zhu Cheng},
  doi          = {10.7717/peerj-cs.2776},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2776},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Photovoltaic panel defect detection algorithm based on infrared imaging and improved YOLOv8},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-label classification for image tamper detection based on swin-T segmentation network in the spatial domain. <em>PEERJCS</em>, <em>11</em>, e2775. (<a href='https://doi.org/10.7717/peerj-cs.2775'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The majority of deep learning methods for detecting image forgery fail to accurately detect and localize the tampering operations. Furthermore, they only support a single image tampering type. Our method introduces three key innovations: (1) A spatial perception module that combines the spatial rich model (SRM) with constrained convolution, enabling focused detection of tampering traces while suppressing interference from image content; (2) A hierarchical feature learning architecture that integrates Swin Transformer with UperNet for effective multi-scale tampering pattern recognition; and (3) A comprehensive optimization strategy including auxiliary supervision, self-supervised learning, and hard example mining, which significantly improves model convergence and detection accuracy. Comprehensive experiments are performed on two established datasets; namely MixTamper and DocTamper with 19,600 and 170,000 images, respectively. The experimental findings demonstrate that the proposed model enhances the IoU index by 13% compared to the leading algorithms. Additionally, it can accurately detect multiple tampering types from a single image.},
  archive      = {J_PEERJCS},
  author       = {Li Li and Kejia Zhang and Jianfeng Lu and Shanqing Zhang},
  doi          = {10.7717/peerj-cs.2775},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2775},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-label classification for image tamper detection based on swin-T segmentation network in the spatial domain},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Personality-based pair programming: Toward intrinsic motivation alignment in very small entities. <em>PEERJCS</em>, <em>11</em>, e2774. (<a href='https://doi.org/10.7717/peerj-cs.2774'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aim This study explores whether personality‐based role assignments (Pilot, Navigator, Solo) can raise intrinsic motivation in pair programming, focusing on designing a framework and process extension for the resource‐constrained environment of very small entities (VSEs). Method We employed a mixed‐methods design across three quasi-experimental datasets (n = 73 participants), applying linear mixed‐effects (LME) modeling to assess motivational outcomes and thematically analyzing (n = 25) interviews for socio‐psychological insights. Findings Openness strongly correlates with Pilot roles; Extraversion & Agreeableness favor Navigator roles; and Neuroticism aligns more comfortably with Solo roles—each yielding substantial boosts in intrinsic motivation (up to 60–65%). Twelve qualitative themes underscore the influence of mentorship, pairing constellations, and flow disruptions on developer experiences. Implications Building on these results, we propose the role‐optimization motivation alignment (ROMA) framework, mapped to the ISO/IEC 29110 Software Basic Profile and Agile Guidelines, with practical tasks (T1–T7) to facilitate systematic role–trait alignments in small agile teams. Although our data primarily involve Gen‐Z undergraduates, the recurring patterns suggest broader applicability, further supported by a separately published application for ongoing generalizability. Conclusion Personality‐driven role optimization may significantly enhance collaboration and developer satisfaction in VSEs, though further studies in professional settings and investigations into AI‐assisted or distributed pair programming are warranted.},
  archive      = {J_PEERJCS},
  author       = {Marcel Valovy and Alena Buchalcevova},
  doi          = {10.7717/peerj-cs.2774},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2774},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Personality-based pair programming: Toward intrinsic motivation alignment in very small entities},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph neural networks embedded with domain knowledge for cyber threat intelligence entity and relationship mining. <em>PEERJCS</em>, <em>11</em>, e2769. (<a href='https://doi.org/10.7717/peerj-cs.2769'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The escalating frequency and severity of cyber-attacks have presented formidable challenges to the safeguarding of cyberspace. Named Entity Recognition (NER) technology is utilized for the rapid identification of threat entities and their relationships within cyber threat intelligence, enabling security researchers to be promptly informed of the occurrence of cyber threats, thereby enhancing the efficiency of security defense and analysis. However, current models for identifying network threat entities and extracting relationships suffer from limitations such as the inadequate representation of textual semantic information, insufficient granularity in threat entity recognition, and errors in relationship extraction propagation. To address these issues, this article proposes a novel model for Network Threat Entity Recognition and Relationship Extraction (CtiErRe). Additionally, it redefines seven network threat entities and two types of relationships between threat entities. Specifically, first, domain knowledge is collected to build a domain knowledge graph, which is then embedded using graph convolutional networks (GCN) to enhance the feature representation of threat intelligence text. Next, the features from domain knowledge graph embedding and those generated by the bidirectional encoder representations from transformers (BERT) model are fused using the Layernorm algorithm. Finally, the fused features are processed using the GlobalPointer algorithm to generate both the threat entity type matrix and the threat entity relation type matrix, thereby enabling the identification of threat entities and their relationships. To validate our proposed model, we conducted extensive experiments, and the results demonstrate its superiority over existing models. Our model performs remarkably in threat entity recognition tasks, with accuracy and F1 scores reaching 92.13% and 93.11%, respectively. In the relationship extraction task, our model achieves accuracy and F1 scores of 91.45% and 92.45%, respectively.},
  archive      = {J_PEERJCS},
  author       = {Gan Liu and Kai Lu and Saiqi Pi},
  doi          = {10.7717/peerj-cs.2769},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2769},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Graph neural networks embedded with domain knowledge for cyber threat intelligence entity and relationship mining},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing cybersecurity through autonomous knowledge graph construction by integrating heterogeneous data sources. <em>PEERJCS</em>, <em>11</em>, e2768. (<a href='https://doi.org/10.7717/peerj-cs.2768'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersecurity plays a critical role in today’s modern human society, and leveraging knowledge graphs can enhance cybersecurity and privacy in the cyberspace. By harnessing the heterogeneous and vast amount of information on potential attacks, organizations can improve their ability to proactively detect and mitigate any threat or damage to their online valuable resources. Integrating critical cyberattack information into a knowledge graph offers a significant boost to cybersecurity, safeguarding cyberspace from malicious activities. This information can be obtained from structured and unstructured data, with a particular focus on extracting valuable insights from unstructured text through natural language processing (NLP). By storing a wide range of cyber threat information in a semantic triples form which machines can interpret autonomously, cybersecurity experts gain improved visibility and are better equipped to identify and address cyber threats. However, constructing an efficient knowledge graph poses challenges. In our research, we construct a cybersecurity knowledge graph (CKG) autonomously using heterogeneous data sources. We further enhance the CKG by applying logical rules and employing graph analytic algorithms. To evaluate the effectiveness of our proposed CKG, we formulate a set of queries as questions to validate the logical rules. Ultimately, the CKG empowers experts to efficiently analyze data and gain comprehensive understanding of cyberattacks, thereby help minimize potential attack vectors.},
  archive      = {J_PEERJCS},
  author       = {Hatoon Alharbi and Ali Hur and Hasan Alkahtani and Hafiz Farooq Ahmad},
  doi          = {10.7717/peerj-cs.2768},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2768},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing cybersecurity through autonomous knowledge graph construction by integrating heterogeneous data sources},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WG-storm: A resource-aware scheduler for distributed stream processing engines. <em>PEERJCS</em>, <em>11</em>, e2767. (<a href='https://doi.org/10.7717/peerj-cs.2767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing engines (SPEs) allow applications to process a large amount of data in real-time. However, to schedule big data applications; the SPEs create several challenges regarding resource utilisation, dynamic configurations, heterogeneous environment, resource awareness, load balancing, etc. As the volume of data increases over time, it also poses a challenge to predict the resource and application requirements for processing. All these factors play an important role, they can cause problems in achieving maximum throughput due to inefficiency in any of them. Most SPEs ignore the topology’s structure, which may minimise throughput during scheduling and may increase network latency. In this article, a topology-aware and resource-aware scheduler (named WG-Storm) is proposed based on a directed acyclic graph (DAG) that enhances resource usage and overall throughput using efficient task assignment. WG-Storm is built on Apache Storm. Results are generated using the two linear topologies and compared with the five state-of-art schedulers including A3-Storm, Default, Isolation, Multi-tenant, and Resource-aware. The experimental results show up to 30% increased throughput using the least required computing resources in a heterogeneous cluster.},
  archive      = {J_PEERJCS},
  author       = {Rizwan Ali and Asif Muhammad and Muhammad Aleem and Omair Shafiq},
  doi          = {10.7717/peerj-cs.2767},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2767},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {WG-storm: A resource-aware scheduler for distributed stream processing engines},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Epileptic seizures diagnosis and prognosis from EEG signals using heterogeneous graph neural network. <em>PEERJCS</em>, <em>11</em>, e2765. (<a href='https://doi.org/10.7717/peerj-cs.2765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epilepsy, often associated with neurodegenerative disorders following brain strokes, manifests as abnormal electrical activity bursts in the cerebral cortex, disrupting regular brain function. Electroencephalogram (EEG) recordings capture these distinctive brain signals, offering crucial insights into seizure detection and management. This study presents a novel approach leveraging a graph neural network (GNN) model with a heterogeneous graph representation to detect epileptic seizures from EEG data. Utilizing the well-established CHB-MIT EEG dataset for training and evaluation, the proposed method includes preprocessing steps such as signal segmentation, resampling, label encoding, normalization, and exploratory data analysis. We employed a standard train-test split with stratified sampling to ensure class distribution and reduce bias. Experimental comparisons with long short-term memory (LSTM) and recurrent neural network (RNN) models highlight the GNN’s superior performance, achieving a classification accuracy of 98.0% and demonstrating incremental improvements in precision and F1-score. These findings emphasize the efficacy of GNN in capturing spatial and temporal dependencies within EEG data, surpassing conventional deep learning techniques. Furthermore, the study highlights the model’s interpretability, which is essential for clinical decision-making. By advancing EEG-based seizure prediction methods, this research offers a robust framework for enhancing patient outcomes in epilepsy management while addressing the limitations of existing approaches.},
  archive      = {J_PEERJCS},
  author       = {Areej Alasiry and Gabriel Avelino Sampedro and Ahmad Almadhor and Roben A. Juanatas and Shtwai Alsubai and Vincent Karovic},
  doi          = {10.7717/peerj-cs.2765},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2765},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Epileptic seizures diagnosis and prognosis from EEG signals using heterogeneous graph neural network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting no-shows at outpatient appointments in internal medicine using machine learning models. <em>PEERJCS</em>, <em>11</em>, e2762. (<a href='https://doi.org/10.7717/peerj-cs.2762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high prevalence of patient absenteeism in medical appointments poses significant challenges for healthcare providers and patients, causing delays in service delivery and increasing operational inefficiencies. Addressing this issue is crucial in the internal medicine department, a fundamental pillar of comprehensive adult healthcare that manages various chronic and complex conditions. To mitigate absenteeism, we present an innovative application of machine learning models specifically designed to predict the risk of patient absenteeism in the internal medicine department of Fundación Valle del Lili, a high-complexity hospital in Colombia. Leveraging an institutional database, we conducted a statistical analysis to identify critical variables influencing absenteeism risk, including clinical and sociodemographic factors and characteristics of previously attended appointments. Our study evaluated seven distinct machine learning models, explored various data processing techniques, and addressed class imbalance through oversampling and undersampling strategies. Hyperparameter optimization was conducted for each model configuration, culminating in selecting the Bagging RandomForest model, which demonstrated outstanding performance when combined with standardized data and balanced using the Synthetic Minority Oversampling Technique (SMOTE). Additionally, Shapley values (SHAP) were applied to enhance the interpretability of the model, enabling the identification of the most influential variables in predicting medical absenteeism, such as the number of previous absences, the day and month of the appointment, and diagnosed diseases. The selected model achieved a predictive accuracy of 84.80 ± 0.81%, an AUC value of 0.89, an F1-score of 84.75%, and a recall of 83.02% in cross-validation experiments. These results highlight the potential of our experimental approach to identify the most suitable model for proactively predicting patients at high risk of absenteeism, optimizing resource allocation, and improving the quality of medical care in internal medicine in the future. Our methodology provides a foundation for reducing operational inefficiencies and strengthening intervention strategies. This benefits healthcare providers and patients through more timely and effective care. Ultimately, this approach contributes to improving patient outcomes and institutional efficiency.},
  archive      = {J_PEERJCS},
  author       = {Felipe Ocampo Osorio and Santiago Pedroza Gomez and David Esteban Rebellón Sanchez and Richard Ramirez Fernandez and Reinel Tabares-Soto and Mario Alejandro Bravo-Ortíz and Gustavo Adolfo Cruz Suarez},
  doi          = {10.7717/peerj-cs.2762},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2762},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting no-shows at outpatient appointments in internal medicine using machine learning models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of deep learning models in automatic classification of coffee bean species. <em>PEERJCS</em>, <em>11</em>, e2759. (<a href='https://doi.org/10.7717/peerj-cs.2759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As one of the most widely consumed beverages worldwide, coffee is characterized by its diverse flavor profiles and complex production processes. In this study, deep learning-based image processing techniques are employed for the automatic classification of coffee bean species with high accuracy. To achieve this, images of three different coffee bean species (Starbucks Pike Place, Espresso, and Kenya) were classified using five CNN-based models: Xception, DenseNet201, InceptionV3, InceptionResNetV2, and DenseNet121. The dataset comprises 1,554 coffee bean images. Cross-validation was applied to assess the models’ performance, and classification accuracy was evaluated using performance metrics. Among the tested models, InceptionV3 achieved the highest classification accuracy (93%) and precision (95%), with the lowest loss rate (0.12), making it the most effective model in this study. As a result of the experiments, the average classification success rates of the models were determined as follows: 93% for InceptionV3, 92% for DenseNet121, 91% for Xception, 91% for InceptionResNetV2, and 90% for DenseNet201. These findings indicate that InceptionV3 demonstrates the highest performance. It is anticipated that this study will make significant contributions to applications in coffee bean classification.},
  archive      = {J_PEERJCS},
  author       = {Adem Korkmaz and Tarık Talan and Selahattin Koşunalp and Teodor Iliev},
  doi          = {10.7717/peerj-cs.2759},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2759},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparison of deep learning models in automatic classification of coffee bean species},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TMAC: A transformer-based partially observable multi-agent communication method. <em>PEERJCS</em>, <em>11</em>, e2758. (<a href='https://doi.org/10.7717/peerj-cs.2758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective communication plays a crucial role in coordinating the actions of multiple agents. Within the realm of multi-agent reinforcement learning, agents have the ability to share information with one another through communication channels, leading to enhanced learning outcomes and successful goal attainment. Agents are limited by their observations and communication ranges due to increasingly complex location arrangements, making multi-agent collaboration based on communication increasingly difficult. In this article, for multi-agent communication in some partially observable scenarios, we propose a Transformer-based Partially Observable Multi-Agent Communication algorithm (TMAC), which improves agents extracting features and generating output messages. Meanwhile, a self-message fusing module is proposed to obtain features from multiple sources. Therefore, agents can achieve better collaboration through communication. At the same time, we performed experimental verification in the surviving and the StarCraft Multi-Agent Challenge (SMAC) environments where agents had limited local observation and could only communicate with neighboring agents. In two test environments, our method achieves an improvement in performance 6% and 10% over the baseline algorithm, respectively. Our code is available at https://gitee.com/xs-lion/tmac.},
  archive      = {J_PEERJCS},
  author       = {Xuesi Li and Shuai Xue and Ziming He and Haobin Shi},
  doi          = {10.7717/peerj-cs.2758},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2758},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TMAC: A transformer-based partially observable multi-agent communication method},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study of trust mining algorithms for beacon nodes in large-scale network environments. <em>PEERJCS</em>, <em>11</em>, e2755. (<a href='https://doi.org/10.7717/peerj-cs.2755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a large-scale network environment, node positioning is prone to large deviations. Mining beacon node trust is the basis for precise node positioning in the network environment. Therefore, this article studies the trust degree mining algorithm of beacon nodes in a large-scale network environment. First, according to the distance error evaluation and probability function of beacon nodes in the large-scale network environment, the direct trust degree of beacon nodes is obtained. The trust degree is converted into influence, and the influence of beacon nodes is mined using the seepage theory to determine the beacon node with the highest impact in the large-scale network environment. Then, according to the influence of nodes, received signal strength indicator (RSSI) is used to optimize the conventional distance vector hop (DV-Hop) node location algorithm. The influence weights the average hop distance of beacon nodes. The weight of the influence of beacon nodes defines the average hop distance of unknown nodes. The average hop distance information of unknown nodes is taken from more high-influence beacon nodes, solving the problem of significant positioning errors caused by the uncertainty of location targets. Finally, the security status of nodes is reflected according to the degree of trust of different nodes to beacon nodes. The experimental results show that the algorithm can accurately locate other nodes in a wide network environment when the number of beacon nodes and communication distance change, and the trust degree of nodes mined can accurately reflect the security status of nodes.},
  archive      = {J_PEERJCS},
  author       = {Yanyan Jiang},
  doi          = {10.7717/peerj-cs.2755},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2755},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A study of trust mining algorithms for beacon nodes in large-scale network environments},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid integration framework based on LOOCV and SARIMA: Relationship exploring and predictive analysis between discipline attention and literature research. <em>PEERJCS</em>, <em>11</em>, e2754. (<a href='https://doi.org/10.7717/peerj-cs.2754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analyzing the relationship between the discipline of network attention and literature research can provide new insights for the innovative development of future disciplines. Many current studies focus on network attention, but its innovative application in the field of subject teaching has not been fully verified. Based on this, this paper proposed a relationship analysis and predictive analysis (RAPA) framework based on leave-one-out cross-validation (LOOCV) and Seasonal Auto-Regressive Integrated Moving Average (SARIMA) to explore the relationship between subject attention and literature research from the perspective of junior high school information technology. Based on the RAPA framework, five key keywords of this subject were extracted by combining the Baidu Index and China National Knowledge Infrastructure (CNKI) in first. Secondly, LOOCV was used to explore the relationship between subject attention represented by keywords and literature researches. Then, SARIMA was used to predict the future trends of subject attention and its literature researches. Finally, the prediction errors of different methods were compared. Based on the RAPA framework, the correlation analysis found that the r-values of subject attention and literature researches were all greater than 0.75, indicating a positive correlation between them. The predictive analysis found that the subject attention of junior high school information technology will be flat or decline in the next 2 years. Meanwhile, the amount of literature in this discipline has decreased compared to previous years, with an average of approximately 136. The prediction comparison showed that the prediction method in this study has a smaller mean absolute error (MAE) than other methods, and the MAE difference is 3.51. This indicated that subject attention, as an auxiliary variable of scientific research literature, is conducive to the quantitative analysis of literature research. At the same time, this study revealed the influence and role of big data represented by Internet attention in educational research.},
  archive      = {J_PEERJCS},
  author       = {Yulin Zhao and Junke Li and Kai Liu and Chaowang Shang},
  doi          = {10.7717/peerj-cs.2754},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2754},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid integration framework based on LOOCV and SARIMA: Relationship exploring and predictive analysis between discipline attention and literature research},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural network-based symmetric encryption algorithm with encrypted traffic protocol identification. <em>PEERJCS</em>, <em>11</em>, e2750. (<a href='https://doi.org/10.7717/peerj-cs.2750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptography is a cornerstone of power grid security, with the symmetry and asymmetry of cryptographic algorithms directly influencing the resilience of power systems against cyberattacks. Cryptographic algorithm identification, a critical component of cryptanalysis, is pivotal to assessing algorithm security and hinges on the core characteristics of symmetric and asymmetric encryption methods. A key challenge lies in discerning subtle spatial distribution patterns within ciphertext data to infer the underlying cryptographic algorithms, which is essential for ensuring the communication security of power systems. In this study, we first introduce a plaintext guessing model (SCGM model) based on symmetric encryption algorithms, leveraging the strengths of convolutional neural networks to evaluate the plaintext guessing capabilities of four symmetric encryption algorithms. This model is assessed for its learning efficacy and practical applicability. We investigate protocol identification for encrypted traffic data, proposing a novel scheme that integrates temporal and spatial features. Special emphasis is placed on the performance of algorithms within both symmetric and asymmetric frameworks. Experimental results demonstrate the effectiveness of our proposed scheme, highlighting its potential for enhancing power grid security.},
  archive      = {J_PEERJCS},
  author       = {Jiakai Hao and Ming Jin and Yuting Li and Yuxin Yang},
  doi          = {10.7717/peerj-cs.2750},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2750},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Neural network-based symmetric encryption algorithm with encrypted traffic protocol identification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Detecting malicious code variants using convolutional neural network (CNN) with transfer learning. <em>PEERJCS</em>, <em>11</em>, e2727. (<a href='https://doi.org/10.7717/peerj-cs.2727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware presents a significant threat to computer networks and devices that lack robust defense mechanisms, despite the widespread use of anti-malware solutions. The rapid growth of the Internet has led to an increase in malicious code attacks, making them one of the most critical challenges in network security. Accurate identification and classification of malware variants are crucial for preventing data theft, security breaches, and other cyber risks. However, existing malware detection methods are often inefficient or inaccurate. Prior research has explored converting malicious code into grayscale images, but these approaches are often computationally intensive, especially in binary form. To address these challenges, we propose the Malware Variants Detection System (MVDS), a novel technique that transforms malicious code into color images, enhancing malware detection capabilities compared to traditional methods. Our approach leverages the richer information in color images to achieve higher classification accuracy than grayscale-based methods. We further improve the detection process by employing transfer learning to automatically identify and classify malware images based on their distinctive features. Empirical results demonstrate that MVDS achieves 97.98% accuracy with high detection speed, highlighting its potential for practical implementation in strengthening network security.},
  archive      = {J_PEERJCS},
  author       = {Nazish Younas and Shazia Riaz and Saqib Ali and Rafiullah Khan and Farman Ali and Daehan Kwak},
  doi          = {10.7717/peerj-cs.2727},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2727},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Detecting malicious code variants using convolutional neural network (CNN) with transfer learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An activity theory-based exploration of “Eyeland”, a task-based serious game for EFL visually impaired students. <em>PEERJCS</em>, <em>11</em>, e2631. (<a href='https://doi.org/10.7717/peerj-cs.2631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates how the Eyeland app, an accessible task-based serious game for English as a foreign language (EFL), can remediate traditional lessons for both visually impaired students (VISs) and sighted students (those without visual impairments) in a public high school in Colombia. Using an activity theory framework and its derived model, the Activity Theory-Based Model of Serious Games (ATMSG), the study explores the characteristics of traditional EFL lessons designed for these students, the adjustments made while integrating the Eyeland app, and the resulting changes in student experiences. The research employed action research cycles involving teachers in reflective processes that included planning, observing, acting, and re-planning to adapt and integrate Eyeland into the classroom. Qualitative research methods—field observations, focus groups, usability surveys, teacher interviews, and document analysis—were used to collect data and analyze how Eyeland was implemented and its effects on teaching and learning. Findings indicate that Eyeland effectively remediated traditional lessons by offering accessible, interactive features such as auditory, tactile, and visual support. These enhancements improved engagement for both sighted and visually impaired students. Traditional EFL lessons, which relied heavily on visual materials and teacher-centered methods, were transformed into more interactive, task-based activities that encouraged greater collaboration and student autonomy. Adjustments included redesigning lesson plans and rearranging classroom layouts to foster inclusion. Both sighted and visually impaired students reported positive experiences, particularly valuing the increased autonomy and engagement provided by Eyeland’s interactive tasks. The study highlights significant changes made to the lessons, with visually impaired students reporting predominantly positive experiences, though some teachers were reluctant to engage with the app. Eyeland contributed to the creation of inclusive classrooms by shifting from a teacher-centered approach to a student-centered learning environment that promoted better language learning outcomes. ATMSG was instrumental in analyzing how Eyeland fostered inclusive learning practices and provided valuable insights into re-mediation strategies, pedagogical planning, and the development of accessible content for EFL learners.},
  archive      = {J_PEERJCS},
  author       = {Karen Villalba and Heydy Robles and Miguel Jimeno and Martha Cecilia Delgado-Cañas and Adriana Perez and Francisco Quintero},
  doi          = {10.7717/peerj-cs.2631},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2631},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An activity theory-based exploration of “Eyeland”, a task-based serious game for EFL visually impaired students},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development and evaluation of customized software to automatically align macula and optic disc centered scanning laser ophthalmoscope fundus images. <em>PEERJCS</em>, <em>11</em>, e2621. (<a href='https://doi.org/10.7717/peerj-cs.2621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ophthalmology, the angle between the center of the optic nerve head and the center of sharpest vision (foveola) is a posterior fundus landmark parameter of the retina of the human eye. Together with the optic disc-fovea distance, it characterizes the position of the optic nerve head in relationship to the foveola. The optic disc-fovea angle markedly influences the regional distribution of retinal layer thickness patterns, specifically the retinal nerve fiber layer thickness measured at the optic disc. Thus, the optic disc-fovea angle needs to be determined and routinely taken into account in morphological glaucoma diagnosis and in the assessment of structure-function relationship in optic nerve diseases. However, despite the urgency of this information, currently the optic disc-fovea line and its angle are routinely not measured. Obtaining it post-measurement requires manual registration of the macula and optic disc optical coherence tomography (OCT) imaging data. OCT manufacturer-delivered software does not provide automated image registration. Therefore researchers are forced to manually perform the alignment over different scanning regions. To fill this gap, we provide two software packages which can be applied to routinely acquired clinical OCT data to automatically align macula and optic disc images. In this work, we introduce and comparatively evaluate two separate software packages (BloodVesselReg and OCTFundusReg) to automatically align macula and optic disc centered OCT volume scans based on their respective scanning laser ophthalmoscope (SLO) fundus images. BloodVesselReg implements an image registration and mosaicing algorithm based on retinal blood vessels. OCTFundusReg optimizes a general-purpose image registration toolkit to operate on SLO images. Both methods were independently developed by different subgroups of authors of this study using a training dataset of 18,047 eyes from a population-based study. The methods were tested on a dataset of 3,570 eyes from glaucoma patients, with success/failure assessed by visual inspection and compared to failure reporting of the methods themselves. BloodVesselReg had a slightly higher accuracy (94.7%) than OCTFundusReg (93.9%). Both methods together failed on only 1% of the eyes. BloodVesselReg reported 165 out of its 190 failures. OCTFundusReg provides a continuous failureAlert parameter which resulted in an area under the receiver operating characteristics curve (AUC) of 0.91 from a logistic regression model. When including the difference of fitting related parameters between the two methods, the AUC improved to 0.95. Both methods had success rates of over 90% when applied in isolation to a clinical testing dataset. When applying them together, the rate of at least one of the method succeeding was 99%. The methods are highly promising for applications under real-world clinical conditions and might help to facilitate disease detection and monitoring over time.},
  archive      = {J_PEERJCS},
  author       = {M. Elena Martinez-Perez and Franziska G. Rauscher and Pingping Zhao and Tobias Elze},
  doi          = {10.7717/peerj-cs.2621},
  journal      = {PeerJ Computer Science},
  month        = {4},
  pages        = {e2621},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Development and evaluation of customized software to automatically align macula and optic disc centered scanning laser ophthalmoscope fundus images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing AI-powered translation education tools: A framework for parallel sentence generation using SauLTC and LLMs. <em>PEERJCS</em>, <em>11</em>, e2788. (<a href='https://doi.org/10.7717/peerj-cs.2788'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Translation education (TE) demands significant effort from educators due to its labor-intensive nature. Developing computational tools powered by artificial intelligence (AI) can alleviate this burden by automating repetitive tasks, allowing instructors to focus on higher-level pedagogical aspects of translation. This integration of AI has the potential to significantly enhance the efficiency and effectiveness of translation education. The development of effective AI-based tools for TE is hampered by a lack of high-quality, comprehensive datasets tailored to this specific need, especially for Arabic. While the Saudi Learner Translation Corpus (SauLTC), a unidirectional English-to-Arabic parallel corpus, constitutes a valuable resource, its current format is inadequate for generating the parallel sentences required for a didactic translation corpus. This article proposes leveraging large language models like the Generative Pre-trained Transformer (GPT) to transform SauLTC into a parallel sentence corpus. Using cosine similarity and human evaluation, we assessed the quality of the generated parallel sentences, achieving promising results with an 85.2% similarity score using Language-agnostic BERT Sentence Embedding (LaBSE) in conjunction with GPT, outperforming other investigated embedding models. The results demonstrate the potential of AI to address critical dataset challenges in quest of effective data driven solutions to support translation education.},
  archive      = {J_PEERJCS},
  author       = {Moneerh Aleedy and Fatma Alshihri and Souham Meshoul and Maha Al-Harthi and Salwa Alramlawi and Badr Aldaihani and Hadil Shaiba and Eric Atwell},
  doi          = {10.7717/peerj-cs.2788},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2788},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Designing AI-powered translation education tools: A framework for parallel sentence generation using SauLTC and LLMs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LIU-NET: Lightweight inception U-net for efficient brain tumor segmentation from multimodal 3D MRI images. <em>PEERJCS</em>, <em>11</em>, e2787. (<a href='https://doi.org/10.7717/peerj-cs.2787'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmenting brain tumors is a critical task in medical imaging that relies on advanced deep-learning methods. However, effectively handling complex tumor regions requires more comprehensive and advanced strategies to overcome challenges such as computational complexity, the gradient vanishing problem, and variations in size and visual impact. To overcome these challenges, this research presents a novel and computationally efficient method termed lightweight Inception U-Net (LIU-Net) for the accurate brain tumor segmentation task. LIU-Net balances model complexity and computational load to provide consistent performance and uses Inception blocks to capture features at different scales, which makes it relatively lightweight. Its capability to efficiently and precisely segment brain tumors, especially in challenging-to-detect regions, distinguishes it from existing models. This Inception-style convolutional block assists the model in capturing multiscale features while preserving spatial information. Moreover, the proposed model utilizes a combination of Dice loss and Focal loss to handle the class imbalance issue. The proposed LIU-Net model was evaluated on the benchmark BraTS 2021 dataset, where it generates remarkable outcomes with a Dice score of 0.8121 for the enhancing tumor (ET) region, 0.8856 for the whole tumor (WT) region, and 0.8444 for the tumor core (TC) region on the test set. To evaluate the robustness of the proposed architecture, LIU-Net was cross-validated on an external cohort BraTS 2020 dataset. The proposed method obtained a Dice score of 0.8646 for the ET region, 0.9027 for the WT region, and 0.9092 for the TC region on the external cohort BraTS 2020 dataset. These results highlight the effectiveness of integrating the Inception blocks into the U-Net architecture, making it a promising candidate for medical image segmentation.},
  archive      = {J_PEERJCS},
  author       = {Gul e Sehar Shahid and Jameel Ahmad and Chaudary Atif Raza Warraich and Amel Ksibi and Shrooq Alsenan and Arfan Arshad and Rehan Raza and Zaffar Ahmed Shaikh},
  doi          = {10.7717/peerj-cs.2787},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2787},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {LIU-NET: Lightweight inception U-net for efficient brain tumor segmentation from multimodal 3D MRI images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DGCFNet: Dual global context fusion network for remote sensing image semantic segmentation. <em>PEERJCS</em>, <em>11</em>, e2786. (<a href='https://doi.org/10.7717/peerj-cs.2786'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semantic segmentation task of remote sensing images often faces various challenges such as complex backgrounds, high inter-class similarity, and significant differences in intra-class visual attributes. Therefore, segmentation models need to capture both rich local information and long-distance contextual information to overcome these challenges. Although convolutional neural networks (CNNs) have strong capabilities in extracting local information, they are limited in establishing long-range dependencies due to the inherent limitations of convolution. While Transformer can extract long-range contextual information through multi-head self attention mechanism, which has significant advantages in capturing global feature dependencies. To achieve high-precision semantic segmentation of remote sensing images, this article proposes a novel remote sensing image semantic segmentation network, named the Dual Global Context Fusion Network (DGCFNet), which is based on an encoder-decoder structure and integrates the advantages of CNN in capturing local information and Transformer in establishing remote contextual information. Specifically, to further enhance the ability of Transformer in modeling global context, a dual-branch global extraction module is proposed, in which the global compensation branch can not only supplement global information but also preserve local information. In addition, to increase the attention to salient regions, a cross-level information interaction module is adopted to enhance the correlation between features at different levels. Finally, to optimize the continuity and consistency of segmentation results, a feature interaction guided module is used to adaptively fuse information from intra layer and inter layer. Extensive experiments on the Vaihingen, Potsdam, and BLU datasets have shown that the proposed DGCFNet method can achieve better segmentation performance, with mIoU reaching 82.20%, 83.84% and 68.87%, respectively.},
  archive      = {J_PEERJCS},
  author       = {Yuan Liao and Tongchi Zhou and Lu Li and Jinming Li and Jiuhao Shen and Askar Hamdulla},
  doi          = {10.7717/peerj-cs.2786},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2786},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DGCFNet: Dual global context fusion network for remote sensing image semantic segmentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing colorectal polyp classification using gaze-based attention networks. <em>PEERJCS</em>, <em>11</em>, e2780. (<a href='https://doi.org/10.7717/peerj-cs.2780'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colorectal polyps are potential precursor lesions of colorectal cancer. Accurate classification of colorectal polyps during endoscopy is crucial for early diagnosis and effective treatment. Automatic and accurate classification of colorectal polyps based on convolutional neural networks (CNNs) during endoscopy is vital for assisting endoscopists in diagnosis and treatment. However, this task remains challenging due to difficulties in the data acquisition and annotation processes, the poor interpretability of the data output, and the lack of widespread acceptance of the CNN models by clinicians. This study proposes an innovative approach that utilizes gaze attention information from endoscopists as an auxiliary supervisory signal to train a CNN-based model for the classification of colorectal polyps. Gaze information from the reading of endoscopic images was first recorded through an eye-tracker. Then, the gaze information was processed and applied to supervise the CNN model’s attention via an attention consistency module. Comprehensive experiments were conducted on a dataset that contained three types of colorectal polyps. The results showed that EfficientNet_b1 with supervised gaze information achieved an overall test accuracy of 86.96%, a precision of 87.92%, a recall of 88.41%, an F1 score of 88.16%, the area under the receiver operating characteristic (ROC) curve (AUC) is 0.9022. All evaluation metrics surpassed those of EfficientNet_b1 without gaze information supervision. The class activation maps generated by the proposed network also indicate that the endoscopist’s gaze-attention information, as auxiliary prior knowledge, increases the accuracy of colorectal polyp classification, offering a new solution to the field of medical image analysis.},
  archive      = {J_PEERJCS},
  author       = {Zhenghao Guo and Yanyan Hu and Peixuan Ge and In Neng Chan and Tao Yan and Pak Kin Wong and Shaoyong Xu and Zheng Li and Shan Gao},
  doi          = {10.7717/peerj-cs.2780},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2780},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing colorectal polyp classification using gaze-based attention networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating artificial intelligence (AI) in healthcare: Advancing older adults’ health management in saudi arabia through AI-powered chatbots. <em>PEERJCS</em>, <em>11</em>, e2773. (<a href='https://doi.org/10.7717/peerj-cs.2773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background The healthcare sector is experiencing rapid digital advancements, with patients increasingly seeking quick and seamless interactions. Artificial intelligence (AI)-driven healthcare chatbots are becoming an integral part of elderly care, transforming provider-patient engagement and supporting health behavior goals tailored to individual preferences, needs, and limitations. Methods This study developed a comprehensive research framework incorporating various theoretical perspectives to explore the determinants of sustained use of AI-powered healthcare chatbots among older adults. The framework also examined the mediating influence of perceived humanness. The model was evaluated using partial least squares structural equation modeling (PLS-SEM) on cross-sectional data collected from 158 individuals aged 60 and above. Results The findings show that satisfaction with AI-powered chatbots is significantly influenced by facilitating conditions, perceived hedonic motivation, confirmation, performance expectancy, and effort expectancy. Perceived security also plays a critical role in shaping satisfaction and the intention to continue using these chatbots. Moreover, the analysis revealed that perceived humanness mediates the relationship between satisfaction and continuous use intentions among elderly users in Saudi Arabia. Discussion This research provides valuable insights into the factors influencing older adults’ acceptance of AI chatbots in Saudi Arabia, particularly in the post-COVID-19 era. These findings enrich academic discourse and offer actionable recommendations for healthcare organizations adapting to the evolving digital landscape.},
  archive      = {J_PEERJCS},
  author       = {Sabah Abdullah Al-Somali},
  doi          = {10.7717/peerj-cs.2773},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2773},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Integrating artificial intelligence (AI) in healthcare: Advancing older adults’ health management in saudi arabia through AI-powered chatbots},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multichannel speech enhancement for automatic speech recognition: A literature review. <em>PEERJCS</em>, <em>11</em>, e2772. (<a href='https://doi.org/10.7717/peerj-cs.2772'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multichannel speech enhancement (MCSE) is crucial for improving the robustness and accuracy of automatic speech recognition (ASR) systems. Due to the importance of ASR systems, extensive research has been conducted in MCSE, leading to rapid advancements in methods, models, and datasets. Most previous reviews point to the lack of a systematic literature review of MCSE for ASR systems. This systematic literature review aims to (1) perform a comprehensive review of the existing approaches in MCSE for ASR, (2) analyze the performance of the MCSE and ASR for various techniques, models, as well as noise data and environments, and (3) discuss the challenges, limitations, and future research directions in this research area. We conducted keyword searches on several electronic databases such as Google Scholar, IEEE Xplore, ScienceDirect, SpringerLink, ACM Digital Library, and ISI Web of Knowledge to identify relevant journal and conference articles. We selected 240 articles based on inclusion criteria from the initial search results and ended with 35 experimental articles when exclusion criteria were applied. Through backward snowballing and the quality assessment, the final tally was 40 articles, comprising 23 journals, and 17 conference articles. The review shows that there is an increasing trend in MCSE for ASR with word error rate (WER), perceptual evaluation of speech quality (PESQ), and short-time objective intelligence (STOI) as common forms of performance measures. One of the major issues that we found in the review is the generality and comparability of the MCSE works, making it difficult to come up with unified solutions to noises in speech recognition. This systematic literature review has extensively examined MCSE and ASR techniques. Key findings include identifying MCSE methods that help ASR performance across various models, techniques, noise, and environments. We also identify several key areas researchers can explore in the future due to their promising potential.},
  archive      = {J_PEERJCS},
  author       = {Zubair Zaland and Mumtaz Begum Mustafa and Miss Laiha Mat Kiah and Hua-Nong Ting and Mansoor Ali Mohamed Yusoof and Zuraidah Mohd Don and Saravanan Muthaiyah},
  doi          = {10.7717/peerj-cs.2772},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2772},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multichannel speech enhancement for automatic speech recognition: A literature review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coronary artery disease classification using ConvMixer based classifier from CT angiography images. <em>PEERJCS</em>, <em>11</em>, e2771. (<a href='https://doi.org/10.7717/peerj-cs.2771'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coronary artery disease (CAD) has recently emerged as a predominant source of morbidity and death worldwide. Assessing the existence and severity of CAD in people is crucial for determining the optimal treatment strategy. Currently, computed tomography (CT) delivers excellent spatial resolution pictures of the heart and coronary arteries at a rapid pace. Conversely, several problems exist in the analysis of cardiac CT images for indications of CAD. Research investigations employ machine learning (ML) and deep learning (DL) techniques to achieve high accuracy and consistent performance, hence addressing existing restrictions. This research proposes convMixer with median filter and morphological operations for the classification of the coronary artery disease from computed tomography angiography images. A total of 5,959 CT angiography images were used for classification. The model achieved an accuracy of 96.30%, sensitivity of 94.39%, and specificity of 99.16% for combination of the morphological operations and convMixer, 88.92% of accuracy and 89.56% of sensitivity, and 93.10% of specificity for the combination of median filter and convMixer and 94.63% of accuracy, 95.82% of sensitivity, and 93.10% of specificity for convMixer. The findings indicate the viability of automated non-invasive identification of individuals necessitating invasive coronary angiography images and maybe future coronary artery operations. This may potentially decrease the number of people who receive invasive coronary angiography images. Lastly, post-image analysis was conducted using DL heat maps to understand the decisions made by the proposed model. The proposed integrated DL intelligent system enhances the efficiency of illness diagnosis, reduces manual involvement in diagnostic processes, supports medical professionals in diagnostic decision-making, and offers supplementary techniques for future medical diagnostic systems based on coronary angioplasty.},
  archive      = {J_PEERJCS},
  author       = {C. Rajeev and Karthika Natarajan},
  doi          = {10.7717/peerj-cs.2771},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2771},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Coronary artery disease classification using ConvMixer based classifier from CT angiography images},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing knee osteoarthritis detection with AI, image denoising, and optimized classification methods and the importance of physical therapy methods. <em>PEERJCS</em>, <em>11</em>, e2766. (<a href='https://doi.org/10.7717/peerj-cs.2766'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Osteoarthritis (OA) is considered one of the most challenging arthritic disorders due to its high disease burden and lack of effective treatment options that can change the course of the disease. Knee osteoarthritis (KOA) reduces people’s quality of life and shortens their daily activities. Therefore, early detection of KOA dramatically impacts patients’ quality of life. This study developed an artificial intelligence-supported system to detect KOA. In the developed system, firstly, the images in the original dataset were denoised with a Gaussian filter. Then, feature maps were extracted from both the original and Gaussian applied datasets with the DenseNet201 selected from eight different pre-trained models, and these two feature maps were concatenated. In this way, it is aimed to bring together different features of the same image. Then, feature selection was made using the neighborhood component analysis (NCA) method for the developed system to produce more successful results, and the optimized feature map was classified into six different classifiers. As a result, a high accuracy rate of 85% was achieved in the proposed model. This value is promising for the automatic diagnosis of KOA with computer-aided systems. As a result, a high accuracy rate of 85% was achieved in the developed system of the support vector machine (SVM) classifier. The proposed model was more successful than the other models used in the study.},
  archive      = {J_PEERJCS},
  author       = {Burak Bugday and Harun Bingol and Muhammed Yildirim and Bilal Alatas},
  doi          = {10.7717/peerj-cs.2766},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2766},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing knee osteoarthritis detection with AI, image denoising, and optimized classification methods and the importance of physical therapy methods},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IoT-based control and monitoring system for hydroponic plant growth using image processing and mobile applications. <em>PEERJCS</em>, <em>11</em>, e2763. (<a href='https://doi.org/10.7717/peerj-cs.2763'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The HydroFarm project presents an innovative IoT-based control and monitoring system for hydroponic plant growth, integrating advanced image processing techniques and mobile applications to enhance urban farming practices. This system addresses critical challenges faced by urban farmers, such as limited space and the need for precise environmental management. By employing a comprehensive approach that combines various sensors (DHT22, DS18B20, pH, TDS) with an ESP32 microcontroller, HydroFarm enables real-time monitoring of essential parameters like temperature, humidity, pH, and nutrient levels. A significant novelty of this project lies in its use of a convolutional neural network (CNN) for plant health assessment through image processing. This technique allows for accurate detection of plant conditions, categorizing leaves as healthy or unhealthy based on visual data captured via a mobile application. The application, developed in Kotlin, not only facilitates user interaction but also provides automated and manual control over nutrient delivery systems based on real-time sensor data. Testing results indicate that the HydroFarm system achieves a high accuracy rate of 96% in detecting plant health conditions, with the sensors providing accurate and consistent data to maintain effective control over hydroponic parameters. The system usability scale (SUS) evaluation yielded an impressive score of 81.875, categorizing the application as excellent and user-friendly. Overall, HydroFarm represents a significant advancement in hydroponic farming technology by integrating IoT capabilities with deep learning for enhanced decision-making and operational efficiency in urban agriculture. The findings underscore the potential for scaling this model to improve food security and promote sustainable agricultural practices in densely populated areas.},
  archive      = {J_PEERJCS},
  author       = {Wizman Rofiansyah and Fayza Rizka Zalianty and Firman Ahmad La Ito and Inung Wijayanto and Harfan Hian Ryanu and Indrarini Dyah Irawati},
  doi          = {10.7717/peerj-cs.2763},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2763},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IoT-based control and monitoring system for hydroponic plant growth using image processing and mobile applications},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IPro-CSAF: Identification of promoters based on convolutional spiking neural networks and spiking attention mechanism. <em>PEERJCS</em>, <em>11</em>, e2761. (<a href='https://doi.org/10.7717/peerj-cs.2761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A promoter is a DNA segment which plays a key role in regulating gene expression. Accurate identification of promoters is significant for understanding the regulatory mechanisms involved in gene expression and genetic disease treatment. Therefore, it is an urgent challenge to develop computational methods for identifying promoters. Most current methods were designed for promoter recognition on few species and required complex feature extraction methods in order to attain high recognition accuracy. Spiking neural networks have inherent recurrence and use spike-based sparse coding. Therefore, they have good property of processing spatio-temporal information and are well suited for learning sequence information. In this study, iPro-CSAF, a convolutional spiking neural network combined with spiking attention mechanism is designed for promoter recognition. The method extracts promoter features by two parallel branches including spiking attention mechanism and a convolutional spiking layer. The promoter recognition of iPro-CSAF is evaluated by exhaustive promoter recognition experiments including both prokaryotic and eukaryotic promoter recognition from seven species. Our results show that iPro-CSAF outperforms promoter recognition methods which used parallel CNN layers, methods which combined CNNs with capsule networks, attention mechanism, LSTM or BiLSTM, and CNNs-based methods which needed priori biological or text feature extraction, while our method has much fewer network parameters. It indicates that iPro-CSAF is an effective computational method with low complexity and good generalization for promoter recognition.},
  archive      = {J_PEERJCS},
  author       = {Qian Zhou and Jie Meng and Hao Luo},
  doi          = {10.7717/peerj-cs.2761},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2761},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IPro-CSAF: Identification of promoters based on convolutional spiking neural networks and spiking attention mechanism},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Activation function cyclically switchable convolutional neural network model. <em>PEERJCS</em>, <em>11</em>, e2756. (<a href='https://doi.org/10.7717/peerj-cs.2756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks are a state-of-the-art approach that performs well for many tasks. The activation function (AF) is an important hyperparameter that creates an output against the coming inputs to the neural network model. AF significantly affects the training and performance of the neural network model. Therefore, selecting the most optimal AF for processing input data in neural networks is important. Determining the optimal AF is often a difficult task. To overcome this difficulty, studies on trainable AFs have been carried out in the literature in recent years. This study presents a different approach apart from fixed or trainable AF approaches. For this purpose, the activation function cyclically switchable convolutional neural network (AFCS-CNN) model structure is proposed. The AFCS-CNN model structure does not use a fixed AF value during training. It is designed in a self-regulating model structure by switching the AF during model training. The proposed model structure is based on the logic of starting training with the most optimal AF selection among many AFs and cyclically selecting the next most optimal AF depending on the performance decrease during neural network training. Any convolutional neural network (CNN) model can be easily used in the proposed model structure. In this way, a simple but effective perspective has been presented. In this study, first, ablation studies have been carried out using the Cifar-10 dataset to determine the CNN models to be used in the AFCS-CNN model structure and the specific hyperparameters of the proposed model structure. After the models and hyperparameters were determined, expansion experiments were carried out using different datasets with the proposed model structure. The results showed that the AFCS-CNN model structure achieved state-of-the-art success in many CNN models and different datasets.},
  archive      = {J_PEERJCS},
  author       = {İsmail Akgül},
  doi          = {10.7717/peerj-cs.2756},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2756},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Activation function cyclically switchable convolutional neural network model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-driven flight path monitoring technique using recurrent neural network for the safety management of commercial aircraft. <em>PEERJCS</em>, <em>11</em>, e2753. (<a href='https://doi.org/10.7717/peerj-cs.2753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aviation spins, particularly at low altitudes, significantly contribute to fatalities due to limited recovery time. Standard recovery procedures typically only become eligible after a spin is fully developed, by which time multiple turns may have already resulted in substantial altitude loss. The primary challenge in upset prevention is heavy reliance on the pilot’s situational awareness, which is only effective before the spin has been fully developed. To address this issue, this study proposes an early detection capability to significantly enhance immediate response actions, potentially mitigating altitude loss and enabling pilots to recognize the initial signs of upset conditions. This research introduces a real-time predictive tool based on a novel recurrent neural network (RNN) model that utilizes data from the NASA Generic Transport Model (GTM)-a research platform designed for experimental flight case studies-to predict nonlinear flight responses during the critical initial seconds of a spin. Rigorous validation against ground truth data demonstrates the RNN model’s superior predictive capabilities in detecting incipient spin phase, offering an essential tool for proactive spin management and reducing the risk of ground collisions. This early detection capability empowers pilots to identify the initial signs of upset conditions and make informed operational decisions, ultimately improving aviation safety. This advancement underscores the potential of advanced machine learning technologies to transform safety protocols by enabling earlier and more effective intervention strategies, thereby preempting catastrophic events.},
  archive      = {J_PEERJCS},
  author       = {Naeun Kim and Mohamed H. Hamza and Bong-Hwan Koh},
  doi          = {10.7717/peerj-cs.2753},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2753},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data-driven flight path monitoring technique using recurrent neural network for the safety management of commercial aircraft},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning techniques for imbalanced multiclass malware classification through adaptive feature selection. <em>PEERJCS</em>, <em>11</em>, e2752. (<a href='https://doi.org/10.7717/peerj-cs.2752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting polymorphic or metamorphic variants of known malware is an ever-growing challenge, just like detecting new malware. Artificial intelligence techniques are preferred over conventional signature-based malware detection as the number of malware variants proliferates. This article proposes an Adaptive Multiclass Malware Classification (AMMC) framework that trains base machine learning models with fewer computational resources to detect malware. Furthermore, this work proposes a novel adaptive feature selection (AFS) technique using the greedy strategy on term frequency and inverse document frequency (TF-IDF) feature weights to address the selection of influential features and ensure better performance metrics in imbalanced multiclass malware classification problems. To assess AMMC’s efficacy using AFS, three open imbalanced multiclass malware datasets (VirusShare with eight classes, VirusSample with six classes, and MAL-API-2019 with eight classes) on Windows API sequence features were used. Experimental results demonstrate the effectiveness of AMMC with AFS, achieving state-of-the-art performance on VirusShare, VirusSample, and MAL-API-2019 with a macro F1-score of 0.92, 0.94, and 0.84 and macro area under the curve (AUC) of 0.99, 0.99, and 0.98, respectively. The performance measurements obtained with AMMC for all datasets were highly promising.},
  archive      = {J_PEERJCS},
  author       = {Binayak Panda and Sudhanshu Shekhar Bisoyi and Sidhanta Panigrahy and Prithviraj Mohanty},
  doi          = {10.7717/peerj-cs.2752},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2752},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Machine learning techniques for imbalanced multiclass malware classification through adaptive feature selection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Federated learning with LSTM for intrusion detection in IoT-based wireless sensor networks: A multi-dataset analysis. <em>PEERJCS</em>, <em>11</em>, e2751. (<a href='https://doi.org/10.7717/peerj-cs.2751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intrusion detection in Internet of Things (IoT)-based wireless sensor networks (WSNs) is essential due to their widespread use and inherent vulnerability to security breaches. Traditional centralized intrusion detection systems (IDS) face significant challenges in data privacy, computational efficiency, and scalability, particularly in resource-constrained IoT environments. This study aims to create and assess a federated learning (FL) framework that integrates with long short-term memory (LSTM) networks for efficient intrusion detection in IoT-based WSNs. We design the framework to enhance detection accuracy, minimize false positive rates (FPR), and ensure data privacy, while maintaining system scalability. Using an FL approach, multiple IoT nodes collaboratively train a global LSTM model without exchanging raw data, thereby addressing privacy concerns and improving detection capabilities. The proposed model was tested on three widely used datasets: WSN-DS, CIC-IDS-2017, and UNSW-NB15. The evaluation metrics for its performance included accuracy, F1 score, FPR, and root mean square error (RMSE). We evaluated the performance of the FL-based LSTM model against traditional centralized models, finding significant improvements in intrusion detection. The FL-based LSTM model achieved higher accuracy and a lower FPR across all datasets than centralized models. It effectively managed sequential data in WSNs, ensuring data privacy while maintaining competitive performance, particularly in complex attack scenarios. FL and LSTM networks work well together to make a strong way to find intrusions in IoT-based WSNs, which improves both privacy and detection. This study underscores the potential of FL-based systems to address key challenges in IoT security, including data privacy, scalability, and performance, making the proposed framework suitable for real-world IoT applications.},
  archive      = {J_PEERJCS},
  author       = {Raja Waseem Anwar and Mohammad Abrar and Abdu Salam and Faizan Ullah},
  doi          = {10.7717/peerj-cs.2751},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2751},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Federated learning with LSTM for intrusion detection in IoT-based wireless sensor networks: A multi-dataset analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A sequential recommendation method using contrastive learning and wasserstein self-attention mechanism. <em>PEERJCS</em>, <em>11</em>, e2749. (<a href='https://doi.org/10.7717/peerj-cs.2749'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research has demonstrated the effectiveness of utilizing contrastive learning for training Transformer-based sequence encoders in sequential recommendation tasks. Items are represented using vectors and the relations between items are measured by the dot product self-attention, the feature representation in sequential recommendation can be enhanced. However, in real-world scenarios, user behavior sequences are unpredictable, and the limitations of dot product-based approaches hinder the complete capture of collaborative transferability. Moreover, the Bayesian personalized ranking (BPR) loss function, commonly utilized in recommendation systems, lacks constraints when considering positive and negative sampled items, potentially leading to suboptimal optimization outcomes. This presents a complex challenge that needs to be addressed. To tackle these issues, this article proposes a novel method involving stochastic self-attention. This article introduces uncertainty into the proposed model by utilizing elliptical Gaussian distribution controlled by mean and covariance vector to explain the unpredictability of items. At the same time, the proposed model combines a Wasserstein self-attention module to compute the positional relationships between items within a sequence in order to effectively incorporate uncertainty into the training process. The Wasserstein self-attention mechanism satisfies the triangular inequality and can not only addresses uncertainty but also promote collaborative transfer learning. Furthermore, embedding a stochastic Gaussian distribution into each item will bring additional uncertainty into the proposed model. Multi-pair contrastive learning relies on high-quality positive samples, and the proposed model combines the cloze task mask and dropout mask mechanisms to generate high-quality positive samples. It demonstrates superior performance and adaptability compared to traditional single-pair contrastive learning methods. Additionally, a dynamic loss reweighting strategy is introduced to balance the cloze task loss and the contrastive loss effectively. We conduct experiments and the results show that the proposed model outperforms the state-of-the-art models, especially on cold start items. For each metric, the hit ratio (HR) and normalized discounted cumulative gain (NDCG) on the Beauty dataset improved by an average of 1.3% and 10.27%, respectively; on the Toys dataset improved by an average of 8.24% and 5.89%, respectively; on the ML-1M dataset improved by an average of 68.62% and 8.22%, respectively; and on the ML-100M dataset improved by an average of 93.57% and 44.87% Our code is available at DOI: 10.5281/zenodo.13634624.},
  archive      = {J_PEERJCS},
  author       = {Shengbin Liang and Jinfeng Ma and Qiuchen Zhao and Tingting Chen and Xixi Lu and Shuanglong Ren and Chenyang Zhao and Lei Fu and Huichao Ding},
  doi          = {10.7717/peerj-cs.2749},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2749},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A sequential recommendation method using contrastive learning and wasserstein self-attention mechanism},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protein-protein interaction prediction using enhanced features with spaced conjoint triad and amino acid pairwise distance. <em>PEERJCS</em>, <em>11</em>, e2748. (<a href='https://doi.org/10.7717/peerj-cs.2748'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein-protein interactions (PPIs) are pivotal in cellular processes, influencing a wide range of functions, from metabolism to immune responses. Despite the advancements in experimental techniques for PPI detection, their inherent limitations, such as high false-positive rates and significant resource demands, necessitate the development of computational approaches. This study presents a novel computational model named MFPIC (Multi-Feature Protein Interaction Classifier) for predicting PPIs, integrating enhanced sequence-based features, including a novel spaced conjoint triad (SCT) and amino acid pairwise distance (AAPD), with existing methods such as position-specific scoring matrices (PSSM) and AAindex-based features. The SCT captures complex sequence motifs by considering non-adjacent amino acid interactions, while AAPD provides critical spatial information about amino acid residues within protein sequences. The proposed model was evaluated across three benchmark datasets—Saccharomyces cerevisiae, Helicobacter pylori, and human proteins—demonstrating superior performance in comparison to state-of-the-art models. The results underscore the efficacy of integrating diverse and complementary features, achieving significant improvements in predictive accuracy, with the model achieving 95.90%, 99.33%, and 90.95% accuracy on the Saccharomyces cerevisiae, Helicobacter pylori, and human dataset, respectively. This approach not only enhances our understanding of PPI mechanisms but also offers valuable insights for the development of targeted therapeutic strategies.},
  archive      = {J_PEERJCS},
  author       = {Yunus Emre Göktepe},
  doi          = {10.7717/peerj-cs.2748},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2748},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Protein-protein interaction prediction using enhanced features with spaced conjoint triad and amino acid pairwise distance},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient unified architecture for post-quantum cryptography: Combining dilithium and kyber. <em>PEERJCS</em>, <em>11</em>, e2746. (<a href='https://doi.org/10.7717/peerj-cs.2746'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the ongoing standardization process of post-quantum schemes yields initial outcomes, it becomes increasingly important to not only optimize standalone implementations but also explore the potential of combining multiple schemes into a single, unified architecture. In this article, we investigate the combination of two National Institute of Standards and Technology (NIST)-selected schemes: the Dilithium digital signature scheme and the Kyber key encapsulation mechanism. We propose a novel set of optimization techniques for a unified hardware implementation of these leading post-quantum schemes, achieving a balanced approach between area efficiency and high performance. Our design demonstrates superior resource efficiency and performance compared to previously reported unified architecture (DOI 10.1109/TCSI.2022.3219555), also achieving results that are better than, or comparable, to those of standalone implementations. The efficient and combined implementation of lattice-based digital signatures and key establishment methods can be deployed for establishing secure sessions in high-speed communication networks at servers and gateways. Moreover, the unique and compact design that requires small hardware resources can be directly used in small and cost-effective field programmable gate array (FPGA) platforms that can be used as security co-processors for embedded devices and in the Internet of Things.},
  archive      = {J_PEERJCS},
  author       = {Patrik Dobias and Lukas Malina and Jan Hajny},
  doi          = {10.7717/peerj-cs.2746},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2746},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Efficient unified architecture for post-quantum cryptography: Combining dilithium and kyber},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal intrusion detection for imbalanced data using bagging method with deep neural network optimized by flower pollination algorithm. <em>PEERJCS</em>, <em>11</em>, e2745. (<a href='https://doi.org/10.7717/peerj-cs.2745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the number of connected devices and Internet of Things (IoT) devices grows, it is becoming more and more important to develop efficient security mechanisms to manage risks and vulnerabilities in IoT networks. Intrusion detection systems (IDSs) have been developed and implemented in IoT networks to discern between regular network traffic and potential malicious attacks. This article proposes a new IDS based on a hybrid method of metaheuristic and deep learning techniques, namely, the flower pollination algorithm (FPA) and deep neural network (DNN), with an ensemble learning paradigm. To handle the problem of imbalance class distribution in intrusion datasets, a roughly-balanced (RB) Bagging strategy is utilized, where DNN models trained by FPA on a cost-sensitive fitness function are used as base learners. The RB Bagging strategy derives multiple RB training subsets from the original dataset and proper class weights are incorporated into the fitness function to attain unbiased DNN models. The performance of our IDS is evaluated using four commonly utilized public datasets, NSL-KDD, UNSW NB-15, CIC-IDS-2017, and BoT-IoT, in terms of different metrics, i.e., accuracy, precision, recall, and F1-score. The results demonstrate that our IDS outperforms existing ones in accurately detecting network intrusions with effective handling of class imbalance problem.},
  archive      = {J_PEERJCS},
  author       = {Hussein Ridha Sayegh and Wang Dong and Bahaa Hussein Taher and Muhanad Mohammed Kadum and Ali Mansour Al-madani},
  doi          = {10.7717/peerj-cs.2745},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2745},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimal intrusion detection for imbalanced data using bagging method with deep neural network optimized by flower pollination algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The line follower robot: A meta-analytic approach. <em>PEERJCS</em>, <em>11</em>, e2744. (<a href='https://doi.org/10.7717/peerj-cs.2744'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Line-follower robots represent a critical segment in autonomous robotics, with broad applications ranging from industrial automation to educational tools. This meta-analytic review synthesizes research on line-follower robots, addressing a noticeable gap in the literature where comprehensive analyses are scarce. The review leverages the Theory of the Consolidated Meta-analytic Approach (TEMAC) to systematically explore 287 documents spanning from 2001 to 2024, highlighting key contributions, trends, and gaps in the field. Through this analysis, it becomes evident that while significant advancements have been made in control strategies, sensor integration, and noise reduction techniques, the literature still lacks comprehensive studies on the scalability of these technologies, especially in large-scale industrial environments. Recent research trends emphasize integrating artificial intelligence and machine learning into line-follower robots, indicating a shift towards more sophisticated, adaptable systems. Despite these advancements, challenges remain in addressing environmental variability, improving real-time adaptability, and exploring novel applications in dynamic environments. This review not only maps the historical evolution and current state of line-follower robots but also identifies future research directions that could drive the next generation of robotic systems. The findings offer valuable insights for researchers, engineers, and educators aiming to enhance the efficiency, reliability, and application scope of line-follower robots.},
  archive      = {J_PEERJCS},
  author       = {Williamson Johnny Hatzinakis Brigido and Jose M. Parente de Oliveira},
  doi          = {10.7717/peerj-cs.2744},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2744},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The line follower robot: A meta-analytic approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved smart city security using a deep maxout network-based intrusion detection system with walrus optimization. <em>PEERJCS</em>, <em>11</em>, e2743. (<a href='https://doi.org/10.7717/peerj-cs.2743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Smart cities, enabled by the Internet of Things (IoT), leverage technology to optimize urban living and enhance infrastructure. As urban environments become interconnected hubs of digital innovation, securing critical components like public transportation infrastructure becomes increasingly important. Methods This research addresses the need for robust intrusion detection systems (IDS) tailored to the unique challenges of securing public transportation within smart cities. Focused on the Tabuk region in Saudi Arabia, the study introduces an IDS model integrating the deep maxout network with walrus optimization (DMN-WO). The DMN is configured with an architecture that includes multiple layers with maxout activation functions. These layers are capable of capturing complex patterns in the data, making the DMN particularly effective for identifying anomalies in IoT network traffic. The DMN-WO model is ensured to be resource-efficient and suitable for real-time deployment on constrained devices like Raspberry Pi, typical in IoT systems. Results Training and validation are conducted using the CIC-IDS-2018 dataset, CIC-IDS -2029 dataset and real-time data from Raspberry Pi devices deployed in the smart city’s public transportation network. Real-time data application maintains robust performance, with 98.06% accuracy, 98.50% detection rate, 98.81% precision, 98.24% specificity, and a 98.57% F1-score. Conclusions This research advances cybersecurity measures in smart city applications by providing a resilient solution for detecting and mitigating security threats in public transportation infrastructure. It lays the groundwork for further refinements and real-world deployments in the dynamic landscape of smart cities.},
  archive      = {J_PEERJCS},
  author       = {Wahid Rajeh and Majed Aborokbah and Manimurugan S. and Umar Albalawi and Ahamed Aljuhani and Osama Shibl Abdalghany Younes and Karthikeyan Periyasami},
  doi          = {10.7717/peerj-cs.2743},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2743},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improved smart city security using a deep maxout network-based intrusion detection system with walrus optimization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on sports activity behavior prediction based on electromyography signal collection and intelligent sensing channel. <em>PEERJCS</em>, <em>11</em>, e2742. (<a href='https://doi.org/10.7717/peerj-cs.2742'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports behavior prediction requires precise and reliable analysis of muscle activity during exercise. This study proposes a multi-channel correlation feature extraction method for electromyographic (EMG) signals to overcome challenges in sports behavior prediction. A wavelet threshold denoising algorithm is enhanced with nonlinear function transitions and control coefficients to improve signal quality, achieving effective noise reduction and a higher signal-to-noise ratio. Furthermore, multi-channel linear and nonlinear correlation features are combined, leveraging mutual information estimation via copula entropy for feature construction. A stacking ensemble learning model, incorporating extreme gradient boosting (XGBoost), K-nearest network (KNN), Random Forest (RF), and naive Bayes (NB) as base learners, further enhances classification accuracy. Experimental results demonstrate that the proposed approach achieves over 95% prediction accuracy, significantly outperforming traditional methods. The robustness of multi-channel correlation features is validated across diverse datasets, proving their effectiveness in mitigating channel crosstalk and noise interference. This work provides a scientific basis for improving sports training strategies and reducing injury risks.},
  archive      = {J_PEERJCS},
  author       = {Fengjin Ye and Yuchao Zhao and Zohaib Latif},
  doi          = {10.7717/peerj-cs.2742},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2742},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on sports activity behavior prediction based on electromyography signal collection and intelligent sensing channel},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-edge MQTT messaging for latency mitigation and broker memory footprint reduction. <em>PEERJCS</em>, <em>11</em>, e2741. (<a href='https://doi.org/10.7717/peerj-cs.2741'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment of smart-city applications has increased the number of Internet of Things (IoT) devices connected to a network cloud. Thanks to its flexibility in matching data publishers and subscribers, broker-based data communication could be a solution for such IoT data delivery, and MQTT is one of the widely used messaging protocols in this class. While MQTT by default does not differentiate message flows by size, it is observed that transient local network congestion may cause size-dependent latency additions, and that the accumulation of large message copies in the cloud broker could run out of the broker memory. In response, in the scope of cloud-edge messaging, this research article presents problem analysis, system design and implementation, and empirical and analytical performance evaluation. The article introduces three message scheduling policies for subscribers deployed at network edge, and a memory allocation scheme for MQTT broker deployed at network cloud. The proposed design has been implemented based on Eclipse Mosquitto, an open-source MQTT broker implementation. Empirical and analytical validations have demonstrated the performance of the proposed design in latency mitigation, and the result also shows that, empirically, the proposed design may save the run-time broker memory footprint by about 75%. Applicability of the proposed design to other messaging services are discussed by the end of the article.},
  archive      = {J_PEERJCS},
  author       = {Yi-Hsuan Tseng and Chao Wang and Yu-Tse Wei and Yu-Ting Chiang},
  doi          = {10.7717/peerj-cs.2741},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2741},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Cloud-edge MQTT messaging for latency mitigation and broker memory footprint reduction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DE-RALBA: Dynamic enhanced resource aware load balancing algorithm for cloud computing. <em>PEERJCS</em>, <em>11</em>, e2739. (<a href='https://doi.org/10.7717/peerj-cs.2739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing provides an opportunity to gain access to the large-scale and high-speed resources without establishing your own computing infrastructure for executing the high-performance computing (HPC) applications. Cloud has the computing resources (i.e., computation power, storage, operating system, network, and database etc.) as a public utility and provides services to the end users on a pay-as-you-go model. From past several years, the efficient utilization of resources on a compute cloud has become a prime interest for the scientific community. One of the key reasons behind inefficient resource utilization is the imbalance distribution of workload while executing the HPC applications in a heterogenous computing environment. The static scheduling technique usually produces lower resource utilization and higher makespan, while the dynamic scheduling achieves better resource utilization and load-balancing by incorporating a dynamic resource pool. The dynamic techniques lead to increased overhead by requiring a continuous system monitoring, job requirement assessments and real-time allocation decisions. This additional load has the potential to impact the performance and responsiveness on computing system. In this article, a dynamic enhanced resource-aware load balancing algorithm (DE-RALBA) is proposed to mitigate the load-imbalance in job scheduling by considering the computing capabilities of all VMs in cloud computing. The empirical assessments are performed on CloudSim simulator using instances of two scientific benchmark datasets (i.e., heterogeneous computing scheduling problems (HCSP) instances and Google Cloud Jobs (GoCJ) dataset). The obtained results revealed that the DE-RALBA mitigates the load imbalance and provides a significant improvement in terms of makespan and resource utilization against existing algorithms, namely PSSLB, PSSELB, Dynamic MaxMin, and DRALBA. Using HCSP instances, the DE-RALBA algorithm achieves up to 52.35% improved resources utilization as compared to existing technique, while more superior resource utilization is achieved using the GoCJ dataset.},
  archive      = {J_PEERJCS},
  author       = {Altaf Hussain and Muhammad Aleem and Atiq Ur Rehman and Umer Arshad},
  doi          = {10.7717/peerj-cs.2739},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2739},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DE-RALBA: Dynamic enhanced resource aware load balancing algorithm for cloud computing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A zero-trust based scheme for detecting illegal terminals in the internet of things of smart grid. <em>PEERJCS</em>, <em>11</em>, e2736. (<a href='https://doi.org/10.7717/peerj-cs.2736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the Internet of Things (IoT) for electricity has faced a series of new challenges. Attackers use a compromised terminal as a springboard to enter the network, steal data, issue malicious commands, and cause great harm. In order to combat the threat of compromised terminals, this article proposes a zero-trust based detection scheme for illegal terminals, based on the principle of “never trust, always verify” security mechanism. Firstly, the detection scheme uses the state secret SM9 secret system to authenticate the access device. Then, it proposes a continuous trust evaluation based on the centroid drift trust algorithm on the characteristics of the traffic of the input device. Finally, it generates a real-time access policy by the access control engine to achieve a dynamic access policy. Finally, the access control engine generates real-time access policies to achieve dynamic access control. Experimental results show that the designed system has a high security detection accuracy and can effectively deal with the threat of compromised terminals.},
  archive      = {J_PEERJCS},
  author       = {Hongyu Zhu and Jianwei Tian and Qian Chen and Zheng Tian and Weiqiang Luo and Mingguang Li},
  doi          = {10.7717/peerj-cs.2736},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2736},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A zero-trust based scheme for detecting illegal terminals in the internet of things of smart grid},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph-based deep fusion for architectural text representation. <em>PEERJCS</em>, <em>11</em>, e2735. (<a href='https://doi.org/10.7717/peerj-cs.2735'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Amidst the swift global urbanization and rapid evolution of the architecture industry, there is a growing demand for the automated processing of architectural textual information. This demand arises from the abundance of specialized vocabulary in architectural texts, posing a challenge for accurate representation using traditional models. To address this, we propose a novel fusion method that integrates Transformer-based models with graph neural networks (GNNs) for architectural text representation. While independently utilizing Bidirectional Encoder Representations from Transformers (BERT) and the robustly optimized BERT approach (RoBERTa) to generate initial document representations, we also employ term frequency-inverse document frequency (TF-IDF) to extract keywords from each document and construct a corresponding keyword set. Subsequently, a graph is created based on the keyword vocabulary and document embeddings, which is then fed into the graph attention network (GAT). The final document embedding is generated by GAT, and the text embedding is crafted by the attention module and neural network structure of the GAT. Experimental results from comparison studies show that the proposed model outperforms all baselines. Additionally, ablation studies demonstrate the effectiveness of each module, further reinforcing the robustness and superiority of our approach.},
  archive      = {J_PEERJCS},
  author       = {Shaoyun Hu and Qingxiong Weng},
  doi          = {10.7717/peerj-cs.2735},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2735},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Graph-based deep fusion for architectural text representation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Protein language model-based prediction for plant miRNA encoded peptides. <em>PEERJCS</em>, <em>11</em>, e2733. (<a href='https://doi.org/10.7717/peerj-cs.2733'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plant miRNA encoded peptides (miPEPs), which are short peptides derived from small open reading frames within primary miRNAs, play a crucial role in regulating diverse plant traits. Plant miPEPs identification is challenging due to limitations in the available number of known miPEPs for training. Existing prediction methods rely on manually encoded features, including miPEPPred-FRL, to infer plant miPEPs. Recent advances in deep learning modeling of protein sequences provide an opportunity to improve the representation of key features, leveraging large datasets of protein sequences. In this study, we propose an accurate prediction model, called pLM4PEP, which integrates ESM2 peptide embedding with machine learning methods. Our model not only demonstrates precise identification capabilities for plant miPEPs, but also achieves remarkable results across diverse datasets that include other bioactive peptides. The source codes, datasets of pLM4PEP are available at https://github.com/xialab-ahu/pLM4PEP.},
  archive      = {J_PEERJCS},
  author       = {Yishan Yue and Henghui Fan and Jianping Zhao and Junfeng Xia},
  doi          = {10.7717/peerj-cs.2733},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2733},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Protein language model-based prediction for plant miRNA encoded peptides},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Customizable pattern synthesis: A deep generative approach for lantern designs. <em>PEERJCS</em>, <em>11</em>, e2732. (<a href='https://doi.org/10.7717/peerj-cs.2732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern design is essential in various domains, especially in traditional lantern production, where patterns convey cultural history and artistic values. Our research presents an innovative generative model that produces customizable lantern patterns, integrating classical aesthetics with modern design features via a generative adversarial network (GAN)-based framework. The model was trained on an extensive dataset of over 17,000 pattern images over ten various categories. Experimental assessment demonstrates the model’s remarkable proficiency, achieving an Inception Score of 5.259, much surpassing the performance of other GAN-based approaches. This exceptional result demonstrates the effective integration of traditional pattern elements with AI-driven design processes. The model offers enhanced design flexibility via noise vector hybridization and post-processing techniques, allowing for accurate control over pattern production while preserving cultural authenticity. These capabilities make our model a valuable tool for modernizing lantern pattern design while maintaining classic artistic elements.},
  archive      = {J_PEERJCS},
  author       = {Mengran Yan and Chun Tang and Jida Yan and Siti Suhaily Surip},
  doi          = {10.7717/peerj-cs.2732},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2732},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Customizable pattern synthesis: A deep generative approach for lantern designs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarial feature learning for semantic communication in human 3D reconstruction. <em>PEERJCS</em>, <em>11</em>, e2731. (<a href='https://doi.org/10.7717/peerj-cs.2731'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the widespread application of human body 3D reconstruction technology across various fields, the demands for data transmission and processing efficiency continue to rise, particularly in scenarios where network bandwidth is limited and low latency is required. This article introduces an Adversarial Feature Learning-based Semantic Communication method (AFLSC) for human body 3D reconstruction, which focuses on extracting and transmitting semantic information crucial for the 3D reconstruction task, thereby significantly optimizing data flow and alleviating bandwidth pressure. At the sender’s end, we propose a multitask learning-based feature extraction method to capture the spatial layout, keypoints, posture, and depth information from 2D human images, and design a semantic encoding technique based on adversarial feature learning to encode these feature information into semantic data. We also develop a dynamic compression technique to efficiently transmit this semantic data, greatly enhancing transmission efficiency and reducing latency. At the receiver’s end, we design an efficient multi-level semantic feature decoding method to convert semantic data back into key image features. Finally, an improved ViT-diffusion model is employed for 3D reconstruction, producing human body 3D mesh models. Experimental results validate the advantages of our method in terms of data transmission efficiency and reconstruction quality, demonstrating its excellent potential for application in bandwidth-limited environments.},
  archive      = {J_PEERJCS},
  author       = {Shaojiang Liu and Jiajun Zou and Zhendan Liu and Meixia Dong and Zhiping Wan},
  doi          = {10.7717/peerj-cs.2731},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2731},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Adversarial feature learning for semantic communication in human 3D reconstruction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data leakage detection in machine learning code: Transfer learning, active learning, or low-shot prompting?. <em>PEERJCS</em>, <em>11</em>, e2730. (<a href='https://doi.org/10.7717/peerj-cs.2730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing reliance on machine learning (ML) across diverse disciplines, ML code has been subject to a number of issues that impact its quality, such as lack of documentation, algorithmic biases, overfitting, lack of reproducibility, inadequate data preprocessing, and potential for data leakage, all of which can significantly affect the performance and reliability of ML models. Data leakage can affect the quality of ML models where sensitive information from the test set inadvertently influences the training process, leading to inflated performance metrics that do not generalize well to new, unseen data. Data leakage can occur at either the dataset-level (i.e., during dataset construction) or at the code-level. Existing studies introduced methods to detect code-level data leakage using manual and code analysis approaches. However, automated tools with advanced ML techniques are increasingly recognized as essential for efficiently identifying quality issues in large and complex codebases, enhancing the overall effectiveness of code review processes. In this article, we aim to explore ML-based approaches for limited annotated datasets to detect code-level data leakage in ML code. We proposed three approaches, namely, transfer learning, active learning, and low-shot prompting. Additionally, we introduced an automated approached to handle the imbalance issues of code data. Our results show that active learning outperformed the other approaches with an F-2 score of 0.72 and reduced the number of needed annotated samples from 1,523 to 698. We conclude that existing ML-based approaches can effectively mitigate the challenges associated with limited data availability.},
  archive      = {J_PEERJCS},
  author       = {Nouf Alturayeif and Jameleddine Hassine},
  doi          = {10.7717/peerj-cs.2730},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2730},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data leakage detection in machine learning code: Transfer learning, active learning, or low-shot prompting?},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TurkSentGraphExp: An inherent graph aware explainability framework from pre-trained LLM for turkish sentiment analysis. <em>PEERJCS</em>, <em>11</em>, e2729. (<a href='https://doi.org/10.7717/peerj-cs.2729'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentiment classification is a widely studied problem in natural language processing (NLP) that focuses on identifying the sentiment expressed in text and categorizing it into predefined classes, such as positive, negative, or neutral. As sentiment classification solutions are increasingly integrated into real-world applications, such as analyzing customer feedback in business reviews (e.g., hotel reviews) or monitoring public sentiment on social media, the importance of both their accuracy and explainability has become widely acknowledged. In the Turkish language, this problem becomes more challenging due to the complex agglutinative structure of the language. Many solutions have been proposed in the literature to solve this problem. However, it is observed that the solutions are generally based on black-box models. Therefore the explainability requirement of such artificial intelligence (AI) models has become as important as the accuracy of the model. This has further increased the importance of studies based on the explainability of the AI model’s decision. Although most existing studies prefer to explain the model decision in terms of the importance of a single feature/token, this does not provide full explainability due to the complex lexical and semantic relations in the texts. To fill these gaps in the Turkish NLP literature, in this article, we propose a graph-aware explainability solution for Turkish sentiment analysis named TurkSentGraphExp. The solution provides both classification and explainability for sentiment classification of Turkish texts by considering the semantic structure of suffixes, accommodating the agglutinative nature of Turkish, and capturing complex relationships through graph representations. Unlike traditional black-box learning models, this framework leverages an inherent graph representation learning (GRL) model to introduce rational phrase-level explainability. We conduct several experiments to quantify the effectiveness of this framework. The experimental results indicate that the proposed model achieves a 10 to 40% improvement in explainability compared to state-of-the-art methods across varying sparsity levels, further highlighting its effectiveness and robustness. Moreover, the experimental results, supported by a case study, reveal that the semantic relationships arising from affixes in Turkish texts can be identified as part of the model’s decision-making process, demonstrating the proposed solution’s ability to effectively capture the agglutinative structure of Turkish.},
  archive      = {J_PEERJCS},
  author       = {Yasir Kilic and Cagatay Neftali Tulu},
  doi          = {10.7717/peerj-cs.2729},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2729},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TurkSentGraphExp: An inherent graph aware explainability framework from pre-trained LLM for turkish sentiment analysis},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of artificial intelligence technology in the economic development of urban intelligent transportation system. <em>PEERJCS</em>, <em>11</em>, e2728. (<a href='https://doi.org/10.7717/peerj-cs.2728'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the social economy and the gradual improvement of residents’ living standards, the increasing number of urban cars has exacerbated urban traffic congestion. This article analyzed the application of artificial intelligence (AI) technology in five aspects of urban intelligent transportation systems. Artificial intelligence technology was used in traffic data collection and processing to provide accurate data support for traffic decision-making. A traffic flow prediction model was established for traffic flow prediction and optimized scheduling algorithms were used to dispatch vehicles on congested urban roads intelligently. Artificial intelligence algorithms can be used to optimize urban traffic signal control systems in intelligent traffic signal control; artificial intelligence technology can be applied to develop intelligent driving systems in the fields of intelligent driving and traffic safety; in terms of data analysis and decision support, it can use AI technology to analyze a large number of traffic data to provide decision support for urban traffic managers, and analyze the impact of the application of AI technology in urban intelligent transportation system on urban economic growth. This article evaluated the economic benefits of artificial intelligence technology in urban intelligent transportation systems. The evaluation results show that the total economic cost of the urban intelligent transportation system after the application of AI technology was 2,961 yuan less than before the application of AI technology, significantly reducing the investment cost of roads. This article analyzes the application of artificial intelligence technology in the economic development of intelligent urban transportation systems, which can meet the needs of healthy urban development and ensure road traffic safety.},
  archive      = {J_PEERJCS},
  author       = {Ziming Zhao and Jinyu Chen},
  doi          = {10.7717/peerj-cs.2728},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2728},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Application of artificial intelligence technology in the economic development of urban intelligent transportation system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel cross-dimensional coarse-fine-grained complementary network for image-text matching. <em>PEERJCS</em>, <em>11</em>, e2725. (<a href='https://doi.org/10.7717/peerj-cs.2725'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fundamental aspects of multimodal applications such as image-text matching, and cross-modal heterogeneity gap between images and texts have always been challenging and complex. Researchers strive to overcome the challenges by proposing numerous significant efforts directed toward narrowing the semantic gap between visual and textual modalities. However, existing methods are usually limited to computing the similarity between images (image regions) and text (text words), ignoring the semantic consistency between fine-grained matching of word regions and coarse-grained overall matching of image and text. Additionally, these methods often ignore the semantic differences across different feature dimensions. Such limitations may result in an overemphasis on specific details at the expense of holistic understanding during image-text matching. To tackle this challenge, this article proposes a new Cross-Dimensional Coarse-Fine-Grained Complementary Network (CDGCN). Firstly, the proposed CDGCN performs fine-grained semantic alignment of image regions and sentence words based on cross-dimensional dependencies. Next, a Coarse-Grained Cross-Dimensional Semantic Aggregation module (CGDSA) is developed to complement local alignment with global image-text matching ensuring semantic consistency. This module aggregates local features across different dimensions as well as within the same dimension to form coherent global features, thus preserving the semantic integrity of the information. The proposed CDGCN is evaluated on two multimodal datasets, Flickr30K and MS-COCO against state-of-the-art methods. The proposed CDGCN achieved substantial improvements with performance increment of 7.7–16% for both datasets.},
  archive      = {J_PEERJCS},
  author       = {Meizhen Liu and Anis Salwa Mohd Khairuddin and Khairunnisa Hasikin and Weitong Liu},
  doi          = {10.7717/peerj-cs.2725},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2725},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Novel cross-dimensional coarse-fine-grained complementary network for image-text matching},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tibyan corpus: Balanced and comprehensive error coverage corpus using ChatGPT for arabic grammatical error correction. <em>PEERJCS</em>, <em>11</em>, e2724. (<a href='https://doi.org/10.7717/peerj-cs.2724'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP) augments text data to overcome sample size constraints. Scarce and low-quality data present particular challenges when learning from these domains. Increasing the sample size is a natural and widely used strategy for alleviating these challenges. Moreover, data-augmentation techniques are commonly used in languages with rich data resources to address problems such as exposure bias. In this study, we chose Arabic to increase the sample size and correct grammatical errors. Arabic is considered one of the languages with limited resources for grammatical error correction (GEC) despite being one of the most popular among Arabs and non-Arabs because of its close connection to Islam. Therefore, this study aims to develop an Arabic corpus called “Tibyan” for grammatical error correction using ChatGPT. ChatGPT is used as a data augmenter tool based on a pair of Arabic sentences containing grammatical errors matched with a sentence free of errors extracted from Arabic books, called guide sentences. Multiple steps were involved in establishing our corpus, including collecting and pre-processing a pair of Arabic texts from various sources, such as books and open-access corpora. We then used ChatGPT to generate a parallel corpus based on the text collected previously, as a guide for generating sentences with multiple types of errors. By engaging linguistic experts to review and validate the automatically generated sentences, we ensured they were correct and error-free. The corpus was validated and refined iteratively based on feedback provided by linguistic experts to improve its accuracy. Finally, we used the Arabic Error Type Annotation tool (ARETA) to analyze the types of errors in the Tibyan corpus. Our corpus contained 49% of errors, including seven types: orthography, morphology, syntax, semantics, punctuation, merge, and split. The Tibyan corpus contains approximately 600 K tokens.},
  archive      = {J_PEERJCS},
  author       = {Ahlam Alrehili and Areej Alhothali},
  doi          = {10.7717/peerj-cs.2724},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2724},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Tibyan corpus: Balanced and comprehensive error coverage corpus using ChatGPT for arabic grammatical error correction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic cassava disease recognition using object segmentation and progressive learning. <em>PEERJCS</em>, <em>11</em>, e2721. (<a href='https://doi.org/10.7717/peerj-cs.2721'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cassava is a vital crop for millions of farmers worldwide, but its cultivation is threatened by various destructive diseases. Current detection methods for cassava diseases are costly, time-consuming, and often limited to controlled environments, making them unsuitable for large-scale agricultural use. This study aims to develop a deep learning framework that enables early, accurate, and efficient detection of cassava diseases in real-world conditions. We propose a self-supervised object segmentation technique, combined with a progressive learning algorithm (PLA) that incorporates both triplet loss and classification loss to learn robust feature embeddings. Our approach achieves superior performance on the Cassava Leaf Disease Classification (CLDC) dataset from the Kaggle competition, with an accuracy of 91.43%, outperforming all other participants. The proposed method offers a practical and efficient solution for cassava disease detection, demonstrating the potential for large-scale, real-world application in agriculture.},
  archive      = {J_PEERJCS},
  author       = {Chang Che and Nian Xue and Zhen Li and Yilin Zhao and Xin Huang},
  doi          = {10.7717/peerj-cs.2721},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2721},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automatic cassava disease recognition using object segmentation and progressive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DeepSpoofNet: A framework for securing UAVs against GPS spoofing attacks. <em>PEERJCS</em>, <em>11</em>, e2714. (<a href='https://doi.org/10.7717/peerj-cs.2714'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed Aerial Vehicles (UAVs) are frequently utilized in several domains such as transportation, distribution, monitoring, and aviation. A significant security vulnerability is the Global Positioning System (GPS) Spoofing attack, wherein the assailant deceives the GPS receiver by transmitting counterfeit signals, thereby gaining control of the UAV. This can result in the UAV being captured or, in certain instances, destroyed. Numerous strategies have been presented to identify counterfeit GPS signals. Although there have been notable advancements in machine learning (ML) for detecting GPS spoofing attacks, there are still challenges and limitations in the current state-of-the-art research. These include imbalanced datasets, sub-optimal feature selection, and the accuracy of attack detection in resource-constrained environments. The proposed framework investigates the optimal pairing of feature selection (FS) methodologies and deep learning techniques for detecting GPS spoofing attacks on UAVs. The primary objective of this study is to address the challenges associated with detecting GPS spoofing attempts in UAVs. The study focuses on tackling the issue of imbalanced datasets by implementing rigorous oversampling techniques. To do this, a comprehensive approach is proposed that combines advanced feature selection techniques with powerful neural network (NN) architectures. The selected attributes from this process are then transmitted to the succeeding tiers of a hybrid NN, which integrates convolutional neural network (CNN) and bidirectional long short-term memory (BiLSTM) components. The Analysis of Variance (ANOVA) + CNN-BiLSTM hybrid model demonstrates superior performance, producing exceptional results with a precision of 98.84%, accuracy of 99.25%, F1 score of 99.26%, and recall of 99.69%. The proposed hybrid model for detecting GPS spoofing attacks exhibits significant improvements in terms of prediction accuracy, true positive and false positive rates, as well as F1 score and recall values.},
  archive      = {J_PEERJCS},
  author       = {Aziz Ur Rehman Badar and Danish Mahmood and Adeel Iqbal and Sung Won Kim and Sedat Akleylek and Korhan Cengiz and Ali Nauman},
  doi          = {10.7717/peerj-cs.2714},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2714},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {DeepSpoofNet: A framework for securing UAVs against GPS spoofing attacks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal fusion transformer-based strategy for efficient multi-cloud content replication. <em>PEERJCS</em>, <em>11</em>, e2713. (<a href='https://doi.org/10.7717/peerj-cs.2713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud computing, ensuring the high availability and reliability of data is dominant for efficient content delivery. Content replication across multiple clouds has emerged as a solution to achieve the above. However, managing optimal replication while considering dynamic changes in data popularity and cloud resource availability remains a formidable challenge. In order to address these challenges, this article employs TFT-based Dynamic Data Replication Strategy (TD2RS), leveraging the Temporal Fusion Transformer (TFT), a deep learning temporal forecasting model. This proposed system collects historical data on content popularity and resource availability from multiple cloud sources, which are then used as input to TFT. Then TFT is used to capture temporal patterns and forecasts future data demands. An intelligent replication is performed to optimize content replication across multiple cloud environments based on these forecasts. The framework’s performance was validated through extensive experiments using synthetic time-series data simulating with varied cloud resource characteristics. Some of the findings include that the proposed TFT approach improves the availability of data by 20% when compared to traditional replication techniques and also cuts down the latency level by 15%. These outcomes indicate that the TFT-based replication strategy targets to improve content delivery efficiency in the dynamic cloud computing environment, thus providing effective solution to dynamically address the availability, reliability, and performance challenges.},
  archive      = {J_PEERJCS},
  author       = {Naganandhini S. and Shanthi D.},
  doi          = {10.7717/peerj-cs.2713},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2713},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Temporal fusion transformer-based strategy for efficient multi-cloud content replication},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative artificial intelligence and machine learning methods to screen social media content. <em>PEERJCS</em>, <em>11</em>, e2710. (<a href='https://doi.org/10.7717/peerj-cs.2710'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Social media research is confronted by the expansive and constantly evolving nature of social media data. Hashtags and keywords are frequently used to identify content related to a specific topic, but these search strategies often result in large numbers of irrelevant results. Therefore, methods are needed to quickly screen social media content based on a specific research question. The primary objective of this article is to present generative artificial intelligence (AI; e.g., ChatGPT) and machine learning methods to screen content from social media platforms. As a proof of concept, we apply these methods to identify TikTok content related to e-cigarette use during pregnancy. Methods We searched TikTok for pregnancy and vaping content using 70 hashtag pairs related to “pregnancy” and “vaping” (e.g., #pregnancytok and #ecigarette) to obtain 11,673 distinct posts. We extracted post videos, descriptions, and metadata using Zeeschuimer and PykTok library. To enhance textual analysis, we employed automatic speech recognition via the Whisper system to transcribe verbal content from each video. Next, we used the OpenCV library to extract frames from the videos, followed by object and text detection analysis using Oracle Cloud Vision. Finally, we merged all text data to create a consolidated dataset and entered this dataset into ChatGPT-4 to determine which posts are related to vaping and pregnancy. To refine the ChatGPT prompt used to screen for content, a human coder cross-checked ChatGPT-4’s outputs for 10 out of every 100 metadata entries, with errors used to inform the final prompt. The final prompt was evaluated through human review, confirming for posts that contain “pregnancy” and “vape” content, comparing determinations to those made by ChatGPT. Results Our results indicated ChatGPT-4 classified 44.86% of the videos as exclusively related to pregnancy, 36.91% to vaping, and 8.91% as containing both topics. A human reviewer confirmed for vaping and pregnancy content in 45.38% of the TikTok posts identified by ChatGPT as containing relevant content. Human review of 10% of the posts screened out by ChatGPT identified a 99.06% agreement rate for excluded posts. Conclusions ChatGPT has mixed capacity to screen social media content that has been converted into text data using machine learning techniques such as object detection. ChatGPT’s sensitivity was found to be lower than a human coder in the current case example but has demonstrated power for screening out irrelevant content and can be used as an initial pass at screening content. Future studies should explore ways to enhance ChatGPT’s sensitivity.},
  archive      = {J_PEERJCS},
  author       = {Kellen Sharp and Rachel R. Ouellette and Rujula Singh Rajendra Singh and Elise E. DeVito and Neil Kamdar and Amanda de la Noval and Dhiraj Murthy and Grace Kong},
  doi          = {10.7717/peerj-cs.2710},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2710},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Generative artificial intelligence and machine learning methods to screen social media content},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A collaborative inference strategy for medical image diagnosis in mobile edge computing environment. <em>PEERJCS</em>, <em>11</em>, e2708. (<a href='https://doi.org/10.7717/peerj-cs.2708'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularity and convenience of mobile medical image analysis and diagnosis in mobile edge computing (MEC) environments have greatly improved the efficiency and quality of healthcare services, necessitating the use of deep neural networks (DNNs) for image analysis. However, DNNs face performance and energy constraints when operating on the mobile side, and are limited by communication costs and privacy issues when operating on the edge side, and previous edge-end collaborative approaches have shown unstable performance and low search efficiency when exploring classification strategies. To address these issues, we propose a DNN edge-optimized collaborative inference strategy (MOCI) for medical image diagnosis, which optimizes data transfer and computation allocation by combining compression techniques and multi-agent reinforcement learning (MARL) methods. The MOCI strategy first uses coding and quantization-based compression methods to reduce the redundancy of image data during transmission at the edge, and then dynamically segments the DNN model through MARL and executes it collaboratively between the edge and the mobile device. To improve policy stability and adaptability, MOCI introduces the optimal transmission distance (Wasserstein) to optimize the policy update process, and uses the long short-term memory (LSTM) network to improve the model’s adaptability to dynamic task complexity. The experimental results show that the MOCI strategy can effectively solve the collaborative inference task of medical image diagnosis and significantly reduce the latency and energy consumption with less than a 2% loss in classification accuracy, with a maximum reduction of 38.5% in processing latency and 71% in energy consumption compared to other inference strategies. In real-world MEC scenarios, MOCI has a wide range of potential applications that can effectively promote the development and application of intelligent healthcare.},
  archive      = {J_PEERJCS},
  author       = {Shiqian Zhang and Yong Cui and Dandan Xu and Yusong Lin},
  doi          = {10.7717/peerj-cs.2708},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2708},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A collaborative inference strategy for medical image diagnosis in mobile edge computing environment},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced mutation strategy based differential evolution for global optimization problems. <em>PEERJCS</em>, <em>11</em>, e2696. (<a href='https://doi.org/10.7717/peerj-cs.2696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential evolution (DE) stands out as a prominent algorithm for addressing global optimization challenges. The efficacy of DE hinges crucially upon its mutation operation, which serves as a pivotal mechanism in generating diverse and high-quality solutions. This article explores various mutation operations aimed at augmenting the performance of DE in global optimization tasks. A distinct mutation strategy is introduced, with the primary objective of achieving a harmonious equilibrium between exploration and exploitation to enhance both convergence speed and solution quality. The proposed DE centres on a novel mutation-based strategy, introducing a new coefficient factor (“σ”) in conjunction with the base vector of the basic mutation strategy (“DE/rand/1”). This innovation aims to fortify the convergence of local variables during exploitation, thereby improving both the convergence rate and quality. The effectiveness of the proposed mutation operations is evaluated across a set of 27 benchmark functions commonly employed in global optimization. Experimental results conclusively demonstrate that these enhanced mutation strategies significantly outperform state-of-the-art algorithms in terms of solution accuracy and convergence speed. This study underscores the critical role of mutation operations in DE and provides valuable insights for designing more potent mutation strategies to tackle complex global optimization problems.},
  archive      = {J_PEERJCS},
  author       = {Pawan Mishra and Musrrat Ali and Pooja and Safiqul Islam},
  doi          = {10.7717/peerj-cs.2696},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2696},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced mutation strategy based differential evolution for global optimization problems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI-driven balance evaluation: A comparative study between blind and non-blind individuals using the mini-BESTest. <em>PEERJCS</em>, <em>11</em>, e2695. (<a href='https://doi.org/10.7717/peerj-cs.2695'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are 2.2 billion visually impaired individuals and 285 million blind people worldwide. The vestibular system plays a fundamental role in the balance of a person related to sight and hearing, and thus blind people require physical therapy to improve their balance. Several clinical tests have been developed to evaluate balance, such as the mini-BESTest. This test has been used to evaluate the balance of people with neurological diseases, but there have been no studies that evaluate the balance of blind individuals before. Furthermore, despite the scoring of these tests being not subjective, the performance of some activities are subject to the physiotherapist’s bias. Tele-rehabilitation is a growing field that aims to provide physical therapy to people with disabilities. Among the technologies used in tele-rehabilitation are inertial measurement units that can be used to monitor the balance of individuals. The amount of data collected by these devices is large and the use of deep learning models can help in analyzing these data. Therefore, the objective of this study is to analyze for the first time the balance of blind individuals using the mini-BESTest and inertial measurement units and to identify the activities that best differentiate between blind and sighted individuals. We use the OpenSense RT monitoring device to collect data from the inertial measurement unit, and we develop machine learning and deep learning models to predict the score of the most relevant mini-BESTest activities. In this study 29 blind and sighted individuals participated. The one-legged stance is the activity that best differentiates between blind and sighted individuals. An analysis on the acceleration data suggests that the evaluation of physiotherapists is not completely adjusted to the test criterion. Cluster analysis suggests that inertial data are not able to distinguish between three levels of evaluation. However, the performance of our models shows an F1-score of 85.6% in predicting the score evaluated by the mini-BESTest in a binary classification problem. The results of this study can help physiotherapists have a more objective evaluation of the balance of their patients and to develop tele-rehabilitation systems for blind individuals.},
  archive      = {J_PEERJCS},
  author       = {Milagros Jaén-Vargas and Josué Pagán and Shiyang Li and María Fernanda Trujillo-Guerrero and Niloufar Kazemi and Alessio Sansò and Benito Codina-Casals and Roy Abi Zeid Daou and Jose Javier Serrano Olmedo},
  doi          = {10.7717/peerj-cs.2695},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2695},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {AI-driven balance evaluation: A comparative study between blind and non-blind individuals using the mini-BESTest},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparing diversity, negativity, and stereotypes in chinese-language AI technologies: An investigation of baidu, ernie and qwen. <em>PEERJCS</em>, <em>11</em>, e2694. (<a href='https://doi.org/10.7717/peerj-cs.2694'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) and search engines have the potential to perpetuate biases and stereotypes by amplifying existing prejudices in their training data and algorithmic processes, thereby influencing public perception and decision-making. While most work has focused on Western-centric AI technologies, we examine social biases embedded in prominent Chinese-based commercial tools, the main search engine Baidu and two leading LLMs, Ernie and Qwen. Leveraging a dataset of 240 social groups across 13 categories describing Chinese society, we collect over 30 k views encoded in the aforementioned tools by prompting them to generate candidate words describing these groups. We find that language models exhibit a broader range of embedded views compared to the search engine, although Baidu and Qwen generate negative content more often than Ernie. We also observe a moderate prevalence of stereotypes embedded in the language models, many of which potentially promote offensive or derogatory views. Our work highlights the importance of prioritizing fairness and inclusivity in AI technologies from a global perspective.},
  archive      = {J_PEERJCS},
  author       = {Geng Liu and Carlo Alberto Bono and Francesco Pierri},
  doi          = {10.7717/peerj-cs.2694},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2694},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Comparing diversity, negativity, and stereotypes in chinese-language AI technologies: An investigation of baidu, ernie and qwen},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fake news detection: State-of-the-art review and advances with attention to arabic language aspects. <em>PEERJCS</em>, <em>11</em>, e2693. (<a href='https://doi.org/10.7717/peerj-cs.2693'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The proliferation of fake news has become a significant threat, influencing individuals, institutions, and societies at large. This issue has been exacerbated by the pervasive integration of social media into daily life, directly shaping opinions, trends, and even the economies of nations. Social media platforms have struggled to mitigate the effects of fake news, relying primarily on traditional methods based on human expertise and knowledge. Consequently, machine learning (ML) and deep learning (DL) techniques now play a critical role in distinguishing fake news, necessitating their extensive deployment to counter the rapid spread of misinformation across all languages, particularly Arabic. Detecting fake news in Arabic presents unique challenges, including complex grammar, diverse dialects, and the scarcity of annotated datasets, along with a lack of research in the field of fake news detection compared to English. This study provides a comprehensive review of fake news, examining its types, domains, characteristics, life cycle, and detection approaches. It further explores recent advancements in research leveraging ML, DL, and transformer-based techniques for fake news detection, with a special attention to Arabic. The research delves into Arabic-specific pre-processing techniques, methodologies tailored for fake news detection in the language, and the datasets employed in these studies. Additionally, it outlines future research directions aimed at developing more effective and robust strategies to address the challenge of fake news detection in Arabic content.},
  archive      = {J_PEERJCS},
  author       = {Eman Salamah Albtoush and Keng Hoon Gan and Saif A. Ahmad Alrababa},
  doi          = {10.7717/peerj-cs.2693},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2693},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fake news detection: State-of-the-art review and advances with attention to arabic language aspects},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and analysis of teaching early warning system based on multimodal data in an intelligent learning environment. <em>PEERJCS</em>, <em>11</em>, e2692. (<a href='https://doi.org/10.7717/peerj-cs.2692'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In online teaching environments, the lack of direct emotional interaction between teachers and students poses challenges for teachers to consciously and effectively manage their emotional expressions. The design and implementation of an early warning system for teaching provide a novel approach to intelligent evaluation and improvement of online education. This study focuses on segmenting different emotional segments and recognizing emotions in instructional videos. An efficient long-video emotional transition point search algorithm is proposed for segmenting video emotional segments. Leveraging the fact that teachers tend to maintain a neutral emotional state for significant portions of their teaching, a neutral emotional segment filtering algorithm based on facial features has been designed. A multimodal emotional recognition model is proposed for emotional recognition in instructional videos. It begins with preprocessing the raw speech and facial image features, employing a semi-supervised iterative feature normalization algorithm to eliminate individual teacher differences while preserving inherent differences between different emotions. A deep learning-based multimodal emotional recognition model for teacher instructional videos is introduced, incorporating an attention mechanism to automatically assign weights for feature-level modal fusion, providing users with accurate emotional classification. Finally, a teaching early warning system is implemented based on these algorithms.},
  archive      = {J_PEERJCS},
  author       = {Xinxin Kang and Yong Nie},
  doi          = {10.7717/peerj-cs.2692},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2692},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design and analysis of teaching early warning system based on multimodal data in an intelligent learning environment},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social big data management through collaborative mobile, regional, and cloud computing. <em>PEERJCS</em>, <em>11</em>, e2689. (<a href='https://doi.org/10.7717/peerj-cs.2689'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The crowd of smart devices surrounds us all the time. These devices popularize social media platforms (SMP), connecting billions of users. The enhanced functionalities of smart devices generate big data that overutilizes the mainstream network, degrading performance and increasing the overall cost, compromising time-sensitive services. Research indicates that about 75% of connections come from local areas, and their workload does not need to be migrated to remote servers in real-time. Collaboration among mobile edge computing (MEC), regional computing (RC), and cloud computing (CC) can effectively fill these gaps. Therefore, we propose a collaborative structure of mobile, regional, and cloud computing to address the issues arising from social big data (SBD). In this model, it may be easily accessed from the nearest device or server rather than downloading a file from the cloud server. Furthermore, instead of transferring each file to the cloud servers during peak hours, they are initially stored on a regional level and subsequently uploaded to the cloud servers during off-peak hours. The outcomes affirm that this approach significantly reduces the impact of substantial SBD on the performance of mainstream and social network platforms, specifically in terms of delay, response time, and cost.},
  archive      = {J_PEERJCS},
  author       = {Afzal Badshah and Ameen Banjar and Safa Habibullah and Abdullah Alharbi and Wael Alosaimi and Ali Daud},
  doi          = {10.7717/peerj-cs.2689},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2689},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Social big data management through collaborative mobile, regional, and cloud computing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data augmentation for arabic text classification: A review of current methods, challenges and prospective directions. <em>PEERJCS</em>, <em>11</em>, e2685. (<a href='https://doi.org/10.7717/peerj-cs.2685'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effectiveness of data augmentation techniques, i.e., methods for artificially creating new data, has been demonstrated in many domains, from images to textual data. Data augmentation methods were established to manage different issues regarding the scarcity of training datasets or the class imbalance to enhance the performance of classifiers. This review article investigates data augmentation techniques for Arabic texts, specifically in the text classification field. A thorough review was conducted to give a concise and comprehensive understanding of these approaches in the context of Arabic classification. The focus of this article is on Arabic studies published from 2019 to 2024 about data augmentation in Arabic text classification. Inclusion and exclusion criteria were applied to ensure a comprehensive vision of these techniques in Arabic natural language processing (ANLP). It was found that data augmentation research for Arabic text classification dominates sentiment analysis and propaganda detection, with initial studies emerging in 2019; very few studies have investigated other domains like sarcasm detection or text categorization. We also observed the lack of benchmark datasets for performing the tasks. Most studies have focused on short texts, such as Twitter data or reviews, while research on long texts still needs to be explored. Additionally, various data augmentation methods still need to be examined for long texts to determine if techniques effective for short texts are also applicable to longer texts. A rigorous investigation and comparison of the most effective strategies is required due to the unique characteristics of the Arabic language. By doing so, we can better understand the processes involved in Arabic text classification and hence be able to select the most suitable data augmentation methods for specific tasks. This review contributes valuable insights into Arabic NLP and enriches the existing body of knowledge.},
  archive      = {J_PEERJCS},
  author       = {Samia F. Abdhood and Nazlia Omar and Sabrina Tiun},
  doi          = {10.7717/peerj-cs.2685},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2685},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Data augmentation for arabic text classification: A review of current methods, challenges and prospective directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization study of intelligent accounting manager system modules in adaptive behavioral pattern learning and simulation. <em>PEERJCS</em>, <em>11</em>, e2684. (<a href='https://doi.org/10.7717/peerj-cs.2684'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the ambit of the digital epoch, the advent of adaptive learning technologies heralds a paradigmatic shift in the realm of accounting management, garnering increasing scrutiny for augmenting learning outcomes via more sagacious educational methodologies and refining the accounting management protocols through the employment of sophisticated optimization techniques. This manuscript delineates an avant-garde health classification schema for accounting management, termed the A-CHMM-FD methodology, which amalgamates the merits of the Analytic Hierarchy Process (AHP) with the Coupled Hidden Markov Model (CHMM) to enhance the precision and efficacy of risk detection. Utilizing the AHP modality, we quantify diverse accounting metrics, subsequently subjected to independent scrutiny via the CHMM. This results in an exhaustive evaluation of entities as healthy, at-risk, or high-risk employing fuzzy delineations. Empirical validation on publicly available financial risk datasets and the pragmatic deployment of bespoke datasets affirm the superior efficiency and precision of the proposed framework. Applying this methodology within the health classification of accounting management emerges as efficacious, charting a novel technological trajectory for managing accounting risks and offering fresh perspectives on the nurturing of accounting understanding and the acquisition of knowledge.},
  archive      = {J_PEERJCS},
  author       = {Yifan Wang and Rongjie Qin and Musadaq Mansoor},
  doi          = {10.7717/peerj-cs.2684},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2684},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimization study of intelligent accounting manager system modules in adaptive behavioral pattern learning and simulation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial expression recognition using visible and IR by early fusion of deep learning with attention mechanism. <em>PEERJCS</em>, <em>11</em>, e2676. (<a href='https://doi.org/10.7717/peerj-cs.2676'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial expression recognition (FER) has garnered significant attention due to advances in artificial intelligence, particularly in applications like driver monitoring, healthcare, and human-computer interaction, which benefit from deep learning techniques. The motivation of this research is to address the challenges of accurately recognizing emotions despite variations in expressions across emotions and similarities between different expressions. In this work, we propose an early fusion approach that combines features from visible and infrared modalities using publicly accessible VIRI and NVIE databases. Initially, we developed single-modality models for visible and infrared datasets by incorporating an attention mechanism into the ResNet-18 architecture. We then extended this to a multi-modal early fusion approach using the same modified ResNet-18 with attention, achieving superior accuracy through the combination of convolutional neural network (CNN) and transfer learning (TL). Our multi-modal approach attained 84.44% accuracy on the VIRI database and 85.20% on the natural visible and infrared facial expression (NVIE) database, outperforming previous methods. These results demonstrate that our single-modal and multi-modal approaches achieve state-of-the-art performance in FER.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Tahir Naseem and Chan-Su Lee and Tariq Shahzad and Muhammad Adnan Khan and Adnan M. Abu-Mahfouz and Khmaies Ouahada},
  doi          = {10.7717/peerj-cs.2676},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2676},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Facial expression recognition using visible and IR by early fusion of deep learning with attention mechanism},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Development of a cryptocurrency price prediction model: Leveraging GRU and LSTM for bitcoin, litecoin and ethereum. <em>PEERJCS</em>, <em>11</em>, e2675. (<a href='https://doi.org/10.7717/peerj-cs.2675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptocurrency represents a form of asset that has arisen from the progress of financial technology, presenting significant prospects for scholarly investigations. The ability to anticipate cryptocurrency prices with extreme accuracy is very desirable to researchers and investors. However, time-series data presents significant challenges due to the nonlinear nature of the cryptocurrency market, complicating precise price predictions. Several studies have explored cryptocurrency price prediction using various deep learning (DL) algorithms. Three leading cryptocurrencies, determined by market capitalization, Ethereum (ETH), Bitcoin (BTC), and Litecoin (LTC), are examined for exchange rate predictions in this study. Two categories of recurrent neural networks (RNNs), specifically long short-term memory (LSTM) and gated recurrent unit (GRU), are employed. Four performance metrics are selected to evaluate the prediction accuracy namely mean squared error (MSE), mean absolute error (MAE), mean absolute percentage error (MAPE), and root mean squared error (RMSE) for three cryptocurrencies which demonstrates that GRU model outperforms LSTM. The GRU model was implemented as a two-layer deep learning network, optimized using the Adam optimizer with a dropout rate of 0.2 to prevent overfitting. The model was trained using normalized historical price data sourced from CryptoDataDownload, with an 80:20 train-test split. In this work, GRU qualifies as the best algorithm for developing a cryptocurrency price prediction model. MAPE values for BTC, LTC and ETH are 0.03540, 0.08703 and 0.04415, respectively, which indicate that GRU offers the most accurate forecasts as compared to LSTM. These prediction models are valuable for traders and investors, offering accurate cryptocurrency price predictions. Future studies should also consider additional variables, such as social media trends and trade volumes that may impact cryptocurrency pricing.},
  archive      = {J_PEERJCS},
  author       = {Ramneet Kaur and Mudita Uppal and Deepali Gupta and Sapna Juneja and Syed Yasser Arafat and Junaid Rashid and Jungeun Kim and Roobaea Alroobaea},
  doi          = {10.7717/peerj-cs.2675},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2675},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Development of a cryptocurrency price prediction model: Leveraging GRU and LSTM for bitcoin, litecoin and ethereum},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced transformer for length-controlled abstractive summarization based on summary output area. <em>PEERJCS</em>, <em>11</em>, e2667. (<a href='https://doi.org/10.7717/peerj-cs.2667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in abstractive summarization models, particularly those built on encoder-decoder architectures, typically produce a single summary for each source text. Controlling the length of summaries is crucial for practical applications, such as crafting cover summaries for newspapers or magazines with varying slot sizes. Current research in length-controllable abstractive summarization employs techniques like length embeddings in the decoder module or a word-level extractive module in the encoder-decoder model. However, these approaches, while effective in determining when to halt decoding, fall short in selecting relevant information to include within the specified length constraint. This article diverges from prior models reliant on predefined lengths. Instead, it introduces a novel approach to length-controllable abstractive summarization by integrating an image processing phase. This phase determines the specific size of the summary output slot. The proposed model harnesses enhanced T5 and GPT models, seamlessly adapting summaries to designated slots. The computed area of a given slot is employed in both models to generate abstractive summaries tailored to fit the output slot perfectly. Experimental evaluations on the CNN/Daily Mail dataset demonstrate the model’s success in performing length-controlled summarization, yielding superior results.},
  archive      = {J_PEERJCS},
  author       = {Yusuf Sunusi and Nazlia Omar and Lailatul Qadri Zakaria},
  doi          = {10.7717/peerj-cs.2667},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2667},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhanced transformer for length-controlled abstractive summarization based on summary output area},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic approaches for query expansion: Taxonomy, challenges, and future research directions. <em>PEERJCS</em>, <em>11</em>, e2664. (<a href='https://doi.org/10.7717/peerj-cs.2664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The internet has been inundated with an ocean of information, and hence, information retrieval systems are failing to provide optimal results to the user. In order to meet the challenge, query expansion techniques have emerged as a game-changer and are improving the results of information retrieval significantly. Of late, semantic query expansion techniques have attracted increased interest among researchers since these techniques offer more pertinent and practical results to the users. These allow the user to retrieve more meaningful and useful information from the web. Currently, few research works provide a comprehensive review on semantic query expansion; usually, they cannot provide a full view on recent advances, diversified data application, and practical challenges. Therefore, it is imperative to go deep in review in order to explain these advances and assist researchers with concrete insights for future development. This article represents the comprehensive review of the query expansion methods, with a particular emphasis on semantic approaches. It overviews the recent frameworks that have been developed within a period of 2015–2024 and reviews the limitations of each approach. Further, it discusses challenges that are inherent in the semantic query expansion field and identifies some future research directions. This article emphasizes that the linguistic approach is the most effective and flexible direction for researchers to follow, while the ontology approach better suits domain-specific search applications. This, in turn, means that development of the ontology field may further open new perspectives for semantic query expansion. Moreover, by employing artificial intelligence (AI) and making most of the query context without relying on user intervention, improvements toward the optimal expanded query can be achieved.},
  archive      = {J_PEERJCS},
  author       = {Azzah Allahim and Asma Cherif and Abdessamad Imine},
  doi          = {10.7717/peerj-cs.2664},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2664},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Semantic approaches for query expansion: Taxonomy, challenges, and future research directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy inference rule based task offloading model (FI-RBTOM) for edge computing. <em>PEERJCS</em>, <em>11</em>, e2657. (<a href='https://doi.org/10.7717/peerj-cs.2657'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The key objective of edge computing is to reduce delays and provide consumers with high-quality services. However, there are certain challenges, such as high user mobility and the dynamic environments created by IoT devices. Additionally, the limitations of constrained device resources impede effective task completion. The challenge of task offloading plays a crucial role as one of the key challenges for edge computing, which is addressed in this research. An efficient rule-based task-offloading model (FI-RBTOM) is proposed in this context. The key decision of the proposed model is to choose either the task to be offloaded over an edge server or the cloud server or it can be processed over a local node. The four important input parameters are bandwidth, CPU utilization, task length, and task size. The proposed (FI-RBTOM), simulation is carried out using MATLAB (fuzzy logic) tool with 75% training and 25% testing with an overall error rate of 0.39875 is achieved.},
  archive      = {J_PEERJCS},
  author       = {Kashif Ibrahim and Ahthasham Sajid and Ihsan Ullah and Inam Ullah Khan and Keshav Kaushik and S S. Askar and Mohamed Abouhawwash},
  doi          = {10.7717/peerj-cs.2657},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2657},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fuzzy inference rule based task offloading model (FI-RBTOM) for edge computing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A laboratory experiment on using different financial-incentivization schemes in software-engineering experimentation. <em>PEERJCS</em>, <em>11</em>, e2650. (<a href='https://doi.org/10.7717/peerj-cs.2650'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In software-engineering research, many empirical studies are conducted with open-source or industry developers. However, in contrast to other research communities like economics or psychology, only few experiments use financial incentives (i.e., paying money) as a strategy to motivate participants’ behavior and reward their performance. The most recent version of the SIGSOFT Empirical Standards mentions payouts only for increasing participation in surveys, but not for mimicking real-world motivations and behavior in experiments. Within this article, we report a controlled experiment in which we tackled this gap by studying how different financial incentivization schemes impact developers. For this purpose, we first conducted a survey on financial incentives used in the real-world, based on which we designed three incentivization schemes: (1) a performance-dependent scheme that employees prefer, (2) a scheme that is performance-independent, and (3) a scheme that mimics open-source development. Then, using a between-subject experimental design, we explored how these three schemes impact participants’ performance. Our findings indicate that the different schemes can impact participants’ performance in software-engineering experiments. Our results are not statistically significant, possibly due to small sample sizes and the consequent lack of statistical power, but with some notable trends that may inspire future hypothesis generation. Our contributions help understand the impact of financial incentives on participants in experiments as well as real-world scenarios, guiding researchers in designing experiments and organizations in compensating developers.},
  archive      = {J_PEERJCS},
  author       = {Dmitri Bershadskyy and Jacob Krüger and Gül Calıklı and Siegmar Otto and Sarah Zabel and Jannik Greif and Robert Heyer},
  doi          = {10.7717/peerj-cs.2650},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2650},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A laboratory experiment on using different financial-incentivization schemes in software-engineering experimentation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transformer-based ensemble model for dialectal arabic sentiment classification. <em>PEERJCS</em>, <em>11</em>, e2644. (<a href='https://doi.org/10.7717/peerj-cs.2644'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social media platforms such as X, Facebook, and Instagram have become essential avenues for individuals to articulate their opinions, especially during global emergencies. These platforms offer valuable insights that necessitate analysis for informed decision-making and a deeper understanding of societal trends. Sentiment analysis is crucial for assessing public sentiment toward specific issues; however, applying it to dialectal Arabic presents considerable challenges in natural language processing. The complexity arises from the language’s intricate semantic and morphological structures, along with the existence of multiple dialects. This form of analysis, also referred to as sentiment classification, opinion mining, emotion mining, and review mining, is the focus of this study, which analyzes tweets from three benchmark datasets: the Arabic Sentiment Tweets Dataset (ASTD), the A Twitter-based Benchmark Arabic Sentiment Analysis Dataset (ASAD), and the Tweets Emoji Arabic Dataset (TEAD). The research involves experimentation with a variety of comparative models, including machine learning, deep learning, transformer-based models, and a transformer-based ensemble model. Feature extraction for both machine learning and deep learning approaches is performed using techniques such as AraVec, FastText, AraBERT, and Term Frequency-Inverse Document Frequency (TF-IDF). The study compares machine learning models such as support vector machine (SVM), naïve Bayes (NB), decision tree (DT), and extreme gradient boosting (XGBoost) with deep learning models such as convolutional neural networks (CNN) and bidirectional long short-term memory (BLSTM) networks. Additionally, it explores transformer-based models such as CAMeLBERT, XLM-RoBERTa, and MARBERT, along with their ensemble configurations. The findings demonstrate that the proposed transformer-based ensemble model achieved superior performance, with average accuracy, recall, precision, and F1-score of 90.4%, 88%, 87.3%, and 87.7%, respectively.},
  archive      = {J_PEERJCS},
  author       = {Omar Mansour and Eman Aboelela and Remon Talaat and Mahmoud Bustami},
  doi          = {10.7717/peerj-cs.2644},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2644},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Transformer-based ensemble model for dialectal arabic sentiment classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of wheat fusarium head blight severity levels in southern henan based on K-means-SMOTE and XGBoost algorithms. <em>PEERJCS</em>, <em>11</em>, e2638. (<a href='https://doi.org/10.7717/peerj-cs.2638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fusarium head blight (FHB) is a destructive disease which adversely affects the yield of wheat. The occurrence and epidemic of wheat FHB are closely related to meteorological information. Firstly, by analyzing eight meteorological factors—rainfall (RAIN), average sunshine hours (ASH), average wind speed (AWS), average temperature (AT), highest temperature (HT), lowest temperature (LT), average relative humidity (ARH), and maximum temperature difference (MTD)—specific periods closely related to wheat FHB severity are identified. Based on this, a dataset for wheat FHB severity is constructed. After that, the wheat FHB severity levels are divided into four levels, and actual field data shows that the proportion of data for the high prevalence severity level is relatively small. To address data imbalance, the K-means-synthetic minority over-sampling technique (K-means-SMOTE) method is introduced to increase samples of underrepresented severity levels. Subsequently, a wheat FHB severity prediction model based on K-means-SMOTE and extreme gradient boosting (XGBoost) is constructed. Lastly, by combining the rankings of meteorological factors provided by the model and the biological characteristics of wheat FHB, the number of meteorological factors is reduced from eight to four (AWS 4.24–4.28, RAIN 4.5–4.19, ARH 4.12–4.16, LT 4.19–4.23), the accuracy and recall of the model remained unchanged at 0.8936, the F1 score increased from 0.8851 to 0.8898, and the precision decreased from 0.9249 to 0.9058. Although the precision has slightly decreased, most of the other evaluation indicators of the model remain unchanged or have improved, therefore the model is considered effective. Finally, comparative experiments with eight other models demonstrate the superiority of this approach.},
  archive      = {J_PEERJCS},
  author       = {Xiaoyun Sun and Shuaiming Su and Qiang Wang and Shufeng Xiong and Yanting Li and Hong Peng and Lei Shi},
  doi          = {10.7717/peerj-cs.2638},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2638},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Prediction of wheat fusarium head blight severity levels in southern henan based on K-means-SMOTE and XGBoost algorithms},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Proactive detection of anomalous behavior in ethereum accounts using XAI-enabled ensemble stacking with bayesian optimization. <em>PEERJCS</em>, <em>11</em>, e2630. (<a href='https://doi.org/10.7717/peerj-cs.2630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decentralized, open-source architecture of blockchain technology, exemplified by the Ethereum platform, has transformed online transactions by enabling secure and transparent exchanges. However, this architecture also exposes the network to various security threats that cyber attackers can exploit. Detecting suspicious behaviors in account on the Ethereum blockchain can help mitigate attacks, including phishing, Ponzi schemes, eclipse attacks, Sybil attacks, and distributed denial of service (DDoS) incidents. The proposed system introduces an ensemble stacking model combining Random Forest (RF), eXtreme Gradient Boosting (XGBoost), and a neural network (NN) to detect potential threats within the Ethereum platform. The ensemble model is fine-tuned using Bayesian optimization to enhance predictive accuracy, while explainable artificial intelligence (XAI) tools—SHAP, LIME, and ELI5—provide interpretable feature insights, improving transparency in model predictions. The dataset used comprises 9,841 Ethereum transactions across 52 initial fields (reduced to 17 relevant features), encompassing both legitimate and fraudulent records. The experimental findings demonstrate that the proposed model achieves a superior accuracy of 99.6%, outperforming that of other cutting-edge methods. These findings demonstrate that the XAI-enabled ensemble stacking model offers a highly effective, interpretable solution for blockchain security, strengthening trust and reliability within the Ethereum ecosystem.},
  archive      = {J_PEERJCS},
  author       = {Vasavi Chithanuru and Mangayarkarasi Ramaiah},
  doi          = {10.7717/peerj-cs.2630},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2630},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Proactive detection of anomalous behavior in ethereum accounts using XAI-enabled ensemble stacking with bayesian optimization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing forensic file classification: Enhancing SFCS with βk hyperparameter tuning. <em>PEERJCS</em>, <em>11</em>, e2608. (<a href='https://doi.org/10.7717/peerj-cs.2608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In forensic topical modelling, the α parameter controls the distribution of topics in documents. However, low, high, or incorrect values of α lead to topic sparsity, model overfitting, and suboptimal topic distribution. To control the word distribution across topics, the β parameter is introduced. However, low, high, or inappropriate β values lead to sparse distribution, disjointed topics, and abundant highly probable words. The βj parameter, in conjunction with seed-guided words based on Term Frequency and Inverse Document Frequency, is introduced to address the issues. Nevertheless, the data often suffers from skewness or noise due to frequent co-occurrences of unrelated polysemic word pairs generated using Pointwise Mutual Information. By integrating α, β, and βj into file classification systems, classification models converge to local optima with O(n log n* |V|) time complexity. To combat these challenges, this research proposes the SDOT Forensic Classification System (SFCS) with a functional parameter βk that identifies seed words by evaluating semantic and contextual similarity of word vectors. As a result, the topic distribution (Θd) is compelled to model the curated seed words within the distribution, generating pertinent topics. Incorporating βk into SFCS allowed the proposed model to remove 278 k irrelevant files from the corpus and identify 5.6 k suspicious files by extracting 700 blacklisted keywords. Furthermore, this research implemented hyperparameter optimization and hyperplane maximization, resulting in a file classification accuracy of 94.6%, 94.4% precision and 96.8% recall within O(n log n) complexity.},
  archive      = {J_PEERJCS},
  author       = {D. Paul Joseph and Viswanathan Perumal},
  doi          = {10.7717/peerj-cs.2608},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2608},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimizing forensic file classification: Enhancing SFCS with βk hyperparameter tuning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain technology and its impact on sustainable supply chain management in SMEs. <em>PEERJCS</em>, <em>11</em>, e2466. (<a href='https://doi.org/10.7717/peerj-cs.2466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The COVID-19 pandemic has had a significant impact on small and medium-sized enterprises (SMEs), leading to disruptions in supply chains, financial losses, and closures. To overcome these challenges, organizations, including those in developing economies like Malaysia, are turning to blockchain technology as a solution to enhance traditional supply chain management frameworks. This study aims to identify the factors that influence the acceptance of blockchain technology among SMEs. By drawing on established adoption theories such as the technology acceptance model (TAM), diffusion of innovation (DOI) theory, and theory of planned behavior (TPB), the researchers developed a research framework. They utilized partial least square structural equation modeling (PLS-SEM) to analyze the causal relationships between different constructs and test their hypotheses. The findings confirmed that the constructs of the technology acceptance model, specifically perceived usefulness, perceived ease of use and attitude were significantly associated with the intention to use blockchain technology. Additionally, the constructs of the diffusion of innovation theory, relative advantage and compatibility, showed significant associations with perceived ease of use, while complexity had a negligible relationship with perceived usefulness and perceived ease of use. The construct of subjective norms from the theory of planned behavior exhibited a significant relationship with perceived usefulness and an insignificant relationship with intention to use. Finally, perceived behavioral control demonstrated a positive relationship with intention to use. The study’s findings provide valuable insights for blockchain developers and organizations aiming to make informed decisions regarding the application of blockchain technology as a process innovation in SMEs.},
  archive      = {J_PEERJCS},
  author       = {Chao Fang and Nazir Ullah and M. Batumalay and Waleed Mugahed Al-Rahmi and Fahad Alblehai},
  doi          = {10.7717/peerj-cs.2466},
  journal      = {PeerJ Computer Science},
  month        = {3},
  pages        = {e2466},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Blockchain technology and its impact on sustainable supply chain management in SMEs},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging LLaMA2 for improved document classification in english. <em>PEERJCS</em>, <em>11</em>, e2740. (<a href='https://doi.org/10.7717/peerj-cs.2740'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document classification is an important component of natural language processing, with applications that include sentiment analysis, content recommendation, and information retrieval. This article investigates the potential of Large Language Model Meta AI (LLaMA2), a cutting-edge language model, to enhance document classification in English. Our experiments show that LLaMA2 outperforms traditional classification methods, achieving higher precision and recall values on the WOS-5736 dataset. Additionally, we analyze the interpretability of LLaMA2’s classification process to reveal the most pertinent features for categorization and the model’s decision-making. These results emphasize the potential of advanced language models to enhance classification outcomes and provide a more profound comprehension of document structures, thereby contributing to the advancement of natural language processing methodologies.},
  archive      = {J_PEERJCS},
  author       = {Jia Xu},
  doi          = {10.7717/peerj-cs.2740},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2740},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging LLaMA2 for improved document classification in english},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A majority voting framework for reliable sentiment analysis of product reviews. <em>PEERJCS</em>, <em>11</em>, e2738. (<a href='https://doi.org/10.7717/peerj-cs.2738'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a tailored majority voting approach for enhancing the consistency and reliability of sentiment analysis in online product reviews. The methodology addresses discrepancies in sentiment classification by leveraging sentiment labels from multiple automated tools and implementing a robust majority decision rule. This consensus-based approach significantly enhances the trustworthiness and consistency of sentiment analysis outcomes, serving as a dependable foundation for training more precise sentiment analysis models. The data labeled with our method was utilized to train deep learning models, achieving competitive accuracy with significantly less data. The findings demonstrate the effectiveness of the method in producing results comparable to commercial tools while ensuring data consistency for model training.},
  archive      = {J_PEERJCS},
  author       = {Darie Moldovan},
  doi          = {10.7717/peerj-cs.2738},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2738},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A majority voting framework for reliable sentiment analysis of product reviews},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Autoregressive models for session-based recommendations using set expansion. <em>PEERJCS</em>, <em>11</em>, e2734. (<a href='https://doi.org/10.7717/peerj-cs.2734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid growth of internet technologies, session-based recommendation systems have emerged as a key paradigm in delivering personalized recommendations by capturing users’ dynamic and short-term preferences. Traditional methods predominantly rely on modeling the sequential order of user interactions, deep learning approaches like recurrent neural networks and Transformer architectures. However, these sequence-based models often struggle in scenarios where the order of interactions is ambiguous or unreliable, limiting their real-world applicability. To address this challenge, we propose a novel session-based recommendation model, Deep Set Session-based Recommendation (DSETRec), which approaches the problem from a set-based perspective, eliminating dependence on the interaction sequence. By conceptualizing session data as unordered sets, our model captures the coupling relationships and co-occurrence patterns between items, enhancing prediction accuracy in settings where sequential information is either unavailable or noisy. The model is implemented using a deep autoregressive framework that iteratively masks known elements within a session, predicting and reconstructing additional items based on set data characteristics. Extensive experiments on benchmark datasets show that DSETRec achieves outperforms state-of-the-art baselines. DSETRec achieves a 13.2% and 11.85% improvement in P@20 and MRR@20, respectively, over its sequence-based variant on Yoochoose. Additionally, DSETRec generalizes effectively across both further short and long sessions. These results highlight the robustness of the set-based approach in capturing unordered interaction patterns and adapting to diverse session lengths. This finding provides a foundation for developing more flexible and generalized session-based recommendation systems.},
  archive      = {J_PEERJCS},
  author       = {Tianhao Yu and Xianghong Zhou and Xinrong Deng},
  doi          = {10.7717/peerj-cs.2734},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2734},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Autoregressive models for session-based recommendations using set expansion},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptive method for determining the optimal number of topics in topic modeling. <em>PEERJCS</em>, <em>11</em>, e2723. (<a href='https://doi.org/10.7717/peerj-cs.2723'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic models have been successfully applied to information classification and retrieval. The difficulty in successfully applying these technologies is to select the appropriate number of topics for a given corpus. Selecting too few topics can result in information loss and topic omission, known as underfitting. Conversely, an excess of topics can introduce noise and complexity, resulting in overfitting. Therefore, this article considers the inter-class distance and proposes a new method to determine the number of topics based on clustering results, named average inter-class distance change rate (AICDR). AICDR employs the Ward’s method to calculate inter-class distances, then calculates the average inter-class distance for different numbers of topics, and determines the optimal number of topics based on the average distance change rate. Experiments show that the number of topics determined by AICDR is more in line with the true classification of datasets, with high inter-class distance and low inter-class similarity, avoiding the phenomenon of topic overlap. AICDR is a technique predicated on clustering results to select the optimal number of topics and has strong adaptability to various topic models.},
  archive      = {J_PEERJCS},
  author       = {Yang Xu and Yueyi Zhang and Yefang Sun and Hanting Zhou},
  doi          = {10.7717/peerj-cs.2723},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2723},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An adaptive method for determining the optimal number of topics in topic modeling},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Atom search optimization: A comprehensive review of its variants, applications, and future directions. <em>PEERJCS</em>, <em>11</em>, e2722. (<a href='https://doi.org/10.7717/peerj-cs.2722'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Atom Search Optimization (ASO) algorithm is a recent advancement in metaheuristic optimization inspired by principles of molecular dynamics. It mathematically models and simulates the natural behavior of atoms, with interactions governed by forces derived from the Lennard-Jones potential and constraint forces based on bond-length potentials. Since its inception in 2019, it has been successfully applied to various challenges across diverse fields in technology and science. Despite its notable achievements and the rapidly growing body of literature on ASO in the metaheuristic optimization domain, a comprehensive study evaluating the success of its various implementations is still lacking. To address this gap, this article provides a thorough review of half a decade of advancements in ASO research, synthesizing a wide range of studies to highlight key ASO variants, their foundational principles, and significant achievements. It examines diverse applications, including single- and multi-objective optimization problems, and introduces a well-structured taxonomy to guide future exploration in ASO-related research. The reviewed literature reveals that several variants of the ASO algorithm, including modifications, hybridizations, and multi-objective implementations, have been developed to tackle complex optimization problems. Moreover, ASO has been effectively applied across various domains, such as engineering, healthcare and medical applications, Internet of Things and communication, clustering and data mining, environmental modeling, and security, with engineering emerging as the most prevalent application area. By addressing the common challenges researchers face in selecting appropriate algorithms for real-world problems, this study provides valuable insights into the practical applications of ASO and offers guidance for designing ASO variants tailored to specific optimization problems.},
  archive      = {J_PEERJCS},
  author       = {Mohammed A. El-Shorbagy and Anas Bouaouda and Laith Abualigah and Fatma A. Hashim},
  doi          = {10.7717/peerj-cs.2722},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2722},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Atom search optimization: A comprehensive review of its variants, applications, and future directions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Filipino sign language alphabet recognition using persistent homology classification algorithm. <em>PEERJCS</em>, <em>11</em>, e2720. (<a href='https://doi.org/10.7717/peerj-cs.2720'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing number of deaf or hard-of-hearing individuals is a crucial problem since communication among and within the deaf population proves to be a challenge. Despite sign languages developing in various countries, there is still lack of formal implementation of programs supporting its needs, especially for the Filipino sign language (FSL). Recently, studies on FSL recognition explored deep networks. Current findings are promising but drawbacks on using deep networks still prevail. This includes low transparency, interpretability, need for big data, and high computational requirements. Hence, this article explores topological data analysis (TDA), an emerging field of study that harnesses techniques from computational topology, for this task. Specifically, we evaluate a TDA-inspired classifier called Persistent Homology Classification algorithm (PHCA) to classify static alphabet signed using FSL and compare its result with classical classifiers. Experiment is implemented on balanced and imbalanced datasets with multiple trials, and hyperparameters are tuned for a comprehensive comparison. Results show that PHCA and support vector machine (SVM) performed better than the other classifiers, having mean Accuracy of 99.45% and 99.31%, respectively. Further analysis shows that PHCA’s performance is not significantly different from SVM, indicating that PHCA performed at par with the best performing classifier. Misclassification analysis shows that PHCA struggles to classify signs with similar gestures, common to FSL recognition. Regardless, outcomes provide evidence on the robustness and stability of PHCA against perturbations to data and noise. It can be concluded that PHCA can serve as an alternative for FSL recognition, offering opportunities for further research.},
  archive      = {J_PEERJCS},
  author       = {Cristian B. Jetomo and Mark Lexter D. De Lara},
  doi          = {10.7717/peerj-cs.2720},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2720},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Filipino sign language alphabet recognition using persistent homology classification algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing market trend prediction using convolutional neural networks on japanese candlestick patterns. <em>PEERJCS</em>, <em>11</em>, e2719. (<a href='https://doi.org/10.7717/peerj-cs.2719'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study discusses using Japanese candlestick (JC) patterns to predict future price movements in financial markets. The history of candlestick trading dates back to the 17th century and involves the analysis of patterns formed during JC trading. Candlestick patterns are practical tools for the technical analysis of traders in financial markets. They may serve as indicators of traders’ documents of a potential change in market sentiment and trend direction. This study aimed to predict the following candle-trend-based JC charts using convolutional neural networks (CNNs). In order to enhance the accuracy of predicting the directional movement of subsequent financial candlesticks, a rich dataset has been constructed by following a structured three-step process, and a CNN model has been trained. Initially, the dataset was analyzed, and sub-charts were generated using a sliding window technique. Subsequently, the Ta-lib library was used to identify whether predefined patterns were present within the windows. The third phase involved the classification of each window’s directional tendency, which was substantiated by employing various technical indicators to validate the direction of the trend. Following the data preparation and analysis phases, a CNN model was developed to extract features from sub-charts and facilitate precise predictions effectively. The experimental results of this approach demonstrated a remarkable predictive accuracy of up to 99.3%. Implementing cross-validation techniques is essential to verify the reliability and overall performance of the model. To achieve this goal, the dataset was divided into several small subsets. Subsequently, the model was trained and evaluated multiple times using different combinations of these subsets. This method allows for a more accurate assessment of the model’s predictive capabilities by examining its performance on unseen data.},
  archive      = {J_PEERJCS},
  author       = {Edrees Ramadan Mersal and Kürşat Mustafa Karaoğlan and Hakan Kutucu},
  doi          = {10.7717/peerj-cs.2719},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2719},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing market trend prediction using convolutional neural networks on japanese candlestick patterns},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Classification of sleep apnea syndrome using the spectrograms of EEG signals and YOLOv8 deep learning model. <em>PEERJCS</em>, <em>11</em>, e2718. (<a href='https://doi.org/10.7717/peerj-cs.2718'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we focus on classifying sleep apnea syndrome by using the spectrograms obtained from electroencephalogram (EEG) signals taken from polysomnography (PSG) recordings and the You Only Look Once (YOLO) v8 deep learning model. For this aim, the spectrograms of segments obtained from EEG signals with different apnea-hypopnea values (AHI) using a 30-s window function are obtained by short-time Fourier transform (STFT). The spectrograms are used as inputs to the YOLOv8 model to classify sleep apnea syndrome as mild, moderate, severe apnea, and healthy. For four-class classification models, the standard reference level is 25%, assuming equal probabilities for all classes or an equal number of samples in each class. In this context, this information is an important reference point for the validity of our study. Deep learning methods are frequently used for the classification of EEG signals. Although ResNet64 and YOLOv5 give effective results, YOLOv8 stands out with fast processing times and high accuracy. In the existing literature, parameter reduction approaches in four-class EEG classification have not been adequately addressed and there are limitations in this area. This study evaluates the performance of parameter reduction methods in EEG classification using YOLOv8, fills gaps in the existing literature for four-class classification, and reduces the number of parameters of the used models. Studies in the literature have generally classified sleep apnea syndrome as binary (apnea/healthy) and ignored distinctions between apnea severity levels. Furthermore, most of the existing studies have used models with a high number of parameters and have been computationally demanding. In this study, on the other hand, the use of spectrograms is proposed to obtain higher correct classification ratios by using more accurate and faster models. The same classification experiments are reimplemented for widely used ResNet64 and YOLOv5 deep learning models to compare with the success of the proposed model. In the implemented experiments, total correct classification (TCC) ratios are 93.7%, 93%, and 88.2% for YOLOv8, ResNet64, and YOLOv5, respectively. These experiments show that the YOLOv8 model reaches higher success ratios than the ResNet64 and YOLOv5 models. Although the TCC ratios of the YOLOv8 and ResNet64 models are comparable, the YOLOv8 model uses fewer parameters and layers than the others, providing a faster processing time and a higher TCC ratio. The findings of the study make a significant contribution to the current state of the art. As a result, this study gives rise to the idea that the YOLOv8 deep learning model can be used as a new tool for classification of sleep apnea syndrome from EEG signals.},
  archive      = {J_PEERJCS},
  author       = {Kubra Tanci and Mahmut Hekim},
  doi          = {10.7717/peerj-cs.2718},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2718},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Classification of sleep apnea syndrome using the spectrograms of EEG signals and YOLOv8 deep learning model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic periodic event graphs for multivariate time series pattern prediction. <em>PEERJCS</em>, <em>11</em>, e2717. (<a href='https://doi.org/10.7717/peerj-cs.2717'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and predicting outcomes in complex real-world systems necessitates robust multivariate time series pattern analysis. Advanced techniques, such as dynamic graph neural networks, have shown significant efficacy for these tasks. However, existing approaches often overlook the inherent periodicity in data, leading to reduced pattern or event prediction accuracy, especially in periodic time series. We introduce a new method, called dynamic Periodic Event Graphs (PEGs), to tackle this challenge. The proposed method involves time series decomposition to extract seasonal components that capture periodically recurring patterns within the data. It also uses frequency analysis to extract representative periods from each seasonal component. Additionally, motif patterns, which are recurring sub-sequences in the time series data, are extracted. These motifs are used to define event nodes using the representative periods extracted from the seasonal components. By constructing periodic motif pattern-based dynamic bipartite event graphs, we specifically aim to enhance the performance of link prediction tasks, leveraging periodic characteristics in multivariate time series data. Our method has been rigorously tested on multiple periodic multivariate time series datasets, demonstrating over a 5% improvement in link prediction performance for both transductive and inductive scenarios. This demonstrates a substantial enhancement in predictive accuracy and generalization, providing confidence in the technique’s effectiveness. Reproducibility is ensured through publicly available source code, enabling future research and applications.},
  archive      = {J_PEERJCS},
  author       = {SoYoung Park and HyeWon Lee and Sungsu Lim},
  doi          = {10.7717/peerj-cs.2717},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2717},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Dynamic periodic event graphs for multivariate time series pattern prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing fraud detection in the ethereum blockchain using ensemble learning. <em>PEERJCS</em>, <em>11</em>, e2716. (<a href='https://doi.org/10.7717/peerj-cs.2716'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Ethereum blockchain operates as a decentralized platform, utilizing blockchain technology to distribute smart contracts across a global network. It enables currency and digital value exchange without centralized control. However, the exponential growth of online commerce has created a fertile ground for a surge in fraudulent activities such as money laundering and phishing, thereby exacerbating significant security vulnerabilities. To combat this, our article introduces an ensemble learning approach to accurately detect fraudulent Ethereum blockchain transactions. Our goal is to integrate a decision-making tool into the decentralized validation process of Ethereum, allowing blockchain miners to identify and flag fraudulent transactions. Additionally, our system can assist governmental organizations in overseeing the blockchain network and identifying fraudulent activities. Our framework incorporates various data pre-processing techniques and evaluates multiple machine learning algorithms, including logistic regression, Isolation Forest, support vector machine, Random Forest, XGBoost, and recurrent neural network. These models are fine-tuned using grid search to enhance their performance. The proposed approach utilizes an ensemble of three distinct models (Random Forest, extreme gradient boosting (XGBoost), and support vector machine) to further improve classification performance. It achieves high scores of over 98% across key classification metrics like accuracy, precision, recall, and F1-score. Moreover, the approach is suitable for real-world usage, with an inference time of 0.13 s.},
  archive      = {J_PEERJCS},
  author       = {Zhexian Gu and Omar Dib},
  doi          = {10.7717/peerj-cs.2716},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2716},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing fraud detection in the ethereum blockchain using ensemble learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing power allocation for URLLC-D2D in 5G networks with rician fading channel. <em>PEERJCS</em>, <em>11</em>, e2712. (<a href='https://doi.org/10.7717/peerj-cs.2712'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid evolution of wireless technologies within the 5G network brings significant challenges in managing the increased connectivity and traffic of mobile devices. This enhanced connectivity brings challenges for base stations, which must handle increased traffic and efficiently serve a growing number of mobile devices. One of the key solutions to address these challenges is integrating device-to-device (D2D) communication with ultra-reliable and low-latency communication (URLLC). This study examines the impact of the Rician fading channel on the performance of D2D communication under URLLC. It addresses the critical problem of optimizing power allocation to maximize the minimum data rate in D2D communication. A significant challenge arises due to interference issues, as the problem of maximizing the minimum data rate is non-convex, which leads to high computational complexity. This complexity makes it difficult to derive optimal solutions efficiently. To address this challenge, we introduce an algorithm that is based on derivatives to find the optimal power allocation. Comparisons are made with the branch and bound (B&B) algorithm, heuristic algorithm, and particle swarm optimization (PSO) algorithm. Our proposed algorithm improves power allocation performance and also achieves faster execution with lower computational complexity compared to the B&B, PSO, and heuristic algorithms.},
  archive      = {J_PEERJCS},
  author       = {Owais Muhammad and Hong Jiang and Muhammad Bilal and Mushtaq Muhammad Umer},
  doi          = {10.7717/peerj-cs.2712},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2712},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimizing power allocation for URLLC-D2D in 5G networks with rician fading channel},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Non-invasive enhanced hypertension detection through ballistocardiograph signals with mamba model. <em>PEERJCS</em>, <em>11</em>, e2711. (<a href='https://doi.org/10.7717/peerj-cs.2711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores using ballistocardiography (BCG), a non-invasive cardiovascular monitoring technique, combined with advanced machine learning and deep learning models for hypertension detection. The motivation behind this research is to develop a non-invasive and efficient approach for long-term hypertension monitoring, facilitating home-based health assessments. A dataset of 128 BCG recordings has been used, capturing body micro-vibrations from cardiac activity. Various classification models, including Mamba Classifier, Transformer, Stacking, Voting, and XGBoost, were applied to differentiate hypertensive individuals from normotensive ones. In this study, integrating BCG signals with deep learning and machine learning models for hypertension detection is distinguished from previous literature by employing the Mamba deep learning architecture and Transformer-based models. Unlike conventional methods in literature, this study enables more effective analysis of time-series data with the Mamba architecture, capturing long-term signal dependencies and achieving higher accuracy rates. In particular, the combined use of Mamba architecture and the Transformer model’s signal processing capabilities represents a novel approach not previously seen in the literature. While existing studies on BCG signals typically rely on traditional machine learning algorithms, this study aims to achieve higher success rates in hypertension detection by integrating signal processing and deep learning stages. The Mamba Classifier outperformed other models, achieving an accuracy of 95.14% and an AUC of 0.9922 in the 25% hold-out validation. Transformer and Stacking models also demonstrated strong performance, while the Voting and XGBoost models showed comparatively lower results. When combined with artificial intelligence techniques, the findings indicate the potential of BCG signals in providing non-invasive, long-term hypertension detection. The results suggest that the Mamba Classifier is the most effective model for this dataset. This research underscores the potential of BCG technology for continuous home-based health monitoring, providing a feasible alternative to traditional methods. Future research should aim to validate these findings with larger datasets and explore the clinical applications of BCG for cardiovascular disease monitoring.},
  archive      = {J_PEERJCS},
  author       = {Adi Alhudhaif and Kemal Polat},
  doi          = {10.7717/peerj-cs.2711},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2711},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Non-invasive enhanced hypertension detection through ballistocardiograph signals with mamba model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Origin-destination prediction from road average speed data using GraphResLSTM model. <em>PEERJCS</em>, <em>11</em>, e2709. (<a href='https://doi.org/10.7717/peerj-cs.2709'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing demand for traffic management and resource allocation in Intelligent Transportation Systems (ITS), accurate origin-destination (OD) prediction has become crucial. This article presents a novel integrated framework, effectively merging the distinctive capabilities of graph convolutional network (GCN), residual neural network (ResNet), and long short-term memory network (LSTM), hereby designated as GraphResLSTM. GraphResLSTM leverages road average speed data for OD prediction. Contrary to traditional reliance on traffic flow data, road average speed data provides richer informational dimensions, reflecting not only vehicle volume but also indirectly indicating congestion levels. We use a real-world road network to generate road average speed data and OD data through simulations in Simulation of Urban Mobility (SUMO), thereby avoiding the influence of external factors such as weather. To enhance training efficiency, we employ a method combining the entropy weight method with the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) for key road segment selection. Using this generated dataset, carefully designed comparative experiments are conducted to compare various different models and data types. The results clearly demonstrate that both the GraphResLSTM model and the road average speed data markedly outperform alternative models and data types in OD prediction.},
  archive      = {J_PEERJCS},
  author       = {Guangtong Hu and Jun Zhang},
  doi          = {10.7717/peerj-cs.2709},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2709},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Origin-destination prediction from road average speed data using GraphResLSTM model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GATI-RS model using bi-LSTM and multi-head attention mechanism to enhance online shopping experience for the elderly with accurate click-through rate prediction. <em>PEERJCS</em>, <em>11</em>, e2707. (<a href='https://doi.org/10.7717/peerj-cs.2707'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of e-commerce and the increasing aging population, more elderly people are engaging in online shopping. However, challenges they face during this process are becoming more apparent. This article proposes a recommendation system based on click-through rate (CTR) prediction, aiming to enhance the online shopping experience for elderly users. By analyzing user characteristics, product features, and their interactions, we constructed a model combining bidirectional long short-term memory (Bi-LSTM) and multi-head self-attention mechanism to predict the item click behavior of elderly users in the recommendation section. Experimental results demonstrated that the model excels in CTR prediction, effectively improving the relevance of recommended content. Compared to the baseline model long short-term memory (LSTM), the GATI-RS framework improved CTR prediction accuracy by 40%, and its loss function rapidly decreased and remained stable during training. Additionally, the GATI-RS framework showed significant performance improvement when considering only elderly users, with accuracy surpassing the baseline model by 42%. These results indicate that the GATI-RS framework, through optimized algorithms, significantly enhances the model’s global information integration and complex pattern recognition capabilities, providing strong support for developing recommendation systems for elderly online shoppers. This research not only offers new insights for e-commerce platforms to optimize services but also contributes to improving the quality of life and well-being of the elderly.},
  archive      = {J_PEERJCS},
  author       = {Ying Liu and Shahriman Zainal Abidin and Verly Veto Vermol and Shaolong Yang and Hanyu Liu},
  doi          = {10.7717/peerj-cs.2707},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2707},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GATI-RS model using bi-LSTM and multi-head attention mechanism to enhance online shopping experience for the elderly with accurate click-through rate prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Collaborative filtering based on GNN with attribute fusion and broad attention. <em>PEERJCS</em>, <em>11</em>, e2706. (<a href='https://doi.org/10.7717/peerj-cs.2706'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recommender systems based on collaborative filtering (CF) have been a prominent area of research. In recent years, graph neural networks (GNN) based CF models have effectively addressed the limitations of nonlinearity and higher-order feature interactions in traditional recommendation methods, such as matrix decomposition-based methods and factorization machine approaches, achieving excellent recommendation performance. However, existing GNN-based CF models still have two problems that affect performance improvement. First, although distinguishing between inner interaction and cross interaction, these models still aggregate all attributes indiscriminately. Second, the models do not exploit higher-order interaction information. To address the problems above, this article proposes a collaborative filtering method based on GNN with attribute fusion and broad attention, named GNN-A2, which incorporates an inner interaction module with self-attention, a cross interaction module with attribute fusion, and a broad attentive cross module. In summary, GNN-A2 model performs inner interactions and cross interactions in different ways, then extracts their higher-order interaction information for prediction. We conduct extensive experiments on three benchmark datasets, i.e., MovieLens 1M, Book-crossing, and Taobao. The experimental results demonstrate that our proposed GNN-A2 model achieves comparable performance on area under the curve (AUC) metric. Notably, GNN-A2 achieves the optimal performance on Normalized Discounted Cumulative Gain at rank 10 (NDCG@10) over three datasets, with values of 0.9506, 0.9137, and 0.1526, corresponding to respective improvements of 0.68%, 1.57%, and 2.14% compared to the state-of-the-art (SOTA) models. The source code and evaluation datasets are available at: https://github.com/LMXue7/GNN-A2.},
  archive      = {J_PEERJCS},
  author       = {MingXue Liu and Min Wang and Baolei Li and Qi Zhong},
  doi          = {10.7717/peerj-cs.2706},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2706},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Collaborative filtering based on GNN with attribute fusion and broad attention},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine learning-based feature selection and classification for cerebral infarction screening: An experimental study. <em>PEERJCS</em>, <em>11</em>, e2704. (<a href='https://doi.org/10.7717/peerj-cs.2704'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cerebral infarction screening (CIS) is critical for timely intervention and improved patient outcomes. We investigate the application of machine learning techniques for feature selection and classification of speech and cognitive function assessments to enhance cerebral infarction screening. We analyze a dataset containing 117 patients (95 patients were diagnosed with cerebral infarction, and 54 were identified as lacunar cerebral infarction of them) comprising speech and cognitive function features from patients with lacunar and non-lacunar cerebral infarction, as well as healthy controls. In this article, we present a framework called CIS which comprises a cerebral infarction screening model to identify cerebral infarction from populations and a diagnostic model to classify lacunar infarction, non-lacunar infarction, and healthy controls. Feature selection method, Recursive Feature Elimination with Cross-Validation (RFECV), is employed to identify the most relevant features. Various classifiers, such as support vector machine, K-nearest neighbor, decision tree, random forest, logistic regression, and eXtreme gradient boosting (XGBoost), were evaluated for their performance in binary and ternary classification tasks. The CIS based on XGBoost classifier achieved the highest accuracy of 88.89% in the binary classification task (i.e., distinguishing cerebral infarction from healthy controls) and 77.78% in the ternary classification task (i.e., distinguishing lacunar infarction, non-lacunar infarction, and healthy controls). The selected features significantly contributed to the classification performance, highlighting their potential in differentiating cerebral infarction subtypes. We develop a comprehensive system to effectively assess cerebral infarction subtypes. This study demonstrates the efficacy of machine learning methods in cerebral infarction screening through the analysis of speech and cognitive function features. These findings suggest that incorporating these techniques into clinical practice could improve early detection and diagnosis of cerebral infarction. Further research with larger and more diverse datasets is warranted to validate and extend these results.},
  archive      = {J_PEERJCS},
  author       = {Yang Niu and Xue Tao and Qinyuan Chang and Mingming Hu and Xin Li and Xiaoping Gao},
  doi          = {10.7717/peerj-cs.2704},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2704},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Machine learning-based feature selection and classification for cerebral infarction screening: An experimental study},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive machine learning approaches utilizing soft decision-making via intuitionistic fuzzy parameterized intuitionistic fuzzy soft matrices. <em>PEERJCS</em>, <em>11</em>, e2703. (<a href='https://doi.org/10.7717/peerj-cs.2703'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential data growth generated by technological advancements presents significant challenges in analysis and decision-making, necessitating innovative and robust methodologies. Machine learning has emerged as a transformative tool to address these challenges, especially in scenarios requiring precision and adaptability. This study introduces two novel adaptive machine learning approaches, i.e., AIFPIFSC1 and AIFPIFSC2. These methods leverage the modeling ability of intuitionistic fuzzy parameterized intuitionistic fuzzy soft matrices (ifpifs-matrices). This state-of-the-art framework enhances the classification task in machine learning by employing soft decision-making through ifpifs-matrices. The proposed approaches are rigorously evaluated against leading fuzzy/soft-based classifiers using 15 widely recognized University of California, Irvine datasets, including accuracy and robustness, across six performance metrics. Statistical analyses conducted using Friedman and Nemenyi tests further substantiate the reliability and superiority of the proposed approaches. The results consistently demonstrate that these approaches outperform their counterparts, highlighting their potential for solving complex classification problems. This study contributes to the field by offering adaptable and effective solutions for modern data analysis challenges, paving the way for future advancements in machine learning and decision-making systems.},
  archive      = {J_PEERJCS},
  author       = {Samet Memiş and Ferhan Şola Erduran and Hivda Aydoğan},
  doi          = {10.7717/peerj-cs.2703},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2703},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Adaptive machine learning approaches utilizing soft decision-making via intuitionistic fuzzy parameterized intuitionistic fuzzy soft matrices},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain and explainable-AI integrated system for polycystic ovary syndrome (PCOS) detection. <em>PEERJCS</em>, <em>11</em>, e2702. (<a href='https://doi.org/10.7717/peerj-cs.2702'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the modern era of digitalization, integration with blockchain and machine learning (ML) technologies is most important for improving applications in healthcare management and secure prediction analysis of health data. This research aims to develop a novel methodology for securely storing patient medical data and analyzing it for PCOS prediction. The main goals are to leverage Hyperledger Fabric for immutable, private data and to integrate Explainable Artificial Intelligence (XAI) techniques to enhance transparency in decision-making. The innovation of this study is the unique integration of blockchain technology with ML and XAI, solving critical issues of data security and model interpretability in healthcare. With the Caliper tool, the Hyperledger Fabric blockchain’s performance is evaluated and enhanced. The suggested Explainable AI-based blockchain system for Polycystic Ovary Syndrome detection (EAIBS-PCOS) system demonstrates outstanding performance and records 98% accuracy, 100% precision, 98.04% recall, and a resultant F1-score of 99.01%. Such quantitative measures ensure the success of the proposed methodology in delivering dependable and intelligible predictions for PCOS diagnosis, therefore making a great addition to the literature while serving as a solid solution for healthcare applications in the near future.},
  archive      = {J_PEERJCS},
  author       = {Gowthami Jaganathan and Shanthi Natesan},
  doi          = {10.7717/peerj-cs.2702},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2702},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Blockchain and explainable-AI integrated system for polycystic ovary syndrome (PCOS) detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An optimal peer selection for peer-to-peer video content distribution using fuzzy linear programming approach. <em>PEERJCS</em>, <em>11</em>, e2701. (<a href='https://doi.org/10.7717/peerj-cs.2701'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of peer selection in peer-to-peer (P2P) video content distribution network is significant to solve since it affects the performance and efficiency of the network widely. In this article, a novel framework is introduced that uses fuzzy linear programming (FLP) to address the inherent uncertainties in peer selection. The primary motivation for the use of FLP lies in its capability to handle the imprecision and vagueness that are characteristic of dynamic P2P environments. Factors such as peer reliability, bandwidth, and proximity are often uncertain in this environment. By using fuzzy logic, the proposed framework models these criteria as fuzzy sets and then integrates uncertainty into the decision-making process. FLP is then applied to optimize peer selection, improving download speed, reducing download time, and enhancing peer reliability. The proposed method is evaluated and analyzed using extensive simulation with SciPy. The result reveals that proposed technique works better compared to some of the traditional methods in terms of download time, download speed and also reliability measure. It also exhibits approximately 20% of increase in download speed as well as a 15% decrease in download time compared to traditional approaches. It leads to faster content retrieval and enhanced the efficiency in content distribution. Also, in selection of reliable peers for content distribution, there is a notable 20% of increase in peer reliability with result of enhanced robustness. The proposed method provides efficient and robust solution to the problem of peer selection. It can be implemented in a broad range of P2P content distribution networks.},
  archive      = {J_PEERJCS},
  author       = {M. Anandaraj and Naganandhini S.},
  doi          = {10.7717/peerj-cs.2701},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2701},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An optimal peer selection for peer-to-peer video content distribution using fuzzy linear programming approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lung image segmentation with improved U-net, V-net and seg-net techniques. <em>PEERJCS</em>, <em>11</em>, e2700. (<a href='https://doi.org/10.7717/peerj-cs.2700'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tuberculosis remains a significant health challenge worldwide, affecting a large population. Therefore, accurate diagnosis of this disease is a critical issue. With advancements in computer systems, imaging devices, and rapid progress in machine learning, tuberculosis diagnosis is being increasingly performed through image analysis. This study proposes three segmentation models based on U-Net, V-Net, and Seg-Net architectures to improve tuberculosis detection using the Shenzhen and Montgomery databases. These deep learning-based methods aim to enhance segmentation accuracy by employing advanced preprocessing techniques, attention mechanisms, and non-local blocks. Experimental results indicate that the proposed models outperform traditional approaches, particularly in terms of the Dice coefficient and accuracy values. The models have demonstrated robust performance on popular datasets. As a result, they contribute to more precise and reliable lung region segmentation, which is crucial for the accurate diagnosis of respiratory diseases like tuberculosis. In evaluations using various performance metrics, the proposed U-Net and V-Net models achieved Dice coefficient scores of 96.43% and 96.42%, respectively, proving their competitiveness and effectiveness in medical image analysis. These findings demonstrate that the Dice coefficient values of the proposed U-Net and V-Net models are more effective in tuberculosis segmentation than Seg-Net and other traditional methods.},
  archive      = {J_PEERJCS},
  author       = {Fuat Turk and Mahmut Kılıçaslan},
  doi          = {10.7717/peerj-cs.2700},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2700},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Lung image segmentation with improved U-net, V-net and seg-net techniques},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized hybrid SVM-RF multi-biometric framework for enhanced authentication using fingerprint, iris, and face recognition. <em>PEERJCS</em>, <em>11</em>, e2699. (<a href='https://doi.org/10.7717/peerj-cs.2699'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a hybrid multi-biometric system incorporating fingerprint, face, and iris recognition to enhance individual authentication. The system addresses limitations of uni-modal approaches by combining multiple biometric modalities, exhibiting superior performance and heightened security in practical scenarios, making it more dependable and resilient for real-world applications. The integration of support vector machine (SVM) and random forest (RF) classifiers, along with optimization techniques like bacterial foraging optimization (BFO) and genetic algorithms (GA), improves efficiency and robustness. Additionally, integrating feature-level fusion and utilizing methods such as Gabor filters for feature extraction enhances overall performance of the model. The system demonstrates superior accuracy and reliability, making it suitable for real-world applications requiring secure and dependable identification solutions.},
  archive      = {J_PEERJCS},
  author       = {Sonal and Ajit Singh and Chander Kant},
  doi          = {10.7717/peerj-cs.2699},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2699},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Optimized hybrid SVM-RF multi-biometric framework for enhanced authentication using fingerprint, iris, and face recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing the prediction of vitamin d deficiency levels using an integrated approach of deep learning and evolutionary computing. <em>PEERJCS</em>, <em>11</em>, e2698. (<a href='https://doi.org/10.7717/peerj-cs.2698'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vitamin D deficiency (VDD) has emerged as a serious global health concern that can lead to far-reaching consequences, including skeletal issues and long-term illness. Classical diagnostic approaches, although effective, often include invasive techniques and lacks to leverage the massive amount of healthcare data. There is an increasing demand for noninvasive prediction approaches for determining the severity of VDD. This work proposes a novel approach to detect VDD levels by combining deep learning techniques with evolutionary computing (EC). Specifically, we employ a hybrid deep learning model that includes convolutional neural networks (CNN) and bidirectional long short-term memory (BiLSTM) networks to predict VDD data effectively. To improve the models effectiveness and guarantee the optimal choice of the features and hyper-parameters, we incorporate evolutionary computing methods, particularly genetic algorithms (GA). The proposed method has been proven effective through a comprehensive assessment on a benchmark dataset, with 97% accuracy, 96% precision, 97% recall, and 96% F1-score. Our approach yielded improved performance, when compared to earlier methods. This research not only push forward predictive healthcare models but also shows the potential of merging deep learning with evolutionary computing to address intricate health-care issues.},
  archive      = {J_PEERJCS},
  author       = {Ahmed Alzahrani and Muhammad Zubair Asghar},
  doi          = {10.7717/peerj-cs.2698},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2698},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing the prediction of vitamin d deficiency levels using an integrated approach of deep learning and evolutionary computing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BiLSTM-enhanced legal text extraction model using fuzzy logic and metaphor recognition. <em>PEERJCS</em>, <em>11</em>, e2697. (<a href='https://doi.org/10.7717/peerj-cs.2697'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The burgeoning field of natural language processing (NLP) has witnessed exponential growth, captivating researchers due to its diverse practical applications across industries. However, the intricate nature of legal texts poses unique challenges for conventional text extraction methods. To surmount these challenges, this article introduces a pioneering legal text extraction model rooted in fuzzy language processing and metaphor recognition, tailored for the domain of online environment governance. Central to this model is the utilization of a bidirectional long short-term memory (Bi-LSTM) network, adept at delineating illicit behaviors by establishing connections between legal provisions and judgments. Additionally, a self-attention module is integrated into the Bi-LSTM architecture, augmented by L2 regularization, to facilitate the efficient extraction of legal text information, thereby enabling the identification and classification of illegal content. This innovative approach effectively resolves the issue of legal text recognition. Experimental findings underscore the efficacy of the proposed method, achieving an impressive macro-F1 score of 0.8005, precision of 0.8047, and recall of 0.8014. Furthermore, the article delves into an analysis and discussion of the potential application prospects of the legal text extraction model, grounded in fuzzy language processing and metaphor recognition, within the realm of online environment governance.},
  archive      = {J_PEERJCS},
  author       = {Jia Chen},
  doi          = {10.7717/peerj-cs.2697},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2697},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {BiLSTM-enhanced legal text extraction model using fuzzy logic and metaphor recognition},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Research on path planning of mobile robots based on improved a* algorithm. <em>PEERJCS</em>, <em>11</em>, e2691. (<a href='https://doi.org/10.7717/peerj-cs.2691'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the issues of low search efficiency, excessive node expansion, and the presence of redundant nodes in the traditional A* algorithm, this article proposes an improved A* algorithm for mobile robot path planning. Firstly, a multi-neighborhood hybrid search method is introduced, optimizing the traditional eight-neighborhood and twenty-four-neighborhood into a new sixteen-neighborhood. The choice between eight-neighborhood search and sixteen-neighborhood search is determined based on the presence of obstacles in the eight-neighborhood around the current node, effectively enhancing the search efficiency of the algorithm and reducing the number of nodes expanded during the search process. Subsequently, unnecessary nodes are eliminated based on the positional relationship between the current node and the target node, according to neighborhood direction search rules, further decreasing the number of expanded nodes. Additionally, improvements to the bidirectional search mechanism along with the incorporation of dynamic weight coefficients further enhance the search efficiency of the algorithm. Furthermore, a strategy for extracting key nodes is employed to effectively remove useless turn points, thus resolving the issue of redundant nodes. Finally, simulation experiments demonstrate that the proposed improved A* algorithm outperforms the traditional A* algorithm in terms of search speed, number of expanded nodes, and path length, validating the effectiveness of the proposed method.},
  archive      = {J_PEERJCS},
  author       = {Xing Fu and Zucheng Huang and Gongxue Zhang and Weijun Wang and Jian Wang},
  doi          = {10.7717/peerj-cs.2691},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2691},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Research on path planning of mobile robots based on improved a* algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stock market trading via actor-critic reinforcement learning and adaptable data structure. <em>PEERJCS</em>, <em>11</em>, e2690. (<a href='https://doi.org/10.7717/peerj-cs.2690'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, the stock market is attractive, and it is challenging to develop an efficient investment model with high accuracy due to changes in the values of the shares for political, economic, and social reasons. This article presents an innovative proposal for a short-term, automatic investment model to reduce capital loss during trading, applying a reinforcement learning (RL) model. On the other hand, we propose an adaptable data window structure to enhance the learning and accuracy of investment agents in three foreign exchange markets: crude oil, gold, and the Euro. In addition, the RL model employs an actor-critic neural network with rectified linear unit (ReLU) neurons to generate specialized investment agents, enabling more efficient trading, minimizing investment losses across different time periods, and reducing the model’s learning time. The proposed RL model obtained a reduction average loss of 0.03% in Euro, 0.25% in gold, and 0.13% in crude oil in the test phase with varying initial conditions.},
  archive      = {J_PEERJCS},
  author       = {Cesar Guevara},
  doi          = {10.7717/peerj-cs.2690},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2690},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Stock market trading via actor-critic reinforcement learning and adaptable data structure},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-task snake optimization algorithm for global optimization and planar kinematic arm control problem. <em>PEERJCS</em>, <em>11</em>, e2688. (<a href='https://doi.org/10.7717/peerj-cs.2688'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task optimization (MTO) algorithms aim to simultaneously solve multiple optimization tasks. Addressing issues such as limited optimization precision and high computational costs in existing MTO algorithms, this article proposes a multi-task snake optimization (MTSO) algorithm. The MTSO algorithm operates in two phases: first, independently handling each optimization problem; second, transferring knowledge. Knowledge transfer is determined by the probability of knowledge transfer and the selection probability of elite individuals. Based on this decision, the algorithm either transfers elite knowledge from other tasks or updates the current task through self-perturbation. Experimental results indicate that, compared to other advanced MTO algorithms, the proposed algorithm achieves the most accurate solutions on multitask benchmark functions, the five-task and 10-task planar kinematic arm control problems, the multitask robot gripper problem, and the multitask car side-impact design problem. The code and data for this article can be obtained from: https://doi.org/10.5281/zenodo.14197420.},
  archive      = {J_PEERJCS},
  author       = {Qingrui Li and Yongquan Zhou and Qifang Luo},
  doi          = {10.7717/peerj-cs.2688},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2688},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Multi-task snake optimization algorithm for global optimization and planar kinematic arm control problem},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Performance and simulation analysis of 802.11ax OFDMA in contention-driven scenarios. <em>PEERJCS</em>, <em>11</em>, e2687. (<a href='https://doi.org/10.7717/peerj-cs.2687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 802.11ax standard introduces Orthogonal Frequency Division Multiple Access (OFDMA), shifting the role of access points (APs) in Wi-Fi networks. This shift integrates intricate scheduling logic, assigning coordinator roles to APs for multi-user uplink (MU-UL) transmissions and streamlining downlink traffic flows. These developments require robust network analysis and simulation tools to investigate the trade-offs associated with using OFDMA. In this study, we validate the implementation of OFDMA in ns-3 Wi-Fi module, enhancing flexibility and support for future updates through a redesign process. Previous studies validate the OFDMA implementation in the ns-3 Wi-Fi module by matching the simulation to predictions of analytical models. In this work, we demonstrate that OFDMA performance aligns with analytical predictions through simulation-based performance evaluations using ns-3 in some contention-driven use cases. The proposed system operates in both the uplink (UL) and downlink (DL) directions, implementing two scheduling logics to manage DL traffic flows and coordinate MU-UL transmissions. Simulation time is reduced by introducing parallel computing in the system. This study provides a reliable network analysis and simulation framework that thoroughly examines the trade-offs involved in using OFDMA.},
  archive      = {J_PEERJCS},
  author       = {Memoona and Sung Won Kim},
  doi          = {10.7717/peerj-cs.2687},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2687},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Performance and simulation analysis of 802.11ax OFDMA in contention-driven scenarios},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-scale CNN with atrous spatial pyramid pooling for enhanced chest-based disease detection. <em>PEERJCS</em>, <em>11</em>, e2686. (<a href='https://doi.org/10.7717/peerj-cs.2686'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a sophisticated deep-learning model designed for the early detection of COVID-19 and pneumonia. The model employs a convolutional neural network-integrated with atrous spatial pyramid pooling. The atrous spatial pyramid pooling mechanism enhances the convolutional neural network model’s ability to capture fine and large-scale features, optimizing detection accuracy in chest X-ray images. This improvement, along with transfer learning, significantly enhances the overall performance. By utilizing data augmentation to address the scarcity of available X-ray images, our atrous spatial pyramid pooling-enhanced convolutional neural network achieved a validation accuracy of 98.66% for COVID-19 and 83.75% for pneumonia, which beats the validation results of the other state of the art approaches (the metrics used for evaluation were accuracy, precision, F1-score, recall, specificity, and area under the curve). The model’s multi-branch architecture facilitates more accurate and adaptable disease prediction, thereby increasing diagnostic precision and robustness. This approach offers the potential for faster and more reliable diagnoses of chest-related conditions.},
  archive      = {J_PEERJCS},
  author       = {Muhammad Abdullah Shah Bukhari and Faisal Bukhari and Muhammad Asif and Hanan Aljuaid and Waheed Iqbal},
  doi          = {10.7717/peerj-cs.2686},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2686},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A multi-scale CNN with atrous spatial pyramid pooling for enhanced chest-based disease detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An online intelligent detection method for slurry density in concept drift data streams based on collaborative computing. <em>PEERJCS</em>, <em>11</em>, e2683. (<a href='https://doi.org/10.7717/peerj-cs.2683'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In industrial environments, slurry density detection models often suffer from performance degradation due to concept drift. To address this, this article proposes an intelligent detection method tailored for slurry density in concept drift data streams. The method begins by building a model using Gaussian process regression (GPR) combined with regularized stochastic configuration. A sliding window-based online GPR is then applied to update the linear model’s parameters, while a forgetting mechanism enables online recursive updates for the nonlinear model. Network pruning and stochastic configuration techniques dynamically adjust the nonlinear model’s structure. These approaches enhance the mechanistic model’s ability to capture dynamic relationships and reduce the data-driven model’s reliance on outdated data. By focusing on recent data to reflect current operating conditions, the method effectively mitigates concept drift in complex process data. Additionally, the method is applied in industrial settings through collaborative computing, ensuring real-time slurry density detection and model adaptability. Experimental results on industrial data show that the proposed method outperforms other algorithms in all density estimation metrics, significantly improving slurry density detection accuracy.},
  archive      = {J_PEERJCS},
  author       = {Lanhao Wang and Hao Wang and Taojie Wei and Wei Dai and Hongyan Wang},
  doi          = {10.7717/peerj-cs.2683},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2683},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An online intelligent detection method for slurry density in concept drift data streams based on collaborative computing},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explainable multi-objective hybrid machine learning model for reducing heart failure mortality. <em>PEERJCS</em>, <em>11</em>, e2682. (<a href='https://doi.org/10.7717/peerj-cs.2682'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the world grapples with pandemics and increasing stress levels among individuals, heart failure (HF) has emerged as a prominent cause of mortality on a global scale. The most effective approach to improving the chances of individuals’ survival is to diagnose this condition at an early stage. Researchers widely utilize supervised feature selection techniques alongside conventional standalone machine learning (ML) algorithms to achieve the goal. However, these approaches may not consistently demonstrate robust performance when applied to data that they have not encountered before, and struggle to discern intricate patterns within the data. Hence, we present a Multi-objective Stacked Enable Hybrid Model (MO-SEHM), that aims to find out the best feature subsets out of numerous different sets, considering multiple objectives. The Stacked Enable Hybrid Model (SEHM) plays the role of classifier and integrates with a multi-objective feature selection method, the Non-dominated Sorting Genetic Algorithm II (NSGA-II). We employed an HF dataset from the Faisalabad Institute of Cardiology (FIOC) and evaluated six ML models, including SEHM with and without NSGA-II for experimental purposes. The Pareto front (PF) demonstrates that our introduced MO-SEHM surpasses the other models, obtaining 94.87% accuracy with the nine relevant features. Finally, we have applied Local Interpretable Model-agnostic Explanations (LIME) with MO-SEHM to explain the reasons for individual outcomes, which makes our model transparent to the patients and stakeholders.},
  archive      = {J_PEERJCS},
  author       = {F M Javed Mehedi Shamrat and Majdi Khalid and Thamir M. Qadah and Majed Farrash and Hanan Alshanbari},
  doi          = {10.7717/peerj-cs.2682},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2682},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An explainable multi-objective hybrid machine learning model for reducing heart failure mortality},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A user-embedded temporal attention neural network for IoT trajectories prediction. <em>PEERJCS</em>, <em>11</em>, e2681. (<a href='https://doi.org/10.7717/peerj-cs.2681'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past two decades, sequential recommendation systems have garnered significant research interest, driven by their potential applications in personalized product recommendations. In this article, we seek to explicitly model an algorithm based on Internet of Things (IoT) data to predict the next cell reached by the user equipment (UE). This algorithm exploits UE embedding and cell embedding combining the visit time interval information, and uses sliding window sampling to process more UE trajectory data. Furthermore, we use the attention mechanism, removed the query matrix operation and the attention mask, to obtain key information in data and reduce the number of parameters to speed up training. In the prediction layer, combining the positive and negative sampling and computing cross entropy loss also provides assistance to increase the precision and dependability of the entire model. We take the six adjacent cells of the current cell as candidates due to the limitation of the space problem, from which we predict the next destination cell of track movement. Extensive empirical study shows the recall of our algorithm reaches 0.5766, which infers the optimal result and high performance of our model.},
  archive      = {J_PEERJCS},
  author       = {Dongdong Feng and Siyao Li and Yong Xiang and Jiahuan Zheng},
  doi          = {10.7717/peerj-cs.2681},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2681},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A user-embedded temporal attention neural network for IoT trajectories prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based novel ensemble method with best score transferred-adaptive neuro fuzzy inference system for energy consumption prediction. <em>PEERJCS</em>, <em>11</em>, e2680. (<a href='https://doi.org/10.7717/peerj-cs.2680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Energy consumption predictions for smart homes and cities benefit many from homeowners to energy suppliers, allowing homeowners to understand and manage their future energy consumption, improve energy efficiency, and reduce energy costs. Predictions can help energy suppliers effectively distribute energy on demand. Therefore, from the past to the present, numerous methods have been conducted using collected data, employing both statistical and artificial intelligence (AI)-based approaches, to achieve successful energy consumption predictions. Methods This study proposes a deep learning-based novel ensemble (DLBNE) method with the best score transferred-adaptive neuro fuzzy inference system (BST-ANFIS) as a high-performance and robust approach for energy consumption prediction. The proposed method uses deep learning (DL)-based algorithms, including convolutional neural networks (CNN), recurrent neural networks (RNN), long short-term memory (LSTM), bidirectional long short-term memory (BI-LSTM), and gated recurrent units (GRUs) as base predictors. The BST-ANFIS architecture combines the individual outcomes of these predictors. In order to build a robust and dynamic prediction model, the interaction between the base predictors and the ANFIS architecture is achieved using a best score transfer approach. The performance of the proposed method in energy consumption prediction was verified through five DL methods, five machine learning (ML) methods, and a DL-based weighted average (DLBWA) ensemble method. Results In experimental studies, the results were obtained from three-stage analyses: fold, average, and periodic performance analyses. In fold analyses, the proposed method, in terms of the root mean square error (RMSE) metric, demonstrated better performance in four folds on the Internet of Things (IoT)-based smart home (IBSH) dataset, two in the homestead city electricity consumption (HCEC) dataset, and two in the individual household power consumption (IHPC) dataset compared to the other methods. In the average performance analyses, it showed significantly higher performance than the other methods in all metrics for the IBSH and IHPC datasets, and in metrics except the mean absolute error (MAE) metric for the HCEC dataset. The performance results in terms of RMSE, MAE, mean square error (MSE), and mean absolute percentage error (MAPE) metrics from these analyses were obtained as 0.001531, 0.001010, 0.0000031, and 0.001573 for the IBSH dataset; 0.025208, 0.005889, 0.001884, and 0.000137 for the HCEC dataset; and 0.013640, 0.006572, 0.000356, and 0.000943 for the IHPC dataset, respectively. The results of the 120-h periodic analyses also showed that the proposed method yielded a better prediction result than the other methods. Furthermore, a comparison of the proposed method with similar studies in the literature revealed that it demonstrated competitive performance in relation to the methods employed in those studies.},
  archive      = {J_PEERJCS},
  author       = {Birce Dağkurs and İsmail Atacak},
  doi          = {10.7717/peerj-cs.2680},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2680},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning-based novel ensemble method with best score transferred-adaptive neuro fuzzy inference system for energy consumption prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient pooling distillation network for lightweight single image super-resolution reconstruction. <em>PEERJCS</em>, <em>11</em>, e2679. (<a href='https://doi.org/10.7717/peerj-cs.2679'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The single image super-resolution (SISR) is a classical problem in the field of computer vision, aiming to enhance high-resolution details from low-resolution images. In recent years, significant progress about SISR has been achieved through the utilization of deep learning technology. However, these deep methods often exhibit large-scale networks architectures, which are computationally intensive and hardware-demanding, and this limits their practical application in some scenarios (e.g., autonomous driving, streaming media) requiring stable and efficient image transmission with high-definition picture quality. In such application settings, computing resources are often restricted. Thus, there is a pressing demand to devise efficient super-resolution algorithms. To address this issue, we propose a gradient pooling distillation network (GPDN), which can enable the efficient construction of a single image super-resolution system. In the GPDN we leverage multi-level stacked feature distillation hybrid units to capture multi-scale feature representations, which are subsequently synthesized for dynamic feature space optimization. The central to the GPDN is the Gradient Pooling Distillation module, which operates through hierarchical pooling to decompose and refine critical features across various dimensions. Furthermore, we introduce the Feature Channel Attention module to accurately filter and strengthen pixel features crucial for recovering high-resolution images. Extensive experimental results demonstrate that our proposed method achieves competitive performance while maintaining relatively low resource occupancy of the model. This model strikes for a balance between excellent performance and resource utilization—particularly when trading off high recovery quality with small memory occupancy.},
  archive      = {J_PEERJCS},
  author       = {Zhiyong Hong and GuanJie Liang and Liping Xiong},
  doi          = {10.7717/peerj-cs.2679},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2679},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Gradient pooling distillation network for lightweight single image super-resolution reconstruction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). U-TSS: A novel time series segmentation model based U-net applied to automatic detection of interference events in geomagnetic field data. <em>PEERJCS</em>, <em>11</em>, e2678. (<a href='https://doi.org/10.7717/peerj-cs.2678'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid advancement of Internet of Things (IoT) technology, the volume of sensor data collection has increased significantly. These data are typically presented in the form of time series, gradually becoming a crucial component of big data. Traditional time series analysis methods struggle with complex patterns and long-term dependencies, whereas deep learning technologies offer new solutions. This study introduces the U-TSS, a U-net-based sequence-to-sequence fully convolutional network, specifically designed for one-dimensional time series segmentation tasks. U-TSS maps input sequences of arbitrary length to corresponding sequences of class labels across different temporal scales. This is achieved by implicitly classifying each individual time point in the input time series and then aggregating these classifications over varying intervals to form the final prediction. This enables precise segmentation at each time step, ensuring both global sequence awareness and accurate classification of complex time series data. We applied U-TSS to geomagnetic field observation data for the detection of high-voltage direct current (HVDC) interference events. In experiments, U-TSS achieved superior performance in detecting HVDC interference events, with accuracies of 99.42%, 94.61%, and 95.54% on the training, validation, and test sets, respectively, outperforming state-of-the-art models in accuracy, precision, recall, F1-score, and AUC. Our code can be accessed openly in the GitHub repository at https://github.com/wangmengyu1/U-TSS.},
  archive      = {J_PEERJCS},
  author       = {Weifeng Shan and Mengyu Wang and Jinzhu Xia and Jun Chen and Qi Li and Lili Xing and Ruilei Zhang and Maofa Wang and Suqin Zhang and Xiuxia Zhang},
  doi          = {10.7717/peerj-cs.2678},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2678},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {U-TSS: A novel time series segmentation model based U-net applied to automatic detection of interference events in geomagnetic field data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A lightweight coal-gangue detection model based on parallel deep residual networks. <em>PEERJCS</em>, <em>11</em>, e2677. (<a href='https://doi.org/10.7717/peerj-cs.2677'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To realize the accurate identification of coal-gangue in the process of underground coal transportation and the low-cost deployment of the model, a lightweight coal-gangue detection model based on the parallel depth residual network, called P-RNet, is proposed. For the problem of images of coal-gangue taken under complex conditions, the feature extraction module (FEM) is designed using decoupling training and inference methods. Furthermore, for the problem of the nearest neighbor interpolation upsampling method being prone to produce mosaic blocks and edge jagged edges, a lightweight upsampling operator is used to optimize the feature fusion module (FFM). Finally, to solve the problem, the stochastic gradient descent algorithm is prone to local suboptimal solutions and saddle point problems in the error function optimization process, numerous experiments are carried out on selecting the initial learning rate, and the Lookahead optimizer is used to optimize parameters during backpropagation. Experimental results show that the proposed model can effectively improve the recognition effect, with a corresponding low deployment cost.},
  archive      = {J_PEERJCS},
  author       = {Shexiang Jiang and Xinrui Zhou},
  doi          = {10.7717/peerj-cs.2677},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2677},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A lightweight coal-gangue detection model based on parallel deep residual networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Construction of a user-friendly software-defined networking management using a graph-based abstraction layer. <em>PEERJCS</em>, <em>11</em>, e2674. (<a href='https://doi.org/10.7717/peerj-cs.2674'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The software-defined networking (SDN) paradigm relies on the decoupling of the control plane and data plane. Northbound interfaces enable the implementation of network services through logical centralised control. Suitable northbound interfaces and application-oriented abstractions are the core of the SDN ecosystem. This article presents an architecture to represent the network as a graph. The purpose of this architecture is to implement an abstraction of the SDN controller at the application plane. We abstract all network elements using a graph model, with the attributes of the elements as the attributes of the graph. This virtualized logical abstraction layer, which is not limited by the physical network, enables network administrators to schedule network resources directly in a global view. The feasibility of the presented graph abstraction was verified through experiments in topological display, dynamic route, access control, and data persistence. The performance of the shortest path in the graph-based abstraction layer and graph database proves the necessity of the graph abstraction layer. Empirical evidence demonstrates that the graph-based abstraction layer can facilitate network slicing, maintain a dependable depiction of the real network, streamline network administration and network application development, and provide a sophisticated abstraction that is easily understandable to network administrators.},
  archive      = {J_PEERJCS},
  author       = {Yufeng Jia and Jiadong Ren and Xianshan Li and Haitao He and Pengwei Zhang and Rong Li},
  doi          = {10.7717/peerj-cs.2674},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2674},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Construction of a user-friendly software-defined networking management using a graph-based abstraction layer},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of perinatal depression among women in pakistan using hybrid RNN-LSTM model. <em>PEERJCS</em>, <em>11</em>, e2673. (<a href='https://doi.org/10.7717/peerj-cs.2673'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Perinatal depression (PND) refers to a complex mental health condition that can occur during pregnancy (prenatal period) or in the first year after childbirth (postnatal period). Prediction of PND holds considerable importance due to its significant role in safeguarding the mental health and overall well-being of both mothers and their infants. Unfortunately, PND is difficult to diagnose at an early stage and thus may elevate the risk of suicide during pregnancy. In addition, it contributes to the development of postnatal depressive disorders. Despite the gravity of the problem, the resources for developing and training AI models in this area remain limited. To this end, in this work, we have locally curated a novel dataset named PERI DEP using the Patient Health Questionnaire (PHQ-9), Edinburgh Postnatal Depression Scale (EPDS), and socio-demographic questionnaires. The dataset consists of 14,008 records of women who participated in the hospitals of Lahore and Gujranwala regions. We have used SMOTE and GAN oversampling for data augmentation on the training set to solve the class imbalance problem. Furthermore, we propose a novel deep-learning framework combining the recurrent neural networks (RNN) and long short-term memory (LSTM) architectures. The results indicate that our hybrid RNN-LSTM model with SMOTE augmentation achieves a higher accuracy of 95% with an F1 score of 96%. Our study reveals the prevalence rate of PND among women in Pakistan (73.1%) indicating the need to prioritize the prevention and intervention strategies to overcome this public health challenge.},
  archive      = {J_PEERJCS},
  author       = {Amna Zafar and Muhammad Wasim and Beenish Ayesha Akram and Maham Riaz and Ivan Miguel Pires and Paulo Jorge Coelho},
  doi          = {10.7717/peerj-cs.2673},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2673},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Prediction of perinatal depression among women in pakistan using hybrid RNN-LSTM model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combination of machine learning and data envelopment analysis to measure the efficiency of the tax service office. <em>PEERJCS</em>, <em>11</em>, e2672. (<a href='https://doi.org/10.7717/peerj-cs.2672'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Tax Service Office, a division of the Directorate General of Taxes, is responsible for providing taxation services to the public and collecting taxes. Achieving tax targets efficiently while utilizing available resources is crucial. To assess the performance efficiency of decision-making units (DMUs), data envelopment analysis (DEA) is commonly employed. However, ensuring homogeneity among the DMUs is often necessary and requires the application of machine learning clustering techniques. In this study, we propose a three-stage approach: Clustering, DEA, and Regression, to measure the efficiency of all tax service office units. Real datasets from Indonesian tax service offices were used while maintaining strict confidentiality. Unlike previous studies that considered both input and output variables, we focus solely on clustering input variables, as it leads to more objective efficiency values when combining the results from each cluster. The results revealed three clusters with a silhouette score of 0.304 and Davies Bouldin Index of 1.119, demonstrating the effectiveness of fuzzy c-means clustering. Out of 352 DMUs, 225 or approximately 64% were identified as efficient using DEA calculations. We propose a regression algorithm to measure the efficiency of DMUs in new office planning, by determining the values of input and output variables. The optimization of multilayer perceptrons using genetic algorithms reduced the mean squared error by about 75.75%, from 0.0144 to 0.0035. Based on our findings, the overall performance of tax service offices in Indonesia has reached an efficiency level of 64%. These results show a significant improvement over the previous study, in which only about 18% of offices were considered efficient. The main contribution of this research is the development of a comprehensive framework for evaluating and predicting tax office efficiency, providing valuable insights for improving performance.},
  archive      = {J_PEERJCS},
  author       = {Shofinurdin Soffan and Arif Bramantoro and Ahmad A. Alzahrani},
  doi          = {10.7717/peerj-cs.2672},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2672},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Combination of machine learning and data envelopment analysis to measure the efficiency of the tax service office},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An improved termite life cycle optimizer algorithm for global function optimization. <em>PEERJCS</em>, <em>11</em>, e2671. (<a href='https://doi.org/10.7717/peerj-cs.2671'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The termite life cycle optimizer algorithm (TLCO) is a new bionic meta-heuristic algorithm that emulates the natural behavior of termites in their natural habitat. This work presents an improved TLCO (ITLCO) to increase the speed and accuracy of convergence. A novel strategy for worker generation is established to enhance communication between individuals in the worker population and termite population. This strategy would prevent the original worker generation strategy from effectively balancing algorithm convergence and population diversity to reduce the risk of the algorithm in reaching a local optimum. A novel soldier generation strategy is proposed, which incorporates a step factor that adheres to the principles of evolution to further enhance the algorithm’s convergence speed. Furthermore, a novel replacement update mechanism is executed when the new individual is of lower quality than the original individual. This mechanism ensures a balance between the convergence of the algorithm and the diversity of the population. The findings from CEC2013, CEC2019, and CEC2020 test sets indicate that ITLCO exhibits notable benefits regarding convergence speed, accuracy, and stability in comparison with the basic TLCO algorithm and the four most exceptional meta-heuristic algorithms thus far.},
  archive      = {J_PEERJCS},
  author       = {Yanjiao Wang and Mengjiao Wei},
  doi          = {10.7717/peerj-cs.2671},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2671},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An improved termite life cycle optimizer algorithm for global function optimization},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight-CancerNet: A deep learning approach for brain tumor detection. <em>PEERJCS</em>, <em>11</em>, e2670. (<a href='https://doi.org/10.7717/peerj-cs.2670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting brain tumors in medical imaging is challenging, requiring precise and rapid diagnosis. Deep learning techniques have shown encouraging results in this field. However, current models require significant computer resources and are computationally demanding. To overcome these constraints, we suggested a new deep learning architecture named Lightweight-CancerNet, designed to detect brain tumors efficiently and accurately. The proposed framework utilizes MobileNet architecture as the backbone and NanoDet as the primary detection component, resulting in a notable mean average precision (mAP) of 93.8% and an accuracy of 98%. In addition, we implemented enhancements to minimize computing time without compromising accuracy, rendering our model appropriate for real-time object detection applications. The framework’s ability to detect brain tumors with different image distortions has been demonstrated through extensive tests combining two magnetic resonance imaging (MRI) datasets. This research has shown that our framework is both resilient and reliable. The proposed model can improve patient outcomes and facilitate decision-making in brain surgery while contributing to the development of deep learning in medical imaging.},
  archive      = {J_PEERJCS},
  author       = {Asif Raza and Muhammad Javed Iqbal},
  doi          = {10.7717/peerj-cs.2670},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2670},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Lightweight-CancerNet: A deep learning approach for brain tumor detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging sentiment analysis of food delivery services reviews using deep learning and word embedding. <em>PEERJCS</em>, <em>11</em>, e2669. (<a href='https://doi.org/10.7717/peerj-cs.2669'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Companies that deliver food (food delivery services, or FDS) try to use customer feedback to identify aspects where the customer experience could be improved. Consumer feedback on purchasing and receiving goods via online platforms is a crucial tool for learning about a company’s performance. Many English-language studies have been conducted on sentiment analysis (SA). Arabic is becoming one of the most extensively written languages on the World Wide Web, but because of its morphological and grammatical difficulty as well as the lack of openly accessible resources for Arabic SA, like as dictionaries and datasets, there has not been much research done on the language. Using a manually annotated FDS dataset, the current study conducts extensive sentiment analysis using reviews related to FDS that include Modern Standard Arabic and dialectal Arabic. It does this by utilizing word embedding models, deep learning techniques, and natural language processing to extract subjective opinions, determine polarity, and recognize customers’ feelings in the FDS domain. Convolutional neural network (CNN), bidirectional long short-term memory recurrent neural network (BiLSTM), and an LSTM-CNN hybrid model were among the deep learning approaches to classification that we evaluated. In addition, the article investigated different effective approaches for word embedding and stemming techniques. Using a dataset of Modern Standard Arabic and dialectal Arabic corpus gathered from Talabat.com, we trained and evaluated our suggested models. Our best accuracy was approximately 84% for multiclass classification and 92.5% for binary classification on the FDS. To verify that the proposed approach is suitable for analyzing human perceptions in diversified domains, we designed and carried out excessive experiments on other existing Arabic datasets. The highest obtained multi-classification accuracy is 88.9% on the Hotels Arabic-Reviews Dataset (HARD) dataset, and the highest obtained binary classification accuracy is 97.2% on the same dataset.},
  archive      = {J_PEERJCS},
  author       = {Dheya Mustafa and Safaa M. Khabour and Mousa Al-kfairy and Ahmed Shatnawi},
  doi          = {10.7717/peerj-cs.2669},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2669},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging sentiment analysis of food delivery services reviews using deep learning and word embedding},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MFI-net: Multi-level feature invertible network image concealment technique. <em>PEERJCS</em>, <em>11</em>, e2668. (<a href='https://doi.org/10.7717/peerj-cs.2668'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The utilization of deep learning and invertible networks for image hiding has been proven effective and secure. These methods can conceal large amounts of information while maintaining high image quality and security. However, existing methods often lack precision in selecting the hidden regions and primarily rely on residual structures. They also fail to fully exploit low-level features, such as edges and textures. These issues lead to reduced quality in model generation results, a heightened risk of network overfitting, and diminished generalization capability. In this article, we propose a novel image hiding method based on invertible networks, called MFI-Net. The method introduces a new upsampling convolution block (UCB) and combines it with a residual dense block that employs the parametric rectified linear unit (PReLU) activation function, effectively utilizing multi-level information (low-level and high-level features) of the image. Additionally, a novel frequency domain loss (FDL) is introduced, which constrains the secret information to be hidden in regions of the cover image that are more suitable for concealing the data. Extensive experiments on the DIV2K, COCO, and ImageNet datasets demonstrate that MFI-Net consistently outperforms state-of-the-art methods, achieving superior image quality metrics. Furthermore, we apply the proposed method to digital collection images, achieving significant success.},
  archive      = {J_PEERJCS},
  author       = {Dapeng Cheng and Minghui Zhu and Bo Yang and Xiaolian Gao and Wanting Jing and Yanyan Mao and Feng Zhao},
  doi          = {10.7717/peerj-cs.2668},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2668},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MFI-net: Multi-level feature invertible network image concealment technique},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated evaluation systems to enhance exam quality and reduce test anxiety. <em>PEERJCS</em>, <em>11</em>, e2666. (<a href='https://doi.org/10.7717/peerj-cs.2666'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {University examination papers play a crucial role in the institution’s quality, impacting the institution’s accreditation status. In this context, ensuring the quality of examination papers is paramount. In practice, however, manual assessments are mostly laborious and time-consuming and generally lack consistency. The last decade has seen digital education acquire immense interest in academic discourse, especially when developing intelligent systems for educational assessment. The presented work proposes an automated system that allows text analysis and evaluation of university exam papers by formal and technical criteria. The research was conducted by analyzing 30 exam papers, which will be included in each of the exam papers, which consist of 60 questions each, in total it holds 1,800 questions. Moreover, it also includes research to understand the quality and relationship with students’ test anxiety. A total of 50 year one first-year students were taken to measure students’ academic stress by a scale. Planning on basic levels and adherence to technical standards were missing in the exam papers. The proposed automated system has improved exam paper quality to a great extent and reduced academic stress among students with an accuracy of 98% in identifying and matching specified criteria.},
  archive      = {J_PEERJCS},
  author       = {Doaa Mohamed Elbourhamy},
  doi          = {10.7717/peerj-cs.2666},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2666},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automated evaluation systems to enhance exam quality and reduce test anxiety},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A secure healthcare data transmission based on synchronization of fractional order chaotic systems. <em>PEERJCS</em>, <em>11</em>, e2665. (<a href='https://doi.org/10.7717/peerj-cs.2665'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The transmission of healthcare data plays a vital role in cities worldwide, facilitating access to patient’s health information across healthcare systems and contributing to the enhancement of care services. Ensuring secure healthcare transmission requires that the transmitted data be reliable. However, verifying this reliability can potentially compromise patient privacy. Given the sensitive nature of health information, preserving privacy remains a paramount concern in healthcare systems. In this work, we present a novel secure communication scheme that leverages a chaos cryptosystem to address the critical concerns of reliability and privacy in healthcare data transmission. Chaos-based cryptosystems are particularly well-suited for such applications due to their inherent sensitivity to initial conditions, which significantly enhances resistance to adversarial violations. This property makes the chaos-based approach highly effective in ensuring the security of sensitive healthcare data. The proposed chaos cryptosystem in this work is built upon the synchronization of fractional-order chaotic systems with varying structures and orders. The synchronization between the primary system (PS) and the secondary system (SS) is achieved through the application of Lyapunov stability theory. For the encryption and decryption of sensitive healthcare data, the scheme employs the n-shift encryption principle. Furthermore, a detailed analysis of the key space was conducted to ensure the scheme’s robustness against potential attacks. Numerical simulations were also performed to validate the effectiveness of the proposed scheme.},
  archive      = {J_PEERJCS},
  author       = {Nur Afiqah Suzelan Amir and Fatin Nabila Abd Latiff and Kok Bin Wong and Wan Ainun Mior Othman},
  doi          = {10.7717/peerj-cs.2665},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2665},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A secure healthcare data transmission based on synchronization of fractional order chaotic systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Predicting amyloid proteins using attention-based long short-term memory. <em>PEERJCS</em>, <em>11</em>, e2660. (<a href='https://doi.org/10.7717/peerj-cs.2660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is one of the genetically inherited neurodegenerative disorders that mostly occur when people get old. It can be recognized by severe memory impairment in the late stage, affecting cognitive function and general daily living. Reliable evidence confirms that the enhanced symptoms of AD are linked to the accumulation of amyloid proteins. The dense population of amyloid proteins forms insoluble fibrillar structures, causing significant pathological impacts in various tissues. Understanding amyloid protein’s mechanisms and identifying them at an early stage plays an essential role in treating AD as well as prevalent amyloid-related diseases. Recently, although several machine learning methods proposed for amyloid protein identification have shown promising results, most of them have not yet fully exploited the sequence information of the amyloid proteins. In this study, we develop a computational model for in silico identification of amyloid proteins using bidirectional long short-term memory in combination with an attention mechanism. In the testing phase, our findings showed that the model developed by our proposed method outperformed those developed by state-of-the-art methods with an area under the receiver operating characteristic curve of 0.9126.},
  archive      = {J_PEERJCS},
  author       = {Zhuowen Li},
  doi          = {10.7717/peerj-cs.2660},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2660},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Predicting amyloid proteins using attention-based long short-term memory},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging a hybrid convolutional gated recursive diabetes prediction and severity grading model through a mobile app. <em>PEERJCS</em>, <em>11</em>, e2642. (<a href='https://doi.org/10.7717/peerj-cs.2642'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes mellitus is a common illness associated with high morbidity and mortality rates. Early detection of diabetes is essential to prevent long-term health complications. The existing machine learning model struggles with accuracy and reliability issues, as well as data imbalance, hindering the creation of a dependable diabetes prediction model. The research addresses the issue using a novel deep learning mechanism called convolutional gated recurrent unit (CGRU), which could accurately detect diabetic disorder and their severity level. To overcome these obstacles, this study presents a brand-new deep learning technique, the CGRU, which enhances prediction accuracy by extracting temporal and spatial characteristics from the data. The proposed mechanism extracts both the spatial and temporal attributes from the input data to enable efficient classification. The proposed framework consists of three primary phases: data preparation, model training, and evaluation. Specifically, the proposed technique is applied to the BRFSS dataset for diabetes prediction. The collected data undergoes pre-processing steps, including missing data imputation, irrelevant feature removal, and normalization, to make it suitable for further processing. Furthermore, the pre-processed data is fed to the CGRU model, which is trained to identify intricate patterns indicating the stages of diabetes. To group the patients based on their characteristics and identity patterns, the research uses the clustering algorithm which helps them to classify the severity level. The efficacy of the proposed CGRU framework is demonstrated by validating the experimental findings against existing state-of-the-art approaches. When compared to existing approaches, such as Attention-based CNN and Ensemble ML model, the proposed model outperforms conventional machine learning techniques, demonstrating the efficacy of the CGRU architecture for diabetes prediction with a high accuracy rate o f 99.9%. Clustering algorithms are more beneficial as they help in identifying the subtle pattern in the dataset. When compared to other methods, it can lead to more accurate and reliable prediction. The study highlights how the cutting-edge CGRU model enhances the early detection and diagnosis of diabetes, which will eventually lead to improved healthcare outcomes. However, the study limits to work on diverse datasets, which is the only thing considered to be the drawback of this research.},
  archive      = {J_PEERJCS},
  author       = {Alhuseen Omar Alsayed and Nor Azman Ismail and Layla Hasan and Muhammad Binsawad and Farhat Embarak},
  doi          = {10.7717/peerj-cs.2642},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2642},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Leveraging a hybrid convolutional gated recursive diabetes prediction and severity grading model through a mobile app},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated essay scoring with SBERT embeddings and LSTM-attention networks. <em>PEERJCS</em>, <em>11</em>, e2634. (<a href='https://doi.org/10.7717/peerj-cs.2634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated essay scoring (AES) is essential in the field of educational technology, providing rapid and accurate evaluations of student writing. This study presents an innovative AES method that integrates Sentence-BERT (SBERT) with Long Short-Term Memory (LSTM) networks and attention mechanisms to improve the scoring process. SBERT generates embedding vectors for each essay, which are subsequently analyzed using a bidirectional LSTM (BiLSTM) to learn the features of these embedding vectors. An attention layer is introduced to enable the system to prioritize the most significant components of the essay. Evaluated using a benchmark dataset, our approach shows significant improvements in scoring accuracy, highlighting its ability to improve the reliability and efficiency of automated assessment systems.},
  archive      = {J_PEERJCS},
  author       = {Yuzhe Nie},
  doi          = {10.7717/peerj-cs.2634},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2634},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Automated essay scoring with SBERT embeddings and LSTM-attention networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating machine learning models for predictive accuracy in cryptocurrency price forecasting. <em>PEERJCS</em>, <em>11</em>, e2626. (<a href='https://doi.org/10.7717/peerj-cs.2626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our research investigates the predictive performance and robustness of machine learning classification models and technical indicators for algorithmic trading in the volatile cryptocurrency market. The main aim is to identify reliable approaches for informed decision-making and profitable strategy development. With the increasing global adoption of cryptocurrency, robust trading models are essential for navigating its unique challenges and seizing investment opportunities. This study contributes to the field by offering a novel comparison of models, including logistic regression, random forest, and gradient boosting, under different data configurations and resampling techniques to address class imbalance. Historical data from cryptocurrency exchanges and data aggregators is collected, preprocessed, and used to train and evaluate these models. The impact of class imbalance, resampling techniques, and hyperparameter tuning on model performance is investigated. By analyzing historical cryptocurrency data, the methodology emphasizes hyperparameter tuning and backtesting, ensuring realistic model assessment. Results highlight the importance of addressing class imbalance and identify consistently outperforming models such as random forest, XGBoost, and gradient boosting. Our findings demonstrate that these models outperform others, indicating promising avenues for future research, particularly in sentiment analysis, reinforcement learning, and deep learning. This study provides valuable guidance for navigating the complex landscape of algorithmic trading in cryptocurrencies. By leveraging the findings and recommendations presented, practitioners can develop more robust and profitable trading strategies tailored to the unique characteristics of this emerging market.},
  archive      = {J_PEERJCS},
  author       = {Shavez Mushtaq Qureshi and Atif Saeed and Farooq Ahmad and Asad Rehman Khattak and Sultan H. Almotiri and Mohammed A. Al Ghamdi and Muhammad Shah Rukh},
  doi          = {10.7717/peerj-cs.2626},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2626},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evaluating machine learning models for predictive accuracy in cryptocurrency price forecasting},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evaluating the method reproducibility of deep learning models in biodiversity research. <em>PEERJCS</em>, <em>11</em>, e2618. (<a href='https://doi.org/10.7717/peerj-cs.2618'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) is revolutionizing biodiversity research by enabling advanced data analysis, species identification, and habitats monitoring, thereby enhancing conservation efforts. Ensuring reproducibility in AI-driven biodiversity research is crucial for fostering transparency, verifying results, and promoting the credibility of ecological findings. This study investigates the reproducibility of deep learning (DL) methods within the biodiversity research. We design a methodology for evaluating the reproducibility of biodiversity-related publications that employ DL techniques across three stages. We define ten variables essential for method reproducibility, divided into four categories: resource requirements, methodological information, uncontrolled randomness, and statistical considerations. These categories subsequently serve as the basis for defining different levels of reproducibility. We manually extract the availability of these variables from a curated dataset comprising 100 publications identified using the keywords provided by biodiversity experts. Our study shows that a dataset is shared in 50% of the publications; however, a significant number of the publications lack comprehensive information on deep learning methods, including details regarding randomness.},
  archive      = {J_PEERJCS},
  author       = {Waqas Ahmed and Vamsi Krishna Kommineni and Birgitta König-Ries and Jitendra Gaikwad and Luiz Gadelha and Sheeba Samuel},
  doi          = {10.7717/peerj-cs.2618},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2618},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evaluating the method reproducibility of deep learning models in biodiversity research},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unveiling personalized and gamification-based cybersecurity risks within financial institutions. <em>PEERJCS</em>, <em>11</em>, e2598. (<a href='https://doi.org/10.7717/peerj-cs.2598'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gamification has emerged as a transformative e-business strategy, introducing innovative methods to engage customers and drive sales. This article explores the integration of game design principles into business contexts, termed “gamification,” a subject of increasing interest among both scholars and industry professionals. The discussion systematically addresses key themes, like the role of gamification in marketing strategies, enhancing website functionality, and its application within the financial sector, including e-banking, drawing insights from academic and industry perspectives. By conducting a systematic literature review of 48 academic articles published between 2015 and 2024, this study examines the use of personalized, gamification-based strategies to mitigate cyber threats in the financial domain. The review highlights the growing digitization of financial services and the corresponding rise in sophisticated cyber threats, including traditional attacks and advanced persistent threats (APTs). This article critically assesses the evolving landscape of cyber threats specific to the financial industry, identifying trends, challenges, and innovative solutions to strengthen cybersecurity practices. Of particular interest is the application of AI-enhanced gamification strategies to reinforce cybersecurity protocols, particularly in the face of novel threats in gaming platforms. Furthermore, the review evaluates techniques grounded in user behavior, motivation, and readiness to enhance cybersecurity. The article also offers a comprehensive taxonomy of financial services, categorizing cyber threats into game-based (e.g., phishing, malware, APTs) and non-game-based (e.g., social engineering, compliance issues) threats. AI-driven measures for prevention and detection emphasize regular security assessments, user training, and system monitoring with incident response plans. This research provides valuable insights into the intersection of gamification and cybersecurity, offering a forward-looking perspective for both academic researchers and industry professionals.},
  archive      = {J_PEERJCS},
  author       = {Amna Shahzadi and Kashif Ishaq and Naeem A. Nawaz and Fadhilah Rosdi and Fawad Ali Khan},
  doi          = {10.7717/peerj-cs.2598},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2598},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Unveiling personalized and gamification-based cybersecurity risks within financial institutions},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Review of models for estimating 3D human pose using deep learning. <em>PEERJCS</em>, <em>11</em>, e2574. (<a href='https://doi.org/10.7717/peerj-cs.2574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation (HPE) is designed to detect and localize various parts of the human body and represent them as a kinematic structure based on input data like images and videos. Three-dimensional (3D) HPE involves determining the positions of articulated joints in 3D space. Given its wide-ranging applications, HPE has become one of the fastest-growing areas in computer vision and artificial intelligence. This review highlights the latest advances in 3D deep-learning-based HPE models, addressing the major challenges such as accuracy, real-time performance, and data constraints. We assess the most widely used datasets and evaluation metrics, providing a comparison of leading algorithms in terms of precision and computational efficiency in tabular form. The review identifies key applications of HPE in industries like healthcare, security, and entertainment. Our findings suggest that while deep learning models have made significant strides, challenges in handling occlusion, real-time estimation, and generalization remain. This study also outlines future research directions, offering a roadmap for both new and experienced researchers to further develop 3D HPE models using deep learning.},
  archive      = {J_PEERJCS},
  author       = {Sani Salisu and Kamaluddeen Usman Danyaro and Maged Nasser and Israa M. Hayder and Hussain A. Younis},
  doi          = {10.7717/peerj-cs.2574},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2574},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Review of models for estimating 3D human pose using deep learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of machine learning on dietary and exercise behaviors in type 2 diabetes self-management: A systematic literature review. <em>PEERJCS</em>, <em>11</em>, e2568. (<a href='https://doi.org/10.7717/peerj-cs.2568'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-awareness and self-management in diabetes are critical as they enhance patient well-being, decrease financial burden, and alleviate strain on healthcare systems by mitigating complications and promoting healthier life expectancy. Incomplete understanding persists regarding the synergistic effects of diet and exercise on diabetes management, as existing research often isolates these factors, creating a knowledge gap in comprehending their combined influence. Current diabetes research overlooks the interplay between diet and exercise in self-management. A holistic study is crucial to mitigate complications and healthcare burdens effectively. Multi-dimensional research questions covering complete diabetic management such as publication channels for diabetic research, existing machine learning solutions, physical activity tacking existing methods, and diabetic-associated datasets are included in this research. In this study, using a proper research protocol primary research articles related to diet, exercise, datasets, and blood analysis are selected and their quality is assessed for diabetic management. This study interrelates two major dimensions of diabetes management together that are diet and exercise.},
  archive      = {J_PEERJCS},
  author       = {Rizwan Riaz Mir and Nazeef Ul Haq and Kashif Ishaq and Nurhizam Safie and Abdul Basit Dogar},
  doi          = {10.7717/peerj-cs.2568},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2568},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Impact of machine learning on dietary and exercise behaviors in type 2 diabetes self-management: A systematic literature review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The technique of fuzzy analytic hierarchy process (FAHP) based on the triangular q-rung fuzzy numbers (TR-q-ROFNS) with applications in best african coffee brand selection. <em>PEERJCS</em>, <em>11</em>, e2555. (<a href='https://doi.org/10.7717/peerj-cs.2555'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The African coffee market offers a rich and diverse range of coffee profiles. The coffee producers of Africa face numerous challenges like climate change, market fluctuations, diseases, soil degradation and limited access to finance. These challenges badly affect their productivity, quality and livelihood. There are different factors like social and cultural, which can affect the coffee production. This study aims to develop multi criteria decision making (MCDM) methods and their applications in coffee market specifically in identifying factors influencing consumers’ coffee brand preferences in South Africa, which is known for its vibrant coffee culture. For this purpose, first we developed the technique of analytic hierarchy process (AHP) in the environment of triangular q-rung orthopair fuzzy numbers. The triangular q-rung fuzzy numbers can effectively handle the uncertainity. The AHP technique has widely been used in decision making due to its flexibility in assigning weights and dealing with vagueness. The weights of critera plays a very important role in an MCDM problem. The development of AHP technique in triangular q-rung orthopair fuzzy environment can improve the decision making (DM) by handling vagueness in data and by using the most appropriate weights. Furthermore this new proposed method improves accuracy and minimize the information loss. The newly peoposed method is applied to different MCDM problems and comparative analysis is conducted to check the validity of results.},
  archive      = {J_PEERJCS},
  author       = {Yupei Huang and Muhammad Gulistan and Amir Rafique and Wathek Chammam and Khursheed Aurangzeb and Ateeq Ur Rehman},
  doi          = {10.7717/peerj-cs.2555},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2555},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The technique of fuzzy analytic hierarchy process (FAHP) based on the triangular q-rung fuzzy numbers (TR-q-ROFNS) with applications in best african coffee brand selection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cloud-to-thing continuum-based sports monitoring system using machine learning and deep learning model. <em>PEERJCS</em>, <em>11</em>, e2539. (<a href='https://doi.org/10.7717/peerj-cs.2539'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sports monitoring and analysis have seen significant advancements by integrating cloud computing and continuum paradigms facilitated by machine learning and deep learning techniques. This study presents a novel approach for sports monitoring, specifically focusing on basketball, that seamlessly transitions from traditional cloud-based architectures to a continuum paradigm, enabling real-time analysis and insights into player performance and team dynamics. Leveraging machine learning and deep learning algorithms, our framework offers enhanced capabilities for player tracking, action recognition, and performance evaluation in various sports scenarios. The proposed Cloud-to-Thing continuum-based sports monitoring system utilizes advanced techniques such as Improved Mask R-CNN for pose estimation and a hybrid metaheuristic algorithm combined with a generative adversarial network (GAN) for classification. Our system significantly improves latency and accuracy, reducing latency to 5.1 ms and achieving an accuracy of 94.25%, which outperforms existing methods in the literature. These results highlight the system’s ability to provide real-time, precise, and scalable sports monitoring, enabling immediate feedback for time-sensitive applications. This research has significantly improved real-time sports event analysis, contributing to improved player performance evaluation, enhanced team strategies, and informed tactical adjustments.},
  archive      = {J_PEERJCS},
  author       = {Amal Alshardan and Hany Mahgoub and Saad Alahmari and Mohammed Alonazi and Radwa Marzouk and Abdullah Mohamed},
  doi          = {10.7717/peerj-cs.2539},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2539},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Cloud-to-thing continuum-based sports monitoring system using machine learning and deep learning model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RCE-IFE: Recursive cluster elimination with intra-cluster feature elimination. <em>PEERJCS</em>, <em>11</em>, e2528. (<a href='https://doi.org/10.7717/peerj-cs.2528'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computational and interpretational difficulties caused by the ever-increasing dimensionality of biological data generated by new technologies pose a significant challenge. Feature selection (FS) methods aim to reduce the dimension, and feature grouping has emerged as a foundation for FS techniques that seek to detect strong correlations among features and identify irrelevant features. In this work, we propose the Recursive Cluster Elimination with Intra-Cluster Feature Elimination (RCE-IFE) method that utilizes feature grouping and iterates grouping and elimination steps in a supervised context. We assess dimensionality reduction and discriminatory capabilities of RCE-IFE on various high-dimensional datasets from different biological domains. For a set of gene expression, microRNA (miRNA) expression, and methylation datasets, the performance of RCE-IFE is comparatively evaluated with RCE-IFE-SVM (the SVM-adapted version of RCE-IFE) and SVM-RCE. On average, RCE-IFE attains an area under the curve (AUC) of 0.85 among tested expression datasets with the fewest features and the shortest running time, while RCE-IFE-SVM (the SVM-adapted version of RCE-IFE) and SVM-RCE achieve similar AUCs of 0.84 and 0.83, respectively. RCE-IFE and SVM-RCE yield AUCs of 0.79 and 0.68, respectively when averaged over seven different metagenomics datasets, with RCE-IFE significantly reducing feature subsets. Furthermore, RCE-IFE surpasses several state-of-the-art FS methods, such as Minimum Redundancy Maximum Relevance (MRMR), Fast Correlation-Based Filter (FCBF), Information Gain (IG), Conditional Mutual Information Maximization (CMIM), SelectKBest (SKB), and eXtreme Gradient Boosting (XGBoost), obtaining an average AUC of 0.76 on five gene expression datasets. Compared with a similar tool, Multi-stage, RCE-IFE gives a similar average accuracy rate of 89.27% using fewer features on four cancer-related datasets. The comparability of RCE-IFE is also verified with other biological domain knowledge-based Grouping-Scoring-Modeling (G-S-M) tools, including mirGediNET, 3Mint, and miRcorrNet. Additionally, the biological relevance of the selected features by RCE-IFE is evaluated. The proposed method also exhibits high consistency in terms of the selected features across multiple runs. Our experimental findings imply that RCE-IFE provides robust classifier performance and significantly reduces feature size while maintaining feature relevance and consistency.},
  archive      = {J_PEERJCS},
  author       = {Cihan Kuzudisli and Burcu Bakir-Gungor and Bahjat Qaqish and Malik Yousef},
  doi          = {10.7717/peerj-cs.2528},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2528},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {RCE-IFE: Recursive cluster elimination with intra-cluster feature elimination},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Tuck-KGC: Based on tensor decomposition for diabetes knowledge graph completion model integrating chinese and western medicine. <em>PEERJCS</em>, <em>11</em>, e2522. (<a href='https://doi.org/10.7717/peerj-cs.2522'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The medical knowledge graph is essential for intelligent medical services, encompassing personalized diagnostics, precision therapies, and intelligent consultations, among others. However, medical knowledge graphs frequently suffer from incompleteness, primarily due to the absence of certain entities or relationships. The incomplete nature of knowledge graphs poses substantial challenges to these tasks. Knowledge graph completion technology is instrumental in addressing this issue. Specifically, tensor decomposition-based approaches for knowledge graph completion embed entities and relationships into the vector space, where tensor decomposition computations are employed to predict missing relationships within the knowledge graph. However, the tensor representation of entities and their relationships often overlooks crucial entity type information, potentially resulting in an abundance of irrational relationships during the prediction process. To mitigate this, we propose the Tucker Decomposition Knowledge Graph Completion (Tuck-KGC) method, which incorporates entity types into the tensor decomposition framework. This method maps the types of medical entities to vectors, which are seamlessly integrated into the knowledge graph representation learning process. This allows the model to thoroughly absorb entity information, thereby enhancing the accuracy of link prediction. To evaluate the Tuck-KGC, we built the Dia dataset, a knowledge graph tailored for precision medical analysis, which integrates both Traditional Chinese Medicine and Western medicine perspectives. The Dia dataset encompasses 10,294 entities with 214 relationships, covering a comprehensive spectrum including diseases, treatments, clinical manifestations, complications, etiology, and so on. Building upon the Dia dataset, experimental results indicate that the Tuck-KGC model boosts link prediction accuracy by roughly 8%, affirming the efficacy of incorporating entity type information into the model.},
  archive      = {J_PEERJCS},
  author       = {Jiangtao ZhangSun and Yu Xin Yang and Beiji Zou and Qinghua Peng and Xiao Xia Xiao},
  doi          = {10.7717/peerj-cs.2522},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2522},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Tuck-KGC: Based on tensor decomposition for diabetes knowledge graph completion model integrating chinese and western medicine},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diagnostic test accuracy of AI-assisted mammography for breast imaging: A narrative review. <em>PEERJCS</em>, <em>11</em>, e2476. (<a href='https://doi.org/10.7717/peerj-cs.2476'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of artificial intelligence into healthcare, particularly in mammography, holds immense potential for improving breast cancer diagnosis. Artificial intelligence (AI), with its ability to process vast amounts of data and detect intricate patterns, offers a solution to the limitations of traditional mammography, including missed diagnoses and false positives. This review focuses on the diagnostic accuracy of AI-assisted mammography, synthesizing findings from studies across different clinical settings and algorithms. The motivation for this research lies in addressing the need for enhanced diagnostic tools in breast cancer screening, where early detection can significantly impact patient outcomes. Although AI models have shown promising improvements in sensitivity and specificity, challenges such as algorithmic bias, interpretability, and the generalizability of models across diverse populations remain. The review concludes that while AI holds transformative potential in breast cancer screening, collaborative efforts between radiologists, AI developers, and policymakers are crucial for ensuring ethical, reliable, and inclusive integration into clinical practice.},
  archive      = {J_PEERJCS},
  author       = {Daksh Dave and Adnan Akhunzada and Nikola Ivković and Sujan Gyawali and Korhan Cengiz and Adeel Ahmed and Ahmad Sami Al-Shamayleh},
  doi          = {10.7717/peerj-cs.2476},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2476},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Diagnostic test accuracy of AI-assisted mammography for breast imaging: A narrative review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing breast cancer prediction through stacking ensemble and deep learning integration. <em>PEERJCS</em>, <em>11</em>, e2461. (<a href='https://doi.org/10.7717/peerj-cs.2461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer is one of the most common types of cancer in women and is recognized as a serious global public health issue. The increasing incidence of breast cancer emphasizes the importance of early detection, which enhances the effectiveness of treatment processes. In addressing this challenge, the importance of machine learning and deep learning technologies is increasingly recognized. The aim of this study is to evaluate the integration of ensemble models and deep learning models using stacking ensemble techniques on the Breast Cancer Wisconsin (Diagnostic) dataset and to enhance breast cancer diagnosis through this methodology. To achieve this, the efficacy of ensemble methods such as Random Forest, XGBoost, LightGBM, ExtraTrees, HistGradientBoosting, AdaBoost, GradientBoosting, and CatBoost in modeling breast cancer diagnosis was comprehensively evaluated. In addition to ensemble methods, deep learning models including convolutional neural network (CNN), recurrent neural network (RNN), gated recurrent unit (GRU), bidirectional long short-term memory (BILSTM), long short-term memory (LSTM) were analyzed as meta predictors. Among these models, CNN stood out for its high accuracy and rapid training time, making it an ideal choice for real-time diagnostic applications. Finally, the study demonstrated how breast cancer prediction was enhanced by integrating a set of base predictors, such as LightGBM, ExtraTrees, and CatBoost, with a deep learning-based meta-predictor, such as CNN, using stacking ensemble methodology. This stacking integration model offers significant potential for healthcare decision support systems with high accuracy, F1 score, and receiver operating characteristic area under the curve (ROC AUC), along with reduced training times. The results from this research offer important insights for enhancing decision-making strategies in the diagnosis and management of breast cancer.},
  archive      = {J_PEERJCS},
  author       = {Fatih Gurcan},
  doi          = {10.7717/peerj-cs.2461},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2461},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enhancing breast cancer prediction through stacking ensemble and deep learning integration},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of GPT in promoting inclusive higher education for people with various learning disabilities: A review. <em>PEERJCS</em>, <em>11</em>, e2400. (<a href='https://doi.org/10.7717/peerj-cs.2400'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generative pre-trained transformer (GPT) is a notable breakthrough in the field of artificial intelligence, as it empowers machines to effectively comprehend and engage in interactions with humans. The GPT exhibits the capacity to enhance inclusivity and accessibility for students with learning disabilities in the context of higher education, hence potentially facilitating substantial advancements in the field. GPT can provide personalized and diverse solutions that successfully cater to the distinct requirements of students with learning disabilities. This motivated us to conduct an extensive review to assess the effectiveness of GPT in enhancing accessibility and inclusivity in higher education for students with learning disabilities. This review offers a comprehensive analysis of the GPT and its significance for enhancing inclusivity in the field of higher education. In this research, we also examined the possible challenges and constraints associated with the integration of GPT into inclusive higher education, along with potential solutions. Overall, this review is intended for educators, students with and without learning disabilities, policymakers, higher education institutes, researchers, and educational technology developers. This review aims to provide a comprehensive understanding of GPT in promoting inclusive higher education for people with various learning disabilities, its impacts on inclusive higher education, emerging challenges, and potential solutions.},
  archive      = {J_PEERJCS},
  author       = {Thippa Reddy Gadekallu and Gokul Yenduri and Rajesh Kaluri and Dharmendra Singh Rajput and Kuruva Lakshmanna and Kai Fang and Junxin Chen and Wei Wang},
  doi          = {10.7717/peerj-cs.2400},
  journal      = {PeerJ Computer Science},
  month        = {2},
  pages        = {e2400},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {The role of GPT in promoting inclusive higher education for people with various learning disabilities: A review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving the accuracy of soil texture determination using pH and electro conductivity values with ultrasound penetration-based digital soil texture analyzer. <em>PEERJCS</em>, <em>11</em>, e2663. (<a href='https://doi.org/10.7717/peerj-cs.2663'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Soil texture analysis is critical for advancing agricultural productivity, ensuring environmental sustainability, and maintaining ecosystem balance. Traditional sedimentation-based methods, such as the hydrometer technique, are fast and practical but prone to inaccuracies due to the effects of water-soluble substances. This study focuses on the practical framework of integrating pH (potential of hydrogen) and EC (electrical conductivity), as indicators of dissolved substances that influence soil texture estimation. Using the Ultrasound Penetration-based Digital Soil Texture Analyzer (USTA), this research combined ultrasound time series data with pH and EC measurements to predict sand, silt, and clay ratios through machine learning methods—support vector regression (SVR), Random Forest (RF), and multi-layer perceptron neural network (MLPNN). Simulations showed that RF yielded the best results, improving R2 values to 0.52, 0.33, and 0.31 for sand, silt, and clay, respectively. The enhanced model performance demonstrates the viability of integrating pH and EC with advanced machine learning techniques to improve soil texture analysis accuracy. These findings suggest that automated systems like USTA, with modular pH and EC sensors, can provide cost-effective, efficient alternatives to traditional methods, offering practical implications for soil management and agricultural optimization.},
  archive      = {J_PEERJCS},
  author       = {Emre Kilinc and Umut Orhan},
  doi          = {10.7717/peerj-cs.2663},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2663},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving the accuracy of soil texture determination using pH and electro conductivity values with ultrasound penetration-based digital soil texture analyzer},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TurkMedNLI: A turkish medical natural language inference dataset through large language model based translation. <em>PEERJCS</em>, <em>11</em>, e2662. (<a href='https://doi.org/10.7717/peerj-cs.2662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language inference (NLI) is a subfield of natural language processing (NLP) that aims to identify the contextual relationship between premise and hypothesis sentences. While high-resource languages like English benefit from robust and rich NLI datasets, creating similar datasets for low-resource languages is challenging due to the cost and complexity of manual annotation. Although translation of existing datasets offers a practical solution, direct translation of domain-specific datasets presents unique challenges, particularly in handling abbreviations, metric conversions, and cultural alignment. This study introduces a pipeline for translating a medical NLI dataset into Turkish, which is a low-resource language. Our approach employs fine-tuning the Llama-3.1 model with selected samples from the Medical Abbreviation dataset (MeDAL) to extract and resolve medical abbreviations. Consequently, NLI pairs are refined with extracted abbreviations and subjected to metric correction. Later, the processed sentences are then translated using Facebook’s No Language Left Behind (NLLB) translation model. To ensure quality, we conducted comprehensive evaluations using both machine learning models and medical expert review. Our results show that BERTurk achieved 75.17% accuracy on TurkMedNLI test data and 76.30% on the normalized test set, while BioBERTurk demonstrated comparable performance with 75.59% accuracy on test data and 72.29% on the normalized dataset. Medical experts further validated the translations through manual assessment of sampled sentences. This work demonstrates the effectiveness of large language models in adapting domain-specific datasets for low-resource languages, establishing a foundation for future research in multilingual biomedical NLP.},
  archive      = {J_PEERJCS},
  author       = {İskender Ülgen Oğul and Fatih Soygazi and Belgin Ergenç Bostanoğlu},
  doi          = {10.7717/peerj-cs.2662},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2662},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {TurkMedNLI: A turkish medical natural language inference dataset through large language model based translation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning for enhanced risk management: A novel approach to analyzing financial reports. <em>PEERJCS</em>, <em>11</em>, e2661. (<a href='https://doi.org/10.7717/peerj-cs.2661'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Risk management is a critical component of today’s financial environment because of the enormity and complexity of data contained in financial statements. Business situations, plans, and schedule risk assessment with the help of conventional ways which involve analytical, technical, and heuristic models are inadequate to address the complex structures of the latest data. This research brings out the Hybrid Financial Risk Predictor (HFRP) model, using the convolutional neural networks (CNN) and long-short term memory (LSTM) networks to improve financial risk prediction. A combination of quantitative and qualitative ratings derived from the analysis of financial texts results in high accuracy and stability compared with the HFRP model. Evaluating key findings, the quantity of training & testing loss decreased considerably and they have their final value as 0.0013 and 0.003, respectively. According to the hypothesis, the selected HFRP model demonstrates the values of the revenue, net income, and earnings per share (EPS), and are closely similar to the actual values. The model achieves substantial risk mitigation: credit risk lowered from 0.75 to 0.20, liquidity risk from 0.70 to 0.25, market risk from 0.65 to 0.30, while operational risk is at 0.80 to 0.35. By analyzing the results of the HFRP model, it can be stated that the proposal promotes improved financial stability and presents a reliable model for the contemporary financial markets, which in turn helps in making sound decisions and improve the assessment of risks.},
  archive      = {J_PEERJCS},
  author       = {Xiangting Shi and Yakang Zhang and Manning Yu and Lihao Zhang},
  doi          = {10.7717/peerj-cs.2661},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2661},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep learning for enhanced risk management: A novel approach to analyzing financial reports},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient skyline query processing with user-specified conditional preference. <em>PEERJCS</em>, <em>11</em>, e2659. (<a href='https://doi.org/10.7717/peerj-cs.2659'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the realm of multi-attribute decision-making, the utilization of skyline queries has gained increasing popularity for assisting users in identifying objects with optimal attribute combinations. With the growing demand for personalization, integrating user’s preferences into skyline queries has emerged as an intriguing and promising research direction. However, the diverse expressions of preferences pose challenges to existing personalized skyline queries. Current methods assume that user preferences are too simplistic and do not represent the interdependencies between attributes. This poses a challenge to the existing skyline methods in effectively managing complex user preferences and dependencies. In this article, we propose an innovative and efficient method for skyline query processing, leveraging conditional preference networks (CP-Nets) to integrate specific user’s conditional preferences into the query process, termed as CP-Skyline. Firstly, we introduce a user-defined conditional preference model based on CP-Nets. By integrating user’s conditional preference information, we prune the candidate dataset, effectively compressing the query space. Secondly, we define a new dominance relation for CP-Skyline computation. Finally, extensive experiments were conducted on both synthetic and real-world datasets to assess the performance and effectiveness of the proposed methods. The experimental results unequivocally demonstrate a significant enhancement in skyline quality, and it presents a practical and potent solution for personalized decision support.},
  archive      = {J_PEERJCS},
  author       = {Senfu Ke and Xiaodong Fu and Jie Li},
  doi          = {10.7717/peerj-cs.2659},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2659},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Efficient skyline query processing with user-specified conditional preference},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Testing convolutional neural network based deep learning systems: A statistical metamorphic approach. <em>PEERJCS</em>, <em>11</em>, e2658. (<a href='https://doi.org/10.7717/peerj-cs.2658'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning technology spans many areas and today plays a significant role in addressing a wide range of problems in critical domains, i.e., healthcare, autonomous driving, finance, manufacturing, cybersecurity, etc. Metamorphic testing (MT) is considered a simple but very powerful approach in testing such computationally complex systems for which either an oracle is not available or is available but difficult to apply. Conventional metamorphic testing techniques have certain limitations in verifying deep learning-based models (i.e., convolutional neural networks (CNNs)) that have a stochastic nature (because of randomly initializing the network weights) in their training. In this article, we attempt to address this problem by using a statistical metamorphic testing (SMT) technique that does not require software testers to worry about fixing the random seeds (to get deterministic results) to verify the metamorphic relations (MRs). We propose seven MRs combined with different statistical methods to statistically verify whether the program under test adheres to the relation(s) specified in the MR(s). We further use mutation testing techniques to show the usefulness of the proposed approach in the healthcare space and test two CNN-based deep learning models (used for pneumonia detection among patients). The empirical results show that our proposed approach uncovers 85.71% of the implementation faults in the classifiers under test (CUT). Furthermore, we also propose an MRs minimization algorithm for the CUT, thus saving computational costs and organizational testing resources.},
  archive      = {J_PEERJCS},
  author       = {Faqeer ur Rehman and Clemente Izurieta},
  doi          = {10.7717/peerj-cs.2658},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2658},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Testing convolutional neural network based deep learning systems: A statistical metamorphic approach},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Applying auxiliary supervised depth-assisted transformer and cross modal attention fusion in monocular 3D object detection. <em>PEERJCS</em>, <em>11</em>, e2656. (<a href='https://doi.org/10.7717/peerj-cs.2656'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular 3D object detection is the most widely applied and challenging solution for autonomous driving, due to 2D images lacking 3D information. Existing methods are limited by inaccurate depth estimations by inequivalent supervised targets. The use of both depth and visual features also faces problems of heterogeneous fusion. In this article, we propose Depth Detection Transformer (Depth-DETR), applying auxiliary supervised depth-assisted transformer and cross modal attention fusion in monocular 3D object detection. Depth-DETR introduces two additional depth encoders besides the visual encoder. Two depth encoders are supervised by ground truth depth and bounding box respectively, working independently to complement each other’s limitations and predicting more accurate target distances. Furthermore, Depth-DETR employs cross modal attention mechanisms to effectively fuse three different features. A parallel structure of two cross modal transformer is applied to fuse two depth features with visual features. Avoiding early fusion between two depth features enhances the final fused feature for better feature representations. Through multiple experimental validations, the Depth-DETR model has achieved highly competitive results in the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) dataset, with an AP score of 17.49, representing its outstanding performance in 3D object detection.},
  archive      = {J_PEERJCS},
  author       = {Zhijian Wang and Jie Liu and Yixiao Sun and Xiang Zhou and Boyan Sun and Dehong Kong and Jay Xu and Xiaoping Yue and Wenyu Zhang},
  doi          = {10.7717/peerj-cs.2656},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2656},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Applying auxiliary supervised depth-assisted transformer and cross modal attention fusion in monocular 3D object detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review of deep learning techniques for apple leaf diseases classification and detection. <em>PEERJCS</em>, <em>11</em>, e2655. (<a href='https://doi.org/10.7717/peerj-cs.2655'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agriculture sustains populations and provides livelihoods, contributing to socioeconomic growth. Apples are one of the most popular fruits and contains various antioxidants that reduce the risk of chronic diseases. Additionally, they are low in calories, making them a healthy snack option for all ages. However, several factors can adversely affect apple production. These issues include diseases that drastically lower yield and quality and cause farmers to lose millions of dollars. To minimize yield loss and economic effects, it is essential to diagnose apple leaf diseases accurately and promptly. This allows targeted pesticide and insecticide use. However, farmers find it difficult to distinguish between different apple leaf diseases since their symptoms are quite similar. Computer vision applications have become an effective tool in recent years for handling these issues. They can provide accurate disease detection and classification through massive image datasets. This research analyzes and evaluates datasets, deep learning methods and frameworks built for apple leaf disease detection and classification. A systematic analysis of 45 articles published between 2016 and 2024 was conducted to evaluate the latest developments, approaches, and research needs in this area.},
  archive      = {J_PEERJCS},
  author       = {Assad Souleyman Doutoum and Bulent Tugrul},
  doi          = {10.7717/peerj-cs.2655},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2655},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A systematic review of deep learning techniques for apple leaf diseases classification and detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A quality assessment algorithm for no-reference images based on transfer learning. <em>PEERJCS</em>, <em>11</em>, e2654. (<a href='https://doi.org/10.7717/peerj-cs.2654'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image quality assessment (IQA) plays a critical role in automatically detecting and correcting defects in images, thereby enhancing the overall performance of image processing and transmission systems. While research on reference-based IQA is well-established, studies on no-reference image IQA remain underdeveloped. In this article, we propose a novel no-reference IQA algorithm based on transfer learning (IQA-NRTL). This algorithm leverages a deep convolutional neural network (CNN) due to its ability to effectively capture multi-scale semantic information features, which are essential for representing the complex visual perception in images. These features are extracted through a visual perception module. Subsequently, an adaptive fusion network integrates these features, and a fully connected regression network correlates the fused semantic information with global semantic information to perform the final quality assessment. Experimental results on authentically distorted datasets (KonIQ-10k, BIQ2021), synthetically distorted datasets (LIVE, TID2013), and an artificial intelligence (AI)-generated content dataset (AGIQA-1K) show that the proposed IQA-NRTL algorithm significantly improves performance compared to mainstream no-reference IQA algorithms, depending on variations in image content and complexity.},
  archive      = {J_PEERJCS},
  author       = {Yang Yang and Chang Liu and Hui Wu and Dingguo Yu},
  doi          = {10.7717/peerj-cs.2654},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2654},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A quality assessment algorithm for no-reference images based on transfer learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A hybrid blockchain-based solution for secure sharing of electronic medical record data. <em>PEERJCS</em>, <em>11</em>, e2653. (<a href='https://doi.org/10.7717/peerj-cs.2653'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patient privacy data security is a pivotal area of research within the burgeoning field of smart healthcare. This study proposes an innovative hybrid blockchain-based framework for the secure sharing of electronic medical record (EMR) data. Unlike traditional privacy protection schemes, our approach employs a novel tripartite blockchain architecture that segregates healthcare data across distinct blockchains for patients and healthcare providers while introducing a separate social blockchain to enable privacy-preserving data sharing with authorized external entities. This structure enhances both security and transparency while fostering collaborative efforts across different stakeholders. To address the inherent complexity of managing multiple blockchains, a unique cross-chain signature algorithm is introduced, based on the Boneh-Lynn-Shacham (BLS) signature aggregation technique. This algorithm not only streamlines the signature process across chains but also strengthens system security and optimizes storage efficiency, addressing a key challenge in multi-chain systems. Additionally, our external sharing algorithm resolves the prevalent issue of medical data silos by facilitating better data categorization and enabling selective, secure external sharing through the social blockchain. Security analyses and experimental results demonstrate that the proposed scheme offers superior security, storage optimization, and flexibility compared to existing solutions, making it a robust choice for safeguarding patient data in smart healthcare environments.},
  archive      = {J_PEERJCS},
  author       = {Gang Han and Yan Ma and Zhongliang Zhang and Yuxin Wang},
  doi          = {10.7717/peerj-cs.2653},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2653},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A hybrid blockchain-based solution for secure sharing of electronic medical record data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Novel transfer learning approach for hand drawn mathematical geometric shapes classification. <em>PEERJCS</em>, <em>11</em>, e2652. (<a href='https://doi.org/10.7717/peerj-cs.2652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand-drawn mathematical geometric shapes are geometric figures, such as circles, triangles, squares, and polygons, sketched manually using pen and paper or digital tools. These shapes are fundamental in mathematics education and geometric problem-solving, serving as intuitive visual aids for understanding complex concepts and theories. Recognizing hand-drawn shapes accurately enables more efficient digitization of handwritten notes, enhances educational tools, and improves user interaction with mathematical software. This research proposes an innovative machine learning algorithm for the automatic classification of mathematical geometric shapes to identify and interpret these shapes from handwritten input, facilitating seamless integration with digital systems. We utilized a benchmark dataset of mathematical shapes based on a total of 20,000 images with eight classes circle, kite, parallelogram, square, rectangle, rhombus, trapezoid, and triangle. We introduced a novel machine-learning algorithm CnN-RFc that uses convolution neural networks (CNN) for spatial feature extraction and the random forest classifier for probabilistic feature extraction from image data. Experimental results illustrate that using the CnN-RFc method, the Light Gradient Boosting Machine (LGBM) algorithm surpasses state-of-the-art approaches with high accuracy scores of 98% for hand-drawn shape classification. Applications of the proposed mathematical geometric shape classification algorithm span various domains, including education, where it enhances interactive learning platforms and provides instant feedback to students.},
  archive      = {J_PEERJCS},
  author       = {Aneeza Alam and Ali Raza and Nisrean Thalji and Laith Abualigah and Helena Garay and Josep Alemany-Iturriaga and Imran Ashraf},
  doi          = {10.7717/peerj-cs.2652},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2652},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Novel transfer learning approach for hand drawn mathematical geometric shapes classification},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improved BCI calibration in multimodal emotion recognition using heterogeneous adversarial transfer learning. <em>PEERJCS</em>, <em>11</em>, e2649. (<a href='https://doi.org/10.7717/peerj-cs.2649'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of brain-computer interface (BCI) technology to identify emotional states has gained significant interest, especially with the rise of virtual reality (VR) applications. However, the extensive calibration required for precise emotion recognition models presents a significant challenge, particularly for sensitive groups such as children, elderly, and patients. This study presents a novel approach that utilizes heterogeneous adversarial transfer learning (HATL) to synthesize electroencephalography (EEG) data from various other signal modalities, reducing the need for lengthy calibration phases. We benchmark the efficacy of three generative adversarial network (GAN) architectures, such as conditional GAN (CGAN), conditional Wasserstein GAN (CWGAN), and CWGAN with gradient penalty (CWGAN-GP) within this framework. The proposed framework is rigorously tested on two conventional open sourced datasets, SEED-V and DEAP. Additionally, the framework was applied to an immersive three-dimensional (3D) dataset named GraffitiVR, which we collected to capture the emotional and behavioral reactions of individuals experiencing urban graffiti in a VR environment. This expanded application provides insights into emotion recognition frameworks in VR settings, providing a wider range of contexts for assessing our methodology. When the accuracy of emotion recognition classifiers trained with CWGAN-GP-generated EEG data combined with non-EEG sensory data was compared against those trained using a combination of real EEG and non-EEG sensory data, the accuracy ratios were 93% on the SEED-V dataset, 99% on the DEAP dataset, and 97% on the GraffitiVR dataset. Moreover, in the GraffitiVR dataset, using CWGAN-GP-generated EEG data with non-EEG sensory data for emotion recognition models resulted in up to a 30% reduction in calibration time compared to classifiers trained on real EEG data with non-EEG sensory data. These results underscore the robustness and versatility of the proposed approach, significantly enhancing emotion recognition processes across a variety of environmental settings.},
  archive      = {J_PEERJCS},
  author       = {Mehmet Ali Sarikaya and Gökhan Ince},
  doi          = {10.7717/peerj-cs.2649},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2649},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improved BCI calibration in multimodal emotion recognition using heterogeneous adversarial transfer learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ensemble graph auto-encoders for clustering and link prediction. <em>PEERJCS</em>, <em>11</em>, e2648. (<a href='https://doi.org/10.7717/peerj-cs.2648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph auto-encoders are a crucial research area within graph neural networks, commonly employed for generating graph embeddings while minimizing errors in unsupervised learning. Traditional graph auto-encoders focus on reconstructing minimal graph data loss to encode neighborhood information for each node, yielding node embedding representations. However, existing graph auto-encoder models often overlook node representations and fail to capture contextual node information within the graph data, resulting in poor embedding effects. Accordingly, this study proposes the ensemble graph auto-encoders (E-GAE) model. It utilizes the ensemble random walk graph auto-encoder, the random walk graph auto-encoder of the ensemble network, and the graph attention auto-encoder to generate three node embedding matrices Z. Then, these techniques are combined using adaptive weights to reconstruct a new node embedding matrix. This method addresses the problem of low-quality embeddings. The model’s performance is evaluated using three publicly available datasets (Cora, Citeseer, and PubMed), indicating its effectiveness through multiple experiments. It achieves up to a 2.0% improvement in the link prediction task and a 9.4% enhancement in the clustering task. Our code for this work can be found at https://github.com/xcgydfjjjderg/graphautoencoder.},
  archive      = {J_PEERJCS},
  author       = {Chengxin Xie and Jingui Huang and Yongjiang Shi and Hui Pang and Liting Gao and Xiumei Wen},
  doi          = {10.7717/peerj-cs.2648},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2648},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Ensemble graph auto-encoders for clustering and link prediction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain enabled policy-based access control mechanism to restrict unauthorized access to electronic health records. <em>PEERJCS</em>, <em>11</em>, e2647. (<a href='https://doi.org/10.7717/peerj-cs.2647'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health record transmission and storage involve sensitive information, requiring robust security measures to ensure access is limited to authorized personnel. In the existing state of the art, there is a growing need for efficient access control approaches for the secure accessibility of patient health data by sustainable electronic health records. Locking medical data in a healthcare center forms information isolation; thus, setting up healthcare data exchange platforms is a driving force behind electronic healthcare centers. The healthcare entities access rights like subject, controller, and requester are defined and regulated by access control policies as defined by the General Data Protection Regulation (GDPR). In this work, we have introduced a blend of policy-based access control (PBAC) system backed by blockchain technology, where smart contracts govern the intrinsic part of security and privacy. As a result, any Subject can know at any time who currently has the right to access his data. The PBAC grants access to electronic health records based on predefined policies. Our proposed PBAC approach employs policies in which the subject, controller, and requester can grant access, revoke access, and check logs and actions made in a particular healthcare system. Smart contracts dynamically enforce access control policies and manage access permissions, ensuring that sensitive data is available only to authorized users. Delineating the proposed access control system and comparing it to other systems demonstrates that our approach is more adaptable to various healthcare data protection scenarios where there is a need to share sensitive data simultaneously and a robust need to safeguard the rights of the involved entities.},
  archive      = {J_PEERJCS},
  author       = {Nadeem Yaqub and Jianbiao Zhang and Muhammad Irfan Khalid and Weiru Wang and Markus Helfert and Mansoor Ahmed and Jungsuk Kim},
  doi          = {10.7717/peerj-cs.2647},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2647},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Blockchain enabled policy-based access control mechanism to restrict unauthorized access to electronic health records},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A comparative study of the performance of ten metaheuristic algorithms for parameter estimation of solar photovoltaic models. <em>PEERJCS</em>, <em>11</em>, e2646. (<a href='https://doi.org/10.7717/peerj-cs.2646'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study conducts a comparative analysis of the performance of ten novel and well-performing metaheuristic algorithms for parameter estimation of solar photovoltaic models. This optimization problem involves accurately identifying parameters that reflect the complex and nonlinear behaviours of photovoltaic cells affected by changing environmental conditions and material inconsistencies. This estimation is challenging due to computational complexity and the risk of optimization errors, which can hinder reliable performance predictions. The algorithms evaluated include the Crayfish Optimization Algorithm, the Golf Optimization Algorithm, the Coati Optimization Algorithm, the Crested Porcupine Optimizer, the Growth Optimizer, the Artificial Protozoa Optimizer, the Secretary Bird Optimization Algorithm, the Mother Optimization Algorithm, the Election Optimizer Algorithm, and the Technical and Vocational Education and Training-Based Optimizer. These algorithms are applied to solve four well-established photovoltaic models: the single-diode model, the double-diode model, the triple-diode model, and different photovoltaic module models. The study focuses on key performance metrics such as execution time, number of function evaluations, and solution optimality. The results reveal significant differences in the efficiency and accuracy of the algorithms, with some algorithms demonstrating superior performance in specific models. The Friedman test was utilized to rank the performance of the various algorithms, revealing the Growth Optimizer as the top performer across all the considered models. This optimizer achieved a root mean square error of 9.8602187789E−04 for the single-diode model, 9.8248487610E−04 for both the double-diode and triple-diode models and 1.2307306856E−02 for the photovoltaic module model. This consistent success indicates that the Growth Optimizer is a strong contender for future enhancements aimed at further boosting its efficiency and effectiveness. Its current performance suggests significant potential for improvement, making it a promising focus for ongoing development efforts. The findings contribute to the understanding of the applicability and performance of metaheuristic algorithms in renewable energy systems, providing valuable insights for optimizing photovoltaic models.},
  archive      = {J_PEERJCS},
  author       = {Adel Zga and Farouq Zitouni and Saad Harous and Karam Sallam and Abdulaziz S. Almazyad and Guojiang Xiong and Ali Wagdy Mohamed},
  doi          = {10.7717/peerj-cs.2646},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2646},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A comparative study of the performance of ten metaheuristic algorithms for parameter estimation of solar photovoltaic models},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid parking space prediction model: Integrating ARIMA, long short-term memory (LSTM), and backpropagation neural network (BPNN) for smart city development. <em>PEERJCS</em>, <em>11</em>, e2645. (<a href='https://doi.org/10.7717/peerj-cs.2645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parking space prediction is a significant aspect of smart cities. It is essential for addressing traffic congestion challenges and low parking availability in urban areas. The present research mainly focuses on proposing a novel scalable hybrid model for accurately predicting parking space. The proposed model works in two phases: in first phase, auto-regressive integrated moving average (ARIMA) and long short-term memory (LSTM) models are integrated. Further, in second phase, backpropagation neural network (BPNN) is used to improve the accuracy of parking space prediction by reducing number of errors. The model utilizes the ARIMA model for handling linear values and the LSTM model for targeting non-linear values of the dataset. The Melbourne Internet of Things (IoT) based dataset, is used for implementing the proposed hybrid model. It consists of the data collected from the sensors that are employed in smart parking areas of the city. Before analysis, data was pre-processed to remove noise from the dataset and real time information collected from different sensors to predict the results accurately. The proposed hybrid model achieves the minimum mean squared error (MSE), mean absolute error (MAE), and root mean squared error (RMSE) values of 0.32, 0.48, and 0.56, respectively. Further, to verify the generalizability of the proposed hybrid model, it is also implemented on the Harvard IoT-based dataset. It achieves the minimum MSE, MAE, and RMSE values of 0.31, 0.47, and 0.56, respectively. Therefore, the proposed hybrid model outperforms both datasets by achieving minimum error, even when compared with the performance of other existing models. The proposed hybrid model can potentially improve parking space prediction, contributing to sustainable and economical smart cities and enhancing the quality of life for citizens.},
  archive      = {J_PEERJCS},
  author       = {Anchal Dahiya and Pooja Mittal and Yogesh Kumar Sharma and Umesh Kumar Lilhore and Sarita Simaiya and Mohd Anul Haq and Mohammed A. Aleisa and Abdullah Alenizi},
  doi          = {10.7717/peerj-cs.2645},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2645},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hybrid parking space prediction model: Integrating ARIMA, long short-term memory (LSTM), and backpropagation neural network (BPNN) for smart city development},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Social media network public opinion emotion classification method based on multi-feature fusion and multi-scale hybrid neural network. <em>PEERJCS</em>, <em>11</em>, e2643. (<a href='https://doi.org/10.7717/peerj-cs.2643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of the internet, an increasing number of users express their subjective opinions on social media platforms. By analyzing the sentiment of these texts, we can gain insights into public sentiment, industry changes, and market trends, enabling timely adjustments and preemptive strategies. This article initially constructs vectors using semantic fusion and word order features. Subsequently, it develops a lexicon vector based on word similarity and leverages supervised corpora training to obtain a more pronounced transfer weight vector of sentiment intensity. A multi-feature fused emotional word vector is ultimately formed by concatenating and fusing these weighted transfer vectors. Experimental comparisons on two multi-class microblog comment datasets demonstrate that the multi-feature fusion (WOOSD-CNN) word vector model achieves notable improvements in sentiment polarity accuracy and categorization effectiveness. Additionally, for aspect-level sentiment analysis of user generated content (UGC) text, a unified learning framework based on an information interaction channel is proposed, which enables the team productivity center (TPC) task. Specifically, an information interaction channel is designed to assist the model in leveraging the latent interactive characteristics of text. An in-depth analysis addresses the label drift phenomenon between aspect term words, and a position-aware module is constructed to mitigate the local development plan (LDP) issue.},
  archive      = {J_PEERJCS},
  author       = {Yuan Yao and Xi Chen and Peng Zhang},
  doi          = {10.7717/peerj-cs.2643},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2643},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Social media network public opinion emotion classification method based on multi-feature fusion and multi-scale hybrid neural network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Secure software development: Leveraging application call graphs to detect security vulnerabilities. <em>PEERJCS</em>, <em>11</em>, e2641. (<a href='https://doi.org/10.7717/peerj-cs.2641'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inconsistency in software development standards frequently leads to vulnerabilities that can jeopardize an application’s cryptographic integrity. This situation can result in incomplete or flawed encryption processes. Vulnerabilities may manifest as missing, bypassed, or improperly executed encryption functions or the absence of critical cryptographic mechanisms, which eventually weaken security goals. This article introduces a thorough method for detecting vulnerabilities using dynamic and static analysis, focusing on a cryptographic function dominance tree. This strategy systematically minimizes the likelihood of integrity breaches in cryptographic applications. A layered and modular model is developed to maintain integrity by mapping the entire flow of cryptographic function calls across various components. The cryptographic function call graph and dominance tree are extracted and subsequently analyzed using an integrated dynamic and static technique. The extracted information undergoes strict evaluation against the anticipated function call sequence in the relevant cryptographic module to identify and localize potential security issues. Experimental findings demonstrate that the proposed method considerably enhances the accuracy and comprehensiveness of vulnerability detection in cryptographic applications, improving implementation security and resilience against misuse vulnerabilities.},
  archive      = {J_PEERJCS},
  author       = {Lei Yan and Guanghuai Zhao and Xiaohui Li and Pengxuan Sun},
  doi          = {10.7717/peerj-cs.2641},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2641},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Secure software development: Leveraging application call graphs to detect security vulnerabilities},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning with semantic ambiguity for unbiased scene graph generation. <em>PEERJCS</em>, <em>11</em>, e2639. (<a href='https://doi.org/10.7717/peerj-cs.2639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene graph generation (SGG) aims to identify and extract objects from images and elucidate their interrelations. This task faces two primary challenges. Firstly, the long-tail distribution of relation categories causes SGG models to favor high-frequency relations, such as “on” and “in”. Secondly, some subject-object pairs may have multiple reasonable relations, which often possess a certain degree of semantic similarity. However, the use of one-hot ground-truth relation labels does not effectively represent the semantic similarities and distinctions among relations. In response to these challenges, we propose a model-agnostic method named Mixup and Balanced Relation Learning (MBRL). This method assigns soft labels to samples exhibiting semantic ambiguities and optimizes model training by adjusting the loss weights for fine-grained and low-frequency relation samples. Its model-agnostic design facilitates seamless integration with diverse SGG models, enhancing their performance across various relation categories. Our approach is evaluated on widely-used datasets, including Visual Genome and Generalized Question Answering, both with over 100,000 images, providing rich visual contexts for scene graph model evaluation. Experimental results show that our method outperforms state-of-the-art approaches on multiple scene graph generation tasks, demonstrating significant improvements in both relation prediction accuracy and the handling of imbalanced data distributions.},
  archive      = {J_PEERJCS},
  author       = {Shanjin Zhong and Yang Cao and Qiaosen Chen and Jie Gong},
  doi          = {10.7717/peerj-cs.2639},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2639},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Learning with semantic ambiguity for unbiased scene graph generation},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design and implementation of an intelligent sports management system (ISMS) using wireless sensor networks. <em>PEERJCS</em>, <em>11</em>, e2637. (<a href='https://doi.org/10.7717/peerj-cs.2637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, growth in technology has significantly impacted various industries, including sports, health, e-commerce, and agriculture. Among these industries, the sports sector is experiencing significant transformation, which needs support in accurately monitoring athlete predicting and performance injuries arising due to traditional methods’ limitations. Keeping the above in mind, in this article, we present the Intelligent Sports Management System (ISMS) with the integration of wireless sensor networks (WSNs) and neural networks (NNs), which enhance athlete monitoring and injury prediction. Our proposed ISMS consists of several layers: user interface, business logic layer, data management layer, integration layer, analytics and AI layer, IoT layer, and security layer. To facilitate interactions for athletes, coaches, and administrators, our planned ISMS integrates a user-friendly interface accessible through web and mobile applications. Besides, scheduling and event management are managed by the business logic layer. Similarly, the data management layer can process and store comprehensive data from various sources. To ensure smooth data exchange, the integration layer connects the ISMS with third-party services, and the analytics and AI layer leverages machine learning to provide actionable insights on performance and outcomes. In addition, the IoT layer collects real-time data from sensors and wearable devices, which is essential for performance analysis and injury prevention. Finally, the security layer ensures data integrity and confidentiality with robust encryption and access controls. To evaluate the system performance in different scenarios, we performed many experiments, which show that the proposed ISMS model shows the system efficacy in improving accuracy (0.94), specificity (0.97), recall (0.91), precision (0.93), F1 score (0.95), mean absolute error (MAE) (0.6), mean square error (MSE) (0.8), and root mean square error (RMSE) (0.9), compared to traditional methods. From these results, it is clear that our suggested approach improves athlete performance monitoring, injury prevention plans, and training schedules by presenting a complete and novel solution for recent sports management.},
  archive      = {J_PEERJCS},
  author       = {ZhiGuo Zhu},
  doi          = {10.7717/peerj-cs.2637},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2637},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design and implementation of an intelligent sports management system (ISMS) using wireless sensor networks},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UltraTimTrack: A kalman-filter-based algorithm to track muscle fascicles in ultrasound image sequences. <em>PEERJCS</em>, <em>11</em>, e2636. (<a href='https://doi.org/10.7717/peerj-cs.2636'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Brightness-mode (B-mode) ultrasound is a valuable tool to non-invasively image skeletal muscle architectural changes during movement, but automatically tracking muscle fascicles remains a major challenge. Existing fascicle tracking algorithms either require time-consuming drift corrections or yield noisy estimates that require post-processing. We therefore aimed to develop an algorithm that tracks fascicles without drift and with low noise across a range of experimental conditions and image acquisition settings. Methods We applied a Kalman filter to combine fascicle length and fascicle angle estimates from existing and openly-available UltraTrack and TimTrack algorithms into a hybrid algorithm called UltraTimTrack. We applied the hybrid algorithm to ultrasound image sequences collected from the human medial gastrocnemius of healthy individuals (N = 8, four women), who performed cyclical submaximal plantar flexion contractions or remained at rest during passive ankle joint rotations at given frequencies and amplitudes whilst seated in a dynamometer chair. We quantified the algorithm’s tracking accuracy, noise, and drift as the respective mean, cycle-to-cycle variability, and accumulated between-contraction variability in fascicle length and fascicle angle. We expected UltraTimTrack’s estimates to be less noisy than TimTrack’s estimates and to drift less than UltraTrack’s estimates across a range of conditions and image acquisition settings. Results The proposed algorithm yielded low-noise estimates like UltraTrack and was drift-free like TimTrack across the broad range of conditions we tested. Over 120 cyclical contractions, fascicle length and fascicle angle deviations of UltraTimTrack accumulated to 2.1 ± 1.3 mm (mean ± sd) and 0.8 ± 0.7 deg, respectively. This was considerably less than UltraTrack (67.0 ± 59.3 mm, 9.3 ± 8.6 deg) and similar to TimTrack (1.9 ± 2.2 mm, 0.9 ± 1.0 deg). Average cycle-to-cycle variability of UltraTimTrack was 1.4 ± 0.4 mm and 0.6 ± 0.3 deg, which was similar to UltraTrack (1.1 ± 0.3 mm, 0.5 ± 0.1 deg) and less than TimTrack (3.5 ± 1.0 mm, 1.4 ± 0.5 deg). UltraTimTrack was less affected by experimental conditions and image acquisition settings than its parent algorithms. It also yielded similar or lower root-mean-square deviations from manual tracking for previously published image sequences (fascicle length: 2.3–2.6 mm, fascicle angle: 0.8–0.9 deg) compared with a recently-proposed hybrid algorithm (4.7 mm, 0.9 deg), and the recently-proposed DL_Track algorithm (3.8 mm, 3.9 deg). Furthermore, UltraTimTrack’s processing time (0.2 s per image) was at least five times shorter than that of these recently-proposed algorithms. Conclusion We developed a Kalman-filter-based algorithm to improve fascicle tracking from B-mode ultrasound image sequences. The proposed algorithm provides low-noise, drift-free estimates of muscle architectural changes that may better inform muscle function interpretations.},
  archive      = {J_PEERJCS},
  author       = {Tim J. van der Zee and Paolo Tecchio and Daniel Hahn and Brent J. Raiteri},
  doi          = {10.7717/peerj-cs.2636},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2636},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {UltraTimTrack: A kalman-filter-based algorithm to track muscle fascicles in ultrasound image sequences},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resolving ambiguity in natural language for enhancement of aspect-based sentiment analysis of hotel reviews. <em>PEERJCS</em>, <em>11</em>, e2635. (<a href='https://doi.org/10.7717/peerj-cs.2635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the ever-expanding digital landscape, the abundance of user-generated content on consumer platforms such as Booking and TripAdvisor offers a rich source of information for both travellers and hoteliers. Sentiment analysis, a fundamental research task of natural language processing (NLP) is used for mining sentiments and opinions within this vast reservoir of text reviews. A more specific type of sentiment analysis, i.e., aspect-based sentiment analysis (ABSA), is used when processing customer reviews is required. In ABSA, we aim to capture aspect-level sentiments and intricate relationships between various aspects within reviews. This article proposes a novel approach to ABSA by introducing a novel technique of word sense disambiguation (WSD) and integrating it with the Transformer architecture bidirectional encoder representations from Transformers (BERT) and graph convolutional networks (GCNs). The proposed approach resolves the intriguing ambiguities of the words and represents the review data as a complex graph structure, facilitating the modeling of intricate relationships between different aspects. The combination of bidirectional long short-term memory (BiLSTM) and GCN proves effective in capturing inter-dependencies among various aspects, providing a nuanced understanding of customer sentiments. The experiments are conducted on the RABSA dataset (an enhanced and richer hotel review data collection), and results demonstrate that our approach outperforms previous baselines, showcasing the effectiveness of integrating WSD in ABSA. Furthermore, an ablation study confirms the significant contribution of the WSD module to the overall performance. Moreover, we explore different similarity measures and find that cosine similarity yields the best results when identifying the real sense of a word in a given sentence using WordNet. The findings of our work and future work related to our work create lots of interest for people in the tourism and hospitality industry. This research gives another boost to the concept of the potential of NLP techniques in sentiment analysis. It emphasizes that if we combine the potential of NLP techniques along with state-of-the-art machine learning frameworks, we can shape the future of this field.},
  archive      = {J_PEERJCS},
  author       = {Asma Nadeem and Malik Muhammad Saad Missen and Mana Saleh Al Reshan and Muhammad Ali Memon and Yousef Asiri and Muhammad Ali Nizamani and Mohammad Alsulami and Asadullah Shaikh},
  doi          = {10.7717/peerj-cs.2635},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2635},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Resolving ambiguity in natural language for enhancement of aspect-based sentiment analysis of hotel reviews},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Integrating humanoid robots with human musicians for synchronized musical performances. <em>PEERJCS</em>, <em>11</em>, e2632. (<a href='https://doi.org/10.7717/peerj-cs.2632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Entertainment robotics has garnered significant attention in recent years, with researchers focusing on developing robots capable of performing a variety of tasks, including magic, drawing, dancing, and music. This article presents our research on forming a musical band that includes both humanoid robots and human musicians, with the goal of achieving natural synchronization and collaboration during musical performances. We utilized two of our humanoid robots for this project: Polaris, a mid-sized humanoid robot, as the drummer, and Oscar, a Robotis-OP3 humanoid robot, as the keyboardist. The technical implementation incorporated essential components such as visual servoing, human-robot interaction, and Robot Operating System (ROS), enabling seamless communication and coordination between the humanoid robots and the human musicians. The success of this collaborative effort can be both seen and heard through the following YouTube link: https://youtu.be/pFOyt1KKCfY?feature=shared.},
  archive      = {J_PEERJCS},
  author       = {MengCheng Lau and John Anderson and Jacky Baltes},
  doi          = {10.7717/peerj-cs.2632},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2632},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Integrating humanoid robots with human musicians for synchronized musical performances},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of an enhanced feature point matching algorithm utilizing 3D laser scanning technology for sculpture design. <em>PEERJCS</em>, <em>11</em>, e2628. (<a href='https://doi.org/10.7717/peerj-cs.2628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the aesthetic appreciation for art continues to grow, there is an increased demand for precision and detailed control in sculptural works. The advent of 3D laser scanning technology introduces transformative new tools and methodologies for refining correction systems in sculpture design. This article proposes a feature point matching algorithm based on fragment measurement and the iterative closest point (ICP) methodology, leveraging 3D laser scanning technology, namely Fragment Measurement Iterative Closest Point Feature Point Matching (FM-ICP-FPM). The FM-ICP-FPM approach uses the overlapping area of the two sculpture perspectives as a reference for attaching feature points. It employs the 3D measurement system to capture physical point cloud data from the two surfaces to enable the initial alignment of feature points. Feature vectors are generated by segmenting the region around the feature points and computing the intra-block gradient histogram. Subsequently, distance threshold conditions are set based on the constructed feature vectors and the preliminary feature point matches established during the coarse alignment to achieve precise feature point matching. Experimental results demonstrate the exceptional performance of the FM-ICP-FPM algorithm, achieving a sampling interval of 200. The correct matching rate reaches an impressive 100%, while the mean translation error (MTE) is a mere 154 mm, and the mean rotation angle error (MRAE) is 0.065 degrees. The indicator represents the degree of deviation in translation and rotation of the registered model, respectively. These low error values demonstrate that the FM-ICP-FPM algorithm excels in registration accuracy and can generate highly consistent three-dimensional models.},
  archive      = {J_PEERJCS},
  author       = {Xiaoxiong Zheng and Zhenwei Weng},
  doi          = {10.7717/peerj-cs.2628},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2628},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of an enhanced feature point matching algorithm utilizing 3D laser scanning technology for sculpture design},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving drug–target affinity prediction by adaptive self-supervised learning. <em>PEERJCS</em>, <em>11</em>, e2622. (<a href='https://doi.org/10.7717/peerj-cs.2622'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational drug-target affinity prediction is important for drug screening and discovery. Currently, self-supervised learning methods face two major challenges in drug-target affinity prediction. The first difficulty lies in the phenomenon of sample mismatch: self-supervised learning processes drug and target samples independently, while actual prediction requires the integration of drug-target pairs. Another challenge is the mismatch between the broadness of self-supervised learning objectives and the precision of biological mechanisms of drug-target affinity (i.e., the induced-fit principle). The former focuses on global feature extraction, while the latter emphasizes the importance of local precise matching. To address these issues, an adaptive self-supervised learning-based drug-target affinity prediction (ASSLDTA) was designed. ASSLDTA integrates a novel adaptive self-supervised learning (ASSL) module with a high-level feature learning network to extract the feature. The ASSL leverages a large amount of unlabeled training data to effectively capture low-level features of drugs and targets. Its goal is to maximize the retention of original feature information, thereby bridging the objective gap between self-supervised learning and drug-target affinity prediction and alleviating the sample mismatch problem. The high-level feature learning network, on the other hand, focuses on extracting effective high-level features for affinity prediction through a small amount of labeled data. Through this two-stage feature extraction design, each stage undertakes specific tasks, fully leveraging the advantages of each model while efficiently integrating information from different data sources, providing a more accurate and comprehensive solution for drug-target affinity prediction. In our experiments, ASSLDTA is much better than other deep methods, and the result of ASSLDTA is significantly increased by learning adaptive self-supervised learning-based features, which validates the effectiveness of our ASSLDTA.},
  archive      = {J_PEERJCS},
  author       = {Qing Ye and Yaxin Sun},
  doi          = {10.7717/peerj-cs.2622},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2622},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Improving drug–target affinity prediction by adaptive self-supervised learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). IMC-YOLO: A detection model for assisted razor clam fishing in the mudflat environment. <em>PEERJCS</em>, <em>11</em>, e2614. (<a href='https://doi.org/10.7717/peerj-cs.2614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In intertidal mudflat culture (IMC), the fishing efficiency and the degree of damage to nature have always been a pair of irreconcilable contradictions. To improve the efficiency of razor clam fishing and at the same time reduce the damage to the natural environment, in this study, a razor clam burrows dataset is established, and an intelligent razor clam fishing method is proposed, which realizes the accurate identification and counting of razor clam burrows by introducing the object detection technology into the razor clam fishing activity. A detection model called intertidal mudflat culture-You Only Look Once (IMC-YOLO) is proposed in this study by making improvements upon You Only Look Once version 8 (YOLOv8). In this study, firstly, at the end of the backbone network, the Iterative Attention-based Intrascale Feature Interaction (IAIFI) module module was designed and adopted to improve the model’s focus on advanced features. Subsequently, to improve the model’s effectiveness in detecting difficult targets such as razor clam burrows with small sizes, the head network was refactored. Then, FasterNet Block is used to replace the Bottleneck, which achieves more effective feature extraction while balancing detection accuracy and model size. Finally, the Three Branch Convolution Attention Mechanism (TBCAM) is proposed, which enables the model to focus on the specific region of interest more accurately. After testing, IMC-YOLO achieved mAP50, mAP50:95, and F1best of 0.963, 0.636, and 0.918, respectively, representing improvements of 2.2%, 3.5%, and 2.4% over the baseline model. Comparison with other mainstream object detection models confirmed that IMC-YOLO strikes a good balance between accuracy and numbers of parameters.},
  archive      = {J_PEERJCS},
  author       = {Jianhao Xu and Lijie Cao and Lanlan Pan and Xiankun Li and Lei Zhang and Hongyong Gao and Weibo Song},
  doi          = {10.7717/peerj-cs.2614},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2614},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {IMC-YOLO: A detection model for assisted razor clam fishing in the mudflat environment},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Process mining applications in healthcare: A systematic literature review. <em>PEERJCS</em>, <em>11</em>, e2613. (<a href='https://doi.org/10.7717/peerj-cs.2613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process mining applications in healthcare is a field widely investigated in the last years. Its diffusion is driven by increasing digitalization and the availability of large quantities of clinical data, enabling hospitals, clinics, and other healthcare organizations to optimize workflows, reduce operational costs, and improve asset management. The importance of process mining lies in its potential to identify inefficiencies in processes, standardize clinical practices, support evidence-based decisions and, in general, improve the quality of care provided. The article aims to systematically review the research landscape in the field of process mining in healthcare, providing an in-depth understanding of how process mining is applied in healthcare. It contributes to the existing literature by highlighting the following aspects: the specific research topics covered (i), the extent of use of various process mining algorithms in different healthcare applications, showing their adaptability and effectiveness in specific contexts (ii), and, finally, the types and characteristics of data employed in these studies, highlighting the needs and challenges related to data in healthcare process mining (iii). Through this systematic literature review, the article can support researchers in identifying the most valuable research topic to be explored by the scientific community working on process mining in healthcare. To achieve this goal, several articles focusing on the algorithms and data employed were selected and analyzed. The final discussion highlights current research gaps, suggesting future areas of investigation, and identifies critical issues and vulnerabilities of existing process mining applications in healthcare.},
  archive      = {J_PEERJCS},
  author       = {Lerina Aversano and Martina Iammarino and Antonella Madau and Giuseppe Pirlo and Gianfranco Semeraro},
  doi          = {10.7717/peerj-cs.2613},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2613},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Process mining applications in healthcare: A systematic literature review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of machine learning techniques for warfarin dosage prediction: A case study on the MIMIC-III dataset. <em>PEERJCS</em>, <em>11</em>, e2612. (<a href='https://doi.org/10.7717/peerj-cs.2612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Warfarin, a commonly prescribed anticoagulant, poses significant dosing challenges due to its narrow therapeutic range and high variability in patient responses. This study applies advanced machine learning techniques to improve the accuracy of international normalized ratio (INR) predictions using the MIMIC-III dataset, addressing the critical issue of missing data. By leveraging dimensionality reduction methods such as principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE), and advanced imputation techniques including denoising autoencoders (DAE) and generative adversarial networks (GAN), we achieved significant improvements in predictive accuracy. The integration of these methods substantially reduced prediction errors compared to traditional approaches. This research demonstrates the potential of machine learning (ML) models to provide more personalized and precise dosing strategies that reduce the risks of adverse drug events. Our method could integrate into clinical workflows to enhance anticoagulation therapy in cases of missing data, with potential applications in other complex medical treatments.},
  archive      = {J_PEERJCS},
  author       = {Aasim Ayaz Wani and Fatima Abeer},
  doi          = {10.7717/peerj-cs.2612},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2612},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Application of machine learning techniques for warfarin dosage prediction: A case study on the MIMIC-III dataset},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Design of low-carbon planning model for vehicle path based on adaptive multi-strategy ant colony optimization algorithm. <em>PEERJCS</em>, <em>11</em>, e2611. (<a href='https://doi.org/10.7717/peerj-cs.2611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contemporary transportation systems, the imperatives of route planning and optimization have become increasingly critical due to vehicles’ burgeoning number and complexity. This includes various vehicle types, such as electric and autonomous vehicles, each with specific needs. Additionally, varying speeds and operational requirements further complicate the process, demanding more sophisticated planning solutions. These systems frequently confront myriad challenges, including traffic congestion, intricate routes, and substantial energy consumption, which collectively undermine transportation efficiency, escalate energy usage, and contribute to environmental pollution. Hence, strategically planning and optimizing routes within complex traffic milieus are paramount to enhancing transportation efficacy and achieving low-carbon and environmentally sustainable objectives. This article proposes a vehicle path low-carbon planning model, Adaptive Cooperative Graph Neural Network (ACGNN), predicated on an adaptive multi-strategy ant colony optimization algorithm, addressing the vehicle path low-carbon planning conundrum. The proposed framework initially employs graph data from road networks and historical trajectories as model inputs, generating high-quality graph data through subgraph screening. Subsequently, a graph neural network (GNN) is utilized to optimize nodes and edges computationally. At the same time, the global search capability of the model is augmented via an ant colony optimization algorithm to ascertain the final optimized path. Experimental results demonstrate that ACGNN yields significant path planning outcomes on both public and custom-built datasets, surpassing the traditional Dijkstra’s shortest path algorithm, random graph network (RGN), and conventional GNN methodologies. Moreover, comparative analyses of various optimization methods on the custom-built dataset reveal that the ant colony optimization algorithm markedly outperforms the simulated annealing algorithm (SA) and particle swarm optimization algorithm (PSO). The method offers an innovative technical approach to vehicle path planning and is instrumental in advancing low-carbon and environmentally sustainable goals while enhancing transportation efficiency.},
  archive      = {J_PEERJCS},
  author       = {Qi Guo and Rui Li and Changjiang Zheng and Gwanggil Jeon},
  doi          = {10.7717/peerj-cs.2611},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2611},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Design of low-carbon planning model for vehicle path based on adaptive multi-strategy ant colony optimization algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALL-net: Integrating CNN and explainable-AI for enhanced diagnosis and interpretation of acute lymphoblastic leukemia. <em>PEERJCS</em>, <em>11</em>, e2600. (<a href='https://doi.org/10.7717/peerj-cs.2600'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a new model, ALL-Net, for the detection of acute lymphoblastic leukemia (ALL) using a custom convolutional neural network (CNN) architecture and explainable Artificial Intelligence (XAI). A dataset consisting of 3,256 peripheral blood smear (PBS) images belonging to four classes—benign (hematogones), and the other three Early B, Pre-B, and Pro-B, which are subtypes of ALL, are utilized for training and evaluation. The ALL-Net CNN is initially designed and trained on the PBS image dataset, achieving an impressive test accuracy of 97.85%. However, data augmentation techniques are applied to augment the benign class and address the class imbalance challenge. The augmented dataset is then used to retrain the ALL-Net, resulting in a notable improvement in test accuracy, reaching 99.32%. Along with accuracy, we have considered other evaluation metrics and the results illustrate the potential of ALLNet with an average precision of 99.35%, recall of 99.33%, and F1 score of 99.58%. Additionally, XAI techniques, specifically the Local Interpretable Model-Agnostic Explanations (LIME) algorithm is employed to interpret the model’s predictions, providing insights into the decision-making process of our ALL-Net CNN. These findings highlight the effectiveness of CNNs in accurately detecting ALL from PBS images and emphasize the importance of addressing data imbalance issues through appropriate preprocessing techniques at the same time demonstrating the usage of XAI in solving the black box approach of the deep learning models. The proposed ALL-Net outperformed EfficientNet, MobileNetV3, VGG-19, Xception, InceptionV3, ResNet50V2, VGG-16, and NASNetLarge except for DenseNet201 with a slight variation of 0.5%. Nevertheless, our ALL-Net model is much less complex than DenseNet201, allowing it to provide faster results. This highlights the need for a more customized and streamlined model, such as ALL-Net, specifically designed for ALL classification. The entire source code of our proposed CNN is publicly available at https://github.com/Abhiram014/ALL-Net-Detection-of-ALL-using-CNN-and-XAI.},
  archive      = {J_PEERJCS},
  author       = {Abhiram Thiriveedhi and Swetha Ghanta and Sujit Biswas and Ashok K. Pradhan},
  doi          = {10.7717/peerj-cs.2600},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2600},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {ALL-net: Integrating CNN and explainable-AI for enhanced diagnosis and interpretation of acute lymphoblastic leukemia},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A temporal knowledge graph reasoning model based on recurrent encoding and contrastive learning. <em>PEERJCS</em>, <em>11</em>, e2595. (<a href='https://doi.org/10.7717/peerj-cs.2595'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal knowledge graphs (TKGs) are critical tools for capturing the dynamic nature of facts that evolve over time, making them highly valuable in a broad spectrum of intelligent applications. In the domain of temporal knowledge graph extrapolation reasoning, the prediction of future occurrences is of great significance and presents considerable obstacles. While current models consider the fact changes over time and recognize that historical facts may recur, they often overlook the influence of past events on future predictions. Motivated by these considerations, this work introduces a novel temporal knowledge graph reasoning model, named Temporal Reasoning with Recurrent Encoding and Contrastive Learning (TRCL), which integrates recurrent encoding and contrastive learning techniques. The proposed model has the ability to capture the evolution of historical facts, generating representations of entities and relationships through recurrent encoding. Additionally, TRCL incorporates a global historical matrix to account for repeated historical occurrences and employs contrastive learning to alleviate the interference of historical facts in predicting future events. The TKG reasoning outcomes are subsequently derived through a time decoder. A quantity of experiments conducted on four benchmark datasets demonstrate the exceptional performance of the proposed TRCL model across a range of metrics, surpassing state-of-the-art TKG reasoning models. When compared to the strong baseline Time-Guided Recurrent Graph Network (TiRGN) model, the proposed TRCL achieves 1.03% improvements on ICEWS14 using mean reciprocal rank (MRR) evaluation metric. This innovative proposed method not only enhances the accuracy of TKG extrapolation, but also sets a new standard for robustness in dynamic knowledge graph applications, paving the way for future research and practical applications in predictive intelligence systems.},
  archive      = {J_PEERJCS},
  author       = {Weitong Liu and Khairunnisa Hasikin and Anis Salwa Mohd Khairuddin and Meizhen Liu and Xuechen Zhao},
  doi          = {10.7717/peerj-cs.2595},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2595},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A temporal knowledge graph reasoning model based on recurrent encoding and contrastive learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A review of deep learning in blink detection. <em>PEERJCS</em>, <em>11</em>, e2594. (<a href='https://doi.org/10.7717/peerj-cs.2594'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blink detection is a highly concerned research direction in the field of computer vision, which plays a key role in various application scenes such as human-computer interaction, fatigue detection and emotion perception. In recent years, with the rapid development of deep learning, the application of deep learning techniques for precise blink detection has emerged as a significant area of interest among researchers. Compared with traditional methods, the blink detection method based on deep learning offers superior feature learning ability and higher detection accuracy. However, the current research on blink detection based on deep learning lacks systematic summarization and comparison. Therefore, the aim of this article is to comprehensively review the research progress in deep learning-based blink detection methods and help researchers to have a clear understanding of the various approaches in this field. This article analyzes the progress made by several classical deep learning models in practical applications of eye blink detection while highlighting their respective strengths and weaknesses. Furthermore, it provides a comprehensive summary of commonly used datasets and evaluation metrics for blink detection. Finally, it discusses the challenges and future directions of deep learning for blink detection applications. Our analysis reveals that deep learning-based blink detection methods demonstrate strong performance in detection. However, they encounter several challenges, including training data imbalance, complex environment interference, real-time processing issues and application device limitations. By overcoming the challenges identified in this study, the application prospects of deep learning-based blink detection algorithms will be significantly enhanced.},
  archive      = {J_PEERJCS},
  author       = {Jianbin Xiong and Weikun Dai and Qi Wang and Xiangjun Dong and Baoyu Ye and Jianxiang Yang},
  doi          = {10.7717/peerj-cs.2594},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2594},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A review of deep learning in blink detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). OBC-YOLOv8: An improved road damage detection model based on YOLOv8. <em>PEERJCS</em>, <em>11</em>, e2593. (<a href='https://doi.org/10.7717/peerj-cs.2593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective and efficient detection of pavement distress is very important for the normal use and maintenance of roads. To achieve this goal, a new road damage detection method based on YOLOv8 is proposed in this article. Firstly, omni-dimensional dynamic convolution (ODConv) block is employed to better grasp the complex and diverse features of damage objects by making dynamic adjustment according to the features of input images. Secondly, to extract the global and local feature information simultaneously to better improve the feature extraction ability of the model, BoTNet is added to the end of the backbone, which can combine the advantages of convolutional neural network (CNN) and Transformer. Finally, the coordinate attention mechanism (CA) is incorporated into the Neck section to make more accurate speculations and enhance detection accuracy further which can effectively mitigate irrelevant feature interference. The new proposed model is named OBC-YOLOv8 and the experimental results on the RDD2022-China dataset demonstrate its superiority compared with baselines, with 1.8% and 1.6% increases in mean average precision 50 (mAP@0.5) and F1-score, respectively.},
  archive      = {J_PEERJCS},
  author       = {Shizheng Zhang and Zhihao Liu and Kunpeng Wang and Wanwei Huang and Pu Li},
  doi          = {10.7717/peerj-cs.2593},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2593},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {OBC-YOLOv8: An improved road damage detection model based on YOLOv8},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Evolving techniques in sentiment analysis: A comprehensive review. <em>PEERJCS</em>, <em>11</em>, e2592. (<a href='https://doi.org/10.7717/peerj-cs.2592'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion of social media and e-commerce platforms, an unprecedented volume of user-generated content has emerged, offering organizations, governments, and researchers invaluable insights into public sentiment. Yet, the vast and unstructured nature of this data challenges traditional analysis methods. Sentiment analysis, a specialized field within natural language processing, has evolved to meet these challenges by automating the detection and categorization of opinions and emotions in text. This review comprehensively examines the evolving techniques in sentiment analysis, detailing foundational processes such as data gathering and feature extraction. It explores a spectrum of methodologies, from classical word embedding techniques and machine learning algorithms to recent contextual embedding and advanced transformer models like Generative Pre-trained Transformer (GPT), Bidirectional Encoder Representations from Transformers (BERT), and T5. With a critical comparison of these methods, this article highlights their appropriate uses and limitations. Additionally, the review provides a thorough overview of current trends, insights into future directions, and a critical exploration of unresolved challenges. By synthesizing these developments, this review equips researchers with a solid foundation for assessing the current state of sentiment analysis and guiding future advancements in this dynamic field.},
  archive      = {J_PEERJCS},
  author       = {Mahander Kumar and Lal Khan and Hsien-Tsung Chang},
  doi          = {10.7717/peerj-cs.2592},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2592},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Evolving techniques in sentiment analysis: A comprehensive review},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fuzzy multi-objective optimization model to design a sustainable closed-loop manufacturing system. <em>PEERJCS</em>, <em>11</em>, e2591. (<a href='https://doi.org/10.7717/peerj-cs.2591'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Republicans and Democrats practically everywhere have been demonstrating concerns about environmental conservation to achieve sustainable development goals (SDGs) since the turn of the century. To promote fuel (energy) savings and a reduction in the amount of carbon dioxide CO2 emissions in several enterprises, actions have been taken based on the concepts described. This study proposes an environmentally friendly manufacturing system designed to minimize environmental impacts. Specifically, it aims to develop a sustainable manufacturing process that accounts for energy consumption and CO2 emissions from direct and indirect energy sources. A multi-objective mathematical model has been formulated, incorporating financial and environmental constraints, to minimize overall costs, energy consumption, and CO2 emissions within the manufacturing framework. The input model parameters for real-world situations are generally unpredictable, so a fuzzy multi-objective model will be developed as a way to handle it. The validity of the proposed ecological industrial design will be tested using a scenario-based approach. Results demonstrate the high reliability, applicability, and effectiveness of the proposed network when analyzed using the developed techniques.},
  archive      = {J_PEERJCS},
  author       = {Sajida Kousar and Asma Alvi and Nasreen Kausar and Harish Garg and Seifedine Kadry and Jungeun Kim},
  doi          = {10.7717/peerj-cs.2591},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2591},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fuzzy multi-objective optimization model to design a sustainable closed-loop manufacturing system},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel group tour trip recommender model for personalized travel systems. <em>PEERJCS</em>, <em>11</em>, e2589. (<a href='https://doi.org/10.7717/peerj-cs.2589'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning personalized travel itineraries for groups with diverse preferences is indeed challenging. This article proposes a novel group tour trip recommender model (GTTRM), which uses ant colony optimization (ACO) to optimize group satisfaction while minimizing conflicts between group members. Unlike existing models, the proposed GTTRM allows dynamic subgroup formation during the trip to handle conflicting preferences and provide tailored recommendations. Experimental results show that GTTRM significantly improves satisfaction levels for individual group members, outperforming state-of-the-art models in terms of both subgroup management and optimization efficiency.},
  archive      = {J_PEERJCS},
  author       = {Mohammed Alatiyyah},
  doi          = {10.7717/peerj-cs.2589},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2589},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {A novel group tour trip recommender model for personalized travel systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Expectation maximization—vector approximate message passing based generalized linear model for channel estimation in intelligent reflecting surface-assisted millimeter multi-user multiple-input multiple-output systems. <em>PEERJCS</em>, <em>11</em>, e2582. (<a href='https://doi.org/10.7717/peerj-cs.2582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel estimation poses a main challenge in intelligent reflecting surface (IRS)-assisted millimeter wave (mmWave) multi-user multiple-input multiple-output (MIMO) systems due to the substantial number of antennas at the base station (BS) and the passive reflective elements within the IRS lacking sufficient signal processing capabilities. This article addresses this challenge by proposing a channel estimation technique for IRS-assisted mmWave MIMO systems. The problem of channel estimation is normally taken as a compressed sensing (CS) problem, typically addressed through algorithms such as Orthogonal Matching Pursuit (OMP), Generalized Approximate Message Passing (GAMP), and Vector Approximate Message Passing with Expectation-Maximization (EM-VAMP). EM-VAMP demonstrates better performance only when a Gaussian mixture (GM) distribution is chosen as the prior for the sparse channel, especially at high signal-to-noise ratios (SNRs). To address this, the article introduces the application of generalized linear models (GLMs), extensions of standard linear models, providing increased flexibility in modeling data that deviates from Gaussian distribution. Numerical results unveil that the proposed Its EM-VAMP-GLM is much more robust to the existing OMP, GAMP and EM-LAMP algorithms.},
  archive      = {J_PEERJCS},
  author       = {Shoukath Ali K and Sajan P Philip and Arfat Ahmad Khan and Leeban Moses and Korhan Cengiz and Sedat Akleylek and Nikola Ivković},
  doi          = {10.7717/peerj-cs.2582},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2582},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Expectation maximization—vector approximate message passing based generalized linear model for channel estimation in intelligent reflecting surface-assisted millimeter multi-user multiple-input multiple-output systems},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust model predictive control for polytopic uncertain systems via a high-rate network with the FlexRay protocol. <em>PEERJCS</em>, <em>11</em>, e2580. (<a href='https://doi.org/10.7717/peerj-cs.2580'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, the robust model predictive control (RMPC) problem is investigated for a class of polytopic uncertain systems over high-rate networks whose signal exchanges are scheduled by the FlexRay protocol (FRP). During signal measurement, a high-rate network is applied to broadcast the data from the sensors to the controller efficiently. The FRP including the characteristics of event-triggered mechanism and the time-triggered mechanism is embedded into the high-rate network to regulate the data transmission in a circular period which can improve the flexibility of data transmission. With the aid of the Round-Robin and Try-Once-Discard protocols, a new expression of the measurement model is formulated by the use of certain data holding strategies. Subsequently, taking both high-rate networks and FRP into account, sufficient conditions are obtained by solving a time-varying terminal constraint set of an auxiliary optimization problem. In addition, an algorithm including both off-line and on-line parts is provided to find a sub-optimal solution. Lastly, two numerical simulations are carried out to substantiate the validity of the proposed RMPC strategy which is based on FRP and a high-rate network.},
  archive      = {J_PEERJCS},
  author       = {Jianhua Wang and Fuqiang Fan and Yanye Yu and Shuxin Du and Xiaorui Guo},
  doi          = {10.7717/peerj-cs.2580},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2580},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Robust model predictive control for polytopic uncertain systems via a high-rate network with the FlexRay protocol},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast binary logistic regression. <em>PEERJCS</em>, <em>11</em>, e2579. (<a href='https://doi.org/10.7717/peerj-cs.2579'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study presents a novel numerical approach that improves the training efficiency of binary logistic regression, a popular statistical model in the machine learning community. Our method achieves training times an order of magnitude faster than traditional logistic regression by employing a novel Soft-Plus approximation, which enables reformulation of logistic regression parameter estimation into matrix-vector form. We also adopt the Lf-norm penalty, which allows using fractional norms, including the L2-norm, L1-norm, and L0-norm, to regularize the model parameters. We put Lf-norm formulation in matrix-vector form, providing flexibility to include or exclude penalization of the intercept term when applying regularization. Furthermore, to address the common problem of collinear features, we apply singular value decomposition (SVD), resulting in a low-rank representation commonly used to reduce computational complexity while preserving essential features and mitigating noise. Moreover, our approach incorporates a randomized SVD alongside a newly developed SVD with row reduction (SVD-RR) method, which aims to manage datasets with many rows and features efficiently. This computational efficiency is crucial in developing a generalized model that requires repeated training over various parameters to balance bias and variance. We also demonstrate the effectiveness of our fast binary logistic regression (FBLR) method on various datasets from the OpenML repository in addition to synthetic datasets.},
  archive      = {J_PEERJCS},
  author       = {Nurdan Ayse Saran and Fatih Nar},
  doi          = {10.7717/peerj-cs.2579},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2579},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Fast binary logistic regression},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An integrated modeling, verification, and code generation for uncrewed aerial systems: Less cost and more efficiency. <em>PEERJCS</em>, <em>11</em>, e2575. (<a href='https://doi.org/10.7717/peerj-cs.2575'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Uncrewed Aerial Systems (UASs) are widely implemented in safety-critical fields such as industrial production, military operations, and disaster relief. Due to the diversity and complexity of implementation scenarios, UASs have become increasingly intricate. The challenge of designing and implementing highly reliable UASs while effectively controlling development costs and improving efficiency has been a pressing issue faced by academia and industry. To address this challenge, this article aims to examine an integrated method for modeling, verification, and code generation for UASs. This article begins to utilize Architecture Analysis and Design Language (AADL) to model UASs, proposing generic UAS models. Then, formal specifications describe a system's safety properties and functions based on these models. Finally, this article introduces a method to generate flight controller codes for UASs based on the verified models. Experiments demonstrate its effectiveness in pinpointing potential vulnerabilities in UASs during the early design phase and generating viable flight controller codes from the verified models. The proposed approach can also improve the efficiency of designing and verifying high-reliability UASs.},
  archive      = {J_PEERJCS},
  author       = {Jianyu Zhang and Long Zhang and Yixuan Wu and Linru Ma and Feng Yang},
  doi          = {10.7717/peerj-cs.2575},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2575},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {An integrated modeling, verification, and code generation for uncrewed aerial systems: Less cost and more efficiency},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning of the user behavior structure based on the time granularity analysis model. <em>PEERJCS</em>, <em>11</em>, e2573. (<a href='https://doi.org/10.7717/peerj-cs.2573'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The construction of a consumption pattern can realize the analysis of consumer characteristics and behaviors, identify the relationship between commodities, and provide technical support for commodity recommendation and market analysis. However the current studies on consumer behavior and consumption patterns are very limited, and most of them are based on market research data. This method of data collection has high cost, low data coverage, and lagging survey results. The algorithm proposed in this article analyzes purchasing data from e-commerce platforms and extracts short- and long-term consumption matrices of consumers. By further processing these two matrices and removing the difference in granularity in time and marginal substitution rate, these matrices are finally integrated to form one consumption pattern matrix that can describe the characteristics of consumer consumption behavior in a period of time. Extensive experiments on various domains demonstrate that our proposed method outperforms state-of-the-art baselines on synthetic and real-world datasets.},
  archive      = {J_PEERJCS},
  author       = {Lin Guo and Xiaoying Liu},
  doi          = {10.7717/peerj-cs.2573},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2573},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Learning of the user behavior structure based on the time granularity analysis model},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Foreign object debris detection in lane images using deep learning methodology. <em>PEERJCS</em>, <em>11</em>, e2570. (<a href='https://doi.org/10.7717/peerj-cs.2570'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background Foreign object debris (FOD) is an unwanted substance that damages vehicular systems, most commonly the wheels of vehicles. In airport runways, these foreign objects can damage the wheels or internal systems of planes, potentially leading to flight crashes. Surveys indicate that FOD-related damage costs over $4 billion annually, affecting airlines, airport tenants, and passengers. Current FOD clearance involves high-cost radars and significant manpower, and existing radar and camera-based surveillance methods are expensive to install. Methods This work proposes a video-based deep learning methodology to address the high cost of radar-based FOD detection. The proposed system consists of two modules for FOD detection: object classification and object localization. The classification module categorizes FOD into specific types of foreign objects. In the object localization module, these classified objects are pinpointed in video frames. Results The proposed system was experimentally tested with a large video dataset and compared with existing methods. The results demonstrated improved accuracy and robustness, allowing the FOD clearance team to quickly detect and remove foreign objects, thereby enhancing the safety and efficiency of airport runway operations.},
  archive      = {J_PEERJCS},
  author       = {Priyadharsini S. and Bhuvaneshwara Raja K. and Kousi Krishnan T. and Senthil Kumar Jagatheesaperumal and Bader Fahad Alkhamees and Mohammad Mehedi Hassan},
  doi          = {10.7717/peerj-cs.2570},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2570},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Foreign object debris detection in lane images using deep learning methodology},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid deep learning approach for brain tumor classification using EfficientNetB0 and novel quantum genetic algorithm. <em>PEERJCS</em>, <em>11</em>, e2556. (<a href='https://doi.org/10.7717/peerj-cs.2556'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most complex and life-threatening pathologies of the central nervous system is brain tumors. Correct diagnosis of these tumors plays an important role in determining the treatment plans of patients. Traditional classification methods often rely on manual assessments, which can be prone to error. Therefore, multiple classification of brain tumors has gained significant interest in recent years in both the medical and computer science fields. The use of artificial intelligence and machine learning, especially in the automatic classification of brain tumors, is increasing significantly. Deep learning models can achieve high accuracy when trained on datasets in diagnosis and classification. This study examined deep learning-based approaches for automatic multi-class classification of brain tumors, and a new approach combining deep learning and quantum genetic algorithms (QGA) was proposed. The powerful feature extraction ability of the pre-trained EfficientNetB0 was utilized and combined with this quantum genetic algorithms, a new approach was proposed. It is aimed to develop the feature selection method. With this hybrid method, high reliability and accuracy in brain tumor classification was achieved. The proposed model achieved high accuracy of 98.36% and 98.25%, respectively, with different data sets and significantly outperformed traditional methods. As a result, the proposed method offers a robust and scalable solution that will help classify brain tumors in early and accurate diagnosis and contribute to the field of medical imaging with patient outcomes.},
  archive      = {J_PEERJCS},
  author       = {Kerem Gencer and Gülcan Gencer},
  doi          = {10.7717/peerj-cs.2556},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2556},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Hybrid deep learning approach for brain tumor classification using EfficientNetB0 and novel quantum genetic algorithm},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN inversion and shifting: Recommending product modifications to sellers for better user preference. <em>PEERJCS</em>, <em>11</em>, e2553. (<a href='https://doi.org/10.7717/peerj-cs.2553'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In efforts to better accommodate users, numerous researchers have endeavored to model customer behavior, seeking to comprehend how they interact with diverse items within online platforms. This exploration has given rise to recommendation systems, which utilize customer similarity with other customers or customer-item interactions to suggest new items based on the existing item catalog. Since these systems primarily focus on enhancing customer experiences, they overlook providing insights to sellers that could help refine the aesthetics of their items and increase their customer coverage. In this study, we go beyond customer recommendations to propose a novel approach: suggesting aesthetic feedback to sellers in the form of refined item images informed by customer-item interactions learned by a recommender system from multiple consumers. These images could serve as guidance for sellers to adapt existing items to meet the dynamic preferences of multiple users simultaneously. To evaluate the effectiveness of our method, we design experiments showcasing how changing the number of consumers and the class of item image used affect the change in preference score. Through these experiments, we found that our methodology outperforms previous approaches by generating distinct, realistic images with user preference higher by 16.7%, thus bridging the gap between customer-centric recommendations and seller-oriented feedback.},
  archive      = {J_PEERJCS},
  author       = {Satyadwyoom Kumar and Abhijith Sharma and Apurva Narayan},
  doi          = {10.7717/peerj-cs.2553},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2553},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {GAN inversion and shifting: Recommending product modifications to sellers for better user preference},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAFE-CAST: Secure AI-federated enumeration for clustering-based automated surveillance and trust in machine-to-machine communication. <em>PEERJCS</em>, <em>11</em>, e2551. (<a href='https://doi.org/10.7717/peerj-cs.2551'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine-to-machine (M2M) communication within the Internet of Things (IoT) faces increasing security and efficiency challenges as networks proliferate. Existing approaches often struggle with balancing robust security measures and energy efficiency, leading to vulnerabilities and reduced performance in resource-constrained environments. To address these limitations, we propose SAFE-CAST, a novel secure AI-federated enumeration for clustering-based automated surveillance and trust framework. This study addresses critical security and efficiency challenges in M2M communication within the context of IoT. SAFE-CAST integrates several innovative components: (1) a federated learning approach using Lloyd’s K-means algorithm for secure clustering, (2) a quality diversity optimization algorithm (QDOA) for secure channel selection, (3) a dynamic trust management system utilizing blockchain technology, and (4) an adaptive multi-agent reinforcement learning for context-aware transmission scheme (AMARLCAT) to minimize latency and improve scalability. Theoretical analysis and extensive simulations using network simulator (NS)-3.26 demonstrate the superiority of SAFE-CAST over existing methods. The results show significant improvements in energy efficiency (21.6% reduction), throughput (14.5% increase), security strength (15.3% enhancement), latency (33.9% decrease), and packet loss rate (12.9% reduction) compared to state-of-the-art approaches. This comprehensive solution addresses the pressing need for robust, efficient, and secure M2M communication in the evolving landscape of IoT and edge computing.},
  archive      = {J_PEERJCS},
  author       = {Yusuf Kursat Tuncel and Kasım Öztoprak},
  doi          = {10.7717/peerj-cs.2551},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2551},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SAFE-CAST: Secure AI-federated enumeration for clustering-based automated surveillance and trust in machine-to-machine communication},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enabling smart parking for smart cities using internet of things (IoT) and machine learning. <em>PEERJCS</em>, <em>11</em>, e2544. (<a href='https://doi.org/10.7717/peerj-cs.2544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the escalating number of vehicles and the lack of parking spaces, the issue of parking has become a significant problem in major cities as it is a daily occurrence for educational institutions, companies, and government facilities, resulting in fuel wastage and time inefficiencies. In their work lives, employees often face problems when parking their cars in the work parking area. Finding a space for their vehicle can take a lot of time and effort, leading to late arrival for work. On the other hand, security guards have difficulty entering their employees’ cars. In this context, our proposed system attempts to address this pressing issue, which consists of two parts: one is a camera at the parking gate that recognizes the license plate using the Automatic Number Plate Recognition (ANPR) algorithm, where the camera captures the license plate and outputs the plate number using the optical character recognition (OCR) technique. After that, the resulting data is cross-referenced with database records for seamless entry authentication. This eliminates the need for security personnel to verify vehicle identities or stickers manually, streamlining access procedures. The second part is a camera in the car parks that distinguishes between vacant and available parking spaces and stores the data collected by the camera in the centralized database, enabling the real-time display of the nearest available parking spots on digital screens at entrance gates, significantly reducing the time and effort spent in locating parking spaces. Through this innovative solution, we aim to enhance urban mobility and alleviate the challenges associated with urban parking congestion, thereby resolving the problem of intelligent parking for smart cities with the help of machine learning.},
  archive      = {J_PEERJCS},
  author       = {Mofadal Alymani and Lenah Abdulaziz Almoqhem and Dhuha Ahmed Alabdulwahab and Abdulrahman Abdullah Alghamdi and Hussain Alshahrani and Khalid Raza},
  doi          = {10.7717/peerj-cs.2544},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2544},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Enabling smart parking for smart cities using internet of things (IoT) and machine learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot reranking with dense encoder models for news background linking. <em>PEERJCS</em>, <em>11</em>, e2534. (<a href='https://doi.org/10.7717/peerj-cs.2534'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {News background linking is the problem of finding useful links to resources that provide contextual background information for a given news article. Many systems were proposed to address this problem. Yet, the most effective and reproducible method, to date, used the entire input article as a search query to retrieve the background links by sparse retrieval. While being effective, that method is still far from being optimal. Furthermore, it only leverages the lexical matching signal between the input article and the candidate background links. Nevertheless, intuitively, there may exist resources with useful background information that do not lexically overlap with the input article’s vocabulary. While many studies proposed systems that adopt semantic matching for addressing news background linking, none were able to outperform the simple lexical-based matching method. In this paper, we investigate multiple methods to integrate both the lexical and semantic relevance signals for better reranking of candidate background links. To represent news articles in the semantic space, we compare multiple Transformer-based encoder models in a zero-shot setting without the need for any labeled data. Our results show that using a hierarchical aggregation of sentence-level representations generates a good semantic representation of news articles, which is then integrated with lexical matching to achieve a new state-of-the-art solution for the problem. We further show that a significant performance improvement is potentially attainable if the degree by which a semantic relevance signal is needed is accurately predicted per input article.},
  archive      = {J_PEERJCS},
  author       = {Marwa Essam and Tamer Elsayed},
  doi          = {10.7717/peerj-cs.2534},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2534},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Zero-shot reranking with dense encoder models for news background linking},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geographic recommender systems in e-commerce based on population. <em>PEERJCS</em>, <em>11</em>, e2525. (<a href='https://doi.org/10.7717/peerj-cs.2525'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advancements have significantly enhanced e-commerce, helping customers find the best products. One key development is recommendation systems, which personalize the shopping experience and boost sales. This paper explores a novel geographic recommendation system that uses demographic data, such as population density, age, and income, to refine recommendations. By integrating geographic and demographic information, like the population size of a country, businesses can tailor their offerings to regional preferences. This targeted approach aims to make recommendations more relevant by considering the behaviors and needs of different geographic areas. We sourced population data from The National Institute of Statistics (Tunisia, INS). This approach improves the importance of product recommendations for particular locations by customizing them based on demographic and geographic measures. The technique creates a better context-aware recommendation system that boosts customer happiness and business proceeds by fusing consumer behavior with extensive demographic data. The method also includes a mathematical model that considers population intensity to refine further recommendations established on the regional model.},
  archive      = {J_PEERJCS},
  author       = {Mohamed Shili and Osama Sohaib},
  doi          = {10.7717/peerj-cs.2525},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2525},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Geographic recommender systems in e-commerce based on population},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modified MobileNetV2 transfer learning model to detect road potholes. <em>PEERJCS</em>, <em>11</em>, e2519. (<a href='https://doi.org/10.7717/peerj-cs.2519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road damage often includes potholes, cracks, lane degradation, and surface shading. Potholes are a common problem in pavements. Detecting them is crucial for maintaining infrastructure and ensuring public safety. A thorough assessment of pavement conditions is required before planning any preventive repairs. Herein, we report the use of transfer learning and deep learning (DL) models to preprocess digital images of pavements for better pothole detection. Fourteen models were evaluated, including MobileNet, MobileNetV2, NASNetMobile, DenseNet121, DenseNet169, InceptionV3, DenseNet201, ResNet152V2, EfficientNetB0, InceptionResNetV2, Xception, and EfficientNetV2M. The study introduces a modified MobileNetV2 (MMNV2) model designed for fast and efficient feature extraction. The MMNV2 model exhibits improved classification, detection, and prediction accuracy by adding a five-layer pre-trained network to the MobileNetV2 framework. It combines deep learning, deep neural networks (DNN), and transfer learning, which resulted in better performance compared to other models. The MMNV2 model was tested using a dataset of 5,000 pavement images. A learning rate of 0.001 was used to optimize the model. It classified images into ‘normal’ or ‘pothole’ categories with 99.95% accuracy. The model also achieved 100% recall, 99.90% precision, 99.95% F1-score, and a 0.05% error rate. The MMNV2 model uses fewer parameters while delivering better results. It offers a promising solution for real-world applications in pothole detection and pavement assessment.},
  archive      = {J_PEERJCS},
  author       = {Neha Tanwar and Anil V. Turukmane},
  doi          = {10.7717/peerj-cs.2519},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2519},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Modified MobileNetV2 transfer learning model to detect road potholes},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediabetes risk classification algorithm via carotid bodies and K-means clustering technique. <em>PEERJCS</em>, <em>11</em>, e2516. (<a href='https://doi.org/10.7717/peerj-cs.2516'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diabetes is a disease that affects millions of people in the world and its early screening prevents serious health problems, also providing relief in the demand for healthcare services. In the search for methods to support early diagnosis, this article introduces a novel prediabetes risk classification algorithm (PRCA) for type-2 diabetes mellitus (T2DM), utilizing the chemosensitivity of carotid bodies (CB) and K-means clustering technique from the field of machine learning. Heart rate (HR) and respiratory rate (RR) data from eight volunteers with prediabetes and 25 without prediabetes were analyzed. Data were collected in basal conditions and after stimulation of the CBs by inhalation of 100% of oxygen and after ingestion of a standardized meal. During the analysis, a greater variability of groups was observed in people with prediabetes compared to the control group, particularly after inhalation of oxygen. The algorithm developed from these results showed an accuracy of 86% in classifying for prediabetes. This approach, centered on CB chemosensitivity deregulation in early disease stages, offers a nuanced detection method beyond conventional techniques. Moreover, the adaptable algorithm and clustering methodology hold promise as risk classifications for other diseases. Future endeavors aim to validate the algorithm through longitudinal studies tracking disease development among volunteers and expand the study’s scope to include a larger participant pool.},
  archive      = {J_PEERJCS},
  author       = {Rafael F. Pinheiro and Maria P. Guarino and Marlene Lages and Rui Fonseca-Pinto},
  doi          = {10.7717/peerj-cs.2516},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2516},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Prediabetes risk classification algorithm via carotid bodies and K-means clustering technique},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EFNet: Estimation of left ventricular ejection fraction from cardiac ultrasound videos using deep learning. <em>PEERJCS</em>, <em>11</em>, e2506. (<a href='https://doi.org/10.7717/peerj-cs.2506'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ejection fraction (EF) is a vital metric for assessing cardiovascular function through cardiac ultrasound. Manual evaluation is time-consuming and exhibits high variability among observers. Deep-learning techniques offer precise and autonomous EF predictions, yet these methods often lack explainability. Accurate heart failure prediction using cardiac ultrasound is challenging due to operator dependency and inconsistent video quality, resulting in significant interobserver variability. To address this, we developed a method integrating convolutional neural networks (CNN) and transformer models for direct EF estimation from ultrasound video scans. This article introduces a Residual Transformer Module (RTM) that extends a 3D ResNet-based network to analyze (2D + t) spatiotemporal cardiac ultrasound video scans. The proposed method, EFNet, utilizes cardiac ultrasound video images for end-to-end EF value prediction. Performance evaluation on the EchoNet-Dynamic dataset yielded a mean absolute error (MAE) of 3.7 and an R2 score of 0.82. Experimental results demonstrate that EFNet outperforms state-of-the-art techniques, providing accurate EF predictions.},
  archive      = {J_PEERJCS},
  author       = {Waqas Ali and Wesam Alsabban and Muhammad Shahbaz and Ali Al-Laith and Bassam Almogadwy},
  doi          = {10.7717/peerj-cs.2506},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2506},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {EFNet: Estimation of left ventricular ejection fraction from cardiac ultrasound videos using deep learning},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSSA: Multi-stage semantic-aware neural network for binary code similarity detection. <em>PEERJCS</em>, <em>11</em>, e2504. (<a href='https://doi.org/10.7717/peerj-cs.2504'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary code similarity detection (BCSD) aims to identify whether a pair of binary code snippets is similar, which is widely used for tasks such as malware analysis, patch analysis, and clone detection. Current state-of-the-art approaches are based on Transformer, which require substantial computation resources. Learning-based approaches remains room for optimization in learning the deeper semantics of binary code. In this paper, we propose MSSA, a multi-stage semantic-aware neural network for BCSD at the function level. It effectively integrates the semantic and structural information of assembly instructions within and between basic blocks, and across the entire function through four semantic-aware neural networks, achieving deep understanding of binary code semantics. MSSA is a lightweight model with only 0.38M parameters in its backbone network, suitable for deployment in CPU environments. Experimental results show that MSSA outperforms Gemini, Asm2Vec, SAFE, and jTrans in classification performance and ranks second only to the Transformer-based jTrans in retrieval performance.},
  archive      = {J_PEERJCS},
  author       = {Bangrui Wan and Jianjun Zhou and Ying Wang and Feng Chen and Ying Qian},
  doi          = {10.7717/peerj-cs.2504},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2504},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {MSSA: Multi-stage semantic-aware neural network for binary code similarity detection},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random k conditional nearest neighbor for high-dimensional data. <em>PEERJCS</em>, <em>11</em>, e2497. (<a href='https://doi.org/10.7717/peerj-cs.2497'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The k nearest neighbor (kNN) approach is a simple and effective algorithm for classification and a number of variants have been proposed based on the kNN algorithm. One of the limitations of kNN is that the method may be less effective when data contains many noisy features due to their non-informative influence in calculating distance. Additionally, information derived from nearest neighbors may be less meaningful in high-dimensional data. To address the limitation of nearest-neighbor based approaches in high-dimensional data, we propose to extend the k conditional nearest neighbor (kCNN) method which is an effective variant of kNN. The proposed approach aggregates multiple kCNN classifiers, each constructed from a randomly sampled feature subset. We also develop a score metric to weigh individual classifiers based on the level of separation of the feature subsets. We investigate the properties of the proposed method using simulation. Moreover, the experiments on gene expression datasets show that the proposed method is promising in terms of predictive classification performance.},
  archive      = {J_PEERJCS},
  author       = {Jiaxuan Lu and Hyukjun Gweon},
  doi          = {10.7717/peerj-cs.2497},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2497},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Random k conditional nearest neighbor for high-dimensional data},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Offline prompt reinforcement learning method based on feature extraction. <em>PEERJCS</em>, <em>11</em>, e2490. (<a href='https://doi.org/10.7717/peerj-cs.2490'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that combining Transformer and conditional strategies to deal with offline reinforcement learning can bring better results. However, in a conventional reinforcement learning scenario, the agent can receive a single frame of observations one by one according to its natural chronological sequence, but in Transformer, a series of observations are received at each step. Individual features cannot be extracted efficiently to make more accurate decisions, and it is still difficult to generalize effectively for data outside the distribution. We focus on the characteristic of few-shot learning in pre-trained models, and combine prompt learning to enhance the ability of real-time policy adjustment. By sampling the specific information in the offline dataset as trajectory samples, the task information is encoded to help the pre-trained model quickly understand the task characteristics and the sequence generation paradigm to quickly adapt to the downstream tasks. In order to understand the dependencies in the sequence more accurately, we also divide the fixed-size state information blocks in the input trajectory, extract the features of the segmented sub-blocks respectively, and finally encode the whole sequence into the GPT model to generate decisions more accurately. Experiments show that the proposed method achieves better performance than the baseline method in related tasks, can be generalized to new environments and tasks better, and effectively improves the stability and accuracy of agent decision making.},
  archive      = {J_PEERJCS},
  author       = {Tianlei Yao and Xiliang Chen and Yi Yao and Weiye Huang and Zhaoyang Chen},
  doi          = {10.7717/peerj-cs.2490},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2490},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Offline prompt reinforcement learning method based on feature extraction},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). On the development of diagnostic support algorithms based on CPET biosignals data via machine learning and wavelets. <em>PEERJCS</em>, <em>11</em>, e2474. (<a href='https://doi.org/10.7717/peerj-cs.2474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For preventing health complications and reducing the strain on healthcare systems, early identification of diseases is imperative. In this context, artificial intelligence has become increasingly prominent in the field of medicine, offering essential support for disease diagnosis. This article introduces an algorithm that builds upon an earlier methodology to assess biosignals acquired through cardiopulmonary exercise testing (CPET) for identifying metabolic syndrome (MS), heart failure (HF), and healthy individuals (H). Leveraging support vector machine (SVM) technology, a well-known machine learning classification method, in combination with wavelet transforms for feature extraction, the algorithm takes an innovative approach. The model was trained on CPET data from 45 participants, including 15 with MS, 15 with HF, and 15 healthy controls. For binary classification tasks, the SVM with a polynomial kernel and 5-level wavelet transform (SVM-POL-BW5) outperformed similar methods described in the literature. Moreover, one of the main contributions of this study is the development of a multi-class classification algorithm using the SVM employing a linear kernel and 3-level wavelet transforms (SVM-LIN-MW3), reaching an average accuracy of 95%. In conclusion, the application of SVM-based algorithms combined with wavelet transforms to analyze CPET data shows promise in diagnosing various diseases, highlighting their adaptability and broader potential applications in healthcare.},
  archive      = {J_PEERJCS},
  author       = {Rafael F. Pinheiro and Rui Fonseca-Pinto},
  doi          = {10.7717/peerj-cs.2474},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2474},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {On the development of diagnostic support algorithms based on CPET biosignals data via machine learning and wavelets},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Edge and texture aware image denoising using median noise residue U-net with hand-crafted features. <em>PEERJCS</em>, <em>11</em>, e2449. (<a href='https://doi.org/10.7717/peerj-cs.2449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image denoising is a complex task that always yields an approximated version of the clean image. Unfortunately, the existing works have focussed only on the peak signal to noise ratio (PSNR) metric and have shown no attention to edge features in a reconstructed image. Although fully convolution neural networks (CNN) are capable of removing the noise using kernel filters and automatic extraction of features, it has failed to reconstruct the images for higher values of noise standard deviation. Additionally, deep learning models require a huge database to learn better from the inputs. This, in turn, increases the computational complexity and memory requirement. Therefore, we propose the Median Noise Residue U-Net (MNRU-Net) with a limited training database without involving image augmentation. In the proposed work, the learning capability of the traditional U-Net model was increased by adding hand-crafted features in the input layers of the U-Net. Further, an approximate version of the noise estimated from the median filter and the gradient information of the image were used to improve the performance of U-Net. Later, the performance of MNRU-Net was evaluated based on PSNR, structural similarity, and figure of merit for different noise standard deviations of 15, 25, and 50 respectively. It is witnessed that the results gained from the suggested work are better than the results yielded by complex denoising models such as the robust deformed denoising CNN (RDDCNN). This work emphasizes that the skip connections along with the hand-crafted features could improve the performance at higher noise levels by using this simple architecture. In addition, the model was found to be less expensive, with low computational complexity.},
  archive      = {J_PEERJCS},
  author       = {Soniya S. and Sriharipriya K. C.},
  doi          = {10.7717/peerj-cs.2449},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2449},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Edge and texture aware image denoising using median noise residue U-net with hand-crafted features},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AI augmented edge and fog computing for internet of health things (IoHT). <em>PEERJCS</em>, <em>11</em>, e2431. (<a href='https://doi.org/10.7717/peerj-cs.2431'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Patients today seek a more advanced and personalized health-care system that keeps up with the pace of modern living. Cloud computing delivers resources over the Internet and enables the deployment of an infinite number of applications to provide services to many sectors. The primary limitation of these cloud frameworks right now is their limited scalability, which results in their inability to meet needs. An edge/fog computing environment, paired with current computing techniques, is the answer to fulfill the energy efficiency and latency requirements for the real-time collection and analysis of health data. Additionally, the Internet of Things (IoT) revolution has been essential in changing contemporary healthcare systems by integrating social, economic, and technological perspectives. This requires transitioning from unadventurous healthcare systems to more adapted healthcare systems that allow patients to be identified, managed, and evaluated more easily. These techniques allow data from many sources to be integrated to effectively assess patient health status and predict potential preventive actions. A subset of the Internet of Things, the Internet of Health Things (IoHT) enables the remote exchange of data for physical processes like patient monitoring, treatment progress, observation, and consultation. Previous surveys related to healthcare mainly focused on architecture and networking, which left untouched important aspects of smart systems like optimal computing techniques such as artificial intelligence, deep learning, advanced technologies, and services that includes 5G and unified communication as a service (UCaaS). This study aims to examine future and existing fog and edge computing architectures and methods that have been augmented with artificial intelligence (AI) for use in healthcare applications, as well as defining the demands and challenges of incorporating fog and edge computing technology in IoHT, thereby helping healthcare professionals and technicians identify the relevant technologies required based on their need for developing IoHT frameworks for remote healthcare. Among the crucial elements to take into account in an IoHT framework are efficient resource management, low latency, and strong security. This review addresses several machine learning techniques for efficient resource management in the IoT, where machine learning (ML) and AI are crucial. It has been noted how the use of modern technologies, such as narrow band-IoT (NB-IoT) for wider coverage and Blockchain technology for security, is transforming IoHT. The last part of the review focuses on the future challenges posed by advanced technologies and services. This study provides prospective research suggestions for enhancing edge and fog computing services for healthcare with modern technologies in order to give patients with an improved quality of life.},
  archive      = {J_PEERJCS},
  author       = {Deepika Rajagopal and Pradeep Kumar Thimma Subramanian},
  doi          = {10.7717/peerj-cs.2431},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2431},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {AI augmented edge and fog computing for internet of health things (IoHT)},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NOMA-MIMO in 5G network: A detailed survey on enhancing data rate. <em>PEERJCS</em>, <em>11</em>, e2388. (<a href='https://doi.org/10.7717/peerj-cs.2388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-orthogonal multiple access (NOMA) is a technology that leverages user channel gains, offers higher spectral efficiency, improves user fairness, better cell-edge throughput, increased reliability, and low latency, making it a potential technology for the next generation of cellular networks. The application of NOMA in the power domain (NOMA-PD) with multiple-input multiple-output (MIMO) and other emerging technologies allows to achieve the demand for higher data rates in next-generation networks. This survey aims to funnel down NOMA MIMO resource allocation issues and different optimization problems that exist in the literature to enhance the data rate. We examine the most recent NOMA-MIMO clustering, power allocation, and joint allocation schemes and analyze various parameters used in optimization methods to design 5G systems. We finally identify a promising research problem based on the signal-to-interference-plus-noise ratio (SINR) parameter in the context of NOMA-PD with MIMO configuration.},
  archive      = {J_PEERJCS},
  author       = {Murad Halabouni and Mardeni Roslee and Sufian Mitani and Osama Abuajwa and Anwar Osman and Fatimah Zaharah binti Ali and Athar Waseem},
  doi          = {10.7717/peerj-cs.2388},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2388},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {NOMA-MIMO in 5G network: A detailed survey on enhancing data rate},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prediction of sentiment polarity in restaurant reviews using an ordinal regression approach based on evolutionary XGBoost. <em>PEERJCS</em>, <em>11</em>, e2370. (<a href='https://doi.org/10.7717/peerj-cs.2370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the business world shifts to the web and tremendous amounts of data become available on multilingual mobile applications, new business and research challenges and opportunities have been explored. This research aims to intensify the usage of data analytics, machine learning, and sentiment analysis of textual data to classify customers’ reviews, feedback, and ratings of businesses in Jordan’s food and restaurant industry. The main methods used in this research were sentiment polarity (to address the challenges posed by businesses to automatically apply text analysis) and bio-metric techniques (to systematically identify users’ emotional states, so reviews can be thoroughly understood). The research was extended to deal with reviews in Arabic, dialectic Arabic, and English, with the main focus on the Arabic language, as the application examined (Talabat) is based in Jordan. Arabic and English reviews were collected from the application, and a new model was proposed to sentimentally analyze reviews. The proposed model has four main stages: data collection, data preparation, model building, and model evaluation. The main purpose of this research is to study the problem expressed above using a model of ordinal regression to overcome issues related to misclassification. Additionally, an automatic multi-language prediction approach for online restaurant reviews was proposed by combining the eXtreme gradient boosting (XGBoost) and particle swarm optimization (PSO) techniques for the ordinal regression of these reviews. The proposed PSO-XGB algorithm showed superior results when compared to support vector machine (SVM) and other optimization methods in terms of root mean square error (RMSE) for the English and Arabic datasets. Specifically, for the Arabic dataset, PSO-XGB achieved an RMSE value of 0.7722, whereas PSO-SVM achieved an RSME value of 0.9988.},
  archive      = {J_PEERJCS},
  author       = {Dana A. Al-Qudah and Ala’ M. Al-Zoubi and Alexandra I. Cristea and Juan J. Merelo-Guervós and Pedro A. Castillo and Hossam Faris},
  doi          = {10.7717/peerj-cs.2370},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2370},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Prediction of sentiment polarity in restaurant reviews using an ordinal regression approach based on evolutionary XGBoost},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep gradient reinforcement learning for music improvisation in cloud computing framework. <em>PEERJCS</em>, <em>11</em>, e2265. (<a href='https://doi.org/10.7717/peerj-cs.2265'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence (AI) in music improvisation offers promising new avenues for developing human creativity. The difficulty of writing dynamic, flexible musical compositions in real time is discussed in this article. We explore using reinforcement learning (RL) techniques to create more interactive and responsive music creation systems. Here, the musical structures train an RL agent to navigate the complex space of musical possibilities to provide improvisations. The melodic framework in the input musical data is initially identified using bi-directional gated recurrent units. The lyrical concepts such as notes, chords, and rhythms from the recognised framework are transformed into a format suitable for RL input. The deep gradient-based reinforcement learning technique used in this research formulates a reward system that directs the agent to compose aesthetically intriguing and harmonically cohesive musical improvisations. The improvised music is further rendered in the MIDI format. The Bach Chorales dataset with six different attributes relevant to musical compositions is employed in implementing the present research. The model was set up in a containerised cloud environment and controlled for smooth load distribution. Five different parameters, such as pitch frequency (PF), standard pitch delay (SPD), average distance between peaks (ADP), note duration gradient (NDG) and pitch class gradient (PCG), are leveraged to assess the quality of the improvised music. The proposed model obtains +0.15 of PF, −0.43 of SPD, −0.07 of ADP and 0.0041 NDG, which is a better value than other improvisation methods.},
  archive      = {J_PEERJCS},
  author       = {Fadwa Alrowais and Munya A. Arasi and Saud S. Alotaibi and Mohammed Alonazi and Radwa Marzouk and Ahmed S. Salama},
  doi          = {10.7717/peerj-cs.2265},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2265},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {Deep gradient reinforcement learning for music improvisation in cloud computing framework},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SLFCNet: An ultra-lightweight and efficient strawberry feature classification network. <em>PEERJCS</em>, <em>11</em>, e2085. (<a href='https://doi.org/10.7717/peerj-cs.2085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background As modern agricultural technology advances, the automated detection, classification, and harvesting of strawberries have become an inevitable trend. Among these tasks, the classification of strawberries stands as a pivotal juncture. Nevertheless, existing object detection methods struggle with substantial computational demands, high resource utilization, and reduced detection efficiency. These challenges make deployment on edge devices difficult and lead to suboptimal user experiences. Methods In this study, we have developed a lightweight model capable of real-time detection and classification of strawberry fruit, named the Strawberry Lightweight Feature Classify Network (SLFCNet). This innovative system incorporates a lightweight encoder and a self-designed feature extraction module called the Combined Convolutional Concatenation and Sequential Convolutional (C3SC). While maintaining model compactness, this architecture significantly enhances its feature decoding capabilities. To evaluate the model’s generalization potential, we utilized a high-resolution strawberry dataset collected directly from the fields. By employing image augmentation techniques, we conducted experimental comparisons between manually counted data and the model’s inference-based detection and classification results. Results The SLFCNet model achieves an average precision of 98.9% in the mAP@0.5 metric, with a precision rate of 94.7% and a recall rate of 93.2%. Notably, SLFCNet features a streamlined design, resulting in a compact model size of only 3.57 MB. On an economical GTX 1080 Ti GPU, the processing time per image is a mere 4.1 ms. This indicates that the model can smoothly run on edge devices, ensuring real-time performance. Thus, it emerges as a novel solution for the automation and management of strawberry harvesting, providing real-time performance and presenting a new solution for the automatic management of strawberry picking.},
  archive      = {J_PEERJCS},
  author       = {Wenchao Xu and Yangxu Wang and Jiahao Yang},
  doi          = {10.7717/peerj-cs.2085},
  journal      = {PeerJ Computer Science},
  month        = {1},
  pages        = {e2085},
  shortjournal = {PeerJ Comput. Sci.},
  title        = {SLFCNet: An ultra-lightweight and efficient strawberry feature classification network},
  volume       = {11},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
